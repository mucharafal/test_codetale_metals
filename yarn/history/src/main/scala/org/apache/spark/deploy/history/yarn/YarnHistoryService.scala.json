[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: too many spaces before `extends`\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-15T23:07:47Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Is there any benefit in extending Hadoop's `AbstractService`? No other code in Spark uses it, there's no generic \"service lifecycle handling\" code in Spark, so it just seems to be extending that class for the sake of extending it.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:45:25Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "I see that it seems to be related to `YarnExtensionService`. Let me look at that. That might be something that could be pulled out as a separate PR, to make the code more easily reviewable.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:49:23Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "1. It makes it easier to aggregate up things, and it has a robust model for thread-safe subclassing through a lifecycle. Most of YARN is a composition of these; in fact the spark YARN client and AM could adopt it for better management of `AMRMClient`, `YarnClient`, `YarnExtensionServices` & the like\n2. The hive-thriftserver code _almost_ uses it. More specifically, it used a cut-and-paste of the Hadoop 2.04-alpha era and then abuses java reflection to bypass bits of the direct parent. Once Hive moves to Hadoop 2.2+ only then it can be moved back into the YARN model â€”though once you modify Hive 1.2.x to provide override points the spark-hivethriftserver module doesn't need the `ReflectedCompositeService` hack any more.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T14:16:23Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: ordering. (I'll stop pointing these out...)\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-15T23:09:15Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "sorry, I'd thought Idea was getting it right. I'll review all of them and see if that 3rd party organiser helps. Keeping the imports organised is critical given how brittle it is across patches.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T14:21:17Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: add blank line before\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-15T23:10:01Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: `private[yarn]`\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-15T23:11:19Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: I'm not a fan of throwing `Exception`. Either just call `timelineClient.get` or throw a better exception (e.g. `require(timelineClient.isDefined)`.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-15T23:13:24Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "OK\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T14:23:09Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "It would be nice to have a better doc - like, explaining what the timeline domain is.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-15T23:16:24Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "OK\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T14:23:15Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Slightly less verbose version:\n\n```\nval aclsOn = sparkConf.getBoolean(\"spark.acls.enable\",\n  sparkConf.getBoolean(\"spark.ui.acls.enable\", false))\n```\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-15T23:17:32Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse("
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "minor, but this feels a little easier to read to me:\n\n```\n(Seq(current) ++ adminAcls ++ modifyAcls ++ viewAcls).mkString(\" \")\n```\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-15T23:19:36Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "done for both lines\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T14:38:39Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This fits in the previous line. The line length limit is 100 chars.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-15T23:20:19Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`serviceStart(): Unit = {`\n\nUsage of this syntax is deprecated in Scala, although 2.10 doesn't really complain about it.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-15T23:25:30Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "-still bad practise though; fixed\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T14:39:46Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Does this need to be a daemon thread?\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-15T23:29:18Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "It ensures that events listened to are pushed out to the timeline service on a separate thread, so if the endpoint is (temporarily) unreachable then the event reporter thread isn't blocked. It also allows for some time for events to be pushed out at the end of the run and yet still stop the service after a delay (we don't want AM teardown to block because the timeline endpoint is playing up).\n\nThe alternative would be for synchronous publishing in the event listener, with the shutdown logic spinning something off there and then. In some respects it would actually make testing easier â€”but what would the impact on the rest of the spark listeners be if it was blocking?\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T14:45:45Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "I don't see how your answer has anything to do with `eventHandlingThread.get.setDaemon(true)`.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T16:36:27Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Maybe I missed something, but why do you need this when the timeline client is disabled?\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-15T23:30:06Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "Primarily for testing. I could make that only take place if some test flag was set on the config\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T14:46:16Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This fits in the previous line.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-15T23:30:22Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set."
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This is indented too far.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-15T23:30:39Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`serviceStop(): Unit = {`\n\nI'll stop pointing these out.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-15T23:34:17Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "I'll reread everything to make sure I'm not doing it anywhere else\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T14:48:10Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: `.synchronized`\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-15T23:35:50Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "done\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T14:48:15Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: `stopTimelineClient()`. convention is to use `()` for anything but simple getters.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-15T23:37:04Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "OK\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T14:48:44Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "IIRC, you can only get one error here because you're just sending one entity in the `putEntities` call, right? If that's the case, the code is correct but a little odd (since it records `errors.size()` instead of `1` to indicate you know there's a single error), so maybe a comment?\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-15T23:41:45Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "I'm not sure now. Go with the \"1\" everywhere, or actually look for >1 event? It is probably safest to handle them all, which I will do. \n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T15:17:15Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "unnecessary comment.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-15T23:42:00Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "including '$e' in the log message is redundant (also in other places).\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-15T23:42:46Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "OK. Historical habit from fielding support issues related to log traces of loggers set up to not log the stack trace.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T15:18:47Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "parentheses not necessary? (also below)\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-15T23:44:40Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "nevermind; that would require a different syntax. either is fine.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-15T23:45:02Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "IntelliJ told me off if I didn't have case classes without the parentheses\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T15:20:58Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Why do you need a lock? Isn't this single-threaded, as in, there's a single thread going over the action queue?\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-15T23:46:54Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "it probably dates from a time when the service stop code could also issue operations -culling\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T15:29:43Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Is this log useful?\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-15T23:47:53Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {\n+      logDebug(s\"Processing event $event\")\n+      if (eventsProcessed.get() % 1000 == 0) {\n+        logDebug(s\"${eventsProcessed} events are processed\")"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "I think that first logDebug went in during some low-level debugging. Cut that and instead you get one log/1000 events, which is a lot less noisy\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T15:32:21Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {\n+      logDebug(s\"Processing event $event\")\n+      if (eventsProcessed.get() % 1000 == 0) {\n+        logDebug(s\"${eventsProcessed} events are processed\")"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`val` not needed; just do `event.sparkEvent match { ...`\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-15T23:48:22Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {\n+      logDebug(s\"Processing event $event\")\n+      if (eventsProcessed.get() % 1000 == 0) {\n+        logDebug(s\"${eventsProcessed} events are processed\")\n+      }\n+      val sparkEvent = event.sparkEvent"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Can this ever happen?\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-15T23:49:13Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {\n+      logDebug(s\"Processing event $event\")\n+      if (eventsProcessed.get() % 1000 == 0) {\n+        logDebug(s\"${eventsProcessed} events are processed\")\n+      }\n+      val sparkEvent = event.sparkEvent\n+      sparkEvent match {\n+        case start: SparkListenerApplicationStart =>\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logDebug(s\"Handling application start event: $event\")\n+          appName = start.appName\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "not any more; removed lock\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T15:33:37Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {\n+      logDebug(s\"Processing event $event\")\n+      if (eventsProcessed.get() % 1000 == 0) {\n+        logDebug(s\"${eventsProcessed} events are processed\")\n+      }\n+      val sparkEvent = event.sparkEvent\n+      sparkEvent match {\n+        case start: SparkListenerApplicationStart =>\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logDebug(s\"Handling application start event: $event\")\n+          appName = start.appName\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "I'm a little confused about this field and its sibling `appEndEventProcessed`. What are they trying to track exactly, and why?\n\nShouldn't you refuse to record anything until the start event arrives, maybe buffering any events that arrive before it?\n\nWhat's the ultimate behavior if this event is not recorded? Won't the history server get really confused?\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-15T23:54:09Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {\n+      logDebug(s\"Processing event $event\")\n+      if (eventsProcessed.get() % 1000 == 0) {\n+        logDebug(s\"${eventsProcessed} events are processed\")\n+      }\n+      val sparkEvent = event.sparkEvent\n+      sparkEvent match {\n+        case start: SparkListenerApplicationStart =>\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logDebug(s\"Handling application start event: $event\")\n+          appName = start.appName\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {\n+            startTime = now()\n+          }\n+          setContextAppAndAttemptInfo(\n+            start.appId,\n+            start.appAttemptId)\n+          // flush old entity\n+          resetCurrentEntity\n+          appStartEventProcessed.set(true)"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "You can get events coming in before the app start, in particular {{onEnvironmentUpdate}} comes in. It's currently going out as an event using the entity built from the Yarn app id (which is guaranteed to be unique cluster-wide, and known from the moment the service starts). \n\nSome tests did re-use the queue, but they should all have been moved off it with the move to multi-attempts (and fixed entity events).\n\nThat's an interesting q. about event history without the full list of events ... I don't think there's any test for it.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T16:41:02Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {\n+      logDebug(s\"Processing event $event\")\n+      if (eventsProcessed.get() % 1000 == 0) {\n+        logDebug(s\"${eventsProcessed} events are processed\")\n+      }\n+      val sparkEvent = event.sparkEvent\n+      sparkEvent match {\n+        case start: SparkListenerApplicationStart =>\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logDebug(s\"Handling application start event: $event\")\n+          appName = start.appName\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {\n+            startTime = now()\n+          }\n+          setContextAppAndAttemptInfo(\n+            start.appId,\n+            start.appAttemptId)\n+          // flush old entity\n+          resetCurrentEntity\n+          appStartEventProcessed.set(true)"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`en` is unused\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-15T23:55:06Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {\n+      logDebug(s\"Processing event $event\")\n+      if (eventsProcessed.get() % 1000 == 0) {\n+        logDebug(s\"${eventsProcessed} events are processed\")\n+      }\n+      val sparkEvent = event.sparkEvent\n+      sparkEvent match {\n+        case start: SparkListenerApplicationStart =>\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logDebug(s\"Handling application start event: $event\")\n+          appName = start.appName\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {\n+            startTime = now()\n+          }\n+          setContextAppAndAttemptInfo(\n+            start.appId,\n+            start.appAttemptId)\n+          // flush old entity\n+          resetCurrentEntity\n+          appStartEventProcessed.set(true)\n+          appEndEventProcessed.set(false)\n+          // force create the new one\n+          val en = started(getOrCreateCurrentEntity, startTime)"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "cutting\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T16:41:37Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {\n+      logDebug(s\"Processing event $event\")\n+      if (eventsProcessed.get() % 1000 == 0) {\n+        logDebug(s\"${eventsProcessed} events are processed\")\n+      }\n+      val sparkEvent = event.sparkEvent\n+      sparkEvent match {\n+        case start: SparkListenerApplicationStart =>\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logDebug(s\"Handling application start event: $event\")\n+          appName = start.appName\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {\n+            startTime = now()\n+          }\n+          setContextAppAndAttemptInfo(\n+            start.appId,\n+            start.appAttemptId)\n+          // flush old entity\n+          resetCurrentEntity\n+          appStartEventProcessed.set(true)\n+          appEndEventProcessed.set(false)\n+          // force create the new one\n+          val en = started(getOrCreateCurrentEntity, startTime)"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: too many empty lines\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-15T23:55:20Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {\n+      logDebug(s\"Processing event $event\")\n+      if (eventsProcessed.get() % 1000 == 0) {\n+        logDebug(s\"${eventsProcessed} events are processed\")\n+      }\n+      val sparkEvent = event.sparkEvent\n+      sparkEvent match {\n+        case start: SparkListenerApplicationStart =>\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logDebug(s\"Handling application start event: $event\")\n+          appName = start.appName\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {\n+            startTime = now()\n+          }\n+          setContextAppAndAttemptInfo(\n+            start.appId,\n+            start.appAttemptId)\n+          // flush old entity\n+          resetCurrentEntity\n+          appStartEventProcessed.set(true)\n+          appEndEventProcessed.set(false)\n+          // force create the new one\n+          val en = started(getOrCreateCurrentEntity, startTime)\n+          logInfo(s\"Application started: $event\")\n+          push = true\n+"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "OK\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T16:42:03Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {\n+      logDebug(s\"Processing event $event\")\n+      if (eventsProcessed.get() % 1000 == 0) {\n+        logDebug(s\"${eventsProcessed} events are processed\")\n+      }\n+      val sparkEvent = event.sparkEvent\n+      sparkEvent match {\n+        case start: SparkListenerApplicationStart =>\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logDebug(s\"Handling application start event: $event\")\n+          appName = start.appName\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {\n+            startTime = now()\n+          }\n+          setContextAppAndAttemptInfo(\n+            start.appId,\n+            start.appAttemptId)\n+          // flush old entity\n+          resetCurrentEntity\n+          appStartEventProcessed.set(true)\n+          appEndEventProcessed.set(false)\n+          // force create the new one\n+          val en = started(getOrCreateCurrentEntity, startTime)\n+          logInfo(s\"Application started: $event\")\n+          push = true\n+"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: missing empty line.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-15T23:55:29Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {\n+      logDebug(s\"Processing event $event\")\n+      if (eventsProcessed.get() % 1000 == 0) {\n+        logDebug(s\"${eventsProcessed} events are processed\")\n+      }\n+      val sparkEvent = event.sparkEvent\n+      sparkEvent match {\n+        case start: SparkListenerApplicationStart =>\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logDebug(s\"Handling application start event: $event\")\n+          appName = start.appName\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {\n+            startTime = now()\n+          }\n+          setContextAppAndAttemptInfo(\n+            start.appId,\n+            start.appAttemptId)\n+          // flush old entity\n+          resetCurrentEntity\n+          appStartEventProcessed.set(true)\n+          appEndEventProcessed.set(false)\n+          // force create the new one\n+          val en = started(getOrCreateCurrentEntity, startTime)\n+          logInfo(s\"Application started: $event\")\n+          push = true\n+\n+\n+        case end: SparkListenerApplicationEnd =>\n+          if (!appEndEventProcessed.get()) {\n+            // we already have all information,\n+            // flush it for old one to switch to new one\n+            logInfo(s\"Application end event: $event\")\n+            if (!appStartEventProcessed.get()) {\n+              logWarning(s\"Handling application end event without application start $event\")\n+            }\n+            // flush old entity\n+            val endtime = if (end.time > 0) end.time else now()\n+            if (startTime == 0) {\n+              // special case: an app end is received without any active start\n+              // event sent yet\n+              logInfo(s\"start time is 0; fixing up to match end time\")\n+              startTime = endtime\n+            }\n+            // reset the current entity\n+            resetCurrentEntity\n+            // then trigger its recreation\n+            val en = completed(getOrCreateCurrentEntity(),\n+              startTime,\n+              endtime,\n+              sparkApplicationId,\n+              sparkApplicationAttemptId)\n+            appStartEventProcessed.set(false)\n+            appEndEventProcessed.set(true)\n+            push = true\n+          }\n+        case _ =>"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "OK\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T16:42:08Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {\n+      logDebug(s\"Processing event $event\")\n+      if (eventsProcessed.get() % 1000 == 0) {\n+        logDebug(s\"${eventsProcessed} events are processed\")\n+      }\n+      val sparkEvent = event.sparkEvent\n+      sparkEvent match {\n+        case start: SparkListenerApplicationStart =>\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logDebug(s\"Handling application start event: $event\")\n+          appName = start.appName\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {\n+            startTime = now()\n+          }\n+          setContextAppAndAttemptInfo(\n+            start.appId,\n+            start.appAttemptId)\n+          // flush old entity\n+          resetCurrentEntity\n+          appStartEventProcessed.set(true)\n+          appEndEventProcessed.set(false)\n+          // force create the new one\n+          val en = started(getOrCreateCurrentEntity, startTime)\n+          logInfo(s\"Application started: $event\")\n+          push = true\n+\n+\n+        case end: SparkListenerApplicationEnd =>\n+          if (!appEndEventProcessed.get()) {\n+            // we already have all information,\n+            // flush it for old one to switch to new one\n+            logInfo(s\"Application end event: $event\")\n+            if (!appStartEventProcessed.get()) {\n+              logWarning(s\"Handling application end event without application start $event\")\n+            }\n+            // flush old entity\n+            val endtime = if (end.time > 0) end.time else now()\n+            if (startTime == 0) {\n+              // special case: an app end is received without any active start\n+              // event sent yet\n+              logInfo(s\"start time is 0; fixing up to match end time\")\n+              startTime = endtime\n+            }\n+            // reset the current entity\n+            resetCurrentEntity\n+            // then trigger its recreation\n+            val en = completed(getOrCreateCurrentEntity(),\n+              startTime,\n+              endtime,\n+              sparkApplicationId,\n+              sparkApplicationAttemptId)\n+            appStartEventProcessed.set(false)\n+            appEndEventProcessed.set(true)\n+            push = true\n+          }\n+        case _ =>"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "when can this happen?\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-15T23:58:29Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {\n+      logDebug(s\"Processing event $event\")\n+      if (eventsProcessed.get() % 1000 == 0) {\n+        logDebug(s\"${eventsProcessed} events are processed\")\n+      }\n+      val sparkEvent = event.sparkEvent\n+      sparkEvent match {\n+        case start: SparkListenerApplicationStart =>\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logDebug(s\"Handling application start event: $event\")\n+          appName = start.appName\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {\n+            startTime = now()\n+          }\n+          setContextAppAndAttemptInfo(\n+            start.appId,\n+            start.appAttemptId)\n+          // flush old entity\n+          resetCurrentEntity\n+          appStartEventProcessed.set(true)\n+          appEndEventProcessed.set(false)\n+          // force create the new one\n+          val en = started(getOrCreateCurrentEntity, startTime)\n+          logInfo(s\"Application started: $event\")\n+          push = true\n+\n+\n+        case end: SparkListenerApplicationEnd =>\n+          if (!appEndEventProcessed.get()) {\n+            // we already have all information,\n+            // flush it for old one to switch to new one\n+            logInfo(s\"Application end event: $event\")\n+            if (!appStartEventProcessed.get()) {\n+              logWarning(s\"Handling application end event without application start $event\")\n+            }\n+            // flush old entity\n+            val endtime = if (end.time > 0) end.time else now()\n+            if (startTime == 0) {\n+              // special case: an app end is received without any active start"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "I haven't seen it, but if it does happen, I want to know.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T16:43:17Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {\n+      logDebug(s\"Processing event $event\")\n+      if (eventsProcessed.get() % 1000 == 0) {\n+        logDebug(s\"${eventsProcessed} events are processed\")\n+      }\n+      val sparkEvent = event.sparkEvent\n+      sparkEvent match {\n+        case start: SparkListenerApplicationStart =>\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logDebug(s\"Handling application start event: $event\")\n+          appName = start.appName\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {\n+            startTime = now()\n+          }\n+          setContextAppAndAttemptInfo(\n+            start.appId,\n+            start.appAttemptId)\n+          // flush old entity\n+          resetCurrentEntity\n+          appStartEventProcessed.set(true)\n+          appEndEventProcessed.set(false)\n+          // force create the new one\n+          val en = started(getOrCreateCurrentEntity, startTime)\n+          logInfo(s\"Application started: $event\")\n+          push = true\n+\n+\n+        case end: SparkListenerApplicationEnd =>\n+          if (!appEndEventProcessed.get()) {\n+            // we already have all information,\n+            // flush it for old one to switch to new one\n+            logInfo(s\"Application end event: $event\")\n+            if (!appStartEventProcessed.get()) {\n+              logWarning(s\"Handling application end event without application start $event\")\n+            }\n+            // flush old entity\n+            val endtime = if (end.time > 0) end.time else now()\n+            if (startTime == 0) {\n+              // special case: an app end is received without any active start"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: `()`\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:00:04Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {\n+      logDebug(s\"Processing event $event\")\n+      if (eventsProcessed.get() % 1000 == 0) {\n+        logDebug(s\"${eventsProcessed} events are processed\")\n+      }\n+      val sparkEvent = event.sparkEvent\n+      sparkEvent match {\n+        case start: SparkListenerApplicationStart =>\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logDebug(s\"Handling application start event: $event\")\n+          appName = start.appName\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {\n+            startTime = now()\n+          }\n+          setContextAppAndAttemptInfo(\n+            start.appId,\n+            start.appAttemptId)\n+          // flush old entity\n+          resetCurrentEntity\n+          appStartEventProcessed.set(true)\n+          appEndEventProcessed.set(false)\n+          // force create the new one\n+          val en = started(getOrCreateCurrentEntity, startTime)\n+          logInfo(s\"Application started: $event\")\n+          push = true\n+\n+\n+        case end: SparkListenerApplicationEnd =>\n+          if (!appEndEventProcessed.get()) {\n+            // we already have all information,\n+            // flush it for old one to switch to new one\n+            logInfo(s\"Application end event: $event\")\n+            if (!appStartEventProcessed.get()) {\n+              logWarning(s\"Handling application end event without application start $event\")\n+            }\n+            // flush old entity\n+            val endtime = if (end.time > 0) end.time else now()\n+            if (startTime == 0) {\n+              // special case: an app end is received without any active start\n+              // event sent yet\n+              logInfo(s\"start time is 0; fixing up to match end time\")\n+              startTime = endtime\n+            }\n+            // reset the current entity\n+            resetCurrentEntity"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "OK\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T16:45:43Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {\n+      logDebug(s\"Processing event $event\")\n+      if (eventsProcessed.get() % 1000 == 0) {\n+        logDebug(s\"${eventsProcessed} events are processed\")\n+      }\n+      val sparkEvent = event.sparkEvent\n+      sparkEvent match {\n+        case start: SparkListenerApplicationStart =>\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logDebug(s\"Handling application start event: $event\")\n+          appName = start.appName\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {\n+            startTime = now()\n+          }\n+          setContextAppAndAttemptInfo(\n+            start.appId,\n+            start.appAttemptId)\n+          // flush old entity\n+          resetCurrentEntity\n+          appStartEventProcessed.set(true)\n+          appEndEventProcessed.set(false)\n+          // force create the new one\n+          val en = started(getOrCreateCurrentEntity, startTime)\n+          logInfo(s\"Application started: $event\")\n+          push = true\n+\n+\n+        case end: SparkListenerApplicationEnd =>\n+          if (!appEndEventProcessed.get()) {\n+            // we already have all information,\n+            // flush it for old one to switch to new one\n+            logInfo(s\"Application end event: $event\")\n+            if (!appStartEventProcessed.get()) {\n+              logWarning(s\"Handling application end event without application start $event\")\n+            }\n+            // flush old entity\n+            val endtime = if (end.time > 0) end.time else now()\n+            if (startTime == 0) {\n+              // special case: an app end is received without any active start\n+              // event sent yet\n+              logInfo(s\"start time is 0; fixing up to match end time\")\n+              startTime = endtime\n+            }\n+            // reset the current entity\n+            resetCurrentEntity"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "unnecessary comment.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:00:11Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {\n+      logDebug(s\"Processing event $event\")\n+      if (eventsProcessed.get() % 1000 == 0) {\n+        logDebug(s\"${eventsProcessed} events are processed\")\n+      }\n+      val sparkEvent = event.sparkEvent\n+      sparkEvent match {\n+        case start: SparkListenerApplicationStart =>\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logDebug(s\"Handling application start event: $event\")\n+          appName = start.appName\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {\n+            startTime = now()\n+          }\n+          setContextAppAndAttemptInfo(\n+            start.appId,\n+            start.appAttemptId)\n+          // flush old entity\n+          resetCurrentEntity\n+          appStartEventProcessed.set(true)\n+          appEndEventProcessed.set(false)\n+          // force create the new one\n+          val en = started(getOrCreateCurrentEntity, startTime)\n+          logInfo(s\"Application started: $event\")\n+          push = true\n+\n+\n+        case end: SparkListenerApplicationEnd =>\n+          if (!appEndEventProcessed.get()) {\n+            // we already have all information,\n+            // flush it for old one to switch to new one\n+            logInfo(s\"Application end event: $event\")\n+            if (!appStartEventProcessed.get()) {\n+              logWarning(s\"Handling application end event without application start $event\")\n+            }\n+            // flush old entity\n+            val endtime = if (end.time > 0) end.time else now()\n+            if (startTime == 0) {\n+              // special case: an app end is received without any active start\n+              // event sent yet\n+              logInfo(s\"start time is 0; fixing up to match end time\")\n+              startTime = endtime\n+            }\n+            // reset the current entity"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`en` is unused.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:01:32Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {\n+      logDebug(s\"Processing event $event\")\n+      if (eventsProcessed.get() % 1000 == 0) {\n+        logDebug(s\"${eventsProcessed} events are processed\")\n+      }\n+      val sparkEvent = event.sparkEvent\n+      sparkEvent match {\n+        case start: SparkListenerApplicationStart =>\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logDebug(s\"Handling application start event: $event\")\n+          appName = start.appName\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {\n+            startTime = now()\n+          }\n+          setContextAppAndAttemptInfo(\n+            start.appId,\n+            start.appAttemptId)\n+          // flush old entity\n+          resetCurrentEntity\n+          appStartEventProcessed.set(true)\n+          appEndEventProcessed.set(false)\n+          // force create the new one\n+          val en = started(getOrCreateCurrentEntity, startTime)\n+          logInfo(s\"Application started: $event\")\n+          push = true\n+\n+\n+        case end: SparkListenerApplicationEnd =>\n+          if (!appEndEventProcessed.get()) {\n+            // we already have all information,\n+            // flush it for old one to switch to new one\n+            logInfo(s\"Application end event: $event\")\n+            if (!appStartEventProcessed.get()) {\n+              logWarning(s\"Handling application end event without application start $event\")\n+            }\n+            // flush old entity\n+            val endtime = if (end.time > 0) end.time else now()\n+            if (startTime == 0) {\n+              // special case: an app end is received without any active start\n+              // event sent yet\n+              logInfo(s\"start time is 0; fixing up to match end time\")\n+              startTime = endtime\n+            }\n+            // reset the current entity\n+            resetCurrentEntity\n+            // then trigger its recreation\n+            val en = completed(getOrCreateCurrentEntity(),"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`currentEntity` is unused.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:01:56Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {\n+      logDebug(s\"Processing event $event\")\n+      if (eventsProcessed.get() % 1000 == 0) {\n+        logDebug(s\"${eventsProcessed} events are processed\")\n+      }\n+      val sparkEvent = event.sparkEvent\n+      sparkEvent match {\n+        case start: SparkListenerApplicationStart =>\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logDebug(s\"Handling application start event: $event\")\n+          appName = start.appName\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {\n+            startTime = now()\n+          }\n+          setContextAppAndAttemptInfo(\n+            start.appId,\n+            start.appAttemptId)\n+          // flush old entity\n+          resetCurrentEntity\n+          appStartEventProcessed.set(true)\n+          appEndEventProcessed.set(false)\n+          // force create the new one\n+          val en = started(getOrCreateCurrentEntity, startTime)\n+          logInfo(s\"Application started: $event\")\n+          push = true\n+\n+\n+        case end: SparkListenerApplicationEnd =>\n+          if (!appEndEventProcessed.get()) {\n+            // we already have all information,\n+            // flush it for old one to switch to new one\n+            logInfo(s\"Application end event: $event\")\n+            if (!appStartEventProcessed.get()) {\n+              logWarning(s\"Handling application end event without application start $event\")\n+            }\n+            // flush old entity\n+            val endtime = if (end.time > 0) end.time else now()\n+            if (startTime == 0) {\n+              // special case: an app end is received without any active start\n+              // event sent yet\n+              logInfo(s\"start time is 0; fixing up to match end time\")\n+              startTime = endtime\n+            }\n+            // reset the current entity\n+            resetCurrentEntity\n+            // then trigger its recreation\n+            val en = completed(getOrCreateCurrentEntity(),\n+              startTime,\n+              endtime,\n+              sparkApplicationId,\n+              sparkApplicationAttemptId)\n+            appStartEventProcessed.set(false)\n+            appEndEventProcessed.set(true)\n+            push = true\n+          }\n+        case _ =>\n+          val currentEntity: TimelineEntity = getOrCreateCurrentEntity()"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Also you're calling `getOrCreateCurrentEntity` in L732 again, so is this even necessary?\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:04:57Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {\n+      logDebug(s\"Processing event $event\")\n+      if (eventsProcessed.get() % 1000 == 0) {\n+        logDebug(s\"${eventsProcessed} events are processed\")\n+      }\n+      val sparkEvent = event.sparkEvent\n+      sparkEvent match {\n+        case start: SparkListenerApplicationStart =>\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logDebug(s\"Handling application start event: $event\")\n+          appName = start.appName\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {\n+            startTime = now()\n+          }\n+          setContextAppAndAttemptInfo(\n+            start.appId,\n+            start.appAttemptId)\n+          // flush old entity\n+          resetCurrentEntity\n+          appStartEventProcessed.set(true)\n+          appEndEventProcessed.set(false)\n+          // force create the new one\n+          val en = started(getOrCreateCurrentEntity, startTime)\n+          logInfo(s\"Application started: $event\")\n+          push = true\n+\n+\n+        case end: SparkListenerApplicationEnd =>\n+          if (!appEndEventProcessed.get()) {\n+            // we already have all information,\n+            // flush it for old one to switch to new one\n+            logInfo(s\"Application end event: $event\")\n+            if (!appStartEventProcessed.get()) {\n+              logWarning(s\"Handling application end event without application start $event\")\n+            }\n+            // flush old entity\n+            val endtime = if (end.time > 0) end.time else now()\n+            if (startTime == 0) {\n+              // special case: an app end is received without any active start\n+              // event sent yet\n+              logInfo(s\"start time is 0; fixing up to match end time\")\n+              startTime = endtime\n+            }\n+            // reset the current entity\n+            resetCurrentEntity\n+            // then trigger its recreation\n+            val en = completed(getOrCreateCurrentEntity(),\n+              startTime,\n+              endtime,\n+              sparkApplicationId,\n+              sparkApplicationAttemptId)\n+            appStartEventProcessed.set(false)\n+            appEndEventProcessed.set(true)\n+            push = true\n+          }\n+        case _ =>\n+          val currentEntity: TimelineEntity = getOrCreateCurrentEntity()"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "probably not. cutting\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T16:46:54Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {\n+      logDebug(s\"Processing event $event\")\n+      if (eventsProcessed.get() % 1000 == 0) {\n+        logDebug(s\"${eventsProcessed} events are processed\")\n+      }\n+      val sparkEvent = event.sparkEvent\n+      sparkEvent match {\n+        case start: SparkListenerApplicationStart =>\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logDebug(s\"Handling application start event: $event\")\n+          appName = start.appName\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {\n+            startTime = now()\n+          }\n+          setContextAppAndAttemptInfo(\n+            start.appId,\n+            start.appAttemptId)\n+          // flush old entity\n+          resetCurrentEntity\n+          appStartEventProcessed.set(true)\n+          appEndEventProcessed.set(false)\n+          // force create the new one\n+          val en = started(getOrCreateCurrentEntity, startTime)\n+          logInfo(s\"Application started: $event\")\n+          push = true\n+\n+\n+        case end: SparkListenerApplicationEnd =>\n+          if (!appEndEventProcessed.get()) {\n+            // we already have all information,\n+            // flush it for old one to switch to new one\n+            logInfo(s\"Application end event: $event\")\n+            if (!appStartEventProcessed.get()) {\n+              logWarning(s\"Handling application end event without application start $event\")\n+            }\n+            // flush old entity\n+            val endtime = if (end.time > 0) end.time else now()\n+            if (startTime == 0) {\n+              // special case: an app end is received without any active start\n+              // event sent yet\n+              logInfo(s\"start time is 0; fixing up to match end time\")\n+              startTime = endtime\n+            }\n+            // reset the current entity\n+            resetCurrentEntity\n+            // then trigger its recreation\n+            val en = completed(getOrCreateCurrentEntity(),\n+              startTime,\n+              endtime,\n+              sparkApplicationId,\n+              sparkApplicationAttemptId)\n+            appStartEventProcessed.set(false)\n+            appEndEventProcessed.set(true)\n+            push = true\n+          }\n+        case _ =>\n+          val currentEntity: TimelineEntity = getOrCreateCurrentEntity()"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "again, unnecessary comment. If the next line says `getOrCreateCurrentEntity`, I think that's already pretty clear.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:03:28Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {\n+      logDebug(s\"Processing event $event\")\n+      if (eventsProcessed.get() % 1000 == 0) {\n+        logDebug(s\"${eventsProcessed} events are processed\")\n+      }\n+      val sparkEvent = event.sparkEvent\n+      sparkEvent match {\n+        case start: SparkListenerApplicationStart =>\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logDebug(s\"Handling application start event: $event\")\n+          appName = start.appName\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {\n+            startTime = now()\n+          }\n+          setContextAppAndAttemptInfo(\n+            start.appId,\n+            start.appAttemptId)\n+          // flush old entity\n+          resetCurrentEntity\n+          appStartEventProcessed.set(true)\n+          appEndEventProcessed.set(false)\n+          // force create the new one\n+          val en = started(getOrCreateCurrentEntity, startTime)\n+          logInfo(s\"Application started: $event\")\n+          push = true\n+\n+\n+        case end: SparkListenerApplicationEnd =>\n+          if (!appEndEventProcessed.get()) {\n+            // we already have all information,\n+            // flush it for old one to switch to new one\n+            logInfo(s\"Application end event: $event\")\n+            if (!appStartEventProcessed.get()) {\n+              logWarning(s\"Handling application end event without application start $event\")\n+            }\n+            // flush old entity\n+            val endtime = if (end.time > 0) end.time else now()\n+            if (startTime == 0) {\n+              // special case: an app end is received without any active start\n+              // event sent yet\n+              logInfo(s\"start time is 0; fixing up to match end time\")\n+              startTime = endtime\n+            }\n+            // reset the current entity\n+            resetCurrentEntity\n+            // then trigger its recreation"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`a`?\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:07:47Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {\n+      logDebug(s\"Processing event $event\")\n+      if (eventsProcessed.get() % 1000 == 0) {\n+        logDebug(s\"${eventsProcessed} events are processed\")\n+      }\n+      val sparkEvent = event.sparkEvent\n+      sparkEvent match {\n+        case start: SparkListenerApplicationStart =>\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logDebug(s\"Handling application start event: $event\")\n+          appName = start.appName\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {\n+            startTime = now()\n+          }\n+          setContextAppAndAttemptInfo(\n+            start.appId,\n+            start.appAttemptId)\n+          // flush old entity\n+          resetCurrentEntity\n+          appStartEventProcessed.set(true)\n+          appEndEventProcessed.set(false)\n+          // force create the new one\n+          val en = started(getOrCreateCurrentEntity, startTime)\n+          logInfo(s\"Application started: $event\")\n+          push = true\n+\n+\n+        case end: SparkListenerApplicationEnd =>\n+          if (!appEndEventProcessed.get()) {\n+            // we already have all information,\n+            // flush it for old one to switch to new one\n+            logInfo(s\"Application end event: $event\")\n+            if (!appStartEventProcessed.get()) {\n+              logWarning(s\"Handling application end event without application start $event\")\n+            }\n+            // flush old entity\n+            val endtime = if (end.time > 0) end.time else now()\n+            if (startTime == 0) {\n+              // special case: an app end is received without any active start\n+              // event sent yet\n+              logInfo(s\"start time is 0; fixing up to match end time\")\n+              startTime = endtime\n+            }\n+            // reset the current entity\n+            resetCurrentEntity\n+            // then trigger its recreation\n+            val en = completed(getOrCreateCurrentEntity(),\n+              startTime,\n+              endtime,\n+              sparkApplicationId,\n+              sparkApplicationAttemptId)\n+            appStartEventProcessed.set(false)\n+            appEndEventProcessed.set(true)\n+            push = true\n+          }\n+        case _ =>\n+          val currentEntity: TimelineEntity = getOrCreateCurrentEntity()\n+      }\n+\n+      val tlEvent = toTimelineEvent(event)\n+      getOrCreateCurrentEntity().addEvent(tlEvent)\n+      logDebug(s\"Event: $event => ${describeEntity(getOrCreateCurrentEntity())}\")\n+\n+      currentEventCount += 1\n+      // set the push flag if the batch limit is reached\n+      push |= currentEventCount == batchSize\n+      // after the processing, increment the counter\n+      // this is done at the tail to count real number of processed events, not count received\n+      incEventsProcessed\n+    } // end of synchronized clause\n+\n+    logDebug(s\"current event num: $currentEventCount\")\n+    if (push) {\n+      logDebug(\"Push triggered\")\n+      flushCurrentEventsToATS\n+    }\n+  }\n+\n+  /**\n+   * Flush the current event set to ATS\n+   */\n+  private def flushCurrentEventsToATS: Unit = {\n+    entityLock synchronized {\n+      resetCurrentEntity\n+      flushEntity\n+    }\n+  }\n+\n+  /**\n+   * Return a time value"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "System time; expanded comment\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T16:48:15Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {\n+      logDebug(s\"Processing event $event\")\n+      if (eventsProcessed.get() % 1000 == 0) {\n+        logDebug(s\"${eventsProcessed} events are processed\")\n+      }\n+      val sparkEvent = event.sparkEvent\n+      sparkEvent match {\n+        case start: SparkListenerApplicationStart =>\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logDebug(s\"Handling application start event: $event\")\n+          appName = start.appName\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {\n+            startTime = now()\n+          }\n+          setContextAppAndAttemptInfo(\n+            start.appId,\n+            start.appAttemptId)\n+          // flush old entity\n+          resetCurrentEntity\n+          appStartEventProcessed.set(true)\n+          appEndEventProcessed.set(false)\n+          // force create the new one\n+          val en = started(getOrCreateCurrentEntity, startTime)\n+          logInfo(s\"Application started: $event\")\n+          push = true\n+\n+\n+        case end: SparkListenerApplicationEnd =>\n+          if (!appEndEventProcessed.get()) {\n+            // we already have all information,\n+            // flush it for old one to switch to new one\n+            logInfo(s\"Application end event: $event\")\n+            if (!appStartEventProcessed.get()) {\n+              logWarning(s\"Handling application end event without application start $event\")\n+            }\n+            // flush old entity\n+            val endtime = if (end.time > 0) end.time else now()\n+            if (startTime == 0) {\n+              // special case: an app end is received without any active start\n+              // event sent yet\n+              logInfo(s\"start time is 0; fixing up to match end time\")\n+              startTime = endtime\n+            }\n+            // reset the current entity\n+            resetCurrentEntity\n+            // then trigger its recreation\n+            val en = completed(getOrCreateCurrentEntity(),\n+              startTime,\n+              endtime,\n+              sparkApplicationId,\n+              sparkApplicationAttemptId)\n+            appStartEventProcessed.set(false)\n+            appEndEventProcessed.set(true)\n+            push = true\n+          }\n+        case _ =>\n+          val currentEntity: TimelineEntity = getOrCreateCurrentEntity()\n+      }\n+\n+      val tlEvent = toTimelineEvent(event)\n+      getOrCreateCurrentEntity().addEvent(tlEvent)\n+      logDebug(s\"Event: $event => ${describeEntity(getOrCreateCurrentEntity())}\")\n+\n+      currentEventCount += 1\n+      // set the push flag if the batch limit is reached\n+      push |= currentEventCount == batchSize\n+      // after the processing, increment the counter\n+      // this is done at the tail to count real number of processed events, not count received\n+      incEventsProcessed\n+    } // end of synchronized clause\n+\n+    logDebug(s\"current event num: $currentEventCount\")\n+    if (push) {\n+      logDebug(\"Push triggered\")\n+      flushCurrentEventsToATS\n+    }\n+  }\n+\n+  /**\n+   * Flush the current event set to ATS\n+   */\n+  private def flushCurrentEventsToATS: Unit = {\n+    entityLock synchronized {\n+      resetCurrentEntity\n+      flushEntity\n+    }\n+  }\n+\n+  /**\n+   * Return a time value"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "this is midly confusing because this method calls `flushCurrentEventsToATS` which in turn calls this method again... it's safe because of the batch handling, but it would feel cleaner to call `asyncFlush` instead.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:10:16Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {\n+      logDebug(s\"Processing event $event\")\n+      if (eventsProcessed.get() % 1000 == 0) {\n+        logDebug(s\"${eventsProcessed} events are processed\")\n+      }\n+      val sparkEvent = event.sparkEvent\n+      sparkEvent match {\n+        case start: SparkListenerApplicationStart =>\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logDebug(s\"Handling application start event: $event\")\n+          appName = start.appName\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {\n+            startTime = now()\n+          }\n+          setContextAppAndAttemptInfo(\n+            start.appId,\n+            start.appAttemptId)\n+          // flush old entity\n+          resetCurrentEntity\n+          appStartEventProcessed.set(true)\n+          appEndEventProcessed.set(false)\n+          // force create the new one\n+          val en = started(getOrCreateCurrentEntity, startTime)\n+          logInfo(s\"Application started: $event\")\n+          push = true\n+\n+\n+        case end: SparkListenerApplicationEnd =>\n+          if (!appEndEventProcessed.get()) {\n+            // we already have all information,\n+            // flush it for old one to switch to new one\n+            logInfo(s\"Application end event: $event\")\n+            if (!appStartEventProcessed.get()) {\n+              logWarning(s\"Handling application end event without application start $event\")\n+            }\n+            // flush old entity\n+            val endtime = if (end.time > 0) end.time else now()\n+            if (startTime == 0) {\n+              // special case: an app end is received without any active start\n+              // event sent yet\n+              logInfo(s\"start time is 0; fixing up to match end time\")\n+              startTime = endtime\n+            }\n+            // reset the current entity\n+            resetCurrentEntity\n+            // then trigger its recreation\n+            val en = completed(getOrCreateCurrentEntity(),\n+              startTime,\n+              endtime,\n+              sparkApplicationId,\n+              sparkApplicationAttemptId)\n+            appStartEventProcessed.set(false)\n+            appEndEventProcessed.set(true)\n+            push = true\n+          }\n+        case _ =>\n+          val currentEntity: TimelineEntity = getOrCreateCurrentEntity()\n+      }\n+\n+      val tlEvent = toTimelineEvent(event)\n+      getOrCreateCurrentEntity().addEvent(tlEvent)\n+      logDebug(s\"Event: $event => ${describeEntity(getOrCreateCurrentEntity())}\")\n+\n+      currentEventCount += 1\n+      // set the push flag if the batch limit is reached\n+      push |= currentEventCount == batchSize\n+      // after the processing, increment the counter\n+      // this is done at the tail to count real number of processed events, not count received\n+      incEventsProcessed\n+    } // end of synchronized clause\n+\n+    logDebug(s\"current event num: $currentEventCount\")\n+    if (push) {\n+      logDebug(\"Push triggered\")\n+      flushCurrentEventsToATS"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`override`, `(): Unit = {`\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:12:11Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {\n+      logDebug(s\"Processing event $event\")\n+      if (eventsProcessed.get() % 1000 == 0) {\n+        logDebug(s\"${eventsProcessed} events are processed\")\n+      }\n+      val sparkEvent = event.sparkEvent\n+      sparkEvent match {\n+        case start: SparkListenerApplicationStart =>\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logDebug(s\"Handling application start event: $event\")\n+          appName = start.appName\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {\n+            startTime = now()\n+          }\n+          setContextAppAndAttemptInfo(\n+            start.appId,\n+            start.appAttemptId)\n+          // flush old entity\n+          resetCurrentEntity\n+          appStartEventProcessed.set(true)\n+          appEndEventProcessed.set(false)\n+          // force create the new one\n+          val en = started(getOrCreateCurrentEntity, startTime)\n+          logInfo(s\"Application started: $event\")\n+          push = true\n+\n+\n+        case end: SparkListenerApplicationEnd =>\n+          if (!appEndEventProcessed.get()) {\n+            // we already have all information,\n+            // flush it for old one to switch to new one\n+            logInfo(s\"Application end event: $event\")\n+            if (!appStartEventProcessed.get()) {\n+              logWarning(s\"Handling application end event without application start $event\")\n+            }\n+            // flush old entity\n+            val endtime = if (end.time > 0) end.time else now()\n+            if (startTime == 0) {\n+              // special case: an app end is received without any active start\n+              // event sent yet\n+              logInfo(s\"start time is 0; fixing up to match end time\")\n+              startTime = endtime\n+            }\n+            // reset the current entity\n+            resetCurrentEntity\n+            // then trigger its recreation\n+            val en = completed(getOrCreateCurrentEntity(),\n+              startTime,\n+              endtime,\n+              sparkApplicationId,\n+              sparkApplicationAttemptId)\n+            appStartEventProcessed.set(false)\n+            appEndEventProcessed.set(true)\n+            push = true\n+          }\n+        case _ =>\n+          val currentEntity: TimelineEntity = getOrCreateCurrentEntity()\n+      }\n+\n+      val tlEvent = toTimelineEvent(event)\n+      getOrCreateCurrentEntity().addEvent(tlEvent)\n+      logDebug(s\"Event: $event => ${describeEntity(getOrCreateCurrentEntity())}\")\n+\n+      currentEventCount += 1\n+      // set the push flag if the batch limit is reached\n+      push |= currentEventCount == batchSize\n+      // after the processing, increment the counter\n+      // this is done at the tail to count real number of processed events, not count received\n+      incEventsProcessed\n+    } // end of synchronized clause\n+\n+    logDebug(s\"current event num: $currentEventCount\")\n+    if (push) {\n+      logDebug(\"Push triggered\")\n+      flushCurrentEventsToATS\n+    }\n+  }\n+\n+  /**\n+   * Flush the current event set to ATS\n+   */\n+  private def flushCurrentEventsToATS: Unit = {\n+    entityLock synchronized {\n+      resetCurrentEntity\n+      flushEntity\n+    }\n+  }\n+\n+  /**\n+   * Return a time value\n+   * @return a time\n+   */\n+  private def now(): Long = {\n+    clock.getTimeMillis()\n+  }\n+\n+  /*\n+   * increment the counter of events processed\n+   */\n+  private def incEventsProcessed: Unit = {\n+    val count = eventsProcessed.incrementAndGet\n+    logDebug(s\"Event processed count at $count\")\n+  }\n+\n+  /**\n+   * Reset the current entity by propagating its values to the entity list\n+   */\n+  protected def resetCurrentEntity(): Unit = {\n+    curEntity.foreach(entityList :+= _)\n+    curEntity = None\n+    currentEventCount = 0\n+  }\n+\n+  /**\n+   * Queue an asynchronous flush operation.\n+   * @return if the flush event was queued\n+   */\n+  def asyncFlush(): Boolean = {\n+    enqueue(FlushTimelineEvents())\n+  }\n+\n+  /**\n+   * Get the number of flush events that have taken place\n+   *\n+   * This includes flushes triggered by the event list being > the batch size,\n+   * but excludes flush operations triggered when the action processor thread\n+   * is stopped, or if the timeline service binding is disabled.\n+   *\n+   * @return count of processed flush events.\n+   */\n+  def getFlushCount(): Int = {\n+    flushCount.get\n+  }\n+\n+  /**\n+   * Get the URI of the timeline service webapp; null until service is started\n+   * @return a URI or null\n+   */\n+  def getTimelineServiceAddress(): URI = {\n+    timelineWebappAddress\n+  }\n+\n+  /**\n+   * Dequeue thread\n+   */\n+  private class Dequeue extends Runnable {\n+    def run {"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This is only used in L826, why have this var at all?\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:12:54Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {\n+      logDebug(s\"Processing event $event\")\n+      if (eventsProcessed.get() % 1000 == 0) {\n+        logDebug(s\"${eventsProcessed} events are processed\")\n+      }\n+      val sparkEvent = event.sparkEvent\n+      sparkEvent match {\n+        case start: SparkListenerApplicationStart =>\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logDebug(s\"Handling application start event: $event\")\n+          appName = start.appName\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {\n+            startTime = now()\n+          }\n+          setContextAppAndAttemptInfo(\n+            start.appId,\n+            start.appAttemptId)\n+          // flush old entity\n+          resetCurrentEntity\n+          appStartEventProcessed.set(true)\n+          appEndEventProcessed.set(false)\n+          // force create the new one\n+          val en = started(getOrCreateCurrentEntity, startTime)\n+          logInfo(s\"Application started: $event\")\n+          push = true\n+\n+\n+        case end: SparkListenerApplicationEnd =>\n+          if (!appEndEventProcessed.get()) {\n+            // we already have all information,\n+            // flush it for old one to switch to new one\n+            logInfo(s\"Application end event: $event\")\n+            if (!appStartEventProcessed.get()) {\n+              logWarning(s\"Handling application end event without application start $event\")\n+            }\n+            // flush old entity\n+            val endtime = if (end.time > 0) end.time else now()\n+            if (startTime == 0) {\n+              // special case: an app end is received without any active start\n+              // event sent yet\n+              logInfo(s\"start time is 0; fixing up to match end time\")\n+              startTime = endtime\n+            }\n+            // reset the current entity\n+            resetCurrentEntity\n+            // then trigger its recreation\n+            val en = completed(getOrCreateCurrentEntity(),\n+              startTime,\n+              endtime,\n+              sparkApplicationId,\n+              sparkApplicationAttemptId)\n+            appStartEventProcessed.set(false)\n+            appEndEventProcessed.set(true)\n+            push = true\n+          }\n+        case _ =>\n+          val currentEntity: TimelineEntity = getOrCreateCurrentEntity()\n+      }\n+\n+      val tlEvent = toTimelineEvent(event)\n+      getOrCreateCurrentEntity().addEvent(tlEvent)\n+      logDebug(s\"Event: $event => ${describeEntity(getOrCreateCurrentEntity())}\")\n+\n+      currentEventCount += 1\n+      // set the push flag if the batch limit is reached\n+      push |= currentEventCount == batchSize\n+      // after the processing, increment the counter\n+      // this is done at the tail to count real number of processed events, not count received\n+      incEventsProcessed\n+    } // end of synchronized clause\n+\n+    logDebug(s\"current event num: $currentEventCount\")\n+    if (push) {\n+      logDebug(\"Push triggered\")\n+      flushCurrentEventsToATS\n+    }\n+  }\n+\n+  /**\n+   * Flush the current event set to ATS\n+   */\n+  private def flushCurrentEventsToATS: Unit = {\n+    entityLock synchronized {\n+      resetCurrentEntity\n+      flushEntity\n+    }\n+  }\n+\n+  /**\n+   * Return a time value\n+   * @return a time\n+   */\n+  private def now(): Long = {\n+    clock.getTimeMillis()\n+  }\n+\n+  /*\n+   * increment the counter of events processed\n+   */\n+  private def incEventsProcessed: Unit = {\n+    val count = eventsProcessed.incrementAndGet\n+    logDebug(s\"Event processed count at $count\")\n+  }\n+\n+  /**\n+   * Reset the current entity by propagating its values to the entity list\n+   */\n+  protected def resetCurrentEntity(): Unit = {\n+    curEntity.foreach(entityList :+= _)\n+    curEntity = None\n+    currentEventCount = 0\n+  }\n+\n+  /**\n+   * Queue an asynchronous flush operation.\n+   * @return if the flush event was queued\n+   */\n+  def asyncFlush(): Boolean = {\n+    enqueue(FlushTimelineEvents())\n+  }\n+\n+  /**\n+   * Get the number of flush events that have taken place\n+   *\n+   * This includes flushes triggered by the event list being > the batch size,\n+   * but excludes flush operations triggered when the action processor thread\n+   * is stopped, or if the timeline service binding is disabled.\n+   *\n+   * @return count of processed flush events.\n+   */\n+  def getFlushCount(): Int = {\n+    flushCount.get\n+  }\n+\n+  /**\n+   * Get the URI of the timeline service webapp; null until service is started\n+   * @return a URI or null\n+   */\n+  def getTimelineServiceAddress(): URI = {\n+    timelineWebappAddress\n+  }\n+\n+  /**\n+   * Dequeue thread\n+   */\n+  private class Dequeue extends Runnable {\n+    def run {\n+      postThreadActive.set(true)\n+      try {\n+        var shouldStop = false"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "it was probably in the while loop; culling\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T16:51:25Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {\n+      logDebug(s\"Processing event $event\")\n+      if (eventsProcessed.get() % 1000 == 0) {\n+        logDebug(s\"${eventsProcessed} events are processed\")\n+      }\n+      val sparkEvent = event.sparkEvent\n+      sparkEvent match {\n+        case start: SparkListenerApplicationStart =>\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logDebug(s\"Handling application start event: $event\")\n+          appName = start.appName\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {\n+            startTime = now()\n+          }\n+          setContextAppAndAttemptInfo(\n+            start.appId,\n+            start.appAttemptId)\n+          // flush old entity\n+          resetCurrentEntity\n+          appStartEventProcessed.set(true)\n+          appEndEventProcessed.set(false)\n+          // force create the new one\n+          val en = started(getOrCreateCurrentEntity, startTime)\n+          logInfo(s\"Application started: $event\")\n+          push = true\n+\n+\n+        case end: SparkListenerApplicationEnd =>\n+          if (!appEndEventProcessed.get()) {\n+            // we already have all information,\n+            // flush it for old one to switch to new one\n+            logInfo(s\"Application end event: $event\")\n+            if (!appStartEventProcessed.get()) {\n+              logWarning(s\"Handling application end event without application start $event\")\n+            }\n+            // flush old entity\n+            val endtime = if (end.time > 0) end.time else now()\n+            if (startTime == 0) {\n+              // special case: an app end is received without any active start\n+              // event sent yet\n+              logInfo(s\"start time is 0; fixing up to match end time\")\n+              startTime = endtime\n+            }\n+            // reset the current entity\n+            resetCurrentEntity\n+            // then trigger its recreation\n+            val en = completed(getOrCreateCurrentEntity(),\n+              startTime,\n+              endtime,\n+              sparkApplicationId,\n+              sparkApplicationAttemptId)\n+            appStartEventProcessed.set(false)\n+            appEndEventProcessed.set(true)\n+            push = true\n+          }\n+        case _ =>\n+          val currentEntity: TimelineEntity = getOrCreateCurrentEntity()\n+      }\n+\n+      val tlEvent = toTimelineEvent(event)\n+      getOrCreateCurrentEntity().addEvent(tlEvent)\n+      logDebug(s\"Event: $event => ${describeEntity(getOrCreateCurrentEntity())}\")\n+\n+      currentEventCount += 1\n+      // set the push flag if the batch limit is reached\n+      push |= currentEventCount == batchSize\n+      // after the processing, increment the counter\n+      // this is done at the tail to count real number of processed events, not count received\n+      incEventsProcessed\n+    } // end of synchronized clause\n+\n+    logDebug(s\"current event num: $currentEventCount\")\n+    if (push) {\n+      logDebug(\"Push triggered\")\n+      flushCurrentEventsToATS\n+    }\n+  }\n+\n+  /**\n+   * Flush the current event set to ATS\n+   */\n+  private def flushCurrentEventsToATS: Unit = {\n+    entityLock synchronized {\n+      resetCurrentEntity\n+      flushEntity\n+    }\n+  }\n+\n+  /**\n+   * Return a time value\n+   * @return a time\n+   */\n+  private def now(): Long = {\n+    clock.getTimeMillis()\n+  }\n+\n+  /*\n+   * increment the counter of events processed\n+   */\n+  private def incEventsProcessed: Unit = {\n+    val count = eventsProcessed.incrementAndGet\n+    logDebug(s\"Event processed count at $count\")\n+  }\n+\n+  /**\n+   * Reset the current entity by propagating its values to the entity list\n+   */\n+  protected def resetCurrentEntity(): Unit = {\n+    curEntity.foreach(entityList :+= _)\n+    curEntity = None\n+    currentEventCount = 0\n+  }\n+\n+  /**\n+   * Queue an asynchronous flush operation.\n+   * @return if the flush event was queued\n+   */\n+  def asyncFlush(): Boolean = {\n+    enqueue(FlushTimelineEvents())\n+  }\n+\n+  /**\n+   * Get the number of flush events that have taken place\n+   *\n+   * This includes flushes triggered by the event list being > the batch size,\n+   * but excludes flush operations triggered when the action processor thread\n+   * is stopped, or if the timeline service binding is disabled.\n+   *\n+   * @return count of processed flush events.\n+   */\n+  def getFlushCount(): Int = {\n+    flushCount.get\n+  }\n+\n+  /**\n+   * Get the URI of the timeline service webapp; null until service is started\n+   * @return a URI or null\n+   */\n+  def getTimelineServiceAddress(): URI = {\n+    timelineWebappAddress\n+  }\n+\n+  /**\n+   * Dequeue thread\n+   */\n+  private class Dequeue extends Runnable {\n+    def run {\n+      postThreadActive.set(true)\n+      try {\n+        var shouldStop = false"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "minor, but this can miss a `notifyAll` if the thread exits for any reason that is not the `StopQueue` message. If you check the value of `postThreadActive` before waiting, you could avoid that.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:15:00Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "fixed; not seen yet, but over time, all conditions surface\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T16:53:44Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: `()`. I'll stop pointing these out.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:15:55Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {\n+      logDebug(s\"Processing event $event\")\n+      if (eventsProcessed.get() % 1000 == 0) {\n+        logDebug(s\"${eventsProcessed} events are processed\")\n+      }\n+      val sparkEvent = event.sparkEvent\n+      sparkEvent match {\n+        case start: SparkListenerApplicationStart =>\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logDebug(s\"Handling application start event: $event\")\n+          appName = start.appName\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {\n+            startTime = now()\n+          }\n+          setContextAppAndAttemptInfo(\n+            start.appId,\n+            start.appAttemptId)\n+          // flush old entity\n+          resetCurrentEntity\n+          appStartEventProcessed.set(true)\n+          appEndEventProcessed.set(false)\n+          // force create the new one\n+          val en = started(getOrCreateCurrentEntity, startTime)\n+          logInfo(s\"Application started: $event\")\n+          push = true\n+\n+\n+        case end: SparkListenerApplicationEnd =>\n+          if (!appEndEventProcessed.get()) {\n+            // we already have all information,\n+            // flush it for old one to switch to new one\n+            logInfo(s\"Application end event: $event\")\n+            if (!appStartEventProcessed.get()) {\n+              logWarning(s\"Handling application end event without application start $event\")\n+            }\n+            // flush old entity\n+            val endtime = if (end.time > 0) end.time else now()\n+            if (startTime == 0) {\n+              // special case: an app end is received without any active start\n+              // event sent yet\n+              logInfo(s\"start time is 0; fixing up to match end time\")\n+              startTime = endtime\n+            }\n+            // reset the current entity\n+            resetCurrentEntity\n+            // then trigger its recreation\n+            val en = completed(getOrCreateCurrentEntity(),\n+              startTime,\n+              endtime,\n+              sparkApplicationId,\n+              sparkApplicationAttemptId)\n+            appStartEventProcessed.set(false)\n+            appEndEventProcessed.set(true)\n+            push = true\n+          }\n+        case _ =>\n+          val currentEntity: TimelineEntity = getOrCreateCurrentEntity()\n+      }\n+\n+      val tlEvent = toTimelineEvent(event)\n+      getOrCreateCurrentEntity().addEvent(tlEvent)\n+      logDebug(s\"Event: $event => ${describeEntity(getOrCreateCurrentEntity())}\")\n+\n+      currentEventCount += 1\n+      // set the push flag if the batch limit is reached\n+      push |= currentEventCount == batchSize\n+      // after the processing, increment the counter\n+      // this is done at the tail to count real number of processed events, not count received\n+      incEventsProcessed\n+    } // end of synchronized clause\n+\n+    logDebug(s\"current event num: $currentEventCount\")\n+    if (push) {\n+      logDebug(\"Push triggered\")\n+      flushCurrentEventsToATS\n+    }\n+  }\n+\n+  /**\n+   * Flush the current event set to ATS\n+   */\n+  private def flushCurrentEventsToATS: Unit = {\n+    entityLock synchronized {\n+      resetCurrentEntity\n+      flushEntity\n+    }\n+  }\n+\n+  /**\n+   * Return a time value\n+   * @return a time\n+   */\n+  private def now(): Long = {\n+    clock.getTimeMillis()\n+  }\n+\n+  /*\n+   * increment the counter of events processed\n+   */\n+  private def incEventsProcessed: Unit = {\n+    val count = eventsProcessed.incrementAndGet\n+    logDebug(s\"Event processed count at $count\")\n+  }\n+\n+  /**\n+   * Reset the current entity by propagating its values to the entity list\n+   */\n+  protected def resetCurrentEntity(): Unit = {\n+    curEntity.foreach(entityList :+= _)\n+    curEntity = None\n+    currentEventCount = 0\n+  }\n+\n+  /**\n+   * Queue an asynchronous flush operation.\n+   * @return if the flush event was queued\n+   */\n+  def asyncFlush(): Boolean = {\n+    enqueue(FlushTimelineEvents())\n+  }\n+\n+  /**\n+   * Get the number of flush events that have taken place\n+   *\n+   * This includes flushes triggered by the event list being > the batch size,\n+   * but excludes flush operations triggered when the action processor thread\n+   * is stopped, or if the timeline service binding is disabled.\n+   *\n+   * @return count of processed flush events.\n+   */\n+  def getFlushCount(): Int = {\n+    flushCount.get\n+  }\n+\n+  /**\n+   * Get the URI of the timeline service webapp; null until service is started\n+   * @return a URI or null\n+   */\n+  def getTimelineServiceAddress(): URI = {\n+    timelineWebappAddress\n+  }\n+\n+  /**\n+   * Dequeue thread\n+   */\n+  private class Dequeue extends Runnable {\n+    def run {\n+      postThreadActive.set(true)\n+      try {\n+        var shouldStop = false\n+        while (!stopped.get) {\n+          try {\n+            val action = actionQueue.take\n+            shouldStop = processAction(action)\n+            if (shouldStop) {\n+              log.info(\"Event handler thread stopping the service\")\n+              stopped.set(true)\n+            }\n+          } catch {\n+            case e: InterruptedException => {\n+              logWarning(\"EventQueue interrupted\")\n+              stopped.set(true)\n+            }\n+            case e: Exception => {\n+              logWarning(s\"Exception in dequeue thread $e\", e)\n+              // if this happened while the service was stopped -stop\n+              // immediately\n+              stopped.set(isInState(Service.STATE.STOPPED))\n+            }\n+          }\n+        }\n+        logInfo(s\"Stopping dequeue service, final queue size is ${actionQueue.size}\")\n+        stopTimelineClient"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "cleaner: `s\"[$time]: $sparkEvent\"`\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:16:35Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those\n+      // which were successfully posted. The remainder are retained for\n+      // followon attempts\n+      entityList = entityList.filter { entity => {\n+          if (entity == null) {\n+            false\n+          } else {\n+            entity.addOtherInfo(FIELD_LAST_UPDATED, lastUpdated)\n+            if (domainId != null) {\n+              entity.setDomainId(domainId)\n+            }\n+            val entityDescription = describeEntity(entity)\n+            logDebug(s\"About to POST $entityDescription\")\n+            try { \n+              val response: TimelinePutResponse = client.putEntities(entity)\n+              val errors = response.getErrors\n+              if (!response.getErrors.isEmpty) {\n+                val err = errors.get(0)\n+                eventPostFailures.addAndGet(errors.size())\n+                if (err.getErrorCode != 0) {\n+                  logError(s\"Failed to post ${entityDescription}\\n:${describeError(err)}\")\n+                }\n+              } else {\n+                // successful submission\n+                logDebug(s\"entity successfully posted: $entityDescription\")\n+              }\n+              // whatever the outcome, this request is not re-issued\n+              false\n+            } catch {\n+              case e: ConnectException =>\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Connection exception submitting $entityDescription\\n$e\", e)\n+                true\n+\n+              case e: RuntimeException =>\n+                // this is probably a retry timeout event; some Hadoop versions don't\n+                // rethrow the exception causing the problem, instead raising an RTE\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Runtime exception submitting $entityDescription\\n$e\", e)\n+                // same policy as before: retry on these\n+                true\n+\n+              case e: Exception =>\n+                // something else has gone wrong.\n+                eventPostFailures.incrementAndGet\n+                logWarning(s\"Could not handle history entity: $entityDescription\\n$e\", e)\n+                false\n+            }\n+          }\n+        }\n+      }\n+      logDebug(s\"after pushEntities: ${entityList.size}\")\n+    }\n+  }\n+\n+  /**\n+   * Process the action placed in the queue\n+   * @param action action\n+   * @return true if the queue processor must now exit\n+   */\n+  private def processAction(action: QueuedAction): Boolean = {\n+    action match {\n+\n+      case StopQueue() =>\n+        logDebug(\"Stop queue action received\")\n+        true\n+\n+      case event: HandleSparkEvent =>\n+        try {\n+          handleEvent(event)\n+          false\n+        } catch {\n+          case NonFatal(e) => false\n+        }\n+\n+      case FlushTimelineEvents() =>\n+        logDebug(\"Flush queue action received\")\n+        flushCurrentEventsToATS\n+        false\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic\n+   *              still applies\n+   * @return true if the event should stop the service.\n+   */\n+  private def handleEvent(event: HandleSparkEvent): Unit = {\n+    var push = false\n+    // if we receive a new appStart event, we always push\n+    // not much contention here, only happens when service is stopped\n+    entityLock synchronized {\n+      logDebug(s\"Processing event $event\")\n+      if (eventsProcessed.get() % 1000 == 0) {\n+        logDebug(s\"${eventsProcessed} events are processed\")\n+      }\n+      val sparkEvent = event.sparkEvent\n+      sparkEvent match {\n+        case start: SparkListenerApplicationStart =>\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logDebug(s\"Handling application start event: $event\")\n+          appName = start.appName\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {\n+            startTime = now()\n+          }\n+          setContextAppAndAttemptInfo(\n+            start.appId,\n+            start.appAttemptId)\n+          // flush old entity\n+          resetCurrentEntity\n+          appStartEventProcessed.set(true)\n+          appEndEventProcessed.set(false)\n+          // force create the new one\n+          val en = started(getOrCreateCurrentEntity, startTime)\n+          logInfo(s\"Application started: $event\")\n+          push = true\n+\n+\n+        case end: SparkListenerApplicationEnd =>\n+          if (!appEndEventProcessed.get()) {\n+            // we already have all information,\n+            // flush it for old one to switch to new one\n+            logInfo(s\"Application end event: $event\")\n+            if (!appStartEventProcessed.get()) {\n+              logWarning(s\"Handling application end event without application start $event\")\n+            }\n+            // flush old entity\n+            val endtime = if (end.time > 0) end.time else now()\n+            if (startTime == 0) {\n+              // special case: an app end is received without any active start\n+              // event sent yet\n+              logInfo(s\"start time is 0; fixing up to match end time\")\n+              startTime = endtime\n+            }\n+            // reset the current entity\n+            resetCurrentEntity\n+            // then trigger its recreation\n+            val en = completed(getOrCreateCurrentEntity(),\n+              startTime,\n+              endtime,\n+              sparkApplicationId,\n+              sparkApplicationAttemptId)\n+            appStartEventProcessed.set(false)\n+            appEndEventProcessed.set(true)\n+            push = true\n+          }\n+        case _ =>\n+          val currentEntity: TimelineEntity = getOrCreateCurrentEntity()\n+      }\n+\n+      val tlEvent = toTimelineEvent(event)\n+      getOrCreateCurrentEntity().addEvent(tlEvent)\n+      logDebug(s\"Event: $event => ${describeEntity(getOrCreateCurrentEntity())}\")\n+\n+      currentEventCount += 1\n+      // set the push flag if the batch limit is reached\n+      push |= currentEventCount == batchSize\n+      // after the processing, increment the counter\n+      // this is done at the tail to count real number of processed events, not count received\n+      incEventsProcessed\n+    } // end of synchronized clause\n+\n+    logDebug(s\"current event num: $currentEventCount\")\n+    if (push) {\n+      logDebug(\"Push triggered\")\n+      flushCurrentEventsToATS\n+    }\n+  }\n+\n+  /**\n+   * Flush the current event set to ATS\n+   */\n+  private def flushCurrentEventsToATS: Unit = {\n+    entityLock synchronized {\n+      resetCurrentEntity\n+      flushEntity\n+    }\n+  }\n+\n+  /**\n+   * Return a time value\n+   * @return a time\n+   */\n+  private def now(): Long = {\n+    clock.getTimeMillis()\n+  }\n+\n+  /*\n+   * increment the counter of events processed\n+   */\n+  private def incEventsProcessed: Unit = {\n+    val count = eventsProcessed.incrementAndGet\n+    logDebug(s\"Event processed count at $count\")\n+  }\n+\n+  /**\n+   * Reset the current entity by propagating its values to the entity list\n+   */\n+  protected def resetCurrentEntity(): Unit = {\n+    curEntity.foreach(entityList :+= _)\n+    curEntity = None\n+    currentEventCount = 0\n+  }\n+\n+  /**\n+   * Queue an asynchronous flush operation.\n+   * @return if the flush event was queued\n+   */\n+  def asyncFlush(): Boolean = {\n+    enqueue(FlushTimelineEvents())\n+  }\n+\n+  /**\n+   * Get the number of flush events that have taken place\n+   *\n+   * This includes flushes triggered by the event list being > the batch size,\n+   * but excludes flush operations triggered when the action processor thread\n+   * is stopped, or if the timeline service binding is disabled.\n+   *\n+   * @return count of processed flush events.\n+   */\n+  def getFlushCount(): Int = {\n+    flushCount.get\n+  }\n+\n+  /**\n+   * Get the URI of the timeline service webapp; null until service is started\n+   * @return a URI or null\n+   */\n+  def getTimelineServiceAddress(): URI = {\n+    timelineWebappAddress\n+  }\n+\n+  /**\n+   * Dequeue thread\n+   */\n+  private class Dequeue extends Runnable {\n+    def run {\n+      postThreadActive.set(true)\n+      try {\n+        var shouldStop = false\n+        while (!stopped.get) {\n+          try {\n+            val action = actionQueue.take\n+            shouldStop = processAction(action)\n+            if (shouldStop) {\n+              log.info(\"Event handler thread stopping the service\")\n+              stopped.set(true)\n+            }\n+          } catch {\n+            case e: InterruptedException => {\n+              logWarning(\"EventQueue interrupted\")\n+              stopped.set(true)\n+            }\n+            case e: Exception => {\n+              logWarning(s\"Exception in dequeue thread $e\", e)\n+              // if this happened while the service was stopped -stop\n+              // immediately\n+              stopped.set(isInState(Service.STATE.STOPPED))\n+            }\n+          }\n+        }\n+        logInfo(s\"Stopping dequeue service, final queue size is ${actionQueue.size}\")\n+        stopTimelineClient\n+      } finally {\n+        postThreadActive synchronized {\n+          // declare that this thread is no longer active\n+          postThreadActive.set(false)\n+          // and notify all listeners of this fact\n+          postThreadActive.notifyAll\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * Set of actions which can be queued for the `eventHandlingThread`\n+ */\n+private[yarn] sealed abstract class QueuedAction\n+\n+/**\n+ * Flush the timeline event list\n+ */\n+private[yarn] case class FlushTimelineEvents() extends QueuedAction {\n+  override def toString: String = \"Flush Events\"\n+}\n+\n+/**\n+ * Stop the queue entirely. This does not force a flush: that must\n+ * be separate\n+ */\n+private[yarn] case class StopQueue() extends QueuedAction {\n+  override def toString: String = \"Stop Queue\"\n+}\n+\n+/**\n+ * A spark event has been received: handle it\n+ * @param sparkEvent spark event\n+ * @param time time of event being received\n+ */\n+private[yarn] case class HandleSparkEvent(sparkEvent: SparkListenerEvent, time: Long)\n+    extends QueuedAction {\n+  override def toString: String = {\n+    s\"[${time }]: ${sparkEvent}\""
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "I think it's ok, but just to confirm: is it ok to post entities out of order? (e.g., you fail to post the first entity in the list, but succeeds to post the second, so you'll retry the first one later).\n\nAlso, what happens if the upload keeps failing and failing? Won't this start creating memory pressure in the driver, since you'll be queuing more and more entities to be posted?\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:20:47Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "That's a good question; that's some code I inherited without looking at that case too closely. And right now we haven't seen/created the failures to really stress the case.\n\nWhen you post an entity with a list of events, you are creating entity if needed, updating it with the new events if not. If two posts apply in a different order, the two event lists will also be out of order. Which presumably means that they are going to come in in a different order, aren't they?\n\nEven though they come with a timestamp, so could in theory be re-ordered in the history server, that's not something you'd want to do: it would require all events to be held in memory raw and then sorted prior to replay: it doesn't scale, and isn't compatible with windowed event GET operations, which is also critical for scalability.\n\nWhat should we do here? We can change that posting loop to stop on a failure, and retry later (which actually highlights a related issue: when to retry. If there are outstanding events to post, it should be attempted with some frequency (after handling every event).\n\nI'll attempt that and see how well it works.\n\nAnd yes, backpressure. I don't what the strategy should be there. Stop accepting new events exception application halt? Discard early ones? Be selective about which events to filter?\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T17:11:35Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "In my original, unfinished code ([1]), I used the local fs to buffer these events, because I wasn't really comfortable with the ATS becoming a bottleneck and causing memory pressure on the driver. That allowed the code to upload events always in order while avoiding caching everything in memory.\n\nIt might be a little paranoid, but it does avoid both issues. And if the ATS is really borked, well, the app's event log would eventually be lost (or left incomplete).\n\n[1] https://github.com/vanzin/spark/blob/yarn-timeline/yarn/timeline/src/main/scala/org/apache/spark/deploy/yarn/timeline/YarnTimelineClientImpl.scala\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T19:59:15Z",
    "diffHunk": "@@ -0,0 +1,981 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+\n+import scala.collection.mutable.LinkedList\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.service.{AbstractService, Service}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelinePutResponse}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.YarnExtensionService\n+import org.apache.spark.util.{SystemClock, Clock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Implements a Hadoop service with the init/start logic replaced by that\n+ * of the YarnService.\n+ * <p>\n+ * As [[AbstractService]] implements `close()`, routing\n+ * to its `stop` method, calling `close()` is sufficient\n+ * to stop the service instance.\n+ * <p>\n+ * However, when registered to receive spark events, the service will continue to\n+ * receive them until the spark context is stopped. Events received when this service\n+ * is in a `STOPPED` state will be discarded.\n+ */\n+private[spark] class YarnHistoryService  extends AbstractService(\"History Service\")\n+  with YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Applicaton name */\n+  private var appName: String = _\n+\n+  /** Application ID from the spark start event */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName: String = Utils.getCurrentUserName\n+\n+  /**\n+   * Clock for recording time\n+   */\n+  private val clock: Clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var batchSize: Int = DEFAULT_BATCH_SIZE\n+\n+  /** queue of actions*/\n+  private val actionQueue = new LinkedBlockingQueue[QueuedAction]\n+\n+  /** cache layer to handle timeline client failure.*/\n+  private var entityList = new LinkedList[TimelineEntity]\n+\n+  /** current entity; wil be created on demand. */\n+  private var curEntity: Option[TimelineEntity] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** How many events have been received in the current entity */\n+  private var currentEventCount = 0\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val eventsProcessed: AtomicInteger = new AtomicInteger(0)\n+\n+  /** counter of events queued. */\n+  private val eventsQueued: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many event postings failed? */\n+  private val eventPostFailures: AtomicInteger = new AtomicInteger(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicInteger(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped: AtomicBoolean = new AtomicBoolean(true)\n+\n+  /**\n+   * boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive: AtomicBoolean = new AtomicBoolean(false)\n+\n+  /**\n+   * object used for a lock on entity operations\n+   */\n+  private val entityLock: AnyRef = new AnyRef\n+\n+  /**\n+   * How long to wait for shutdown before giving up\n+   */\n+  private var maxTimeToWaitOnShutdown: Long = SHUTDOWN_WAIT_TIME\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: String = null\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private [yarn] def createTimelineClient(): TimelineClient = {\n+    require(timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def getTimelineClient: TimelineClient = {\n+    timelineClient.getOrElse(throw new Exception(\"Timeline client not running\"))\n+  }\n+\n+  /**\n+   * Get the total number of processed events\n+   * @return counter of events processed\n+   */\n+  def getEventsProcessed: Int = {\n+    eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def getEventsQueued: Int = {\n+    eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    actionQueue.size\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def getBatchSize: Int = {\n+    batchSize\n+  }\n+\n+  /**\n+   * Get the total number of failed posts events\n+   * @return counter of timeline post operations which failed\n+   */\n+  def getEventPostFailures: Int = {\n+    eventPostFailures.get\n+  }\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = { _applicationId }\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = { _attemptId }\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    stopOptionalService(timelineClient)\n+    timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain\n+   * @return a domain string or null\n+   */\n+  private def createTimelineDomain(): String = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsFlag = sparkConf.getOption(\"spark.acls.enable\")\n+    val aclsOn = aclsFlag.getOrElse(\n+      sparkConf.get(\"spark.ui.acls.enable\", \"false\")).toBoolean\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return null\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      return predefDomain.get\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (adminAcls ++ modifyAcls ++ viewAcls).foldLeft(current)(_ + \" \" + _)\n+    val writers = (adminAcls ++ modifyAcls).foldLeft(current)(_ + \" \" + _)\n+    var domainId = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domainId with\" +\n+      s\" readers: readers  and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domainId)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      getTimelineClient.putDomain(timelineDomain)\n+    } catch {\n+      case e: Exception => {\n+        logError(\"cannot create the domain\")\n+        // fallback to default\n+        domainId = null\n+      }\n+    }\n+    domainId\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param context spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @return true if the service is hooked up to the timeline service; that is: it is live\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean = {\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    \n+    val yarnConf = new YarnConfiguration(context.hadoopConfiguration)\n+    // the init() operation checks the state machine & prevents invocation out of sequence\n+    init(yarnConf)\n+    this.sparkContext = context\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    batchSize = sparkContext.conf.getInt(BATCH_SIZE, batchSize)\n+\n+    start()\n+    if (timelineServiceEnabled) {\n+      true\n+    } else {\n+      logInfo(\"Yarn timeline service not available, disabling client.\")\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Service start.\n+   *\n+   * If the timeline client is enabled,\n+   * Create the timeline client, the timeline domain, and the event handling thread.\n+   *\n+   * Irrespective of the timeline enabled flag, the service will attempt to register\n+   * as a listener for events. They will merely be discarded.\n+   */\n+  override protected def serviceStart {\n+    require(sparkContext != null, \"No spark context set\")\n+    val conf: Configuration = getConfig\n+    if (timelineServiceEnabled) {\n+      timelineWebappAddress = getTimelineEndpoint(conf)\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at $timelineWebappAddress\")\n+      timelineClient = Some(createTimelineClient)\n+      domainId = createTimelineDomain\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    // irrespective of state, hook up to the listener\n+    val registered = registerListener\n+    if (registered) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+    super.serviceStart()\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED`\n+   *         is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(getConfig)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString: String = {\n+        s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+        s\" endpoint=$timelineWebappAddress;\" +\n+        s\" bonded to ATS=$bondedToATS;\" +\n+        s\" listening=$listening;\" +\n+        s\" batchSize=$batchSize;\" +\n+        s\" flush count=$getFlushCount;\" +\n+        s\" current queue size=$getQueueSize;\" +\n+        s\" total number queued=$getEventsQueued, processed=$getEventsProcessed;\" +\n+        s\" post failures=$getEventPostFailures;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    this._applicationId = appId\n+    this._attemptId = attemptId\n+  }\n+  \n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private [yarn] def registerListener: Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param action action to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(action: QueuedAction): Boolean = {\n+    if (!stopped.get) {\n+      innerEnqueue(action)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $action\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Inner operation to queue the event. This does not check for service state\n+   * @param event event to queue\n+   */\n+  private def innerEnqueue(event: QueuedAction) = {\n+    eventsQueued.incrementAndGet\n+    logDebug(s\"Enqueue $event\")\n+    actionQueue.add(event)\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override protected def serviceStop {\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (!appEndEventProcessed.get()) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        val current = now()\n+        innerEnqueue(new HandleSparkEvent(SparkListenerApplicationEnd(current), current))\n+      }\n+\n+      // flush\n+      logInfo(s\"Shutting down: pushing out ${actionQueue.size} events\")\n+      innerEnqueue(FlushTimelineEvents())\n+\n+      // stop operation\n+      postThreadActive synchronized {\n+        innerEnqueue(StopQueue())\n+        // now await that marker flag\n+        postThreadActive.wait(maxTimeToWaitOnShutdown)\n+      }\n+\n+      if (!actionQueue.isEmpty) {\n+        // likely cause is ATS not responding.\n+        // interrupt the thread, albeit at the risk of app stop events\n+        // being lost. There's not much else that can be done if\n+        // ATS is being unresponsive\n+        logWarning(s\"Did not finish flushing actionQueue before \" +\n+          s\"stopping ATSService, eventQueueBacklog= ${actionQueue.size}\")\n+        eventHandlingThread.foreach(_.interrupt())\n+      }\n+      stopTimelineClient\n+      logInfo(s\"Stopped: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Get the current entity, creating one on demand if needed, setting up its entity Id\n+   * from the current appID and attempt ID values, filters from the current name and user\n+   * @return the current entity\n+   */\n+  private def getOrCreateCurrentEntity() = {\n+    curEntity.getOrElse {\n+      currentEventCount = 0\n+      val entity = createTimelineEntity(\n+        _applicationId,\n+        _attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        appStartEventProcessed.get(),\n+        appName,\n+        userName)\n+      curEntity = Some(entity)\n+      logDebug(s\"Demand creation of new entity ${describeEntity(entity)}\")\n+      entity\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   * @param entity timeline entity to review.\n+   */\n+  private[yarn] def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * If there is any available entity to be sent, push to timeline server\n+   */\n+  private def flushEntity(): Unit = {\n+    if (entityList.nonEmpty) {\n+      val count = flushCount.incrementAndGet()\n+      logDebug(s\"flushEntity #$count: list size ${entityList.size}\")\n+      var client = getTimelineClient\n+      val lastUpdated = now()\n+      // attempt to post each entity, removing from the list all of those"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: ordering\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-28T20:26:22Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "fixed. I've been very reluctant to trust the intelliJ import optimiser as I think it gets things wrong.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-10-20T17:22:00Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: add empty line\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-28T20:27:04Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{TimeUnit, LinkedBlockingDeque}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{YarnExtensionService, YarnExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered\n+ * YARN Timeline Server.\n+ */\n+private[spark] class YarnHistoryService extends YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+  def enterState(state: Int): Int = {"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: please be consistent re: single line vs. multi-line docs, and starting with capital or lower case letters.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-28T20:28:13Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{TimeUnit, LinkedBlockingDeque}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{YarnExtensionService, YarnExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered\n+ * YARN Timeline Server.\n+ */\n+private[spark] class YarnHistoryService extends YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+  def enterState(state: Int): Int = {\n+    logDebug(s\"Entering state $state from $serviceState\")\n+    _serviceState.getAndSet(state)\n+  }\n+\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** YARN configuration from the spark context */\n+  private var config: YarnConfiguration = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var _timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Application name  from the spark start event */\n+  private var applicationName: String = _\n+\n+  /** Application ID*/\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName = Utils.getCurrentUserName()\n+\n+  /** Clock for recording time */\n+  private val clock = new SystemClock()\n+\n+  /**"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "will do; will go to upper case word & review all javadocs to verify\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-10-20T17:24:45Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{TimeUnit, LinkedBlockingDeque}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{YarnExtensionService, YarnExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered\n+ * YARN Timeline Server.\n+ */\n+private[spark] class YarnHistoryService extends YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+  def enterState(state: Int): Int = {\n+    logDebug(s\"Entering state $state from $serviceState\")\n+    _serviceState.getAndSet(state)\n+  }\n+\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** YARN configuration from the spark context */\n+  private var config: YarnConfiguration = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var _timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Application name  from the spark start event */\n+  private var applicationName: String = _\n+\n+  /** Application ID*/\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName = Utils.getCurrentUserName()\n+\n+  /** Clock for recording time */\n+  private val clock = new SystemClock()\n+\n+  /**"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "unit?\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-28T20:29:03Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{TimeUnit, LinkedBlockingDeque}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{YarnExtensionService, YarnExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered\n+ * YARN Timeline Server.\n+ */\n+private[spark] class YarnHistoryService extends YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+  def enterState(state: Int): Int = {\n+    logDebug(s\"Entering state $state from $serviceState\")\n+    _serviceState.getAndSet(state)\n+  }\n+\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** YARN configuration from the spark context */\n+  private var config: YarnConfiguration = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var _timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Application name  from the spark start event */\n+  private var applicationName: String = _\n+\n+  /** Application ID*/\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName = Utils.getCurrentUserName()\n+\n+  /** Clock for recording time */\n+  private val clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /**\n+   * Start time of the application, as received in the end event.\n+   */\n+  private var endTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var _batchSize = DEFAULT_BATCH_SIZE\n+\n+  /** queue of entities to asynchronously post, plus the number of events in each entry */\n+  private var _entityQueue = new LinkedBlockingDeque[TimelineEntity]()\n+\n+  /** limit on the total number of events permitted */\n+  private var _postQueueLimit = DEFAULT_POST_QUEUE_LIMIT\n+\n+  /**\n+   * List of events which will be pulled into a timeline\n+   * entity when created\n+   */\n+  private var pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+\n+  private var applicationStartEvent: Option[SparkListenerApplicationStart] = None\n+  private var applicationEndEvent: Option[SparkListenerApplicationEnd] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val _eventsProcessed = new AtomicLong(0)\n+\n+  /** counter of events queued. */\n+  private val _eventsQueued = new AtomicLong(0)\n+\n+  private val _entityPostAttempts = new AtomicLong(0)\n+  private val _entityPostSuccesses = new AtomicLong(0)\n+  /** how many entity postings failed? */\n+  private val _entityPostFailures = new AtomicLong(0)\n+  private val _eventsDropped = new AtomicLong(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicLong(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped = new AtomicBoolean(true)\n+\n+  /**\n+   * Boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive = new AtomicBoolean(false)\n+\n+  /** How long to wait for shutdown before giving up */\n+  private var shutdownWaitTime = 0L"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "millis: now noted in javadocs\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-10-20T18:04:34Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{TimeUnit, LinkedBlockingDeque}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{YarnExtensionService, YarnExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered\n+ * YARN Timeline Server.\n+ */\n+private[spark] class YarnHistoryService extends YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+  def enterState(state: Int): Int = {\n+    logDebug(s\"Entering state $state from $serviceState\")\n+    _serviceState.getAndSet(state)\n+  }\n+\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** YARN configuration from the spark context */\n+  private var config: YarnConfiguration = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var _timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Application name  from the spark start event */\n+  private var applicationName: String = _\n+\n+  /** Application ID*/\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName = Utils.getCurrentUserName()\n+\n+  /** Clock for recording time */\n+  private val clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /**\n+   * Start time of the application, as received in the end event.\n+   */\n+  private var endTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var _batchSize = DEFAULT_BATCH_SIZE\n+\n+  /** queue of entities to asynchronously post, plus the number of events in each entry */\n+  private var _entityQueue = new LinkedBlockingDeque[TimelineEntity]()\n+\n+  /** limit on the total number of events permitted */\n+  private var _postQueueLimit = DEFAULT_POST_QUEUE_LIMIT\n+\n+  /**\n+   * List of events which will be pulled into a timeline\n+   * entity when created\n+   */\n+  private var pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+\n+  private var applicationStartEvent: Option[SparkListenerApplicationStart] = None\n+  private var applicationEndEvent: Option[SparkListenerApplicationEnd] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val _eventsProcessed = new AtomicLong(0)\n+\n+  /** counter of events queued. */\n+  private val _eventsQueued = new AtomicLong(0)\n+\n+  private val _entityPostAttempts = new AtomicLong(0)\n+  private val _entityPostSuccesses = new AtomicLong(0)\n+  /** how many entity postings failed? */\n+  private val _entityPostFailures = new AtomicLong(0)\n+  private val _eventsDropped = new AtomicLong(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicLong(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped = new AtomicBoolean(true)\n+\n+  /**\n+   * Boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive = new AtomicBoolean(false)\n+\n+  /** How long to wait for shutdown before giving up */\n+  private var shutdownWaitTime = 0L"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`.get` will already throw an exception if it's not set.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-28T20:30:26Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{TimeUnit, LinkedBlockingDeque}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{YarnExtensionService, YarnExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered\n+ * YARN Timeline Server.\n+ */\n+private[spark] class YarnHistoryService extends YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+  def enterState(state: Int): Int = {\n+    logDebug(s\"Entering state $state from $serviceState\")\n+    _serviceState.getAndSet(state)\n+  }\n+\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** YARN configuration from the spark context */\n+  private var config: YarnConfiguration = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var _timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Application name  from the spark start event */\n+  private var applicationName: String = _\n+\n+  /** Application ID*/\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName = Utils.getCurrentUserName()\n+\n+  /** Clock for recording time */\n+  private val clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /**\n+   * Start time of the application, as received in the end event.\n+   */\n+  private var endTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var _batchSize = DEFAULT_BATCH_SIZE\n+\n+  /** queue of entities to asynchronously post, plus the number of events in each entry */\n+  private var _entityQueue = new LinkedBlockingDeque[TimelineEntity]()\n+\n+  /** limit on the total number of events permitted */\n+  private var _postQueueLimit = DEFAULT_POST_QUEUE_LIMIT\n+\n+  /**\n+   * List of events which will be pulled into a timeline\n+   * entity when created\n+   */\n+  private var pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+\n+  private var applicationStartEvent: Option[SparkListenerApplicationStart] = None\n+  private var applicationEndEvent: Option[SparkListenerApplicationEnd] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val _eventsProcessed = new AtomicLong(0)\n+\n+  /** counter of events queued. */\n+  private val _eventsQueued = new AtomicLong(0)\n+\n+  private val _entityPostAttempts = new AtomicLong(0)\n+  private val _entityPostSuccesses = new AtomicLong(0)\n+  /** how many entity postings failed? */\n+  private val _entityPostFailures = new AtomicLong(0)\n+  private val _eventsDropped = new AtomicLong(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicLong(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped = new AtomicBoolean(true)\n+\n+  /**\n+   * Boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive = new AtomicBoolean(false)\n+\n+  /** How long to wait for shutdown before giving up */\n+  private var shutdownWaitTime = 0L\n+\n+  /**\n+   * What is the initial and incrementing interval for POST retries?\n+   */\n+  private var retryInterval = 0L\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: Option[String] = None\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var _timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private[yarn] def createTimelineClient(): TimelineClient = {\n+    require(_timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def timelineClient: TimelineClient = {\n+    require(_timelineClient.isDefined)"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "-cut\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-10-20T18:42:58Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{TimeUnit, LinkedBlockingDeque}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{YarnExtensionService, YarnExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered\n+ * YARN Timeline Server.\n+ */\n+private[spark] class YarnHistoryService extends YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+  def enterState(state: Int): Int = {\n+    logDebug(s\"Entering state $state from $serviceState\")\n+    _serviceState.getAndSet(state)\n+  }\n+\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** YARN configuration from the spark context */\n+  private var config: YarnConfiguration = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var _timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Application name  from the spark start event */\n+  private var applicationName: String = _\n+\n+  /** Application ID*/\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName = Utils.getCurrentUserName()\n+\n+  /** Clock for recording time */\n+  private val clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /**\n+   * Start time of the application, as received in the end event.\n+   */\n+  private var endTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var _batchSize = DEFAULT_BATCH_SIZE\n+\n+  /** queue of entities to asynchronously post, plus the number of events in each entry */\n+  private var _entityQueue = new LinkedBlockingDeque[TimelineEntity]()\n+\n+  /** limit on the total number of events permitted */\n+  private var _postQueueLimit = DEFAULT_POST_QUEUE_LIMIT\n+\n+  /**\n+   * List of events which will be pulled into a timeline\n+   * entity when created\n+   */\n+  private var pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+\n+  private var applicationStartEvent: Option[SparkListenerApplicationStart] = None\n+  private var applicationEndEvent: Option[SparkListenerApplicationEnd] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val _eventsProcessed = new AtomicLong(0)\n+\n+  /** counter of events queued. */\n+  private val _eventsQueued = new AtomicLong(0)\n+\n+  private val _entityPostAttempts = new AtomicLong(0)\n+  private val _entityPostSuccesses = new AtomicLong(0)\n+  /** how many entity postings failed? */\n+  private val _entityPostFailures = new AtomicLong(0)\n+  private val _eventsDropped = new AtomicLong(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicLong(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped = new AtomicBoolean(true)\n+\n+  /**\n+   * Boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive = new AtomicBoolean(false)\n+\n+  /** How long to wait for shutdown before giving up */\n+  private var shutdownWaitTime = 0L\n+\n+  /**\n+   * What is the initial and incrementing interval for POST retries?\n+   */\n+  private var retryInterval = 0L\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: Option[String] = None\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var _timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private[yarn] def createTimelineClient(): TimelineClient = {\n+    require(_timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def timelineClient: TimelineClient = {\n+    require(_timelineClient.isDefined)"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Doesn't this mean you'll write the events without any of the access permission restrictions the user requested?\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-28T20:32:21Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{TimeUnit, LinkedBlockingDeque}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{YarnExtensionService, YarnExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered\n+ * YARN Timeline Server.\n+ */\n+private[spark] class YarnHistoryService extends YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+  def enterState(state: Int): Int = {\n+    logDebug(s\"Entering state $state from $serviceState\")\n+    _serviceState.getAndSet(state)\n+  }\n+\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** YARN configuration from the spark context */\n+  private var config: YarnConfiguration = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var _timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Application name  from the spark start event */\n+  private var applicationName: String = _\n+\n+  /** Application ID*/\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName = Utils.getCurrentUserName()\n+\n+  /** Clock for recording time */\n+  private val clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /**\n+   * Start time of the application, as received in the end event.\n+   */\n+  private var endTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var _batchSize = DEFAULT_BATCH_SIZE\n+\n+  /** queue of entities to asynchronously post, plus the number of events in each entry */\n+  private var _entityQueue = new LinkedBlockingDeque[TimelineEntity]()\n+\n+  /** limit on the total number of events permitted */\n+  private var _postQueueLimit = DEFAULT_POST_QUEUE_LIMIT\n+\n+  /**\n+   * List of events which will be pulled into a timeline\n+   * entity when created\n+   */\n+  private var pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+\n+  private var applicationStartEvent: Option[SparkListenerApplicationStart] = None\n+  private var applicationEndEvent: Option[SparkListenerApplicationEnd] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val _eventsProcessed = new AtomicLong(0)\n+\n+  /** counter of events queued. */\n+  private val _eventsQueued = new AtomicLong(0)\n+\n+  private val _entityPostAttempts = new AtomicLong(0)\n+  private val _entityPostSuccesses = new AtomicLong(0)\n+  /** how many entity postings failed? */\n+  private val _entityPostFailures = new AtomicLong(0)\n+  private val _eventsDropped = new AtomicLong(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicLong(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped = new AtomicBoolean(true)\n+\n+  /**\n+   * Boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive = new AtomicBoolean(false)\n+\n+  /** How long to wait for shutdown before giving up */\n+  private var shutdownWaitTime = 0L\n+\n+  /**\n+   * What is the initial and incrementing interval for POST retries?\n+   */\n+  private var retryInterval = 0L\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: Option[String] = None\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var _timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private[yarn] def createTimelineClient(): TimelineClient = {\n+    require(_timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def timelineClient: TimelineClient = {\n+    require(_timelineClient.isDefined)\n+    _timelineClient.get\n+  }\n+\n+  /**\n+   * Get the total number of events dropped due to the queue of\n+   * outstanding posts being too long\n+   * @return counter of events processed\n+   */\n+\n+  def eventsDropped: Long = {\n+    _eventsDropped.get()\n+  }\n+\n+  /**\n+   * Get the total number of processed events, those handled in the back-end thread without\n+   * being rejected\n+   * @return counter of events processed\n+   */\n+  def eventsProcessed: Long = {\n+    _eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def eventsQueued: Long = {\n+    _eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    _entityQueue.size()\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def batchSize: Int = {\n+    _batchSize\n+  }\n+\n+  /**\n+   * Query the counter of attempts to post events\n+   * @return\n+   */\n+  def postAttempts: Long = _entityPostAttempts.get()\n+\n+  /**\n+   * Get the total number of failed post operations\n+   * @return counter of timeline post operations which failed\n+   */\n+  def postFailures: Long = {\n+    _entityPostFailures.get\n+  }\n+\n+  /**\n+   * Query the counter of post successes\n+   * @return the number of successful posts\n+   */\n+  def postSuccesses: Long = _entityPostSuccesses.get()\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId =  _applicationId\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] =  _attemptId\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    _timelineClient.foreach(_.stop())\n+    _timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain.\n+   *\n+   * A Timeline Domain is a uniquely identified 'namespace' for accessing parts of the timeline.\n+   * Security levels are are managed at the domain level, so one is created if the\n+   * spark acls are enabled. Full access is then granted to the current user,\n+   * all users in the configuration options `\"spark.modify.acls\"` and `\"spark.admin.acls\"`;\n+   * read access to those users and those listed in `\"spark.ui.view.acls\"`\n+   *\n+   * @return an optional domain string. If `None`, then no domain was created.\n+   */\n+  private def createTimelineDomain(): Option[String] = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsOn = sparkConf.getBoolean(\"spark.ui.acls.enable\",\n+        sparkConf.getBoolean(\"spark.acls.enable\", false))\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return None\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      logDebug(s\"Using predefined domain $predefDomain\")\n+      return predefDomain\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (Seq(current) ++ adminAcls ++ modifyAcls ++ viewAcls).mkString(\" \")\n+    val writers = (Seq(current) ++ adminAcls ++ modifyAcls).mkString(\" \")\n+    var domain = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domain with readers: $readers and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domain)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      timelineClient.putDomain(timelineDomain)\n+      Some(domain)\n+    } catch {\n+      case e: Exception => {\n+        logError(s\"cannot create the domain $domain\", e)\n+        // fallback to default\n+        None"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "security in ATS is best described as \"weak\". For the timeline support, the history server has to have access to all the histories saved to ATS, and, as it doesn't have a SPNEGO front end, it's not keeping its information secret. My own stance (this code predates me working on the patch) is that trying to lock down the ATS data here isn't worth trying. \n\nIf it is, then maybe the entity name used to save & retrieve data should be something customizable -different user groups could then publish securely, with separate history server instances needed to access the different dataset\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-10-20T18:49:52Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{TimeUnit, LinkedBlockingDeque}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{YarnExtensionService, YarnExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered\n+ * YARN Timeline Server.\n+ */\n+private[spark] class YarnHistoryService extends YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+  def enterState(state: Int): Int = {\n+    logDebug(s\"Entering state $state from $serviceState\")\n+    _serviceState.getAndSet(state)\n+  }\n+\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** YARN configuration from the spark context */\n+  private var config: YarnConfiguration = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var _timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Application name  from the spark start event */\n+  private var applicationName: String = _\n+\n+  /** Application ID*/\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName = Utils.getCurrentUserName()\n+\n+  /** Clock for recording time */\n+  private val clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /**\n+   * Start time of the application, as received in the end event.\n+   */\n+  private var endTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var _batchSize = DEFAULT_BATCH_SIZE\n+\n+  /** queue of entities to asynchronously post, plus the number of events in each entry */\n+  private var _entityQueue = new LinkedBlockingDeque[TimelineEntity]()\n+\n+  /** limit on the total number of events permitted */\n+  private var _postQueueLimit = DEFAULT_POST_QUEUE_LIMIT\n+\n+  /**\n+   * List of events which will be pulled into a timeline\n+   * entity when created\n+   */\n+  private var pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+\n+  private var applicationStartEvent: Option[SparkListenerApplicationStart] = None\n+  private var applicationEndEvent: Option[SparkListenerApplicationEnd] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val _eventsProcessed = new AtomicLong(0)\n+\n+  /** counter of events queued. */\n+  private val _eventsQueued = new AtomicLong(0)\n+\n+  private val _entityPostAttempts = new AtomicLong(0)\n+  private val _entityPostSuccesses = new AtomicLong(0)\n+  /** how many entity postings failed? */\n+  private val _entityPostFailures = new AtomicLong(0)\n+  private val _eventsDropped = new AtomicLong(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicLong(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped = new AtomicBoolean(true)\n+\n+  /**\n+   * Boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive = new AtomicBoolean(false)\n+\n+  /** How long to wait for shutdown before giving up */\n+  private var shutdownWaitTime = 0L\n+\n+  /**\n+   * What is the initial and incrementing interval for POST retries?\n+   */\n+  private var retryInterval = 0L\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: Option[String] = None\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var _timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private[yarn] def createTimelineClient(): TimelineClient = {\n+    require(_timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def timelineClient: TimelineClient = {\n+    require(_timelineClient.isDefined)\n+    _timelineClient.get\n+  }\n+\n+  /**\n+   * Get the total number of events dropped due to the queue of\n+   * outstanding posts being too long\n+   * @return counter of events processed\n+   */\n+\n+  def eventsDropped: Long = {\n+    _eventsDropped.get()\n+  }\n+\n+  /**\n+   * Get the total number of processed events, those handled in the back-end thread without\n+   * being rejected\n+   * @return counter of events processed\n+   */\n+  def eventsProcessed: Long = {\n+    _eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def eventsQueued: Long = {\n+    _eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    _entityQueue.size()\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def batchSize: Int = {\n+    _batchSize\n+  }\n+\n+  /**\n+   * Query the counter of attempts to post events\n+   * @return\n+   */\n+  def postAttempts: Long = _entityPostAttempts.get()\n+\n+  /**\n+   * Get the total number of failed post operations\n+   * @return counter of timeline post operations which failed\n+   */\n+  def postFailures: Long = {\n+    _entityPostFailures.get\n+  }\n+\n+  /**\n+   * Query the counter of post successes\n+   * @return the number of successful posts\n+   */\n+  def postSuccesses: Long = _entityPostSuccesses.get()\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId =  _applicationId\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] =  _attemptId\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    _timelineClient.foreach(_.stop())\n+    _timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain.\n+   *\n+   * A Timeline Domain is a uniquely identified 'namespace' for accessing parts of the timeline.\n+   * Security levels are are managed at the domain level, so one is created if the\n+   * spark acls are enabled. Full access is then granted to the current user,\n+   * all users in the configuration options `\"spark.modify.acls\"` and `\"spark.admin.acls\"`;\n+   * read access to those users and those listed in `\"spark.ui.view.acls\"`\n+   *\n+   * @return an optional domain string. If `None`, then no domain was created.\n+   */\n+  private def createTimelineDomain(): Option[String] = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsOn = sparkConf.getBoolean(\"spark.ui.acls.enable\",\n+        sparkConf.getBoolean(\"spark.acls.enable\", false))\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return None\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      logDebug(s\"Using predefined domain $predefDomain\")\n+      return predefDomain\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (Seq(current) ++ adminAcls ++ modifyAcls ++ viewAcls).mkString(\" \")\n+    val writers = (Seq(current) ++ adminAcls ++ modifyAcls).mkString(\" \")\n+    var domain = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domain with readers: $readers and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domain)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      timelineClient.putDomain(timelineDomain)\n+      Some(domain)\n+    } catch {\n+      case e: Exception => {\n+        logError(s\"cannot create the domain $domain\", e)\n+        // fallback to default\n+        None"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "I'm confused; where is the AM involved here?\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-28T21:08:49Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{TimeUnit, LinkedBlockingDeque}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{YarnExtensionService, YarnExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered\n+ * YARN Timeline Server.\n+ */\n+private[spark] class YarnHistoryService extends YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+  def enterState(state: Int): Int = {\n+    logDebug(s\"Entering state $state from $serviceState\")\n+    _serviceState.getAndSet(state)\n+  }\n+\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** YARN configuration from the spark context */\n+  private var config: YarnConfiguration = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var _timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Application name  from the spark start event */\n+  private var applicationName: String = _\n+\n+  /** Application ID*/\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName = Utils.getCurrentUserName()\n+\n+  /** Clock for recording time */\n+  private val clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /**\n+   * Start time of the application, as received in the end event.\n+   */\n+  private var endTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var _batchSize = DEFAULT_BATCH_SIZE\n+\n+  /** queue of entities to asynchronously post, plus the number of events in each entry */\n+  private var _entityQueue = new LinkedBlockingDeque[TimelineEntity]()\n+\n+  /** limit on the total number of events permitted */\n+  private var _postQueueLimit = DEFAULT_POST_QUEUE_LIMIT\n+\n+  /**\n+   * List of events which will be pulled into a timeline\n+   * entity when created\n+   */\n+  private var pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+\n+  private var applicationStartEvent: Option[SparkListenerApplicationStart] = None\n+  private var applicationEndEvent: Option[SparkListenerApplicationEnd] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val _eventsProcessed = new AtomicLong(0)\n+\n+  /** counter of events queued. */\n+  private val _eventsQueued = new AtomicLong(0)\n+\n+  private val _entityPostAttempts = new AtomicLong(0)\n+  private val _entityPostSuccesses = new AtomicLong(0)\n+  /** how many entity postings failed? */\n+  private val _entityPostFailures = new AtomicLong(0)\n+  private val _eventsDropped = new AtomicLong(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicLong(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped = new AtomicBoolean(true)\n+\n+  /**\n+   * Boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive = new AtomicBoolean(false)\n+\n+  /** How long to wait for shutdown before giving up */\n+  private var shutdownWaitTime = 0L\n+\n+  /**\n+   * What is the initial and incrementing interval for POST retries?\n+   */\n+  private var retryInterval = 0L\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: Option[String] = None\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var _timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private[yarn] def createTimelineClient(): TimelineClient = {\n+    require(_timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def timelineClient: TimelineClient = {\n+    require(_timelineClient.isDefined)\n+    _timelineClient.get\n+  }\n+\n+  /**\n+   * Get the total number of events dropped due to the queue of\n+   * outstanding posts being too long\n+   * @return counter of events processed\n+   */\n+\n+  def eventsDropped: Long = {\n+    _eventsDropped.get()\n+  }\n+\n+  /**\n+   * Get the total number of processed events, those handled in the back-end thread without\n+   * being rejected\n+   * @return counter of events processed\n+   */\n+  def eventsProcessed: Long = {\n+    _eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def eventsQueued: Long = {\n+    _eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    _entityQueue.size()\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def batchSize: Int = {\n+    _batchSize\n+  }\n+\n+  /**\n+   * Query the counter of attempts to post events\n+   * @return\n+   */\n+  def postAttempts: Long = _entityPostAttempts.get()\n+\n+  /**\n+   * Get the total number of failed post operations\n+   * @return counter of timeline post operations which failed\n+   */\n+  def postFailures: Long = {\n+    _entityPostFailures.get\n+  }\n+\n+  /**\n+   * Query the counter of post successes\n+   * @return the number of successful posts\n+   */\n+  def postSuccesses: Long = _entityPostSuccesses.get()\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId =  _applicationId\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] =  _attemptId\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    _timelineClient.foreach(_.stop())\n+    _timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain.\n+   *\n+   * A Timeline Domain is a uniquely identified 'namespace' for accessing parts of the timeline.\n+   * Security levels are are managed at the domain level, so one is created if the\n+   * spark acls are enabled. Full access is then granted to the current user,\n+   * all users in the configuration options `\"spark.modify.acls\"` and `\"spark.admin.acls\"`;\n+   * read access to those users and those listed in `\"spark.ui.view.acls\"`\n+   *\n+   * @return an optional domain string. If `None`, then no domain was created.\n+   */\n+  private def createTimelineDomain(): Option[String] = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsOn = sparkConf.getBoolean(\"spark.ui.acls.enable\",\n+        sparkConf.getBoolean(\"spark.acls.enable\", false))\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return None\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      logDebug(s\"Using predefined domain $predefDomain\")\n+      return predefDomain\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (Seq(current) ++ adminAcls ++ modifyAcls ++ viewAcls).mkString(\" \")\n+    val writers = (Seq(current) ++ adminAcls ++ modifyAcls).mkString(\" \")\n+    var domain = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domain with readers: $readers and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domain)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      timelineClient.putDomain(timelineDomain)\n+      Some(domain)\n+    } catch {\n+      case e: Exception => {\n+        logError(s\"cannot create the domain $domain\", e)\n+        // fallback to default\n+        None\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param binding binding to the spark application and YARN\n+   */\n+  override def start(binding: YarnExtensionServiceBinding): Unit = {\n+    val oldstate = enterState(StartedState)\n+    if (oldstate != CreatedState) {\n+      // state model violation\n+      _serviceState.set(oldstate)\n+      throw new IllegalArgumentException(s\"Cannot start the service from state $oldstate\")\n+    }\n+    val context = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    this.sparkContext = context\n+\n+    this.config = new YarnConfiguration(context.hadoopConfiguration)\n+\n+    val sparkConf = sparkContext.conf\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "the AM (YarnClusterScheduler) builds an attempt ID by scanning the string value of the yarn attempt ID and getting the last \"_\" separated number, parsing it an using it as the value. This is unique for the app, but not across apps, and can't be used. Trying to switch it to a full YARN ID doesn't caused problems, I forget what, as well as just complicating life. \n\nI'll cut the comment so as to avoid scaring people.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-10-20T18:57:25Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{TimeUnit, LinkedBlockingDeque}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{YarnExtensionService, YarnExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered\n+ * YARN Timeline Server.\n+ */\n+private[spark] class YarnHistoryService extends YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+  def enterState(state: Int): Int = {\n+    logDebug(s\"Entering state $state from $serviceState\")\n+    _serviceState.getAndSet(state)\n+  }\n+\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** YARN configuration from the spark context */\n+  private var config: YarnConfiguration = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var _timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Application name  from the spark start event */\n+  private var applicationName: String = _\n+\n+  /** Application ID*/\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName = Utils.getCurrentUserName()\n+\n+  /** Clock for recording time */\n+  private val clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /**\n+   * Start time of the application, as received in the end event.\n+   */\n+  private var endTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var _batchSize = DEFAULT_BATCH_SIZE\n+\n+  /** queue of entities to asynchronously post, plus the number of events in each entry */\n+  private var _entityQueue = new LinkedBlockingDeque[TimelineEntity]()\n+\n+  /** limit on the total number of events permitted */\n+  private var _postQueueLimit = DEFAULT_POST_QUEUE_LIMIT\n+\n+  /**\n+   * List of events which will be pulled into a timeline\n+   * entity when created\n+   */\n+  private var pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+\n+  private var applicationStartEvent: Option[SparkListenerApplicationStart] = None\n+  private var applicationEndEvent: Option[SparkListenerApplicationEnd] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val _eventsProcessed = new AtomicLong(0)\n+\n+  /** counter of events queued. */\n+  private val _eventsQueued = new AtomicLong(0)\n+\n+  private val _entityPostAttempts = new AtomicLong(0)\n+  private val _entityPostSuccesses = new AtomicLong(0)\n+  /** how many entity postings failed? */\n+  private val _entityPostFailures = new AtomicLong(0)\n+  private val _eventsDropped = new AtomicLong(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicLong(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped = new AtomicBoolean(true)\n+\n+  /**\n+   * Boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive = new AtomicBoolean(false)\n+\n+  /** How long to wait for shutdown before giving up */\n+  private var shutdownWaitTime = 0L\n+\n+  /**\n+   * What is the initial and incrementing interval for POST retries?\n+   */\n+  private var retryInterval = 0L\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: Option[String] = None\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var _timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private[yarn] def createTimelineClient(): TimelineClient = {\n+    require(_timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def timelineClient: TimelineClient = {\n+    require(_timelineClient.isDefined)\n+    _timelineClient.get\n+  }\n+\n+  /**\n+   * Get the total number of events dropped due to the queue of\n+   * outstanding posts being too long\n+   * @return counter of events processed\n+   */\n+\n+  def eventsDropped: Long = {\n+    _eventsDropped.get()\n+  }\n+\n+  /**\n+   * Get the total number of processed events, those handled in the back-end thread without\n+   * being rejected\n+   * @return counter of events processed\n+   */\n+  def eventsProcessed: Long = {\n+    _eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def eventsQueued: Long = {\n+    _eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    _entityQueue.size()\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def batchSize: Int = {\n+    _batchSize\n+  }\n+\n+  /**\n+   * Query the counter of attempts to post events\n+   * @return\n+   */\n+  def postAttempts: Long = _entityPostAttempts.get()\n+\n+  /**\n+   * Get the total number of failed post operations\n+   * @return counter of timeline post operations which failed\n+   */\n+  def postFailures: Long = {\n+    _entityPostFailures.get\n+  }\n+\n+  /**\n+   * Query the counter of post successes\n+   * @return the number of successful posts\n+   */\n+  def postSuccesses: Long = _entityPostSuccesses.get()\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId =  _applicationId\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] =  _attemptId\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    _timelineClient.foreach(_.stop())\n+    _timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain.\n+   *\n+   * A Timeline Domain is a uniquely identified 'namespace' for accessing parts of the timeline.\n+   * Security levels are are managed at the domain level, so one is created if the\n+   * spark acls are enabled. Full access is then granted to the current user,\n+   * all users in the configuration options `\"spark.modify.acls\"` and `\"spark.admin.acls\"`;\n+   * read access to those users and those listed in `\"spark.ui.view.acls\"`\n+   *\n+   * @return an optional domain string. If `None`, then no domain was created.\n+   */\n+  private def createTimelineDomain(): Option[String] = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsOn = sparkConf.getBoolean(\"spark.ui.acls.enable\",\n+        sparkConf.getBoolean(\"spark.acls.enable\", false))\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return None\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      logDebug(s\"Using predefined domain $predefDomain\")\n+      return predefDomain\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (Seq(current) ++ adminAcls ++ modifyAcls ++ viewAcls).mkString(\" \")\n+    val writers = (Seq(current) ++ adminAcls ++ modifyAcls).mkString(\" \")\n+    var domain = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domain with readers: $readers and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domain)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      timelineClient.putDomain(timelineDomain)\n+      Some(domain)\n+    } catch {\n+      case e: Exception => {\n+        logError(s\"cannot create the domain $domain\", e)\n+        // fallback to default\n+        None\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param binding binding to the spark application and YARN\n+   */\n+  override def start(binding: YarnExtensionServiceBinding): Unit = {\n+    val oldstate = enterState(StartedState)\n+    if (oldstate != CreatedState) {\n+      // state model violation\n+      _serviceState.set(oldstate)\n+      throw new IllegalArgumentException(s\"Cannot start the service from state $oldstate\")\n+    }\n+    val context = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    this.sparkContext = context\n+\n+    this.config = new YarnConfiguration(context.hadoopConfiguration)\n+\n+    val sparkConf = sparkContext.conf\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`getTimeAsMs`?\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-28T21:10:01Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{TimeUnit, LinkedBlockingDeque}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{YarnExtensionService, YarnExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered\n+ * YARN Timeline Server.\n+ */\n+private[spark] class YarnHistoryService extends YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+  def enterState(state: Int): Int = {\n+    logDebug(s\"Entering state $state from $serviceState\")\n+    _serviceState.getAndSet(state)\n+  }\n+\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** YARN configuration from the spark context */\n+  private var config: YarnConfiguration = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var _timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Application name  from the spark start event */\n+  private var applicationName: String = _\n+\n+  /** Application ID*/\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName = Utils.getCurrentUserName()\n+\n+  /** Clock for recording time */\n+  private val clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /**\n+   * Start time of the application, as received in the end event.\n+   */\n+  private var endTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var _batchSize = DEFAULT_BATCH_SIZE\n+\n+  /** queue of entities to asynchronously post, plus the number of events in each entry */\n+  private var _entityQueue = new LinkedBlockingDeque[TimelineEntity]()\n+\n+  /** limit on the total number of events permitted */\n+  private var _postQueueLimit = DEFAULT_POST_QUEUE_LIMIT\n+\n+  /**\n+   * List of events which will be pulled into a timeline\n+   * entity when created\n+   */\n+  private var pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+\n+  private var applicationStartEvent: Option[SparkListenerApplicationStart] = None\n+  private var applicationEndEvent: Option[SparkListenerApplicationEnd] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val _eventsProcessed = new AtomicLong(0)\n+\n+  /** counter of events queued. */\n+  private val _eventsQueued = new AtomicLong(0)\n+\n+  private val _entityPostAttempts = new AtomicLong(0)\n+  private val _entityPostSuccesses = new AtomicLong(0)\n+  /** how many entity postings failed? */\n+  private val _entityPostFailures = new AtomicLong(0)\n+  private val _eventsDropped = new AtomicLong(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicLong(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped = new AtomicBoolean(true)\n+\n+  /**\n+   * Boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive = new AtomicBoolean(false)\n+\n+  /** How long to wait for shutdown before giving up */\n+  private var shutdownWaitTime = 0L\n+\n+  /**\n+   * What is the initial and incrementing interval for POST retries?\n+   */\n+  private var retryInterval = 0L\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: Option[String] = None\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var _timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private[yarn] def createTimelineClient(): TimelineClient = {\n+    require(_timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def timelineClient: TimelineClient = {\n+    require(_timelineClient.isDefined)\n+    _timelineClient.get\n+  }\n+\n+  /**\n+   * Get the total number of events dropped due to the queue of\n+   * outstanding posts being too long\n+   * @return counter of events processed\n+   */\n+\n+  def eventsDropped: Long = {\n+    _eventsDropped.get()\n+  }\n+\n+  /**\n+   * Get the total number of processed events, those handled in the back-end thread without\n+   * being rejected\n+   * @return counter of events processed\n+   */\n+  def eventsProcessed: Long = {\n+    _eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def eventsQueued: Long = {\n+    _eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    _entityQueue.size()\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def batchSize: Int = {\n+    _batchSize\n+  }\n+\n+  /**\n+   * Query the counter of attempts to post events\n+   * @return\n+   */\n+  def postAttempts: Long = _entityPostAttempts.get()\n+\n+  /**\n+   * Get the total number of failed post operations\n+   * @return counter of timeline post operations which failed\n+   */\n+  def postFailures: Long = {\n+    _entityPostFailures.get\n+  }\n+\n+  /**\n+   * Query the counter of post successes\n+   * @return the number of successful posts\n+   */\n+  def postSuccesses: Long = _entityPostSuccesses.get()\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId =  _applicationId\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] =  _attemptId\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    _timelineClient.foreach(_.stop())\n+    _timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain.\n+   *\n+   * A Timeline Domain is a uniquely identified 'namespace' for accessing parts of the timeline.\n+   * Security levels are are managed at the domain level, so one is created if the\n+   * spark acls are enabled. Full access is then granted to the current user,\n+   * all users in the configuration options `\"spark.modify.acls\"` and `\"spark.admin.acls\"`;\n+   * read access to those users and those listed in `\"spark.ui.view.acls\"`\n+   *\n+   * @return an optional domain string. If `None`, then no domain was created.\n+   */\n+  private def createTimelineDomain(): Option[String] = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsOn = sparkConf.getBoolean(\"spark.ui.acls.enable\",\n+        sparkConf.getBoolean(\"spark.acls.enable\", false))\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return None\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      logDebug(s\"Using predefined domain $predefDomain\")\n+      return predefDomain\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (Seq(current) ++ adminAcls ++ modifyAcls ++ viewAcls).mkString(\" \")\n+    val writers = (Seq(current) ++ adminAcls ++ modifyAcls).mkString(\" \")\n+    var domain = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domain with readers: $readers and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domain)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      timelineClient.putDomain(timelineDomain)\n+      Some(domain)\n+    } catch {\n+      case e: Exception => {\n+        logError(s\"cannot create the domain $domain\", e)\n+        // fallback to default\n+        None\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param binding binding to the spark application and YARN\n+   */\n+  override def start(binding: YarnExtensionServiceBinding): Unit = {\n+    val oldstate = enterState(StartedState)\n+    if (oldstate != CreatedState) {\n+      // state model violation\n+      _serviceState.set(oldstate)\n+      throw new IllegalArgumentException(s\"Cannot start the service from state $oldstate\")\n+    }\n+    val context = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    this.sparkContext = context\n+\n+    this.config = new YarnConfiguration(context.hadoopConfiguration)\n+\n+    val sparkConf = sparkContext.conf\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    _batchSize = sparkConf.getInt(BATCH_SIZE, _batchSize)\n+    _postQueueLimit = sparkConf.getInt(POST_QUEUE_LIMIT, _postQueueLimit)\n+    retryInterval = 1000 * sparkConf.getTimeAsSeconds(POST_RETRY_INTERVAL,"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "getTimeAsMs would mean the default would be in millis. Asking for it in s and then multiplying by *1000 gives a time unit slightly better for humans. This is the same pattern as used in `FsHistoryProvider`.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-10-20T19:03:09Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{TimeUnit, LinkedBlockingDeque}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{YarnExtensionService, YarnExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered\n+ * YARN Timeline Server.\n+ */\n+private[spark] class YarnHistoryService extends YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+  def enterState(state: Int): Int = {\n+    logDebug(s\"Entering state $state from $serviceState\")\n+    _serviceState.getAndSet(state)\n+  }\n+\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** YARN configuration from the spark context */\n+  private var config: YarnConfiguration = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var _timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Application name  from the spark start event */\n+  private var applicationName: String = _\n+\n+  /** Application ID*/\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName = Utils.getCurrentUserName()\n+\n+  /** Clock for recording time */\n+  private val clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /**\n+   * Start time of the application, as received in the end event.\n+   */\n+  private var endTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var _batchSize = DEFAULT_BATCH_SIZE\n+\n+  /** queue of entities to asynchronously post, plus the number of events in each entry */\n+  private var _entityQueue = new LinkedBlockingDeque[TimelineEntity]()\n+\n+  /** limit on the total number of events permitted */\n+  private var _postQueueLimit = DEFAULT_POST_QUEUE_LIMIT\n+\n+  /**\n+   * List of events which will be pulled into a timeline\n+   * entity when created\n+   */\n+  private var pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+\n+  private var applicationStartEvent: Option[SparkListenerApplicationStart] = None\n+  private var applicationEndEvent: Option[SparkListenerApplicationEnd] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val _eventsProcessed = new AtomicLong(0)\n+\n+  /** counter of events queued. */\n+  private val _eventsQueued = new AtomicLong(0)\n+\n+  private val _entityPostAttempts = new AtomicLong(0)\n+  private val _entityPostSuccesses = new AtomicLong(0)\n+  /** how many entity postings failed? */\n+  private val _entityPostFailures = new AtomicLong(0)\n+  private val _eventsDropped = new AtomicLong(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicLong(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped = new AtomicBoolean(true)\n+\n+  /**\n+   * Boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive = new AtomicBoolean(false)\n+\n+  /** How long to wait for shutdown before giving up */\n+  private var shutdownWaitTime = 0L\n+\n+  /**\n+   * What is the initial and incrementing interval for POST retries?\n+   */\n+  private var retryInterval = 0L\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: Option[String] = None\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var _timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private[yarn] def createTimelineClient(): TimelineClient = {\n+    require(_timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def timelineClient: TimelineClient = {\n+    require(_timelineClient.isDefined)\n+    _timelineClient.get\n+  }\n+\n+  /**\n+   * Get the total number of events dropped due to the queue of\n+   * outstanding posts being too long\n+   * @return counter of events processed\n+   */\n+\n+  def eventsDropped: Long = {\n+    _eventsDropped.get()\n+  }\n+\n+  /**\n+   * Get the total number of processed events, those handled in the back-end thread without\n+   * being rejected\n+   * @return counter of events processed\n+   */\n+  def eventsProcessed: Long = {\n+    _eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def eventsQueued: Long = {\n+    _eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    _entityQueue.size()\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def batchSize: Int = {\n+    _batchSize\n+  }\n+\n+  /**\n+   * Query the counter of attempts to post events\n+   * @return\n+   */\n+  def postAttempts: Long = _entityPostAttempts.get()\n+\n+  /**\n+   * Get the total number of failed post operations\n+   * @return counter of timeline post operations which failed\n+   */\n+  def postFailures: Long = {\n+    _entityPostFailures.get\n+  }\n+\n+  /**\n+   * Query the counter of post successes\n+   * @return the number of successful posts\n+   */\n+  def postSuccesses: Long = _entityPostSuccesses.get()\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId =  _applicationId\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] =  _attemptId\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    _timelineClient.foreach(_.stop())\n+    _timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain.\n+   *\n+   * A Timeline Domain is a uniquely identified 'namespace' for accessing parts of the timeline.\n+   * Security levels are are managed at the domain level, so one is created if the\n+   * spark acls are enabled. Full access is then granted to the current user,\n+   * all users in the configuration options `\"spark.modify.acls\"` and `\"spark.admin.acls\"`;\n+   * read access to those users and those listed in `\"spark.ui.view.acls\"`\n+   *\n+   * @return an optional domain string. If `None`, then no domain was created.\n+   */\n+  private def createTimelineDomain(): Option[String] = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsOn = sparkConf.getBoolean(\"spark.ui.acls.enable\",\n+        sparkConf.getBoolean(\"spark.acls.enable\", false))\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return None\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      logDebug(s\"Using predefined domain $predefDomain\")\n+      return predefDomain\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (Seq(current) ++ adminAcls ++ modifyAcls ++ viewAcls).mkString(\" \")\n+    val writers = (Seq(current) ++ adminAcls ++ modifyAcls).mkString(\" \")\n+    var domain = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domain with readers: $readers and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domain)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      timelineClient.putDomain(timelineDomain)\n+      Some(domain)\n+    } catch {\n+      case e: Exception => {\n+        logError(s\"cannot create the domain $domain\", e)\n+        // fallback to default\n+        None\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param binding binding to the spark application and YARN\n+   */\n+  override def start(binding: YarnExtensionServiceBinding): Unit = {\n+    val oldstate = enterState(StartedState)\n+    if (oldstate != CreatedState) {\n+      // state model violation\n+      _serviceState.set(oldstate)\n+      throw new IllegalArgumentException(s\"Cannot start the service from state $oldstate\")\n+    }\n+    val context = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    this.sparkContext = context\n+\n+    this.config = new YarnConfiguration(context.hadoopConfiguration)\n+\n+    val sparkConf = sparkContext.conf\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    _batchSize = sparkConf.getInt(BATCH_SIZE, _batchSize)\n+    _postQueueLimit = sparkConf.getInt(POST_QUEUE_LIMIT, _postQueueLimit)\n+    retryInterval = 1000 * sparkConf.getTimeAsSeconds(POST_RETRY_INTERVAL,"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`getTimeAsMs`\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-28T21:10:11Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{TimeUnit, LinkedBlockingDeque}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{YarnExtensionService, YarnExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered\n+ * YARN Timeline Server.\n+ */\n+private[spark] class YarnHistoryService extends YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+  def enterState(state: Int): Int = {\n+    logDebug(s\"Entering state $state from $serviceState\")\n+    _serviceState.getAndSet(state)\n+  }\n+\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** YARN configuration from the spark context */\n+  private var config: YarnConfiguration = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var _timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Application name  from the spark start event */\n+  private var applicationName: String = _\n+\n+  /** Application ID*/\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName = Utils.getCurrentUserName()\n+\n+  /** Clock for recording time */\n+  private val clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /**\n+   * Start time of the application, as received in the end event.\n+   */\n+  private var endTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var _batchSize = DEFAULT_BATCH_SIZE\n+\n+  /** queue of entities to asynchronously post, plus the number of events in each entry */\n+  private var _entityQueue = new LinkedBlockingDeque[TimelineEntity]()\n+\n+  /** limit on the total number of events permitted */\n+  private var _postQueueLimit = DEFAULT_POST_QUEUE_LIMIT\n+\n+  /**\n+   * List of events which will be pulled into a timeline\n+   * entity when created\n+   */\n+  private var pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+\n+  private var applicationStartEvent: Option[SparkListenerApplicationStart] = None\n+  private var applicationEndEvent: Option[SparkListenerApplicationEnd] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val _eventsProcessed = new AtomicLong(0)\n+\n+  /** counter of events queued. */\n+  private val _eventsQueued = new AtomicLong(0)\n+\n+  private val _entityPostAttempts = new AtomicLong(0)\n+  private val _entityPostSuccesses = new AtomicLong(0)\n+  /** how many entity postings failed? */\n+  private val _entityPostFailures = new AtomicLong(0)\n+  private val _eventsDropped = new AtomicLong(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicLong(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped = new AtomicBoolean(true)\n+\n+  /**\n+   * Boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive = new AtomicBoolean(false)\n+\n+  /** How long to wait for shutdown before giving up */\n+  private var shutdownWaitTime = 0L\n+\n+  /**\n+   * What is the initial and incrementing interval for POST retries?\n+   */\n+  private var retryInterval = 0L\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: Option[String] = None\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var _timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private[yarn] def createTimelineClient(): TimelineClient = {\n+    require(_timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def timelineClient: TimelineClient = {\n+    require(_timelineClient.isDefined)\n+    _timelineClient.get\n+  }\n+\n+  /**\n+   * Get the total number of events dropped due to the queue of\n+   * outstanding posts being too long\n+   * @return counter of events processed\n+   */\n+\n+  def eventsDropped: Long = {\n+    _eventsDropped.get()\n+  }\n+\n+  /**\n+   * Get the total number of processed events, those handled in the back-end thread without\n+   * being rejected\n+   * @return counter of events processed\n+   */\n+  def eventsProcessed: Long = {\n+    _eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def eventsQueued: Long = {\n+    _eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    _entityQueue.size()\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def batchSize: Int = {\n+    _batchSize\n+  }\n+\n+  /**\n+   * Query the counter of attempts to post events\n+   * @return\n+   */\n+  def postAttempts: Long = _entityPostAttempts.get()\n+\n+  /**\n+   * Get the total number of failed post operations\n+   * @return counter of timeline post operations which failed\n+   */\n+  def postFailures: Long = {\n+    _entityPostFailures.get\n+  }\n+\n+  /**\n+   * Query the counter of post successes\n+   * @return the number of successful posts\n+   */\n+  def postSuccesses: Long = _entityPostSuccesses.get()\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId =  _applicationId\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] =  _attemptId\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    _timelineClient.foreach(_.stop())\n+    _timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain.\n+   *\n+   * A Timeline Domain is a uniquely identified 'namespace' for accessing parts of the timeline.\n+   * Security levels are are managed at the domain level, so one is created if the\n+   * spark acls are enabled. Full access is then granted to the current user,\n+   * all users in the configuration options `\"spark.modify.acls\"` and `\"spark.admin.acls\"`;\n+   * read access to those users and those listed in `\"spark.ui.view.acls\"`\n+   *\n+   * @return an optional domain string. If `None`, then no domain was created.\n+   */\n+  private def createTimelineDomain(): Option[String] = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsOn = sparkConf.getBoolean(\"spark.ui.acls.enable\",\n+        sparkConf.getBoolean(\"spark.acls.enable\", false))\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return None\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      logDebug(s\"Using predefined domain $predefDomain\")\n+      return predefDomain\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (Seq(current) ++ adminAcls ++ modifyAcls ++ viewAcls).mkString(\" \")\n+    val writers = (Seq(current) ++ adminAcls ++ modifyAcls).mkString(\" \")\n+    var domain = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domain with readers: $readers and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domain)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      timelineClient.putDomain(timelineDomain)\n+      Some(domain)\n+    } catch {\n+      case e: Exception => {\n+        logError(s\"cannot create the domain $domain\", e)\n+        // fallback to default\n+        None\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param binding binding to the spark application and YARN\n+   */\n+  override def start(binding: YarnExtensionServiceBinding): Unit = {\n+    val oldstate = enterState(StartedState)\n+    if (oldstate != CreatedState) {\n+      // state model violation\n+      _serviceState.set(oldstate)\n+      throw new IllegalArgumentException(s\"Cannot start the service from state $oldstate\")\n+    }\n+    val context = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    this.sparkContext = context\n+\n+    this.config = new YarnConfiguration(context.hadoopConfiguration)\n+\n+    val sparkConf = sparkContext.conf\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    _batchSize = sparkConf.getInt(BATCH_SIZE, _batchSize)\n+    _postQueueLimit = sparkConf.getInt(POST_QUEUE_LIMIT, _postQueueLimit)\n+    retryInterval = 1000 * sparkConf.getTimeAsSeconds(POST_RETRY_INTERVAL,\n+      DEFAULT_POST_RETRY_INTERVAL)\n+    shutdownWaitTime = 1000 * sparkConf.getTimeAsSeconds(SHUTDOWN_WAIT_TIME,"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "see above. I just went with the same default convention as that for the FS history provider. Happy to change it though.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-10-20T19:03:57Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{TimeUnit, LinkedBlockingDeque}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{YarnExtensionService, YarnExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered\n+ * YARN Timeline Server.\n+ */\n+private[spark] class YarnHistoryService extends YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+  def enterState(state: Int): Int = {\n+    logDebug(s\"Entering state $state from $serviceState\")\n+    _serviceState.getAndSet(state)\n+  }\n+\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** YARN configuration from the spark context */\n+  private var config: YarnConfiguration = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var _timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Application name  from the spark start event */\n+  private var applicationName: String = _\n+\n+  /** Application ID*/\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName = Utils.getCurrentUserName()\n+\n+  /** Clock for recording time */\n+  private val clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /**\n+   * Start time of the application, as received in the end event.\n+   */\n+  private var endTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var _batchSize = DEFAULT_BATCH_SIZE\n+\n+  /** queue of entities to asynchronously post, plus the number of events in each entry */\n+  private var _entityQueue = new LinkedBlockingDeque[TimelineEntity]()\n+\n+  /** limit on the total number of events permitted */\n+  private var _postQueueLimit = DEFAULT_POST_QUEUE_LIMIT\n+\n+  /**\n+   * List of events which will be pulled into a timeline\n+   * entity when created\n+   */\n+  private var pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+\n+  private var applicationStartEvent: Option[SparkListenerApplicationStart] = None\n+  private var applicationEndEvent: Option[SparkListenerApplicationEnd] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val _eventsProcessed = new AtomicLong(0)\n+\n+  /** counter of events queued. */\n+  private val _eventsQueued = new AtomicLong(0)\n+\n+  private val _entityPostAttempts = new AtomicLong(0)\n+  private val _entityPostSuccesses = new AtomicLong(0)\n+  /** how many entity postings failed? */\n+  private val _entityPostFailures = new AtomicLong(0)\n+  private val _eventsDropped = new AtomicLong(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicLong(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped = new AtomicBoolean(true)\n+\n+  /**\n+   * Boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive = new AtomicBoolean(false)\n+\n+  /** How long to wait for shutdown before giving up */\n+  private var shutdownWaitTime = 0L\n+\n+  /**\n+   * What is the initial and incrementing interval for POST retries?\n+   */\n+  private var retryInterval = 0L\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: Option[String] = None\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var _timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private[yarn] def createTimelineClient(): TimelineClient = {\n+    require(_timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def timelineClient: TimelineClient = {\n+    require(_timelineClient.isDefined)\n+    _timelineClient.get\n+  }\n+\n+  /**\n+   * Get the total number of events dropped due to the queue of\n+   * outstanding posts being too long\n+   * @return counter of events processed\n+   */\n+\n+  def eventsDropped: Long = {\n+    _eventsDropped.get()\n+  }\n+\n+  /**\n+   * Get the total number of processed events, those handled in the back-end thread without\n+   * being rejected\n+   * @return counter of events processed\n+   */\n+  def eventsProcessed: Long = {\n+    _eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def eventsQueued: Long = {\n+    _eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    _entityQueue.size()\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def batchSize: Int = {\n+    _batchSize\n+  }\n+\n+  /**\n+   * Query the counter of attempts to post events\n+   * @return\n+   */\n+  def postAttempts: Long = _entityPostAttempts.get()\n+\n+  /**\n+   * Get the total number of failed post operations\n+   * @return counter of timeline post operations which failed\n+   */\n+  def postFailures: Long = {\n+    _entityPostFailures.get\n+  }\n+\n+  /**\n+   * Query the counter of post successes\n+   * @return the number of successful posts\n+   */\n+  def postSuccesses: Long = _entityPostSuccesses.get()\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId =  _applicationId\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] =  _attemptId\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    _timelineClient.foreach(_.stop())\n+    _timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain.\n+   *\n+   * A Timeline Domain is a uniquely identified 'namespace' for accessing parts of the timeline.\n+   * Security levels are are managed at the domain level, so one is created if the\n+   * spark acls are enabled. Full access is then granted to the current user,\n+   * all users in the configuration options `\"spark.modify.acls\"` and `\"spark.admin.acls\"`;\n+   * read access to those users and those listed in `\"spark.ui.view.acls\"`\n+   *\n+   * @return an optional domain string. If `None`, then no domain was created.\n+   */\n+  private def createTimelineDomain(): Option[String] = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsOn = sparkConf.getBoolean(\"spark.ui.acls.enable\",\n+        sparkConf.getBoolean(\"spark.acls.enable\", false))\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return None\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      logDebug(s\"Using predefined domain $predefDomain\")\n+      return predefDomain\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (Seq(current) ++ adminAcls ++ modifyAcls ++ viewAcls).mkString(\" \")\n+    val writers = (Seq(current) ++ adminAcls ++ modifyAcls).mkString(\" \")\n+    var domain = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domain with readers: $readers and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domain)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      timelineClient.putDomain(timelineDomain)\n+      Some(domain)\n+    } catch {\n+      case e: Exception => {\n+        logError(s\"cannot create the domain $domain\", e)\n+        // fallback to default\n+        None\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param binding binding to the spark application and YARN\n+   */\n+  override def start(binding: YarnExtensionServiceBinding): Unit = {\n+    val oldstate = enterState(StartedState)\n+    if (oldstate != CreatedState) {\n+      // state model violation\n+      _serviceState.set(oldstate)\n+      throw new IllegalArgumentException(s\"Cannot start the service from state $oldstate\")\n+    }\n+    val context = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    this.sparkContext = context\n+\n+    this.config = new YarnConfiguration(context.hadoopConfiguration)\n+\n+    val sparkConf = sparkContext.conf\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    _batchSize = sparkConf.getInt(BATCH_SIZE, _batchSize)\n+    _postQueueLimit = sparkConf.getInt(POST_QUEUE_LIMIT, _postQueueLimit)\n+    retryInterval = 1000 * sparkConf.getTimeAsSeconds(POST_RETRY_INTERVAL,\n+      DEFAULT_POST_RETRY_INTERVAL)\n+    shutdownWaitTime = 1000 * sparkConf.getTimeAsSeconds(SHUTDOWN_WAIT_TIME,"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Same question as before: why not make this a daemon thread?\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-28T21:10:58Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{TimeUnit, LinkedBlockingDeque}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{YarnExtensionService, YarnExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered\n+ * YARN Timeline Server.\n+ */\n+private[spark] class YarnHistoryService extends YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+  def enterState(state: Int): Int = {\n+    logDebug(s\"Entering state $state from $serviceState\")\n+    _serviceState.getAndSet(state)\n+  }\n+\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** YARN configuration from the spark context */\n+  private var config: YarnConfiguration = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var _timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Application name  from the spark start event */\n+  private var applicationName: String = _\n+\n+  /** Application ID*/\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName = Utils.getCurrentUserName()\n+\n+  /** Clock for recording time */\n+  private val clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /**\n+   * Start time of the application, as received in the end event.\n+   */\n+  private var endTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var _batchSize = DEFAULT_BATCH_SIZE\n+\n+  /** queue of entities to asynchronously post, plus the number of events in each entry */\n+  private var _entityQueue = new LinkedBlockingDeque[TimelineEntity]()\n+\n+  /** limit on the total number of events permitted */\n+  private var _postQueueLimit = DEFAULT_POST_QUEUE_LIMIT\n+\n+  /**\n+   * List of events which will be pulled into a timeline\n+   * entity when created\n+   */\n+  private var pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+\n+  private var applicationStartEvent: Option[SparkListenerApplicationStart] = None\n+  private var applicationEndEvent: Option[SparkListenerApplicationEnd] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val _eventsProcessed = new AtomicLong(0)\n+\n+  /** counter of events queued. */\n+  private val _eventsQueued = new AtomicLong(0)\n+\n+  private val _entityPostAttempts = new AtomicLong(0)\n+  private val _entityPostSuccesses = new AtomicLong(0)\n+  /** how many entity postings failed? */\n+  private val _entityPostFailures = new AtomicLong(0)\n+  private val _eventsDropped = new AtomicLong(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicLong(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped = new AtomicBoolean(true)\n+\n+  /**\n+   * Boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive = new AtomicBoolean(false)\n+\n+  /** How long to wait for shutdown before giving up */\n+  private var shutdownWaitTime = 0L\n+\n+  /**\n+   * What is the initial and incrementing interval for POST retries?\n+   */\n+  private var retryInterval = 0L\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: Option[String] = None\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var _timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private[yarn] def createTimelineClient(): TimelineClient = {\n+    require(_timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def timelineClient: TimelineClient = {\n+    require(_timelineClient.isDefined)\n+    _timelineClient.get\n+  }\n+\n+  /**\n+   * Get the total number of events dropped due to the queue of\n+   * outstanding posts being too long\n+   * @return counter of events processed\n+   */\n+\n+  def eventsDropped: Long = {\n+    _eventsDropped.get()\n+  }\n+\n+  /**\n+   * Get the total number of processed events, those handled in the back-end thread without\n+   * being rejected\n+   * @return counter of events processed\n+   */\n+  def eventsProcessed: Long = {\n+    _eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def eventsQueued: Long = {\n+    _eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    _entityQueue.size()\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def batchSize: Int = {\n+    _batchSize\n+  }\n+\n+  /**\n+   * Query the counter of attempts to post events\n+   * @return\n+   */\n+  def postAttempts: Long = _entityPostAttempts.get()\n+\n+  /**\n+   * Get the total number of failed post operations\n+   * @return counter of timeline post operations which failed\n+   */\n+  def postFailures: Long = {\n+    _entityPostFailures.get\n+  }\n+\n+  /**\n+   * Query the counter of post successes\n+   * @return the number of successful posts\n+   */\n+  def postSuccesses: Long = _entityPostSuccesses.get()\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId =  _applicationId\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] =  _attemptId\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    _timelineClient.foreach(_.stop())\n+    _timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain.\n+   *\n+   * A Timeline Domain is a uniquely identified 'namespace' for accessing parts of the timeline.\n+   * Security levels are are managed at the domain level, so one is created if the\n+   * spark acls are enabled. Full access is then granted to the current user,\n+   * all users in the configuration options `\"spark.modify.acls\"` and `\"spark.admin.acls\"`;\n+   * read access to those users and those listed in `\"spark.ui.view.acls\"`\n+   *\n+   * @return an optional domain string. If `None`, then no domain was created.\n+   */\n+  private def createTimelineDomain(): Option[String] = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsOn = sparkConf.getBoolean(\"spark.ui.acls.enable\",\n+        sparkConf.getBoolean(\"spark.acls.enable\", false))\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return None\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      logDebug(s\"Using predefined domain $predefDomain\")\n+      return predefDomain\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (Seq(current) ++ adminAcls ++ modifyAcls ++ viewAcls).mkString(\" \")\n+    val writers = (Seq(current) ++ adminAcls ++ modifyAcls).mkString(\" \")\n+    var domain = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domain with readers: $readers and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domain)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      timelineClient.putDomain(timelineDomain)\n+      Some(domain)\n+    } catch {\n+      case e: Exception => {\n+        logError(s\"cannot create the domain $domain\", e)\n+        // fallback to default\n+        None\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param binding binding to the spark application and YARN\n+   */\n+  override def start(binding: YarnExtensionServiceBinding): Unit = {\n+    val oldstate = enterState(StartedState)\n+    if (oldstate != CreatedState) {\n+      // state model violation\n+      _serviceState.set(oldstate)\n+      throw new IllegalArgumentException(s\"Cannot start the service from state $oldstate\")\n+    }\n+    val context = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    this.sparkContext = context\n+\n+    this.config = new YarnConfiguration(context.hadoopConfiguration)\n+\n+    val sparkConf = sparkContext.conf\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    _batchSize = sparkConf.getInt(BATCH_SIZE, _batchSize)\n+    _postQueueLimit = sparkConf.getInt(POST_QUEUE_LIMIT, _postQueueLimit)\n+    retryInterval = 1000 * sparkConf.getTimeAsSeconds(POST_RETRY_INTERVAL,\n+      DEFAULT_POST_RETRY_INTERVAL)\n+    shutdownWaitTime = 1000 * sparkConf.getTimeAsSeconds(SHUTDOWN_WAIT_TIME,\n+      DEFAULT_SHUTDOWN_WAIT_TIME)\n+\n+\n+    if (timelineServiceEnabled) {\n+      _timelineWebappAddress = getTimelineEndpoint(config)\n+\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at ${_timelineWebappAddress}\")\n+      _timelineClient = Some(createTimelineClient())\n+      domainId = createTimelineDomain()\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "done\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-10-20T19:05:26Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{TimeUnit, LinkedBlockingDeque}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{YarnExtensionService, YarnExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered\n+ * YARN Timeline Server.\n+ */\n+private[spark] class YarnHistoryService extends YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+  def enterState(state: Int): Int = {\n+    logDebug(s\"Entering state $state from $serviceState\")\n+    _serviceState.getAndSet(state)\n+  }\n+\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** YARN configuration from the spark context */\n+  private var config: YarnConfiguration = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var _timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Application name  from the spark start event */\n+  private var applicationName: String = _\n+\n+  /** Application ID*/\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName = Utils.getCurrentUserName()\n+\n+  /** Clock for recording time */\n+  private val clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /**\n+   * Start time of the application, as received in the end event.\n+   */\n+  private var endTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var _batchSize = DEFAULT_BATCH_SIZE\n+\n+  /** queue of entities to asynchronously post, plus the number of events in each entry */\n+  private var _entityQueue = new LinkedBlockingDeque[TimelineEntity]()\n+\n+  /** limit on the total number of events permitted */\n+  private var _postQueueLimit = DEFAULT_POST_QUEUE_LIMIT\n+\n+  /**\n+   * List of events which will be pulled into a timeline\n+   * entity when created\n+   */\n+  private var pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+\n+  private var applicationStartEvent: Option[SparkListenerApplicationStart] = None\n+  private var applicationEndEvent: Option[SparkListenerApplicationEnd] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val _eventsProcessed = new AtomicLong(0)\n+\n+  /** counter of events queued. */\n+  private val _eventsQueued = new AtomicLong(0)\n+\n+  private val _entityPostAttempts = new AtomicLong(0)\n+  private val _entityPostSuccesses = new AtomicLong(0)\n+  /** how many entity postings failed? */\n+  private val _entityPostFailures = new AtomicLong(0)\n+  private val _eventsDropped = new AtomicLong(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicLong(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped = new AtomicBoolean(true)\n+\n+  /**\n+   * Boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive = new AtomicBoolean(false)\n+\n+  /** How long to wait for shutdown before giving up */\n+  private var shutdownWaitTime = 0L\n+\n+  /**\n+   * What is the initial and incrementing interval for POST retries?\n+   */\n+  private var retryInterval = 0L\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: Option[String] = None\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var _timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private[yarn] def createTimelineClient(): TimelineClient = {\n+    require(_timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def timelineClient: TimelineClient = {\n+    require(_timelineClient.isDefined)\n+    _timelineClient.get\n+  }\n+\n+  /**\n+   * Get the total number of events dropped due to the queue of\n+   * outstanding posts being too long\n+   * @return counter of events processed\n+   */\n+\n+  def eventsDropped: Long = {\n+    _eventsDropped.get()\n+  }\n+\n+  /**\n+   * Get the total number of processed events, those handled in the back-end thread without\n+   * being rejected\n+   * @return counter of events processed\n+   */\n+  def eventsProcessed: Long = {\n+    _eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def eventsQueued: Long = {\n+    _eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    _entityQueue.size()\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def batchSize: Int = {\n+    _batchSize\n+  }\n+\n+  /**\n+   * Query the counter of attempts to post events\n+   * @return\n+   */\n+  def postAttempts: Long = _entityPostAttempts.get()\n+\n+  /**\n+   * Get the total number of failed post operations\n+   * @return counter of timeline post operations which failed\n+   */\n+  def postFailures: Long = {\n+    _entityPostFailures.get\n+  }\n+\n+  /**\n+   * Query the counter of post successes\n+   * @return the number of successful posts\n+   */\n+  def postSuccesses: Long = _entityPostSuccesses.get()\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId =  _applicationId\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] =  _attemptId\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    _timelineClient.foreach(_.stop())\n+    _timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain.\n+   *\n+   * A Timeline Domain is a uniquely identified 'namespace' for accessing parts of the timeline.\n+   * Security levels are are managed at the domain level, so one is created if the\n+   * spark acls are enabled. Full access is then granted to the current user,\n+   * all users in the configuration options `\"spark.modify.acls\"` and `\"spark.admin.acls\"`;\n+   * read access to those users and those listed in `\"spark.ui.view.acls\"`\n+   *\n+   * @return an optional domain string. If `None`, then no domain was created.\n+   */\n+  private def createTimelineDomain(): Option[String] = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsOn = sparkConf.getBoolean(\"spark.ui.acls.enable\",\n+        sparkConf.getBoolean(\"spark.acls.enable\", false))\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return None\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      logDebug(s\"Using predefined domain $predefDomain\")\n+      return predefDomain\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (Seq(current) ++ adminAcls ++ modifyAcls ++ viewAcls).mkString(\" \")\n+    val writers = (Seq(current) ++ adminAcls ++ modifyAcls).mkString(\" \")\n+    var domain = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domain with readers: $readers and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domain)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      timelineClient.putDomain(timelineDomain)\n+      Some(domain)\n+    } catch {\n+      case e: Exception => {\n+        logError(s\"cannot create the domain $domain\", e)\n+        // fallback to default\n+        None\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param binding binding to the spark application and YARN\n+   */\n+  override def start(binding: YarnExtensionServiceBinding): Unit = {\n+    val oldstate = enterState(StartedState)\n+    if (oldstate != CreatedState) {\n+      // state model violation\n+      _serviceState.set(oldstate)\n+      throw new IllegalArgumentException(s\"Cannot start the service from state $oldstate\")\n+    }\n+    val context = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    this.sparkContext = context\n+\n+    this.config = new YarnConfiguration(context.hadoopConfiguration)\n+\n+    val sparkConf = sparkContext.conf\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    _batchSize = sparkConf.getInt(BATCH_SIZE, _batchSize)\n+    _postQueueLimit = sparkConf.getInt(POST_QUEUE_LIMIT, _postQueueLimit)\n+    retryInterval = 1000 * sparkConf.getTimeAsSeconds(POST_RETRY_INTERVAL,\n+      DEFAULT_POST_RETRY_INTERVAL)\n+    shutdownWaitTime = 1000 * sparkConf.getTimeAsSeconds(SHUTDOWN_WAIT_TIME,\n+      DEFAULT_SHUTDOWN_WAIT_TIME)\n+\n+\n+    if (timelineServiceEnabled) {\n+      _timelineWebappAddress = getTimelineEndpoint(config)\n+\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at ${_timelineWebappAddress}\")\n+      _timelineClient = Some(createTimelineClient())\n+      domainId = createTimelineDomain()\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "I'm confused as to why you need both `bindToYarnApplication` and this. In what situation would `_applicationId` be different than `sparkApplicationId`? If that situation exists, it sounds like a bug to me.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-28T21:13:26Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{TimeUnit, LinkedBlockingDeque}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{YarnExtensionService, YarnExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered\n+ * YARN Timeline Server.\n+ */\n+private[spark] class YarnHistoryService extends YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+  def enterState(state: Int): Int = {\n+    logDebug(s\"Entering state $state from $serviceState\")\n+    _serviceState.getAndSet(state)\n+  }\n+\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** YARN configuration from the spark context */\n+  private var config: YarnConfiguration = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var _timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Application name  from the spark start event */\n+  private var applicationName: String = _\n+\n+  /** Application ID*/\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName = Utils.getCurrentUserName()\n+\n+  /** Clock for recording time */\n+  private val clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /**\n+   * Start time of the application, as received in the end event.\n+   */\n+  private var endTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var _batchSize = DEFAULT_BATCH_SIZE\n+\n+  /** queue of entities to asynchronously post, plus the number of events in each entry */\n+  private var _entityQueue = new LinkedBlockingDeque[TimelineEntity]()\n+\n+  /** limit on the total number of events permitted */\n+  private var _postQueueLimit = DEFAULT_POST_QUEUE_LIMIT\n+\n+  /**\n+   * List of events which will be pulled into a timeline\n+   * entity when created\n+   */\n+  private var pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+\n+  private var applicationStartEvent: Option[SparkListenerApplicationStart] = None\n+  private var applicationEndEvent: Option[SparkListenerApplicationEnd] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val _eventsProcessed = new AtomicLong(0)\n+\n+  /** counter of events queued. */\n+  private val _eventsQueued = new AtomicLong(0)\n+\n+  private val _entityPostAttempts = new AtomicLong(0)\n+  private val _entityPostSuccesses = new AtomicLong(0)\n+  /** how many entity postings failed? */\n+  private val _entityPostFailures = new AtomicLong(0)\n+  private val _eventsDropped = new AtomicLong(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicLong(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped = new AtomicBoolean(true)\n+\n+  /**\n+   * Boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive = new AtomicBoolean(false)\n+\n+  /** How long to wait for shutdown before giving up */\n+  private var shutdownWaitTime = 0L\n+\n+  /**\n+   * What is the initial and incrementing interval for POST retries?\n+   */\n+  private var retryInterval = 0L\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: Option[String] = None\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var _timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private[yarn] def createTimelineClient(): TimelineClient = {\n+    require(_timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def timelineClient: TimelineClient = {\n+    require(_timelineClient.isDefined)\n+    _timelineClient.get\n+  }\n+\n+  /**\n+   * Get the total number of events dropped due to the queue of\n+   * outstanding posts being too long\n+   * @return counter of events processed\n+   */\n+\n+  def eventsDropped: Long = {\n+    _eventsDropped.get()\n+  }\n+\n+  /**\n+   * Get the total number of processed events, those handled in the back-end thread without\n+   * being rejected\n+   * @return counter of events processed\n+   */\n+  def eventsProcessed: Long = {\n+    _eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def eventsQueued: Long = {\n+    _eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    _entityQueue.size()\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def batchSize: Int = {\n+    _batchSize\n+  }\n+\n+  /**\n+   * Query the counter of attempts to post events\n+   * @return\n+   */\n+  def postAttempts: Long = _entityPostAttempts.get()\n+\n+  /**\n+   * Get the total number of failed post operations\n+   * @return counter of timeline post operations which failed\n+   */\n+  def postFailures: Long = {\n+    _entityPostFailures.get\n+  }\n+\n+  /**\n+   * Query the counter of post successes\n+   * @return the number of successful posts\n+   */\n+  def postSuccesses: Long = _entityPostSuccesses.get()\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId =  _applicationId\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] =  _attemptId\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    _timelineClient.foreach(_.stop())\n+    _timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain.\n+   *\n+   * A Timeline Domain is a uniquely identified 'namespace' for accessing parts of the timeline.\n+   * Security levels are are managed at the domain level, so one is created if the\n+   * spark acls are enabled. Full access is then granted to the current user,\n+   * all users in the configuration options `\"spark.modify.acls\"` and `\"spark.admin.acls\"`;\n+   * read access to those users and those listed in `\"spark.ui.view.acls\"`\n+   *\n+   * @return an optional domain string. If `None`, then no domain was created.\n+   */\n+  private def createTimelineDomain(): Option[String] = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsOn = sparkConf.getBoolean(\"spark.ui.acls.enable\",\n+        sparkConf.getBoolean(\"spark.acls.enable\", false))\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return None\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      logDebug(s\"Using predefined domain $predefDomain\")\n+      return predefDomain\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (Seq(current) ++ adminAcls ++ modifyAcls ++ viewAcls).mkString(\" \")\n+    val writers = (Seq(current) ++ adminAcls ++ modifyAcls).mkString(\" \")\n+    var domain = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domain with readers: $readers and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domain)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      timelineClient.putDomain(timelineDomain)\n+      Some(domain)\n+    } catch {\n+      case e: Exception => {\n+        logError(s\"cannot create the domain $domain\", e)\n+        // fallback to default\n+        None\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param binding binding to the spark application and YARN\n+   */\n+  override def start(binding: YarnExtensionServiceBinding): Unit = {\n+    val oldstate = enterState(StartedState)\n+    if (oldstate != CreatedState) {\n+      // state model violation\n+      _serviceState.set(oldstate)\n+      throw new IllegalArgumentException(s\"Cannot start the service from state $oldstate\")\n+    }\n+    val context = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    this.sparkContext = context\n+\n+    this.config = new YarnConfiguration(context.hadoopConfiguration)\n+\n+    val sparkConf = sparkContext.conf\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    _batchSize = sparkConf.getInt(BATCH_SIZE, _batchSize)\n+    _postQueueLimit = sparkConf.getInt(POST_QUEUE_LIMIT, _postQueueLimit)\n+    retryInterval = 1000 * sparkConf.getTimeAsSeconds(POST_RETRY_INTERVAL,\n+      DEFAULT_POST_RETRY_INTERVAL)\n+    shutdownWaitTime = 1000 * sparkConf.getTimeAsSeconds(SHUTDOWN_WAIT_TIME,\n+      DEFAULT_SHUTDOWN_WAIT_TIME)\n+\n+\n+    if (timelineServiceEnabled) {\n+      _timelineWebappAddress = getTimelineEndpoint(config)\n+\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at ${_timelineWebappAddress}\")\n+      _timelineClient = Some(createTimelineClient())\n+      domainId = createTimelineDomain()\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start()\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    if (registerListener()) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED` is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(config)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString(): String = {\n+    s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+    s\" state=$serviceState;\" +\n+    s\" endpoint=${_timelineWebappAddress};\" +\n+    s\" bonded to ATS=$bondedToATS;\" +\n+    s\" listening=$listening;\" +\n+    s\" batchSize=$batchSize;\" +\n+    s\" flush count=$getFlushCount;\" +\n+    s\" total number queued=$eventsQueued, processed=$eventsProcessed;\" +\n+    s\" attempted entity posts=$postAttempts\" +\n+    s\" successful entity posts=$postSuccesses\" +\n+    s\" failed entity posts=$postFailures;\" +\n+    s\" events dropped=$eventsDropped;\" +\n+    s\" app start event received=$appStartEventProcessed;\" +\n+    s\" app end event received=$appEndEventProcessed;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    _timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    _applicationId = appId\n+    _attemptId = attemptId\n+  }\n+\n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "spark app ID comes in as \"1\", \"2\", \"3\", not a unique ID. Also, you get events coming in before the spark app start has  happened `SparkListenerEnvironmentUpdate`, to name one. It's not a bug, the one coming in in the context is ideal for UIs and subdirectories, and is used in some tests as its value is predictable. The YARN one has the strength of effectively being a UUID.\n\nCurrently the history publisher only uses that string value; it could take them as properties (spark.yarn.appId, spark.yarn.attemptId)?, if the back ends set them up right. Passing them down as the real YARN values provides the option of allowing other services to do more with them (i.e. if some other YARN/Hadoop APIs needed them, they'd be there) without having to rebuild them from strings, with all its brittleness. That said, putting them as properties would make it possible to pull the extension service code into spark core, and potentially allow the extension services to be deployed in the Mesos Scheduler.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-10-20T19:11:53Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{TimeUnit, LinkedBlockingDeque}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{YarnExtensionService, YarnExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered\n+ * YARN Timeline Server.\n+ */\n+private[spark] class YarnHistoryService extends YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+  def enterState(state: Int): Int = {\n+    logDebug(s\"Entering state $state from $serviceState\")\n+    _serviceState.getAndSet(state)\n+  }\n+\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** YARN configuration from the spark context */\n+  private var config: YarnConfiguration = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var _timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Application name  from the spark start event */\n+  private var applicationName: String = _\n+\n+  /** Application ID*/\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName = Utils.getCurrentUserName()\n+\n+  /** Clock for recording time */\n+  private val clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /**\n+   * Start time of the application, as received in the end event.\n+   */\n+  private var endTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var _batchSize = DEFAULT_BATCH_SIZE\n+\n+  /** queue of entities to asynchronously post, plus the number of events in each entry */\n+  private var _entityQueue = new LinkedBlockingDeque[TimelineEntity]()\n+\n+  /** limit on the total number of events permitted */\n+  private var _postQueueLimit = DEFAULT_POST_QUEUE_LIMIT\n+\n+  /**\n+   * List of events which will be pulled into a timeline\n+   * entity when created\n+   */\n+  private var pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+\n+  private var applicationStartEvent: Option[SparkListenerApplicationStart] = None\n+  private var applicationEndEvent: Option[SparkListenerApplicationEnd] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val _eventsProcessed = new AtomicLong(0)\n+\n+  /** counter of events queued. */\n+  private val _eventsQueued = new AtomicLong(0)\n+\n+  private val _entityPostAttempts = new AtomicLong(0)\n+  private val _entityPostSuccesses = new AtomicLong(0)\n+  /** how many entity postings failed? */\n+  private val _entityPostFailures = new AtomicLong(0)\n+  private val _eventsDropped = new AtomicLong(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicLong(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped = new AtomicBoolean(true)\n+\n+  /**\n+   * Boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive = new AtomicBoolean(false)\n+\n+  /** How long to wait for shutdown before giving up */\n+  private var shutdownWaitTime = 0L\n+\n+  /**\n+   * What is the initial and incrementing interval for POST retries?\n+   */\n+  private var retryInterval = 0L\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: Option[String] = None\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var _timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private[yarn] def createTimelineClient(): TimelineClient = {\n+    require(_timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def timelineClient: TimelineClient = {\n+    require(_timelineClient.isDefined)\n+    _timelineClient.get\n+  }\n+\n+  /**\n+   * Get the total number of events dropped due to the queue of\n+   * outstanding posts being too long\n+   * @return counter of events processed\n+   */\n+\n+  def eventsDropped: Long = {\n+    _eventsDropped.get()\n+  }\n+\n+  /**\n+   * Get the total number of processed events, those handled in the back-end thread without\n+   * being rejected\n+   * @return counter of events processed\n+   */\n+  def eventsProcessed: Long = {\n+    _eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def eventsQueued: Long = {\n+    _eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    _entityQueue.size()\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def batchSize: Int = {\n+    _batchSize\n+  }\n+\n+  /**\n+   * Query the counter of attempts to post events\n+   * @return\n+   */\n+  def postAttempts: Long = _entityPostAttempts.get()\n+\n+  /**\n+   * Get the total number of failed post operations\n+   * @return counter of timeline post operations which failed\n+   */\n+  def postFailures: Long = {\n+    _entityPostFailures.get\n+  }\n+\n+  /**\n+   * Query the counter of post successes\n+   * @return the number of successful posts\n+   */\n+  def postSuccesses: Long = _entityPostSuccesses.get()\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId =  _applicationId\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] =  _attemptId\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    _timelineClient.foreach(_.stop())\n+    _timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain.\n+   *\n+   * A Timeline Domain is a uniquely identified 'namespace' for accessing parts of the timeline.\n+   * Security levels are are managed at the domain level, so one is created if the\n+   * spark acls are enabled. Full access is then granted to the current user,\n+   * all users in the configuration options `\"spark.modify.acls\"` and `\"spark.admin.acls\"`;\n+   * read access to those users and those listed in `\"spark.ui.view.acls\"`\n+   *\n+   * @return an optional domain string. If `None`, then no domain was created.\n+   */\n+  private def createTimelineDomain(): Option[String] = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsOn = sparkConf.getBoolean(\"spark.ui.acls.enable\",\n+        sparkConf.getBoolean(\"spark.acls.enable\", false))\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return None\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      logDebug(s\"Using predefined domain $predefDomain\")\n+      return predefDomain\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (Seq(current) ++ adminAcls ++ modifyAcls ++ viewAcls).mkString(\" \")\n+    val writers = (Seq(current) ++ adminAcls ++ modifyAcls).mkString(\" \")\n+    var domain = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domain with readers: $readers and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domain)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      timelineClient.putDomain(timelineDomain)\n+      Some(domain)\n+    } catch {\n+      case e: Exception => {\n+        logError(s\"cannot create the domain $domain\", e)\n+        // fallback to default\n+        None\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param binding binding to the spark application and YARN\n+   */\n+  override def start(binding: YarnExtensionServiceBinding): Unit = {\n+    val oldstate = enterState(StartedState)\n+    if (oldstate != CreatedState) {\n+      // state model violation\n+      _serviceState.set(oldstate)\n+      throw new IllegalArgumentException(s\"Cannot start the service from state $oldstate\")\n+    }\n+    val context = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    this.sparkContext = context\n+\n+    this.config = new YarnConfiguration(context.hadoopConfiguration)\n+\n+    val sparkConf = sparkContext.conf\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    _batchSize = sparkConf.getInt(BATCH_SIZE, _batchSize)\n+    _postQueueLimit = sparkConf.getInt(POST_QUEUE_LIMIT, _postQueueLimit)\n+    retryInterval = 1000 * sparkConf.getTimeAsSeconds(POST_RETRY_INTERVAL,\n+      DEFAULT_POST_RETRY_INTERVAL)\n+    shutdownWaitTime = 1000 * sparkConf.getTimeAsSeconds(SHUTDOWN_WAIT_TIME,\n+      DEFAULT_SHUTDOWN_WAIT_TIME)\n+\n+\n+    if (timelineServiceEnabled) {\n+      _timelineWebappAddress = getTimelineEndpoint(config)\n+\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at ${_timelineWebappAddress}\")\n+      _timelineClient = Some(createTimelineClient())\n+      domainId = createTimelineDomain()\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start()\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    if (registerListener()) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED` is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(config)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString(): String = {\n+    s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+    s\" state=$serviceState;\" +\n+    s\" endpoint=${_timelineWebappAddress};\" +\n+    s\" bonded to ATS=$bondedToATS;\" +\n+    s\" listening=$listening;\" +\n+    s\" batchSize=$batchSize;\" +\n+    s\" flush count=$getFlushCount;\" +\n+    s\" total number queued=$eventsQueued, processed=$eventsProcessed;\" +\n+    s\" attempted entity posts=$postAttempts\" +\n+    s\" successful entity posts=$postSuccesses\" +\n+    s\" failed entity posts=$postFailures;\" +\n+    s\" events dropped=$eventsDropped;\" +\n+    s\" app start event received=$appStartEventProcessed;\" +\n+    s\" app end event received=$appEndEventProcessed;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    _timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    _applicationId = appId\n+    _attemptId = attemptId\n+  }\n+\n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This doesn't really look thread-safe. This thread can be incrementing counters and calling `handleEvent` while the thread calling `stop()` might be running code in `asyncFlush()` or interrupting the event-handling thread. Are those situations safe?\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-28T21:17:18Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{TimeUnit, LinkedBlockingDeque}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{YarnExtensionService, YarnExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered\n+ * YARN Timeline Server.\n+ */\n+private[spark] class YarnHistoryService extends YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+  def enterState(state: Int): Int = {\n+    logDebug(s\"Entering state $state from $serviceState\")\n+    _serviceState.getAndSet(state)\n+  }\n+\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** YARN configuration from the spark context */\n+  private var config: YarnConfiguration = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var _timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Application name  from the spark start event */\n+  private var applicationName: String = _\n+\n+  /** Application ID*/\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName = Utils.getCurrentUserName()\n+\n+  /** Clock for recording time */\n+  private val clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /**\n+   * Start time of the application, as received in the end event.\n+   */\n+  private var endTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var _batchSize = DEFAULT_BATCH_SIZE\n+\n+  /** queue of entities to asynchronously post, plus the number of events in each entry */\n+  private var _entityQueue = new LinkedBlockingDeque[TimelineEntity]()\n+\n+  /** limit on the total number of events permitted */\n+  private var _postQueueLimit = DEFAULT_POST_QUEUE_LIMIT\n+\n+  /**\n+   * List of events which will be pulled into a timeline\n+   * entity when created\n+   */\n+  private var pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+\n+  private var applicationStartEvent: Option[SparkListenerApplicationStart] = None\n+  private var applicationEndEvent: Option[SparkListenerApplicationEnd] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val _eventsProcessed = new AtomicLong(0)\n+\n+  /** counter of events queued. */\n+  private val _eventsQueued = new AtomicLong(0)\n+\n+  private val _entityPostAttempts = new AtomicLong(0)\n+  private val _entityPostSuccesses = new AtomicLong(0)\n+  /** how many entity postings failed? */\n+  private val _entityPostFailures = new AtomicLong(0)\n+  private val _eventsDropped = new AtomicLong(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicLong(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped = new AtomicBoolean(true)\n+\n+  /**\n+   * Boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive = new AtomicBoolean(false)\n+\n+  /** How long to wait for shutdown before giving up */\n+  private var shutdownWaitTime = 0L\n+\n+  /**\n+   * What is the initial and incrementing interval for POST retries?\n+   */\n+  private var retryInterval = 0L\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: Option[String] = None\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var _timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private[yarn] def createTimelineClient(): TimelineClient = {\n+    require(_timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def timelineClient: TimelineClient = {\n+    require(_timelineClient.isDefined)\n+    _timelineClient.get\n+  }\n+\n+  /**\n+   * Get the total number of events dropped due to the queue of\n+   * outstanding posts being too long\n+   * @return counter of events processed\n+   */\n+\n+  def eventsDropped: Long = {\n+    _eventsDropped.get()\n+  }\n+\n+  /**\n+   * Get the total number of processed events, those handled in the back-end thread without\n+   * being rejected\n+   * @return counter of events processed\n+   */\n+  def eventsProcessed: Long = {\n+    _eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def eventsQueued: Long = {\n+    _eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    _entityQueue.size()\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def batchSize: Int = {\n+    _batchSize\n+  }\n+\n+  /**\n+   * Query the counter of attempts to post events\n+   * @return\n+   */\n+  def postAttempts: Long = _entityPostAttempts.get()\n+\n+  /**\n+   * Get the total number of failed post operations\n+   * @return counter of timeline post operations which failed\n+   */\n+  def postFailures: Long = {\n+    _entityPostFailures.get\n+  }\n+\n+  /**\n+   * Query the counter of post successes\n+   * @return the number of successful posts\n+   */\n+  def postSuccesses: Long = _entityPostSuccesses.get()\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId =  _applicationId\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] =  _attemptId\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    _timelineClient.foreach(_.stop())\n+    _timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain.\n+   *\n+   * A Timeline Domain is a uniquely identified 'namespace' for accessing parts of the timeline.\n+   * Security levels are are managed at the domain level, so one is created if the\n+   * spark acls are enabled. Full access is then granted to the current user,\n+   * all users in the configuration options `\"spark.modify.acls\"` and `\"spark.admin.acls\"`;\n+   * read access to those users and those listed in `\"spark.ui.view.acls\"`\n+   *\n+   * @return an optional domain string. If `None`, then no domain was created.\n+   */\n+  private def createTimelineDomain(): Option[String] = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsOn = sparkConf.getBoolean(\"spark.ui.acls.enable\",\n+        sparkConf.getBoolean(\"spark.acls.enable\", false))\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return None\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      logDebug(s\"Using predefined domain $predefDomain\")\n+      return predefDomain\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (Seq(current) ++ adminAcls ++ modifyAcls ++ viewAcls).mkString(\" \")\n+    val writers = (Seq(current) ++ adminAcls ++ modifyAcls).mkString(\" \")\n+    var domain = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domain with readers: $readers and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domain)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      timelineClient.putDomain(timelineDomain)\n+      Some(domain)\n+    } catch {\n+      case e: Exception => {\n+        logError(s\"cannot create the domain $domain\", e)\n+        // fallback to default\n+        None\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param binding binding to the spark application and YARN\n+   */\n+  override def start(binding: YarnExtensionServiceBinding): Unit = {\n+    val oldstate = enterState(StartedState)\n+    if (oldstate != CreatedState) {\n+      // state model violation\n+      _serviceState.set(oldstate)\n+      throw new IllegalArgumentException(s\"Cannot start the service from state $oldstate\")\n+    }\n+    val context = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    this.sparkContext = context\n+\n+    this.config = new YarnConfiguration(context.hadoopConfiguration)\n+\n+    val sparkConf = sparkContext.conf\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    _batchSize = sparkConf.getInt(BATCH_SIZE, _batchSize)\n+    _postQueueLimit = sparkConf.getInt(POST_QUEUE_LIMIT, _postQueueLimit)\n+    retryInterval = 1000 * sparkConf.getTimeAsSeconds(POST_RETRY_INTERVAL,\n+      DEFAULT_POST_RETRY_INTERVAL)\n+    shutdownWaitTime = 1000 * sparkConf.getTimeAsSeconds(SHUTDOWN_WAIT_TIME,\n+      DEFAULT_SHUTDOWN_WAIT_TIME)\n+\n+\n+    if (timelineServiceEnabled) {\n+      _timelineWebappAddress = getTimelineEndpoint(config)\n+\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at ${_timelineWebappAddress}\")\n+      _timelineClient = Some(createTimelineClient())\n+      domainId = createTimelineDomain()\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start()\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    if (registerListener()) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED` is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(config)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString(): String = {\n+    s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+    s\" state=$serviceState;\" +\n+    s\" endpoint=${_timelineWebappAddress};\" +\n+    s\" bonded to ATS=$bondedToATS;\" +\n+    s\" listening=$listening;\" +\n+    s\" batchSize=$batchSize;\" +\n+    s\" flush count=$getFlushCount;\" +\n+    s\" total number queued=$eventsQueued, processed=$eventsProcessed;\" +\n+    s\" attempted entity posts=$postAttempts\" +\n+    s\" successful entity posts=$postSuccesses\" +\n+    s\" failed entity posts=$postFailures;\" +\n+    s\" events dropped=$eventsDropped;\" +\n+    s\" app start event received=$appStartEventProcessed;\" +\n+    s\" app end event received=$appEndEventProcessed;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    _timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    _applicationId = appId\n+    _attemptId = attemptId\n+  }\n+\n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private[yarn] def registerListener(): Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param event event to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(event: SparkListenerEvent): Boolean = {\n+    if (!stopped.get) {"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "let me do a full review of this again; it's always worth investing the time to re-review threading.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-10-20T19:24:21Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{TimeUnit, LinkedBlockingDeque}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{YarnExtensionService, YarnExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered\n+ * YARN Timeline Server.\n+ */\n+private[spark] class YarnHistoryService extends YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+  def enterState(state: Int): Int = {\n+    logDebug(s\"Entering state $state from $serviceState\")\n+    _serviceState.getAndSet(state)\n+  }\n+\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** YARN configuration from the spark context */\n+  private var config: YarnConfiguration = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var _timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Application name  from the spark start event */\n+  private var applicationName: String = _\n+\n+  /** Application ID*/\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName = Utils.getCurrentUserName()\n+\n+  /** Clock for recording time */\n+  private val clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /**\n+   * Start time of the application, as received in the end event.\n+   */\n+  private var endTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var _batchSize = DEFAULT_BATCH_SIZE\n+\n+  /** queue of entities to asynchronously post, plus the number of events in each entry */\n+  private var _entityQueue = new LinkedBlockingDeque[TimelineEntity]()\n+\n+  /** limit on the total number of events permitted */\n+  private var _postQueueLimit = DEFAULT_POST_QUEUE_LIMIT\n+\n+  /**\n+   * List of events which will be pulled into a timeline\n+   * entity when created\n+   */\n+  private var pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+\n+  private var applicationStartEvent: Option[SparkListenerApplicationStart] = None\n+  private var applicationEndEvent: Option[SparkListenerApplicationEnd] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val _eventsProcessed = new AtomicLong(0)\n+\n+  /** counter of events queued. */\n+  private val _eventsQueued = new AtomicLong(0)\n+\n+  private val _entityPostAttempts = new AtomicLong(0)\n+  private val _entityPostSuccesses = new AtomicLong(0)\n+  /** how many entity postings failed? */\n+  private val _entityPostFailures = new AtomicLong(0)\n+  private val _eventsDropped = new AtomicLong(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicLong(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped = new AtomicBoolean(true)\n+\n+  /**\n+   * Boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive = new AtomicBoolean(false)\n+\n+  /** How long to wait for shutdown before giving up */\n+  private var shutdownWaitTime = 0L\n+\n+  /**\n+   * What is the initial and incrementing interval for POST retries?\n+   */\n+  private var retryInterval = 0L\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: Option[String] = None\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var _timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private[yarn] def createTimelineClient(): TimelineClient = {\n+    require(_timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def timelineClient: TimelineClient = {\n+    require(_timelineClient.isDefined)\n+    _timelineClient.get\n+  }\n+\n+  /**\n+   * Get the total number of events dropped due to the queue of\n+   * outstanding posts being too long\n+   * @return counter of events processed\n+   */\n+\n+  def eventsDropped: Long = {\n+    _eventsDropped.get()\n+  }\n+\n+  /**\n+   * Get the total number of processed events, those handled in the back-end thread without\n+   * being rejected\n+   * @return counter of events processed\n+   */\n+  def eventsProcessed: Long = {\n+    _eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def eventsQueued: Long = {\n+    _eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    _entityQueue.size()\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def batchSize: Int = {\n+    _batchSize\n+  }\n+\n+  /**\n+   * Query the counter of attempts to post events\n+   * @return\n+   */\n+  def postAttempts: Long = _entityPostAttempts.get()\n+\n+  /**\n+   * Get the total number of failed post operations\n+   * @return counter of timeline post operations which failed\n+   */\n+  def postFailures: Long = {\n+    _entityPostFailures.get\n+  }\n+\n+  /**\n+   * Query the counter of post successes\n+   * @return the number of successful posts\n+   */\n+  def postSuccesses: Long = _entityPostSuccesses.get()\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId =  _applicationId\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] =  _attemptId\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    _timelineClient.foreach(_.stop())\n+    _timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain.\n+   *\n+   * A Timeline Domain is a uniquely identified 'namespace' for accessing parts of the timeline.\n+   * Security levels are are managed at the domain level, so one is created if the\n+   * spark acls are enabled. Full access is then granted to the current user,\n+   * all users in the configuration options `\"spark.modify.acls\"` and `\"spark.admin.acls\"`;\n+   * read access to those users and those listed in `\"spark.ui.view.acls\"`\n+   *\n+   * @return an optional domain string. If `None`, then no domain was created.\n+   */\n+  private def createTimelineDomain(): Option[String] = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsOn = sparkConf.getBoolean(\"spark.ui.acls.enable\",\n+        sparkConf.getBoolean(\"spark.acls.enable\", false))\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return None\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      logDebug(s\"Using predefined domain $predefDomain\")\n+      return predefDomain\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (Seq(current) ++ adminAcls ++ modifyAcls ++ viewAcls).mkString(\" \")\n+    val writers = (Seq(current) ++ adminAcls ++ modifyAcls).mkString(\" \")\n+    var domain = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domain with readers: $readers and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domain)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      timelineClient.putDomain(timelineDomain)\n+      Some(domain)\n+    } catch {\n+      case e: Exception => {\n+        logError(s\"cannot create the domain $domain\", e)\n+        // fallback to default\n+        None\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param binding binding to the spark application and YARN\n+   */\n+  override def start(binding: YarnExtensionServiceBinding): Unit = {\n+    val oldstate = enterState(StartedState)\n+    if (oldstate != CreatedState) {\n+      // state model violation\n+      _serviceState.set(oldstate)\n+      throw new IllegalArgumentException(s\"Cannot start the service from state $oldstate\")\n+    }\n+    val context = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    this.sparkContext = context\n+\n+    this.config = new YarnConfiguration(context.hadoopConfiguration)\n+\n+    val sparkConf = sparkContext.conf\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    _batchSize = sparkConf.getInt(BATCH_SIZE, _batchSize)\n+    _postQueueLimit = sparkConf.getInt(POST_QUEUE_LIMIT, _postQueueLimit)\n+    retryInterval = 1000 * sparkConf.getTimeAsSeconds(POST_RETRY_INTERVAL,\n+      DEFAULT_POST_RETRY_INTERVAL)\n+    shutdownWaitTime = 1000 * sparkConf.getTimeAsSeconds(SHUTDOWN_WAIT_TIME,\n+      DEFAULT_SHUTDOWN_WAIT_TIME)\n+\n+\n+    if (timelineServiceEnabled) {\n+      _timelineWebappAddress = getTimelineEndpoint(config)\n+\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at ${_timelineWebappAddress}\")\n+      _timelineClient = Some(createTimelineClient())\n+      domainId = createTimelineDomain()\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start()\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    if (registerListener()) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED` is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(config)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString(): String = {\n+    s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+    s\" state=$serviceState;\" +\n+    s\" endpoint=${_timelineWebappAddress};\" +\n+    s\" bonded to ATS=$bondedToATS;\" +\n+    s\" listening=$listening;\" +\n+    s\" batchSize=$batchSize;\" +\n+    s\" flush count=$getFlushCount;\" +\n+    s\" total number queued=$eventsQueued, processed=$eventsProcessed;\" +\n+    s\" attempted entity posts=$postAttempts\" +\n+    s\" successful entity posts=$postSuccesses\" +\n+    s\" failed entity posts=$postFailures;\" +\n+    s\" events dropped=$eventsDropped;\" +\n+    s\" app start event received=$appStartEventProcessed;\" +\n+    s\" app end event received=$appEndEventProcessed;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    _timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    _applicationId = appId\n+    _attemptId = attemptId\n+  }\n+\n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private[yarn] def registerListener(): Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param event event to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(event: SparkListenerEvent): Boolean = {\n+    if (!stopped.get) {"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "It feels like this variable is kinda redundant given `enterState` and friends.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-28T21:18:53Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{TimeUnit, LinkedBlockingDeque}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{YarnExtensionService, YarnExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered\n+ * YARN Timeline Server.\n+ */\n+private[spark] class YarnHistoryService extends YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+  def enterState(state: Int): Int = {\n+    logDebug(s\"Entering state $state from $serviceState\")\n+    _serviceState.getAndSet(state)\n+  }\n+\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** YARN configuration from the spark context */\n+  private var config: YarnConfiguration = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var _timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Application name  from the spark start event */\n+  private var applicationName: String = _\n+\n+  /** Application ID*/\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName = Utils.getCurrentUserName()\n+\n+  /** Clock for recording time */\n+  private val clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /**\n+   * Start time of the application, as received in the end event.\n+   */\n+  private var endTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var _batchSize = DEFAULT_BATCH_SIZE\n+\n+  /** queue of entities to asynchronously post, plus the number of events in each entry */\n+  private var _entityQueue = new LinkedBlockingDeque[TimelineEntity]()\n+\n+  /** limit on the total number of events permitted */\n+  private var _postQueueLimit = DEFAULT_POST_QUEUE_LIMIT\n+\n+  /**\n+   * List of events which will be pulled into a timeline\n+   * entity when created\n+   */\n+  private var pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+\n+  private var applicationStartEvent: Option[SparkListenerApplicationStart] = None\n+  private var applicationEndEvent: Option[SparkListenerApplicationEnd] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val _eventsProcessed = new AtomicLong(0)\n+\n+  /** counter of events queued. */\n+  private val _eventsQueued = new AtomicLong(0)\n+\n+  private val _entityPostAttempts = new AtomicLong(0)\n+  private val _entityPostSuccesses = new AtomicLong(0)\n+  /** how many entity postings failed? */\n+  private val _entityPostFailures = new AtomicLong(0)\n+  private val _eventsDropped = new AtomicLong(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicLong(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped = new AtomicBoolean(true)\n+\n+  /**\n+   * Boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive = new AtomicBoolean(false)\n+\n+  /** How long to wait for shutdown before giving up */\n+  private var shutdownWaitTime = 0L\n+\n+  /**\n+   * What is the initial and incrementing interval for POST retries?\n+   */\n+  private var retryInterval = 0L\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: Option[String] = None\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var _timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private[yarn] def createTimelineClient(): TimelineClient = {\n+    require(_timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def timelineClient: TimelineClient = {\n+    require(_timelineClient.isDefined)\n+    _timelineClient.get\n+  }\n+\n+  /**\n+   * Get the total number of events dropped due to the queue of\n+   * outstanding posts being too long\n+   * @return counter of events processed\n+   */\n+\n+  def eventsDropped: Long = {\n+    _eventsDropped.get()\n+  }\n+\n+  /**\n+   * Get the total number of processed events, those handled in the back-end thread without\n+   * being rejected\n+   * @return counter of events processed\n+   */\n+  def eventsProcessed: Long = {\n+    _eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def eventsQueued: Long = {\n+    _eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    _entityQueue.size()\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def batchSize: Int = {\n+    _batchSize\n+  }\n+\n+  /**\n+   * Query the counter of attempts to post events\n+   * @return\n+   */\n+  def postAttempts: Long = _entityPostAttempts.get()\n+\n+  /**\n+   * Get the total number of failed post operations\n+   * @return counter of timeline post operations which failed\n+   */\n+  def postFailures: Long = {\n+    _entityPostFailures.get\n+  }\n+\n+  /**\n+   * Query the counter of post successes\n+   * @return the number of successful posts\n+   */\n+  def postSuccesses: Long = _entityPostSuccesses.get()\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId =  _applicationId\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] =  _attemptId\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    _timelineClient.foreach(_.stop())\n+    _timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain.\n+   *\n+   * A Timeline Domain is a uniquely identified 'namespace' for accessing parts of the timeline.\n+   * Security levels are are managed at the domain level, so one is created if the\n+   * spark acls are enabled. Full access is then granted to the current user,\n+   * all users in the configuration options `\"spark.modify.acls\"` and `\"spark.admin.acls\"`;\n+   * read access to those users and those listed in `\"spark.ui.view.acls\"`\n+   *\n+   * @return an optional domain string. If `None`, then no domain was created.\n+   */\n+  private def createTimelineDomain(): Option[String] = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsOn = sparkConf.getBoolean(\"spark.ui.acls.enable\",\n+        sparkConf.getBoolean(\"spark.acls.enable\", false))\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return None\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      logDebug(s\"Using predefined domain $predefDomain\")\n+      return predefDomain\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (Seq(current) ++ adminAcls ++ modifyAcls ++ viewAcls).mkString(\" \")\n+    val writers = (Seq(current) ++ adminAcls ++ modifyAcls).mkString(\" \")\n+    var domain = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domain with readers: $readers and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domain)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      timelineClient.putDomain(timelineDomain)\n+      Some(domain)\n+    } catch {\n+      case e: Exception => {\n+        logError(s\"cannot create the domain $domain\", e)\n+        // fallback to default\n+        None\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param binding binding to the spark application and YARN\n+   */\n+  override def start(binding: YarnExtensionServiceBinding): Unit = {\n+    val oldstate = enterState(StartedState)\n+    if (oldstate != CreatedState) {\n+      // state model violation\n+      _serviceState.set(oldstate)\n+      throw new IllegalArgumentException(s\"Cannot start the service from state $oldstate\")\n+    }\n+    val context = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    this.sparkContext = context\n+\n+    this.config = new YarnConfiguration(context.hadoopConfiguration)\n+\n+    val sparkConf = sparkContext.conf\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    _batchSize = sparkConf.getInt(BATCH_SIZE, _batchSize)\n+    _postQueueLimit = sparkConf.getInt(POST_QUEUE_LIMIT, _postQueueLimit)\n+    retryInterval = 1000 * sparkConf.getTimeAsSeconds(POST_RETRY_INTERVAL,\n+      DEFAULT_POST_RETRY_INTERVAL)\n+    shutdownWaitTime = 1000 * sparkConf.getTimeAsSeconds(SHUTDOWN_WAIT_TIME,\n+      DEFAULT_SHUTDOWN_WAIT_TIME)\n+\n+\n+    if (timelineServiceEnabled) {\n+      _timelineWebappAddress = getTimelineEndpoint(config)\n+\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at ${_timelineWebappAddress}\")\n+      _timelineClient = Some(createTimelineClient())\n+      domainId = createTimelineDomain()\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start()\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    if (registerListener()) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED` is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(config)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString(): String = {\n+    s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+    s\" state=$serviceState;\" +\n+    s\" endpoint=${_timelineWebappAddress};\" +\n+    s\" bonded to ATS=$bondedToATS;\" +\n+    s\" listening=$listening;\" +\n+    s\" batchSize=$batchSize;\" +\n+    s\" flush count=$getFlushCount;\" +\n+    s\" total number queued=$eventsQueued, processed=$eventsProcessed;\" +\n+    s\" attempted entity posts=$postAttempts\" +\n+    s\" successful entity posts=$postSuccesses\" +\n+    s\" failed entity posts=$postFailures;\" +\n+    s\" events dropped=$eventsDropped;\" +\n+    s\" app start event received=$appStartEventProcessed;\" +\n+    s\" app end event received=$appEndEventProcessed;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    _timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    _applicationId = appId\n+    _attemptId = attemptId\n+  }\n+\n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private[yarn] def registerListener(): Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param event event to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(event: SparkListenerEvent): Boolean = {\n+    if (!stopped.get) {\n+      _eventsQueued.incrementAndGet\n+      logDebug(s\"Enqueue $event\")\n+      handleEvent(event)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $event\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override def stop(): Unit = {\n+    val oldState = enterState(StoppedState)\n+    if (oldState != StartedState) {\n+      // stopping from a different state\n+      logDebug(s\"Ignoring stop() request from state $oldState\")\n+      return\n+    }\n+    // if the queue is live\n+    if (!stopped.get) {"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Is the synchronization necessary here? The queue itself is thread-safe, and you're only adding to the queue from the listener bus thread from what I understand. So you won't have two writers trying to add events, which would actually cause a race in `handleEvent()`.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-28T21:22:39Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{TimeUnit, LinkedBlockingDeque}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{YarnExtensionService, YarnExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered\n+ * YARN Timeline Server.\n+ */\n+private[spark] class YarnHistoryService extends YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+  def enterState(state: Int): Int = {\n+    logDebug(s\"Entering state $state from $serviceState\")\n+    _serviceState.getAndSet(state)\n+  }\n+\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** YARN configuration from the spark context */\n+  private var config: YarnConfiguration = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var _timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Application name  from the spark start event */\n+  private var applicationName: String = _\n+\n+  /** Application ID*/\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName = Utils.getCurrentUserName()\n+\n+  /** Clock for recording time */\n+  private val clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /**\n+   * Start time of the application, as received in the end event.\n+   */\n+  private var endTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var _batchSize = DEFAULT_BATCH_SIZE\n+\n+  /** queue of entities to asynchronously post, plus the number of events in each entry */\n+  private var _entityQueue = new LinkedBlockingDeque[TimelineEntity]()\n+\n+  /** limit on the total number of events permitted */\n+  private var _postQueueLimit = DEFAULT_POST_QUEUE_LIMIT\n+\n+  /**\n+   * List of events which will be pulled into a timeline\n+   * entity when created\n+   */\n+  private var pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+\n+  private var applicationStartEvent: Option[SparkListenerApplicationStart] = None\n+  private var applicationEndEvent: Option[SparkListenerApplicationEnd] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val _eventsProcessed = new AtomicLong(0)\n+\n+  /** counter of events queued. */\n+  private val _eventsQueued = new AtomicLong(0)\n+\n+  private val _entityPostAttempts = new AtomicLong(0)\n+  private val _entityPostSuccesses = new AtomicLong(0)\n+  /** how many entity postings failed? */\n+  private val _entityPostFailures = new AtomicLong(0)\n+  private val _eventsDropped = new AtomicLong(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicLong(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped = new AtomicBoolean(true)\n+\n+  /**\n+   * Boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive = new AtomicBoolean(false)\n+\n+  /** How long to wait for shutdown before giving up */\n+  private var shutdownWaitTime = 0L\n+\n+  /**\n+   * What is the initial and incrementing interval for POST retries?\n+   */\n+  private var retryInterval = 0L\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: Option[String] = None\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var _timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private[yarn] def createTimelineClient(): TimelineClient = {\n+    require(_timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def timelineClient: TimelineClient = {\n+    require(_timelineClient.isDefined)\n+    _timelineClient.get\n+  }\n+\n+  /**\n+   * Get the total number of events dropped due to the queue of\n+   * outstanding posts being too long\n+   * @return counter of events processed\n+   */\n+\n+  def eventsDropped: Long = {\n+    _eventsDropped.get()\n+  }\n+\n+  /**\n+   * Get the total number of processed events, those handled in the back-end thread without\n+   * being rejected\n+   * @return counter of events processed\n+   */\n+  def eventsProcessed: Long = {\n+    _eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def eventsQueued: Long = {\n+    _eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    _entityQueue.size()\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def batchSize: Int = {\n+    _batchSize\n+  }\n+\n+  /**\n+   * Query the counter of attempts to post events\n+   * @return\n+   */\n+  def postAttempts: Long = _entityPostAttempts.get()\n+\n+  /**\n+   * Get the total number of failed post operations\n+   * @return counter of timeline post operations which failed\n+   */\n+  def postFailures: Long = {\n+    _entityPostFailures.get\n+  }\n+\n+  /**\n+   * Query the counter of post successes\n+   * @return the number of successful posts\n+   */\n+  def postSuccesses: Long = _entityPostSuccesses.get()\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId =  _applicationId\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] =  _attemptId\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    _timelineClient.foreach(_.stop())\n+    _timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain.\n+   *\n+   * A Timeline Domain is a uniquely identified 'namespace' for accessing parts of the timeline.\n+   * Security levels are are managed at the domain level, so one is created if the\n+   * spark acls are enabled. Full access is then granted to the current user,\n+   * all users in the configuration options `\"spark.modify.acls\"` and `\"spark.admin.acls\"`;\n+   * read access to those users and those listed in `\"spark.ui.view.acls\"`\n+   *\n+   * @return an optional domain string. If `None`, then no domain was created.\n+   */\n+  private def createTimelineDomain(): Option[String] = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsOn = sparkConf.getBoolean(\"spark.ui.acls.enable\",\n+        sparkConf.getBoolean(\"spark.acls.enable\", false))\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return None\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      logDebug(s\"Using predefined domain $predefDomain\")\n+      return predefDomain\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (Seq(current) ++ adminAcls ++ modifyAcls ++ viewAcls).mkString(\" \")\n+    val writers = (Seq(current) ++ adminAcls ++ modifyAcls).mkString(\" \")\n+    var domain = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domain with readers: $readers and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domain)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      timelineClient.putDomain(timelineDomain)\n+      Some(domain)\n+    } catch {\n+      case e: Exception => {\n+        logError(s\"cannot create the domain $domain\", e)\n+        // fallback to default\n+        None\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param binding binding to the spark application and YARN\n+   */\n+  override def start(binding: YarnExtensionServiceBinding): Unit = {\n+    val oldstate = enterState(StartedState)\n+    if (oldstate != CreatedState) {\n+      // state model violation\n+      _serviceState.set(oldstate)\n+      throw new IllegalArgumentException(s\"Cannot start the service from state $oldstate\")\n+    }\n+    val context = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    this.sparkContext = context\n+\n+    this.config = new YarnConfiguration(context.hadoopConfiguration)\n+\n+    val sparkConf = sparkContext.conf\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    _batchSize = sparkConf.getInt(BATCH_SIZE, _batchSize)\n+    _postQueueLimit = sparkConf.getInt(POST_QUEUE_LIMIT, _postQueueLimit)\n+    retryInterval = 1000 * sparkConf.getTimeAsSeconds(POST_RETRY_INTERVAL,\n+      DEFAULT_POST_RETRY_INTERVAL)\n+    shutdownWaitTime = 1000 * sparkConf.getTimeAsSeconds(SHUTDOWN_WAIT_TIME,\n+      DEFAULT_SHUTDOWN_WAIT_TIME)\n+\n+\n+    if (timelineServiceEnabled) {\n+      _timelineWebappAddress = getTimelineEndpoint(config)\n+\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at ${_timelineWebappAddress}\")\n+      _timelineClient = Some(createTimelineClient())\n+      domainId = createTimelineDomain()\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start()\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    if (registerListener()) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED` is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(config)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString(): String = {\n+    s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+    s\" state=$serviceState;\" +\n+    s\" endpoint=${_timelineWebappAddress};\" +\n+    s\" bonded to ATS=$bondedToATS;\" +\n+    s\" listening=$listening;\" +\n+    s\" batchSize=$batchSize;\" +\n+    s\" flush count=$getFlushCount;\" +\n+    s\" total number queued=$eventsQueued, processed=$eventsProcessed;\" +\n+    s\" attempted entity posts=$postAttempts\" +\n+    s\" successful entity posts=$postSuccesses\" +\n+    s\" failed entity posts=$postFailures;\" +\n+    s\" events dropped=$eventsDropped;\" +\n+    s\" app start event received=$appStartEventProcessed;\" +\n+    s\" app end event received=$appEndEventProcessed;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    _timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    _applicationId = appId\n+    _attemptId = attemptId\n+  }\n+\n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private[yarn] def registerListener(): Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param event event to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(event: SparkListenerEvent): Boolean = {\n+    if (!stopped.get) {\n+      _eventsQueued.incrementAndGet\n+      logDebug(s\"Enqueue $event\")\n+      handleEvent(event)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $event\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override def stop(): Unit = {\n+    val oldState = enterState(StoppedState)\n+    if (oldState != StartedState) {\n+      // stopping from a different state\n+      logDebug(s\"Ignoring stop() request from state $oldState\")\n+      return\n+    }\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (appStartEventProcessed.get && !appEndEventProcessed.get) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        enqueue(SparkListenerApplicationEnd(now()))\n+      }\n+\n+      // flush out the events\n+      asyncFlush()\n+\n+      // stop operation\n+      postThreadActive.synchronized {\n+        // now await that marker flag\n+        if (postThreadActive.get) {\n+          logDebug(s\"Stopping posting thread and waiting $shutdownWaitTime mS\")\n+          stopped.set(true)\n+          eventHandlingThread.foreach(_.interrupt())\n+          postThreadActive.wait(shutdownWaitTime)\n+        } else {\n+          stopTimelineClient()\n+          logInfo(s\"Stopped: $this\")\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Can an event be added?\n+   * Policy is only if the number of queued entities is below the limit, or the\n+   * event marks the end of the application.\n+   * @param isLifecycleEvent is this operation triggered by an application start/end?\n+   * @return true if the event can be added to the queue\n+   */\n+  private def canAddEvent(isLifecycleEvent: Boolean): Boolean = {\n+    _entityQueue.synchronized {"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Why synchronized? It's a `BlockingQueue`.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-28T21:29:16Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{TimeUnit, LinkedBlockingDeque}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{YarnExtensionService, YarnExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered\n+ * YARN Timeline Server.\n+ */\n+private[spark] class YarnHistoryService extends YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+  def enterState(state: Int): Int = {\n+    logDebug(s\"Entering state $state from $serviceState\")\n+    _serviceState.getAndSet(state)\n+  }\n+\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** YARN configuration from the spark context */\n+  private var config: YarnConfiguration = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var _timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Application name  from the spark start event */\n+  private var applicationName: String = _\n+\n+  /** Application ID*/\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName = Utils.getCurrentUserName()\n+\n+  /** Clock for recording time */\n+  private val clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /**\n+   * Start time of the application, as received in the end event.\n+   */\n+  private var endTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var _batchSize = DEFAULT_BATCH_SIZE\n+\n+  /** queue of entities to asynchronously post, plus the number of events in each entry */\n+  private var _entityQueue = new LinkedBlockingDeque[TimelineEntity]()\n+\n+  /** limit on the total number of events permitted */\n+  private var _postQueueLimit = DEFAULT_POST_QUEUE_LIMIT\n+\n+  /**\n+   * List of events which will be pulled into a timeline\n+   * entity when created\n+   */\n+  private var pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+\n+  private var applicationStartEvent: Option[SparkListenerApplicationStart] = None\n+  private var applicationEndEvent: Option[SparkListenerApplicationEnd] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val _eventsProcessed = new AtomicLong(0)\n+\n+  /** counter of events queued. */\n+  private val _eventsQueued = new AtomicLong(0)\n+\n+  private val _entityPostAttempts = new AtomicLong(0)\n+  private val _entityPostSuccesses = new AtomicLong(0)\n+  /** how many entity postings failed? */\n+  private val _entityPostFailures = new AtomicLong(0)\n+  private val _eventsDropped = new AtomicLong(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicLong(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped = new AtomicBoolean(true)\n+\n+  /**\n+   * Boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive = new AtomicBoolean(false)\n+\n+  /** How long to wait for shutdown before giving up */\n+  private var shutdownWaitTime = 0L\n+\n+  /**\n+   * What is the initial and incrementing interval for POST retries?\n+   */\n+  private var retryInterval = 0L\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: Option[String] = None\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var _timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private[yarn] def createTimelineClient(): TimelineClient = {\n+    require(_timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def timelineClient: TimelineClient = {\n+    require(_timelineClient.isDefined)\n+    _timelineClient.get\n+  }\n+\n+  /**\n+   * Get the total number of events dropped due to the queue of\n+   * outstanding posts being too long\n+   * @return counter of events processed\n+   */\n+\n+  def eventsDropped: Long = {\n+    _eventsDropped.get()\n+  }\n+\n+  /**\n+   * Get the total number of processed events, those handled in the back-end thread without\n+   * being rejected\n+   * @return counter of events processed\n+   */\n+  def eventsProcessed: Long = {\n+    _eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def eventsQueued: Long = {\n+    _eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    _entityQueue.size()\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def batchSize: Int = {\n+    _batchSize\n+  }\n+\n+  /**\n+   * Query the counter of attempts to post events\n+   * @return\n+   */\n+  def postAttempts: Long = _entityPostAttempts.get()\n+\n+  /**\n+   * Get the total number of failed post operations\n+   * @return counter of timeline post operations which failed\n+   */\n+  def postFailures: Long = {\n+    _entityPostFailures.get\n+  }\n+\n+  /**\n+   * Query the counter of post successes\n+   * @return the number of successful posts\n+   */\n+  def postSuccesses: Long = _entityPostSuccesses.get()\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId =  _applicationId\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] =  _attemptId\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    _timelineClient.foreach(_.stop())\n+    _timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain.\n+   *\n+   * A Timeline Domain is a uniquely identified 'namespace' for accessing parts of the timeline.\n+   * Security levels are are managed at the domain level, so one is created if the\n+   * spark acls are enabled. Full access is then granted to the current user,\n+   * all users in the configuration options `\"spark.modify.acls\"` and `\"spark.admin.acls\"`;\n+   * read access to those users and those listed in `\"spark.ui.view.acls\"`\n+   *\n+   * @return an optional domain string. If `None`, then no domain was created.\n+   */\n+  private def createTimelineDomain(): Option[String] = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsOn = sparkConf.getBoolean(\"spark.ui.acls.enable\",\n+        sparkConf.getBoolean(\"spark.acls.enable\", false))\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return None\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      logDebug(s\"Using predefined domain $predefDomain\")\n+      return predefDomain\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (Seq(current) ++ adminAcls ++ modifyAcls ++ viewAcls).mkString(\" \")\n+    val writers = (Seq(current) ++ adminAcls ++ modifyAcls).mkString(\" \")\n+    var domain = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domain with readers: $readers and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domain)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      timelineClient.putDomain(timelineDomain)\n+      Some(domain)\n+    } catch {\n+      case e: Exception => {\n+        logError(s\"cannot create the domain $domain\", e)\n+        // fallback to default\n+        None\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param binding binding to the spark application and YARN\n+   */\n+  override def start(binding: YarnExtensionServiceBinding): Unit = {\n+    val oldstate = enterState(StartedState)\n+    if (oldstate != CreatedState) {\n+      // state model violation\n+      _serviceState.set(oldstate)\n+      throw new IllegalArgumentException(s\"Cannot start the service from state $oldstate\")\n+    }\n+    val context = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    this.sparkContext = context\n+\n+    this.config = new YarnConfiguration(context.hadoopConfiguration)\n+\n+    val sparkConf = sparkContext.conf\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    _batchSize = sparkConf.getInt(BATCH_SIZE, _batchSize)\n+    _postQueueLimit = sparkConf.getInt(POST_QUEUE_LIMIT, _postQueueLimit)\n+    retryInterval = 1000 * sparkConf.getTimeAsSeconds(POST_RETRY_INTERVAL,\n+      DEFAULT_POST_RETRY_INTERVAL)\n+    shutdownWaitTime = 1000 * sparkConf.getTimeAsSeconds(SHUTDOWN_WAIT_TIME,\n+      DEFAULT_SHUTDOWN_WAIT_TIME)\n+\n+\n+    if (timelineServiceEnabled) {\n+      _timelineWebappAddress = getTimelineEndpoint(config)\n+\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at ${_timelineWebappAddress}\")\n+      _timelineClient = Some(createTimelineClient())\n+      domainId = createTimelineDomain()\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start()\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    if (registerListener()) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED` is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(config)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString(): String = {\n+    s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+    s\" state=$serviceState;\" +\n+    s\" endpoint=${_timelineWebappAddress};\" +\n+    s\" bonded to ATS=$bondedToATS;\" +\n+    s\" listening=$listening;\" +\n+    s\" batchSize=$batchSize;\" +\n+    s\" flush count=$getFlushCount;\" +\n+    s\" total number queued=$eventsQueued, processed=$eventsProcessed;\" +\n+    s\" attempted entity posts=$postAttempts\" +\n+    s\" successful entity posts=$postSuccesses\" +\n+    s\" failed entity posts=$postFailures;\" +\n+    s\" events dropped=$eventsDropped;\" +\n+    s\" app start event received=$appStartEventProcessed;\" +\n+    s\" app end event received=$appEndEventProcessed;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    _timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    _applicationId = appId\n+    _attemptId = attemptId\n+  }\n+\n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private[yarn] def registerListener(): Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param event event to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(event: SparkListenerEvent): Boolean = {\n+    if (!stopped.get) {\n+      _eventsQueued.incrementAndGet\n+      logDebug(s\"Enqueue $event\")\n+      handleEvent(event)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $event\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override def stop(): Unit = {\n+    val oldState = enterState(StoppedState)\n+    if (oldState != StartedState) {\n+      // stopping from a different state\n+      logDebug(s\"Ignoring stop() request from state $oldState\")\n+      return\n+    }\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (appStartEventProcessed.get && !appEndEventProcessed.get) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        enqueue(SparkListenerApplicationEnd(now()))\n+      }\n+\n+      // flush out the events\n+      asyncFlush()\n+\n+      // stop operation\n+      postThreadActive.synchronized {\n+        // now await that marker flag\n+        if (postThreadActive.get) {\n+          logDebug(s\"Stopping posting thread and waiting $shutdownWaitTime mS\")\n+          stopped.set(true)\n+          eventHandlingThread.foreach(_.interrupt())\n+          postThreadActive.wait(shutdownWaitTime)\n+        } else {\n+          stopTimelineClient()\n+          logInfo(s\"Stopped: $this\")\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Can an event be added?\n+   * Policy is only if the number of queued entities is below the limit, or the\n+   * event marks the end of the application.\n+   * @param isLifecycleEvent is this operation triggered by an application start/end?\n+   * @return true if the event can be added to the queue\n+   */\n+  private def canAddEvent(isLifecycleEvent: Boolean): Boolean = {\n+    _entityQueue.synchronized {\n+      _entityQueue.size() < _postQueueLimit  || isLifecycleEvent\n+    }\n+  }\n+\n+  /**\n+   * Add another event to the pending event list.\n+   * Returns the size of the event list after the event was added\n+   * (thread safe).\n+   * @param event event to add\n+   * @return the event list size\n+   */\n+  private def addPendingEvent(event: TimelineEvent): Int = {\n+    pendingEvents.synchronized {\n+      pendingEvents :+= event\n+      pendingEvents.size\n+    }\n+  }\n+\n+  /**\n+   * Publish next set of pending events.\n+   *\n+   * Builds the next event to push on the entity Queue; resets\n+   * the current [[pendingEvents]] list and then notifies\n+   * any listener of the [[_entityQueue]] that there is new data.\n+   */\n+  private def publishPendingEvents(): Boolean = {\n+    // verify that there are events to publish\n+    val size = pendingEvents.synchronized {\n+      pendingEvents.size\n+    }\n+    if (size > 0 && applicationStartEvent.isDefined) {\n+      flushCount.incrementAndGet()\n+      val timelineEntity = createTimelineEntity(\n+        applicationId,\n+        attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        applicationName,\n+        userName,\n+        startTime,\n+        endTime,\n+        now())\n+\n+      // copy in pending events and then clear the list\n+      pendingEvents.synchronized {\n+        pendingEvents.foreach(timelineEntity.addEvent)\n+        pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+      }\n+      // queue the entity for posting\n+      preflightCheck(timelineEntity)\n+      _entityQueue.synchronized {"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "The first item in the return value of `postOneEntity` is never used; why return it?\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-28T21:32:46Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{TimeUnit, LinkedBlockingDeque}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{YarnExtensionService, YarnExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered\n+ * YARN Timeline Server.\n+ */\n+private[spark] class YarnHistoryService extends YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+  def enterState(state: Int): Int = {\n+    logDebug(s\"Entering state $state from $serviceState\")\n+    _serviceState.getAndSet(state)\n+  }\n+\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** YARN configuration from the spark context */\n+  private var config: YarnConfiguration = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var _timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Application name  from the spark start event */\n+  private var applicationName: String = _\n+\n+  /** Application ID*/\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName = Utils.getCurrentUserName()\n+\n+  /** Clock for recording time */\n+  private val clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /**\n+   * Start time of the application, as received in the end event.\n+   */\n+  private var endTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var _batchSize = DEFAULT_BATCH_SIZE\n+\n+  /** queue of entities to asynchronously post, plus the number of events in each entry */\n+  private var _entityQueue = new LinkedBlockingDeque[TimelineEntity]()\n+\n+  /** limit on the total number of events permitted */\n+  private var _postQueueLimit = DEFAULT_POST_QUEUE_LIMIT\n+\n+  /**\n+   * List of events which will be pulled into a timeline\n+   * entity when created\n+   */\n+  private var pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+\n+  private var applicationStartEvent: Option[SparkListenerApplicationStart] = None\n+  private var applicationEndEvent: Option[SparkListenerApplicationEnd] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val _eventsProcessed = new AtomicLong(0)\n+\n+  /** counter of events queued. */\n+  private val _eventsQueued = new AtomicLong(0)\n+\n+  private val _entityPostAttempts = new AtomicLong(0)\n+  private val _entityPostSuccesses = new AtomicLong(0)\n+  /** how many entity postings failed? */\n+  private val _entityPostFailures = new AtomicLong(0)\n+  private val _eventsDropped = new AtomicLong(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicLong(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped = new AtomicBoolean(true)\n+\n+  /**\n+   * Boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive = new AtomicBoolean(false)\n+\n+  /** How long to wait for shutdown before giving up */\n+  private var shutdownWaitTime = 0L\n+\n+  /**\n+   * What is the initial and incrementing interval for POST retries?\n+   */\n+  private var retryInterval = 0L\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: Option[String] = None\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var _timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private[yarn] def createTimelineClient(): TimelineClient = {\n+    require(_timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def timelineClient: TimelineClient = {\n+    require(_timelineClient.isDefined)\n+    _timelineClient.get\n+  }\n+\n+  /**\n+   * Get the total number of events dropped due to the queue of\n+   * outstanding posts being too long\n+   * @return counter of events processed\n+   */\n+\n+  def eventsDropped: Long = {\n+    _eventsDropped.get()\n+  }\n+\n+  /**\n+   * Get the total number of processed events, those handled in the back-end thread without\n+   * being rejected\n+   * @return counter of events processed\n+   */\n+  def eventsProcessed: Long = {\n+    _eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def eventsQueued: Long = {\n+    _eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    _entityQueue.size()\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def batchSize: Int = {\n+    _batchSize\n+  }\n+\n+  /**\n+   * Query the counter of attempts to post events\n+   * @return\n+   */\n+  def postAttempts: Long = _entityPostAttempts.get()\n+\n+  /**\n+   * Get the total number of failed post operations\n+   * @return counter of timeline post operations which failed\n+   */\n+  def postFailures: Long = {\n+    _entityPostFailures.get\n+  }\n+\n+  /**\n+   * Query the counter of post successes\n+   * @return the number of successful posts\n+   */\n+  def postSuccesses: Long = _entityPostSuccesses.get()\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId =  _applicationId\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] =  _attemptId\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    _timelineClient.foreach(_.stop())\n+    _timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain.\n+   *\n+   * A Timeline Domain is a uniquely identified 'namespace' for accessing parts of the timeline.\n+   * Security levels are are managed at the domain level, so one is created if the\n+   * spark acls are enabled. Full access is then granted to the current user,\n+   * all users in the configuration options `\"spark.modify.acls\"` and `\"spark.admin.acls\"`;\n+   * read access to those users and those listed in `\"spark.ui.view.acls\"`\n+   *\n+   * @return an optional domain string. If `None`, then no domain was created.\n+   */\n+  private def createTimelineDomain(): Option[String] = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsOn = sparkConf.getBoolean(\"spark.ui.acls.enable\",\n+        sparkConf.getBoolean(\"spark.acls.enable\", false))\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return None\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      logDebug(s\"Using predefined domain $predefDomain\")\n+      return predefDomain\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (Seq(current) ++ adminAcls ++ modifyAcls ++ viewAcls).mkString(\" \")\n+    val writers = (Seq(current) ++ adminAcls ++ modifyAcls).mkString(\" \")\n+    var domain = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domain with readers: $readers and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domain)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      timelineClient.putDomain(timelineDomain)\n+      Some(domain)\n+    } catch {\n+      case e: Exception => {\n+        logError(s\"cannot create the domain $domain\", e)\n+        // fallback to default\n+        None\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param binding binding to the spark application and YARN\n+   */\n+  override def start(binding: YarnExtensionServiceBinding): Unit = {\n+    val oldstate = enterState(StartedState)\n+    if (oldstate != CreatedState) {\n+      // state model violation\n+      _serviceState.set(oldstate)\n+      throw new IllegalArgumentException(s\"Cannot start the service from state $oldstate\")\n+    }\n+    val context = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    this.sparkContext = context\n+\n+    this.config = new YarnConfiguration(context.hadoopConfiguration)\n+\n+    val sparkConf = sparkContext.conf\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    _batchSize = sparkConf.getInt(BATCH_SIZE, _batchSize)\n+    _postQueueLimit = sparkConf.getInt(POST_QUEUE_LIMIT, _postQueueLimit)\n+    retryInterval = 1000 * sparkConf.getTimeAsSeconds(POST_RETRY_INTERVAL,\n+      DEFAULT_POST_RETRY_INTERVAL)\n+    shutdownWaitTime = 1000 * sparkConf.getTimeAsSeconds(SHUTDOWN_WAIT_TIME,\n+      DEFAULT_SHUTDOWN_WAIT_TIME)\n+\n+\n+    if (timelineServiceEnabled) {\n+      _timelineWebappAddress = getTimelineEndpoint(config)\n+\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at ${_timelineWebappAddress}\")\n+      _timelineClient = Some(createTimelineClient())\n+      domainId = createTimelineDomain()\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start()\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    if (registerListener()) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED` is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(config)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString(): String = {\n+    s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+    s\" state=$serviceState;\" +\n+    s\" endpoint=${_timelineWebappAddress};\" +\n+    s\" bonded to ATS=$bondedToATS;\" +\n+    s\" listening=$listening;\" +\n+    s\" batchSize=$batchSize;\" +\n+    s\" flush count=$getFlushCount;\" +\n+    s\" total number queued=$eventsQueued, processed=$eventsProcessed;\" +\n+    s\" attempted entity posts=$postAttempts\" +\n+    s\" successful entity posts=$postSuccesses\" +\n+    s\" failed entity posts=$postFailures;\" +\n+    s\" events dropped=$eventsDropped;\" +\n+    s\" app start event received=$appStartEventProcessed;\" +\n+    s\" app end event received=$appEndEventProcessed;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    _timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    _applicationId = appId\n+    _attemptId = attemptId\n+  }\n+\n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private[yarn] def registerListener(): Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param event event to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(event: SparkListenerEvent): Boolean = {\n+    if (!stopped.get) {\n+      _eventsQueued.incrementAndGet\n+      logDebug(s\"Enqueue $event\")\n+      handleEvent(event)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $event\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override def stop(): Unit = {\n+    val oldState = enterState(StoppedState)\n+    if (oldState != StartedState) {\n+      // stopping from a different state\n+      logDebug(s\"Ignoring stop() request from state $oldState\")\n+      return\n+    }\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (appStartEventProcessed.get && !appEndEventProcessed.get) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        enqueue(SparkListenerApplicationEnd(now()))\n+      }\n+\n+      // flush out the events\n+      asyncFlush()\n+\n+      // stop operation\n+      postThreadActive.synchronized {\n+        // now await that marker flag\n+        if (postThreadActive.get) {\n+          logDebug(s\"Stopping posting thread and waiting $shutdownWaitTime mS\")\n+          stopped.set(true)\n+          eventHandlingThread.foreach(_.interrupt())\n+          postThreadActive.wait(shutdownWaitTime)\n+        } else {\n+          stopTimelineClient()\n+          logInfo(s\"Stopped: $this\")\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Can an event be added?\n+   * Policy is only if the number of queued entities is below the limit, or the\n+   * event marks the end of the application.\n+   * @param isLifecycleEvent is this operation triggered by an application start/end?\n+   * @return true if the event can be added to the queue\n+   */\n+  private def canAddEvent(isLifecycleEvent: Boolean): Boolean = {\n+    _entityQueue.synchronized {\n+      _entityQueue.size() < _postQueueLimit  || isLifecycleEvent\n+    }\n+  }\n+\n+  /**\n+   * Add another event to the pending event list.\n+   * Returns the size of the event list after the event was added\n+   * (thread safe).\n+   * @param event event to add\n+   * @return the event list size\n+   */\n+  private def addPendingEvent(event: TimelineEvent): Int = {\n+    pendingEvents.synchronized {\n+      pendingEvents :+= event\n+      pendingEvents.size\n+    }\n+  }\n+\n+  /**\n+   * Publish next set of pending events.\n+   *\n+   * Builds the next event to push on the entity Queue; resets\n+   * the current [[pendingEvents]] list and then notifies\n+   * any listener of the [[_entityQueue]] that there is new data.\n+   */\n+  private def publishPendingEvents(): Boolean = {\n+    // verify that there are events to publish\n+    val size = pendingEvents.synchronized {\n+      pendingEvents.size\n+    }\n+    if (size > 0 && applicationStartEvent.isDefined) {\n+      flushCount.incrementAndGet()\n+      val timelineEntity = createTimelineEntity(\n+        applicationId,\n+        attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        applicationName,\n+        userName,\n+        startTime,\n+        endTime,\n+        now())\n+\n+      // copy in pending events and then clear the list\n+      pendingEvents.synchronized {\n+        pendingEvents.foreach(timelineEntity.addEvent)\n+        pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+      }\n+      // queue the entity for posting\n+      preflightCheck(timelineEntity)\n+      _entityQueue.synchronized {\n+        _entityQueue.push(timelineEntity)\n+      }\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   *\n+   * @param entity timeline entity to review.\n+   */\n+  private def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * Post a single entity.\n+   *\n+   * Any network/connectivity errors will be caught and logged, and returned as the\n+   * exception field in the returned tuple.\n+   *\n+   * Any posting which generates a response will result in the timeline response being\n+   * returned. This response *may* contain errors; these are almost invariably going\n+   * to re-occur when resubmitted.\n+   *\n+   * @param entity entity to post\n+   * @return tuple with exactly one of the response or the exception set.\n+   */\n+  private def postOneEntity(entity: TimelineEntity):\n+      (Option[TimelinePutResponse], Option[Exception]) = {\n+    domainId.foreach {\n+      entity.setDomainId\n+    }\n+    val entityDescription = describeEntity(entity)\n+    logDebug(s\"About to POST $entityDescription\")\n+    _entityPostAttempts.incrementAndGet()\n+    try {\n+      val response = timelineClient.putEntities(entity)\n+      val errors = response.getErrors\n+      if (errors.isEmpty) {\n+        logDebug(s\"entity successfully posted\")\n+        _entityPostSuccesses.incrementAndGet()\n+      } else {\n+        // something went wrong.\n+        errors.asScala.foreach { err =>\n+          _entityPostFailures.incrementAndGet()\n+          logError(s\"Failed to post $entityDescription\\n:${describeError(err)}\")\n+        }\n+      }\n+      // whatever the outcome, this request is not re-issued\n+      (Some(response), None)\n+    } catch {\n+      case e: ConnectException =>\n+        _entityPostFailures.incrementAndGet\n+        logWarning(s\"Connection exception submitting $entityDescription\", e)\n+        (None, Some(e))\n+\n+      case e: RuntimeException =>\n+        // this is probably a retry timeout event; some Hadoop versions don't\n+        // rethrow the exception causing the problem, instead raising an RTE\n+        _entityPostFailures.incrementAndGet\n+        logWarning(s\"Runtime exception submitting $entityDescription\", e)\n+        // same policy as before: retry on these\n+        (None, Some(e))\n+\n+      case e: Exception =>\n+        // something else has gone wrong.\n+        _entityPostFailures.incrementAndGet\n+        logWarning(s\"Could not handle history entity: $entityDescription\", e)\n+        (None, Some(e))\n+    }\n+  }\n+\n+  /**\n+   * Spin and post until stopped.\n+   *\n+   * Algorithm.\n+   *\n+   * 1. The thread waits for events in the _entityQueue until stopped or interrupted\n+   * 1. Failures result in the entity being queued for resending, after a delay which grows\n+   * linearly on every retry.\n+   * 1. Successful posts reset the retry delay.\n+   * 1. If the process is interrupted, the loop continues with the `stopFlag` flag being checked.\n+   *\n+   * To stop this process then, first set the `stopFlag` flag, then interrupt the thread.\n+   *\n+   * @param stopFlag a flag to set to stop the loop.\n+   * @param retryInterval delay in milliseconds for the first retry delay; the delay increases\n+   *        by this value on every future failure. If zero, there is no delay, ever.\n+   */\n+  private def postEntities(stopFlag: AtomicBoolean, retryInterval: Long): Unit = {\n+    var lastAttemptFailed = false\n+    var currentRetryDelay = retryInterval\n+    while (!stopped.get) {\n+      try {\n+        val head = _entityQueue.take()\n+        val (_, ex) = postOneEntity(head)"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: semi-colon\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-28T21:36:12Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{TimeUnit, LinkedBlockingDeque}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{YarnExtensionService, YarnExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered\n+ * YARN Timeline Server.\n+ */\n+private[spark] class YarnHistoryService extends YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+  def enterState(state: Int): Int = {\n+    logDebug(s\"Entering state $state from $serviceState\")\n+    _serviceState.getAndSet(state)\n+  }\n+\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** YARN configuration from the spark context */\n+  private var config: YarnConfiguration = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var _timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Application name  from the spark start event */\n+  private var applicationName: String = _\n+\n+  /** Application ID*/\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName = Utils.getCurrentUserName()\n+\n+  /** Clock for recording time */\n+  private val clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /**\n+   * Start time of the application, as received in the end event.\n+   */\n+  private var endTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var _batchSize = DEFAULT_BATCH_SIZE\n+\n+  /** queue of entities to asynchronously post, plus the number of events in each entry */\n+  private var _entityQueue = new LinkedBlockingDeque[TimelineEntity]()\n+\n+  /** limit on the total number of events permitted */\n+  private var _postQueueLimit = DEFAULT_POST_QUEUE_LIMIT\n+\n+  /**\n+   * List of events which will be pulled into a timeline\n+   * entity when created\n+   */\n+  private var pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+\n+  private var applicationStartEvent: Option[SparkListenerApplicationStart] = None\n+  private var applicationEndEvent: Option[SparkListenerApplicationEnd] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val _eventsProcessed = new AtomicLong(0)\n+\n+  /** counter of events queued. */\n+  private val _eventsQueued = new AtomicLong(0)\n+\n+  private val _entityPostAttempts = new AtomicLong(0)\n+  private val _entityPostSuccesses = new AtomicLong(0)\n+  /** how many entity postings failed? */\n+  private val _entityPostFailures = new AtomicLong(0)\n+  private val _eventsDropped = new AtomicLong(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicLong(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped = new AtomicBoolean(true)\n+\n+  /**\n+   * Boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive = new AtomicBoolean(false)\n+\n+  /** How long to wait for shutdown before giving up */\n+  private var shutdownWaitTime = 0L\n+\n+  /**\n+   * What is the initial and incrementing interval for POST retries?\n+   */\n+  private var retryInterval = 0L\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: Option[String] = None\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var _timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private[yarn] def createTimelineClient(): TimelineClient = {\n+    require(_timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def timelineClient: TimelineClient = {\n+    require(_timelineClient.isDefined)\n+    _timelineClient.get\n+  }\n+\n+  /**\n+   * Get the total number of events dropped due to the queue of\n+   * outstanding posts being too long\n+   * @return counter of events processed\n+   */\n+\n+  def eventsDropped: Long = {\n+    _eventsDropped.get()\n+  }\n+\n+  /**\n+   * Get the total number of processed events, those handled in the back-end thread without\n+   * being rejected\n+   * @return counter of events processed\n+   */\n+  def eventsProcessed: Long = {\n+    _eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def eventsQueued: Long = {\n+    _eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    _entityQueue.size()\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def batchSize: Int = {\n+    _batchSize\n+  }\n+\n+  /**\n+   * Query the counter of attempts to post events\n+   * @return\n+   */\n+  def postAttempts: Long = _entityPostAttempts.get()\n+\n+  /**\n+   * Get the total number of failed post operations\n+   * @return counter of timeline post operations which failed\n+   */\n+  def postFailures: Long = {\n+    _entityPostFailures.get\n+  }\n+\n+  /**\n+   * Query the counter of post successes\n+   * @return the number of successful posts\n+   */\n+  def postSuccesses: Long = _entityPostSuccesses.get()\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId =  _applicationId\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] =  _attemptId\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    _timelineClient.foreach(_.stop())\n+    _timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain.\n+   *\n+   * A Timeline Domain is a uniquely identified 'namespace' for accessing parts of the timeline.\n+   * Security levels are are managed at the domain level, so one is created if the\n+   * spark acls are enabled. Full access is then granted to the current user,\n+   * all users in the configuration options `\"spark.modify.acls\"` and `\"spark.admin.acls\"`;\n+   * read access to those users and those listed in `\"spark.ui.view.acls\"`\n+   *\n+   * @return an optional domain string. If `None`, then no domain was created.\n+   */\n+  private def createTimelineDomain(): Option[String] = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsOn = sparkConf.getBoolean(\"spark.ui.acls.enable\",\n+        sparkConf.getBoolean(\"spark.acls.enable\", false))\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return None\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      logDebug(s\"Using predefined domain $predefDomain\")\n+      return predefDomain\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (Seq(current) ++ adminAcls ++ modifyAcls ++ viewAcls).mkString(\" \")\n+    val writers = (Seq(current) ++ adminAcls ++ modifyAcls).mkString(\" \")\n+    var domain = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domain with readers: $readers and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domain)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      timelineClient.putDomain(timelineDomain)\n+      Some(domain)\n+    } catch {\n+      case e: Exception => {\n+        logError(s\"cannot create the domain $domain\", e)\n+        // fallback to default\n+        None\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param binding binding to the spark application and YARN\n+   */\n+  override def start(binding: YarnExtensionServiceBinding): Unit = {\n+    val oldstate = enterState(StartedState)\n+    if (oldstate != CreatedState) {\n+      // state model violation\n+      _serviceState.set(oldstate)\n+      throw new IllegalArgumentException(s\"Cannot start the service from state $oldstate\")\n+    }\n+    val context = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    this.sparkContext = context\n+\n+    this.config = new YarnConfiguration(context.hadoopConfiguration)\n+\n+    val sparkConf = sparkContext.conf\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    _batchSize = sparkConf.getInt(BATCH_SIZE, _batchSize)\n+    _postQueueLimit = sparkConf.getInt(POST_QUEUE_LIMIT, _postQueueLimit)\n+    retryInterval = 1000 * sparkConf.getTimeAsSeconds(POST_RETRY_INTERVAL,\n+      DEFAULT_POST_RETRY_INTERVAL)\n+    shutdownWaitTime = 1000 * sparkConf.getTimeAsSeconds(SHUTDOWN_WAIT_TIME,\n+      DEFAULT_SHUTDOWN_WAIT_TIME)\n+\n+\n+    if (timelineServiceEnabled) {\n+      _timelineWebappAddress = getTimelineEndpoint(config)\n+\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at ${_timelineWebappAddress}\")\n+      _timelineClient = Some(createTimelineClient())\n+      domainId = createTimelineDomain()\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start()\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    if (registerListener()) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED` is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(config)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString(): String = {\n+    s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+    s\" state=$serviceState;\" +\n+    s\" endpoint=${_timelineWebappAddress};\" +\n+    s\" bonded to ATS=$bondedToATS;\" +\n+    s\" listening=$listening;\" +\n+    s\" batchSize=$batchSize;\" +\n+    s\" flush count=$getFlushCount;\" +\n+    s\" total number queued=$eventsQueued, processed=$eventsProcessed;\" +\n+    s\" attempted entity posts=$postAttempts\" +\n+    s\" successful entity posts=$postSuccesses\" +\n+    s\" failed entity posts=$postFailures;\" +\n+    s\" events dropped=$eventsDropped;\" +\n+    s\" app start event received=$appStartEventProcessed;\" +\n+    s\" app end event received=$appEndEventProcessed;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    _timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    _applicationId = appId\n+    _attemptId = attemptId\n+  }\n+\n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private[yarn] def registerListener(): Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param event event to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(event: SparkListenerEvent): Boolean = {\n+    if (!stopped.get) {\n+      _eventsQueued.incrementAndGet\n+      logDebug(s\"Enqueue $event\")\n+      handleEvent(event)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $event\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override def stop(): Unit = {\n+    val oldState = enterState(StoppedState)\n+    if (oldState != StartedState) {\n+      // stopping from a different state\n+      logDebug(s\"Ignoring stop() request from state $oldState\")\n+      return\n+    }\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (appStartEventProcessed.get && !appEndEventProcessed.get) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        enqueue(SparkListenerApplicationEnd(now()))\n+      }\n+\n+      // flush out the events\n+      asyncFlush()\n+\n+      // stop operation\n+      postThreadActive.synchronized {\n+        // now await that marker flag\n+        if (postThreadActive.get) {\n+          logDebug(s\"Stopping posting thread and waiting $shutdownWaitTime mS\")\n+          stopped.set(true)\n+          eventHandlingThread.foreach(_.interrupt())\n+          postThreadActive.wait(shutdownWaitTime)\n+        } else {\n+          stopTimelineClient()\n+          logInfo(s\"Stopped: $this\")\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Can an event be added?\n+   * Policy is only if the number of queued entities is below the limit, or the\n+   * event marks the end of the application.\n+   * @param isLifecycleEvent is this operation triggered by an application start/end?\n+   * @return true if the event can be added to the queue\n+   */\n+  private def canAddEvent(isLifecycleEvent: Boolean): Boolean = {\n+    _entityQueue.synchronized {\n+      _entityQueue.size() < _postQueueLimit  || isLifecycleEvent\n+    }\n+  }\n+\n+  /**\n+   * Add another event to the pending event list.\n+   * Returns the size of the event list after the event was added\n+   * (thread safe).\n+   * @param event event to add\n+   * @return the event list size\n+   */\n+  private def addPendingEvent(event: TimelineEvent): Int = {\n+    pendingEvents.synchronized {\n+      pendingEvents :+= event\n+      pendingEvents.size\n+    }\n+  }\n+\n+  /**\n+   * Publish next set of pending events.\n+   *\n+   * Builds the next event to push on the entity Queue; resets\n+   * the current [[pendingEvents]] list and then notifies\n+   * any listener of the [[_entityQueue]] that there is new data.\n+   */\n+  private def publishPendingEvents(): Boolean = {\n+    // verify that there are events to publish\n+    val size = pendingEvents.synchronized {\n+      pendingEvents.size\n+    }\n+    if (size > 0 && applicationStartEvent.isDefined) {\n+      flushCount.incrementAndGet()\n+      val timelineEntity = createTimelineEntity(\n+        applicationId,\n+        attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        applicationName,\n+        userName,\n+        startTime,\n+        endTime,\n+        now())\n+\n+      // copy in pending events and then clear the list\n+      pendingEvents.synchronized {\n+        pendingEvents.foreach(timelineEntity.addEvent)\n+        pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+      }\n+      // queue the entity for posting\n+      preflightCheck(timelineEntity)\n+      _entityQueue.synchronized {\n+        _entityQueue.push(timelineEntity)\n+      }\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   *\n+   * @param entity timeline entity to review.\n+   */\n+  private def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * Post a single entity.\n+   *\n+   * Any network/connectivity errors will be caught and logged, and returned as the\n+   * exception field in the returned tuple.\n+   *\n+   * Any posting which generates a response will result in the timeline response being\n+   * returned. This response *may* contain errors; these are almost invariably going\n+   * to re-occur when resubmitted.\n+   *\n+   * @param entity entity to post\n+   * @return tuple with exactly one of the response or the exception set.\n+   */\n+  private def postOneEntity(entity: TimelineEntity):\n+      (Option[TimelinePutResponse], Option[Exception]) = {\n+    domainId.foreach {\n+      entity.setDomainId\n+    }\n+    val entityDescription = describeEntity(entity)\n+    logDebug(s\"About to POST $entityDescription\")\n+    _entityPostAttempts.incrementAndGet()\n+    try {\n+      val response = timelineClient.putEntities(entity)\n+      val errors = response.getErrors\n+      if (errors.isEmpty) {\n+        logDebug(s\"entity successfully posted\")\n+        _entityPostSuccesses.incrementAndGet()\n+      } else {\n+        // something went wrong.\n+        errors.asScala.foreach { err =>\n+          _entityPostFailures.incrementAndGet()\n+          logError(s\"Failed to post $entityDescription\\n:${describeError(err)}\")\n+        }\n+      }\n+      // whatever the outcome, this request is not re-issued\n+      (Some(response), None)\n+    } catch {\n+      case e: ConnectException =>\n+        _entityPostFailures.incrementAndGet\n+        logWarning(s\"Connection exception submitting $entityDescription\", e)\n+        (None, Some(e))\n+\n+      case e: RuntimeException =>\n+        // this is probably a retry timeout event; some Hadoop versions don't\n+        // rethrow the exception causing the problem, instead raising an RTE\n+        _entityPostFailures.incrementAndGet\n+        logWarning(s\"Runtime exception submitting $entityDescription\", e)\n+        // same policy as before: retry on these\n+        (None, Some(e))\n+\n+      case e: Exception =>\n+        // something else has gone wrong.\n+        _entityPostFailures.incrementAndGet\n+        logWarning(s\"Could not handle history entity: $entityDescription\", e)\n+        (None, Some(e))\n+    }\n+  }\n+\n+  /**\n+   * Spin and post until stopped.\n+   *\n+   * Algorithm.\n+   *\n+   * 1. The thread waits for events in the _entityQueue until stopped or interrupted\n+   * 1. Failures result in the entity being queued for resending, after a delay which grows\n+   * linearly on every retry.\n+   * 1. Successful posts reset the retry delay.\n+   * 1. If the process is interrupted, the loop continues with the `stopFlag` flag being checked.\n+   *\n+   * To stop this process then, first set the `stopFlag` flag, then interrupt the thread.\n+   *\n+   * @param stopFlag a flag to set to stop the loop.\n+   * @param retryInterval delay in milliseconds for the first retry delay; the delay increases\n+   *        by this value on every future failure. If zero, there is no delay, ever.\n+   */\n+  private def postEntities(stopFlag: AtomicBoolean, retryInterval: Long): Unit = {\n+    var lastAttemptFailed = false\n+    var currentRetryDelay = retryInterval\n+    while (!stopped.get) {\n+      try {\n+        val head = _entityQueue.take()\n+        val (_, ex) = postOneEntity(head)\n+        if (ex.isDefined && !stopped.get()) {\n+          // something went wrong and it wasn't being told to stop\n+          if (!lastAttemptFailed) {\n+            // avoid filling up logs with repeated failures\n+            logWarning(s\"Exception submitting entity to ${_timelineWebappAddress}\", ex.get)\n+          }\n+          // log failure and repost\n+          lastAttemptFailed = true\n+          currentRetryDelay += retryInterval\n+          _entityQueue.addFirst(head)\n+          if (currentRetryDelay > 0 ) {\n+            Thread.sleep(currentRetryDelay)\n+          }\n+        } else {\n+          // success\n+          lastAttemptFailed = false\n+          currentRetryDelay = retryInterval\n+        }\n+      } catch {\n+        case ex: InterruptedException =>\n+          // interrupted; this will break out of IO/Sleep operations and\n+          // trigger a rescan of the stopped() event.\n+          // The ATS code implements some of its own retry logic, which\n+          // can catch interrupts and convert to other exceptions â€”this catch clause picks up\n+          // interrupts outside of that code.\n+          logDebug(s\"Interrupted\", ex)\n+        case other: Exception =>\n+          throw other\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Shutdown phase: continually post oustanding entities until the timeout has been exceeded.\n+   * The interval between failures is the retryInterval: there is no escalation, and if\n+   * is longer than the remaining time in the shutdown, the remaining time sets the limit.\n+   * @param timeout timeout in milliseconds.\n+   * @param retryInterval delay in milliseconds for every delay.\n+   */\n+  private def postEntitiesShutdownPhase(timeout: Long, retryInterval: Long): Unit = {\n+    val timeLimit = now() + timeout;\n+    logDebug(s\"Entering shutdown post phase, time period = $timeout mS\")\n+    while (now() < timeLimit && !_entityQueue.isEmpty) {\n+\n+      try {\n+        val head = _entityQueue.poll(timeLimit - now(), TimeUnit.MILLISECONDS)\n+        if (head != null) {\n+          val (_, ex) = postOneEntity(head)\n+          if (ex.isDefined) {\n+            // failure, push back to try again\n+            _entityQueue.addFirst(head)\n+            if (retryInterval > 0) {\n+              Thread.sleep(Math.min(retryInterval, timeLimit - now()))\n+            }\n+          }\n+        }\n+      } catch {\n+        case ex: InterruptedException =>\n+          // ignoring, as this is shutdown time anyway\n+          logDebug(\"Ignoring Interrupt\", ex)\n+        case ex: Exception =>\n+          throw ex\n+      }\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic still applies\n+   */\n+  private def handleEvent(event: SparkListenerEvent): Unit = {\n+    var push = false\n+    var isLifecycleEvent = false;"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: unneeded noise. `logDebug`, at most.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-28T21:39:12Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{TimeUnit, LinkedBlockingDeque}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{YarnExtensionService, YarnExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered\n+ * YARN Timeline Server.\n+ */\n+private[spark] class YarnHistoryService extends YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+  def enterState(state: Int): Int = {\n+    logDebug(s\"Entering state $state from $serviceState\")\n+    _serviceState.getAndSet(state)\n+  }\n+\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** YARN configuration from the spark context */\n+  private var config: YarnConfiguration = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var _timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Application name  from the spark start event */\n+  private var applicationName: String = _\n+\n+  /** Application ID*/\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName = Utils.getCurrentUserName()\n+\n+  /** Clock for recording time */\n+  private val clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /**\n+   * Start time of the application, as received in the end event.\n+   */\n+  private var endTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var _batchSize = DEFAULT_BATCH_SIZE\n+\n+  /** queue of entities to asynchronously post, plus the number of events in each entry */\n+  private var _entityQueue = new LinkedBlockingDeque[TimelineEntity]()\n+\n+  /** limit on the total number of events permitted */\n+  private var _postQueueLimit = DEFAULT_POST_QUEUE_LIMIT\n+\n+  /**\n+   * List of events which will be pulled into a timeline\n+   * entity when created\n+   */\n+  private var pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+\n+  private var applicationStartEvent: Option[SparkListenerApplicationStart] = None\n+  private var applicationEndEvent: Option[SparkListenerApplicationEnd] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val _eventsProcessed = new AtomicLong(0)\n+\n+  /** counter of events queued. */\n+  private val _eventsQueued = new AtomicLong(0)\n+\n+  private val _entityPostAttempts = new AtomicLong(0)\n+  private val _entityPostSuccesses = new AtomicLong(0)\n+  /** how many entity postings failed? */\n+  private val _entityPostFailures = new AtomicLong(0)\n+  private val _eventsDropped = new AtomicLong(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicLong(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped = new AtomicBoolean(true)\n+\n+  /**\n+   * Boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive = new AtomicBoolean(false)\n+\n+  /** How long to wait for shutdown before giving up */\n+  private var shutdownWaitTime = 0L\n+\n+  /**\n+   * What is the initial and incrementing interval for POST retries?\n+   */\n+  private var retryInterval = 0L\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: Option[String] = None\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var _timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private[yarn] def createTimelineClient(): TimelineClient = {\n+    require(_timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def timelineClient: TimelineClient = {\n+    require(_timelineClient.isDefined)\n+    _timelineClient.get\n+  }\n+\n+  /**\n+   * Get the total number of events dropped due to the queue of\n+   * outstanding posts being too long\n+   * @return counter of events processed\n+   */\n+\n+  def eventsDropped: Long = {\n+    _eventsDropped.get()\n+  }\n+\n+  /**\n+   * Get the total number of processed events, those handled in the back-end thread without\n+   * being rejected\n+   * @return counter of events processed\n+   */\n+  def eventsProcessed: Long = {\n+    _eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def eventsQueued: Long = {\n+    _eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    _entityQueue.size()\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def batchSize: Int = {\n+    _batchSize\n+  }\n+\n+  /**\n+   * Query the counter of attempts to post events\n+   * @return\n+   */\n+  def postAttempts: Long = _entityPostAttempts.get()\n+\n+  /**\n+   * Get the total number of failed post operations\n+   * @return counter of timeline post operations which failed\n+   */\n+  def postFailures: Long = {\n+    _entityPostFailures.get\n+  }\n+\n+  /**\n+   * Query the counter of post successes\n+   * @return the number of successful posts\n+   */\n+  def postSuccesses: Long = _entityPostSuccesses.get()\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId =  _applicationId\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] =  _attemptId\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    _timelineClient.foreach(_.stop())\n+    _timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain.\n+   *\n+   * A Timeline Domain is a uniquely identified 'namespace' for accessing parts of the timeline.\n+   * Security levels are are managed at the domain level, so one is created if the\n+   * spark acls are enabled. Full access is then granted to the current user,\n+   * all users in the configuration options `\"spark.modify.acls\"` and `\"spark.admin.acls\"`;\n+   * read access to those users and those listed in `\"spark.ui.view.acls\"`\n+   *\n+   * @return an optional domain string. If `None`, then no domain was created.\n+   */\n+  private def createTimelineDomain(): Option[String] = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsOn = sparkConf.getBoolean(\"spark.ui.acls.enable\",\n+        sparkConf.getBoolean(\"spark.acls.enable\", false))\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return None\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      logDebug(s\"Using predefined domain $predefDomain\")\n+      return predefDomain\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (Seq(current) ++ adminAcls ++ modifyAcls ++ viewAcls).mkString(\" \")\n+    val writers = (Seq(current) ++ adminAcls ++ modifyAcls).mkString(\" \")\n+    var domain = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domain with readers: $readers and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domain)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      timelineClient.putDomain(timelineDomain)\n+      Some(domain)\n+    } catch {\n+      case e: Exception => {\n+        logError(s\"cannot create the domain $domain\", e)\n+        // fallback to default\n+        None\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param binding binding to the spark application and YARN\n+   */\n+  override def start(binding: YarnExtensionServiceBinding): Unit = {\n+    val oldstate = enterState(StartedState)\n+    if (oldstate != CreatedState) {\n+      // state model violation\n+      _serviceState.set(oldstate)\n+      throw new IllegalArgumentException(s\"Cannot start the service from state $oldstate\")\n+    }\n+    val context = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    this.sparkContext = context\n+\n+    this.config = new YarnConfiguration(context.hadoopConfiguration)\n+\n+    val sparkConf = sparkContext.conf\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    _batchSize = sparkConf.getInt(BATCH_SIZE, _batchSize)\n+    _postQueueLimit = sparkConf.getInt(POST_QUEUE_LIMIT, _postQueueLimit)\n+    retryInterval = 1000 * sparkConf.getTimeAsSeconds(POST_RETRY_INTERVAL,\n+      DEFAULT_POST_RETRY_INTERVAL)\n+    shutdownWaitTime = 1000 * sparkConf.getTimeAsSeconds(SHUTDOWN_WAIT_TIME,\n+      DEFAULT_SHUTDOWN_WAIT_TIME)\n+\n+\n+    if (timelineServiceEnabled) {\n+      _timelineWebappAddress = getTimelineEndpoint(config)\n+\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at ${_timelineWebappAddress}\")\n+      _timelineClient = Some(createTimelineClient())\n+      domainId = createTimelineDomain()\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start()\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    if (registerListener()) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED` is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(config)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString(): String = {\n+    s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+    s\" state=$serviceState;\" +\n+    s\" endpoint=${_timelineWebappAddress};\" +\n+    s\" bonded to ATS=$bondedToATS;\" +\n+    s\" listening=$listening;\" +\n+    s\" batchSize=$batchSize;\" +\n+    s\" flush count=$getFlushCount;\" +\n+    s\" total number queued=$eventsQueued, processed=$eventsProcessed;\" +\n+    s\" attempted entity posts=$postAttempts\" +\n+    s\" successful entity posts=$postSuccesses\" +\n+    s\" failed entity posts=$postFailures;\" +\n+    s\" events dropped=$eventsDropped;\" +\n+    s\" app start event received=$appStartEventProcessed;\" +\n+    s\" app end event received=$appEndEventProcessed;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    _timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    _applicationId = appId\n+    _attemptId = attemptId\n+  }\n+\n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private[yarn] def registerListener(): Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param event event to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(event: SparkListenerEvent): Boolean = {\n+    if (!stopped.get) {\n+      _eventsQueued.incrementAndGet\n+      logDebug(s\"Enqueue $event\")\n+      handleEvent(event)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $event\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override def stop(): Unit = {\n+    val oldState = enterState(StoppedState)\n+    if (oldState != StartedState) {\n+      // stopping from a different state\n+      logDebug(s\"Ignoring stop() request from state $oldState\")\n+      return\n+    }\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (appStartEventProcessed.get && !appEndEventProcessed.get) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        enqueue(SparkListenerApplicationEnd(now()))\n+      }\n+\n+      // flush out the events\n+      asyncFlush()\n+\n+      // stop operation\n+      postThreadActive.synchronized {\n+        // now await that marker flag\n+        if (postThreadActive.get) {\n+          logDebug(s\"Stopping posting thread and waiting $shutdownWaitTime mS\")\n+          stopped.set(true)\n+          eventHandlingThread.foreach(_.interrupt())\n+          postThreadActive.wait(shutdownWaitTime)\n+        } else {\n+          stopTimelineClient()\n+          logInfo(s\"Stopped: $this\")\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Can an event be added?\n+   * Policy is only if the number of queued entities is below the limit, or the\n+   * event marks the end of the application.\n+   * @param isLifecycleEvent is this operation triggered by an application start/end?\n+   * @return true if the event can be added to the queue\n+   */\n+  private def canAddEvent(isLifecycleEvent: Boolean): Boolean = {\n+    _entityQueue.synchronized {\n+      _entityQueue.size() < _postQueueLimit  || isLifecycleEvent\n+    }\n+  }\n+\n+  /**\n+   * Add another event to the pending event list.\n+   * Returns the size of the event list after the event was added\n+   * (thread safe).\n+   * @param event event to add\n+   * @return the event list size\n+   */\n+  private def addPendingEvent(event: TimelineEvent): Int = {\n+    pendingEvents.synchronized {\n+      pendingEvents :+= event\n+      pendingEvents.size\n+    }\n+  }\n+\n+  /**\n+   * Publish next set of pending events.\n+   *\n+   * Builds the next event to push on the entity Queue; resets\n+   * the current [[pendingEvents]] list and then notifies\n+   * any listener of the [[_entityQueue]] that there is new data.\n+   */\n+  private def publishPendingEvents(): Boolean = {\n+    // verify that there are events to publish\n+    val size = pendingEvents.synchronized {\n+      pendingEvents.size\n+    }\n+    if (size > 0 && applicationStartEvent.isDefined) {\n+      flushCount.incrementAndGet()\n+      val timelineEntity = createTimelineEntity(\n+        applicationId,\n+        attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        applicationName,\n+        userName,\n+        startTime,\n+        endTime,\n+        now())\n+\n+      // copy in pending events and then clear the list\n+      pendingEvents.synchronized {\n+        pendingEvents.foreach(timelineEntity.addEvent)\n+        pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+      }\n+      // queue the entity for posting\n+      preflightCheck(timelineEntity)\n+      _entityQueue.synchronized {\n+        _entityQueue.push(timelineEntity)\n+      }\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   *\n+   * @param entity timeline entity to review.\n+   */\n+  private def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * Post a single entity.\n+   *\n+   * Any network/connectivity errors will be caught and logged, and returned as the\n+   * exception field in the returned tuple.\n+   *\n+   * Any posting which generates a response will result in the timeline response being\n+   * returned. This response *may* contain errors; these are almost invariably going\n+   * to re-occur when resubmitted.\n+   *\n+   * @param entity entity to post\n+   * @return tuple with exactly one of the response or the exception set.\n+   */\n+  private def postOneEntity(entity: TimelineEntity):\n+      (Option[TimelinePutResponse], Option[Exception]) = {\n+    domainId.foreach {\n+      entity.setDomainId\n+    }\n+    val entityDescription = describeEntity(entity)\n+    logDebug(s\"About to POST $entityDescription\")\n+    _entityPostAttempts.incrementAndGet()\n+    try {\n+      val response = timelineClient.putEntities(entity)\n+      val errors = response.getErrors\n+      if (errors.isEmpty) {\n+        logDebug(s\"entity successfully posted\")\n+        _entityPostSuccesses.incrementAndGet()\n+      } else {\n+        // something went wrong.\n+        errors.asScala.foreach { err =>\n+          _entityPostFailures.incrementAndGet()\n+          logError(s\"Failed to post $entityDescription\\n:${describeError(err)}\")\n+        }\n+      }\n+      // whatever the outcome, this request is not re-issued\n+      (Some(response), None)\n+    } catch {\n+      case e: ConnectException =>\n+        _entityPostFailures.incrementAndGet\n+        logWarning(s\"Connection exception submitting $entityDescription\", e)\n+        (None, Some(e))\n+\n+      case e: RuntimeException =>\n+        // this is probably a retry timeout event; some Hadoop versions don't\n+        // rethrow the exception causing the problem, instead raising an RTE\n+        _entityPostFailures.incrementAndGet\n+        logWarning(s\"Runtime exception submitting $entityDescription\", e)\n+        // same policy as before: retry on these\n+        (None, Some(e))\n+\n+      case e: Exception =>\n+        // something else has gone wrong.\n+        _entityPostFailures.incrementAndGet\n+        logWarning(s\"Could not handle history entity: $entityDescription\", e)\n+        (None, Some(e))\n+    }\n+  }\n+\n+  /**\n+   * Spin and post until stopped.\n+   *\n+   * Algorithm.\n+   *\n+   * 1. The thread waits for events in the _entityQueue until stopped or interrupted\n+   * 1. Failures result in the entity being queued for resending, after a delay which grows\n+   * linearly on every retry.\n+   * 1. Successful posts reset the retry delay.\n+   * 1. If the process is interrupted, the loop continues with the `stopFlag` flag being checked.\n+   *\n+   * To stop this process then, first set the `stopFlag` flag, then interrupt the thread.\n+   *\n+   * @param stopFlag a flag to set to stop the loop.\n+   * @param retryInterval delay in milliseconds for the first retry delay; the delay increases\n+   *        by this value on every future failure. If zero, there is no delay, ever.\n+   */\n+  private def postEntities(stopFlag: AtomicBoolean, retryInterval: Long): Unit = {\n+    var lastAttemptFailed = false\n+    var currentRetryDelay = retryInterval\n+    while (!stopped.get) {\n+      try {\n+        val head = _entityQueue.take()\n+        val (_, ex) = postOneEntity(head)\n+        if (ex.isDefined && !stopped.get()) {\n+          // something went wrong and it wasn't being told to stop\n+          if (!lastAttemptFailed) {\n+            // avoid filling up logs with repeated failures\n+            logWarning(s\"Exception submitting entity to ${_timelineWebappAddress}\", ex.get)\n+          }\n+          // log failure and repost\n+          lastAttemptFailed = true\n+          currentRetryDelay += retryInterval\n+          _entityQueue.addFirst(head)\n+          if (currentRetryDelay > 0 ) {\n+            Thread.sleep(currentRetryDelay)\n+          }\n+        } else {\n+          // success\n+          lastAttemptFailed = false\n+          currentRetryDelay = retryInterval\n+        }\n+      } catch {\n+        case ex: InterruptedException =>\n+          // interrupted; this will break out of IO/Sleep operations and\n+          // trigger a rescan of the stopped() event.\n+          // The ATS code implements some of its own retry logic, which\n+          // can catch interrupts and convert to other exceptions â€”this catch clause picks up\n+          // interrupts outside of that code.\n+          logDebug(s\"Interrupted\", ex)\n+        case other: Exception =>\n+          throw other\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Shutdown phase: continually post oustanding entities until the timeout has been exceeded.\n+   * The interval between failures is the retryInterval: there is no escalation, and if\n+   * is longer than the remaining time in the shutdown, the remaining time sets the limit.\n+   * @param timeout timeout in milliseconds.\n+   * @param retryInterval delay in milliseconds for every delay.\n+   */\n+  private def postEntitiesShutdownPhase(timeout: Long, retryInterval: Long): Unit = {\n+    val timeLimit = now() + timeout;\n+    logDebug(s\"Entering shutdown post phase, time period = $timeout mS\")\n+    while (now() < timeLimit && !_entityQueue.isEmpty) {\n+\n+      try {\n+        val head = _entityQueue.poll(timeLimit - now(), TimeUnit.MILLISECONDS)\n+        if (head != null) {\n+          val (_, ex) = postOneEntity(head)\n+          if (ex.isDefined) {\n+            // failure, push back to try again\n+            _entityQueue.addFirst(head)\n+            if (retryInterval > 0) {\n+              Thread.sleep(Math.min(retryInterval, timeLimit - now()))\n+            }\n+          }\n+        }\n+      } catch {\n+        case ex: InterruptedException =>\n+          // ignoring, as this is shutdown time anyway\n+          logDebug(\"Ignoring Interrupt\", ex)\n+        case ex: Exception =>\n+          throw ex\n+      }\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic still applies\n+   */\n+  private def handleEvent(event: SparkListenerEvent): Unit = {\n+    var push = false\n+    var isLifecycleEvent = false;\n+    val timestamp = now()\n+    if (_eventsProcessed.incrementAndGet() % 1000 == 0) {\n+      logDebug(s\"${_eventsProcessed} events are processed\")\n+    }\n+    event match {\n+      case start: SparkListenerApplicationStart =>\n+        // we already have all information,\n+        // flush it for old one to switch to new one\n+        logDebug(s\"Handling application start event: $event\")\n+        if (!appStartEventProcessed.getAndSet(true)) {\n+          applicationStartEvent = Some(start)\n+          applicationName = start.appName\n+          if (applicationName == null || applicationName.isEmpty) {\n+            logWarning(\"Application does not have a name\")\n+            applicationName = applicationId.toString\n+          }\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {\n+            startTime = timestamp\n+          }\n+          setContextAppAndAttemptInfo(start.appId, start.appAttemptId)\n+          logInfo(s\"Application started: $event\")\n+          isLifecycleEvent = true\n+          push = true\n+        } else {\n+          logWarning(s\"More than one application start event received -ignoring: $start\")\n+        }\n+\n+      case end: SparkListenerApplicationEnd =>\n+        if (!appStartEventProcessed.get()) {\n+          logError(s\"Received application end event without application start $event\")\n+        } else if (!appEndEventProcessed.getAndSet(true)) {\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logInfo(s\"Application end event: $event\")"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: semi-colon.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-28T21:39:20Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{TimeUnit, LinkedBlockingDeque}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{YarnExtensionService, YarnExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered\n+ * YARN Timeline Server.\n+ */\n+private[spark] class YarnHistoryService extends YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+  def enterState(state: Int): Int = {\n+    logDebug(s\"Entering state $state from $serviceState\")\n+    _serviceState.getAndSet(state)\n+  }\n+\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** YARN configuration from the spark context */\n+  private var config: YarnConfiguration = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var _timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Application name  from the spark start event */\n+  private var applicationName: String = _\n+\n+  /** Application ID*/\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName = Utils.getCurrentUserName()\n+\n+  /** Clock for recording time */\n+  private val clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /**\n+   * Start time of the application, as received in the end event.\n+   */\n+  private var endTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var _batchSize = DEFAULT_BATCH_SIZE\n+\n+  /** queue of entities to asynchronously post, plus the number of events in each entry */\n+  private var _entityQueue = new LinkedBlockingDeque[TimelineEntity]()\n+\n+  /** limit on the total number of events permitted */\n+  private var _postQueueLimit = DEFAULT_POST_QUEUE_LIMIT\n+\n+  /**\n+   * List of events which will be pulled into a timeline\n+   * entity when created\n+   */\n+  private var pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+\n+  private var applicationStartEvent: Option[SparkListenerApplicationStart] = None\n+  private var applicationEndEvent: Option[SparkListenerApplicationEnd] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val _eventsProcessed = new AtomicLong(0)\n+\n+  /** counter of events queued. */\n+  private val _eventsQueued = new AtomicLong(0)\n+\n+  private val _entityPostAttempts = new AtomicLong(0)\n+  private val _entityPostSuccesses = new AtomicLong(0)\n+  /** how many entity postings failed? */\n+  private val _entityPostFailures = new AtomicLong(0)\n+  private val _eventsDropped = new AtomicLong(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicLong(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped = new AtomicBoolean(true)\n+\n+  /**\n+   * Boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive = new AtomicBoolean(false)\n+\n+  /** How long to wait for shutdown before giving up */\n+  private var shutdownWaitTime = 0L\n+\n+  /**\n+   * What is the initial and incrementing interval for POST retries?\n+   */\n+  private var retryInterval = 0L\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: Option[String] = None\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var _timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private[yarn] def createTimelineClient(): TimelineClient = {\n+    require(_timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def timelineClient: TimelineClient = {\n+    require(_timelineClient.isDefined)\n+    _timelineClient.get\n+  }\n+\n+  /**\n+   * Get the total number of events dropped due to the queue of\n+   * outstanding posts being too long\n+   * @return counter of events processed\n+   */\n+\n+  def eventsDropped: Long = {\n+    _eventsDropped.get()\n+  }\n+\n+  /**\n+   * Get the total number of processed events, those handled in the back-end thread without\n+   * being rejected\n+   * @return counter of events processed\n+   */\n+  def eventsProcessed: Long = {\n+    _eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def eventsQueued: Long = {\n+    _eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    _entityQueue.size()\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def batchSize: Int = {\n+    _batchSize\n+  }\n+\n+  /**\n+   * Query the counter of attempts to post events\n+   * @return\n+   */\n+  def postAttempts: Long = _entityPostAttempts.get()\n+\n+  /**\n+   * Get the total number of failed post operations\n+   * @return counter of timeline post operations which failed\n+   */\n+  def postFailures: Long = {\n+    _entityPostFailures.get\n+  }\n+\n+  /**\n+   * Query the counter of post successes\n+   * @return the number of successful posts\n+   */\n+  def postSuccesses: Long = _entityPostSuccesses.get()\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId =  _applicationId\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] =  _attemptId\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    _timelineClient.foreach(_.stop())\n+    _timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain.\n+   *\n+   * A Timeline Domain is a uniquely identified 'namespace' for accessing parts of the timeline.\n+   * Security levels are are managed at the domain level, so one is created if the\n+   * spark acls are enabled. Full access is then granted to the current user,\n+   * all users in the configuration options `\"spark.modify.acls\"` and `\"spark.admin.acls\"`;\n+   * read access to those users and those listed in `\"spark.ui.view.acls\"`\n+   *\n+   * @return an optional domain string. If `None`, then no domain was created.\n+   */\n+  private def createTimelineDomain(): Option[String] = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsOn = sparkConf.getBoolean(\"spark.ui.acls.enable\",\n+        sparkConf.getBoolean(\"spark.acls.enable\", false))\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return None\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      logDebug(s\"Using predefined domain $predefDomain\")\n+      return predefDomain\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (Seq(current) ++ adminAcls ++ modifyAcls ++ viewAcls).mkString(\" \")\n+    val writers = (Seq(current) ++ adminAcls ++ modifyAcls).mkString(\" \")\n+    var domain = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domain with readers: $readers and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domain)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      timelineClient.putDomain(timelineDomain)\n+      Some(domain)\n+    } catch {\n+      case e: Exception => {\n+        logError(s\"cannot create the domain $domain\", e)\n+        // fallback to default\n+        None\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param binding binding to the spark application and YARN\n+   */\n+  override def start(binding: YarnExtensionServiceBinding): Unit = {\n+    val oldstate = enterState(StartedState)\n+    if (oldstate != CreatedState) {\n+      // state model violation\n+      _serviceState.set(oldstate)\n+      throw new IllegalArgumentException(s\"Cannot start the service from state $oldstate\")\n+    }\n+    val context = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    this.sparkContext = context\n+\n+    this.config = new YarnConfiguration(context.hadoopConfiguration)\n+\n+    val sparkConf = sparkContext.conf\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    _batchSize = sparkConf.getInt(BATCH_SIZE, _batchSize)\n+    _postQueueLimit = sparkConf.getInt(POST_QUEUE_LIMIT, _postQueueLimit)\n+    retryInterval = 1000 * sparkConf.getTimeAsSeconds(POST_RETRY_INTERVAL,\n+      DEFAULT_POST_RETRY_INTERVAL)\n+    shutdownWaitTime = 1000 * sparkConf.getTimeAsSeconds(SHUTDOWN_WAIT_TIME,\n+      DEFAULT_SHUTDOWN_WAIT_TIME)\n+\n+\n+    if (timelineServiceEnabled) {\n+      _timelineWebappAddress = getTimelineEndpoint(config)\n+\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at ${_timelineWebappAddress}\")\n+      _timelineClient = Some(createTimelineClient())\n+      domainId = createTimelineDomain()\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start()\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    if (registerListener()) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED` is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(config)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString(): String = {\n+    s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+    s\" state=$serviceState;\" +\n+    s\" endpoint=${_timelineWebappAddress};\" +\n+    s\" bonded to ATS=$bondedToATS;\" +\n+    s\" listening=$listening;\" +\n+    s\" batchSize=$batchSize;\" +\n+    s\" flush count=$getFlushCount;\" +\n+    s\" total number queued=$eventsQueued, processed=$eventsProcessed;\" +\n+    s\" attempted entity posts=$postAttempts\" +\n+    s\" successful entity posts=$postSuccesses\" +\n+    s\" failed entity posts=$postFailures;\" +\n+    s\" events dropped=$eventsDropped;\" +\n+    s\" app start event received=$appStartEventProcessed;\" +\n+    s\" app end event received=$appEndEventProcessed;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    _timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    _applicationId = appId\n+    _attemptId = attemptId\n+  }\n+\n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private[yarn] def registerListener(): Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param event event to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(event: SparkListenerEvent): Boolean = {\n+    if (!stopped.get) {\n+      _eventsQueued.incrementAndGet\n+      logDebug(s\"Enqueue $event\")\n+      handleEvent(event)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $event\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override def stop(): Unit = {\n+    val oldState = enterState(StoppedState)\n+    if (oldState != StartedState) {\n+      // stopping from a different state\n+      logDebug(s\"Ignoring stop() request from state $oldState\")\n+      return\n+    }\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (appStartEventProcessed.get && !appEndEventProcessed.get) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        enqueue(SparkListenerApplicationEnd(now()))\n+      }\n+\n+      // flush out the events\n+      asyncFlush()\n+\n+      // stop operation\n+      postThreadActive.synchronized {\n+        // now await that marker flag\n+        if (postThreadActive.get) {\n+          logDebug(s\"Stopping posting thread and waiting $shutdownWaitTime mS\")\n+          stopped.set(true)\n+          eventHandlingThread.foreach(_.interrupt())\n+          postThreadActive.wait(shutdownWaitTime)\n+        } else {\n+          stopTimelineClient()\n+          logInfo(s\"Stopped: $this\")\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Can an event be added?\n+   * Policy is only if the number of queued entities is below the limit, or the\n+   * event marks the end of the application.\n+   * @param isLifecycleEvent is this operation triggered by an application start/end?\n+   * @return true if the event can be added to the queue\n+   */\n+  private def canAddEvent(isLifecycleEvent: Boolean): Boolean = {\n+    _entityQueue.synchronized {\n+      _entityQueue.size() < _postQueueLimit  || isLifecycleEvent\n+    }\n+  }\n+\n+  /**\n+   * Add another event to the pending event list.\n+   * Returns the size of the event list after the event was added\n+   * (thread safe).\n+   * @param event event to add\n+   * @return the event list size\n+   */\n+  private def addPendingEvent(event: TimelineEvent): Int = {\n+    pendingEvents.synchronized {\n+      pendingEvents :+= event\n+      pendingEvents.size\n+    }\n+  }\n+\n+  /**\n+   * Publish next set of pending events.\n+   *\n+   * Builds the next event to push on the entity Queue; resets\n+   * the current [[pendingEvents]] list and then notifies\n+   * any listener of the [[_entityQueue]] that there is new data.\n+   */\n+  private def publishPendingEvents(): Boolean = {\n+    // verify that there are events to publish\n+    val size = pendingEvents.synchronized {\n+      pendingEvents.size\n+    }\n+    if (size > 0 && applicationStartEvent.isDefined) {\n+      flushCount.incrementAndGet()\n+      val timelineEntity = createTimelineEntity(\n+        applicationId,\n+        attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        applicationName,\n+        userName,\n+        startTime,\n+        endTime,\n+        now())\n+\n+      // copy in pending events and then clear the list\n+      pendingEvents.synchronized {\n+        pendingEvents.foreach(timelineEntity.addEvent)\n+        pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+      }\n+      // queue the entity for posting\n+      preflightCheck(timelineEntity)\n+      _entityQueue.synchronized {\n+        _entityQueue.push(timelineEntity)\n+      }\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   *\n+   * @param entity timeline entity to review.\n+   */\n+  private def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * Post a single entity.\n+   *\n+   * Any network/connectivity errors will be caught and logged, and returned as the\n+   * exception field in the returned tuple.\n+   *\n+   * Any posting which generates a response will result in the timeline response being\n+   * returned. This response *may* contain errors; these are almost invariably going\n+   * to re-occur when resubmitted.\n+   *\n+   * @param entity entity to post\n+   * @return tuple with exactly one of the response or the exception set.\n+   */\n+  private def postOneEntity(entity: TimelineEntity):\n+      (Option[TimelinePutResponse], Option[Exception]) = {\n+    domainId.foreach {\n+      entity.setDomainId\n+    }\n+    val entityDescription = describeEntity(entity)\n+    logDebug(s\"About to POST $entityDescription\")\n+    _entityPostAttempts.incrementAndGet()\n+    try {\n+      val response = timelineClient.putEntities(entity)\n+      val errors = response.getErrors\n+      if (errors.isEmpty) {\n+        logDebug(s\"entity successfully posted\")\n+        _entityPostSuccesses.incrementAndGet()\n+      } else {\n+        // something went wrong.\n+        errors.asScala.foreach { err =>\n+          _entityPostFailures.incrementAndGet()\n+          logError(s\"Failed to post $entityDescription\\n:${describeError(err)}\")\n+        }\n+      }\n+      // whatever the outcome, this request is not re-issued\n+      (Some(response), None)\n+    } catch {\n+      case e: ConnectException =>\n+        _entityPostFailures.incrementAndGet\n+        logWarning(s\"Connection exception submitting $entityDescription\", e)\n+        (None, Some(e))\n+\n+      case e: RuntimeException =>\n+        // this is probably a retry timeout event; some Hadoop versions don't\n+        // rethrow the exception causing the problem, instead raising an RTE\n+        _entityPostFailures.incrementAndGet\n+        logWarning(s\"Runtime exception submitting $entityDescription\", e)\n+        // same policy as before: retry on these\n+        (None, Some(e))\n+\n+      case e: Exception =>\n+        // something else has gone wrong.\n+        _entityPostFailures.incrementAndGet\n+        logWarning(s\"Could not handle history entity: $entityDescription\", e)\n+        (None, Some(e))\n+    }\n+  }\n+\n+  /**\n+   * Spin and post until stopped.\n+   *\n+   * Algorithm.\n+   *\n+   * 1. The thread waits for events in the _entityQueue until stopped or interrupted\n+   * 1. Failures result in the entity being queued for resending, after a delay which grows\n+   * linearly on every retry.\n+   * 1. Successful posts reset the retry delay.\n+   * 1. If the process is interrupted, the loop continues with the `stopFlag` flag being checked.\n+   *\n+   * To stop this process then, first set the `stopFlag` flag, then interrupt the thread.\n+   *\n+   * @param stopFlag a flag to set to stop the loop.\n+   * @param retryInterval delay in milliseconds for the first retry delay; the delay increases\n+   *        by this value on every future failure. If zero, there is no delay, ever.\n+   */\n+  private def postEntities(stopFlag: AtomicBoolean, retryInterval: Long): Unit = {\n+    var lastAttemptFailed = false\n+    var currentRetryDelay = retryInterval\n+    while (!stopped.get) {\n+      try {\n+        val head = _entityQueue.take()\n+        val (_, ex) = postOneEntity(head)\n+        if (ex.isDefined && !stopped.get()) {\n+          // something went wrong and it wasn't being told to stop\n+          if (!lastAttemptFailed) {\n+            // avoid filling up logs with repeated failures\n+            logWarning(s\"Exception submitting entity to ${_timelineWebappAddress}\", ex.get)\n+          }\n+          // log failure and repost\n+          lastAttemptFailed = true\n+          currentRetryDelay += retryInterval\n+          _entityQueue.addFirst(head)\n+          if (currentRetryDelay > 0 ) {\n+            Thread.sleep(currentRetryDelay)\n+          }\n+        } else {\n+          // success\n+          lastAttemptFailed = false\n+          currentRetryDelay = retryInterval\n+        }\n+      } catch {\n+        case ex: InterruptedException =>\n+          // interrupted; this will break out of IO/Sleep operations and\n+          // trigger a rescan of the stopped() event.\n+          // The ATS code implements some of its own retry logic, which\n+          // can catch interrupts and convert to other exceptions â€”this catch clause picks up\n+          // interrupts outside of that code.\n+          logDebug(s\"Interrupted\", ex)\n+        case other: Exception =>\n+          throw other\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Shutdown phase: continually post oustanding entities until the timeout has been exceeded.\n+   * The interval between failures is the retryInterval: there is no escalation, and if\n+   * is longer than the remaining time in the shutdown, the remaining time sets the limit.\n+   * @param timeout timeout in milliseconds.\n+   * @param retryInterval delay in milliseconds for every delay.\n+   */\n+  private def postEntitiesShutdownPhase(timeout: Long, retryInterval: Long): Unit = {\n+    val timeLimit = now() + timeout;\n+    logDebug(s\"Entering shutdown post phase, time period = $timeout mS\")\n+    while (now() < timeLimit && !_entityQueue.isEmpty) {\n+\n+      try {\n+        val head = _entityQueue.poll(timeLimit - now(), TimeUnit.MILLISECONDS)\n+        if (head != null) {\n+          val (_, ex) = postOneEntity(head)\n+          if (ex.isDefined) {\n+            // failure, push back to try again\n+            _entityQueue.addFirst(head)\n+            if (retryInterval > 0) {\n+              Thread.sleep(Math.min(retryInterval, timeLimit - now()))\n+            }\n+          }\n+        }\n+      } catch {\n+        case ex: InterruptedException =>\n+          // ignoring, as this is shutdown time anyway\n+          logDebug(\"Ignoring Interrupt\", ex)\n+        case ex: Exception =>\n+          throw ex\n+      }\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic still applies\n+   */\n+  private def handleEvent(event: SparkListenerEvent): Unit = {\n+    var push = false\n+    var isLifecycleEvent = false;\n+    val timestamp = now()\n+    if (_eventsProcessed.incrementAndGet() % 1000 == 0) {\n+      logDebug(s\"${_eventsProcessed} events are processed\")\n+    }\n+    event match {\n+      case start: SparkListenerApplicationStart =>\n+        // we already have all information,\n+        // flush it for old one to switch to new one\n+        logDebug(s\"Handling application start event: $event\")\n+        if (!appStartEventProcessed.getAndSet(true)) {\n+          applicationStartEvent = Some(start)\n+          applicationName = start.appName\n+          if (applicationName == null || applicationName.isEmpty) {\n+            logWarning(\"Application does not have a name\")\n+            applicationName = applicationId.toString\n+          }\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {\n+            startTime = timestamp\n+          }\n+          setContextAppAndAttemptInfo(start.appId, start.appAttemptId)\n+          logInfo(s\"Application started: $event\")\n+          isLifecycleEvent = true\n+          push = true\n+        } else {\n+          logWarning(s\"More than one application start event received -ignoring: $start\")\n+        }\n+\n+      case end: SparkListenerApplicationEnd =>\n+        if (!appStartEventProcessed.get()) {\n+          logError(s\"Received application end event without application start $event\")\n+        } else if (!appEndEventProcessed.getAndSet(true)) {\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logInfo(s\"Application end event: $event\")\n+          applicationEndEvent = Some(end);"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "So this is a bit weird. `canAddEvent` checks `_entityQueue`, but `addPendingEvent` modifies `pendingEvents`. What's the queue that has a size limit?\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-28T21:41:29Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{TimeUnit, LinkedBlockingDeque}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{YarnExtensionService, YarnExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered\n+ * YARN Timeline Server.\n+ */\n+private[spark] class YarnHistoryService extends YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+  def enterState(state: Int): Int = {\n+    logDebug(s\"Entering state $state from $serviceState\")\n+    _serviceState.getAndSet(state)\n+  }\n+\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** YARN configuration from the spark context */\n+  private var config: YarnConfiguration = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var _timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Application name  from the spark start event */\n+  private var applicationName: String = _\n+\n+  /** Application ID*/\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName = Utils.getCurrentUserName()\n+\n+  /** Clock for recording time */\n+  private val clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /**\n+   * Start time of the application, as received in the end event.\n+   */\n+  private var endTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var _batchSize = DEFAULT_BATCH_SIZE\n+\n+  /** queue of entities to asynchronously post, plus the number of events in each entry */\n+  private var _entityQueue = new LinkedBlockingDeque[TimelineEntity]()\n+\n+  /** limit on the total number of events permitted */\n+  private var _postQueueLimit = DEFAULT_POST_QUEUE_LIMIT\n+\n+  /**\n+   * List of events which will be pulled into a timeline\n+   * entity when created\n+   */\n+  private var pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+\n+  private var applicationStartEvent: Option[SparkListenerApplicationStart] = None\n+  private var applicationEndEvent: Option[SparkListenerApplicationEnd] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val _eventsProcessed = new AtomicLong(0)\n+\n+  /** counter of events queued. */\n+  private val _eventsQueued = new AtomicLong(0)\n+\n+  private val _entityPostAttempts = new AtomicLong(0)\n+  private val _entityPostSuccesses = new AtomicLong(0)\n+  /** how many entity postings failed? */\n+  private val _entityPostFailures = new AtomicLong(0)\n+  private val _eventsDropped = new AtomicLong(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicLong(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped = new AtomicBoolean(true)\n+\n+  /**\n+   * Boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive = new AtomicBoolean(false)\n+\n+  /** How long to wait for shutdown before giving up */\n+  private var shutdownWaitTime = 0L\n+\n+  /**\n+   * What is the initial and incrementing interval for POST retries?\n+   */\n+  private var retryInterval = 0L\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: Option[String] = None\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var _timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private[yarn] def createTimelineClient(): TimelineClient = {\n+    require(_timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def timelineClient: TimelineClient = {\n+    require(_timelineClient.isDefined)\n+    _timelineClient.get\n+  }\n+\n+  /**\n+   * Get the total number of events dropped due to the queue of\n+   * outstanding posts being too long\n+   * @return counter of events processed\n+   */\n+\n+  def eventsDropped: Long = {\n+    _eventsDropped.get()\n+  }\n+\n+  /**\n+   * Get the total number of processed events, those handled in the back-end thread without\n+   * being rejected\n+   * @return counter of events processed\n+   */\n+  def eventsProcessed: Long = {\n+    _eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def eventsQueued: Long = {\n+    _eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    _entityQueue.size()\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def batchSize: Int = {\n+    _batchSize\n+  }\n+\n+  /**\n+   * Query the counter of attempts to post events\n+   * @return\n+   */\n+  def postAttempts: Long = _entityPostAttempts.get()\n+\n+  /**\n+   * Get the total number of failed post operations\n+   * @return counter of timeline post operations which failed\n+   */\n+  def postFailures: Long = {\n+    _entityPostFailures.get\n+  }\n+\n+  /**\n+   * Query the counter of post successes\n+   * @return the number of successful posts\n+   */\n+  def postSuccesses: Long = _entityPostSuccesses.get()\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId =  _applicationId\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] =  _attemptId\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    _timelineClient.foreach(_.stop())\n+    _timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain.\n+   *\n+   * A Timeline Domain is a uniquely identified 'namespace' for accessing parts of the timeline.\n+   * Security levels are are managed at the domain level, so one is created if the\n+   * spark acls are enabled. Full access is then granted to the current user,\n+   * all users in the configuration options `\"spark.modify.acls\"` and `\"spark.admin.acls\"`;\n+   * read access to those users and those listed in `\"spark.ui.view.acls\"`\n+   *\n+   * @return an optional domain string. If `None`, then no domain was created.\n+   */\n+  private def createTimelineDomain(): Option[String] = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsOn = sparkConf.getBoolean(\"spark.ui.acls.enable\",\n+        sparkConf.getBoolean(\"spark.acls.enable\", false))\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return None\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      logDebug(s\"Using predefined domain $predefDomain\")\n+      return predefDomain\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (Seq(current) ++ adminAcls ++ modifyAcls ++ viewAcls).mkString(\" \")\n+    val writers = (Seq(current) ++ adminAcls ++ modifyAcls).mkString(\" \")\n+    var domain = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domain with readers: $readers and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domain)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      timelineClient.putDomain(timelineDomain)\n+      Some(domain)\n+    } catch {\n+      case e: Exception => {\n+        logError(s\"cannot create the domain $domain\", e)\n+        // fallback to default\n+        None\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param binding binding to the spark application and YARN\n+   */\n+  override def start(binding: YarnExtensionServiceBinding): Unit = {\n+    val oldstate = enterState(StartedState)\n+    if (oldstate != CreatedState) {\n+      // state model violation\n+      _serviceState.set(oldstate)\n+      throw new IllegalArgumentException(s\"Cannot start the service from state $oldstate\")\n+    }\n+    val context = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    this.sparkContext = context\n+\n+    this.config = new YarnConfiguration(context.hadoopConfiguration)\n+\n+    val sparkConf = sparkContext.conf\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    _batchSize = sparkConf.getInt(BATCH_SIZE, _batchSize)\n+    _postQueueLimit = sparkConf.getInt(POST_QUEUE_LIMIT, _postQueueLimit)\n+    retryInterval = 1000 * sparkConf.getTimeAsSeconds(POST_RETRY_INTERVAL,\n+      DEFAULT_POST_RETRY_INTERVAL)\n+    shutdownWaitTime = 1000 * sparkConf.getTimeAsSeconds(SHUTDOWN_WAIT_TIME,\n+      DEFAULT_SHUTDOWN_WAIT_TIME)\n+\n+\n+    if (timelineServiceEnabled) {\n+      _timelineWebappAddress = getTimelineEndpoint(config)\n+\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at ${_timelineWebappAddress}\")\n+      _timelineClient = Some(createTimelineClient())\n+      domainId = createTimelineDomain()\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start()\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    if (registerListener()) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED` is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(config)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString(): String = {\n+    s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+    s\" state=$serviceState;\" +\n+    s\" endpoint=${_timelineWebappAddress};\" +\n+    s\" bonded to ATS=$bondedToATS;\" +\n+    s\" listening=$listening;\" +\n+    s\" batchSize=$batchSize;\" +\n+    s\" flush count=$getFlushCount;\" +\n+    s\" total number queued=$eventsQueued, processed=$eventsProcessed;\" +\n+    s\" attempted entity posts=$postAttempts\" +\n+    s\" successful entity posts=$postSuccesses\" +\n+    s\" failed entity posts=$postFailures;\" +\n+    s\" events dropped=$eventsDropped;\" +\n+    s\" app start event received=$appStartEventProcessed;\" +\n+    s\" app end event received=$appEndEventProcessed;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    _timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    _applicationId = appId\n+    _attemptId = attemptId\n+  }\n+\n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private[yarn] def registerListener(): Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param event event to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(event: SparkListenerEvent): Boolean = {\n+    if (!stopped.get) {\n+      _eventsQueued.incrementAndGet\n+      logDebug(s\"Enqueue $event\")\n+      handleEvent(event)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $event\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override def stop(): Unit = {\n+    val oldState = enterState(StoppedState)\n+    if (oldState != StartedState) {\n+      // stopping from a different state\n+      logDebug(s\"Ignoring stop() request from state $oldState\")\n+      return\n+    }\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (appStartEventProcessed.get && !appEndEventProcessed.get) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        enqueue(SparkListenerApplicationEnd(now()))\n+      }\n+\n+      // flush out the events\n+      asyncFlush()\n+\n+      // stop operation\n+      postThreadActive.synchronized {\n+        // now await that marker flag\n+        if (postThreadActive.get) {\n+          logDebug(s\"Stopping posting thread and waiting $shutdownWaitTime mS\")\n+          stopped.set(true)\n+          eventHandlingThread.foreach(_.interrupt())\n+          postThreadActive.wait(shutdownWaitTime)\n+        } else {\n+          stopTimelineClient()\n+          logInfo(s\"Stopped: $this\")\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Can an event be added?\n+   * Policy is only if the number of queued entities is below the limit, or the\n+   * event marks the end of the application.\n+   * @param isLifecycleEvent is this operation triggered by an application start/end?\n+   * @return true if the event can be added to the queue\n+   */\n+  private def canAddEvent(isLifecycleEvent: Boolean): Boolean = {\n+    _entityQueue.synchronized {\n+      _entityQueue.size() < _postQueueLimit  || isLifecycleEvent\n+    }\n+  }\n+\n+  /**\n+   * Add another event to the pending event list.\n+   * Returns the size of the event list after the event was added\n+   * (thread safe).\n+   * @param event event to add\n+   * @return the event list size\n+   */\n+  private def addPendingEvent(event: TimelineEvent): Int = {\n+    pendingEvents.synchronized {\n+      pendingEvents :+= event\n+      pendingEvents.size\n+    }\n+  }\n+\n+  /**\n+   * Publish next set of pending events.\n+   *\n+   * Builds the next event to push on the entity Queue; resets\n+   * the current [[pendingEvents]] list and then notifies\n+   * any listener of the [[_entityQueue]] that there is new data.\n+   */\n+  private def publishPendingEvents(): Boolean = {\n+    // verify that there are events to publish\n+    val size = pendingEvents.synchronized {\n+      pendingEvents.size\n+    }\n+    if (size > 0 && applicationStartEvent.isDefined) {\n+      flushCount.incrementAndGet()\n+      val timelineEntity = createTimelineEntity(\n+        applicationId,\n+        attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        applicationName,\n+        userName,\n+        startTime,\n+        endTime,\n+        now())\n+\n+      // copy in pending events and then clear the list\n+      pendingEvents.synchronized {\n+        pendingEvents.foreach(timelineEntity.addEvent)\n+        pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+      }\n+      // queue the entity for posting\n+      preflightCheck(timelineEntity)\n+      _entityQueue.synchronized {\n+        _entityQueue.push(timelineEntity)\n+      }\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   *\n+   * @param entity timeline entity to review.\n+   */\n+  private def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * Post a single entity.\n+   *\n+   * Any network/connectivity errors will be caught and logged, and returned as the\n+   * exception field in the returned tuple.\n+   *\n+   * Any posting which generates a response will result in the timeline response being\n+   * returned. This response *may* contain errors; these are almost invariably going\n+   * to re-occur when resubmitted.\n+   *\n+   * @param entity entity to post\n+   * @return tuple with exactly one of the response or the exception set.\n+   */\n+  private def postOneEntity(entity: TimelineEntity):\n+      (Option[TimelinePutResponse], Option[Exception]) = {\n+    domainId.foreach {\n+      entity.setDomainId\n+    }\n+    val entityDescription = describeEntity(entity)\n+    logDebug(s\"About to POST $entityDescription\")\n+    _entityPostAttempts.incrementAndGet()\n+    try {\n+      val response = timelineClient.putEntities(entity)\n+      val errors = response.getErrors\n+      if (errors.isEmpty) {\n+        logDebug(s\"entity successfully posted\")\n+        _entityPostSuccesses.incrementAndGet()\n+      } else {\n+        // something went wrong.\n+        errors.asScala.foreach { err =>\n+          _entityPostFailures.incrementAndGet()\n+          logError(s\"Failed to post $entityDescription\\n:${describeError(err)}\")\n+        }\n+      }\n+      // whatever the outcome, this request is not re-issued\n+      (Some(response), None)\n+    } catch {\n+      case e: ConnectException =>\n+        _entityPostFailures.incrementAndGet\n+        logWarning(s\"Connection exception submitting $entityDescription\", e)\n+        (None, Some(e))\n+\n+      case e: RuntimeException =>\n+        // this is probably a retry timeout event; some Hadoop versions don't\n+        // rethrow the exception causing the problem, instead raising an RTE\n+        _entityPostFailures.incrementAndGet\n+        logWarning(s\"Runtime exception submitting $entityDescription\", e)\n+        // same policy as before: retry on these\n+        (None, Some(e))\n+\n+      case e: Exception =>\n+        // something else has gone wrong.\n+        _entityPostFailures.incrementAndGet\n+        logWarning(s\"Could not handle history entity: $entityDescription\", e)\n+        (None, Some(e))\n+    }\n+  }\n+\n+  /**\n+   * Spin and post until stopped.\n+   *\n+   * Algorithm.\n+   *\n+   * 1. The thread waits for events in the _entityQueue until stopped or interrupted\n+   * 1. Failures result in the entity being queued for resending, after a delay which grows\n+   * linearly on every retry.\n+   * 1. Successful posts reset the retry delay.\n+   * 1. If the process is interrupted, the loop continues with the `stopFlag` flag being checked.\n+   *\n+   * To stop this process then, first set the `stopFlag` flag, then interrupt the thread.\n+   *\n+   * @param stopFlag a flag to set to stop the loop.\n+   * @param retryInterval delay in milliseconds for the first retry delay; the delay increases\n+   *        by this value on every future failure. If zero, there is no delay, ever.\n+   */\n+  private def postEntities(stopFlag: AtomicBoolean, retryInterval: Long): Unit = {\n+    var lastAttemptFailed = false\n+    var currentRetryDelay = retryInterval\n+    while (!stopped.get) {\n+      try {\n+        val head = _entityQueue.take()\n+        val (_, ex) = postOneEntity(head)\n+        if (ex.isDefined && !stopped.get()) {\n+          // something went wrong and it wasn't being told to stop\n+          if (!lastAttemptFailed) {\n+            // avoid filling up logs with repeated failures\n+            logWarning(s\"Exception submitting entity to ${_timelineWebappAddress}\", ex.get)\n+          }\n+          // log failure and repost\n+          lastAttemptFailed = true\n+          currentRetryDelay += retryInterval\n+          _entityQueue.addFirst(head)\n+          if (currentRetryDelay > 0 ) {\n+            Thread.sleep(currentRetryDelay)\n+          }\n+        } else {\n+          // success\n+          lastAttemptFailed = false\n+          currentRetryDelay = retryInterval\n+        }\n+      } catch {\n+        case ex: InterruptedException =>\n+          // interrupted; this will break out of IO/Sleep operations and\n+          // trigger a rescan of the stopped() event.\n+          // The ATS code implements some of its own retry logic, which\n+          // can catch interrupts and convert to other exceptions â€”this catch clause picks up\n+          // interrupts outside of that code.\n+          logDebug(s\"Interrupted\", ex)\n+        case other: Exception =>\n+          throw other\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Shutdown phase: continually post oustanding entities until the timeout has been exceeded.\n+   * The interval between failures is the retryInterval: there is no escalation, and if\n+   * is longer than the remaining time in the shutdown, the remaining time sets the limit.\n+   * @param timeout timeout in milliseconds.\n+   * @param retryInterval delay in milliseconds for every delay.\n+   */\n+  private def postEntitiesShutdownPhase(timeout: Long, retryInterval: Long): Unit = {\n+    val timeLimit = now() + timeout;\n+    logDebug(s\"Entering shutdown post phase, time period = $timeout mS\")\n+    while (now() < timeLimit && !_entityQueue.isEmpty) {\n+\n+      try {\n+        val head = _entityQueue.poll(timeLimit - now(), TimeUnit.MILLISECONDS)\n+        if (head != null) {\n+          val (_, ex) = postOneEntity(head)\n+          if (ex.isDefined) {\n+            // failure, push back to try again\n+            _entityQueue.addFirst(head)\n+            if (retryInterval > 0) {\n+              Thread.sleep(Math.min(retryInterval, timeLimit - now()))\n+            }\n+          }\n+        }\n+      } catch {\n+        case ex: InterruptedException =>\n+          // ignoring, as this is shutdown time anyway\n+          logDebug(\"Ignoring Interrupt\", ex)\n+        case ex: Exception =>\n+          throw ex\n+      }\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic still applies\n+   */\n+  private def handleEvent(event: SparkListenerEvent): Unit = {\n+    var push = false\n+    var isLifecycleEvent = false;\n+    val timestamp = now()\n+    if (_eventsProcessed.incrementAndGet() % 1000 == 0) {\n+      logDebug(s\"${_eventsProcessed} events are processed\")\n+    }\n+    event match {\n+      case start: SparkListenerApplicationStart =>\n+        // we already have all information,\n+        // flush it for old one to switch to new one\n+        logDebug(s\"Handling application start event: $event\")\n+        if (!appStartEventProcessed.getAndSet(true)) {\n+          applicationStartEvent = Some(start)\n+          applicationName = start.appName\n+          if (applicationName == null || applicationName.isEmpty) {\n+            logWarning(\"Application does not have a name\")\n+            applicationName = applicationId.toString\n+          }\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {\n+            startTime = timestamp\n+          }\n+          setContextAppAndAttemptInfo(start.appId, start.appAttemptId)\n+          logInfo(s\"Application started: $event\")\n+          isLifecycleEvent = true\n+          push = true\n+        } else {\n+          logWarning(s\"More than one application start event received -ignoring: $start\")\n+        }\n+\n+      case end: SparkListenerApplicationEnd =>\n+        if (!appStartEventProcessed.get()) {\n+          logError(s\"Received application end event without application start $event\")\n+        } else if (!appEndEventProcessed.getAndSet(true)) {\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logInfo(s\"Application end event: $event\")\n+          applicationEndEvent = Some(end);\n+          // flush old entity\n+          endTime = if (end.time > 0) end.time else timestamp\n+          push = true\n+          isLifecycleEvent = true\n+        } else {\n+          logInfo(s\"Discarding duplicate application end event $end\")\n+        }\n+\n+      case _ =>\n+    }\n+\n+    val tlEvent = toTimelineEvent(event, timestamp)\n+    val eventCount = if (canAddEvent(isLifecycleEvent)) {"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "This was a tricky one to code. I decided just to block on the number of queued postings, rather than the actual number of queued items. It's easier to measure (you don't need to count the length of postings), and it's a sign of post problems. \n\nI could switch to doing a count of the number of events in each posting, for a total number: but what would it gain? More succinctly: Is it better to discard excess events because \"we have reached our event limit\" or because \"we have too many pending post operations\"?\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-10-21T18:59:47Z",
    "diffHunk": "@@ -0,0 +1,1048 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicLong, AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{TimeUnit, LinkedBlockingDeque}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{YarnExtensionService, YarnExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered\n+ * YARN Timeline Server.\n+ */\n+private[spark] class YarnHistoryService extends YarnExtensionService with Logging {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+  def enterState(state: Int): Int = {\n+    logDebug(s\"Entering state $state from $serviceState\")\n+    _serviceState.getAndSet(state)\n+  }\n+\n+  /**\n+   * Spark context; valid once started\n+   */\n+  private var sparkContext: SparkContext = _\n+\n+  /** YARN configuration from the spark context */\n+  private var config: YarnConfiguration = _\n+\n+  /** application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** attempt ID this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var _timelineClient: Option[TimelineClient] = None\n+\n+  /** registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Application name  from the spark start event */\n+  private var applicationName: String = _\n+\n+  /** Application ID*/\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** user name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName = Utils.getCurrentUserName()\n+\n+  /** Clock for recording time */\n+  private val clock = new SystemClock()\n+\n+  /**\n+   * Start time of the application, as received in the start event.\n+   */\n+  private var startTime: Long = _\n+\n+  /**\n+   * Start time of the application, as received in the end event.\n+   */\n+  private var endTime: Long = _\n+\n+  /** number of events to batch up before posting*/\n+  private var _batchSize = DEFAULT_BATCH_SIZE\n+\n+  /** queue of entities to asynchronously post, plus the number of events in each entry */\n+  private var _entityQueue = new LinkedBlockingDeque[TimelineEntity]()\n+\n+  /** limit on the total number of events permitted */\n+  private var _postQueueLimit = DEFAULT_POST_QUEUE_LIMIT\n+\n+  /**\n+   * List of events which will be pulled into a timeline\n+   * entity when created\n+   */\n+  private var pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+\n+  private var applicationStartEvent: Option[SparkListenerApplicationStart] = None\n+  private var applicationEndEvent: Option[SparkListenerApplicationEnd] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /* has the application event event been processed */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** counter of events processed -that is have been through handleEvent()*/\n+  private val _eventsProcessed = new AtomicLong(0)\n+\n+  /** counter of events queued. */\n+  private val _eventsQueued = new AtomicLong(0)\n+\n+  private val _entityPostAttempts = new AtomicLong(0)\n+  private val _entityPostSuccesses = new AtomicLong(0)\n+  /** how many entity postings failed? */\n+  private val _entityPostFailures = new AtomicLong(0)\n+  private val _eventsDropped = new AtomicLong(0)\n+\n+  /** how many flushes have taken place? */\n+  private val flushCount = new AtomicLong(0)\n+\n+  /** Event handler */\n+  private var eventHandlingThread: Option[Thread] = None\n+\n+  /**\n+   * Flag to indicate the thread is stopped; events aren't being\n+   * processed.\n+   */\n+  private val stopped = new AtomicBoolean(true)\n+\n+  /**\n+   * Boolean to track whether a thread is active or not, for tests to\n+   * monitor and see if the thread has completed.\n+   */\n+  private val postThreadActive = new AtomicBoolean(false)\n+\n+  /** How long to wait for shutdown before giving up */\n+  private var shutdownWaitTime = 0L\n+\n+  /**\n+   * What is the initial and incrementing interval for POST retries?\n+   */\n+  private var retryInterval = 0L\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: Option[String] = None\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var _timelineWebappAddress: URI = _\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private[yarn] def createTimelineClient(): TimelineClient = {\n+    require(_timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def timelineClient: TimelineClient = {\n+    require(_timelineClient.isDefined)\n+    _timelineClient.get\n+  }\n+\n+  /**\n+   * Get the total number of events dropped due to the queue of\n+   * outstanding posts being too long\n+   * @return counter of events processed\n+   */\n+\n+  def eventsDropped: Long = {\n+    _eventsDropped.get()\n+  }\n+\n+  /**\n+   * Get the total number of processed events, those handled in the back-end thread without\n+   * being rejected\n+   * @return counter of events processed\n+   */\n+  def eventsProcessed: Long = {\n+    _eventsProcessed.get\n+  }\n+\n+  /**\n+   * Get the total number of events queued\n+   * @return the total event count\n+   */\n+  def eventsQueued: Long = {\n+    _eventsQueued.get\n+  }\n+\n+  /**\n+   * Get the current size of the queue\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    _entityQueue.size()\n+  }\n+\n+  /**\n+   * Get the current batch size\n+   * @return the batch size\n+   */\n+  def batchSize: Int = {\n+    _batchSize\n+  }\n+\n+  /**\n+   * Query the counter of attempts to post events\n+   * @return\n+   */\n+  def postAttempts: Long = _entityPostAttempts.get()\n+\n+  /**\n+   * Get the total number of failed post operations\n+   * @return counter of timeline post operations which failed\n+   */\n+  def postFailures: Long = {\n+    _entityPostFailures.get\n+  }\n+\n+  /**\n+   * Query the counter of post successes\n+   * @return the number of successful posts\n+   */\n+  def postSuccesses: Long = _entityPostSuccesses.get()\n+\n+  /**\n+   * is the asynchronous posting thread active?\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId =  _applicationId\n+\n+  /**\n+   * The YARN attempt ID of this history service\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] =  _attemptId\n+\n+  /**\n+   * Reset the timeline client\n+   * <p>\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    _timelineClient.foreach(_.stop())\n+    _timelineClient = None\n+  }\n+\n+  /**\n+   * Create the timeline domain.\n+   *\n+   * A Timeline Domain is a uniquely identified 'namespace' for accessing parts of the timeline.\n+   * Security levels are are managed at the domain level, so one is created if the\n+   * spark acls are enabled. Full access is then granted to the current user,\n+   * all users in the configuration options `\"spark.modify.acls\"` and `\"spark.admin.acls\"`;\n+   * read access to those users and those listed in `\"spark.ui.view.acls\"`\n+   *\n+   * @return an optional domain string. If `None`, then no domain was created.\n+   */\n+  private def createTimelineDomain(): Option[String] = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsOn = sparkConf.getBoolean(\"spark.ui.acls.enable\",\n+        sparkConf.getBoolean(\"spark.acls.enable\", false))\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return None\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      logDebug(s\"Using predefined domain $predefDomain\")\n+      return predefDomain\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls  = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (Seq(current) ++ adminAcls ++ modifyAcls ++ viewAcls).mkString(\" \")\n+    val writers = (Seq(current) ++ adminAcls ++ modifyAcls).mkString(\" \")\n+    var domain = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domain with readers: $readers and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domain)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      timelineClient.putDomain(timelineDomain)\n+      Some(domain)\n+    } catch {\n+      case e: Exception => {\n+        logError(s\"cannot create the domain $domain\", e)\n+        // fallback to default\n+        None\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Start the service, calling the service's `init()` and `start()` actions in the\n+   * correct order\n+   * @param binding binding to the spark application and YARN\n+   */\n+  override def start(binding: YarnExtensionServiceBinding): Unit = {\n+    val oldstate = enterState(StartedState)\n+    if (oldstate != CreatedState) {\n+      // state model violation\n+      _serviceState.set(oldstate)\n+      throw new IllegalArgumentException(s\"Cannot start the service from state $oldstate\")\n+    }\n+    val context = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    this.sparkContext = context\n+\n+    this.config = new YarnConfiguration(context.hadoopConfiguration)\n+\n+    val sparkConf = sparkContext.conf\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    // this is assumed by the AM, which uses it when creating a path to an attempt\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    _batchSize = sparkConf.getInt(BATCH_SIZE, _batchSize)\n+    _postQueueLimit = sparkConf.getInt(POST_QUEUE_LIMIT, _postQueueLimit)\n+    retryInterval = 1000 * sparkConf.getTimeAsSeconds(POST_RETRY_INTERVAL,\n+      DEFAULT_POST_RETRY_INTERVAL)\n+    shutdownWaitTime = 1000 * sparkConf.getTimeAsSeconds(SHUTDOWN_WAIT_TIME,\n+      DEFAULT_SHUTDOWN_WAIT_TIME)\n+\n+\n+    if (timelineServiceEnabled) {\n+      _timelineWebappAddress = getTimelineEndpoint(config)\n+\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service at ${_timelineWebappAddress}\")\n+      _timelineClient = Some(createTimelineClient())\n+      domainId = createTimelineDomain()\n+      // declare that the processing is started\n+      stopped.set(false)\n+      eventHandlingThread = Some(new Thread(new Dequeue(), \"HistoryEventHandlingThread\"))\n+      eventHandlingThread.get.start()\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    if (registerListener()) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED` is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(config)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production\n+   * @return a summary of the current service state\n+   */\n+  override def toString(): String = {\n+    s\"YarnHistoryService for application $applicationId attempt $attemptId;\" +\n+    s\" state=$serviceState;\" +\n+    s\" endpoint=${_timelineWebappAddress};\" +\n+    s\" bonded to ATS=$bondedToATS;\" +\n+    s\" listening=$listening;\" +\n+    s\" batchSize=$batchSize;\" +\n+    s\" flush count=$getFlushCount;\" +\n+    s\" total number queued=$eventsQueued, processed=$eventsProcessed;\" +\n+    s\" attempted entity posts=$postAttempts\" +\n+    s\" successful entity posts=$postSuccesses\" +\n+    s\" failed entity posts=$postFailures;\" +\n+    s\" events dropped=$eventsDropped;\" +\n+    s\" app start event received=$appStartEventProcessed;\" +\n+    s\" app end event received=$appEndEventProcessed;\"\n+  }\n+\n+  /**\n+   * Is the service listening to events from the spark context?\n+   * @return true if it has registered as a listener\n+   */\n+  def listening: Boolean = {\n+    listener.isDefined\n+  }\n+\n+  /**\n+   * Is the service hooked up to an ATS server. This does not\n+   * check the validity of the link, only whether or not the service\n+   * has been set up to talk to ATS.\n+   * @return true if the service has a timeline client\n+   */\n+  def bondedToATS: Boolean = {\n+    _timelineClient.isDefined\n+  }\n+\n+  /**\n+   * Set the YARN binding information. This is called during startup. It is private\n+   * to the package so that tests may update this data\n+   * @param appId YARN application ID\n+   * @param attemptId optional attempt ID\n+   */\n+  private[yarn] def bindToYarnApplication(appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Unit = {\n+    require(appId != null, \"Null appId parameter\")\n+    _applicationId = appId\n+    _attemptId = attemptId\n+  }\n+\n+  /**\n+   * Set the \"spark\" application and attempt information -the information\n+   * provided in the start event. The attempt ID here may be `None`; even\n+   * if set it may only be unique amongst the attempts of this application.\n+   * That is: not unique enough to be used as the entity ID\n+   * @param appId application ID\n+   * @param attemptId attempt ID\n+   */\n+  private def setContextAppAndAttemptInfo(appId: Option[String],\n+      attemptId: Option[String]): Unit = {\n+    logDebug(s\"Setting application ID to $appId; attempt ID to $attemptId\")\n+    sparkApplicationId = appId\n+    sparkApplicationAttemptId = attemptId\n+  }\n+\n+  /**\n+   * Add the listener if it is not disabled.\n+   * This is accessible in the same package purely for testing\n+   * @return true if the register was enabled\n+   */\n+  private[yarn] def registerListener(): Boolean = {\n+    assert(sparkContext != null, \"Null context\")\n+    if (sparkContext.conf.getBoolean(REGISTER_LISTENER, true)) {\n+      logDebug(\"Registering listener to spark context\")\n+      val l = new YarnEventListener(sparkContext, this)\n+      listener = Some(l)\n+      sparkContext.listenerBus.addListener(l)\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Queue an action, or if the service's `stopped` flag\n+   * is set, discard it\n+   * @param event event to process\n+   * @return true if the event was queued\n+   */\n+  def enqueue(event: SparkListenerEvent): Boolean = {\n+    if (!stopped.get) {\n+      _eventsQueued.incrementAndGet\n+      logDebug(s\"Enqueue $event\")\n+      handleEvent(event)\n+      true\n+    } else {\n+      if (timelineServiceEnabled) {\n+        // if the timeline service was ever enabled, log the fact the event\n+        // is being discarded. Don't do this if it was not, as it will\n+        // only make the logs noisy.\n+        logInfo(s\"History service stopped; ignoring queued event : $event\")\n+      }\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Stop the service; this triggers flushing the queue and, if not already processed,\n+   * a pushing out of an application end event.\n+   *\n+   * This operation will block for up to `maxTimeToWaitOnShutdown` milliseconds\n+   * to await the asynchronous action queue completing.\n+   */\n+  override def stop(): Unit = {\n+    val oldState = enterState(StoppedState)\n+    if (oldState != StartedState) {\n+      // stopping from a different state\n+      logDebug(s\"Ignoring stop() request from state $oldState\")\n+      return\n+    }\n+    // if the queue is live\n+    if (!stopped.get) {\n+\n+      if (appStartEventProcessed.get && !appEndEventProcessed.get) {\n+        // push out an application stop event if none has been received\n+        logDebug(\"Generating a SparkListenerApplicationEnd during service stop()\")\n+        enqueue(SparkListenerApplicationEnd(now()))\n+      }\n+\n+      // flush out the events\n+      asyncFlush()\n+\n+      // stop operation\n+      postThreadActive.synchronized {\n+        // now await that marker flag\n+        if (postThreadActive.get) {\n+          logDebug(s\"Stopping posting thread and waiting $shutdownWaitTime mS\")\n+          stopped.set(true)\n+          eventHandlingThread.foreach(_.interrupt())\n+          postThreadActive.wait(shutdownWaitTime)\n+        } else {\n+          stopTimelineClient()\n+          logInfo(s\"Stopped: $this\")\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Can an event be added?\n+   * Policy is only if the number of queued entities is below the limit, or the\n+   * event marks the end of the application.\n+   * @param isLifecycleEvent is this operation triggered by an application start/end?\n+   * @return true if the event can be added to the queue\n+   */\n+  private def canAddEvent(isLifecycleEvent: Boolean): Boolean = {\n+    _entityQueue.synchronized {\n+      _entityQueue.size() < _postQueueLimit  || isLifecycleEvent\n+    }\n+  }\n+\n+  /**\n+   * Add another event to the pending event list.\n+   * Returns the size of the event list after the event was added\n+   * (thread safe).\n+   * @param event event to add\n+   * @return the event list size\n+   */\n+  private def addPendingEvent(event: TimelineEvent): Int = {\n+    pendingEvents.synchronized {\n+      pendingEvents :+= event\n+      pendingEvents.size\n+    }\n+  }\n+\n+  /**\n+   * Publish next set of pending events.\n+   *\n+   * Builds the next event to push on the entity Queue; resets\n+   * the current [[pendingEvents]] list and then notifies\n+   * any listener of the [[_entityQueue]] that there is new data.\n+   */\n+  private def publishPendingEvents(): Boolean = {\n+    // verify that there are events to publish\n+    val size = pendingEvents.synchronized {\n+      pendingEvents.size\n+    }\n+    if (size > 0 && applicationStartEvent.isDefined) {\n+      flushCount.incrementAndGet()\n+      val timelineEntity = createTimelineEntity(\n+        applicationId,\n+        attemptId,\n+        sparkApplicationId,\n+        sparkApplicationAttemptId,\n+        applicationName,\n+        userName,\n+        startTime,\n+        endTime,\n+        now())\n+\n+      // copy in pending events and then clear the list\n+      pendingEvents.synchronized {\n+        pendingEvents.foreach(timelineEntity.addEvent)\n+        pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+      }\n+      // queue the entity for posting\n+      preflightCheck(timelineEntity)\n+      _entityQueue.synchronized {\n+        _entityQueue.push(timelineEntity)\n+      }\n+      true\n+    } else {\n+      false\n+    }\n+  }\n+\n+  /**\n+   * Perform any preflight checks.\n+   *\n+   * @param entity timeline entity to review.\n+   */\n+  private def preflightCheck(entity: TimelineEntity): Unit = {\n+    require(entity.getStartTime != null,\n+      s\"No start time in ${describeEntity(entity)}\")\n+  }\n+\n+  /**\n+   * Post a single entity.\n+   *\n+   * Any network/connectivity errors will be caught and logged, and returned as the\n+   * exception field in the returned tuple.\n+   *\n+   * Any posting which generates a response will result in the timeline response being\n+   * returned. This response *may* contain errors; these are almost invariably going\n+   * to re-occur when resubmitted.\n+   *\n+   * @param entity entity to post\n+   * @return tuple with exactly one of the response or the exception set.\n+   */\n+  private def postOneEntity(entity: TimelineEntity):\n+      (Option[TimelinePutResponse], Option[Exception]) = {\n+    domainId.foreach {\n+      entity.setDomainId\n+    }\n+    val entityDescription = describeEntity(entity)\n+    logDebug(s\"About to POST $entityDescription\")\n+    _entityPostAttempts.incrementAndGet()\n+    try {\n+      val response = timelineClient.putEntities(entity)\n+      val errors = response.getErrors\n+      if (errors.isEmpty) {\n+        logDebug(s\"entity successfully posted\")\n+        _entityPostSuccesses.incrementAndGet()\n+      } else {\n+        // something went wrong.\n+        errors.asScala.foreach { err =>\n+          _entityPostFailures.incrementAndGet()\n+          logError(s\"Failed to post $entityDescription\\n:${describeError(err)}\")\n+        }\n+      }\n+      // whatever the outcome, this request is not re-issued\n+      (Some(response), None)\n+    } catch {\n+      case e: ConnectException =>\n+        _entityPostFailures.incrementAndGet\n+        logWarning(s\"Connection exception submitting $entityDescription\", e)\n+        (None, Some(e))\n+\n+      case e: RuntimeException =>\n+        // this is probably a retry timeout event; some Hadoop versions don't\n+        // rethrow the exception causing the problem, instead raising an RTE\n+        _entityPostFailures.incrementAndGet\n+        logWarning(s\"Runtime exception submitting $entityDescription\", e)\n+        // same policy as before: retry on these\n+        (None, Some(e))\n+\n+      case e: Exception =>\n+        // something else has gone wrong.\n+        _entityPostFailures.incrementAndGet\n+        logWarning(s\"Could not handle history entity: $entityDescription\", e)\n+        (None, Some(e))\n+    }\n+  }\n+\n+  /**\n+   * Spin and post until stopped.\n+   *\n+   * Algorithm.\n+   *\n+   * 1. The thread waits for events in the _entityQueue until stopped or interrupted\n+   * 1. Failures result in the entity being queued for resending, after a delay which grows\n+   * linearly on every retry.\n+   * 1. Successful posts reset the retry delay.\n+   * 1. If the process is interrupted, the loop continues with the `stopFlag` flag being checked.\n+   *\n+   * To stop this process then, first set the `stopFlag` flag, then interrupt the thread.\n+   *\n+   * @param stopFlag a flag to set to stop the loop.\n+   * @param retryInterval delay in milliseconds for the first retry delay; the delay increases\n+   *        by this value on every future failure. If zero, there is no delay, ever.\n+   */\n+  private def postEntities(stopFlag: AtomicBoolean, retryInterval: Long): Unit = {\n+    var lastAttemptFailed = false\n+    var currentRetryDelay = retryInterval\n+    while (!stopped.get) {\n+      try {\n+        val head = _entityQueue.take()\n+        val (_, ex) = postOneEntity(head)\n+        if (ex.isDefined && !stopped.get()) {\n+          // something went wrong and it wasn't being told to stop\n+          if (!lastAttemptFailed) {\n+            // avoid filling up logs with repeated failures\n+            logWarning(s\"Exception submitting entity to ${_timelineWebappAddress}\", ex.get)\n+          }\n+          // log failure and repost\n+          lastAttemptFailed = true\n+          currentRetryDelay += retryInterval\n+          _entityQueue.addFirst(head)\n+          if (currentRetryDelay > 0 ) {\n+            Thread.sleep(currentRetryDelay)\n+          }\n+        } else {\n+          // success\n+          lastAttemptFailed = false\n+          currentRetryDelay = retryInterval\n+        }\n+      } catch {\n+        case ex: InterruptedException =>\n+          // interrupted; this will break out of IO/Sleep operations and\n+          // trigger a rescan of the stopped() event.\n+          // The ATS code implements some of its own retry logic, which\n+          // can catch interrupts and convert to other exceptions â€”this catch clause picks up\n+          // interrupts outside of that code.\n+          logDebug(s\"Interrupted\", ex)\n+        case other: Exception =>\n+          throw other\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Shutdown phase: continually post oustanding entities until the timeout has been exceeded.\n+   * The interval between failures is the retryInterval: there is no escalation, and if\n+   * is longer than the remaining time in the shutdown, the remaining time sets the limit.\n+   * @param timeout timeout in milliseconds.\n+   * @param retryInterval delay in milliseconds for every delay.\n+   */\n+  private def postEntitiesShutdownPhase(timeout: Long, retryInterval: Long): Unit = {\n+    val timeLimit = now() + timeout;\n+    logDebug(s\"Entering shutdown post phase, time period = $timeout mS\")\n+    while (now() < timeLimit && !_entityQueue.isEmpty) {\n+\n+      try {\n+        val head = _entityQueue.poll(timeLimit - now(), TimeUnit.MILLISECONDS)\n+        if (head != null) {\n+          val (_, ex) = postOneEntity(head)\n+          if (ex.isDefined) {\n+            // failure, push back to try again\n+            _entityQueue.addFirst(head)\n+            if (retryInterval > 0) {\n+              Thread.sleep(Math.min(retryInterval, timeLimit - now()))\n+            }\n+          }\n+        }\n+      } catch {\n+        case ex: InterruptedException =>\n+          // ignoring, as this is shutdown time anyway\n+          logDebug(\"Ignoring Interrupt\", ex)\n+        case ex: Exception =>\n+          throw ex\n+      }\n+    }\n+  }\n+\n+  /**\n+   * If the event reaches the batch size or flush is true, push events to ATS.\n+   *\n+   * @param event event. If null no event is queued, but the post-queue flush logic still applies\n+   */\n+  private def handleEvent(event: SparkListenerEvent): Unit = {\n+    var push = false\n+    var isLifecycleEvent = false;\n+    val timestamp = now()\n+    if (_eventsProcessed.incrementAndGet() % 1000 == 0) {\n+      logDebug(s\"${_eventsProcessed} events are processed\")\n+    }\n+    event match {\n+      case start: SparkListenerApplicationStart =>\n+        // we already have all information,\n+        // flush it for old one to switch to new one\n+        logDebug(s\"Handling application start event: $event\")\n+        if (!appStartEventProcessed.getAndSet(true)) {\n+          applicationStartEvent = Some(start)\n+          applicationName = start.appName\n+          if (applicationName == null || applicationName.isEmpty) {\n+            logWarning(\"Application does not have a name\")\n+            applicationName = applicationId.toString\n+          }\n+          userName = start.sparkUser\n+          startTime = start.time\n+          if (startTime == 0) {\n+            startTime = timestamp\n+          }\n+          setContextAppAndAttemptInfo(start.appId, start.appAttemptId)\n+          logInfo(s\"Application started: $event\")\n+          isLifecycleEvent = true\n+          push = true\n+        } else {\n+          logWarning(s\"More than one application start event received -ignoring: $start\")\n+        }\n+\n+      case end: SparkListenerApplicationEnd =>\n+        if (!appStartEventProcessed.get()) {\n+          logError(s\"Received application end event without application start $event\")\n+        } else if (!appEndEventProcessed.getAndSet(true)) {\n+          // we already have all information,\n+          // flush it for old one to switch to new one\n+          logInfo(s\"Application end event: $event\")\n+          applicationEndEvent = Some(end);\n+          // flush old entity\n+          endTime = if (end.time > 0) end.time else timestamp\n+          push = true\n+          isLifecycleEvent = true\n+        } else {\n+          logInfo(s\"Discarding duplicate application end event $end\")\n+        }\n+\n+      case _ =>\n+    }\n+\n+    val tlEvent = toTimelineEvent(event, timestamp)\n+    val eventCount = if (canAddEvent(isLifecycleEvent)) {"
  }],
  "prId": 8744
}]