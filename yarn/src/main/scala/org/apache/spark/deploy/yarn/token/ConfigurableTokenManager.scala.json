[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "I'd rather use `java.util.ServiceLoader` for this. You'll need something like that at some point anyway, to support other token providers. Doing that now has the extra benefit of using the same code for built-in and third party providers.\n",
    "commit": "bce8cd6a5796fc95a432c169c8c40bea552382f0",
    "createdAt": "2016-07-07T21:19:26Z",
    "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn.token\n+\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.Token\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A [[ConfigurableTokenManager]] to manage all the token providers register in this class. Also\n+ * it provides other modules the functionality to obtain tokens, get token renewal interval and\n+ * calculate the time length till next renewal.\n+ *\n+ * By default ConfigurableTokenManager has 3 built-in token providers, HDFSTokenProvider,\n+ * HiveTokenProvider and HBaseTokenProvider, and this 3 token providers can also be controlled\n+ * by configuration spark.yarn.security.tokens.{service}.enabled, if it is set to false, this\n+ * provider will not be loaded.\n+ *\n+ * For other token providers which need to be loaded in should:\n+ * 1. Implement [[ServiceTokenProvider]] or [[ServiceTokenRenewable]] if token renewal is\n+ * required for this service.\n+ * 2. set spark.yarn.security.tokens.{service}.enabled to true\n+ * 3. Specify the class name through spark.yarn.security.tokens.{service}.class\n+ *\n+ */\n+final class ConfigurableTokenManager private[yarn] (sparkConf: SparkConf) extends Logging {\n+  private val tokenProviderEnabledConfig = \"spark\\\\.yarn\\\\.security\\\\.tokens\\\\.(.+)\\\\.enabled\".r\n+  private val tokenProviderClsConfig = \"spark.yarn.security.tokens.%s.class\"\n+\n+  // Maintain all the registered token providers\n+  private val tokenProviders = mutable.HashMap[String, ServiceTokenProvider]()\n+\n+  private val defaultTokenProviders = Map("
  }],
  "prId": 14065
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "A lot of this method would go away by using `java.util.ServiceLoader`. It's fine to instantiate the token providers even if they're disabled - just filter the final list to only include the enabled providers.\n",
    "commit": "bce8cd6a5796fc95a432c169c8c40bea552382f0",
    "createdAt": "2016-07-07T21:21:31Z",
    "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn.token\n+\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.Token\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A [[ConfigurableTokenManager]] to manage all the token providers register in this class. Also\n+ * it provides other modules the functionality to obtain tokens, get token renewal interval and\n+ * calculate the time length till next renewal.\n+ *\n+ * By default ConfigurableTokenManager has 3 built-in token providers, HDFSTokenProvider,\n+ * HiveTokenProvider and HBaseTokenProvider, and this 3 token providers can also be controlled\n+ * by configuration spark.yarn.security.tokens.{service}.enabled, if it is set to false, this\n+ * provider will not be loaded.\n+ *\n+ * For other token providers which need to be loaded in should:\n+ * 1. Implement [[ServiceTokenProvider]] or [[ServiceTokenRenewable]] if token renewal is\n+ * required for this service.\n+ * 2. set spark.yarn.security.tokens.{service}.enabled to true\n+ * 3. Specify the class name through spark.yarn.security.tokens.{service}.class\n+ *\n+ */\n+final class ConfigurableTokenManager private[yarn] (sparkConf: SparkConf) extends Logging {\n+  private val tokenProviderEnabledConfig = \"spark\\\\.yarn\\\\.security\\\\.tokens\\\\.(.+)\\\\.enabled\".r\n+  private val tokenProviderClsConfig = \"spark.yarn.security.tokens.%s.class\"\n+\n+  // Maintain all the registered token providers\n+  private val tokenProviders = mutable.HashMap[String, ServiceTokenProvider]()\n+\n+  private val defaultTokenProviders = Map(\n+    \"hdfs\" -> \"org.apache.spark.deploy.yarn.token.HDFSTokenProvider\",\n+    \"hive\" -> \"org.apache.spark.deploy.yarn.token.HiveTokenProvider\",\n+    \"hbase\" -> \"org.apache.spark.deploy.yarn.token.HBaseTokenProvider\"\n+  )\n+\n+  // AMDelegationTokenRenewer, this will only be create and started in the AM\n+  private var _delegationTokenRenewer: AMDelegationTokenRenewer = null\n+\n+  // ExecutorDelegationTokenUpdater, this will only be created and started in the driver and\n+  // executor side.\n+  private var _delegationTokenUpdater: ExecutorDelegationTokenUpdater = null\n+\n+  def initialize(): Unit = {"
  }],
  "prId": 14065
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "minor: you can use `flatMap` instead of a mutable buffer.\n",
    "commit": "bce8cd6a5796fc95a432c169c8c40bea552382f0",
    "createdAt": "2016-07-07T21:23:34Z",
    "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn.token\n+\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.Token\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A [[ConfigurableTokenManager]] to manage all the token providers register in this class. Also\n+ * it provides other modules the functionality to obtain tokens, get token renewal interval and\n+ * calculate the time length till next renewal.\n+ *\n+ * By default ConfigurableTokenManager has 3 built-in token providers, HDFSTokenProvider,\n+ * HiveTokenProvider and HBaseTokenProvider, and this 3 token providers can also be controlled\n+ * by configuration spark.yarn.security.tokens.{service}.enabled, if it is set to false, this\n+ * provider will not be loaded.\n+ *\n+ * For other token providers which need to be loaded in should:\n+ * 1. Implement [[ServiceTokenProvider]] or [[ServiceTokenRenewable]] if token renewal is\n+ * required for this service.\n+ * 2. set spark.yarn.security.tokens.{service}.enabled to true\n+ * 3. Specify the class name through spark.yarn.security.tokens.{service}.class\n+ *\n+ */\n+final class ConfigurableTokenManager private[yarn] (sparkConf: SparkConf) extends Logging {\n+  private val tokenProviderEnabledConfig = \"spark\\\\.yarn\\\\.security\\\\.tokens\\\\.(.+)\\\\.enabled\".r\n+  private val tokenProviderClsConfig = \"spark.yarn.security.tokens.%s.class\"\n+\n+  // Maintain all the registered token providers\n+  private val tokenProviders = mutable.HashMap[String, ServiceTokenProvider]()\n+\n+  private val defaultTokenProviders = Map(\n+    \"hdfs\" -> \"org.apache.spark.deploy.yarn.token.HDFSTokenProvider\",\n+    \"hive\" -> \"org.apache.spark.deploy.yarn.token.HiveTokenProvider\",\n+    \"hbase\" -> \"org.apache.spark.deploy.yarn.token.HBaseTokenProvider\"\n+  )\n+\n+  // AMDelegationTokenRenewer, this will only be create and started in the AM\n+  private var _delegationTokenRenewer: AMDelegationTokenRenewer = null\n+\n+  // ExecutorDelegationTokenUpdater, this will only be created and started in the driver and\n+  // executor side.\n+  private var _delegationTokenUpdater: ExecutorDelegationTokenUpdater = null\n+\n+  def initialize(): Unit = {\n+    // Copy SparkConf and add default enabled token provider configurations to SparkConf.\n+    val clonedConf = sparkConf.clone\n+    defaultTokenProviders.keys.foreach { key =>\n+      clonedConf.setIfMissing(s\"spark.yarn.security.tokens.$key.enabled\", \"true\")\n+    }\n+\n+    // Instantialize all the service token providers according to the configurations.\n+    clonedConf.getAll.filter { case (key, value) =>\n+      if (tokenProviderEnabledConfig.findPrefixOf(key).isDefined) {\n+        value.toBoolean\n+      } else {\n+        false\n+      }\n+    }.map { case (key, _) =>\n+      val tokenProviderEnabledConfig(service) = key\n+      val cls = sparkConf.getOption(tokenProviderClsConfig.format(service))\n+        .orElse(defaultTokenProviders.get(service))\n+      (service, cls)\n+    }.foreach { case (service, cls) =>\n+      if (cls.isDefined) {\n+        try {\n+          val tokenProvider =\n+            Utils.classForName(cls.get).newInstance().asInstanceOf[ServiceTokenProvider]\n+          tokenProviders += (service -> tokenProvider)\n+        } catch {\n+          case NonFatal(e) =>\n+            logWarning(s\"Fail to instantiate class ${cls.get}\", e)\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get service token provider by name.\n+   */\n+  def getServiceTokenProvider(service: String): Option[ServiceTokenProvider] = {\n+    tokenProviders.get(service)\n+  }\n+\n+  /**\n+   * Obtain tokens from all the token providers and add into credentials, also return as an array.\n+   */\n+  def obtainTokens(conf: Configuration, creds: Credentials): Array[Token[_]] = {\n+    val tokenBuf = mutable.ArrayBuffer[Token[_]]()\n+    tokenProviders.values.foreach { provider =>"
  }],
  "prId": 14065
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Seems like you could remote providers that don't require tokens during `initialize` if you make a `SparkConf` available to it. Then you could avoid passing `SparkConf` to all of these methods.\n\nYou could also make `Configuration` a constructor args for the same reasons.\n",
    "commit": "bce8cd6a5796fc95a432c169c8c40bea552382f0",
    "createdAt": "2016-07-07T21:25:41Z",
    "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn.token\n+\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.Credentials\n+import org.apache.hadoop.security.token.Token\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A [[ConfigurableTokenManager]] to manage all the token providers register in this class. Also\n+ * it provides other modules the functionality to obtain tokens, get token renewal interval and\n+ * calculate the time length till next renewal.\n+ *\n+ * By default ConfigurableTokenManager has 3 built-in token providers, HDFSTokenProvider,\n+ * HiveTokenProvider and HBaseTokenProvider, and this 3 token providers can also be controlled\n+ * by configuration spark.yarn.security.tokens.{service}.enabled, if it is set to false, this\n+ * provider will not be loaded.\n+ *\n+ * For other token providers which need to be loaded in should:\n+ * 1. Implement [[ServiceTokenProvider]] or [[ServiceTokenRenewable]] if token renewal is\n+ * required for this service.\n+ * 2. set spark.yarn.security.tokens.{service}.enabled to true\n+ * 3. Specify the class name through spark.yarn.security.tokens.{service}.class\n+ *\n+ */\n+final class ConfigurableTokenManager private[yarn] (sparkConf: SparkConf) extends Logging {\n+  private val tokenProviderEnabledConfig = \"spark\\\\.yarn\\\\.security\\\\.tokens\\\\.(.+)\\\\.enabled\".r\n+  private val tokenProviderClsConfig = \"spark.yarn.security.tokens.%s.class\"\n+\n+  // Maintain all the registered token providers\n+  private val tokenProviders = mutable.HashMap[String, ServiceTokenProvider]()\n+\n+  private val defaultTokenProviders = Map(\n+    \"hdfs\" -> \"org.apache.spark.deploy.yarn.token.HDFSTokenProvider\",\n+    \"hive\" -> \"org.apache.spark.deploy.yarn.token.HiveTokenProvider\",\n+    \"hbase\" -> \"org.apache.spark.deploy.yarn.token.HBaseTokenProvider\"\n+  )\n+\n+  // AMDelegationTokenRenewer, this will only be create and started in the AM\n+  private var _delegationTokenRenewer: AMDelegationTokenRenewer = null\n+\n+  // ExecutorDelegationTokenUpdater, this will only be created and started in the driver and\n+  // executor side.\n+  private var _delegationTokenUpdater: ExecutorDelegationTokenUpdater = null\n+\n+  def initialize(): Unit = {\n+    // Copy SparkConf and add default enabled token provider configurations to SparkConf.\n+    val clonedConf = sparkConf.clone\n+    defaultTokenProviders.keys.foreach { key =>\n+      clonedConf.setIfMissing(s\"spark.yarn.security.tokens.$key.enabled\", \"true\")\n+    }\n+\n+    // Instantialize all the service token providers according to the configurations.\n+    clonedConf.getAll.filter { case (key, value) =>\n+      if (tokenProviderEnabledConfig.findPrefixOf(key).isDefined) {\n+        value.toBoolean\n+      } else {\n+        false\n+      }\n+    }.map { case (key, _) =>\n+      val tokenProviderEnabledConfig(service) = key\n+      val cls = sparkConf.getOption(tokenProviderClsConfig.format(service))\n+        .orElse(defaultTokenProviders.get(service))\n+      (service, cls)\n+    }.foreach { case (service, cls) =>\n+      if (cls.isDefined) {\n+        try {\n+          val tokenProvider =\n+            Utils.classForName(cls.get).newInstance().asInstanceOf[ServiceTokenProvider]\n+          tokenProviders += (service -> tokenProvider)\n+        } catch {\n+          case NonFatal(e) =>\n+            logWarning(s\"Fail to instantiate class ${cls.get}\", e)\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get service token provider by name.\n+   */\n+  def getServiceTokenProvider(service: String): Option[ServiceTokenProvider] = {\n+    tokenProviders.get(service)\n+  }\n+\n+  /**\n+   * Obtain tokens from all the token providers and add into credentials, also return as an array.\n+   */\n+  def obtainTokens(conf: Configuration, creds: Credentials): Array[Token[_]] = {\n+    val tokenBuf = mutable.ArrayBuffer[Token[_]]()\n+    tokenProviders.values.foreach { provider =>\n+      if (provider.isTokenRequired(conf)) {"
  }],
  "prId": 14065
}]