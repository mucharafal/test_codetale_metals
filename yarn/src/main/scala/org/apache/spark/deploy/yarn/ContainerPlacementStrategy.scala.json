[{
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "Can we put all the information that's used to determine the locality requests as arguments to this function?  It will make it clearer for later readers to understand what's used in this calculation.\n",
    "commit": "d45fecb50aecff0c0bfeb627ec1ac8a4fc9e9644",
    "createdAt": "2015-06-18T20:51:57Z",
    "diffHunk": "@@ -0,0 +1,203 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.util.RackResolver\n+\n+import org.apache.spark.SparkConf\n+\n+private[yarn] trait ContainerPlacementStrategy {\n+  type Locality = Array[String]\n+\n+  /**\n+   * Calculate each container's node locality and rack locality\n+   * @param numContainer number of containers to calculate\n+   * @return node localities and rack localities, each locality is an array of string,\n+   *         the length of localities is the same as number of containers\n+   */\n+  def localityOfRequestedContainers(numContainer: Int): (Array[Locality], Array[Locality])"
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "OK, I will change it.\n",
    "commit": "d45fecb50aecff0c0bfeb627ec1ac8a4fc9e9644",
    "createdAt": "2015-06-19T02:25:37Z",
    "diffHunk": "@@ -0,0 +1,203 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.util.RackResolver\n+\n+import org.apache.spark.SparkConf\n+\n+private[yarn] trait ContainerPlacementStrategy {\n+  type Locality = Array[String]\n+\n+  /**\n+   * Calculate each container's node locality and rack locality\n+   * @param numContainer number of containers to calculate\n+   * @return node localities and rack localities, each locality is an array of string,\n+   *         the length of localities is the same as number of containers\n+   */\n+  def localityOfRequestedContainers(numContainer: Int): (Array[Locality], Array[Locality])"
  }],
  "prId": 6394
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "If I understand correctly, a pair of localities is used for a single ContainerRequest.\n\nIf this is the case, it might be most clear to remove the locality type, and replace it with a ContainerLocalityPreferences class that has two fields: `nodes: Array[String]` and `racks: Array[String]`.  Then `localityOfRequestedContainers` could just return an Array of these.\n",
    "commit": "d45fecb50aecff0c0bfeb627ec1ac8a4fc9e9644",
    "createdAt": "2015-06-24T20:58:45Z",
    "diffHunk": "@@ -0,0 +1,200 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.util.RackResolver\n+\n+import org.apache.spark.SparkConf\n+\n+private[yarn] trait ContainerPlacementStrategy {\n+  type Locality = Array[String]"
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "Yeah, that is more clear, I will change this.\n",
    "commit": "d45fecb50aecff0c0bfeb627ec1ac8a4fc9e9644",
    "createdAt": "2015-06-25T03:16:28Z",
    "diffHunk": "@@ -0,0 +1,200 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.util.RackResolver\n+\n+import org.apache.spark.SparkConf\n+\n+private[yarn] trait ContainerPlacementStrategy {\n+  type Locality = Array[String]"
  }],
  "prId": 6394
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "Can we mention here what this strategy is trying to optimize?\n",
    "commit": "d45fecb50aecff0c0bfeb627ec1ac8a4fc9e9644",
    "createdAt": "2015-06-24T20:59:52Z",
    "diffHunk": "@@ -0,0 +1,200 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.util.RackResolver\n+\n+import org.apache.spark.SparkConf\n+\n+private[yarn] trait ContainerPlacementStrategy {\n+  type Locality = Array[String]\n+\n+  /**\n+   * Calculate each container's node locality and rack locality\n+   * @param numContainer number of containers to calculate\n+   *\n+   * @return node localities and rack localities, each locality is an array of string,\n+   *         the length of localities is the same as number of containers\n+   */\n+  def localityOfRequestedContainers(numContainer: Int, numLocalityAwarePendingTasks: Int,\n+      preferredLocalityToCount: Map[String, Int]): (Array[Locality], Array[Locality])\n+}\n+\n+/**\n+ * This strategy calculates the preferred localities by considering the node ratio of pending"
  }, {
    "author": {
      "login": "sryza"
    },
    "body": "Which I think is something like:\nIf YARN were to give us containers that satisfied our locality preferences, and we were to fill them up with tasks until all tasks completed, we're trying to maximize the number of tasks that would run locally.\n\n(this statement maybe needs to be revised to account for already-running containers and already-running tasks)\n",
    "commit": "d45fecb50aecff0c0bfeb627ec1ac8a4fc9e9644",
    "createdAt": "2015-06-24T21:06:37Z",
    "diffHunk": "@@ -0,0 +1,200 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.util.RackResolver\n+\n+import org.apache.spark.SparkConf\n+\n+private[yarn] trait ContainerPlacementStrategy {\n+  type Locality = Array[String]\n+\n+  /**\n+   * Calculate each container's node locality and rack locality\n+   * @param numContainer number of containers to calculate\n+   *\n+   * @return node localities and rack localities, each locality is an array of string,\n+   *         the length of localities is the same as number of containers\n+   */\n+  def localityOfRequestedContainers(numContainer: Int, numLocalityAwarePendingTasks: Int,\n+      preferredLocalityToCount: Map[String, Int]): (Array[Locality], Array[Locality])\n+}\n+\n+/**\n+ * This strategy calculates the preferred localities by considering the node ratio of pending"
  }],
  "prId": 6394
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "When arguments overflow a single line, each gets its own line.\n",
    "commit": "d45fecb50aecff0c0bfeb627ec1ac8a4fc9e9644",
    "createdAt": "2015-06-24T21:00:10Z",
    "diffHunk": "@@ -0,0 +1,200 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.util.RackResolver\n+\n+import org.apache.spark.SparkConf\n+\n+private[yarn] trait ContainerPlacementStrategy {\n+  type Locality = Array[String]\n+\n+  /**\n+   * Calculate each container's node locality and rack locality\n+   * @param numContainer number of containers to calculate\n+   *\n+   * @return node localities and rack localities, each locality is an array of string,\n+   *         the length of localities is the same as number of containers\n+   */\n+  def localityOfRequestedContainers(numContainer: Int, numLocalityAwarePendingTasks: Int,"
  }],
  "prId": 6394
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "Use javadoc of the form `/** */` for method headers.\n\nAlso, in the doc, provide an explanation of what each entry in the returned map represents.\n",
    "commit": "d45fecb50aecff0c0bfeb627ec1ac8a4fc9e9644",
    "createdAt": "2015-06-25T23:24:40Z",
    "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.util.RackResolver\n+\n+import org.apache.spark.SparkConf\n+\n+private[yarn] case class ContainerLocalityPreferences(nodes: Array[String], racks: Array[String])\n+\n+private[yarn] trait ContainerPlacementStrategy {\n+\n+  /**\n+   * Calculate each container's node locality and rack locality\n+   * @param numContainer number of containers to calculate\n+   * @param numLocalityAwarePendingTasks number of locality required pending tasks\n+   * @param preferredLocalityToCount a map to store the preferred host name and its required count\n+   *\n+   * @return node localities and rack localities, each locality is an array of string,\n+   *         the length of localities is the same as number of containers\n+   */\n+  def localityOfRequestedContainers(\n+      numContainer: Int,\n+      numLocalityAwarePendingTasks: Int,\n+      preferredLocalityToCount: Map[String, Int]\n+    ): Array[ContainerLocalityPreferences]\n+}\n+\n+/**\n+ * This strategy is calculating the optimal locality preferences of YARN containers by considering\n+ * the node ratio of pending tasks, number of required cores/containers and and locality of current\n+ * existed containers. The target of this algorithm is to maximize the number of tasks that\n+ * would run locally.\n+ *\n+ * The details of this algorithm is described as below, if we have 20 tasks which\n+ * require (host1, host2, host3) and 10 tasks which require (host1, host2, host4),\n+ * besides each container has 2 cores and cpus per task is 1, so the required container number is\n+ * 15, and host ratio is (host1: 30, host2: 30, host3: 20, host4: 10).\n+ *\n+ * 1. If requested container number (18) is more than the required container number (15):\n+ *\n+ * requests for 5 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 5 containers with nodes: (host1, host2, host3)\n+ * requests for 5 containers with nodes: (host1, host2)\n+ * requests for 3 containers with no locality preferences.\n+ *\n+ * The placement ratio is 3 : 3 : 2 : 1, and set the additional containers with no locality\n+ * preferences.\n+ *\n+ * 2. If requested container number (10) is less than or equal to the required container number\n+ * (15):\n+ *\n+ * requests for 4 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 3 containers with nodes: (host1, host2, host3)\n+ * requests for 3 containers with nodes: (host1, host2)\n+ *\n+ * The placement ratio is 10 : 10 : 7 : 4, close to expected ratio (3 : 3 : 2 : 1)\n+ *\n+ * 3. If containers are existed but no matching localities, follow the method of 1 and 2.\n+ *\n+ * 4. If containers are existed and some localities are matched. For example if we have 1\n+ * containers on each node (host1: 1, host2: 1: host3: 1, host4: 1), and the expected containers\n+ * on each node would be (host1: 5, host2: 5, host3: 4, host4: 2),\n+ * so the newly requested containers on each node would be updated to (host1: 4, host2: 4,\n+ * host3: 3, host4: 1), 12 containers by total.\n+ *\n+ *   4.1 If requested container number (18) is more than newly required containers (12). Follow\n+ *   method 1 with updated ratio 4 : 4 : 3 : 1.\n+ *\n+ *   4.2 If request container number (10) is more than newly required containers (12). Follow\n+ *   method 2 with updated ratio 4 : 4 : 3 : 1.\n+ *\n+ * 5. If containers are existed and existing localities can fully cover the requested localities.\n+ * For example if we have 5 containers on each node (host1: 5, host2: 5, host3: 5, host4: 5),\n+ * which could cover the current requested localities. This algorithm will allocate all the\n+ * requested containers with no localities.\n+ */\n+private[yarn] class LocalityPreferredContainerPlacementStrategy(\n+    val sparkConf: SparkConf,\n+    val yarnConf: Configuration,\n+    val yarnAllocator: YarnAllocator) extends ContainerPlacementStrategy {\n+\n+  // Number of CPUs per task\n+  private val CPUS_PER_TASK = sparkConf.getInt(\"spark.task.cpus\", 1)\n+\n+  // Get the required cores of locality aware task\n+  private def localityAwareTaskCores(localityAwarePendingTasks: Int): Int = {\n+    localityAwarePendingTasks * CPUS_PER_TASK\n+  }\n+\n+  // Get the expected number of locality aware containers\n+  private def expectedLocalityAwareContainerNum(localityAwarePendingTasks: Int): Int = {\n+    (localityAwareTaskCores(localityAwarePendingTasks) +\n+      yarnAllocator.resource.getVirtualCores - 1) /\n+        yarnAllocator.resource.getVirtualCores\n+  }\n+\n+  // Update the expected locality distribution by considering the existing allocated container"
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "Do we also need to use javadoc style for private method?\n",
    "commit": "d45fecb50aecff0c0bfeb627ec1ac8a4fc9e9644",
    "createdAt": "2015-06-29T02:01:24Z",
    "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.util.RackResolver\n+\n+import org.apache.spark.SparkConf\n+\n+private[yarn] case class ContainerLocalityPreferences(nodes: Array[String], racks: Array[String])\n+\n+private[yarn] trait ContainerPlacementStrategy {\n+\n+  /**\n+   * Calculate each container's node locality and rack locality\n+   * @param numContainer number of containers to calculate\n+   * @param numLocalityAwarePendingTasks number of locality required pending tasks\n+   * @param preferredLocalityToCount a map to store the preferred host name and its required count\n+   *\n+   * @return node localities and rack localities, each locality is an array of string,\n+   *         the length of localities is the same as number of containers\n+   */\n+  def localityOfRequestedContainers(\n+      numContainer: Int,\n+      numLocalityAwarePendingTasks: Int,\n+      preferredLocalityToCount: Map[String, Int]\n+    ): Array[ContainerLocalityPreferences]\n+}\n+\n+/**\n+ * This strategy is calculating the optimal locality preferences of YARN containers by considering\n+ * the node ratio of pending tasks, number of required cores/containers and and locality of current\n+ * existed containers. The target of this algorithm is to maximize the number of tasks that\n+ * would run locally.\n+ *\n+ * The details of this algorithm is described as below, if we have 20 tasks which\n+ * require (host1, host2, host3) and 10 tasks which require (host1, host2, host4),\n+ * besides each container has 2 cores and cpus per task is 1, so the required container number is\n+ * 15, and host ratio is (host1: 30, host2: 30, host3: 20, host4: 10).\n+ *\n+ * 1. If requested container number (18) is more than the required container number (15):\n+ *\n+ * requests for 5 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 5 containers with nodes: (host1, host2, host3)\n+ * requests for 5 containers with nodes: (host1, host2)\n+ * requests for 3 containers with no locality preferences.\n+ *\n+ * The placement ratio is 3 : 3 : 2 : 1, and set the additional containers with no locality\n+ * preferences.\n+ *\n+ * 2. If requested container number (10) is less than or equal to the required container number\n+ * (15):\n+ *\n+ * requests for 4 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 3 containers with nodes: (host1, host2, host3)\n+ * requests for 3 containers with nodes: (host1, host2)\n+ *\n+ * The placement ratio is 10 : 10 : 7 : 4, close to expected ratio (3 : 3 : 2 : 1)\n+ *\n+ * 3. If containers are existed but no matching localities, follow the method of 1 and 2.\n+ *\n+ * 4. If containers are existed and some localities are matched. For example if we have 1\n+ * containers on each node (host1: 1, host2: 1: host3: 1, host4: 1), and the expected containers\n+ * on each node would be (host1: 5, host2: 5, host3: 4, host4: 2),\n+ * so the newly requested containers on each node would be updated to (host1: 4, host2: 4,\n+ * host3: 3, host4: 1), 12 containers by total.\n+ *\n+ *   4.1 If requested container number (18) is more than newly required containers (12). Follow\n+ *   method 1 with updated ratio 4 : 4 : 3 : 1.\n+ *\n+ *   4.2 If request container number (10) is more than newly required containers (12). Follow\n+ *   method 2 with updated ratio 4 : 4 : 3 : 1.\n+ *\n+ * 5. If containers are existed and existing localities can fully cover the requested localities.\n+ * For example if we have 5 containers on each node (host1: 5, host2: 5, host3: 5, host4: 5),\n+ * which could cover the current requested localities. This algorithm will allocate all the\n+ * requested containers with no localities.\n+ */\n+private[yarn] class LocalityPreferredContainerPlacementStrategy(\n+    val sparkConf: SparkConf,\n+    val yarnConf: Configuration,\n+    val yarnAllocator: YarnAllocator) extends ContainerPlacementStrategy {\n+\n+  // Number of CPUs per task\n+  private val CPUS_PER_TASK = sparkConf.getInt(\"spark.task.cpus\", 1)\n+\n+  // Get the required cores of locality aware task\n+  private def localityAwareTaskCores(localityAwarePendingTasks: Int): Int = {\n+    localityAwarePendingTasks * CPUS_PER_TASK\n+  }\n+\n+  // Get the expected number of locality aware containers\n+  private def expectedLocalityAwareContainerNum(localityAwarePendingTasks: Int): Int = {\n+    (localityAwareTaskCores(localityAwarePendingTasks) +\n+      yarnAllocator.resource.getVirtualCores - 1) /\n+        yarnAllocator.resource.getVirtualCores\n+  }\n+\n+  // Update the expected locality distribution by considering the existing allocated container"
  }],
  "prId": 6394
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "mark this with `override`\n",
    "commit": "d45fecb50aecff0c0bfeb627ec1ac8a4fc9e9644",
    "createdAt": "2015-06-25T23:29:35Z",
    "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.util.RackResolver\n+\n+import org.apache.spark.SparkConf\n+\n+private[yarn] case class ContainerLocalityPreferences(nodes: Array[String], racks: Array[String])\n+\n+private[yarn] trait ContainerPlacementStrategy {\n+\n+  /**\n+   * Calculate each container's node locality and rack locality\n+   * @param numContainer number of containers to calculate\n+   * @param numLocalityAwarePendingTasks number of locality required pending tasks\n+   * @param preferredLocalityToCount a map to store the preferred host name and its required count\n+   *\n+   * @return node localities and rack localities, each locality is an array of string,\n+   *         the length of localities is the same as number of containers\n+   */\n+  def localityOfRequestedContainers(\n+      numContainer: Int,\n+      numLocalityAwarePendingTasks: Int,\n+      preferredLocalityToCount: Map[String, Int]\n+    ): Array[ContainerLocalityPreferences]\n+}\n+\n+/**\n+ * This strategy is calculating the optimal locality preferences of YARN containers by considering\n+ * the node ratio of pending tasks, number of required cores/containers and and locality of current\n+ * existed containers. The target of this algorithm is to maximize the number of tasks that\n+ * would run locally.\n+ *\n+ * The details of this algorithm is described as below, if we have 20 tasks which\n+ * require (host1, host2, host3) and 10 tasks which require (host1, host2, host4),\n+ * besides each container has 2 cores and cpus per task is 1, so the required container number is\n+ * 15, and host ratio is (host1: 30, host2: 30, host3: 20, host4: 10).\n+ *\n+ * 1. If requested container number (18) is more than the required container number (15):\n+ *\n+ * requests for 5 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 5 containers with nodes: (host1, host2, host3)\n+ * requests for 5 containers with nodes: (host1, host2)\n+ * requests for 3 containers with no locality preferences.\n+ *\n+ * The placement ratio is 3 : 3 : 2 : 1, and set the additional containers with no locality\n+ * preferences.\n+ *\n+ * 2. If requested container number (10) is less than or equal to the required container number\n+ * (15):\n+ *\n+ * requests for 4 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 3 containers with nodes: (host1, host2, host3)\n+ * requests for 3 containers with nodes: (host1, host2)\n+ *\n+ * The placement ratio is 10 : 10 : 7 : 4, close to expected ratio (3 : 3 : 2 : 1)\n+ *\n+ * 3. If containers are existed but no matching localities, follow the method of 1 and 2.\n+ *\n+ * 4. If containers are existed and some localities are matched. For example if we have 1\n+ * containers on each node (host1: 1, host2: 1: host3: 1, host4: 1), and the expected containers\n+ * on each node would be (host1: 5, host2: 5, host3: 4, host4: 2),\n+ * so the newly requested containers on each node would be updated to (host1: 4, host2: 4,\n+ * host3: 3, host4: 1), 12 containers by total.\n+ *\n+ *   4.1 If requested container number (18) is more than newly required containers (12). Follow\n+ *   method 1 with updated ratio 4 : 4 : 3 : 1.\n+ *\n+ *   4.2 If request container number (10) is more than newly required containers (12). Follow\n+ *   method 2 with updated ratio 4 : 4 : 3 : 1.\n+ *\n+ * 5. If containers are existed and existing localities can fully cover the requested localities.\n+ * For example if we have 5 containers on each node (host1: 5, host2: 5, host3: 5, host4: 5),\n+ * which could cover the current requested localities. This algorithm will allocate all the\n+ * requested containers with no localities.\n+ */\n+private[yarn] class LocalityPreferredContainerPlacementStrategy(\n+    val sparkConf: SparkConf,\n+    val yarnConf: Configuration,\n+    val yarnAllocator: YarnAllocator) extends ContainerPlacementStrategy {\n+\n+  // Number of CPUs per task\n+  private val CPUS_PER_TASK = sparkConf.getInt(\"spark.task.cpus\", 1)\n+\n+  // Get the required cores of locality aware task\n+  private def localityAwareTaskCores(localityAwarePendingTasks: Int): Int = {\n+    localityAwarePendingTasks * CPUS_PER_TASK\n+  }\n+\n+  // Get the expected number of locality aware containers\n+  private def expectedLocalityAwareContainerNum(localityAwarePendingTasks: Int): Int = {\n+    (localityAwareTaskCores(localityAwarePendingTasks) +\n+      yarnAllocator.resource.getVirtualCores - 1) /\n+        yarnAllocator.resource.getVirtualCores\n+  }\n+\n+  // Update the expected locality distribution by considering the existing allocated container\n+  // host distributions.\n+  private def updateExpectedLocalityToCounts(localityAwarePendingTasks: Int,\n+      preferredLocalityToCounts: Map[String, Int]): Map[String, Int] = {\n+    val totalPreferredLocalities = preferredLocalityToCounts.values.sum\n+    preferredLocalityToCounts.map { case (host, count) =>\n+      val expectedCount =\n+        count.toDouble * expectedLocalityAwareContainerNum(localityAwarePendingTasks) /\n+          totalPreferredLocalities\n+      val existedCount = yarnAllocator.allocatedHostToContainersMap.get(host)\n+        .map(_.size)\n+        .getOrElse(0)\n+\n+      if (expectedCount > existedCount) {\n+        // Get the actual container number if existing container can not fully satisfy the\n+        // expected number of container\n+        (host, (expectedCount - existedCount).ceil.toInt)\n+      } else {\n+        // If the current existed container number can fully satisfy the expected number of\n+        // containers, set the required containers to be 0\n+        (host, 0)\n+      }\n+    }\n+  }\n+\n+  def localityOfRequestedContainers("
  }],
  "prId": 6394
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "Can this just be `(host, math.max(0, (expectedCount - existedCount).ceil.toInt))`?\n",
    "commit": "d45fecb50aecff0c0bfeb627ec1ac8a4fc9e9644",
    "createdAt": "2015-06-25T23:40:11Z",
    "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.util.RackResolver\n+\n+import org.apache.spark.SparkConf\n+\n+private[yarn] case class ContainerLocalityPreferences(nodes: Array[String], racks: Array[String])\n+\n+private[yarn] trait ContainerPlacementStrategy {\n+\n+  /**\n+   * Calculate each container's node locality and rack locality\n+   * @param numContainer number of containers to calculate\n+   * @param numLocalityAwarePendingTasks number of locality required pending tasks\n+   * @param preferredLocalityToCount a map to store the preferred host name and its required count\n+   *\n+   * @return node localities and rack localities, each locality is an array of string,\n+   *         the length of localities is the same as number of containers\n+   */\n+  def localityOfRequestedContainers(\n+      numContainer: Int,\n+      numLocalityAwarePendingTasks: Int,\n+      preferredLocalityToCount: Map[String, Int]\n+    ): Array[ContainerLocalityPreferences]\n+}\n+\n+/**\n+ * This strategy is calculating the optimal locality preferences of YARN containers by considering\n+ * the node ratio of pending tasks, number of required cores/containers and and locality of current\n+ * existed containers. The target of this algorithm is to maximize the number of tasks that\n+ * would run locally.\n+ *\n+ * The details of this algorithm is described as below, if we have 20 tasks which\n+ * require (host1, host2, host3) and 10 tasks which require (host1, host2, host4),\n+ * besides each container has 2 cores and cpus per task is 1, so the required container number is\n+ * 15, and host ratio is (host1: 30, host2: 30, host3: 20, host4: 10).\n+ *\n+ * 1. If requested container number (18) is more than the required container number (15):\n+ *\n+ * requests for 5 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 5 containers with nodes: (host1, host2, host3)\n+ * requests for 5 containers with nodes: (host1, host2)\n+ * requests for 3 containers with no locality preferences.\n+ *\n+ * The placement ratio is 3 : 3 : 2 : 1, and set the additional containers with no locality\n+ * preferences.\n+ *\n+ * 2. If requested container number (10) is less than or equal to the required container number\n+ * (15):\n+ *\n+ * requests for 4 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 3 containers with nodes: (host1, host2, host3)\n+ * requests for 3 containers with nodes: (host1, host2)\n+ *\n+ * The placement ratio is 10 : 10 : 7 : 4, close to expected ratio (3 : 3 : 2 : 1)\n+ *\n+ * 3. If containers are existed but no matching localities, follow the method of 1 and 2.\n+ *\n+ * 4. If containers are existed and some localities are matched. For example if we have 1\n+ * containers on each node (host1: 1, host2: 1: host3: 1, host4: 1), and the expected containers\n+ * on each node would be (host1: 5, host2: 5, host3: 4, host4: 2),\n+ * so the newly requested containers on each node would be updated to (host1: 4, host2: 4,\n+ * host3: 3, host4: 1), 12 containers by total.\n+ *\n+ *   4.1 If requested container number (18) is more than newly required containers (12). Follow\n+ *   method 1 with updated ratio 4 : 4 : 3 : 1.\n+ *\n+ *   4.2 If request container number (10) is more than newly required containers (12). Follow\n+ *   method 2 with updated ratio 4 : 4 : 3 : 1.\n+ *\n+ * 5. If containers are existed and existing localities can fully cover the requested localities.\n+ * For example if we have 5 containers on each node (host1: 5, host2: 5, host3: 5, host4: 5),\n+ * which could cover the current requested localities. This algorithm will allocate all the\n+ * requested containers with no localities.\n+ */\n+private[yarn] class LocalityPreferredContainerPlacementStrategy(\n+    val sparkConf: SparkConf,\n+    val yarnConf: Configuration,\n+    val yarnAllocator: YarnAllocator) extends ContainerPlacementStrategy {\n+\n+  // Number of CPUs per task\n+  private val CPUS_PER_TASK = sparkConf.getInt(\"spark.task.cpus\", 1)\n+\n+  // Get the required cores of locality aware task\n+  private def localityAwareTaskCores(localityAwarePendingTasks: Int): Int = {\n+    localityAwarePendingTasks * CPUS_PER_TASK\n+  }\n+\n+  // Get the expected number of locality aware containers\n+  private def expectedLocalityAwareContainerNum(localityAwarePendingTasks: Int): Int = {\n+    (localityAwareTaskCores(localityAwarePendingTasks) +\n+      yarnAllocator.resource.getVirtualCores - 1) /\n+        yarnAllocator.resource.getVirtualCores\n+  }\n+\n+  // Update the expected locality distribution by considering the existing allocated container\n+  // host distributions.\n+  private def updateExpectedLocalityToCounts(localityAwarePendingTasks: Int,\n+      preferredLocalityToCounts: Map[String, Int]): Map[String, Int] = {\n+    val totalPreferredLocalities = preferredLocalityToCounts.values.sum\n+    preferredLocalityToCounts.map { case (host, count) =>\n+      val expectedCount =\n+        count.toDouble * expectedLocalityAwareContainerNum(localityAwarePendingTasks) /\n+          totalPreferredLocalities\n+      val existedCount = yarnAllocator.allocatedHostToContainersMap.get(host)\n+        .map(_.size)\n+        .getOrElse(0)\n+\n+      if (expectedCount > existedCount) {"
  }],
  "prId": 6394
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "Can this if/else block be replaced with:\n\n```\nrequiredLocalityFreeContainerNum = math.max(0, numContainer - updatedLocalityAwareContainerNum)\nrequiredLocalityAwareContainerNum = numContainer - requiredLocalityFreeContainerNum\n```\n\n?\n",
    "commit": "d45fecb50aecff0c0bfeb627ec1ac8a4fc9e9644",
    "createdAt": "2015-06-25T23:44:18Z",
    "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.util.RackResolver\n+\n+import org.apache.spark.SparkConf\n+\n+private[yarn] case class ContainerLocalityPreferences(nodes: Array[String], racks: Array[String])\n+\n+private[yarn] trait ContainerPlacementStrategy {\n+\n+  /**\n+   * Calculate each container's node locality and rack locality\n+   * @param numContainer number of containers to calculate\n+   * @param numLocalityAwarePendingTasks number of locality required pending tasks\n+   * @param preferredLocalityToCount a map to store the preferred host name and its required count\n+   *\n+   * @return node localities and rack localities, each locality is an array of string,\n+   *         the length of localities is the same as number of containers\n+   */\n+  def localityOfRequestedContainers(\n+      numContainer: Int,\n+      numLocalityAwarePendingTasks: Int,\n+      preferredLocalityToCount: Map[String, Int]\n+    ): Array[ContainerLocalityPreferences]\n+}\n+\n+/**\n+ * This strategy is calculating the optimal locality preferences of YARN containers by considering\n+ * the node ratio of pending tasks, number of required cores/containers and and locality of current\n+ * existed containers. The target of this algorithm is to maximize the number of tasks that\n+ * would run locally.\n+ *\n+ * The details of this algorithm is described as below, if we have 20 tasks which\n+ * require (host1, host2, host3) and 10 tasks which require (host1, host2, host4),\n+ * besides each container has 2 cores and cpus per task is 1, so the required container number is\n+ * 15, and host ratio is (host1: 30, host2: 30, host3: 20, host4: 10).\n+ *\n+ * 1. If requested container number (18) is more than the required container number (15):\n+ *\n+ * requests for 5 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 5 containers with nodes: (host1, host2, host3)\n+ * requests for 5 containers with nodes: (host1, host2)\n+ * requests for 3 containers with no locality preferences.\n+ *\n+ * The placement ratio is 3 : 3 : 2 : 1, and set the additional containers with no locality\n+ * preferences.\n+ *\n+ * 2. If requested container number (10) is less than or equal to the required container number\n+ * (15):\n+ *\n+ * requests for 4 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 3 containers with nodes: (host1, host2, host3)\n+ * requests for 3 containers with nodes: (host1, host2)\n+ *\n+ * The placement ratio is 10 : 10 : 7 : 4, close to expected ratio (3 : 3 : 2 : 1)\n+ *\n+ * 3. If containers are existed but no matching localities, follow the method of 1 and 2.\n+ *\n+ * 4. If containers are existed and some localities are matched. For example if we have 1\n+ * containers on each node (host1: 1, host2: 1: host3: 1, host4: 1), and the expected containers\n+ * on each node would be (host1: 5, host2: 5, host3: 4, host4: 2),\n+ * so the newly requested containers on each node would be updated to (host1: 4, host2: 4,\n+ * host3: 3, host4: 1), 12 containers by total.\n+ *\n+ *   4.1 If requested container number (18) is more than newly required containers (12). Follow\n+ *   method 1 with updated ratio 4 : 4 : 3 : 1.\n+ *\n+ *   4.2 If request container number (10) is more than newly required containers (12). Follow\n+ *   method 2 with updated ratio 4 : 4 : 3 : 1.\n+ *\n+ * 5. If containers are existed and existing localities can fully cover the requested localities.\n+ * For example if we have 5 containers on each node (host1: 5, host2: 5, host3: 5, host4: 5),\n+ * which could cover the current requested localities. This algorithm will allocate all the\n+ * requested containers with no localities.\n+ */\n+private[yarn] class LocalityPreferredContainerPlacementStrategy(\n+    val sparkConf: SparkConf,\n+    val yarnConf: Configuration,\n+    val yarnAllocator: YarnAllocator) extends ContainerPlacementStrategy {\n+\n+  // Number of CPUs per task\n+  private val CPUS_PER_TASK = sparkConf.getInt(\"spark.task.cpus\", 1)\n+\n+  // Get the required cores of locality aware task\n+  private def localityAwareTaskCores(localityAwarePendingTasks: Int): Int = {\n+    localityAwarePendingTasks * CPUS_PER_TASK\n+  }\n+\n+  // Get the expected number of locality aware containers\n+  private def expectedLocalityAwareContainerNum(localityAwarePendingTasks: Int): Int = {\n+    (localityAwareTaskCores(localityAwarePendingTasks) +\n+      yarnAllocator.resource.getVirtualCores - 1) /\n+        yarnAllocator.resource.getVirtualCores\n+  }\n+\n+  // Update the expected locality distribution by considering the existing allocated container\n+  // host distributions.\n+  private def updateExpectedLocalityToCounts(localityAwarePendingTasks: Int,\n+      preferredLocalityToCounts: Map[String, Int]): Map[String, Int] = {\n+    val totalPreferredLocalities = preferredLocalityToCounts.values.sum\n+    preferredLocalityToCounts.map { case (host, count) =>\n+      val expectedCount =\n+        count.toDouble * expectedLocalityAwareContainerNum(localityAwarePendingTasks) /\n+          totalPreferredLocalities\n+      val existedCount = yarnAllocator.allocatedHostToContainersMap.get(host)\n+        .map(_.size)\n+        .getOrElse(0)\n+\n+      if (expectedCount > existedCount) {\n+        // Get the actual container number if existing container can not fully satisfy the\n+        // expected number of container\n+        (host, (expectedCount - existedCount).ceil.toInt)\n+      } else {\n+        // If the current existed container number can fully satisfy the expected number of\n+        // containers, set the required containers to be 0\n+        (host, 0)\n+      }\n+    }\n+  }\n+\n+  def localityOfRequestedContainers(\n+      numContainer: Int,\n+      numLocalityAwarePendingTasks: Int,\n+      preferredLocalityToCounts: Map[String, Int]\n+    ): Array[ContainerLocalityPreferences] = {\n+    val updatedLocalityToCounts =\n+      updateExpectedLocalityToCounts(numLocalityAwarePendingTasks, preferredLocalityToCounts)\n+    val updatedLocalityAwareContainerNum = updatedLocalityToCounts.values.sum\n+\n+    // The number of containers to allocate, divided into two groups, one with preferred locality,\n+    // and the other without locality preference.\n+    var requiredLocalityFreeContainerNum: Int = 0\n+    var requiredLocalityAwareContainerNum: Int = 0\n+"
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "Thanks, that's really simplify the code.\n",
    "commit": "d45fecb50aecff0c0bfeb627ec1ac8a4fc9e9644",
    "createdAt": "2015-06-29T02:49:14Z",
    "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.util.RackResolver\n+\n+import org.apache.spark.SparkConf\n+\n+private[yarn] case class ContainerLocalityPreferences(nodes: Array[String], racks: Array[String])\n+\n+private[yarn] trait ContainerPlacementStrategy {\n+\n+  /**\n+   * Calculate each container's node locality and rack locality\n+   * @param numContainer number of containers to calculate\n+   * @param numLocalityAwarePendingTasks number of locality required pending tasks\n+   * @param preferredLocalityToCount a map to store the preferred host name and its required count\n+   *\n+   * @return node localities and rack localities, each locality is an array of string,\n+   *         the length of localities is the same as number of containers\n+   */\n+  def localityOfRequestedContainers(\n+      numContainer: Int,\n+      numLocalityAwarePendingTasks: Int,\n+      preferredLocalityToCount: Map[String, Int]\n+    ): Array[ContainerLocalityPreferences]\n+}\n+\n+/**\n+ * This strategy is calculating the optimal locality preferences of YARN containers by considering\n+ * the node ratio of pending tasks, number of required cores/containers and and locality of current\n+ * existed containers. The target of this algorithm is to maximize the number of tasks that\n+ * would run locally.\n+ *\n+ * The details of this algorithm is described as below, if we have 20 tasks which\n+ * require (host1, host2, host3) and 10 tasks which require (host1, host2, host4),\n+ * besides each container has 2 cores and cpus per task is 1, so the required container number is\n+ * 15, and host ratio is (host1: 30, host2: 30, host3: 20, host4: 10).\n+ *\n+ * 1. If requested container number (18) is more than the required container number (15):\n+ *\n+ * requests for 5 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 5 containers with nodes: (host1, host2, host3)\n+ * requests for 5 containers with nodes: (host1, host2)\n+ * requests for 3 containers with no locality preferences.\n+ *\n+ * The placement ratio is 3 : 3 : 2 : 1, and set the additional containers with no locality\n+ * preferences.\n+ *\n+ * 2. If requested container number (10) is less than or equal to the required container number\n+ * (15):\n+ *\n+ * requests for 4 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 3 containers with nodes: (host1, host2, host3)\n+ * requests for 3 containers with nodes: (host1, host2)\n+ *\n+ * The placement ratio is 10 : 10 : 7 : 4, close to expected ratio (3 : 3 : 2 : 1)\n+ *\n+ * 3. If containers are existed but no matching localities, follow the method of 1 and 2.\n+ *\n+ * 4. If containers are existed and some localities are matched. For example if we have 1\n+ * containers on each node (host1: 1, host2: 1: host3: 1, host4: 1), and the expected containers\n+ * on each node would be (host1: 5, host2: 5, host3: 4, host4: 2),\n+ * so the newly requested containers on each node would be updated to (host1: 4, host2: 4,\n+ * host3: 3, host4: 1), 12 containers by total.\n+ *\n+ *   4.1 If requested container number (18) is more than newly required containers (12). Follow\n+ *   method 1 with updated ratio 4 : 4 : 3 : 1.\n+ *\n+ *   4.2 If request container number (10) is more than newly required containers (12). Follow\n+ *   method 2 with updated ratio 4 : 4 : 3 : 1.\n+ *\n+ * 5. If containers are existed and existing localities can fully cover the requested localities.\n+ * For example if we have 5 containers on each node (host1: 5, host2: 5, host3: 5, host4: 5),\n+ * which could cover the current requested localities. This algorithm will allocate all the\n+ * requested containers with no localities.\n+ */\n+private[yarn] class LocalityPreferredContainerPlacementStrategy(\n+    val sparkConf: SparkConf,\n+    val yarnConf: Configuration,\n+    val yarnAllocator: YarnAllocator) extends ContainerPlacementStrategy {\n+\n+  // Number of CPUs per task\n+  private val CPUS_PER_TASK = sparkConf.getInt(\"spark.task.cpus\", 1)\n+\n+  // Get the required cores of locality aware task\n+  private def localityAwareTaskCores(localityAwarePendingTasks: Int): Int = {\n+    localityAwarePendingTasks * CPUS_PER_TASK\n+  }\n+\n+  // Get the expected number of locality aware containers\n+  private def expectedLocalityAwareContainerNum(localityAwarePendingTasks: Int): Int = {\n+    (localityAwareTaskCores(localityAwarePendingTasks) +\n+      yarnAllocator.resource.getVirtualCores - 1) /\n+        yarnAllocator.resource.getVirtualCores\n+  }\n+\n+  // Update the expected locality distribution by considering the existing allocated container\n+  // host distributions.\n+  private def updateExpectedLocalityToCounts(localityAwarePendingTasks: Int,\n+      preferredLocalityToCounts: Map[String, Int]): Map[String, Int] = {\n+    val totalPreferredLocalities = preferredLocalityToCounts.values.sum\n+    preferredLocalityToCounts.map { case (host, count) =>\n+      val expectedCount =\n+        count.toDouble * expectedLocalityAwareContainerNum(localityAwarePendingTasks) /\n+          totalPreferredLocalities\n+      val existedCount = yarnAllocator.allocatedHostToContainersMap.get(host)\n+        .map(_.size)\n+        .getOrElse(0)\n+\n+      if (expectedCount > existedCount) {\n+        // Get the actual container number if existing container can not fully satisfy the\n+        // expected number of container\n+        (host, (expectedCount - existedCount).ceil.toInt)\n+      } else {\n+        // If the current existed container number can fully satisfy the expected number of\n+        // containers, set the required containers to be 0\n+        (host, 0)\n+      }\n+    }\n+  }\n+\n+  def localityOfRequestedContainers(\n+      numContainer: Int,\n+      numLocalityAwarePendingTasks: Int,\n+      preferredLocalityToCounts: Map[String, Int]\n+    ): Array[ContainerLocalityPreferences] = {\n+    val updatedLocalityToCounts =\n+      updateExpectedLocalityToCounts(numLocalityAwarePendingTasks, preferredLocalityToCounts)\n+    val updatedLocalityAwareContainerNum = updatedLocalityToCounts.values.sum\n+\n+    // The number of containers to allocate, divided into two groups, one with preferred locality,\n+    // and the other without locality preference.\n+    var requiredLocalityFreeContainerNum: Int = 0\n+    var requiredLocalityAwareContainerNum: Int = 0\n+"
  }],
  "prId": 6394
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "Split out arguments to multiple lines\n",
    "commit": "d45fecb50aecff0c0bfeb627ec1ac8a4fc9e9644",
    "createdAt": "2015-06-25T23:50:38Z",
    "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.util.RackResolver\n+\n+import org.apache.spark.SparkConf\n+\n+private[yarn] case class ContainerLocalityPreferences(nodes: Array[String], racks: Array[String])\n+\n+private[yarn] trait ContainerPlacementStrategy {\n+\n+  /**\n+   * Calculate each container's node locality and rack locality\n+   * @param numContainer number of containers to calculate\n+   * @param numLocalityAwarePendingTasks number of locality required pending tasks\n+   * @param preferredLocalityToCount a map to store the preferred host name and its required count\n+   *\n+   * @return node localities and rack localities, each locality is an array of string,\n+   *         the length of localities is the same as number of containers\n+   */\n+  def localityOfRequestedContainers(\n+      numContainer: Int,\n+      numLocalityAwarePendingTasks: Int,\n+      preferredLocalityToCount: Map[String, Int]\n+    ): Array[ContainerLocalityPreferences]\n+}\n+\n+/**\n+ * This strategy is calculating the optimal locality preferences of YARN containers by considering\n+ * the node ratio of pending tasks, number of required cores/containers and and locality of current\n+ * existed containers. The target of this algorithm is to maximize the number of tasks that\n+ * would run locally.\n+ *\n+ * The details of this algorithm is described as below, if we have 20 tasks which\n+ * require (host1, host2, host3) and 10 tasks which require (host1, host2, host4),\n+ * besides each container has 2 cores and cpus per task is 1, so the required container number is\n+ * 15, and host ratio is (host1: 30, host2: 30, host3: 20, host4: 10).\n+ *\n+ * 1. If requested container number (18) is more than the required container number (15):\n+ *\n+ * requests for 5 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 5 containers with nodes: (host1, host2, host3)\n+ * requests for 5 containers with nodes: (host1, host2)\n+ * requests for 3 containers with no locality preferences.\n+ *\n+ * The placement ratio is 3 : 3 : 2 : 1, and set the additional containers with no locality\n+ * preferences.\n+ *\n+ * 2. If requested container number (10) is less than or equal to the required container number\n+ * (15):\n+ *\n+ * requests for 4 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 3 containers with nodes: (host1, host2, host3)\n+ * requests for 3 containers with nodes: (host1, host2)\n+ *\n+ * The placement ratio is 10 : 10 : 7 : 4, close to expected ratio (3 : 3 : 2 : 1)\n+ *\n+ * 3. If containers are existed but no matching localities, follow the method of 1 and 2.\n+ *\n+ * 4. If containers are existed and some localities are matched. For example if we have 1\n+ * containers on each node (host1: 1, host2: 1: host3: 1, host4: 1), and the expected containers\n+ * on each node would be (host1: 5, host2: 5, host3: 4, host4: 2),\n+ * so the newly requested containers on each node would be updated to (host1: 4, host2: 4,\n+ * host3: 3, host4: 1), 12 containers by total.\n+ *\n+ *   4.1 If requested container number (18) is more than newly required containers (12). Follow\n+ *   method 1 with updated ratio 4 : 4 : 3 : 1.\n+ *\n+ *   4.2 If request container number (10) is more than newly required containers (12). Follow\n+ *   method 2 with updated ratio 4 : 4 : 3 : 1.\n+ *\n+ * 5. If containers are existed and existing localities can fully cover the requested localities.\n+ * For example if we have 5 containers on each node (host1: 5, host2: 5, host3: 5, host4: 5),\n+ * which could cover the current requested localities. This algorithm will allocate all the\n+ * requested containers with no localities.\n+ */\n+private[yarn] class LocalityPreferredContainerPlacementStrategy(\n+    val sparkConf: SparkConf,\n+    val yarnConf: Configuration,\n+    val yarnAllocator: YarnAllocator) extends ContainerPlacementStrategy {\n+\n+  // Number of CPUs per task\n+  private val CPUS_PER_TASK = sparkConf.getInt(\"spark.task.cpus\", 1)\n+\n+  // Get the required cores of locality aware task\n+  private def localityAwareTaskCores(localityAwarePendingTasks: Int): Int = {\n+    localityAwarePendingTasks * CPUS_PER_TASK\n+  }\n+\n+  // Get the expected number of locality aware containers\n+  private def expectedLocalityAwareContainerNum(localityAwarePendingTasks: Int): Int = {\n+    (localityAwareTaskCores(localityAwarePendingTasks) +\n+      yarnAllocator.resource.getVirtualCores - 1) /\n+        yarnAllocator.resource.getVirtualCores\n+  }\n+\n+  // Update the expected locality distribution by considering the existing allocated container\n+  // host distributions.\n+  private def updateExpectedLocalityToCounts(localityAwarePendingTasks: Int,"
  }],
  "prId": 6394
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "This is only used in expectedLocalityAwareContainerNum, which is a short method, so you can just move it in there.\n",
    "commit": "d45fecb50aecff0c0bfeb627ec1ac8a4fc9e9644",
    "createdAt": "2015-07-09T00:53:49Z",
    "diffHunk": "@@ -0,0 +1,205 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.util.RackResolver\n+\n+import org.apache.spark.SparkConf\n+\n+private[yarn] case class ContainerLocalityPreferences(nodes: Array[String], racks: Array[String])\n+\n+private[yarn] trait ContainerPlacementStrategy {\n+\n+  /**\n+   * Calculate each container's node locality and rack locality\n+   * @param numContainer number of containers to calculate\n+   * @param numLocalityAwarePendingTasks number of locality required pending tasks\n+   * @param preferredLocalityToCounts a map to store the preferred hostname and possible task\n+   *                                  numbers running on it, used as hints for container allocation\n+   * @return node localities and rack localities, each locality is an array of string,\n+   *         the length of localities is the same as number of containers\n+   */\n+  def localityOfRequestedContainers(\n+      numContainer: Int,\n+      numLocalityAwarePendingTasks: Int,\n+      preferredLocalityToCounts: Map[String, Int]\n+    ): Array[ContainerLocalityPreferences]\n+}\n+\n+/**\n+ * This strategy is calculating the optimal locality preferences of YARN containers by considering\n+ * the node ratio of pending tasks, number of required cores/containers and and locality of current\n+ * existed containers. The target of this algorithm is to maximize the number of tasks that\n+ * would run locally.\n+ *\n+ * The details of this algorithm is described as below, if we have 20 tasks which\n+ * require (host1, host2, host3) and 10 tasks which require (host1, host2, host4),\n+ * besides each container has 2 cores and cpus per task is 1, so the required container number is\n+ * 15, and host ratio is (host1: 30, host2: 30, host3: 20, host4: 10).\n+ *\n+ * 1. If requested container number (18) is more than the required container number (15):\n+ *\n+ * requests for 5 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 5 containers with nodes: (host1, host2, host3)\n+ * requests for 5 containers with nodes: (host1, host2)\n+ * requests for 3 containers with no locality preferences.\n+ *\n+ * The placement ratio is 3 : 3 : 2 : 1, and set the additional containers with no locality\n+ * preferences.\n+ *\n+ * 2. If requested container number (10) is less than or equal to the required container number\n+ * (15):\n+ *\n+ * requests for 4 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 3 containers with nodes: (host1, host2, host3)\n+ * requests for 3 containers with nodes: (host1, host2)\n+ *\n+ * The placement ratio is 10 : 10 : 7 : 4, close to expected ratio (3 : 3 : 2 : 1)\n+ *\n+ * 3. If containers are existed but no matching localities, follow the method of 1 and 2.\n+ *\n+ * 4. If containers are existed and some localities are matched. For example if we have 1\n+ * containers on each node (host1: 1, host2: 1: host3: 1, host4: 1), and the expected containers\n+ * on each node would be (host1: 5, host2: 5, host3: 4, host4: 2),\n+ * so the newly requested containers on each node would be updated to (host1: 4, host2: 4,\n+ * host3: 3, host4: 1), 12 containers by total.\n+ *\n+ *   4.1 If requested container number (18) is more than newly required containers (12). Follow\n+ *   method 1 with updated ratio 4 : 4 : 3 : 1.\n+ *\n+ *   4.2 If request container number (10) is more than newly required containers (12). Follow\n+ *   method 2 with updated ratio 4 : 4 : 3 : 1.\n+ *\n+ * 5. If containers are existed and existing localities can fully cover the requested localities.\n+ * For example if we have 5 containers on each node (host1: 5, host2: 5, host3: 5, host4: 5),\n+ * which could cover the current requested localities. This algorithm will allocate all the\n+ * requested containers with no localities.\n+ */\n+private[yarn] class LocalityPreferredContainerPlacementStrategy(\n+    val sparkConf: SparkConf,\n+    val yarnConf: Configuration,\n+    val yarnAllocator: YarnAllocator) extends ContainerPlacementStrategy {\n+\n+  // Number of CPUs per task\n+  private val CPUS_PER_TASK = sparkConf.getInt(\"spark.task.cpus\", 1)\n+\n+  /**\n+   * Get the required cores of locality aware tasks\n+   */\n+  private def localityAwareTaskCores(localityAwarePendingTasks: Int): Int = {"
  }],
  "prId": 6394
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "I'd say `val coresPerExecutor = yarnAllocator.resource.getVirtualCores` and then use `coresPerExecutor` in the equation, which can then be places on 2 lines.\n",
    "commit": "d45fecb50aecff0c0bfeb627ec1ac8a4fc9e9644",
    "createdAt": "2015-07-09T00:56:00Z",
    "diffHunk": "@@ -0,0 +1,205 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.util.RackResolver\n+\n+import org.apache.spark.SparkConf\n+\n+private[yarn] case class ContainerLocalityPreferences(nodes: Array[String], racks: Array[String])\n+\n+private[yarn] trait ContainerPlacementStrategy {\n+\n+  /**\n+   * Calculate each container's node locality and rack locality\n+   * @param numContainer number of containers to calculate\n+   * @param numLocalityAwarePendingTasks number of locality required pending tasks\n+   * @param preferredLocalityToCounts a map to store the preferred hostname and possible task\n+   *                                  numbers running on it, used as hints for container allocation\n+   * @return node localities and rack localities, each locality is an array of string,\n+   *         the length of localities is the same as number of containers\n+   */\n+  def localityOfRequestedContainers(\n+      numContainer: Int,\n+      numLocalityAwarePendingTasks: Int,\n+      preferredLocalityToCounts: Map[String, Int]\n+    ): Array[ContainerLocalityPreferences]\n+}\n+\n+/**\n+ * This strategy is calculating the optimal locality preferences of YARN containers by considering\n+ * the node ratio of pending tasks, number of required cores/containers and and locality of current\n+ * existed containers. The target of this algorithm is to maximize the number of tasks that\n+ * would run locally.\n+ *\n+ * The details of this algorithm is described as below, if we have 20 tasks which\n+ * require (host1, host2, host3) and 10 tasks which require (host1, host2, host4),\n+ * besides each container has 2 cores and cpus per task is 1, so the required container number is\n+ * 15, and host ratio is (host1: 30, host2: 30, host3: 20, host4: 10).\n+ *\n+ * 1. If requested container number (18) is more than the required container number (15):\n+ *\n+ * requests for 5 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 5 containers with nodes: (host1, host2, host3)\n+ * requests for 5 containers with nodes: (host1, host2)\n+ * requests for 3 containers with no locality preferences.\n+ *\n+ * The placement ratio is 3 : 3 : 2 : 1, and set the additional containers with no locality\n+ * preferences.\n+ *\n+ * 2. If requested container number (10) is less than or equal to the required container number\n+ * (15):\n+ *\n+ * requests for 4 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 3 containers with nodes: (host1, host2, host3)\n+ * requests for 3 containers with nodes: (host1, host2)\n+ *\n+ * The placement ratio is 10 : 10 : 7 : 4, close to expected ratio (3 : 3 : 2 : 1)\n+ *\n+ * 3. If containers are existed but no matching localities, follow the method of 1 and 2.\n+ *\n+ * 4. If containers are existed and some localities are matched. For example if we have 1\n+ * containers on each node (host1: 1, host2: 1: host3: 1, host4: 1), and the expected containers\n+ * on each node would be (host1: 5, host2: 5, host3: 4, host4: 2),\n+ * so the newly requested containers on each node would be updated to (host1: 4, host2: 4,\n+ * host3: 3, host4: 1), 12 containers by total.\n+ *\n+ *   4.1 If requested container number (18) is more than newly required containers (12). Follow\n+ *   method 1 with updated ratio 4 : 4 : 3 : 1.\n+ *\n+ *   4.2 If request container number (10) is more than newly required containers (12). Follow\n+ *   method 2 with updated ratio 4 : 4 : 3 : 1.\n+ *\n+ * 5. If containers are existed and existing localities can fully cover the requested localities.\n+ * For example if we have 5 containers on each node (host1: 5, host2: 5, host3: 5, host4: 5),\n+ * which could cover the current requested localities. This algorithm will allocate all the\n+ * requested containers with no localities.\n+ */\n+private[yarn] class LocalityPreferredContainerPlacementStrategy(\n+    val sparkConf: SparkConf,\n+    val yarnConf: Configuration,\n+    val yarnAllocator: YarnAllocator) extends ContainerPlacementStrategy {\n+\n+  // Number of CPUs per task\n+  private val CPUS_PER_TASK = sparkConf.getInt(\"spark.task.cpus\", 1)\n+\n+  /**\n+   * Get the required cores of locality aware tasks\n+   */\n+  private def localityAwareTaskCores(localityAwarePendingTasks: Int): Int = {\n+    localityAwarePendingTasks * CPUS_PER_TASK\n+  }\n+\n+  /**\n+   * Get the expected number of locality aware containers\n+   */\n+  private def expectedLocalityAwareContainerNum(localityAwarePendingTasks: Int): Int = {\n+    (localityAwareTaskCores(localityAwarePendingTasks) +"
  }],
  "prId": 6394
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "I'd rename this method to `numExecutorsPending`, rename the argument to `numTasksPending`, and change the doc to \"Calculate the number of executors needed to satisfy the given number of pending tasks\".\n",
    "commit": "d45fecb50aecff0c0bfeb627ec1ac8a4fc9e9644",
    "createdAt": "2015-07-09T01:00:13Z",
    "diffHunk": "@@ -0,0 +1,205 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.util.RackResolver\n+\n+import org.apache.spark.SparkConf\n+\n+private[yarn] case class ContainerLocalityPreferences(nodes: Array[String], racks: Array[String])\n+\n+private[yarn] trait ContainerPlacementStrategy {\n+\n+  /**\n+   * Calculate each container's node locality and rack locality\n+   * @param numContainer number of containers to calculate\n+   * @param numLocalityAwarePendingTasks number of locality required pending tasks\n+   * @param preferredLocalityToCounts a map to store the preferred hostname and possible task\n+   *                                  numbers running on it, used as hints for container allocation\n+   * @return node localities and rack localities, each locality is an array of string,\n+   *         the length of localities is the same as number of containers\n+   */\n+  def localityOfRequestedContainers(\n+      numContainer: Int,\n+      numLocalityAwarePendingTasks: Int,\n+      preferredLocalityToCounts: Map[String, Int]\n+    ): Array[ContainerLocalityPreferences]\n+}\n+\n+/**\n+ * This strategy is calculating the optimal locality preferences of YARN containers by considering\n+ * the node ratio of pending tasks, number of required cores/containers and and locality of current\n+ * existed containers. The target of this algorithm is to maximize the number of tasks that\n+ * would run locally.\n+ *\n+ * The details of this algorithm is described as below, if we have 20 tasks which\n+ * require (host1, host2, host3) and 10 tasks which require (host1, host2, host4),\n+ * besides each container has 2 cores and cpus per task is 1, so the required container number is\n+ * 15, and host ratio is (host1: 30, host2: 30, host3: 20, host4: 10).\n+ *\n+ * 1. If requested container number (18) is more than the required container number (15):\n+ *\n+ * requests for 5 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 5 containers with nodes: (host1, host2, host3)\n+ * requests for 5 containers with nodes: (host1, host2)\n+ * requests for 3 containers with no locality preferences.\n+ *\n+ * The placement ratio is 3 : 3 : 2 : 1, and set the additional containers with no locality\n+ * preferences.\n+ *\n+ * 2. If requested container number (10) is less than or equal to the required container number\n+ * (15):\n+ *\n+ * requests for 4 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 3 containers with nodes: (host1, host2, host3)\n+ * requests for 3 containers with nodes: (host1, host2)\n+ *\n+ * The placement ratio is 10 : 10 : 7 : 4, close to expected ratio (3 : 3 : 2 : 1)\n+ *\n+ * 3. If containers are existed but no matching localities, follow the method of 1 and 2.\n+ *\n+ * 4. If containers are existed and some localities are matched. For example if we have 1\n+ * containers on each node (host1: 1, host2: 1: host3: 1, host4: 1), and the expected containers\n+ * on each node would be (host1: 5, host2: 5, host3: 4, host4: 2),\n+ * so the newly requested containers on each node would be updated to (host1: 4, host2: 4,\n+ * host3: 3, host4: 1), 12 containers by total.\n+ *\n+ *   4.1 If requested container number (18) is more than newly required containers (12). Follow\n+ *   method 1 with updated ratio 4 : 4 : 3 : 1.\n+ *\n+ *   4.2 If request container number (10) is more than newly required containers (12). Follow\n+ *   method 2 with updated ratio 4 : 4 : 3 : 1.\n+ *\n+ * 5. If containers are existed and existing localities can fully cover the requested localities.\n+ * For example if we have 5 containers on each node (host1: 5, host2: 5, host3: 5, host4: 5),\n+ * which could cover the current requested localities. This algorithm will allocate all the\n+ * requested containers with no localities.\n+ */\n+private[yarn] class LocalityPreferredContainerPlacementStrategy(\n+    val sparkConf: SparkConf,\n+    val yarnConf: Configuration,\n+    val yarnAllocator: YarnAllocator) extends ContainerPlacementStrategy {\n+\n+  // Number of CPUs per task\n+  private val CPUS_PER_TASK = sparkConf.getInt(\"spark.task.cpus\", 1)\n+\n+  /**\n+   * Get the required cores of locality aware tasks\n+   */\n+  private def localityAwareTaskCores(localityAwarePendingTasks: Int): Int = {\n+    localityAwarePendingTasks * CPUS_PER_TASK\n+  }\n+\n+  /**\n+   * Get the expected number of locality aware containers\n+   */\n+  private def expectedLocalityAwareContainerNum(localityAwarePendingTasks: Int): Int = {"
  }],
  "prId": 6394
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "nit: add space between `-1`\n",
    "commit": "d45fecb50aecff0c0bfeb627ec1ac8a4fc9e9644",
    "createdAt": "2015-07-21T19:46:08Z",
    "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.util.RackResolver\n+\n+import org.apache.spark.SparkConf\n+\n+private[yarn] case class ContainerLocalityPreferences(nodes: Array[String], racks: Array[String])\n+\n+private[yarn] trait ContainerPlacementStrategy {\n+\n+  /**\n+   * Calculate each container's node locality and rack locality\n+   * @param numContainer number of containers to calculate\n+   * @param numLocalityAwarePendingTasks number of locality required pending tasks\n+   * @param hostToLocalTaskCount a map to store the preferred hostname and possible task\n+   *                             numbers running on it, used as hints for container allocation\n+   * @return node localities and rack localities, each locality is an array of string,\n+   *         the length of localities is the same as number of containers\n+   */\n+  def localityOfRequestedContainers(\n+      numContainer: Int,\n+      numLocalityAwarePendingTasks: Int,\n+      hostToLocalTaskCount: Map[String, Int]\n+    ): Array[ContainerLocalityPreferences]\n+}\n+\n+/**\n+ * This strategy is calculating the optimal locality preferences of YARN containers by considering\n+ * the node ratio of pending tasks, number of required cores/containers and and locality of current\n+ * existed containers. The target of this algorithm is to maximize the number of tasks that\n+ * would run locally.\n+ *\n+ * The details of this algorithm is described as below, if we have 20 tasks which\n+ * require (host1, host2, host3) and 10 tasks which require (host1, host2, host4),\n+ * besides each container has 2 cores and cpus per task is 1, so the required container number is\n+ * 15, and host ratio is (host1: 30, host2: 30, host3: 20, host4: 10).\n+ *\n+ * 1. If requested container number (18) is more than the required container number (15):\n+ *\n+ * requests for 5 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 5 containers with nodes: (host1, host2, host3)\n+ * requests for 5 containers with nodes: (host1, host2)\n+ * requests for 3 containers with no locality preferences.\n+ *\n+ * The placement ratio is 3 : 3 : 2 : 1, and set the additional containers with no locality\n+ * preferences.\n+ *\n+ * 2. If requested container number (10) is less than or equal to the required container number\n+ * (15):\n+ *\n+ * requests for 4 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 3 containers with nodes: (host1, host2, host3)\n+ * requests for 3 containers with nodes: (host1, host2)\n+ *\n+ * The placement ratio is 10 : 10 : 7 : 4, close to expected ratio (3 : 3 : 2 : 1)\n+ *\n+ * 3. If containers are existed but no matching localities, follow the method of 1 and 2.\n+ *\n+ * 4. If containers are existed and some localities are matched. For example if we have 1\n+ * containers on each node (host1: 1, host2: 1: host3: 1, host4: 1), and the expected containers\n+ * on each node would be (host1: 5, host2: 5, host3: 4, host4: 2),\n+ * so the newly requested containers on each node would be updated to (host1: 4, host2: 4,\n+ * host3: 3, host4: 1), 12 containers by total.\n+ *\n+ *   4.1 If requested container number (18) is more than newly required containers (12). Follow\n+ *   method 1 with updated ratio 4 : 4 : 3 : 1.\n+ *\n+ *   4.2 If request container number (10) is more than newly required containers (12). Follow\n+ *   method 2 with updated ratio 4 : 4 : 3 : 1.\n+ *\n+ * 5. If containers are existed and existing localities can fully cover the requested localities.\n+ * For example if we have 5 containers on each node (host1: 5, host2: 5, host3: 5, host4: 5),\n+ * which could cover the current requested localities. This algorithm will allocate all the\n+ * requested containers with no localities.\n+ */\n+private[yarn] class LocalityPreferredContainerPlacementStrategy(\n+    val sparkConf: SparkConf,\n+    val yarnConf: Configuration,\n+    val yarnAllocator: YarnAllocator) extends ContainerPlacementStrategy {\n+\n+  // Number of CPUs per task\n+  private val CPUS_PER_TASK = sparkConf.getInt(\"spark.task.cpus\", 1)\n+\n+  /**\n+   * Calculate the number of executors need to satisfy the given number of pending tasks.\n+   */\n+  private def numExecutorsPending(numTasksPending: Int): Int = {\n+    val coresPerExecutor = yarnAllocator.resource.getVirtualCores\n+    (numTasksPending * CPUS_PER_TASK + coresPerExecutor -1) / coresPerExecutor"
  }],
  "prId": 6394
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "Consolidate this with LocalityPreferredContainerPlacementStrategy because there are no other implementations.  I agree that it's often nice to have interfaces separate from their implementations, even if there's only a single implementation, but I haven't seen this generally practiced in Spark.\n",
    "commit": "d45fecb50aecff0c0bfeb627ec1ac8a4fc9e9644",
    "createdAt": "2015-07-21T20:21:03Z",
    "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.util.RackResolver\n+\n+import org.apache.spark.SparkConf\n+\n+private[yarn] case class ContainerLocalityPreferences(nodes: Array[String], racks: Array[String])\n+\n+private[yarn] trait ContainerPlacementStrategy {"
  }],
  "prId": 6394
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "Put the classes's main public method before its other methods.\n",
    "commit": "d45fecb50aecff0c0bfeb627ec1ac8a4fc9e9644",
    "createdAt": "2015-07-21T20:21:29Z",
    "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.util.RackResolver\n+\n+import org.apache.spark.SparkConf\n+\n+private[yarn] case class ContainerLocalityPreferences(nodes: Array[String], racks: Array[String])\n+\n+private[yarn] trait ContainerPlacementStrategy {\n+\n+  /**\n+   * Calculate each container's node locality and rack locality\n+   * @param numContainer number of containers to calculate\n+   * @param numLocalityAwarePendingTasks number of locality required pending tasks\n+   * @param hostToLocalTaskCount a map to store the preferred hostname and possible task\n+   *                             numbers running on it, used as hints for container allocation\n+   * @return node localities and rack localities, each locality is an array of string,\n+   *         the length of localities is the same as number of containers\n+   */\n+  def localityOfRequestedContainers(\n+      numContainer: Int,\n+      numLocalityAwarePendingTasks: Int,\n+      hostToLocalTaskCount: Map[String, Int]\n+    ): Array[ContainerLocalityPreferences]\n+}\n+\n+/**\n+ * This strategy is calculating the optimal locality preferences of YARN containers by considering\n+ * the node ratio of pending tasks, number of required cores/containers and and locality of current\n+ * existed containers. The target of this algorithm is to maximize the number of tasks that\n+ * would run locally.\n+ *\n+ * The details of this algorithm is described as below, if we have 20 tasks which\n+ * require (host1, host2, host3) and 10 tasks which require (host1, host2, host4),\n+ * besides each container has 2 cores and cpus per task is 1, so the required container number is\n+ * 15, and host ratio is (host1: 30, host2: 30, host3: 20, host4: 10).\n+ *\n+ * 1. If requested container number (18) is more than the required container number (15):\n+ *\n+ * requests for 5 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 5 containers with nodes: (host1, host2, host3)\n+ * requests for 5 containers with nodes: (host1, host2)\n+ * requests for 3 containers with no locality preferences.\n+ *\n+ * The placement ratio is 3 : 3 : 2 : 1, and set the additional containers with no locality\n+ * preferences.\n+ *\n+ * 2. If requested container number (10) is less than or equal to the required container number\n+ * (15):\n+ *\n+ * requests for 4 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 3 containers with nodes: (host1, host2, host3)\n+ * requests for 3 containers with nodes: (host1, host2)\n+ *\n+ * The placement ratio is 10 : 10 : 7 : 4, close to expected ratio (3 : 3 : 2 : 1)\n+ *\n+ * 3. If containers are existed but no matching localities, follow the method of 1 and 2.\n+ *\n+ * 4. If containers are existed and some localities are matched. For example if we have 1\n+ * containers on each node (host1: 1, host2: 1: host3: 1, host4: 1), and the expected containers\n+ * on each node would be (host1: 5, host2: 5, host3: 4, host4: 2),\n+ * so the newly requested containers on each node would be updated to (host1: 4, host2: 4,\n+ * host3: 3, host4: 1), 12 containers by total.\n+ *\n+ *   4.1 If requested container number (18) is more than newly required containers (12). Follow\n+ *   method 1 with updated ratio 4 : 4 : 3 : 1.\n+ *\n+ *   4.2 If request container number (10) is more than newly required containers (12). Follow\n+ *   method 2 with updated ratio 4 : 4 : 3 : 1.\n+ *\n+ * 5. If containers are existed and existing localities can fully cover the requested localities.\n+ * For example if we have 5 containers on each node (host1: 5, host2: 5, host3: 5, host4: 5),\n+ * which could cover the current requested localities. This algorithm will allocate all the\n+ * requested containers with no localities.\n+ */\n+private[yarn] class LocalityPreferredContainerPlacementStrategy(\n+    val sparkConf: SparkConf,\n+    val yarnConf: Configuration,\n+    val yarnAllocator: YarnAllocator) extends ContainerPlacementStrategy {\n+\n+  // Number of CPUs per task\n+  private val CPUS_PER_TASK = sparkConf.getInt(\"spark.task.cpus\", 1)\n+\n+  /**\n+   * Calculate the number of executors need to satisfy the given number of pending tasks.\n+   */\n+  private def numExecutorsPending(numTasksPending: Int): Int = {\n+    val coresPerExecutor = yarnAllocator.resource.getVirtualCores\n+    (numTasksPending * CPUS_PER_TASK + coresPerExecutor -1) / coresPerExecutor\n+  }\n+\n+  /**\n+   * Update the expected host to number of containers by considering with allocated containers.\n+   * @param localityAwarePendingTasks number of locality aware pending tasks\n+   * @param hostToLocalTaskCount a map to store the preferred hostname and possible task\n+   *                             numbers running on it, used as hints for container allocation\n+   * @return a map with hostname as key and required number of containers on this host as value\n+   */\n+  private def updateExpectedHostToContainerCount(\n+      localityAwarePendingTasks: Int,\n+      hostToLocalTaskCount: Map[String, Int]\n+    ): Map[String, Int] = {\n+    val totalLocalTaskNum = hostToLocalTaskCount.values.sum\n+    hostToLocalTaskCount.map { case (host, count) =>\n+      val expectedCount =\n+        count.toDouble * numExecutorsPending(localityAwarePendingTasks) / totalLocalTaskNum\n+      val existedCount = yarnAllocator.allocatedHostToContainersMap.get(host)\n+        .map(_.size)\n+        .getOrElse(0)\n+\n+      // If existing container can not fully satisfy the expected number of container,\n+      // the required container number is expected count minus existed count. Otherwise the\n+      // required container number is 0.\n+      (host, math.max(0, (expectedCount - existedCount).ceil.toInt))\n+    }\n+  }\n+\n+  /**\n+   * Calculate each container's node locality and rack locality\n+   * @param numContainer number of containers to calculate\n+   * @param numLocalityAwarePendingTasks number of locality required pending tasks\n+   * @param hostToLocalTaskCount a map to store the preferred hostname and possible task\n+   *                             numbers running on it, used as hints for container allocation\n+   * @return node localities and rack localities, each locality is an array of string,\n+   *         the length of localities is the same as number of containers\n+   */\n+  override def localityOfRequestedContainers("
  }],
  "prId": 6394
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "`existed` -> `existing`\n",
    "commit": "d45fecb50aecff0c0bfeb627ec1ac8a4fc9e9644",
    "createdAt": "2015-07-21T20:38:01Z",
    "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.util.RackResolver\n+\n+import org.apache.spark.SparkConf\n+\n+private[yarn] case class ContainerLocalityPreferences(nodes: Array[String], racks: Array[String])\n+\n+private[yarn] trait ContainerPlacementStrategy {\n+\n+  /**\n+   * Calculate each container's node locality and rack locality\n+   * @param numContainer number of containers to calculate\n+   * @param numLocalityAwarePendingTasks number of locality required pending tasks\n+   * @param hostToLocalTaskCount a map to store the preferred hostname and possible task\n+   *                             numbers running on it, used as hints for container allocation\n+   * @return node localities and rack localities, each locality is an array of string,\n+   *         the length of localities is the same as number of containers\n+   */\n+  def localityOfRequestedContainers(\n+      numContainer: Int,\n+      numLocalityAwarePendingTasks: Int,\n+      hostToLocalTaskCount: Map[String, Int]\n+    ): Array[ContainerLocalityPreferences]\n+}\n+\n+/**\n+ * This strategy is calculating the optimal locality preferences of YARN containers by considering\n+ * the node ratio of pending tasks, number of required cores/containers and and locality of current\n+ * existed containers. The target of this algorithm is to maximize the number of tasks that"
  }],
  "prId": 6394
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "Replace this with \"Consider a situation in which we have 20 tasks...\"\n",
    "commit": "d45fecb50aecff0c0bfeb627ec1ac8a4fc9e9644",
    "createdAt": "2015-07-21T20:38:44Z",
    "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.util.RackResolver\n+\n+import org.apache.spark.SparkConf\n+\n+private[yarn] case class ContainerLocalityPreferences(nodes: Array[String], racks: Array[String])\n+\n+private[yarn] trait ContainerPlacementStrategy {\n+\n+  /**\n+   * Calculate each container's node locality and rack locality\n+   * @param numContainer number of containers to calculate\n+   * @param numLocalityAwarePendingTasks number of locality required pending tasks\n+   * @param hostToLocalTaskCount a map to store the preferred hostname and possible task\n+   *                             numbers running on it, used as hints for container allocation\n+   * @return node localities and rack localities, each locality is an array of string,\n+   *         the length of localities is the same as number of containers\n+   */\n+  def localityOfRequestedContainers(\n+      numContainer: Int,\n+      numLocalityAwarePendingTasks: Int,\n+      hostToLocalTaskCount: Map[String, Int]\n+    ): Array[ContainerLocalityPreferences]\n+}\n+\n+/**\n+ * This strategy is calculating the optimal locality preferences of YARN containers by considering\n+ * the node ratio of pending tasks, number of required cores/containers and and locality of current\n+ * existed containers. The target of this algorithm is to maximize the number of tasks that\n+ * would run locally.\n+ *\n+ * The details of this algorithm is described as below, if we have 20 tasks which"
  }],
  "prId": 6394
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "What exact is meant by \"matching localities\"?\n",
    "commit": "d45fecb50aecff0c0bfeb627ec1ac8a4fc9e9644",
    "createdAt": "2015-07-21T20:40:19Z",
    "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.util.RackResolver\n+\n+import org.apache.spark.SparkConf\n+\n+private[yarn] case class ContainerLocalityPreferences(nodes: Array[String], racks: Array[String])\n+\n+private[yarn] trait ContainerPlacementStrategy {\n+\n+  /**\n+   * Calculate each container's node locality and rack locality\n+   * @param numContainer number of containers to calculate\n+   * @param numLocalityAwarePendingTasks number of locality required pending tasks\n+   * @param hostToLocalTaskCount a map to store the preferred hostname and possible task\n+   *                             numbers running on it, used as hints for container allocation\n+   * @return node localities and rack localities, each locality is an array of string,\n+   *         the length of localities is the same as number of containers\n+   */\n+  def localityOfRequestedContainers(\n+      numContainer: Int,\n+      numLocalityAwarePendingTasks: Int,\n+      hostToLocalTaskCount: Map[String, Int]\n+    ): Array[ContainerLocalityPreferences]\n+}\n+\n+/**\n+ * This strategy is calculating the optimal locality preferences of YARN containers by considering\n+ * the node ratio of pending tasks, number of required cores/containers and and locality of current\n+ * existed containers. The target of this algorithm is to maximize the number of tasks that\n+ * would run locally.\n+ *\n+ * The details of this algorithm is described as below, if we have 20 tasks which\n+ * require (host1, host2, host3) and 10 tasks which require (host1, host2, host4),\n+ * besides each container has 2 cores and cpus per task is 1, so the required container number is\n+ * 15, and host ratio is (host1: 30, host2: 30, host3: 20, host4: 10).\n+ *\n+ * 1. If requested container number (18) is more than the required container number (15):\n+ *\n+ * requests for 5 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 5 containers with nodes: (host1, host2, host3)\n+ * requests for 5 containers with nodes: (host1, host2)\n+ * requests for 3 containers with no locality preferences.\n+ *\n+ * The placement ratio is 3 : 3 : 2 : 1, and set the additional containers with no locality\n+ * preferences.\n+ *\n+ * 2. If requested container number (10) is less than or equal to the required container number\n+ * (15):\n+ *\n+ * requests for 4 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 3 containers with nodes: (host1, host2, host3)\n+ * requests for 3 containers with nodes: (host1, host2)\n+ *\n+ * The placement ratio is 10 : 10 : 7 : 4, close to expected ratio (3 : 3 : 2 : 1)\n+ *\n+ * 3. If containers are existed but no matching localities, follow the method of 1 and 2."
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "I think \"matching localities\" means the existed container's localities that matches to the expected localities. Here means if all the existed container's localities cannot match the expected localities.\n",
    "commit": "d45fecb50aecff0c0bfeb627ec1ac8a4fc9e9644",
    "createdAt": "2015-07-23T07:39:23Z",
    "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.util.RackResolver\n+\n+import org.apache.spark.SparkConf\n+\n+private[yarn] case class ContainerLocalityPreferences(nodes: Array[String], racks: Array[String])\n+\n+private[yarn] trait ContainerPlacementStrategy {\n+\n+  /**\n+   * Calculate each container's node locality and rack locality\n+   * @param numContainer number of containers to calculate\n+   * @param numLocalityAwarePendingTasks number of locality required pending tasks\n+   * @param hostToLocalTaskCount a map to store the preferred hostname and possible task\n+   *                             numbers running on it, used as hints for container allocation\n+   * @return node localities and rack localities, each locality is an array of string,\n+   *         the length of localities is the same as number of containers\n+   */\n+  def localityOfRequestedContainers(\n+      numContainer: Int,\n+      numLocalityAwarePendingTasks: Int,\n+      hostToLocalTaskCount: Map[String, Int]\n+    ): Array[ContainerLocalityPreferences]\n+}\n+\n+/**\n+ * This strategy is calculating the optimal locality preferences of YARN containers by considering\n+ * the node ratio of pending tasks, number of required cores/containers and and locality of current\n+ * existed containers. The target of this algorithm is to maximize the number of tasks that\n+ * would run locally.\n+ *\n+ * The details of this algorithm is described as below, if we have 20 tasks which\n+ * require (host1, host2, host3) and 10 tasks which require (host1, host2, host4),\n+ * besides each container has 2 cores and cpus per task is 1, so the required container number is\n+ * 15, and host ratio is (host1: 30, host2: 30, host3: 20, host4: 10).\n+ *\n+ * 1. If requested container number (18) is more than the required container number (15):\n+ *\n+ * requests for 5 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 5 containers with nodes: (host1, host2, host3)\n+ * requests for 5 containers with nodes: (host1, host2)\n+ * requests for 3 containers with no locality preferences.\n+ *\n+ * The placement ratio is 3 : 3 : 2 : 1, and set the additional containers with no locality\n+ * preferences.\n+ *\n+ * 2. If requested container number (10) is less than or equal to the required container number\n+ * (15):\n+ *\n+ * requests for 4 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 3 containers with nodes: (host1, host2, host3)\n+ * requests for 3 containers with nodes: (host1, host2)\n+ *\n+ * The placement ratio is 10 : 10 : 7 : 4, close to expected ratio (3 : 3 : 2 : 1)\n+ *\n+ * 3. If containers are existed but no matching localities, follow the method of 1 and 2."
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "I will change a better word.\n",
    "commit": "d45fecb50aecff0c0bfeb627ec1ac8a4fc9e9644",
    "createdAt": "2015-07-23T07:40:42Z",
    "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.util.RackResolver\n+\n+import org.apache.spark.SparkConf\n+\n+private[yarn] case class ContainerLocalityPreferences(nodes: Array[String], racks: Array[String])\n+\n+private[yarn] trait ContainerPlacementStrategy {\n+\n+  /**\n+   * Calculate each container's node locality and rack locality\n+   * @param numContainer number of containers to calculate\n+   * @param numLocalityAwarePendingTasks number of locality required pending tasks\n+   * @param hostToLocalTaskCount a map to store the preferred hostname and possible task\n+   *                             numbers running on it, used as hints for container allocation\n+   * @return node localities and rack localities, each locality is an array of string,\n+   *         the length of localities is the same as number of containers\n+   */\n+  def localityOfRequestedContainers(\n+      numContainer: Int,\n+      numLocalityAwarePendingTasks: Int,\n+      hostToLocalTaskCount: Map[String, Int]\n+    ): Array[ContainerLocalityPreferences]\n+}\n+\n+/**\n+ * This strategy is calculating the optimal locality preferences of YARN containers by considering\n+ * the node ratio of pending tasks, number of required cores/containers and and locality of current\n+ * existed containers. The target of this algorithm is to maximize the number of tasks that\n+ * would run locally.\n+ *\n+ * The details of this algorithm is described as below, if we have 20 tasks which\n+ * require (host1, host2, host3) and 10 tasks which require (host1, host2, host4),\n+ * besides each container has 2 cores and cpus per task is 1, so the required container number is\n+ * 15, and host ratio is (host1: 30, host2: 30, host3: 20, host4: 10).\n+ *\n+ * 1. If requested container number (18) is more than the required container number (15):\n+ *\n+ * requests for 5 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 5 containers with nodes: (host1, host2, host3)\n+ * requests for 5 containers with nodes: (host1, host2)\n+ * requests for 3 containers with no locality preferences.\n+ *\n+ * The placement ratio is 3 : 3 : 2 : 1, and set the additional containers with no locality\n+ * preferences.\n+ *\n+ * 2. If requested container number (10) is less than or equal to the required container number\n+ * (15):\n+ *\n+ * requests for 4 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 3 containers with nodes: (host1, host2, host3)\n+ * requests for 3 containers with nodes: (host1, host2)\n+ *\n+ * The placement ratio is 10 : 10 : 7 : 4, close to expected ratio (3 : 3 : 2 : 1)\n+ *\n+ * 3. If containers are existed but no matching localities, follow the method of 1 and 2."
  }],
  "prId": 6394
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "\"If containers are existed\" -> \"If containers exist\"\n",
    "commit": "d45fecb50aecff0c0bfeb627ec1ac8a4fc9e9644",
    "createdAt": "2015-07-21T20:41:20Z",
    "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.util.RackResolver\n+\n+import org.apache.spark.SparkConf\n+\n+private[yarn] case class ContainerLocalityPreferences(nodes: Array[String], racks: Array[String])\n+\n+private[yarn] trait ContainerPlacementStrategy {\n+\n+  /**\n+   * Calculate each container's node locality and rack locality\n+   * @param numContainer number of containers to calculate\n+   * @param numLocalityAwarePendingTasks number of locality required pending tasks\n+   * @param hostToLocalTaskCount a map to store the preferred hostname and possible task\n+   *                             numbers running on it, used as hints for container allocation\n+   * @return node localities and rack localities, each locality is an array of string,\n+   *         the length of localities is the same as number of containers\n+   */\n+  def localityOfRequestedContainers(\n+      numContainer: Int,\n+      numLocalityAwarePendingTasks: Int,\n+      hostToLocalTaskCount: Map[String, Int]\n+    ): Array[ContainerLocalityPreferences]\n+}\n+\n+/**\n+ * This strategy is calculating the optimal locality preferences of YARN containers by considering\n+ * the node ratio of pending tasks, number of required cores/containers and and locality of current\n+ * existed containers. The target of this algorithm is to maximize the number of tasks that\n+ * would run locally.\n+ *\n+ * The details of this algorithm is described as below, if we have 20 tasks which\n+ * require (host1, host2, host3) and 10 tasks which require (host1, host2, host4),\n+ * besides each container has 2 cores and cpus per task is 1, so the required container number is\n+ * 15, and host ratio is (host1: 30, host2: 30, host3: 20, host4: 10).\n+ *\n+ * 1. If requested container number (18) is more than the required container number (15):\n+ *\n+ * requests for 5 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 5 containers with nodes: (host1, host2, host3)\n+ * requests for 5 containers with nodes: (host1, host2)\n+ * requests for 3 containers with no locality preferences.\n+ *\n+ * The placement ratio is 3 : 3 : 2 : 1, and set the additional containers with no locality\n+ * preferences.\n+ *\n+ * 2. If requested container number (10) is less than or equal to the required container number\n+ * (15):\n+ *\n+ * requests for 4 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 3 containers with nodes: (host1, host2, host3)\n+ * requests for 3 containers with nodes: (host1, host2)\n+ *\n+ * The placement ratio is 10 : 10 : 7 : 4, close to expected ratio (3 : 3 : 2 : 1)\n+ *\n+ * 3. If containers are existed but no matching localities, follow the method of 1 and 2.\n+ *\n+ * 4. If containers are existed and some localities are matched. For example if we have 1"
  }],
  "prId": 6394
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "\"update\" is a slightly confusing start to the class name because normally it would imply that some internal state is being modified.  It's also unclear what we're updating because we haven't computed the expectedHostToContainerCount before.  Maybe just call this `expectedHostToContainerCount\"?\n",
    "commit": "d45fecb50aecff0c0bfeb627ec1ac8a4fc9e9644",
    "createdAt": "2015-07-21T20:49:35Z",
    "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.util.RackResolver\n+\n+import org.apache.spark.SparkConf\n+\n+private[yarn] case class ContainerLocalityPreferences(nodes: Array[String], racks: Array[String])\n+\n+private[yarn] trait ContainerPlacementStrategy {\n+\n+  /**\n+   * Calculate each container's node locality and rack locality\n+   * @param numContainer number of containers to calculate\n+   * @param numLocalityAwarePendingTasks number of locality required pending tasks\n+   * @param hostToLocalTaskCount a map to store the preferred hostname and possible task\n+   *                             numbers running on it, used as hints for container allocation\n+   * @return node localities and rack localities, each locality is an array of string,\n+   *         the length of localities is the same as number of containers\n+   */\n+  def localityOfRequestedContainers(\n+      numContainer: Int,\n+      numLocalityAwarePendingTasks: Int,\n+      hostToLocalTaskCount: Map[String, Int]\n+    ): Array[ContainerLocalityPreferences]\n+}\n+\n+/**\n+ * This strategy is calculating the optimal locality preferences of YARN containers by considering\n+ * the node ratio of pending tasks, number of required cores/containers and and locality of current\n+ * existed containers. The target of this algorithm is to maximize the number of tasks that\n+ * would run locally.\n+ *\n+ * The details of this algorithm is described as below, if we have 20 tasks which\n+ * require (host1, host2, host3) and 10 tasks which require (host1, host2, host4),\n+ * besides each container has 2 cores and cpus per task is 1, so the required container number is\n+ * 15, and host ratio is (host1: 30, host2: 30, host3: 20, host4: 10).\n+ *\n+ * 1. If requested container number (18) is more than the required container number (15):\n+ *\n+ * requests for 5 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 5 containers with nodes: (host1, host2, host3)\n+ * requests for 5 containers with nodes: (host1, host2)\n+ * requests for 3 containers with no locality preferences.\n+ *\n+ * The placement ratio is 3 : 3 : 2 : 1, and set the additional containers with no locality\n+ * preferences.\n+ *\n+ * 2. If requested container number (10) is less than or equal to the required container number\n+ * (15):\n+ *\n+ * requests for 4 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 3 containers with nodes: (host1, host2, host3)\n+ * requests for 3 containers with nodes: (host1, host2)\n+ *\n+ * The placement ratio is 10 : 10 : 7 : 4, close to expected ratio (3 : 3 : 2 : 1)\n+ *\n+ * 3. If containers are existed but no matching localities, follow the method of 1 and 2.\n+ *\n+ * 4. If containers are existed and some localities are matched. For example if we have 1\n+ * containers on each node (host1: 1, host2: 1: host3: 1, host4: 1), and the expected containers\n+ * on each node would be (host1: 5, host2: 5, host3: 4, host4: 2),\n+ * so the newly requested containers on each node would be updated to (host1: 4, host2: 4,\n+ * host3: 3, host4: 1), 12 containers by total.\n+ *\n+ *   4.1 If requested container number (18) is more than newly required containers (12). Follow\n+ *   method 1 with updated ratio 4 : 4 : 3 : 1.\n+ *\n+ *   4.2 If request container number (10) is more than newly required containers (12). Follow\n+ *   method 2 with updated ratio 4 : 4 : 3 : 1.\n+ *\n+ * 5. If containers are existed and existing localities can fully cover the requested localities.\n+ * For example if we have 5 containers on each node (host1: 5, host2: 5, host3: 5, host4: 5),\n+ * which could cover the current requested localities. This algorithm will allocate all the\n+ * requested containers with no localities.\n+ */\n+private[yarn] class LocalityPreferredContainerPlacementStrategy(\n+    val sparkConf: SparkConf,\n+    val yarnConf: Configuration,\n+    val yarnAllocator: YarnAllocator) extends ContainerPlacementStrategy {\n+\n+  // Number of CPUs per task\n+  private val CPUS_PER_TASK = sparkConf.getInt(\"spark.task.cpus\", 1)\n+\n+  /**\n+   * Calculate the number of executors need to satisfy the given number of pending tasks.\n+   */\n+  private def numExecutorsPending(numTasksPending: Int): Int = {\n+    val coresPerExecutor = yarnAllocator.resource.getVirtualCores\n+    (numTasksPending * CPUS_PER_TASK + coresPerExecutor -1) / coresPerExecutor\n+  }\n+\n+  /**\n+   * Update the expected host to number of containers by considering with allocated containers.\n+   * @param localityAwarePendingTasks number of locality aware pending tasks\n+   * @param hostToLocalTaskCount a map to store the preferred hostname and possible task\n+   *                             numbers running on it, used as hints for container allocation\n+   * @return a map with hostname as key and required number of containers on this host as value\n+   */\n+  private def updateExpectedHostToContainerCount("
  }],
  "prId": 6394
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "Instead of passing in the whole yarnAllocator, can we pass in `resource` and `allocatedHostToContainersMap` explicitly as arguments to the `localityOfRequestedContainers` method so that it's clear what information is used to make our decision?\n",
    "commit": "d45fecb50aecff0c0bfeb627ec1ac8a4fc9e9644",
    "createdAt": "2015-07-21T20:52:18Z",
    "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.util.RackResolver\n+\n+import org.apache.spark.SparkConf\n+\n+private[yarn] case class ContainerLocalityPreferences(nodes: Array[String], racks: Array[String])\n+\n+private[yarn] trait ContainerPlacementStrategy {\n+\n+  /**\n+   * Calculate each container's node locality and rack locality\n+   * @param numContainer number of containers to calculate\n+   * @param numLocalityAwarePendingTasks number of locality required pending tasks\n+   * @param hostToLocalTaskCount a map to store the preferred hostname and possible task\n+   *                             numbers running on it, used as hints for container allocation\n+   * @return node localities and rack localities, each locality is an array of string,\n+   *         the length of localities is the same as number of containers\n+   */\n+  def localityOfRequestedContainers(\n+      numContainer: Int,\n+      numLocalityAwarePendingTasks: Int,\n+      hostToLocalTaskCount: Map[String, Int]\n+    ): Array[ContainerLocalityPreferences]\n+}\n+\n+/**\n+ * This strategy is calculating the optimal locality preferences of YARN containers by considering\n+ * the node ratio of pending tasks, number of required cores/containers and and locality of current\n+ * existed containers. The target of this algorithm is to maximize the number of tasks that\n+ * would run locally.\n+ *\n+ * The details of this algorithm is described as below, if we have 20 tasks which\n+ * require (host1, host2, host3) and 10 tasks which require (host1, host2, host4),\n+ * besides each container has 2 cores and cpus per task is 1, so the required container number is\n+ * 15, and host ratio is (host1: 30, host2: 30, host3: 20, host4: 10).\n+ *\n+ * 1. If requested container number (18) is more than the required container number (15):\n+ *\n+ * requests for 5 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 5 containers with nodes: (host1, host2, host3)\n+ * requests for 5 containers with nodes: (host1, host2)\n+ * requests for 3 containers with no locality preferences.\n+ *\n+ * The placement ratio is 3 : 3 : 2 : 1, and set the additional containers with no locality\n+ * preferences.\n+ *\n+ * 2. If requested container number (10) is less than or equal to the required container number\n+ * (15):\n+ *\n+ * requests for 4 containers with nodes: (host1, host2, host3, host4)\n+ * requests for 3 containers with nodes: (host1, host2, host3)\n+ * requests for 3 containers with nodes: (host1, host2)\n+ *\n+ * The placement ratio is 10 : 10 : 7 : 4, close to expected ratio (3 : 3 : 2 : 1)\n+ *\n+ * 3. If containers are existed but no matching localities, follow the method of 1 and 2.\n+ *\n+ * 4. If containers are existed and some localities are matched. For example if we have 1\n+ * containers on each node (host1: 1, host2: 1: host3: 1, host4: 1), and the expected containers\n+ * on each node would be (host1: 5, host2: 5, host3: 4, host4: 2),\n+ * so the newly requested containers on each node would be updated to (host1: 4, host2: 4,\n+ * host3: 3, host4: 1), 12 containers by total.\n+ *\n+ *   4.1 If requested container number (18) is more than newly required containers (12). Follow\n+ *   method 1 with updated ratio 4 : 4 : 3 : 1.\n+ *\n+ *   4.2 If request container number (10) is more than newly required containers (12). Follow\n+ *   method 2 with updated ratio 4 : 4 : 3 : 1.\n+ *\n+ * 5. If containers are existed and existing localities can fully cover the requested localities.\n+ * For example if we have 5 containers on each node (host1: 5, host2: 5, host3: 5, host4: 5),\n+ * which could cover the current requested localities. This algorithm will allocate all the\n+ * requested containers with no localities.\n+ */\n+private[yarn] class LocalityPreferredContainerPlacementStrategy(\n+    val sparkConf: SparkConf,\n+    val yarnConf: Configuration,\n+    val yarnAllocator: YarnAllocator) extends ContainerPlacementStrategy {"
  }],
  "prId": 6394
}]