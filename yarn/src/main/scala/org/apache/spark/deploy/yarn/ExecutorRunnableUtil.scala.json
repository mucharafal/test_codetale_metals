[{
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "This line is underindented relative to `filter`; I'd move the `filter { case (k, v) =>` to the previous line, and the matching brace to the next line.\n",
    "commit": "8cdf96dd7ec41ade12c49c3a2a9f59fd4e842a0c",
    "createdAt": "2014-12-30T17:46:43Z",
    "diffHunk": "@@ -76,7 +76,9 @@ trait ExecutorRunnableUtil extends Logging {\n     // uses Akka to connect to the scheduler, the akka settings are needed as well as the\n     // authentication settings.\n     sparkConf.getAll.\n-      filter { case (k, v) => k.startsWith(\"spark.auth\") || k.startsWith(\"spark.akka\") }.\n+      filter { case (k, v) =>\n+      k.startsWith(\"spark.auth\") || k.startsWith(\"spark.akka\") || k.equals(\"spark.port.maxRetries\")"
  }],
  "prId": 3841
}, {
  "comments": [{
    "author": {
      "login": "ash211"
    },
    "body": "The crux of the problem here seems to be that YARN executors receive some config settings via System properties as JVM flags, and other config settings via the SparkEnv object they receive after getting connected up to the rest of the cluster.\n\nThis whitelist is for passing config options that must be received by the executor before connection, and `spark.port.maxRetries` seems to be one of those indeed.\n",
    "commit": "8cdf96dd7ec41ade12c49c3a2a9f59fd4e842a0c",
    "createdAt": "2015-01-05T19:18:23Z",
    "diffHunk": "@@ -75,8 +75,9 @@ trait ExecutorRunnableUtil extends Logging {\n     // registers with the Scheduler and transfers the spark configs. Since the Executor backend\n     // uses Akka to connect to the scheduler, the akka settings are needed as well as the\n     // authentication settings.\n-    sparkConf.getAll.\n-      filter { case (k, v) => k.startsWith(\"spark.auth\") || k.startsWith(\"spark.akka\") }.\n+    sparkConf.getAll.filter { case (k, v) =>\n+      k.startsWith(\"spark.auth\") || k.startsWith(\"spark.akka\") || k.equals(\"spark.port.maxRetries\")\n+    }."
  }, {
    "author": {
      "login": "WangTaoTheTonic"
    },
    "body": "I haven't test it on standalone mode. Will do that later.\n",
    "commit": "8cdf96dd7ec41ade12c49c3a2a9f59fd4e842a0c",
    "createdAt": "2015-01-08T02:02:47Z",
    "diffHunk": "@@ -75,8 +75,9 @@ trait ExecutorRunnableUtil extends Logging {\n     // registers with the Scheduler and transfers the spark configs. Since the Executor backend\n     // uses Akka to connect to the scheduler, the akka settings are needed as well as the\n     // authentication settings.\n-    sparkConf.getAll.\n-      filter { case (k, v) => k.startsWith(\"spark.auth\") || k.startsWith(\"spark.akka\") }.\n+    sparkConf.getAll.filter { case (k, v) =>\n+      k.startsWith(\"spark.auth\") || k.startsWith(\"spark.akka\") || k.equals(\"spark.port.maxRetries\")\n+    }."
  }],
  "prId": 3841
}]