[{
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "Nit: import ordering should be alphabetical\n",
    "commit": "6c9676a219f2b910e317e329aec39c2053221829",
    "createdAt": "2015-01-21T08:16:12Z",
    "diffHunk": "@@ -19,7 +19,7 @@ package org.apache.spark.deploy.yarn\n \n import scala.collection.mutable.ArrayBuffer\n \n-import org.apache.spark.SparkConf\n+import org.apache.spark.{SparkException, SparkConf}"
  }],
  "prId": 4123
}, {
  "comments": [{
    "author": {
      "login": "tgravescs"
    },
    "body": "only nit here is that I might not have specified it via spark.executor.cores but rather via the spark-submit --executor-cores option.  Perhaps we should just say \"Executor cores must\".  \n",
    "commit": "6c9676a219f2b910e317e329aec39c2053221829",
    "createdAt": "2015-01-21T14:52:27Z",
    "diffHunk": "@@ -95,6 +95,10 @@ private[spark] class ClientArguments(args: Array[String], sparkConf: SparkConf)\n       throw new IllegalArgumentException(\n         \"You must specify at least 1 executor!\\n\" + getUsageMessage())\n     }\n+    if (executorCores < sparkConf.getInt(\"spark.task.cpus\", 1)) {\n+      throw new SparkException(\"spark.executor.cores must not be less than \" +"
  }],
  "prId": 4123
}]