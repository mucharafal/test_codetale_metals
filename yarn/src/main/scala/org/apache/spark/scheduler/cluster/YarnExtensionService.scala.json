[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "\"sservice\"\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:48:32Z",
    "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.io.Closeable\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.service.AbstractService\n+import org.apache.hadoop.service.Service.STATE\n+import org.apache.hadoop.util.ShutdownHookManager\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * An extension sservice that can be loaded into a Spark YARN application."
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`SparkContext` now has its own shutdown hook, so this shouldn't happen anymore.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:50:39Z",
    "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.io.Closeable\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.service.AbstractService\n+import org.apache.hadoop.service.Service.STATE\n+import org.apache.hadoop.util.ShutdownHookManager\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * An extension sservice that can be loaded into a Spark YARN application.\n+ * A Service that can be started and closed (==stop).\n+ *\n+ * The `close()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait YarnExtensionService extends Closeable {\n+\n+  /**\n+   * For Yarn services, SparkContext, and ApplicationId is the basic info required.\n+   * This operation must be called before `init`\n+   * @param sc spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   */\n+  def start(sc: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean\n+}\n+\n+\n+/**\n+ * Class which loads child Yarn extension Services from the configuration,\n+ * and closes them all when closed/stopped itself.\n+ *\n+ * It extends YARN's `AbstractService` to pick up its state model, and adds an alternate entry\n+ * point for actually configuring and starting the service.\n+ *\n+ * An instance may be configured to register a shutdown hook. This addresses the issue in\n+ * which a spark context is not always closed in an application, especially a `yarn-client`"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "OK, cutting\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T17:27:03Z",
    "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.io.Closeable\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.service.AbstractService\n+import org.apache.hadoop.service.Service.STATE\n+import org.apache.hadoop.util.ShutdownHookManager\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * An extension sservice that can be loaded into a Spark YARN application.\n+ * A Service that can be started and closed (==stop).\n+ *\n+ * The `close()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait YarnExtensionService extends Closeable {\n+\n+  /**\n+   * For Yarn services, SparkContext, and ApplicationId is the basic info required.\n+   * This operation must be called before `init`\n+   * @param sc spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   */\n+  def start(sc: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean\n+}\n+\n+\n+/**\n+ * Class which loads child Yarn extension Services from the configuration,\n+ * and closes them all when closed/stopped itself.\n+ *\n+ * It extends YARN's `AbstractService` to pick up its state model, and adds an alternate entry\n+ * point for actually configuring and starting the service.\n+ *\n+ * An instance may be configured to register a shutdown hook. This addresses the issue in\n+ * which a spark context is not always closed in an application, especially a `yarn-client`"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "move to previous line\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:51:10Z",
    "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.io.Closeable\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.service.AbstractService\n+import org.apache.hadoop.service.Service.STATE\n+import org.apache.hadoop.util.ShutdownHookManager\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * An extension sservice that can be loaded into a Spark YARN application.\n+ * A Service that can be started and closed (==stop).\n+ *\n+ * The `close()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait YarnExtensionService extends Closeable {\n+\n+  /**\n+   * For Yarn services, SparkContext, and ApplicationId is the basic info required.\n+   * This operation must be called before `init`\n+   * @param sc spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   */\n+  def start(sc: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean\n+}\n+\n+\n+/**\n+ * Class which loads child Yarn extension Services from the configuration,\n+ * and closes them all when closed/stopped itself.\n+ *\n+ * It extends YARN's `AbstractService` to pick up its state model, and adds an alternate entry\n+ * point for actually configuring and starting the service.\n+ *\n+ * An instance may be configured to register a shutdown hook. This addresses the issue in\n+ * which a spark context is not always closed in an application, especially a `yarn-client`\n+ * application. The shutdown hook allows services an opportunity to perform any final\n+ * actions their `Service.stop` invocations trigger.\n+ */\n+private[spark] class YarnExtensionServices extends AbstractService(\"YarnExtensionServices\")\n+    with Logging {\n+  private var services: List[YarnExtensionService] = Nil\n+  private var sparkContext: SparkContext = _\n+  private var appId: ApplicationId = _\n+  private var attemptId: Option[ApplicationAttemptId] = _\n+  private val started = new AtomicBoolean(false)\n+  private var registerShutdownHook = false\n+  private var shutdownHookPriority = 0\n+\n+  /**\n+   * This is the routine to optionally be registered as a shutdown hook.\n+   * It logs the invocation and then calls the `close()` method to stop the\n+   * service instance. It is public to aid in testing.\n+   */\n+  val shutdownHook = new Runnable {\n+    override def run(): Unit = {\n+      logInfo(s\"In shutdown hook for $this\")\n+      try {\n+        stop()\n+      }\n+      catch {"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Spark has its own `ShutdownHookManager`; also, I don't think this is necessary anymore.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:52:18Z",
    "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.io.Closeable\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.service.AbstractService\n+import org.apache.hadoop.service.Service.STATE\n+import org.apache.hadoop.util.ShutdownHookManager"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "cutting out all the shutdown logic if it is not needed; shutdown hooks are ugly things anyway\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T17:34:06Z",
    "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.io.Closeable\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.service.AbstractService\n+import org.apache.hadoop.service.Service.STATE\n+import org.apache.hadoop.util.ShutdownHookManager"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "I prefer the slightly less noisy approach:\n\n```\nservices = sparkContext.getConf.getOption(YarnExtensionServices.SPARK_YARN_SERVICES)\n  .flatMap(_.split(\",\"))\n  .map(_.trim())\n  .filter(!_.isEmpty)\n  .flatMap { sClass => ...\n  }\n```\n\n(Or something.) You lose the \"no services\" debug message but it's not like it's useful.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-09-16T00:55:31Z",
    "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.io.Closeable\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.service.AbstractService\n+import org.apache.hadoop.service.Service.STATE\n+import org.apache.hadoop.util.ShutdownHookManager\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * An extension sservice that can be loaded into a Spark YARN application.\n+ * A Service that can be started and closed (==stop).\n+ *\n+ * The `close()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait YarnExtensionService extends Closeable {\n+\n+  /**\n+   * For Yarn services, SparkContext, and ApplicationId is the basic info required.\n+   * This operation must be called before `init`\n+   * @param sc spark context\n+   * @param appId YARN application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   */\n+  def start(sc: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId]): Boolean\n+}\n+\n+\n+/**\n+ * Class which loads child Yarn extension Services from the configuration,\n+ * and closes them all when closed/stopped itself.\n+ *\n+ * It extends YARN's `AbstractService` to pick up its state model, and adds an alternate entry\n+ * point for actually configuring and starting the service.\n+ *\n+ * An instance may be configured to register a shutdown hook. This addresses the issue in\n+ * which a spark context is not always closed in an application, especially a `yarn-client`\n+ * application. The shutdown hook allows services an opportunity to perform any final\n+ * actions their `Service.stop` invocations trigger.\n+ */\n+private[spark] class YarnExtensionServices extends AbstractService(\"YarnExtensionServices\")\n+    with Logging {\n+  private var services: List[YarnExtensionService] = Nil\n+  private var sparkContext: SparkContext = _\n+  private var appId: ApplicationId = _\n+  private var attemptId: Option[ApplicationAttemptId] = _\n+  private val started = new AtomicBoolean(false)\n+  private var registerShutdownHook = false\n+  private var shutdownHookPriority = 0\n+\n+  /**\n+   * This is the routine to optionally be registered as a shutdown hook.\n+   * It logs the invocation and then calls the `close()` method to stop the\n+   * service instance. It is public to aid in testing.\n+   */\n+  val shutdownHook = new Runnable {\n+    override def run(): Unit = {\n+      logInfo(s\"In shutdown hook for $this\")\n+      try {\n+        stop()\n+      }\n+      catch {\n+        case e: Exception =>\n+          logInfo(s\"During shutdown: $e\", e)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Binding operation will load the named services and call bind on them too; the\n+   * entire set of services are then ready for `init()` and `start()` calls\n+   * @param context spark context\n+   * @param appId application ID\n+   * @param attemptId YARN attempt ID if known. Otherwise, `None`.\n+   * @param registerShutdownHook should the services register as a hadoop shutdown hook?\n+   * @param shutdownHookPriority and if so, what priority\n+   */\n+  def start(context: SparkContext,\n+      appId: ApplicationId,\n+      attemptId: Option[ApplicationAttemptId] = None,\n+      registerShutdownHook: Boolean = false,\n+      shutdownHookPriority: Int = 0): Unit = {\n+    if (isInState(STATE.STARTED)) {\n+      logWarning(\"Ignoring re-entrant start operation\")\n+      return\n+    }\n+    require(context != null, \"Null context parameter\")\n+    require(appId != null, \"Null appId parameter\")\n+    require(!registerShutdownHook || shutdownHookPriority > 0,\n+           \"if registerShutdownHook is set, shutdownHookPriority must be greater than zero\")\n+    // this operation will trigger the state change checks\n+    super.init(context.hadoopConfiguration)\n+    this.sparkContext = context\n+    this.appId = appId\n+    this.attemptId = attemptId\n+    this.registerShutdownHook = registerShutdownHook\n+    this.shutdownHookPriority = shutdownHookPriority\n+    logInfo(s\"Starting Yarn extension services with app $appId and attemptId $attemptId\")\n+    super.start()\n+  }\n+\n+  /**\n+   * Loads a comma separated list of yarn services\n+   */\n+  override def serviceStart(): Unit = {\n+    val sNames = sparkContext.getConf.getOption(YarnExtensionServices.SPARK_YARN_SERVICES)"
  }],
  "prId": 8744
}]