[{
  "comments": [{
    "author": {
      "login": "jerryshao"
    },
    "body": "nit: 2 spaces indent.\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-10-28T05:19:41Z",
    "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped\n+ *\n+ * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]]\n+ * @param sparkContext current spark context\n+ * @param applicationId YARN application ID\n+ * @param attemptId optional AttemptID.\n+ */\n+case class SchedulerExtensionServiceBinding(\n+    sparkContext: SparkContext,\n+    applicationId: ApplicationId,\n+    attemptId: Option[ApplicationAttemptId] = None)\n+\n+/**\n+ * Container for [[SchedulerExtensionService]] instances.\n+ *\n+ * Loads Extension Services from the configuration property\n+ * `\"spark.yarn.services\"`, instantiates and starts them.\n+ * When stopped, it stops all child entries.\n+ *\n+ * The order in which child extension services are started and stopped\n+ * is undefined.\n+ *\n+ */\n+private[spark] class SchedulerExtensionServices extends SchedulerExtensionService\n+    with Logging {\n+  private var services: List[SchedulerExtensionService] = Nil\n+  private var sparkContext: SparkContext = _\n+  private var appId: ApplicationId = _\n+  private var attemptId: Option[ApplicationAttemptId] = _\n+  private val started = new AtomicBoolean(false)\n+  private var binding: SchedulerExtensionServiceBinding = _\n+\n+  /**\n+   * Binding operation will load the named services and call bind on them too; the\n+   * entire set of services are then ready for `init()` and `start()` calls\n+\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit = {\n+    if (started.getAndSet(true)) {\n+      logWarning(\"Ignoring re-entrant start operation\")\n+      return\n+    }\n+    require(binding.sparkContext != null, \"Null context parameter\")\n+    require(binding.applicationId != null, \"Null appId parameter\")\n+    this.binding = binding\n+    sparkContext = binding.sparkContext\n+    appId = binding.applicationId\n+    attemptId = binding.attemptId\n+    logInfo(s\"Starting Yarn extension services with app ${binding.applicationId}\" +\n+        s\" and attemptId $attemptId\")"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "fixed, +lines directly below\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-10-28T11:41:16Z",
    "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped\n+ *\n+ * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]]\n+ * @param sparkContext current spark context\n+ * @param applicationId YARN application ID\n+ * @param attemptId optional AttemptID.\n+ */\n+case class SchedulerExtensionServiceBinding(\n+    sparkContext: SparkContext,\n+    applicationId: ApplicationId,\n+    attemptId: Option[ApplicationAttemptId] = None)\n+\n+/**\n+ * Container for [[SchedulerExtensionService]] instances.\n+ *\n+ * Loads Extension Services from the configuration property\n+ * `\"spark.yarn.services\"`, instantiates and starts them.\n+ * When stopped, it stops all child entries.\n+ *\n+ * The order in which child extension services are started and stopped\n+ * is undefined.\n+ *\n+ */\n+private[spark] class SchedulerExtensionServices extends SchedulerExtensionService\n+    with Logging {\n+  private var services: List[SchedulerExtensionService] = Nil\n+  private var sparkContext: SparkContext = _\n+  private var appId: ApplicationId = _\n+  private var attemptId: Option[ApplicationAttemptId] = _\n+  private val started = new AtomicBoolean(false)\n+  private var binding: SchedulerExtensionServiceBinding = _\n+\n+  /**\n+   * Binding operation will load the named services and call bind on them too; the\n+   * entire set of services are then ready for `init()` and `start()` calls\n+\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit = {\n+    if (started.getAndSet(true)) {\n+      logWarning(\"Ignoring re-entrant start operation\")\n+      return\n+    }\n+    require(binding.sparkContext != null, \"Null context parameter\")\n+    require(binding.applicationId != null, \"Null appId parameter\")\n+    this.binding = binding\n+    sparkContext = binding.sparkContext\n+    appId = binding.applicationId\n+    attemptId = binding.attemptId\n+    logInfo(s\"Starting Yarn extension services with app ${binding.applicationId}\" +\n+        s\" and attemptId $attemptId\")"
  }],
  "prId": 9182
}, {
  "comments": [{
    "author": {
      "login": "jerryshao"
    },
    "body": "I think here we need to set `started` to `false`.\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-10-28T05:23:53Z",
    "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped\n+ *\n+ * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]]\n+ * @param sparkContext current spark context\n+ * @param applicationId YARN application ID\n+ * @param attemptId optional AttemptID.\n+ */\n+case class SchedulerExtensionServiceBinding(\n+    sparkContext: SparkContext,\n+    applicationId: ApplicationId,\n+    attemptId: Option[ApplicationAttemptId] = None)\n+\n+/**\n+ * Container for [[SchedulerExtensionService]] instances.\n+ *\n+ * Loads Extension Services from the configuration property\n+ * `\"spark.yarn.services\"`, instantiates and starts them.\n+ * When stopped, it stops all child entries.\n+ *\n+ * The order in which child extension services are started and stopped\n+ * is undefined.\n+ *\n+ */\n+private[spark] class SchedulerExtensionServices extends SchedulerExtensionService\n+    with Logging {\n+  private var services: List[SchedulerExtensionService] = Nil\n+  private var sparkContext: SparkContext = _\n+  private var appId: ApplicationId = _\n+  private var attemptId: Option[ApplicationAttemptId] = _\n+  private val started = new AtomicBoolean(false)\n+  private var binding: SchedulerExtensionServiceBinding = _\n+\n+  /**\n+   * Binding operation will load the named services and call bind on them too; the\n+   * entire set of services are then ready for `init()` and `start()` calls\n+\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit = {\n+    if (started.getAndSet(true)) {\n+      logWarning(\"Ignoring re-entrant start operation\")\n+      return\n+    }\n+    require(binding.sparkContext != null, \"Null context parameter\")\n+    require(binding.applicationId != null, \"Null appId parameter\")\n+    this.binding = binding\n+    sparkContext = binding.sparkContext\n+    appId = binding.applicationId\n+    attemptId = binding.attemptId\n+    logInfo(s\"Starting Yarn extension services with app ${binding.applicationId}\" +\n+        s\" and attemptId $attemptId\")\n+\n+    services = sparkContext.getConf.getOption(SchedulerExtensionServices.SPARK_YARN_SERVICES)\n+        .map { s =>\n+      s.split(\",\").map(_.trim()).filter(!_.isEmpty)\n+        .map { sClass =>\n+            val instance = Utils.classForName(sClass)\n+                .newInstance()\n+                .asInstanceOf[SchedulerExtensionService]\n+            // bind this service\n+            instance.start(binding)\n+            logInfo(s\"Service $sClass started\")\n+            instance\n+          }\n+    }.map(_.toList).getOrElse(Nil)\n+  }\n+\n+  /**\n+   * Get the list of services\n+   * @return a list of services; Nil until the service is started\n+   */\n+  def getServices: List[SchedulerExtensionService] = {\n+    services\n+  }\n+\n+  override def stop(): Unit = {\n+    logInfo(s\"Stopping $this\")"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "good catch. will set & skip the operation if !started. This will permit reissuing of the start() call from a stopped service, but a full state machine here is probably overkill.\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-10-28T11:44:05Z",
    "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped\n+ *\n+ * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]]\n+ * @param sparkContext current spark context\n+ * @param applicationId YARN application ID\n+ * @param attemptId optional AttemptID.\n+ */\n+case class SchedulerExtensionServiceBinding(\n+    sparkContext: SparkContext,\n+    applicationId: ApplicationId,\n+    attemptId: Option[ApplicationAttemptId] = None)\n+\n+/**\n+ * Container for [[SchedulerExtensionService]] instances.\n+ *\n+ * Loads Extension Services from the configuration property\n+ * `\"spark.yarn.services\"`, instantiates and starts them.\n+ * When stopped, it stops all child entries.\n+ *\n+ * The order in which child extension services are started and stopped\n+ * is undefined.\n+ *\n+ */\n+private[spark] class SchedulerExtensionServices extends SchedulerExtensionService\n+    with Logging {\n+  private var services: List[SchedulerExtensionService] = Nil\n+  private var sparkContext: SparkContext = _\n+  private var appId: ApplicationId = _\n+  private var attemptId: Option[ApplicationAttemptId] = _\n+  private val started = new AtomicBoolean(false)\n+  private var binding: SchedulerExtensionServiceBinding = _\n+\n+  /**\n+   * Binding operation will load the named services and call bind on them too; the\n+   * entire set of services are then ready for `init()` and `start()` calls\n+\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit = {\n+    if (started.getAndSet(true)) {\n+      logWarning(\"Ignoring re-entrant start operation\")\n+      return\n+    }\n+    require(binding.sparkContext != null, \"Null context parameter\")\n+    require(binding.applicationId != null, \"Null appId parameter\")\n+    this.binding = binding\n+    sparkContext = binding.sparkContext\n+    appId = binding.applicationId\n+    attemptId = binding.attemptId\n+    logInfo(s\"Starting Yarn extension services with app ${binding.applicationId}\" +\n+        s\" and attemptId $attemptId\")\n+\n+    services = sparkContext.getConf.getOption(SchedulerExtensionServices.SPARK_YARN_SERVICES)\n+        .map { s =>\n+      s.split(\",\").map(_.trim()).filter(!_.isEmpty)\n+        .map { sClass =>\n+            val instance = Utils.classForName(sClass)\n+                .newInstance()\n+                .asInstanceOf[SchedulerExtensionService]\n+            // bind this service\n+            instance.start(binding)\n+            logInfo(s\"Service $sClass started\")\n+            instance\n+          }\n+    }.map(_.toList).getOrElse(Nil)\n+  }\n+\n+  /**\n+   * Get the list of services\n+   * @return a list of services; Nil until the service is started\n+   */\n+  def getServices: List[SchedulerExtensionService] = {\n+    services\n+  }\n+\n+  override def stop(): Unit = {\n+    logInfo(s\"Stopping $this\")"
  }],
  "prId": 9182
}, {
  "comments": [{
    "author": {
      "login": "jerryshao"
    },
    "body": "Do we need to try catch some exceptions like `ClassNotFound` here?\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-10-28T05:25:00Z",
    "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped\n+ *\n+ * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]]\n+ * @param sparkContext current spark context\n+ * @param applicationId YARN application ID\n+ * @param attemptId optional AttemptID.\n+ */\n+case class SchedulerExtensionServiceBinding(\n+    sparkContext: SparkContext,\n+    applicationId: ApplicationId,\n+    attemptId: Option[ApplicationAttemptId] = None)\n+\n+/**\n+ * Container for [[SchedulerExtensionService]] instances.\n+ *\n+ * Loads Extension Services from the configuration property\n+ * `\"spark.yarn.services\"`, instantiates and starts them.\n+ * When stopped, it stops all child entries.\n+ *\n+ * The order in which child extension services are started and stopped\n+ * is undefined.\n+ *\n+ */\n+private[spark] class SchedulerExtensionServices extends SchedulerExtensionService\n+    with Logging {\n+  private var services: List[SchedulerExtensionService] = Nil\n+  private var sparkContext: SparkContext = _\n+  private var appId: ApplicationId = _\n+  private var attemptId: Option[ApplicationAttemptId] = _\n+  private val started = new AtomicBoolean(false)\n+  private var binding: SchedulerExtensionServiceBinding = _\n+\n+  /**\n+   * Binding operation will load the named services and call bind on them too; the\n+   * entire set of services are then ready for `init()` and `start()` calls\n+\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit = {\n+    if (started.getAndSet(true)) {\n+      logWarning(\"Ignoring re-entrant start operation\")\n+      return\n+    }\n+    require(binding.sparkContext != null, \"Null context parameter\")\n+    require(binding.applicationId != null, \"Null appId parameter\")\n+    this.binding = binding\n+    sparkContext = binding.sparkContext\n+    appId = binding.applicationId\n+    attemptId = binding.attemptId\n+    logInfo(s\"Starting Yarn extension services with app ${binding.applicationId}\" +\n+        s\" and attemptId $attemptId\")\n+\n+    services = sparkContext.getConf.getOption(SchedulerExtensionServices.SPARK_YARN_SERVICES)\n+        .map { s =>\n+      s.split(\",\").map(_.trim()).filter(!_.isEmpty)\n+        .map { sClass =>\n+            val instance = Utils.classForName(sClass)\n+                .newInstance()"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "I thought about that, but consider this: when would you want failure to load your listed extension services as something not to fail on? Do you want it to quitely downgrade, vs noisily fail?\n\nmaybe we could make it an option\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-10-28T11:45:26Z",
    "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped\n+ *\n+ * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]]\n+ * @param sparkContext current spark context\n+ * @param applicationId YARN application ID\n+ * @param attemptId optional AttemptID.\n+ */\n+case class SchedulerExtensionServiceBinding(\n+    sparkContext: SparkContext,\n+    applicationId: ApplicationId,\n+    attemptId: Option[ApplicationAttemptId] = None)\n+\n+/**\n+ * Container for [[SchedulerExtensionService]] instances.\n+ *\n+ * Loads Extension Services from the configuration property\n+ * `\"spark.yarn.services\"`, instantiates and starts them.\n+ * When stopped, it stops all child entries.\n+ *\n+ * The order in which child extension services are started and stopped\n+ * is undefined.\n+ *\n+ */\n+private[spark] class SchedulerExtensionServices extends SchedulerExtensionService\n+    with Logging {\n+  private var services: List[SchedulerExtensionService] = Nil\n+  private var sparkContext: SparkContext = _\n+  private var appId: ApplicationId = _\n+  private var attemptId: Option[ApplicationAttemptId] = _\n+  private val started = new AtomicBoolean(false)\n+  private var binding: SchedulerExtensionServiceBinding = _\n+\n+  /**\n+   * Binding operation will load the named services and call bind on them too; the\n+   * entire set of services are then ready for `init()` and `start()` calls\n+\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit = {\n+    if (started.getAndSet(true)) {\n+      logWarning(\"Ignoring re-entrant start operation\")\n+      return\n+    }\n+    require(binding.sparkContext != null, \"Null context parameter\")\n+    require(binding.applicationId != null, \"Null appId parameter\")\n+    this.binding = binding\n+    sparkContext = binding.sparkContext\n+    appId = binding.applicationId\n+    attemptId = binding.attemptId\n+    logInfo(s\"Starting Yarn extension services with app ${binding.applicationId}\" +\n+        s\" and attemptId $attemptId\")\n+\n+    services = sparkContext.getConf.getOption(SchedulerExtensionServices.SPARK_YARN_SERVICES)\n+        .map { s =>\n+      s.split(\",\").map(_.trim()).filter(!_.isEmpty)\n+        .map { sClass =>\n+            val instance = Utils.classForName(sClass)\n+                .newInstance()"
  }],
  "prId": 9182
}, {
  "comments": [{
    "author": {
      "login": "jerryshao"
    },
    "body": "Here `binding` is actually duplicated with below 3 parameters, from my understanding in this code, we could choose either.\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-10-28T05:48:18Z",
    "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped\n+ *\n+ * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]]\n+ * @param sparkContext current spark context\n+ * @param applicationId YARN application ID\n+ * @param attemptId optional AttemptID.\n+ */\n+case class SchedulerExtensionServiceBinding(\n+    sparkContext: SparkContext,\n+    applicationId: ApplicationId,\n+    attemptId: Option[ApplicationAttemptId] = None)\n+\n+/**\n+ * Container for [[SchedulerExtensionService]] instances.\n+ *\n+ * Loads Extension Services from the configuration property\n+ * `\"spark.yarn.services\"`, instantiates and starts them.\n+ * When stopped, it stops all child entries.\n+ *\n+ * The order in which child extension services are started and stopped\n+ * is undefined.\n+ *\n+ */\n+private[spark] class SchedulerExtensionServices extends SchedulerExtensionService\n+    with Logging {\n+  private var services: List[SchedulerExtensionService] = Nil\n+  private var sparkContext: SparkContext = _\n+  private var appId: ApplicationId = _\n+  private var attemptId: Option[ApplicationAttemptId] = _\n+  private val started = new AtomicBoolean(false)\n+  private var binding: SchedulerExtensionServiceBinding = _\n+\n+  /**\n+   * Binding operation will load the named services and call bind on them too; the\n+   * entire set of services are then ready for `init()` and `start()` calls\n+\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit = {\n+    if (started.getAndSet(true)) {\n+      logWarning(\"Ignoring re-entrant start operation\")\n+      return\n+    }\n+    require(binding.sparkContext != null, \"Null context parameter\")\n+    require(binding.applicationId != null, \"Null appId parameter\")\n+    this.binding = binding",
    "line": 100
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "OK, saving binding as a field; converting the others to local vars.\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-10-28T12:05:31Z",
    "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped\n+ *\n+ * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]]\n+ * @param sparkContext current spark context\n+ * @param applicationId YARN application ID\n+ * @param attemptId optional AttemptID.\n+ */\n+case class SchedulerExtensionServiceBinding(\n+    sparkContext: SparkContext,\n+    applicationId: ApplicationId,\n+    attemptId: Option[ApplicationAttemptId] = None)\n+\n+/**\n+ * Container for [[SchedulerExtensionService]] instances.\n+ *\n+ * Loads Extension Services from the configuration property\n+ * `\"spark.yarn.services\"`, instantiates and starts them.\n+ * When stopped, it stops all child entries.\n+ *\n+ * The order in which child extension services are started and stopped\n+ * is undefined.\n+ *\n+ */\n+private[spark] class SchedulerExtensionServices extends SchedulerExtensionService\n+    with Logging {\n+  private var services: List[SchedulerExtensionService] = Nil\n+  private var sparkContext: SparkContext = _\n+  private var appId: ApplicationId = _\n+  private var attemptId: Option[ApplicationAttemptId] = _\n+  private val started = new AtomicBoolean(false)\n+  private var binding: SchedulerExtensionServiceBinding = _\n+\n+  /**\n+   * Binding operation will load the named services and call bind on them too; the\n+   * entire set of services are then ready for `init()` and `start()` calls\n+\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit = {\n+    if (started.getAndSet(true)) {\n+      logWarning(\"Ignoring re-entrant start operation\")\n+      return\n+    }\n+    require(binding.sparkContext != null, \"Null context parameter\")\n+    require(binding.applicationId != null, \"Null appId parameter\")\n+    this.binding = binding",
    "line": 100
  }],
  "prId": 9182
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: this goes before the previous import\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-10-28T13:08:38Z",
    "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SparkContext}"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Still out of order.\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-10-29T08:16:13Z",
    "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SparkContext}"
  }],
  "prId": 9182
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "When will this be not set? I assume in client mode? Could you mention that in the scaladoc above?\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-10-28T13:10:06Z",
    "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped\n+ *\n+ * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]]\n+ * @param sparkContext current spark context\n+ * @param applicationId YARN application ID\n+ * @param attemptId optional AttemptID.\n+ */\n+case class SchedulerExtensionServiceBinding(\n+    sparkContext: SparkContext,\n+    applicationId: ApplicationId,\n+    attemptId: Option[ApplicationAttemptId] = None)",
    "line": 68
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "correct: only the AM-side bindings have it. Will cover in the javadocs.\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-10-28T16:23:55Z",
    "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped\n+ *\n+ * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]]\n+ * @param sparkContext current spark context\n+ * @param applicationId YARN application ID\n+ * @param attemptId optional AttemptID.\n+ */\n+case class SchedulerExtensionServiceBinding(\n+    sparkContext: SparkContext,\n+    applicationId: ApplicationId,\n+    attemptId: Option[ApplicationAttemptId] = None)",
    "line": 68
  }],
  "prId": 9182
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: indentation here is weird.\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-10-28T13:12:19Z",
    "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped\n+ *\n+ * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]]\n+ * @param sparkContext current spark context\n+ * @param applicationId YARN application ID\n+ * @param attemptId optional AttemptID.\n+ */\n+case class SchedulerExtensionServiceBinding(\n+    sparkContext: SparkContext,\n+    applicationId: ApplicationId,\n+    attemptId: Option[ApplicationAttemptId] = None)\n+\n+/**\n+ * Container for [[SchedulerExtensionService]] instances.\n+ *\n+ * Loads Extension Services from the configuration property\n+ * `\"spark.yarn.services\"`, instantiates and starts them.\n+ * When stopped, it stops all child entries.\n+ *\n+ * The order in which child extension services are started and stopped\n+ * is undefined.\n+ *\n+ */\n+private[spark] class SchedulerExtensionServices extends SchedulerExtensionService\n+    with Logging {\n+  private var services: List[SchedulerExtensionService] = Nil\n+  private val started = new AtomicBoolean(false)\n+  private var binding: SchedulerExtensionServiceBinding = _\n+\n+  /**\n+   * Binding operation will load the named services and call bind on them too; the\n+   * entire set of services are then ready for `init()` and `start()` calls\n+\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit = {\n+    if (started.getAndSet(true)) {\n+      logWarning(\"Ignoring re-entrant start operation\")\n+      return\n+    }\n+    require(binding.sparkContext != null, \"Null context parameter\")\n+    require(binding.applicationId != null, \"Null appId parameter\")\n+    this.binding = binding\n+    val sparkContext = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    logInfo(s\"Starting Yarn extension services with app ${binding.applicationId}\" +\n+      s\" and attemptId $attemptId\")\n+\n+    services = sparkContext.getConf.getOption(SchedulerExtensionServices.SPARK_YARN_SERVICES)\n+      .map { s =>\n+      s.split(\",\").map(_.trim()).filter(!_.isEmpty)"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "I thin I'd tried to manually edit it from Jerry's comment & got it wrong. Will try the IDE this time.\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-10-28T16:26:11Z",
    "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped\n+ *\n+ * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]]\n+ * @param sparkContext current spark context\n+ * @param applicationId YARN application ID\n+ * @param attemptId optional AttemptID.\n+ */\n+case class SchedulerExtensionServiceBinding(\n+    sparkContext: SparkContext,\n+    applicationId: ApplicationId,\n+    attemptId: Option[ApplicationAttemptId] = None)\n+\n+/**\n+ * Container for [[SchedulerExtensionService]] instances.\n+ *\n+ * Loads Extension Services from the configuration property\n+ * `\"spark.yarn.services\"`, instantiates and starts them.\n+ * When stopped, it stops all child entries.\n+ *\n+ * The order in which child extension services are started and stopped\n+ * is undefined.\n+ *\n+ */\n+private[spark] class SchedulerExtensionServices extends SchedulerExtensionService\n+    with Logging {\n+  private var services: List[SchedulerExtensionService] = Nil\n+  private val started = new AtomicBoolean(false)\n+  private var binding: SchedulerExtensionServiceBinding = _\n+\n+  /**\n+   * Binding operation will load the named services and call bind on them too; the\n+   * entire set of services are then ready for `init()` and `start()` calls\n+\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit = {\n+    if (started.getAndSet(true)) {\n+      logWarning(\"Ignoring re-entrant start operation\")\n+      return\n+    }\n+    require(binding.sparkContext != null, \"Null context parameter\")\n+    require(binding.applicationId != null, \"Null appId parameter\")\n+    this.binding = binding\n+    val sparkContext = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    logInfo(s\"Starting Yarn extension services with app ${binding.applicationId}\" +\n+      s\" and attemptId $attemptId\")\n+\n+    services = sparkContext.getConf.getOption(SchedulerExtensionServices.SPARK_YARN_SERVICES)\n+      .map { s =>\n+      s.split(\",\").map(_.trim()).filter(!_.isEmpty)"
  }],
  "prId": 9182
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Hmmm... `SchedulerExtensionServices` should probably mention that implementations must have an empty constructor.\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-10-28T13:13:18Z",
    "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped\n+ *\n+ * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]]\n+ * @param sparkContext current spark context\n+ * @param applicationId YARN application ID\n+ * @param attemptId optional AttemptID.\n+ */\n+case class SchedulerExtensionServiceBinding(\n+    sparkContext: SparkContext,\n+    applicationId: ApplicationId,\n+    attemptId: Option[ApplicationAttemptId] = None)\n+\n+/**\n+ * Container for [[SchedulerExtensionService]] instances.\n+ *\n+ * Loads Extension Services from the configuration property\n+ * `\"spark.yarn.services\"`, instantiates and starts them.\n+ * When stopped, it stops all child entries.\n+ *\n+ * The order in which child extension services are started and stopped\n+ * is undefined.\n+ *\n+ */\n+private[spark] class SchedulerExtensionServices extends SchedulerExtensionService\n+    with Logging {\n+  private var services: List[SchedulerExtensionService] = Nil\n+  private val started = new AtomicBoolean(false)\n+  private var binding: SchedulerExtensionServiceBinding = _\n+\n+  /**\n+   * Binding operation will load the named services and call bind on them too; the\n+   * entire set of services are then ready for `init()` and `start()` calls\n+\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit = {\n+    if (started.getAndSet(true)) {\n+      logWarning(\"Ignoring re-entrant start operation\")\n+      return\n+    }\n+    require(binding.sparkContext != null, \"Null context parameter\")\n+    require(binding.applicationId != null, \"Null appId parameter\")\n+    this.binding = binding\n+    val sparkContext = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    logInfo(s\"Starting Yarn extension services with app ${binding.applicationId}\" +\n+      s\" and attemptId $attemptId\")\n+\n+    services = sparkContext.getConf.getOption(SchedulerExtensionServices.SPARK_YARN_SERVICES)\n+      .map { s =>\n+      s.split(\",\").map(_.trim()).filter(!_.isEmpty)\n+        .map { sClass =>\n+          val instance = Utils.classForName(sClass)\n+            .newInstance()"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "I can add that in the docs —or what if converted `SchedulerExtensionService` from a trait to a class, and added an empty constructor there, with the documentation on that constructor?\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-10-28T16:27:49Z",
    "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped\n+ *\n+ * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]]\n+ * @param sparkContext current spark context\n+ * @param applicationId YARN application ID\n+ * @param attemptId optional AttemptID.\n+ */\n+case class SchedulerExtensionServiceBinding(\n+    sparkContext: SparkContext,\n+    applicationId: ApplicationId,\n+    attemptId: Option[ApplicationAttemptId] = None)\n+\n+/**\n+ * Container for [[SchedulerExtensionService]] instances.\n+ *\n+ * Loads Extension Services from the configuration property\n+ * `\"spark.yarn.services\"`, instantiates and starts them.\n+ * When stopped, it stops all child entries.\n+ *\n+ * The order in which child extension services are started and stopped\n+ * is undefined.\n+ *\n+ */\n+private[spark] class SchedulerExtensionServices extends SchedulerExtensionService\n+    with Logging {\n+  private var services: List[SchedulerExtensionService] = Nil\n+  private val started = new AtomicBoolean(false)\n+  private var binding: SchedulerExtensionServiceBinding = _\n+\n+  /**\n+   * Binding operation will load the named services and call bind on them too; the\n+   * entire set of services are then ready for `init()` and `start()` calls\n+\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit = {\n+    if (started.getAndSet(true)) {\n+      logWarning(\"Ignoring re-entrant start operation\")\n+      return\n+    }\n+    require(binding.sparkContext != null, \"Null context parameter\")\n+    require(binding.applicationId != null, \"Null appId parameter\")\n+    this.binding = binding\n+    val sparkContext = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    logInfo(s\"Starting Yarn extension services with app ${binding.applicationId}\" +\n+      s\" and attemptId $attemptId\")\n+\n+    services = sparkContext.getConf.getOption(SchedulerExtensionServices.SPARK_YARN_SERVICES)\n+      .map { s =>\n+      s.split(\",\").map(_.trim()).filter(!_.isEmpty)\n+        .map { sClass =>\n+          val instance = Utils.classForName(sClass)\n+            .newInstance()"
  }],
  "prId": 9182
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "minor: instead of another call to `map` you could add the `toList` call to the code inside the previous closure.\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-10-28T13:21:43Z",
    "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped\n+ *\n+ * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]]\n+ * @param sparkContext current spark context\n+ * @param applicationId YARN application ID\n+ * @param attemptId optional AttemptID.\n+ */\n+case class SchedulerExtensionServiceBinding(\n+    sparkContext: SparkContext,\n+    applicationId: ApplicationId,\n+    attemptId: Option[ApplicationAttemptId] = None)\n+\n+/**\n+ * Container for [[SchedulerExtensionService]] instances.\n+ *\n+ * Loads Extension Services from the configuration property\n+ * `\"spark.yarn.services\"`, instantiates and starts them.\n+ * When stopped, it stops all child entries.\n+ *\n+ * The order in which child extension services are started and stopped\n+ * is undefined.\n+ *\n+ */\n+private[spark] class SchedulerExtensionServices extends SchedulerExtensionService\n+    with Logging {\n+  private var services: List[SchedulerExtensionService] = Nil\n+  private val started = new AtomicBoolean(false)\n+  private var binding: SchedulerExtensionServiceBinding = _\n+\n+  /**\n+   * Binding operation will load the named services and call bind on them too; the\n+   * entire set of services are then ready for `init()` and `start()` calls\n+\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit = {\n+    if (started.getAndSet(true)) {\n+      logWarning(\"Ignoring re-entrant start operation\")\n+      return\n+    }\n+    require(binding.sparkContext != null, \"Null context parameter\")\n+    require(binding.applicationId != null, \"Null appId parameter\")\n+    this.binding = binding\n+    val sparkContext = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    logInfo(s\"Starting Yarn extension services with app ${binding.applicationId}\" +\n+      s\" and attemptId $attemptId\")\n+\n+    services = sparkContext.getConf.getOption(SchedulerExtensionServices.SPARK_YARN_SERVICES)\n+      .map { s =>\n+      s.split(\",\").map(_.trim()).filter(!_.isEmpty)\n+        .map { sClass =>\n+          val instance = Utils.classForName(sClass)\n+            .newInstance()\n+            .asInstanceOf[SchedulerExtensionService]\n+          // bind this service\n+          instance.start(binding)\n+          logInfo(s\"Service $sClass started\")\n+          instance\n+        }\n+    }.map(_.toList).getOrElse(Nil)"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "OK\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-10-28T16:31:59Z",
    "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped\n+ *\n+ * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]]\n+ * @param sparkContext current spark context\n+ * @param applicationId YARN application ID\n+ * @param attemptId optional AttemptID.\n+ */\n+case class SchedulerExtensionServiceBinding(\n+    sparkContext: SparkContext,\n+    applicationId: ApplicationId,\n+    attemptId: Option[ApplicationAttemptId] = None)\n+\n+/**\n+ * Container for [[SchedulerExtensionService]] instances.\n+ *\n+ * Loads Extension Services from the configuration property\n+ * `\"spark.yarn.services\"`, instantiates and starts them.\n+ * When stopped, it stops all child entries.\n+ *\n+ * The order in which child extension services are started and stopped\n+ * is undefined.\n+ *\n+ */\n+private[spark] class SchedulerExtensionServices extends SchedulerExtensionService\n+    with Logging {\n+  private var services: List[SchedulerExtensionService] = Nil\n+  private val started = new AtomicBoolean(false)\n+  private var binding: SchedulerExtensionServiceBinding = _\n+\n+  /**\n+   * Binding operation will load the named services and call bind on them too; the\n+   * entire set of services are then ready for `init()` and `start()` calls\n+\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit = {\n+    if (started.getAndSet(true)) {\n+      logWarning(\"Ignoring re-entrant start operation\")\n+      return\n+    }\n+    require(binding.sparkContext != null, \"Null context parameter\")\n+    require(binding.applicationId != null, \"Null appId parameter\")\n+    this.binding = binding\n+    val sparkContext = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    logInfo(s\"Starting Yarn extension services with app ${binding.applicationId}\" +\n+      s\" and attemptId $attemptId\")\n+\n+    services = sparkContext.getConf.getOption(SchedulerExtensionServices.SPARK_YARN_SERVICES)\n+      .map { s =>\n+      s.split(\",\").map(_.trim()).filter(!_.isEmpty)\n+        .map { sClass =>\n+          val instance = Utils.classForName(sClass)\n+            .newInstance()\n+            .asInstanceOf[SchedulerExtensionService]\n+          // bind this service\n+          instance.start(binding)\n+          logInfo(s\"Service $sClass started\")\n+          instance\n+        }\n+    }.map(_.toList).getOrElse(Nil)"
  }],
  "prId": 9182
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Just a suggestion in general (you don't need to change that here if you don't want to), but you can use a shorter syntax in these cases:\n\n```\ndef getServices: List[SchedulerExtensionService] = services\n```\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-10-29T08:19:06Z",
    "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped.\n+ *\n+ * 1. For implementations to be loadable by [[SchedulerExtensionServices]],\n+ * they must provide an empty constructor.\n+ * 2. The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]].\n+ *\n+ * The attempt ID will be set if the service is started within a YARN application master;\n+ * there is then a different attempt ID for every time that AM is restarted.\n+ * When the service binding is instantiated on a client, there's no attempt ID, as it lacks\n+ * this information.\n+ * @param sparkContext current spark context\n+ * @param applicationId YARN application ID\n+ * @param attemptId YARN attemptID -if known.\n+ */\n+case class SchedulerExtensionServiceBinding(\n+    sparkContext: SparkContext,\n+    applicationId: ApplicationId,\n+    attemptId: Option[ApplicationAttemptId] = None)\n+\n+/**\n+ * Container for [[SchedulerExtensionService]] instances.\n+ *\n+ * Loads Extension Services from the configuration property\n+ * `\"spark.yarn.services\"`, instantiates and starts them.\n+ * When stopped, it stops all child entries.\n+ *\n+ * The order in which child extension services are started and stopped\n+ * is undefined.\n+ *\n+ */\n+private[spark] class SchedulerExtensionServices extends SchedulerExtensionService\n+    with Logging {\n+  private var services: List[SchedulerExtensionService] = Nil\n+  private val started = new AtomicBoolean(false)\n+  private var binding: SchedulerExtensionServiceBinding = _\n+\n+  /**\n+   * Binding operation will load the named services and call bind on them too; the\n+   * entire set of services are then ready for `init()` and `start()` calls.\n+   *\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit = {\n+    if (started.getAndSet(true)) {\n+      logWarning(\"Ignoring re-entrant start operation\")\n+      return\n+    }\n+    require(binding.sparkContext != null, \"Null context parameter\")\n+    require(binding.applicationId != null, \"Null appId parameter\")\n+    this.binding = binding\n+    val sparkContext = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    logInfo(s\"Starting Yarn extension services with app ${binding.applicationId}\" +\n+      s\" and attemptId $attemptId\")\n+\n+    services = sparkContext.getConf.getOption(SchedulerExtensionServices.SPARK_YARN_SERVICES)\n+      .map { s =>\n+        s.split(\",\").map(_.trim()).filter(!_.isEmpty)\n+          .map { sClass =>\n+            val instance = Utils.classForName(sClass)\n+              .newInstance()\n+              .asInstanceOf[SchedulerExtensionService]\n+            // bind this service\n+            instance.start(binding)\n+            logInfo(s\"Service $sClass started\")\n+            instance\n+          }.toList\n+      }.getOrElse(Nil)\n+  }\n+\n+  /**\n+   * Get the list of services.\n+   *\n+   * @return a list of services; Nil until the service is started\n+   */\n+  def getServices: List[SchedulerExtensionService] = {"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "OK\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-10-29T11:26:01Z",
    "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped.\n+ *\n+ * 1. For implementations to be loadable by [[SchedulerExtensionServices]],\n+ * they must provide an empty constructor.\n+ * 2. The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]].\n+ *\n+ * The attempt ID will be set if the service is started within a YARN application master;\n+ * there is then a different attempt ID for every time that AM is restarted.\n+ * When the service binding is instantiated on a client, there's no attempt ID, as it lacks\n+ * this information.\n+ * @param sparkContext current spark context\n+ * @param applicationId YARN application ID\n+ * @param attemptId YARN attemptID -if known.\n+ */\n+case class SchedulerExtensionServiceBinding(\n+    sparkContext: SparkContext,\n+    applicationId: ApplicationId,\n+    attemptId: Option[ApplicationAttemptId] = None)\n+\n+/**\n+ * Container for [[SchedulerExtensionService]] instances.\n+ *\n+ * Loads Extension Services from the configuration property\n+ * `\"spark.yarn.services\"`, instantiates and starts them.\n+ * When stopped, it stops all child entries.\n+ *\n+ * The order in which child extension services are started and stopped\n+ * is undefined.\n+ *\n+ */\n+private[spark] class SchedulerExtensionServices extends SchedulerExtensionService\n+    with Logging {\n+  private var services: List[SchedulerExtensionService] = Nil\n+  private val started = new AtomicBoolean(false)\n+  private var binding: SchedulerExtensionServiceBinding = _\n+\n+  /**\n+   * Binding operation will load the named services and call bind on them too; the\n+   * entire set of services are then ready for `init()` and `start()` calls.\n+   *\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit = {\n+    if (started.getAndSet(true)) {\n+      logWarning(\"Ignoring re-entrant start operation\")\n+      return\n+    }\n+    require(binding.sparkContext != null, \"Null context parameter\")\n+    require(binding.applicationId != null, \"Null appId parameter\")\n+    this.binding = binding\n+    val sparkContext = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    logInfo(s\"Starting Yarn extension services with app ${binding.applicationId}\" +\n+      s\" and attemptId $attemptId\")\n+\n+    services = sparkContext.getConf.getOption(SchedulerExtensionServices.SPARK_YARN_SERVICES)\n+      .map { s =>\n+        s.split(\",\").map(_.trim()).filter(!_.isEmpty)\n+          .map { sClass =>\n+            val instance = Utils.classForName(sClass)\n+              .newInstance()\n+              .asInstanceOf[SchedulerExtensionService]\n+            // bind this service\n+            instance.start(binding)\n+            logInfo(s\"Service $sClass started\")\n+            instance\n+          }.toList\n+      }.getOrElse(Nil)\n+  }\n+\n+  /**\n+   * Get the list of services.\n+   *\n+   * @return a list of services; Nil until the service is started\n+   */\n+  def getServices: List[SchedulerExtensionService] = {"
  }],
  "prId": 9182
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This class doesn't have a `toString` so this will probably look ugly.\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-10-29T08:21:39Z",
    "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped.\n+ *\n+ * 1. For implementations to be loadable by [[SchedulerExtensionServices]],\n+ * they must provide an empty constructor.\n+ * 2. The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]].\n+ *\n+ * The attempt ID will be set if the service is started within a YARN application master;\n+ * there is then a different attempt ID for every time that AM is restarted.\n+ * When the service binding is instantiated on a client, there's no attempt ID, as it lacks\n+ * this information.\n+ * @param sparkContext current spark context\n+ * @param applicationId YARN application ID\n+ * @param attemptId YARN attemptID -if known.\n+ */\n+case class SchedulerExtensionServiceBinding(\n+    sparkContext: SparkContext,\n+    applicationId: ApplicationId,\n+    attemptId: Option[ApplicationAttemptId] = None)\n+\n+/**\n+ * Container for [[SchedulerExtensionService]] instances.\n+ *\n+ * Loads Extension Services from the configuration property\n+ * `\"spark.yarn.services\"`, instantiates and starts them.\n+ * When stopped, it stops all child entries.\n+ *\n+ * The order in which child extension services are started and stopped\n+ * is undefined.\n+ *\n+ */\n+private[spark] class SchedulerExtensionServices extends SchedulerExtensionService\n+    with Logging {\n+  private var services: List[SchedulerExtensionService] = Nil\n+  private val started = new AtomicBoolean(false)\n+  private var binding: SchedulerExtensionServiceBinding = _\n+\n+  /**\n+   * Binding operation will load the named services and call bind on them too; the\n+   * entire set of services are then ready for `init()` and `start()` calls.\n+   *\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit = {\n+    if (started.getAndSet(true)) {\n+      logWarning(\"Ignoring re-entrant start operation\")\n+      return\n+    }\n+    require(binding.sparkContext != null, \"Null context parameter\")\n+    require(binding.applicationId != null, \"Null appId parameter\")\n+    this.binding = binding\n+    val sparkContext = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    logInfo(s\"Starting Yarn extension services with app ${binding.applicationId}\" +\n+      s\" and attemptId $attemptId\")\n+\n+    services = sparkContext.getConf.getOption(SchedulerExtensionServices.SPARK_YARN_SERVICES)\n+      .map { s =>\n+        s.split(\",\").map(_.trim()).filter(!_.isEmpty)\n+          .map { sClass =>\n+            val instance = Utils.classForName(sClass)\n+              .newInstance()\n+              .asInstanceOf[SchedulerExtensionService]\n+            // bind this service\n+            instance.start(binding)\n+            logInfo(s\"Service $sClass started\")\n+            instance\n+          }.toList\n+      }.getOrElse(Nil)\n+  }\n+\n+  /**\n+   * Get the list of services.\n+   *\n+   * @return a list of services; Nil until the service is started\n+   */\n+  def getServices: List[SchedulerExtensionService] = {\n+    services\n+  }\n+\n+  /**\n+   * Stop the services; idempotent.\n+    *\n+    * Any\n+   */\n+  override def stop(): Unit = {\n+    if (started.getAndSet(false)) {\n+      logInfo(s\"Stopping $this\")"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "added one\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-10-29T12:16:25Z",
    "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped.\n+ *\n+ * 1. For implementations to be loadable by [[SchedulerExtensionServices]],\n+ * they must provide an empty constructor.\n+ * 2. The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]].\n+ *\n+ * The attempt ID will be set if the service is started within a YARN application master;\n+ * there is then a different attempt ID for every time that AM is restarted.\n+ * When the service binding is instantiated on a client, there's no attempt ID, as it lacks\n+ * this information.\n+ * @param sparkContext current spark context\n+ * @param applicationId YARN application ID\n+ * @param attemptId YARN attemptID -if known.\n+ */\n+case class SchedulerExtensionServiceBinding(\n+    sparkContext: SparkContext,\n+    applicationId: ApplicationId,\n+    attemptId: Option[ApplicationAttemptId] = None)\n+\n+/**\n+ * Container for [[SchedulerExtensionService]] instances.\n+ *\n+ * Loads Extension Services from the configuration property\n+ * `\"spark.yarn.services\"`, instantiates and starts them.\n+ * When stopped, it stops all child entries.\n+ *\n+ * The order in which child extension services are started and stopped\n+ * is undefined.\n+ *\n+ */\n+private[spark] class SchedulerExtensionServices extends SchedulerExtensionService\n+    with Logging {\n+  private var services: List[SchedulerExtensionService] = Nil\n+  private val started = new AtomicBoolean(false)\n+  private var binding: SchedulerExtensionServiceBinding = _\n+\n+  /**\n+   * Binding operation will load the named services and call bind on them too; the\n+   * entire set of services are then ready for `init()` and `start()` calls.\n+   *\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit = {\n+    if (started.getAndSet(true)) {\n+      logWarning(\"Ignoring re-entrant start operation\")\n+      return\n+    }\n+    require(binding.sparkContext != null, \"Null context parameter\")\n+    require(binding.applicationId != null, \"Null appId parameter\")\n+    this.binding = binding\n+    val sparkContext = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    logInfo(s\"Starting Yarn extension services with app ${binding.applicationId}\" +\n+      s\" and attemptId $attemptId\")\n+\n+    services = sparkContext.getConf.getOption(SchedulerExtensionServices.SPARK_YARN_SERVICES)\n+      .map { s =>\n+        s.split(\",\").map(_.trim()).filter(!_.isEmpty)\n+          .map { sClass =>\n+            val instance = Utils.classForName(sClass)\n+              .newInstance()\n+              .asInstanceOf[SchedulerExtensionService]\n+            // bind this service\n+            instance.start(binding)\n+            logInfo(s\"Service $sClass started\")\n+            instance\n+          }.toList\n+      }.getOrElse(Nil)\n+  }\n+\n+  /**\n+   * Get the list of services.\n+   *\n+   * @return a list of services; Nil until the service is started\n+   */\n+  def getServices: List[SchedulerExtensionService] = {\n+    services\n+  }\n+\n+  /**\n+   * Stop the services; idempotent.\n+    *\n+    * Any\n+   */\n+  override def stop(): Unit = {\n+    if (started.getAndSet(false)) {\n+      logInfo(s\"Stopping $this\")"
  }],
  "prId": 9182
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: \"on a client\" -> \"in client mode\"\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-11-13T19:38:40Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.{Logging, SparkContext}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped.\n+ *\n+ * 1. For implementations to be loadable by `SchedulerExtensionServices`,\n+ * they must provide an empty constructor.\n+ * 2. The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]].\n+ *\n+ * The attempt ID will be set if the service is started within a YARN application master;\n+ * there is then a different attempt ID for every time that AM is restarted.\n+ * When the service binding is instantiated on a client, there's no attempt ID, as it lacks"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "fixed\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-11-18T16:52:08Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.{Logging, SparkContext}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped.\n+ *\n+ * 1. For implementations to be loadable by `SchedulerExtensionServices`,\n+ * they must provide an empty constructor.\n+ * 2. The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]].\n+ *\n+ * The attempt ID will be set if the service is started within a YARN application master;\n+ * there is then a different attempt ID for every time that AM is restarted.\n+ * When the service binding is instantiated on a client, there's no attempt ID, as it lacks"
  }],
  "prId": 9182
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "super nit: stray empty line\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-11-13T19:38:56Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.{Logging, SparkContext}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped.\n+ *\n+ * 1. For implementations to be loadable by `SchedulerExtensionServices`,\n+ * they must provide an empty constructor.\n+ * 2. The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]].\n+ *\n+ * The attempt ID will be set if the service is started within a YARN application master;\n+ * there is then a different attempt ID for every time that AM is restarted.\n+ * When the service binding is instantiated on a client, there's no attempt ID, as it lacks\n+ * this information.\n+ * @param sparkContext current spark context\n+ * @param applicationId YARN application ID\n+ * @param attemptId YARN attemptID -if known.\n+ */\n+case class SchedulerExtensionServiceBinding(\n+    sparkContext: SparkContext,\n+    applicationId: ApplicationId,\n+    attemptId: Option[ApplicationAttemptId] = None)\n+\n+/**\n+ * Container for [[SchedulerExtensionService]] instances.\n+ *\n+ * Loads Extension Services from the configuration property\n+ * `\"spark.yarn.services\"`, instantiates and starts them.\n+ * When stopped, it stops all child entries.\n+ *\n+ * The order in which child extension services are started and stopped\n+ * is undefined.\n+ *"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "fixed\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-11-18T16:54:38Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.{Logging, SparkContext}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped.\n+ *\n+ * 1. For implementations to be loadable by `SchedulerExtensionServices`,\n+ * they must provide an empty constructor.\n+ * 2. The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]].\n+ *\n+ * The attempt ID will be set if the service is started within a YARN application master;\n+ * there is then a different attempt ID for every time that AM is restarted.\n+ * When the service binding is instantiated on a client, there's no attempt ID, as it lacks\n+ * this information.\n+ * @param sparkContext current spark context\n+ * @param applicationId YARN application ID\n+ * @param attemptId YARN attemptID -if known.\n+ */\n+case class SchedulerExtensionServiceBinding(\n+    sparkContext: SparkContext,\n+    applicationId: ApplicationId,\n+    attemptId: Option[ApplicationAttemptId] = None)\n+\n+/**\n+ * Container for [[SchedulerExtensionService]] instances.\n+ *\n+ * Loads Extension Services from the configuration property\n+ * `\"spark.yarn.services\"`, instantiates and starts them.\n+ * When stopped, it stops all child entries.\n+ *\n+ * The order in which child extension services are started and stopped\n+ * is undefined.\n+ *"
  }],
  "prId": 9182
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: if you use `$appId` then the whole statement fits in the same line.\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-11-13T19:39:21Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.{Logging, SparkContext}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped.\n+ *\n+ * 1. For implementations to be loadable by `SchedulerExtensionServices`,\n+ * they must provide an empty constructor.\n+ * 2. The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]].\n+ *\n+ * The attempt ID will be set if the service is started within a YARN application master;\n+ * there is then a different attempt ID for every time that AM is restarted.\n+ * When the service binding is instantiated on a client, there's no attempt ID, as it lacks\n+ * this information.\n+ * @param sparkContext current spark context\n+ * @param applicationId YARN application ID\n+ * @param attemptId YARN attemptID -if known.\n+ */\n+case class SchedulerExtensionServiceBinding(\n+    sparkContext: SparkContext,\n+    applicationId: ApplicationId,\n+    attemptId: Option[ApplicationAttemptId] = None)\n+\n+/**\n+ * Container for [[SchedulerExtensionService]] instances.\n+ *\n+ * Loads Extension Services from the configuration property\n+ * `\"spark.yarn.services\"`, instantiates and starts them.\n+ * When stopped, it stops all child entries.\n+ *\n+ * The order in which child extension services are started and stopped\n+ * is undefined.\n+ *\n+ */\n+private[spark] class SchedulerExtensionServices extends SchedulerExtensionService\n+    with Logging {\n+  private var serviceOption: Option[String] = None\n+  private var services: List[SchedulerExtensionService] = Nil\n+  private val started = new AtomicBoolean(false)\n+  private var binding: SchedulerExtensionServiceBinding = _\n+\n+  /**\n+   * Binding operation will load the named services and call bind on them too; the\n+   * entire set of services are then ready for `init()` and `start()` calls.\n+   *\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit = {\n+    if (started.getAndSet(true)) {\n+      logWarning(\"Ignoring re-entrant start operation\")\n+      return\n+    }\n+    require(binding.sparkContext != null, \"Null context parameter\")\n+    require(binding.applicationId != null, \"Null appId parameter\")\n+    this.binding = binding\n+    val sparkContext = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    logInfo(s\"Starting Yarn extension services with app ${binding.applicationId}\" +"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "nice! done.\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-11-18T16:55:28Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.{Logging, SparkContext}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped.\n+ *\n+ * 1. For implementations to be loadable by `SchedulerExtensionServices`,\n+ * they must provide an empty constructor.\n+ * 2. The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]].\n+ *\n+ * The attempt ID will be set if the service is started within a YARN application master;\n+ * there is then a different attempt ID for every time that AM is restarted.\n+ * When the service binding is instantiated on a client, there's no attempt ID, as it lacks\n+ * this information.\n+ * @param sparkContext current spark context\n+ * @param applicationId YARN application ID\n+ * @param attemptId YARN attemptID -if known.\n+ */\n+case class SchedulerExtensionServiceBinding(\n+    sparkContext: SparkContext,\n+    applicationId: ApplicationId,\n+    attemptId: Option[ApplicationAttemptId] = None)\n+\n+/**\n+ * Container for [[SchedulerExtensionService]] instances.\n+ *\n+ * Loads Extension Services from the configuration property\n+ * `\"spark.yarn.services\"`, instantiates and starts them.\n+ * When stopped, it stops all child entries.\n+ *\n+ * The order in which child extension services are started and stopped\n+ * is undefined.\n+ *\n+ */\n+private[spark] class SchedulerExtensionServices extends SchedulerExtensionService\n+    with Logging {\n+  private var serviceOption: Option[String] = None\n+  private var services: List[SchedulerExtensionService] = Nil\n+  private val started = new AtomicBoolean(false)\n+  private var binding: SchedulerExtensionServiceBinding = _\n+\n+  /**\n+   * Binding operation will load the named services and call bind on them too; the\n+   * entire set of services are then ready for `init()` and `start()` calls.\n+   *\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit = {\n+    if (started.getAndSet(true)) {\n+      logWarning(\"Ignoring re-entrant start operation\")\n+      return\n+    }\n+    require(binding.sparkContext != null, \"Null context parameter\")\n+    require(binding.applicationId != null, \"Null appId parameter\")\n+    this.binding = binding\n+    val sparkContext = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    logInfo(s\"Starting Yarn extension services with app ${binding.applicationId}\" +"
  }],
  "prId": 9182
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`getConf` creates a copy of the conf; you can use `sparkContext.conf` which doesn't.\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-11-13T19:40:54Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.{Logging, SparkContext}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped.\n+ *\n+ * 1. For implementations to be loadable by `SchedulerExtensionServices`,\n+ * they must provide an empty constructor.\n+ * 2. The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]].\n+ *\n+ * The attempt ID will be set if the service is started within a YARN application master;\n+ * there is then a different attempt ID for every time that AM is restarted.\n+ * When the service binding is instantiated on a client, there's no attempt ID, as it lacks\n+ * this information.\n+ * @param sparkContext current spark context\n+ * @param applicationId YARN application ID\n+ * @param attemptId YARN attemptID -if known.\n+ */\n+case class SchedulerExtensionServiceBinding(\n+    sparkContext: SparkContext,\n+    applicationId: ApplicationId,\n+    attemptId: Option[ApplicationAttemptId] = None)\n+\n+/**\n+ * Container for [[SchedulerExtensionService]] instances.\n+ *\n+ * Loads Extension Services from the configuration property\n+ * `\"spark.yarn.services\"`, instantiates and starts them.\n+ * When stopped, it stops all child entries.\n+ *\n+ * The order in which child extension services are started and stopped\n+ * is undefined.\n+ *\n+ */\n+private[spark] class SchedulerExtensionServices extends SchedulerExtensionService\n+    with Logging {\n+  private var serviceOption: Option[String] = None\n+  private var services: List[SchedulerExtensionService] = Nil\n+  private val started = new AtomicBoolean(false)\n+  private var binding: SchedulerExtensionServiceBinding = _\n+\n+  /**\n+   * Binding operation will load the named services and call bind on them too; the\n+   * entire set of services are then ready for `init()` and `start()` calls.\n+   *\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit = {\n+    if (started.getAndSet(true)) {\n+      logWarning(\"Ignoring re-entrant start operation\")\n+      return\n+    }\n+    require(binding.sparkContext != null, \"Null context parameter\")\n+    require(binding.applicationId != null, \"Null appId parameter\")\n+    this.binding = binding\n+    val sparkContext = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    logInfo(s\"Starting Yarn extension services with app ${binding.applicationId}\" +\n+      s\" and attemptId $attemptId\")\n+\n+    serviceOption = sparkContext.getConf.getOption(SchedulerExtensionServices.SPARK_YARN_SERVICES)"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "fixed. also scanning test code to see where I've done it\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-11-18T16:56:34Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.{Logging, SparkContext}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped.\n+ *\n+ * 1. For implementations to be loadable by `SchedulerExtensionServices`,\n+ * they must provide an empty constructor.\n+ * 2. The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]].\n+ *\n+ * The attempt ID will be set if the service is started within a YARN application master;\n+ * there is then a different attempt ID for every time that AM is restarted.\n+ * When the service binding is instantiated on a client, there's no attempt ID, as it lacks\n+ * this information.\n+ * @param sparkContext current spark context\n+ * @param applicationId YARN application ID\n+ * @param attemptId YARN attemptID -if known.\n+ */\n+case class SchedulerExtensionServiceBinding(\n+    sparkContext: SparkContext,\n+    applicationId: ApplicationId,\n+    attemptId: Option[ApplicationAttemptId] = None)\n+\n+/**\n+ * Container for [[SchedulerExtensionService]] instances.\n+ *\n+ * Loads Extension Services from the configuration property\n+ * `\"spark.yarn.services\"`, instantiates and starts them.\n+ * When stopped, it stops all child entries.\n+ *\n+ * The order in which child extension services are started and stopped\n+ * is undefined.\n+ *\n+ */\n+private[spark] class SchedulerExtensionServices extends SchedulerExtensionService\n+    with Logging {\n+  private var serviceOption: Option[String] = None\n+  private var services: List[SchedulerExtensionService] = Nil\n+  private val started = new AtomicBoolean(false)\n+  private var binding: SchedulerExtensionServiceBinding = _\n+\n+  /**\n+   * Binding operation will load the named services and call bind on them too; the\n+   * entire set of services are then ready for `init()` and `start()` calls.\n+   *\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit = {\n+    if (started.getAndSet(true)) {\n+      logWarning(\"Ignoring re-entrant start operation\")\n+      return\n+    }\n+    require(binding.sparkContext != null, \"Null context parameter\")\n+    require(binding.applicationId != null, \"Null appId parameter\")\n+    this.binding = binding\n+    val sparkContext = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    logInfo(s\"Starting Yarn extension services with app ${binding.applicationId}\" +\n+      s\" and attemptId $attemptId\")\n+\n+    serviceOption = sparkContext.getConf.getOption(SchedulerExtensionServices.SPARK_YARN_SERVICES)"
  }],
  "prId": 9182
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: nuke this line and the previous one? (they're also indented wrongly.)\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-11-13T19:44:31Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.{Logging, SparkContext}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped.\n+ *\n+ * 1. For implementations to be loadable by `SchedulerExtensionServices`,\n+ * they must provide an empty constructor.\n+ * 2. The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]].\n+ *\n+ * The attempt ID will be set if the service is started within a YARN application master;\n+ * there is then a different attempt ID for every time that AM is restarted.\n+ * When the service binding is instantiated on a client, there's no attempt ID, as it lacks\n+ * this information.\n+ * @param sparkContext current spark context\n+ * @param applicationId YARN application ID\n+ * @param attemptId YARN attemptID -if known.\n+ */\n+case class SchedulerExtensionServiceBinding(\n+    sparkContext: SparkContext,\n+    applicationId: ApplicationId,\n+    attemptId: Option[ApplicationAttemptId] = None)\n+\n+/**\n+ * Container for [[SchedulerExtensionService]] instances.\n+ *\n+ * Loads Extension Services from the configuration property\n+ * `\"spark.yarn.services\"`, instantiates and starts them.\n+ * When stopped, it stops all child entries.\n+ *\n+ * The order in which child extension services are started and stopped\n+ * is undefined.\n+ *\n+ */\n+private[spark] class SchedulerExtensionServices extends SchedulerExtensionService\n+    with Logging {\n+  private var serviceOption: Option[String] = None\n+  private var services: List[SchedulerExtensionService] = Nil\n+  private val started = new AtomicBoolean(false)\n+  private var binding: SchedulerExtensionServiceBinding = _\n+\n+  /**\n+   * Binding operation will load the named services and call bind on them too; the\n+   * entire set of services are then ready for `init()` and `start()` calls.\n+   *\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit = {\n+    if (started.getAndSet(true)) {\n+      logWarning(\"Ignoring re-entrant start operation\")\n+      return\n+    }\n+    require(binding.sparkContext != null, \"Null context parameter\")\n+    require(binding.applicationId != null, \"Null appId parameter\")\n+    this.binding = binding\n+    val sparkContext = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    logInfo(s\"Starting Yarn extension services with app ${binding.applicationId}\" +\n+      s\" and attemptId $attemptId\")\n+\n+    serviceOption = sparkContext.getConf.getOption(SchedulerExtensionServices.SPARK_YARN_SERVICES)\n+    services = serviceOption\n+      .map { s =>\n+        s.split(\",\").map(_.trim()).filter(!_.isEmpty)\n+          .map { sClass =>\n+            val instance = Utils.classForName(sClass)\n+              .newInstance()\n+              .asInstanceOf[SchedulerExtensionService]\n+            // bind this service\n+            instance.start(binding)\n+            logInfo(s\"Service $sClass started\")\n+            instance\n+          }.toList\n+      }.getOrElse(Nil)\n+  }\n+\n+  /**\n+   * Get the list of services.\n+   *\n+   * @return a list of services; Nil until the service is started\n+   */\n+  def getServices: List[SchedulerExtensionService] = services\n+\n+  /**\n+   * Stop the services; idempotent.\n+    *\n+    * Any"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "done.\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-11-18T16:59:15Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.{Logging, SparkContext}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped.\n+ *\n+ * 1. For implementations to be loadable by `SchedulerExtensionServices`,\n+ * they must provide an empty constructor.\n+ * 2. The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]].\n+ *\n+ * The attempt ID will be set if the service is started within a YARN application master;\n+ * there is then a different attempt ID for every time that AM is restarted.\n+ * When the service binding is instantiated on a client, there's no attempt ID, as it lacks\n+ * this information.\n+ * @param sparkContext current spark context\n+ * @param applicationId YARN application ID\n+ * @param attemptId YARN attemptID -if known.\n+ */\n+case class SchedulerExtensionServiceBinding(\n+    sparkContext: SparkContext,\n+    applicationId: ApplicationId,\n+    attemptId: Option[ApplicationAttemptId] = None)\n+\n+/**\n+ * Container for [[SchedulerExtensionService]] instances.\n+ *\n+ * Loads Extension Services from the configuration property\n+ * `\"spark.yarn.services\"`, instantiates and starts them.\n+ * When stopped, it stops all child entries.\n+ *\n+ * The order in which child extension services are started and stopped\n+ * is undefined.\n+ *\n+ */\n+private[spark] class SchedulerExtensionServices extends SchedulerExtensionService\n+    with Logging {\n+  private var serviceOption: Option[String] = None\n+  private var services: List[SchedulerExtensionService] = Nil\n+  private val started = new AtomicBoolean(false)\n+  private var binding: SchedulerExtensionServiceBinding = _\n+\n+  /**\n+   * Binding operation will load the named services and call bind on them too; the\n+   * entire set of services are then ready for `init()` and `start()` calls.\n+   *\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit = {\n+    if (started.getAndSet(true)) {\n+      logWarning(\"Ignoring re-entrant start operation\")\n+      return\n+    }\n+    require(binding.sparkContext != null, \"Null context parameter\")\n+    require(binding.applicationId != null, \"Null appId parameter\")\n+    this.binding = binding\n+    val sparkContext = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    logInfo(s\"Starting Yarn extension services with app ${binding.applicationId}\" +\n+      s\" and attemptId $attemptId\")\n+\n+    serviceOption = sparkContext.getConf.getOption(SchedulerExtensionServices.SPARK_YARN_SERVICES)\n+    services = serviceOption\n+      .map { s =>\n+        s.split(\",\").map(_.trim()).filter(!_.isEmpty)\n+          .map { sClass =>\n+            val instance = Utils.classForName(sClass)\n+              .newInstance()\n+              .asInstanceOf[SchedulerExtensionService]\n+            // bind this service\n+            instance.start(binding)\n+            logInfo(s\"Service $sClass started\")\n+            instance\n+          }.toList\n+      }.getOrElse(Nil)\n+  }\n+\n+  /**\n+   * Get the list of services.\n+   *\n+   * @return a list of services; Nil until the service is started\n+   */\n+  def getServices: List[SchedulerExtensionService] = services\n+\n+  /**\n+   * Stop the services; idempotent.\n+    *\n+    * Any"
  }],
  "prId": 9182
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`Utils.tryLogNonFatalError`?\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-12-01T20:43:34Z",
    "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.{Logging, SparkContext}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped.\n+ *\n+ * 1. For implementations to be loadable by `SchedulerExtensionServices`,\n+ * they must provide an empty constructor.\n+ * 2. The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]].\n+ *\n+ * The attempt ID will be set if the service is started within a YARN application master;\n+ * there is then a different attempt ID for every time that AM is restarted.\n+ * When the service binding is instantiated in client mode, there's no attempt ID, as it lacks\n+ * this information.\n+ * @param sparkContext current spark context\n+ * @param applicationId YARN application ID\n+ * @param attemptId YARN attemptID. This will always be unset in client mode, and always set in\n+ *                  cluster mode.\n+ */\n+case class SchedulerExtensionServiceBinding(\n+    sparkContext: SparkContext,\n+    applicationId: ApplicationId,\n+    attemptId: Option[ApplicationAttemptId] = None)\n+\n+/**\n+ * Container for [[SchedulerExtensionService]] instances.\n+ *\n+ * Loads Extension Services from the configuration property\n+ * `\"spark.yarn.services\"`, instantiates and starts them.\n+ * When stopped, it stops all child entries.\n+ *\n+ * The order in which child extension services are started and stopped\n+ * is undefined.\n+ */\n+private[spark] class SchedulerExtensionServices extends SchedulerExtensionService\n+    with Logging {\n+  private var serviceOption: Option[String] = None\n+  private var services: List[SchedulerExtensionService] = Nil\n+  private val started = new AtomicBoolean(false)\n+  private var binding: SchedulerExtensionServiceBinding = _\n+\n+  /**\n+   * Binding operation will load the named services and call bind on them too; the\n+   * entire set of services are then ready for `init()` and `start()` calls.\n+   *\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit = {\n+    if (started.getAndSet(true)) {\n+      logWarning(\"Ignoring re-entrant start operation\")\n+      return\n+    }\n+    require(binding.sparkContext != null, \"Null context parameter\")\n+    require(binding.applicationId != null, \"Null appId parameter\")\n+    this.binding = binding\n+    val sparkContext = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    logInfo(s\"Starting Yarn extension services with app $appId and attemptId $attemptId\")\n+\n+    serviceOption = sparkContext.getConf.getOption(SchedulerExtensionServices.SPARK_YARN_SERVICES)\n+    services = serviceOption\n+      .map { s =>\n+        s.split(\",\").map(_.trim()).filter(!_.isEmpty)\n+          .map { sClass =>\n+            val instance = Utils.classForName(sClass)\n+              .newInstance()\n+              .asInstanceOf[SchedulerExtensionService]\n+            // bind this service\n+            instance.start(binding)\n+            logInfo(s\"Service $sClass started\")\n+            instance\n+          }.toList\n+      }.getOrElse(Nil)\n+  }\n+\n+  /**\n+   * Get the list of services.\n+   *\n+   * @return a list of services; Nil until the service is started\n+   */\n+  def getServices: List[SchedulerExtensionService] = services\n+\n+  /**\n+   * Stop the services; idempotent.\n+   *\n+   */\n+  override def stop(): Unit = {\n+    if (started.getAndSet(false)) {\n+      logInfo(s\"Stopping $this\")\n+      services.foreach { s =>\n+        try {"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "done. Will look for other uses elsewhere too\n",
    "commit": "b9a183467e529dbde79bbf1548c0c921eba18d0e",
    "createdAt": "2015-12-02T16:06:30Z",
    "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster\n+\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+\n+import org.apache.spark.{Logging, SparkContext}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * An extension service that can be loaded into a Spark YARN scheduler.\n+ * A Service that can be started and stopped.\n+ *\n+ * 1. For implementations to be loadable by `SchedulerExtensionServices`,\n+ * they must provide an empty constructor.\n+ * 2. The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+ * never invoked.\n+ */\n+trait SchedulerExtensionService {\n+\n+  /**\n+   * Start the extension service. This should be a no-op if\n+   * called more than once.\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit\n+\n+  /**\n+   * Stop the service\n+   * The `stop()` operation MUST be idempotent, and succeed even if `start()` was\n+   * never invoked.\n+   */\n+  def stop(): Unit\n+}\n+\n+/**\n+ * Binding information for a [[SchedulerExtensionService]].\n+ *\n+ * The attempt ID will be set if the service is started within a YARN application master;\n+ * there is then a different attempt ID for every time that AM is restarted.\n+ * When the service binding is instantiated in client mode, there's no attempt ID, as it lacks\n+ * this information.\n+ * @param sparkContext current spark context\n+ * @param applicationId YARN application ID\n+ * @param attemptId YARN attemptID. This will always be unset in client mode, and always set in\n+ *                  cluster mode.\n+ */\n+case class SchedulerExtensionServiceBinding(\n+    sparkContext: SparkContext,\n+    applicationId: ApplicationId,\n+    attemptId: Option[ApplicationAttemptId] = None)\n+\n+/**\n+ * Container for [[SchedulerExtensionService]] instances.\n+ *\n+ * Loads Extension Services from the configuration property\n+ * `\"spark.yarn.services\"`, instantiates and starts them.\n+ * When stopped, it stops all child entries.\n+ *\n+ * The order in which child extension services are started and stopped\n+ * is undefined.\n+ */\n+private[spark] class SchedulerExtensionServices extends SchedulerExtensionService\n+    with Logging {\n+  private var serviceOption: Option[String] = None\n+  private var services: List[SchedulerExtensionService] = Nil\n+  private val started = new AtomicBoolean(false)\n+  private var binding: SchedulerExtensionServiceBinding = _\n+\n+  /**\n+   * Binding operation will load the named services and call bind on them too; the\n+   * entire set of services are then ready for `init()` and `start()` calls.\n+   *\n+   * @param binding binding to the spark application and YARN\n+   */\n+  def start(binding: SchedulerExtensionServiceBinding): Unit = {\n+    if (started.getAndSet(true)) {\n+      logWarning(\"Ignoring re-entrant start operation\")\n+      return\n+    }\n+    require(binding.sparkContext != null, \"Null context parameter\")\n+    require(binding.applicationId != null, \"Null appId parameter\")\n+    this.binding = binding\n+    val sparkContext = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    logInfo(s\"Starting Yarn extension services with app $appId and attemptId $attemptId\")\n+\n+    serviceOption = sparkContext.getConf.getOption(SchedulerExtensionServices.SPARK_YARN_SERVICES)\n+    services = serviceOption\n+      .map { s =>\n+        s.split(\",\").map(_.trim()).filter(!_.isEmpty)\n+          .map { sClass =>\n+            val instance = Utils.classForName(sClass)\n+              .newInstance()\n+              .asInstanceOf[SchedulerExtensionService]\n+            // bind this service\n+            instance.start(binding)\n+            logInfo(s\"Service $sClass started\")\n+            instance\n+          }.toList\n+      }.getOrElse(Nil)\n+  }\n+\n+  /**\n+   * Get the list of services.\n+   *\n+   * @return a list of services; Nil until the service is started\n+   */\n+  def getServices: List[SchedulerExtensionService] = services\n+\n+  /**\n+   * Stop the services; idempotent.\n+   *\n+   */\n+  override def stop(): Unit = {\n+    if (started.getAndSet(false)) {\n+      logInfo(s\"Stopping $this\")\n+      services.foreach { s =>\n+        try {"
  }],
  "prId": 9182
}]