[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "What is `drop(13)`? (hint: magic numbers are bad.)\n",
    "commit": "d86ac9c532896cd84474f194279e216081970a8c",
    "createdAt": "2016-07-11T17:20:51Z",
    "diffHunk": "@@ -274,6 +288,37 @@ private object YarnClusterDriverWithFailure extends Logging with Matchers {\n   }\n }\n \n+private object YarnClusterDriverUseSparkHadoopUtilConf extends Logging with Matchers {\n+  def main(args: Array[String]): Unit = {\n+    if (args.length != 2) {\n+      // scalastyle:off println\n+      System.err.println(\n+        s\"\"\"\n+        |Invalid command line: ${args.mkString(\" \")}\n+        |\n+        |Usage: YarnClusterDriverUseSparkHadoopUtilConf [propertyKey=value] [result file]\n+        \"\"\".stripMargin)\n+      // scalastyle:on println\n+      System.exit(1)\n+    }\n+\n+    val sc = new SparkContext(new SparkConf()\n+      .set(\"spark.extraListeners\", classOf[SaveExecutorInfo].getName)\n+      .setAppName(\"yarn test using SparkHadoopUtil's conf\"))\n+\n+    val propertyKeyValue = args(0).split(\"=\")\n+    val status = new File(args(1))\n+    var result = \"failure\"\n+    try {\n+      SparkHadoopUtil.get.conf.get(propertyKeyValue(0).drop(13)) should be (propertyKeyValue(1))"
  }, {
    "author": {
      "login": "sharkdtu"
    },
    "body": "@vanzin  it means drop `spark.hadoop.`. it may be hard to understand and i will fix it.\n",
    "commit": "d86ac9c532896cd84474f194279e216081970a8c",
    "createdAt": "2016-07-12T00:45:35Z",
    "diffHunk": "@@ -274,6 +288,37 @@ private object YarnClusterDriverWithFailure extends Logging with Matchers {\n   }\n }\n \n+private object YarnClusterDriverUseSparkHadoopUtilConf extends Logging with Matchers {\n+  def main(args: Array[String]): Unit = {\n+    if (args.length != 2) {\n+      // scalastyle:off println\n+      System.err.println(\n+        s\"\"\"\n+        |Invalid command line: ${args.mkString(\" \")}\n+        |\n+        |Usage: YarnClusterDriverUseSparkHadoopUtilConf [propertyKey=value] [result file]\n+        \"\"\".stripMargin)\n+      // scalastyle:on println\n+      System.exit(1)\n+    }\n+\n+    val sc = new SparkContext(new SparkConf()\n+      .set(\"spark.extraListeners\", classOf[SaveExecutorInfo].getName)\n+      .setAppName(\"yarn test using SparkHadoopUtil's conf\"))\n+\n+    val propertyKeyValue = args(0).split(\"=\")\n+    val status = new File(args(1))\n+    var result = \"failure\"\n+    try {\n+      SparkHadoopUtil.get.conf.get(propertyKeyValue(0).drop(13)) should be (propertyKeyValue(1))"
  }],
  "prId": 14088
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Seems that you could pass `key` and `value` separately instead of having to parse the config; and that you don't need to pass `key` with the `spark.hadoop` prefix here either. You might even just hardcode the values instead...\n",
    "commit": "d86ac9c532896cd84474f194279e216081970a8c",
    "createdAt": "2016-07-11T17:22:20Z",
    "diffHunk": "@@ -274,6 +288,37 @@ private object YarnClusterDriverWithFailure extends Logging with Matchers {\n   }\n }\n \n+private object YarnClusterDriverUseSparkHadoopUtilConf extends Logging with Matchers {\n+  def main(args: Array[String]): Unit = {\n+    if (args.length != 2) {\n+      // scalastyle:off println\n+      System.err.println(\n+        s\"\"\"\n+        |Invalid command line: ${args.mkString(\" \")}\n+        |\n+        |Usage: YarnClusterDriverUseSparkHadoopUtilConf [propertyKey=value] [result file]\n+        \"\"\".stripMargin)\n+      // scalastyle:on println\n+      System.exit(1)\n+    }\n+\n+    val sc = new SparkContext(new SparkConf()\n+      .set(\"spark.extraListeners\", classOf[SaveExecutorInfo].getName)\n+      .setAppName(\"yarn test using SparkHadoopUtil's conf\"))\n+\n+    val propertyKeyValue = args(0).split(\"=\")"
  }],
  "prId": 14088
}]