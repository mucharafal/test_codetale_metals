[{
  "comments": [{
    "author": {
      "login": "jerryshao"
    },
    "body": "Can we make `Source` related things into a sub-class or separated class. Here in this class there's so many class parameters, it is not easy to understand and track the state.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-10-28T06:17:53Z",
    "diffHunk": "@@ -0,0 +1,1328 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.InterruptedIOException\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{LinkedBlockingDeque, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Counter, MetricRegistry, Timer}\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.metrics.source.Source\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{SchedulerExtensionService, SchedulerExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered YARN Timeline Server.\n+ *\n+ * Posting algorithm\n+ *\n+ * 1. The service subscribes to all events coming from the Spark Context.\n+ * 1. These events are serialized into JSON objects for publishing to the timeline service through\n+ * HTTP(S) posts.\n+ * 1. Events are buffered into `pendingEvents` until a batch is aggregated into a\n+ * [[TimelineEntity]] for posting.\n+ * 1. That aggregation happens when a lifecycle event (application start/stop) takes place,\n+ * or the number of pending events in a running application exceeds the limit set in\n+ * `spark.hadoop.yarn.timeline.batch.size`.\n+ * 1. Posting operations take place in a separate thread from the spark event listener.\n+ * 1. If an attempt to post to the timeline server fails, the service sleeps and then\n+ * it is re-attempted after the retry period defined by\n+ * `spark.hadoop.yarn.timeline.post.retry.interval`.\n+ * 1. If the number of events buffered in the history service exceed the limit set in\n+ * `spark.hadoop.yarn.timeline.post.limit`, then further events other than application start/stop\n+ * are dropped.\n+ * 1. When the service is stopped, it will make a best-effort attempt to post all queued events.\n+ * the call of [[stop()]] can block up to the duration of\n+ * `spark.hadoop.yarn.timeline.shutdown.waittime` for this to take place.\n+ * 1. No events are posted until the service receives a [[SparkListenerApplicationStart]] event.\n+ *\n+ * If the spark context has a metrics registry, then the internal counters of queued entities,\n+ * post failures and successes, and the performance of the posting operation are all registered\n+ * as metrics.\n+ *\n+ * The shutdown logic is somewhat convoluted, as the posting thread may be blocked on HTTP IO\n+ * when the shutdown process begins. In this situation, the thread continues to be blocked, and\n+ * will be interrupted once the wait time has expired. All time consumed during the ongoing\n+ * operation will be counted as part of the shutdown time period.\n+ */\n+private[spark] class YarnHistoryService extends SchedulerExtensionService\n+  with Logging with Source {"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "OK, I'll pull out the counters into their source subclass\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-10-29T14:03:30Z",
    "diffHunk": "@@ -0,0 +1,1328 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.InterruptedIOException\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{LinkedBlockingDeque, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Counter, MetricRegistry, Timer}\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.metrics.source.Source\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{SchedulerExtensionService, SchedulerExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered YARN Timeline Server.\n+ *\n+ * Posting algorithm\n+ *\n+ * 1. The service subscribes to all events coming from the Spark Context.\n+ * 1. These events are serialized into JSON objects for publishing to the timeline service through\n+ * HTTP(S) posts.\n+ * 1. Events are buffered into `pendingEvents` until a batch is aggregated into a\n+ * [[TimelineEntity]] for posting.\n+ * 1. That aggregation happens when a lifecycle event (application start/stop) takes place,\n+ * or the number of pending events in a running application exceeds the limit set in\n+ * `spark.hadoop.yarn.timeline.batch.size`.\n+ * 1. Posting operations take place in a separate thread from the spark event listener.\n+ * 1. If an attempt to post to the timeline server fails, the service sleeps and then\n+ * it is re-attempted after the retry period defined by\n+ * `spark.hadoop.yarn.timeline.post.retry.interval`.\n+ * 1. If the number of events buffered in the history service exceed the limit set in\n+ * `spark.hadoop.yarn.timeline.post.limit`, then further events other than application start/stop\n+ * are dropped.\n+ * 1. When the service is stopped, it will make a best-effort attempt to post all queued events.\n+ * the call of [[stop()]] can block up to the duration of\n+ * `spark.hadoop.yarn.timeline.shutdown.waittime` for this to take place.\n+ * 1. No events are posted until the service receives a [[SparkListenerApplicationStart]] event.\n+ *\n+ * If the spark context has a metrics registry, then the internal counters of queued entities,\n+ * post failures and successes, and the performance of the posting operation are all registered\n+ * as metrics.\n+ *\n+ * The shutdown logic is somewhat convoluted, as the posting thread may be blocked on HTTP IO\n+ * when the shutdown process begins. In this situation, the thread continues to be blocked, and\n+ * will be interrupted once the wait time has expired. All time consumed during the ongoing\n+ * operation will be counted as part of the shutdown time period.\n+ */\n+private[spark] class YarnHistoryService extends SchedulerExtensionService\n+  with Logging with Source {"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "jerryshao"
    },
    "body": "It would be better here to use:\n\n``` scala\ns\"\"\"\n|\n|\n\"\"\".stripMargin\n```\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-10-28T06:23:00Z",
    "diffHunk": "@@ -0,0 +1,1328 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.InterruptedIOException\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{LinkedBlockingDeque, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Counter, MetricRegistry, Timer}\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.metrics.source.Source\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{SchedulerExtensionService, SchedulerExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered YARN Timeline Server.\n+ *\n+ * Posting algorithm\n+ *\n+ * 1. The service subscribes to all events coming from the Spark Context.\n+ * 1. These events are serialized into JSON objects for publishing to the timeline service through\n+ * HTTP(S) posts.\n+ * 1. Events are buffered into `pendingEvents` until a batch is aggregated into a\n+ * [[TimelineEntity]] for posting.\n+ * 1. That aggregation happens when a lifecycle event (application start/stop) takes place,\n+ * or the number of pending events in a running application exceeds the limit set in\n+ * `spark.hadoop.yarn.timeline.batch.size`.\n+ * 1. Posting operations take place in a separate thread from the spark event listener.\n+ * 1. If an attempt to post to the timeline server fails, the service sleeps and then\n+ * it is re-attempted after the retry period defined by\n+ * `spark.hadoop.yarn.timeline.post.retry.interval`.\n+ * 1. If the number of events buffered in the history service exceed the limit set in\n+ * `spark.hadoop.yarn.timeline.post.limit`, then further events other than application start/stop\n+ * are dropped.\n+ * 1. When the service is stopped, it will make a best-effort attempt to post all queued events.\n+ * the call of [[stop()]] can block up to the duration of\n+ * `spark.hadoop.yarn.timeline.shutdown.waittime` for this to take place.\n+ * 1. No events are posted until the service receives a [[SparkListenerApplicationStart]] event.\n+ *\n+ * If the spark context has a metrics registry, then the internal counters of queued entities,\n+ * post failures and successes, and the performance of the posting operation are all registered\n+ * as metrics.\n+ *\n+ * The shutdown logic is somewhat convoluted, as the posting thread may be blocked on HTTP IO\n+ * when the shutdown process begins. In this situation, the thread continues to be blocked, and\n+ * will be interrupted once the wait time has expired. All time consumed during the ongoing\n+ * operation will be counted as part of the shutdown time period.\n+ */\n+private[spark] class YarnHistoryService extends SchedulerExtensionService\n+  with Logging with Source {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  /** Get the current state */\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+\n+  /**\n+   * Enter a new state, return the old one. Atomic.\n+   * There are no checks on state model.\n+   * @param state new state\n+   * @return previous state\n+   */\n+  private def enterState(state: Int): Int = {\n+    logDebug(s\"Entering state $state from $serviceState\")\n+    _serviceState.getAndSet(state)\n+  }\n+\n+  /** Spark context; valid once started */\n+  private var sparkContext: SparkContext = _\n+\n+  /** YARN configuration from the spark context */\n+  private var config: YarnConfiguration = _\n+\n+  /** Application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** Attempt ID -this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var _timelineClient: Option[TimelineClient] = None\n+\n+  /** Registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Application name  from the spark start event */\n+  private var applicationName: String = _\n+\n+  /** Application ID */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** User name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName = Utils.getCurrentUserName()\n+\n+  /** Clock for recording time */\n+  private val clock = new SystemClock()\n+\n+  /** Start time of the application, as received in the start event. */\n+  private var startTime: Long = _\n+\n+  /** Start time of the application, as received in the end event. */\n+  private var endTime: Long = _\n+\n+  /** Number of events to batch up before posting */\n+  private var _batchSize = DEFAULT_BATCH_SIZE\n+\n+  /** Queue of entities to asynchronously post, plus the number of events in each entry */\n+  private val _postingQueue = new LinkedBlockingDeque[PostQueueAction]()\n+\n+  /** Size of post queue events */\n+  private val postQueueEventSize = new Counter()\n+\n+  /** Limit on the total number of events permitted */\n+  private var _postQueueLimit = DEFAULT_POST_EVENT_LIMIT\n+\n+  /** List of events which will be pulled into a timeline entity when created */\n+  private var pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+\n+  /** The received application started event; `None` if no event has been received */\n+  private var applicationStartEvent: Option[SparkListenerApplicationStart] = None\n+\n+  /** The received application end event; `None` if no event has been received */\n+  private var applicationEndEvent: Option[SparkListenerApplicationEnd] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /** Has the application event event been processed? */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** Counter of events processed -that is have been through handleEvent() */\n+  private val _eventsProcessed = new Counter()\n+\n+  /** Counter of events queued. */\n+  private val _eventsQueued = new Counter()\n+\n+  /** Counter of number of attempts to post entities. */\n+  private val _entityPostAttempts = new Counter()\n+\n+  /** Counter of number of successful entity post operations. */\n+  private val _entityPostSuccesses = new Counter()\n+\n+  /** How many entity postings failed? */\n+  private val _entityPostFailures = new Counter()\n+\n+  /** How many entity postings were rejected? */\n+  private val _entityPostRejections = new Counter()\n+\n+  /** The number of events which were dropped as the backlog of pending posts was too big. */\n+  private val _eventsDropped = new Counter()\n+\n+  /** How many flushes have taken place? */\n+  private val flushCount = new Counter()\n+\n+  /** Event handler thread */\n+  private var entityPostThread: Option[Thread] = None\n+\n+  /** Flag to indicate the queue is stopped; events aren't being processed. */\n+  private val queueStopped = new AtomicBoolean(true)\n+\n+  /** Boolean to track when the post thread is active; Set and reset in the thread itself. */\n+  private val postThreadActive = new AtomicBoolean(false)\n+\n+  /** How long to wait in millseconds for shutdown before giving up? */\n+  private var shutdownWaitTime = 0L\n+\n+  /** What is the initial and incrementing interval for POST retries? */\n+  private var retryInterval = 0L\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: Option[String] = None\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var _timelineWebappAddress: URI = _\n+\n+  /** Name for metrics: yarn_history */\n+  override val sourceName = METRICS_NAME\n+\n+  /** Metrics registry */\n+  override val metricRegistry = new MetricRegistry()\n+\n+  /** Timer to build up statistics on post operation times */\n+  private val postOperationTimer: Timer = metricRegistry.timer(MetricRegistry.name(\"posts\"))\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private[yarn] def createTimelineClient(): TimelineClient = {\n+    require(_timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def timelineClient: TimelineClient = {\n+    synchronized { _timelineClient.get }\n+  }\n+\n+  /**\n+   * Get the total number of events dropped due to the queue of\n+   * outstanding posts being too long.\n+   * @return counter of events processed\n+   */\n+\n+  def eventsDropped: Long = {\n+    _eventsDropped.getCount\n+  }\n+\n+  /**\n+   * Get the total number of processed events, those handled in the back-end thread without\n+   * being rejected.\n+   *\n+   * @return counter of events processed\n+   */\n+  def eventsProcessed: Long = {\n+    _eventsProcessed.getCount\n+  }\n+\n+  /**\n+   * Get the total number of events queued.\n+   *\n+   * @return the total event count\n+   */\n+  def eventsQueued: Long = {\n+    _eventsQueued.getCount\n+  }\n+\n+  /**\n+   * Get the current size of the queue.\n+   *\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    _postingQueue.size()\n+  }\n+\n+  /**\n+   * Get the current batch size.\n+   *\n+   * @return the batch size\n+   */\n+  def batchSize: Int = {\n+    _batchSize\n+  }\n+\n+  /**\n+   * Query the counter of attempts to post entities to the timeline service.\n+   *\n+   * @return the current value\n+   */\n+  def postAttempts: Long = _entityPostAttempts.getCount\n+\n+  /**\n+   * Get the total number of failed post operations.\n+   *\n+   * @return counter of timeline post operations which failed\n+   */\n+  def postFailures: Long = {\n+    _entityPostFailures.getCount\n+  }\n+\n+  /**\n+   * Query the counter of successful post operations (this is not the same as the\n+   * number of events posted).\n+   *\n+   * @return the number of successful post operations.\n+   */\n+  def postSuccesses: Long = _entityPostSuccesses.getCount\n+\n+  /**\n+   * Is the asynchronous posting thread active?\n+   *\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service.\n+   *\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = _applicationId\n+\n+  /**\n+   * The YARN attempt ID of this history service.\n+   *\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = _attemptId\n+\n+  /**\n+   * Reset the timeline client. Idempotent.\n+   *\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    synchronized {\n+      _timelineClient.foreach(_.stop())\n+      _timelineClient = None\n+    }\n+  }\n+\n+  /**\n+   * Create the timeline domain.\n+   *\n+   * A Timeline Domain is a uniquely identified 'namespace' for accessing parts of the timeline.\n+   * Security levels are are managed at the domain level, so one is created if the\n+   * spark acls are enabled. Full access is then granted to the current user,\n+   * all users in the configuration options `\"spark.modify.acls\"` and `\"spark.admin.acls\"`;\n+   * read access to those users and those listed in `\"spark.ui.view.acls\"`\n+   *\n+   * @return an optional domain string. If `None`, then no domain was created.\n+   */\n+  private def createTimelineDomain(): Option[String] = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsOn = sparkConf.getBoolean(\"spark.ui.acls.enable\",\n+        sparkConf.getBoolean(\"spark.acls.enable\", false))\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return None\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      logDebug(s\"Using predefined domain $predefDomain\")\n+      return predefDomain\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (Seq(current) ++ adminAcls ++ modifyAcls ++ viewAcls).mkString(\" \")\n+    val writers = (Seq(current) ++ adminAcls ++ modifyAcls).mkString(\" \")\n+    val domain = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domain with readers: $readers and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domain)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      timelineClient.putDomain(timelineDomain)\n+      Some(domain)\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"cannot create the domain $domain\", e)\n+        // fallback to default\n+        None\n+    }\n+  }\n+\n+  /**\n+   * Start the service.\n+   *\n+   * @param binding binding to the spark application and YARN\n+   */\n+  override def start(binding: SchedulerExtensionServiceBinding): Unit = {\n+    val oldstate = enterState(StartedState)\n+    if (oldstate != CreatedState) {\n+      // state model violation\n+      _serviceState.set(oldstate)\n+      throw new IllegalArgumentException(s\"Cannot start the service from state $oldstate\")\n+    }\n+    val context = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    this.sparkContext = context\n+    this.config = new YarnConfiguration(context.hadoopConfiguration)\n+    val sparkConf = sparkContext.conf\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    _batchSize = sparkConf.getInt(BATCH_SIZE, _batchSize)\n+    _postQueueLimit = sparkConf.getInt(POST_EVENT_LIMIT, _postQueueLimit)\n+    retryInterval = 1000 * sparkConf.getTimeAsSeconds(POST_RETRY_INTERVAL,\n+      DEFAULT_POST_RETRY_INTERVAL)\n+    shutdownWaitTime = 1000 * sparkConf.getTimeAsSeconds(SHUTDOWN_WAIT_TIME,\n+      DEFAULT_SHUTDOWN_WAIT_TIME)\n+\n+    // register metrics\n+    // the local registration always takes place, so test runs catch regressions.\n+    registerMetrics()\n+\n+    // the full metrics integration happens if the spark context has a metrics system\n+    val metrics = sparkContext.metricsSystem\n+    if (metrics != null) {\n+      metrics.registerSource(this)\n+    }\n+\n+    // set up the timeline service, unless it's been disabled for testing\n+    if (timelineServiceEnabled) {\n+      _timelineWebappAddress = getTimelineEndpoint(config)\n+\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service\" +\n+          s\" at${_timelineWebappAddress}\")\n+      _timelineClient = Some(createTimelineClient())\n+      domainId = createTimelineDomain()\n+      // declare that the processing is started\n+      queueStopped.set(false)\n+      val thread = new Thread(new EntityPoster(), \"EventPoster\")\n+      entityPostThread = Some(thread)\n+      thread.setDaemon(true)\n+      thread.start()\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    if (registerListener()) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled.\n+   *\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED` is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(config)\n+  }\n+\n+  /**\n+   * Register all system metrics into the [[metricRegistry]].\n+   */\n+  private def registerMetrics(): Unit = {\n+    metricRegistry.register(\"eventsProcessed\", _eventsProcessed)\n+    metricRegistry.register(\"eventsQueued\", _eventsQueued)\n+    metricRegistry.register(\"entityPostAttempts\", _entityPostAttempts)\n+    metricRegistry.register(\"entityPostSuccesses\", _entityPostSuccesses)\n+    metricRegistry.register(\"entityPostFailures\", _entityPostFailures)\n+    metricRegistry.register(\"entityPostRejections\", _entityPostRejections)\n+    metricRegistry.register(\"eventsDropped\", _eventsDropped)\n+    metricRegistry.register(\"flushCount\", flushCount)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production.\n+   *\n+   * @return a summary of the current service state\n+   */\n+  override def toString(): String = {"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "done\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-10-29T14:07:57Z",
    "diffHunk": "@@ -0,0 +1,1328 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.InterruptedIOException\n+import java.net.{ConnectException, URI}\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}\n+import java.util.concurrent.{LinkedBlockingDeque, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import com.codahale.metrics.{Counter, MetricRegistry, Timer}\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineDomain, TimelineEntity, TimelineEvent}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+\n+import org.apache.spark.deploy.history.yarn.YarnTimelineUtils._\n+import org.apache.spark.metrics.source.Source\n+import org.apache.spark.scheduler._\n+import org.apache.spark.scheduler.cluster.{SchedulerExtensionService, SchedulerExtensionServiceBinding}\n+import org.apache.spark.util.{SystemClock, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * A Yarn Extension Service to post lifecycle events to a registered YARN Timeline Server.\n+ *\n+ * Posting algorithm\n+ *\n+ * 1. The service subscribes to all events coming from the Spark Context.\n+ * 1. These events are serialized into JSON objects for publishing to the timeline service through\n+ * HTTP(S) posts.\n+ * 1. Events are buffered into `pendingEvents` until a batch is aggregated into a\n+ * [[TimelineEntity]] for posting.\n+ * 1. That aggregation happens when a lifecycle event (application start/stop) takes place,\n+ * or the number of pending events in a running application exceeds the limit set in\n+ * `spark.hadoop.yarn.timeline.batch.size`.\n+ * 1. Posting operations take place in a separate thread from the spark event listener.\n+ * 1. If an attempt to post to the timeline server fails, the service sleeps and then\n+ * it is re-attempted after the retry period defined by\n+ * `spark.hadoop.yarn.timeline.post.retry.interval`.\n+ * 1. If the number of events buffered in the history service exceed the limit set in\n+ * `spark.hadoop.yarn.timeline.post.limit`, then further events other than application start/stop\n+ * are dropped.\n+ * 1. When the service is stopped, it will make a best-effort attempt to post all queued events.\n+ * the call of [[stop()]] can block up to the duration of\n+ * `spark.hadoop.yarn.timeline.shutdown.waittime` for this to take place.\n+ * 1. No events are posted until the service receives a [[SparkListenerApplicationStart]] event.\n+ *\n+ * If the spark context has a metrics registry, then the internal counters of queued entities,\n+ * post failures and successes, and the performance of the posting operation are all registered\n+ * as metrics.\n+ *\n+ * The shutdown logic is somewhat convoluted, as the posting thread may be blocked on HTTP IO\n+ * when the shutdown process begins. In this situation, the thread continues to be blocked, and\n+ * will be interrupted once the wait time has expired. All time consumed during the ongoing\n+ * operation will be counted as part of the shutdown time period.\n+ */\n+private[spark] class YarnHistoryService extends SchedulerExtensionService\n+  with Logging with Source {\n+\n+  import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+\n+  /** Simple state model implemented in an atomic integer */\n+  private val _serviceState = new AtomicInteger(CreatedState)\n+\n+  /** Get the current state */\n+  def serviceState: Int = {\n+    _serviceState.get()\n+  }\n+\n+  /**\n+   * Enter a new state, return the old one. Atomic.\n+   * There are no checks on state model.\n+   * @param state new state\n+   * @return previous state\n+   */\n+  private def enterState(state: Int): Int = {\n+    logDebug(s\"Entering state $state from $serviceState\")\n+    _serviceState.getAndSet(state)\n+  }\n+\n+  /** Spark context; valid once started */\n+  private var sparkContext: SparkContext = _\n+\n+  /** YARN configuration from the spark context */\n+  private var config: YarnConfiguration = _\n+\n+  /** Application ID. */\n+  private var _applicationId: ApplicationId = _\n+\n+  /** Attempt ID -this will be null if the service is started in yarn-client mode */\n+  private var _attemptId: Option[ApplicationAttemptId] = None\n+\n+  /** YARN timeline client */\n+  private var _timelineClient: Option[TimelineClient] = None\n+\n+  /** Registered event listener */\n+  private var listener: Option[YarnEventListener] = None\n+\n+  /** Application name  from the spark start event */\n+  private var applicationName: String = _\n+\n+  /** Application ID */\n+  private var sparkApplicationId: Option[String] = None\n+\n+  /** Optional Attempt ID from the spark start event */\n+  private var sparkApplicationAttemptId: Option[String] = None\n+\n+  /** User name as derived from `SPARK_USER` env var or `UGI` */\n+  private var userName = Utils.getCurrentUserName()\n+\n+  /** Clock for recording time */\n+  private val clock = new SystemClock()\n+\n+  /** Start time of the application, as received in the start event. */\n+  private var startTime: Long = _\n+\n+  /** Start time of the application, as received in the end event. */\n+  private var endTime: Long = _\n+\n+  /** Number of events to batch up before posting */\n+  private var _batchSize = DEFAULT_BATCH_SIZE\n+\n+  /** Queue of entities to asynchronously post, plus the number of events in each entry */\n+  private val _postingQueue = new LinkedBlockingDeque[PostQueueAction]()\n+\n+  /** Size of post queue events */\n+  private val postQueueEventSize = new Counter()\n+\n+  /** Limit on the total number of events permitted */\n+  private var _postQueueLimit = DEFAULT_POST_EVENT_LIMIT\n+\n+  /** List of events which will be pulled into a timeline entity when created */\n+  private var pendingEvents = new mutable.LinkedList[TimelineEvent]()\n+\n+  /** The received application started event; `None` if no event has been received */\n+  private var applicationStartEvent: Option[SparkListenerApplicationStart] = None\n+\n+  /** The received application end event; `None` if no event has been received */\n+  private var applicationEndEvent: Option[SparkListenerApplicationEnd] = None\n+\n+  /** Has a start event been processed? */\n+  private val appStartEventProcessed = new AtomicBoolean(false)\n+\n+  /** Has the application event event been processed? */\n+  private val appEndEventProcessed = new AtomicBoolean(false)\n+\n+  /** Counter of events processed -that is have been through handleEvent() */\n+  private val _eventsProcessed = new Counter()\n+\n+  /** Counter of events queued. */\n+  private val _eventsQueued = new Counter()\n+\n+  /** Counter of number of attempts to post entities. */\n+  private val _entityPostAttempts = new Counter()\n+\n+  /** Counter of number of successful entity post operations. */\n+  private val _entityPostSuccesses = new Counter()\n+\n+  /** How many entity postings failed? */\n+  private val _entityPostFailures = new Counter()\n+\n+  /** How many entity postings were rejected? */\n+  private val _entityPostRejections = new Counter()\n+\n+  /** The number of events which were dropped as the backlog of pending posts was too big. */\n+  private val _eventsDropped = new Counter()\n+\n+  /** How many flushes have taken place? */\n+  private val flushCount = new Counter()\n+\n+  /** Event handler thread */\n+  private var entityPostThread: Option[Thread] = None\n+\n+  /** Flag to indicate the queue is stopped; events aren't being processed. */\n+  private val queueStopped = new AtomicBoolean(true)\n+\n+  /** Boolean to track when the post thread is active; Set and reset in the thread itself. */\n+  private val postThreadActive = new AtomicBoolean(false)\n+\n+  /** How long to wait in millseconds for shutdown before giving up? */\n+  private var shutdownWaitTime = 0L\n+\n+  /** What is the initial and incrementing interval for POST retries? */\n+  private var retryInterval = 0L\n+\n+  /** Domain ID for entities: may be null */\n+  private var domainId: Option[String] = None\n+\n+  /** URI to timeline web application -valid after `serviceStart()` */\n+  private var _timelineWebappAddress: URI = _\n+\n+  /** Name for metrics: yarn_history */\n+  override val sourceName = METRICS_NAME\n+\n+  /** Metrics registry */\n+  override val metricRegistry = new MetricRegistry()\n+\n+  /** Timer to build up statistics on post operation times */\n+  private val postOperationTimer: Timer = metricRegistry.timer(MetricRegistry.name(\"posts\"))\n+\n+  /**\n+   * Create a timeline client and start it. This does not update the\n+   * `timelineClient` field, though it does verify that the field\n+   * is unset.\n+   *\n+   * The method is private to the package so that tests can access it, which\n+   * some of the mock tests do to override the timeline client creation.\n+   * @return the timeline client\n+   */\n+  private[yarn] def createTimelineClient(): TimelineClient = {\n+    require(_timelineClient.isEmpty, \"timeline client already set\")\n+    YarnTimelineUtils.createTimelineClient(sparkContext)\n+  }\n+\n+  /**\n+   * Get the timeline client.\n+   * @return the client\n+   * @throws Exception if the timeline client is not currently running\n+   */\n+  def timelineClient: TimelineClient = {\n+    synchronized { _timelineClient.get }\n+  }\n+\n+  /**\n+   * Get the total number of events dropped due to the queue of\n+   * outstanding posts being too long.\n+   * @return counter of events processed\n+   */\n+\n+  def eventsDropped: Long = {\n+    _eventsDropped.getCount\n+  }\n+\n+  /**\n+   * Get the total number of processed events, those handled in the back-end thread without\n+   * being rejected.\n+   *\n+   * @return counter of events processed\n+   */\n+  def eventsProcessed: Long = {\n+    _eventsProcessed.getCount\n+  }\n+\n+  /**\n+   * Get the total number of events queued.\n+   *\n+   * @return the total event count\n+   */\n+  def eventsQueued: Long = {\n+    _eventsQueued.getCount\n+  }\n+\n+  /**\n+   * Get the current size of the queue.\n+   *\n+   * @return the current queue length\n+   */\n+  def getQueueSize: Int = {\n+    _postingQueue.size()\n+  }\n+\n+  /**\n+   * Get the current batch size.\n+   *\n+   * @return the batch size\n+   */\n+  def batchSize: Int = {\n+    _batchSize\n+  }\n+\n+  /**\n+   * Query the counter of attempts to post entities to the timeline service.\n+   *\n+   * @return the current value\n+   */\n+  def postAttempts: Long = _entityPostAttempts.getCount\n+\n+  /**\n+   * Get the total number of failed post operations.\n+   *\n+   * @return counter of timeline post operations which failed\n+   */\n+  def postFailures: Long = {\n+    _entityPostFailures.getCount\n+  }\n+\n+  /**\n+   * Query the counter of successful post operations (this is not the same as the\n+   * number of events posted).\n+   *\n+   * @return the number of successful post operations.\n+   */\n+  def postSuccesses: Long = _entityPostSuccesses.getCount\n+\n+  /**\n+   * Is the asynchronous posting thread active?\n+   *\n+   * @return true if the post thread has started; false if it has not yet/ever started, or\n+   *         if it has finished.\n+   */\n+  def isPostThreadActive: Boolean = {\n+    postThreadActive.get\n+  }\n+\n+  /**\n+   * The YARN application ID of this history service.\n+   *\n+   * @return the application ID provided when the service started\n+   */\n+  def applicationId: ApplicationId = _applicationId\n+\n+  /**\n+   * The YARN attempt ID of this history service.\n+   *\n+   * @return the attempt ID provided when the service started\n+   */\n+  def attemptId: Option[ApplicationAttemptId] = _attemptId\n+\n+  /**\n+   * Reset the timeline client. Idempotent.\n+   *\n+   * 1. Stop the timeline client service if running.\n+   * 2. set the `timelineClient` field to `None`\n+   */\n+  def stopTimelineClient(): Unit = {\n+    synchronized {\n+      _timelineClient.foreach(_.stop())\n+      _timelineClient = None\n+    }\n+  }\n+\n+  /**\n+   * Create the timeline domain.\n+   *\n+   * A Timeline Domain is a uniquely identified 'namespace' for accessing parts of the timeline.\n+   * Security levels are are managed at the domain level, so one is created if the\n+   * spark acls are enabled. Full access is then granted to the current user,\n+   * all users in the configuration options `\"spark.modify.acls\"` and `\"spark.admin.acls\"`;\n+   * read access to those users and those listed in `\"spark.ui.view.acls\"`\n+   *\n+   * @return an optional domain string. If `None`, then no domain was created.\n+   */\n+  private def createTimelineDomain(): Option[String] = {\n+    val sparkConf = sparkContext.getConf\n+    val aclsOn = sparkConf.getBoolean(\"spark.ui.acls.enable\",\n+        sparkConf.getBoolean(\"spark.acls.enable\", false))\n+    if (!aclsOn) {\n+      logDebug(\"ACLs are disabled; not creating the timeline domain\")\n+      return None\n+    }\n+    val predefDomain = sparkConf.getOption(TIMELINE_DOMAIN)\n+    if (predefDomain.isDefined) {\n+      logDebug(s\"Using predefined domain $predefDomain\")\n+      return predefDomain\n+    }\n+    val current = UserGroupInformation.getCurrentUser.getShortUserName\n+    val adminAcls = stringToSet(sparkConf.get(\"spark.admin.acls\", \"\"))\n+    val viewAcls = stringToSet(sparkConf.get(\"spark.ui.view.acls\", \"\"))\n+    val modifyAcls = stringToSet(sparkConf.get(\"spark.modify.acls\", \"\"))\n+\n+    val readers = (Seq(current) ++ adminAcls ++ modifyAcls ++ viewAcls).mkString(\" \")\n+    val writers = (Seq(current) ++ adminAcls ++ modifyAcls).mkString(\" \")\n+    val domain = DOMAIN_ID_PREFIX + _applicationId\n+    logInfo(s\"Creating domain $domain with readers: $readers and writers: $writers\")\n+\n+    // create the timeline domain with the reader and writer permissions\n+    val timelineDomain = new TimelineDomain()\n+    timelineDomain.setId(domain)\n+    timelineDomain.setReaders(readers)\n+    timelineDomain.setWriters(writers)\n+    try {\n+      timelineClient.putDomain(timelineDomain)\n+      Some(domain)\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"cannot create the domain $domain\", e)\n+        // fallback to default\n+        None\n+    }\n+  }\n+\n+  /**\n+   * Start the service.\n+   *\n+   * @param binding binding to the spark application and YARN\n+   */\n+  override def start(binding: SchedulerExtensionServiceBinding): Unit = {\n+    val oldstate = enterState(StartedState)\n+    if (oldstate != CreatedState) {\n+      // state model violation\n+      _serviceState.set(oldstate)\n+      throw new IllegalArgumentException(s\"Cannot start the service from state $oldstate\")\n+    }\n+    val context = binding.sparkContext\n+    val appId = binding.applicationId\n+    val attemptId = binding.attemptId\n+    require(context != null, \"Null context parameter\")\n+    bindToYarnApplication(appId, attemptId)\n+    this.sparkContext = context\n+    this.config = new YarnConfiguration(context.hadoopConfiguration)\n+    val sparkConf = sparkContext.conf\n+\n+    // work out the attempt ID from the YARN attempt ID. No attempt, assume \"1\".\n+    val attempt1 = attemptId match {\n+      case Some(attempt) => attempt.getAttemptId.toString\n+      case None => CLIENT_BACKEND_ATTEMPT_ID\n+    }\n+    setContextAppAndAttemptInfo(Some(appId.toString), Some(attempt1))\n+    _batchSize = sparkConf.getInt(BATCH_SIZE, _batchSize)\n+    _postQueueLimit = sparkConf.getInt(POST_EVENT_LIMIT, _postQueueLimit)\n+    retryInterval = 1000 * sparkConf.getTimeAsSeconds(POST_RETRY_INTERVAL,\n+      DEFAULT_POST_RETRY_INTERVAL)\n+    shutdownWaitTime = 1000 * sparkConf.getTimeAsSeconds(SHUTDOWN_WAIT_TIME,\n+      DEFAULT_SHUTDOWN_WAIT_TIME)\n+\n+    // register metrics\n+    // the local registration always takes place, so test runs catch regressions.\n+    registerMetrics()\n+\n+    // the full metrics integration happens if the spark context has a metrics system\n+    val metrics = sparkContext.metricsSystem\n+    if (metrics != null) {\n+      metrics.registerSource(this)\n+    }\n+\n+    // set up the timeline service, unless it's been disabled for testing\n+    if (timelineServiceEnabled) {\n+      _timelineWebappAddress = getTimelineEndpoint(config)\n+\n+      logInfo(s\"Starting $this\")\n+      logInfo(s\"Spark events will be published to the Timeline service\" +\n+          s\" at${_timelineWebappAddress}\")\n+      _timelineClient = Some(createTimelineClient())\n+      domainId = createTimelineDomain()\n+      // declare that the processing is started\n+      queueStopped.set(false)\n+      val thread = new Thread(new EntityPoster(), \"EventPoster\")\n+      entityPostThread = Some(thread)\n+      thread.setDaemon(true)\n+      thread.start()\n+    } else {\n+      logInfo(\"Timeline service is disabled\")\n+    }\n+    if (registerListener()) {\n+      logInfo(s\"History Service listening for events: $this\")\n+    } else {\n+      logInfo(s\"History Service is not listening for events: $this\")\n+    }\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled.\n+   *\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED` is set.\n+   */\n+  def timelineServiceEnabled: Boolean = {\n+    YarnTimelineUtils.timelineServiceEnabled(config)\n+  }\n+\n+  /**\n+   * Register all system metrics into the [[metricRegistry]].\n+   */\n+  private def registerMetrics(): Unit = {\n+    metricRegistry.register(\"eventsProcessed\", _eventsProcessed)\n+    metricRegistry.register(\"eventsQueued\", _eventsQueued)\n+    metricRegistry.register(\"entityPostAttempts\", _entityPostAttempts)\n+    metricRegistry.register(\"entityPostSuccesses\", _entityPostSuccesses)\n+    metricRegistry.register(\"entityPostFailures\", _entityPostFailures)\n+    metricRegistry.register(\"entityPostRejections\", _entityPostRejections)\n+    metricRegistry.register(\"eventsDropped\", _eventsDropped)\n+    metricRegistry.register(\"flushCount\", flushCount)\n+  }\n+\n+  /**\n+   * Return a summary of the service state to help diagnose problems\n+   * during test runs, possibly even production.\n+   *\n+   * @return a summary of the current service state\n+   */\n+  override def toString(): String = {"
  }],
  "prId": 8744
}]