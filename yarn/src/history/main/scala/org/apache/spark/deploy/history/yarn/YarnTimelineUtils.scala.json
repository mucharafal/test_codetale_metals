[{
  "comments": [{
    "author": {
      "login": "jerryshao"
    },
    "body": "this can be simplified as `Option(en.getOtherInfo.get(name))`.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-10-28T06:32:53Z",
    "diffHunk": "@@ -0,0 +1,757 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.IOException\n+import java.net.{InetSocketAddress, NoRouteToHostException, URI, URL}\n+import java.text.DateFormat\n+import java.util.concurrent.atomic.AtomicLong\n+import java.util.{ArrayList => JArrayList, Collection => JCollection, Date, HashMap => JHashMap, Map => JMap}\n+import java.{lang, util}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse.TimelinePutError\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.apache.spark\n+import org.json4s.JsonAST.JObject\n+import org.json4s._\n+import org.json4s.jackson.JsonMethods._\n+\n+import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+import org.apache.spark.scheduler.{SparkListenerApplicationEnd, SparkListenerApplicationStart, SparkListenerEvent, SparkListenerExecutorAdded, SparkListenerExecutorRemoved, SparkListenerJobEnd, SparkListenerJobStart, SparkListenerStageCompleted, SparkListenerStageSubmitted}\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Utility methods for timeline classes.\n+ */\n+private[spark] object YarnTimelineUtils extends Logging {\n+\n+  /**\n+   * What attempt ID to use as the attempt ID field (not the entity ID) when\n+   * there is no attempt info.\n+   */\n+  val SINGLE_ATTEMPT = \"1\"\n+\n+  /**\n+   * Exception text when there is no event info data to unmarshall.\n+   */\n+  val E_NO_EVENTINFO = \"No 'eventinfo' entry\"\n+\n+  /**\n+   * Exception text when there is event info entry in the timeline event, but it is empty.\n+   */\n+\n+  val E_EMPTY_EVENTINFO = \"Empty 'eventinfo' entry\"\n+\n+  /**\n+   * counter incremented on every spark event to timeline event creation,\n+   * so guaranteeing uniqueness of event IDs across a single application attempt\n+   * (which is implicitly, one per JVM).\n+   */\n+  val uid = new AtomicLong(System.currentTimeMillis())\n+\n+  /**\n+   * Converts a Java object to its equivalent json4s representation.\n+   */\n+  def toJValue(obj: Object): JValue = {\n+    obj match {\n+      case str: String => JString(str)\n+      case dbl: java.lang.Double => JDouble(dbl)\n+      case dec: java.math.BigDecimal => JDecimal(dec)\n+      case int: java.lang.Integer => JInt(BigInt(int))\n+      case long: java.lang.Long => JInt(BigInt(long))\n+      case bool: java.lang.Boolean => JBool(bool)\n+      case map: JMap[_, _] =>\n+        val jmap = map.asInstanceOf[JMap[String, Object]]\n+        JObject(jmap.entrySet().asScala.map { e => e.getKey -> toJValue(e.getValue) }.toList)\n+      case array: JCollection[_] =>\n+        JArray(array.asInstanceOf[JCollection[Object]].asScala.map(o => toJValue(o)).toList)\n+      case null => JNothing\n+    }\n+  }\n+\n+  /**\n+   * Converts a JValue into its Java equivalent.\n+   */\n+  def toJavaObject(v: JValue): Object = {\n+    v match {\n+      case JNothing => null\n+      case JNull => null\n+      case JString(s) => s\n+      case JDouble(num) => java.lang.Double.valueOf(num)\n+      case JDecimal(num) => num.bigDecimal\n+      case JInt(num) => java.lang.Long.valueOf(num.longValue())\n+      case JBool(value) => java.lang.Boolean.valueOf(value)\n+      case obj: JObject => toJavaMap(obj)\n+      case JArray(vals) =>\n+        val list = new JArrayList[Object]()\n+        vals.foreach(x => list.add(toJavaObject(x)))\n+        list\n+    }\n+  }\n+\n+  /**\n+   * Converts a json4s list of fields into a Java Map suitable for serialization by Jackson,\n+   * which is used by the ATS client library.\n+   */\n+  def toJavaMap(sourceObj: JObject): JHashMap[String, Object] = {\n+    val map = new JHashMap[String, Object]()\n+    sourceObj.obj.foreach { case (k, v) => map.put(k, toJavaObject(v)) }\n+    map\n+  }\n+\n+  /**\n+   * Convert a timeline event to a spark one. Includes some basic checks for validity of\n+   * the event payload.\n+   * @param event timeline event\n+   * @return an unmarshalled event\n+   */\n+  def toSparkEvent(event: TimelineEvent): SparkListenerEvent = {\n+    val info = event.getEventInfo\n+    if (info == null) {\n+      throw new IOException(E_NO_EVENTINFO)\n+    }\n+    if (info.size() == 0) {\n+      throw new IOException(E_EMPTY_EVENTINFO)\n+    }\n+    val payload = toJValue(info)\n+    def jsonToString: String = {\n+      val json = compact(render(payload))\n+      val limit = 256\n+      if (json.length < limit) {\n+        json\n+      } else {\n+         json.substring(0, limit) + \" ... }\"\n+      }\n+    }\n+    logDebug(s\"toSparkEvent payload is $jsonToString\")\n+    val eventField = payload \\ \"Event\"\n+    if (eventField == JNothing) {\n+      throw new IOException(s\"No 'Event' entry in $jsonToString\")\n+    }\n+\n+    // now the real unmarshalling\n+    try {\n+      JsonProtocol.sparkEventFromJson(payload)\n+    } catch {\n+      // failure in the marshalling; include payload in the message\n+      case ex: MappingException =>\n+        logDebug(s\"$ex while rendering $jsonToString\", ex)\n+        throw ex\n+    }\n+  }\n+\n+  /**\n+   * Convert a spark event to a timeline event\n+   * @param event handled spark event\n+   * @return a timeline event\n+   */\n+  def toTimelineEvent(event: SparkListenerEvent, timestamp: Long): TimelineEvent = {\n+    val tlEvent = new TimelineEvent()\n+    tlEvent.setEventType(Utils.getFormattedClassName(event)\n+        + \"-\" + YarnTimelineUtils.uid.incrementAndGet.toString)\n+    tlEvent.setTimestamp(timestamp)\n+    val kvMap = new JHashMap[String, Object]()\n+    val json = JsonProtocol.sparkEventToJson(event)\n+    val jObject = json.asInstanceOf[JObject]\n+    // the timeline event wants a map of java objects for Jackson to serialize\n+    val hashMap = toJavaMap(jObject)\n+    tlEvent.setEventInfo(hashMap)\n+    tlEvent\n+  }\n+\n+  /**\n+   * Describe the event for logging.\n+   *\n+   * @param event timeline event\n+   * @return a description\n+   */\n+  def describeEvent(event: TimelineEvent): String = {\n+    val sparkEventDetails = try { {\n+      toSparkEvent(event).toString\n+    }\n+    } catch {\n+      case _: MappingException =>\n+       \"(cannot convert event details to spark exception)\"\n+    }\n+    s\"${event.getEventType()} @ ${new Date(event.getTimestamp())}\" +\n+      s\"\\n    $sparkEventDetails\"\n+  }\n+\n+  /**\n+   * Create details of a timeline entity, by describing every event inside it.\n+   *\n+   * @param entity entity containing a possibly empty or null list of events\n+   * @return a list of event details, with a newline between each one\n+   */\n+  def eventDetails(entity: TimelineEntity): String = {\n+    val events = entity.getEvents\n+    if (events != null) {\n+      events.asScala.map(describeEvent).mkString(\"\\n\")\n+    } else {\n+      \"\"\n+    }\n+  }\n+\n+  /**\n+   * Describe a timeline entity.\n+   * @param entity entity\n+   * @return a string description.\n+   */\n+  def describeEntity(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    val eventSummary = if (events != null) {\n+      s\"contains ${events.size()} event(s)\"\n+    } else {\n+      \"contains no events\"\n+    }\n+\n+    val domain = if (entity.getDomainId != null) s\" Domain ${entity.getDomainId}\" else \"\"\n+    val header = s\"${entity.getEntityType}/${entity.getEntityId} $domain\"\n+    try {\n+      events.asScala.map(describeEvent).mkString(\"\\n\")\n+      val otherInfo = entity.getOtherInfo.asScala.map {\n+        case (k, v) => s\" $k = $v;\"\n+      }.mkString(\"\\n\")\n+      s\"Timeline Entity \" + header +\n+          \" \" + otherInfo + \"\\n\" +\n+          \" started: \" + timeFieldToString(entity.getStartTime, \"start\") + \"\\n\" +\n+          \" \" + eventSummary\n+    } catch {\n+      case e: MappingException =>\n+        // failure to marshall/unmarshall; downgrade\n+        s\"Timeline Entity $header\"\n+    }\n+  }\n+\n+  /**\n+   * Convert a `java.lang.Long` reference to a string value, or, if the reference is null,\n+   * to text declaring that the named field is empty.\n+   *\n+   * @param time time reference\n+   * @param field field name for error message\n+   * @return a string to describe the field\n+   */\n+  def timeFieldToString(time: lang.Long, field: String): String = {\n+    if (time != null) {\n+      new Date(time).toString\n+    } else {\n+      s\"no $field time\"\n+    }\n+  }\n+\n+  /**\n+   * A verbose description of the entity which contains event details and info about\n+   * primary/secondary keys.\n+   *\n+   * @param entity timeline entity\n+   * @return a verbose description of the field\n+   */\n+  def describeEntityVerbose(entity: TimelineEntity): String = {\n+    val header = describeEntity(entity)\n+    val primaryFilters = entity.getPrimaryFilters.asScala.toMap\n+    var filterElements = \"\"\n+    for ((k, v) <- primaryFilters) {\n+      filterElements = filterElements +\n+        \" filter \" + k + \": [ \" + v.asScala.foldLeft(\"\")((s, o) => s + o.toString + \" \") + \"]\\n\"\n+    }\n+    val events = eventDetails(entity)\n+    header + \"\\n\" + filterElements + events\n+  }\n+\n+  /**\n+   * Split a comma separated String, filter out any empty items, and return a `Set` of strings.\n+   */\n+  def stringToSet(list: String): Set[String] = {\n+    list.split(',').map(_.trim).filter(!_.isEmpty).toSet\n+  }\n+\n+  /**\n+   * Try to get the event time off an event. Not all events have the required information.\n+   *\n+   * @param event event to process\n+   * @return the event time\n+   */\n+  def eventTime(event: SparkListenerEvent): Option[Long] = {\n+    event match {\n+      case evt: SparkListenerApplicationStart =>\n+        Some(evt.time)\n+      case evt: SparkListenerApplicationEnd =>\n+        Some(evt.time)\n+      case evt: SparkListenerJobStart =>\n+        Some(evt.time)\n+      case evt: SparkListenerJobEnd =>\n+        Some(evt.time)\n+      case evt: SparkListenerExecutorAdded =>\n+        Some(evt.time)\n+      case evt: SparkListenerExecutorRemoved =>\n+        Some(evt.time)\n+      case evt: SparkListenerStageSubmitted =>\n+        evt.stageInfo.submissionTime\n+      case evt: SparkListenerStageCompleted =>\n+        evt.stageInfo.completionTime\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+   * Create and start a timeline client, using the configuration context to\n+   * set up the binding.\n+   *\n+   * @param sparkContext spark context\n+   * @return the started instance\n+   */\n+  def createTimelineClient(sparkContext: SparkContext): TimelineClient = {\n+    val client = TimelineClient.createTimelineClient\n+    client.init(sparkContext.hadoopConfiguration)\n+    client.start()\n+    client\n+  }\n+\n+  /**\n+   * The path for the V1 ATS REST API.\n+   */\n+  val TIMELINE_REST_PATH = s\"/ws/v1/timeline/\"\n+\n+  /**\n+   * Build the URI to the base of the timeline web application\n+   * from the Hadoop context.\n+   *\n+   * Raises an exception if the address cannot be determined or is considered invalid from\n+   * a networking perspective.\n+   *\n+   * Does not perform any checks as to whether or not the timeline service is enabled\n+   * @param conf configuration\n+   * @return the URI to the timeline service.\n+   */\n+  def getTimelineEndpoint(conf: Configuration): URI = {\n+    val isHttps = YarnConfiguration.useHttps(conf)\n+    val address = if (isHttps) {\n+      conf.get(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS,\n+                YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS)\n+    } else {\n+      conf.get(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_ADDRESS,\n+                YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_ADDRESS)\n+    }\n+    val protocol = if (isHttps) \"https://\" else \"http://\"\n+    require(address != null, s\"No timeline service defined\")\n+    validateEndpoint(URI.create(s\"$protocol$address$TIMELINE_REST_PATH\"))\n+  }\n+\n+  /**\n+   * Create a URI to the history service. This uses the entity type of\n+   * [[YarnHistoryService#ENTITY_TYPE]] for spark application histories.\n+   * @param conf hadoop configuration to examine\n+   * @return\n+   */\n+  def timelineWebappUri(conf: Configuration): URI = {\n+    timelineWebappUri(conf, YarnHistoryService.SPARK_EVENT_ENTITY_TYPE)\n+  }\n+\n+  /**\n+   * Get the URI of a path under the timeline web UI.\n+   *\n+   * @param conf configuration\n+   * @param subpath path under the root web UI\n+   * @return a URI\n+   */\n+  def timelineWebappUri(conf: Configuration, subpath: String): URI = {\n+    val base = getTimelineEndpoint(conf)\n+    new URL(base.toURL, subpath).toURI\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled.\n+   *\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED` is set.\n+   */\n+  def timelineServiceEnabled(conf: Configuration): Boolean = {\n+    conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED,\n+                    YarnConfiguration.DEFAULT_TIMELINE_SERVICE_ENABLED)\n+  }\n+\n+  /**\n+   * Get the URI to an application under the timeline\n+   * (this requires the applicationID to have been used to\n+   * publish entities there)\n+   * @param timelineUri timeline URI\n+   * @param appId App ID (really, the entityId used to publish)\n+   * @return the path\n+   */\n+  def applicationURI(timelineUri: URI, appId: String): URI = {\n+    require(appId != null && !appId.isEmpty, \"No application ID\")\n+    require(!appId.contains(\"/\"), s\"Illegal character '/' in $appId\")\n+    timelineUri.resolve(s\"${timelineUri.getPath()}/$appId\")\n+  }\n+\n+  /**\n+   * Map an error code to a string. For known codes, it returns\n+   * a description; for others it just returns the error code.\n+   *\n+   * @param code error code\n+   * @return a string description for error messages\n+   */\n+  def timelineErrorCodeToString(code: Int): String = {\n+    code match {\n+      case 0 => \"0: no error\"\n+      case 1 => \"No start time\"\n+      case 2 => \"IO Exception\"\n+      case 3 => \"System Filter Conflict\"\n+      case 4 => \"Access Denied\"\n+      case 5 => \"No Domain\"\n+      case 6 => \"Forbidden Relation\"\n+      case other: Int => s\"Error code $other\"\n+    }\n+  }\n+\n+  /**\n+   * Convert a timeline error response to a slightly more meaningful string.\n+   * @param error error\n+   * @return text for diagnostics\n+   */\n+  def describeError(error: TimelinePutError): String = {\n+    s\"Entity ID=${error.getEntityId()}; Entity type=${error.getEntityType}\" +\n+    s\" Error code ${error.getErrorCode}\" +\n+    s\": ${timelineErrorCodeToString(error.getErrorCode)}\"\n+  }\n+\n+  /**\n+   * Describe a put response by enumerating and describing all errors.\n+   * (if present. A null `errors` element is handled robustly).\n+   *\n+   * @param response response to describe\n+   * @return text for diagnostics\n+   */\n+  def describePutResponse(response: TimelinePutResponse) : String = {\n+    val responseErrs = response.getErrors\n+    if (responseErrs != null) {\n+      val errors = mutable.MutableList(s\"TimelinePutResponse with ${responseErrs.size()} errors\")\n+      for (err <- responseErrs.asScala) {\n+        errors += describeError(err)\n+      }\n+      errors.foldLeft(\"\")((buff, elt) => buff + \"\\n\" + elt)\n+    } else {\n+      s\"TimelinePutResponse with null error list\"\n+    }\n+  }\n+\n+  /**\n+   * This is used to highlight an undefined field.\n+   */\n+  val UNDEFINED_FIELD = \"Undefined\"\n+\n+  /**\n+   * Lookup a field in the `otherInfo` section of a [[TimelineEntity]].\n+   *\n+   * @param en entity\n+   * @param name field name\n+   * @return the value or the string [[UNDEFINED_FIELD]] if not\n+   * @throws Exception if the field is not found\n+   */\n+  def field(en: TimelineEntity, name: String) : Object = {\n+    fieldOption(en, name) match {\n+      case Some(v) => v\n+      case None => UNDEFINED_FIELD\n+    }\n+  }\n+\n+  /**\n+   * Lookup a field in the `otherInfo` section of a [[TimelineEntity]].\n+   *\n+   * @param en entity\n+   * @param name field name\n+   * @return the value\n+   * @throws Exception if the field is not found\n+   */\n+  def fieldOption(en: TimelineEntity, name: String) : Option[Object] = {\n+    Option.apply(en.getOtherInfo.get(name))"
  }],
  "prId": 8744
}, {
  "comments": [{
    "author": {
      "login": "jerryshao"
    },
    "body": "Here can be `fieldOption(en, name).getOrElse(UNDEFINED_FIELD)`.\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-10-28T06:33:50Z",
    "diffHunk": "@@ -0,0 +1,757 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.IOException\n+import java.net.{InetSocketAddress, NoRouteToHostException, URI, URL}\n+import java.text.DateFormat\n+import java.util.concurrent.atomic.AtomicLong\n+import java.util.{ArrayList => JArrayList, Collection => JCollection, Date, HashMap => JHashMap, Map => JMap}\n+import java.{lang, util}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse.TimelinePutError\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.apache.spark\n+import org.json4s.JsonAST.JObject\n+import org.json4s._\n+import org.json4s.jackson.JsonMethods._\n+\n+import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+import org.apache.spark.scheduler.{SparkListenerApplicationEnd, SparkListenerApplicationStart, SparkListenerEvent, SparkListenerExecutorAdded, SparkListenerExecutorRemoved, SparkListenerJobEnd, SparkListenerJobStart, SparkListenerStageCompleted, SparkListenerStageSubmitted}\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Utility methods for timeline classes.\n+ */\n+private[spark] object YarnTimelineUtils extends Logging {\n+\n+  /**\n+   * What attempt ID to use as the attempt ID field (not the entity ID) when\n+   * there is no attempt info.\n+   */\n+  val SINGLE_ATTEMPT = \"1\"\n+\n+  /**\n+   * Exception text when there is no event info data to unmarshall.\n+   */\n+  val E_NO_EVENTINFO = \"No 'eventinfo' entry\"\n+\n+  /**\n+   * Exception text when there is event info entry in the timeline event, but it is empty.\n+   */\n+\n+  val E_EMPTY_EVENTINFO = \"Empty 'eventinfo' entry\"\n+\n+  /**\n+   * counter incremented on every spark event to timeline event creation,\n+   * so guaranteeing uniqueness of event IDs across a single application attempt\n+   * (which is implicitly, one per JVM).\n+   */\n+  val uid = new AtomicLong(System.currentTimeMillis())\n+\n+  /**\n+   * Converts a Java object to its equivalent json4s representation.\n+   */\n+  def toJValue(obj: Object): JValue = {\n+    obj match {\n+      case str: String => JString(str)\n+      case dbl: java.lang.Double => JDouble(dbl)\n+      case dec: java.math.BigDecimal => JDecimal(dec)\n+      case int: java.lang.Integer => JInt(BigInt(int))\n+      case long: java.lang.Long => JInt(BigInt(long))\n+      case bool: java.lang.Boolean => JBool(bool)\n+      case map: JMap[_, _] =>\n+        val jmap = map.asInstanceOf[JMap[String, Object]]\n+        JObject(jmap.entrySet().asScala.map { e => e.getKey -> toJValue(e.getValue) }.toList)\n+      case array: JCollection[_] =>\n+        JArray(array.asInstanceOf[JCollection[Object]].asScala.map(o => toJValue(o)).toList)\n+      case null => JNothing\n+    }\n+  }\n+\n+  /**\n+   * Converts a JValue into its Java equivalent.\n+   */\n+  def toJavaObject(v: JValue): Object = {\n+    v match {\n+      case JNothing => null\n+      case JNull => null\n+      case JString(s) => s\n+      case JDouble(num) => java.lang.Double.valueOf(num)\n+      case JDecimal(num) => num.bigDecimal\n+      case JInt(num) => java.lang.Long.valueOf(num.longValue())\n+      case JBool(value) => java.lang.Boolean.valueOf(value)\n+      case obj: JObject => toJavaMap(obj)\n+      case JArray(vals) =>\n+        val list = new JArrayList[Object]()\n+        vals.foreach(x => list.add(toJavaObject(x)))\n+        list\n+    }\n+  }\n+\n+  /**\n+   * Converts a json4s list of fields into a Java Map suitable for serialization by Jackson,\n+   * which is used by the ATS client library.\n+   */\n+  def toJavaMap(sourceObj: JObject): JHashMap[String, Object] = {\n+    val map = new JHashMap[String, Object]()\n+    sourceObj.obj.foreach { case (k, v) => map.put(k, toJavaObject(v)) }\n+    map\n+  }\n+\n+  /**\n+   * Convert a timeline event to a spark one. Includes some basic checks for validity of\n+   * the event payload.\n+   * @param event timeline event\n+   * @return an unmarshalled event\n+   */\n+  def toSparkEvent(event: TimelineEvent): SparkListenerEvent = {\n+    val info = event.getEventInfo\n+    if (info == null) {\n+      throw new IOException(E_NO_EVENTINFO)\n+    }\n+    if (info.size() == 0) {\n+      throw new IOException(E_EMPTY_EVENTINFO)\n+    }\n+    val payload = toJValue(info)\n+    def jsonToString: String = {\n+      val json = compact(render(payload))\n+      val limit = 256\n+      if (json.length < limit) {\n+        json\n+      } else {\n+         json.substring(0, limit) + \" ... }\"\n+      }\n+    }\n+    logDebug(s\"toSparkEvent payload is $jsonToString\")\n+    val eventField = payload \\ \"Event\"\n+    if (eventField == JNothing) {\n+      throw new IOException(s\"No 'Event' entry in $jsonToString\")\n+    }\n+\n+    // now the real unmarshalling\n+    try {\n+      JsonProtocol.sparkEventFromJson(payload)\n+    } catch {\n+      // failure in the marshalling; include payload in the message\n+      case ex: MappingException =>\n+        logDebug(s\"$ex while rendering $jsonToString\", ex)\n+        throw ex\n+    }\n+  }\n+\n+  /**\n+   * Convert a spark event to a timeline event\n+   * @param event handled spark event\n+   * @return a timeline event\n+   */\n+  def toTimelineEvent(event: SparkListenerEvent, timestamp: Long): TimelineEvent = {\n+    val tlEvent = new TimelineEvent()\n+    tlEvent.setEventType(Utils.getFormattedClassName(event)\n+        + \"-\" + YarnTimelineUtils.uid.incrementAndGet.toString)\n+    tlEvent.setTimestamp(timestamp)\n+    val kvMap = new JHashMap[String, Object]()\n+    val json = JsonProtocol.sparkEventToJson(event)\n+    val jObject = json.asInstanceOf[JObject]\n+    // the timeline event wants a map of java objects for Jackson to serialize\n+    val hashMap = toJavaMap(jObject)\n+    tlEvent.setEventInfo(hashMap)\n+    tlEvent\n+  }\n+\n+  /**\n+   * Describe the event for logging.\n+   *\n+   * @param event timeline event\n+   * @return a description\n+   */\n+  def describeEvent(event: TimelineEvent): String = {\n+    val sparkEventDetails = try { {\n+      toSparkEvent(event).toString\n+    }\n+    } catch {\n+      case _: MappingException =>\n+       \"(cannot convert event details to spark exception)\"\n+    }\n+    s\"${event.getEventType()} @ ${new Date(event.getTimestamp())}\" +\n+      s\"\\n    $sparkEventDetails\"\n+  }\n+\n+  /**\n+   * Create details of a timeline entity, by describing every event inside it.\n+   *\n+   * @param entity entity containing a possibly empty or null list of events\n+   * @return a list of event details, with a newline between each one\n+   */\n+  def eventDetails(entity: TimelineEntity): String = {\n+    val events = entity.getEvents\n+    if (events != null) {\n+      events.asScala.map(describeEvent).mkString(\"\\n\")\n+    } else {\n+      \"\"\n+    }\n+  }\n+\n+  /**\n+   * Describe a timeline entity.\n+   * @param entity entity\n+   * @return a string description.\n+   */\n+  def describeEntity(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    val eventSummary = if (events != null) {\n+      s\"contains ${events.size()} event(s)\"\n+    } else {\n+      \"contains no events\"\n+    }\n+\n+    val domain = if (entity.getDomainId != null) s\" Domain ${entity.getDomainId}\" else \"\"\n+    val header = s\"${entity.getEntityType}/${entity.getEntityId} $domain\"\n+    try {\n+      events.asScala.map(describeEvent).mkString(\"\\n\")\n+      val otherInfo = entity.getOtherInfo.asScala.map {\n+        case (k, v) => s\" $k = $v;\"\n+      }.mkString(\"\\n\")\n+      s\"Timeline Entity \" + header +\n+          \" \" + otherInfo + \"\\n\" +\n+          \" started: \" + timeFieldToString(entity.getStartTime, \"start\") + \"\\n\" +\n+          \" \" + eventSummary\n+    } catch {\n+      case e: MappingException =>\n+        // failure to marshall/unmarshall; downgrade\n+        s\"Timeline Entity $header\"\n+    }\n+  }\n+\n+  /**\n+   * Convert a `java.lang.Long` reference to a string value, or, if the reference is null,\n+   * to text declaring that the named field is empty.\n+   *\n+   * @param time time reference\n+   * @param field field name for error message\n+   * @return a string to describe the field\n+   */\n+  def timeFieldToString(time: lang.Long, field: String): String = {\n+    if (time != null) {\n+      new Date(time).toString\n+    } else {\n+      s\"no $field time\"\n+    }\n+  }\n+\n+  /**\n+   * A verbose description of the entity which contains event details and info about\n+   * primary/secondary keys.\n+   *\n+   * @param entity timeline entity\n+   * @return a verbose description of the field\n+   */\n+  def describeEntityVerbose(entity: TimelineEntity): String = {\n+    val header = describeEntity(entity)\n+    val primaryFilters = entity.getPrimaryFilters.asScala.toMap\n+    var filterElements = \"\"\n+    for ((k, v) <- primaryFilters) {\n+      filterElements = filterElements +\n+        \" filter \" + k + \": [ \" + v.asScala.foldLeft(\"\")((s, o) => s + o.toString + \" \") + \"]\\n\"\n+    }\n+    val events = eventDetails(entity)\n+    header + \"\\n\" + filterElements + events\n+  }\n+\n+  /**\n+   * Split a comma separated String, filter out any empty items, and return a `Set` of strings.\n+   */\n+  def stringToSet(list: String): Set[String] = {\n+    list.split(',').map(_.trim).filter(!_.isEmpty).toSet\n+  }\n+\n+  /**\n+   * Try to get the event time off an event. Not all events have the required information.\n+   *\n+   * @param event event to process\n+   * @return the event time\n+   */\n+  def eventTime(event: SparkListenerEvent): Option[Long] = {\n+    event match {\n+      case evt: SparkListenerApplicationStart =>\n+        Some(evt.time)\n+      case evt: SparkListenerApplicationEnd =>\n+        Some(evt.time)\n+      case evt: SparkListenerJobStart =>\n+        Some(evt.time)\n+      case evt: SparkListenerJobEnd =>\n+        Some(evt.time)\n+      case evt: SparkListenerExecutorAdded =>\n+        Some(evt.time)\n+      case evt: SparkListenerExecutorRemoved =>\n+        Some(evt.time)\n+      case evt: SparkListenerStageSubmitted =>\n+        evt.stageInfo.submissionTime\n+      case evt: SparkListenerStageCompleted =>\n+        evt.stageInfo.completionTime\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+   * Create and start a timeline client, using the configuration context to\n+   * set up the binding.\n+   *\n+   * @param sparkContext spark context\n+   * @return the started instance\n+   */\n+  def createTimelineClient(sparkContext: SparkContext): TimelineClient = {\n+    val client = TimelineClient.createTimelineClient\n+    client.init(sparkContext.hadoopConfiguration)\n+    client.start()\n+    client\n+  }\n+\n+  /**\n+   * The path for the V1 ATS REST API.\n+   */\n+  val TIMELINE_REST_PATH = s\"/ws/v1/timeline/\"\n+\n+  /**\n+   * Build the URI to the base of the timeline web application\n+   * from the Hadoop context.\n+   *\n+   * Raises an exception if the address cannot be determined or is considered invalid from\n+   * a networking perspective.\n+   *\n+   * Does not perform any checks as to whether or not the timeline service is enabled\n+   * @param conf configuration\n+   * @return the URI to the timeline service.\n+   */\n+  def getTimelineEndpoint(conf: Configuration): URI = {\n+    val isHttps = YarnConfiguration.useHttps(conf)\n+    val address = if (isHttps) {\n+      conf.get(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS,\n+                YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS)\n+    } else {\n+      conf.get(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_ADDRESS,\n+                YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_ADDRESS)\n+    }\n+    val protocol = if (isHttps) \"https://\" else \"http://\"\n+    require(address != null, s\"No timeline service defined\")\n+    validateEndpoint(URI.create(s\"$protocol$address$TIMELINE_REST_PATH\"))\n+  }\n+\n+  /**\n+   * Create a URI to the history service. This uses the entity type of\n+   * [[YarnHistoryService#ENTITY_TYPE]] for spark application histories.\n+   * @param conf hadoop configuration to examine\n+   * @return\n+   */\n+  def timelineWebappUri(conf: Configuration): URI = {\n+    timelineWebappUri(conf, YarnHistoryService.SPARK_EVENT_ENTITY_TYPE)\n+  }\n+\n+  /**\n+   * Get the URI of a path under the timeline web UI.\n+   *\n+   * @param conf configuration\n+   * @param subpath path under the root web UI\n+   * @return a URI\n+   */\n+  def timelineWebappUri(conf: Configuration, subpath: String): URI = {\n+    val base = getTimelineEndpoint(conf)\n+    new URL(base.toURL, subpath).toURI\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled.\n+   *\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED` is set.\n+   */\n+  def timelineServiceEnabled(conf: Configuration): Boolean = {\n+    conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED,\n+                    YarnConfiguration.DEFAULT_TIMELINE_SERVICE_ENABLED)\n+  }\n+\n+  /**\n+   * Get the URI to an application under the timeline\n+   * (this requires the applicationID to have been used to\n+   * publish entities there)\n+   * @param timelineUri timeline URI\n+   * @param appId App ID (really, the entityId used to publish)\n+   * @return the path\n+   */\n+  def applicationURI(timelineUri: URI, appId: String): URI = {\n+    require(appId != null && !appId.isEmpty, \"No application ID\")\n+    require(!appId.contains(\"/\"), s\"Illegal character '/' in $appId\")\n+    timelineUri.resolve(s\"${timelineUri.getPath()}/$appId\")\n+  }\n+\n+  /**\n+   * Map an error code to a string. For known codes, it returns\n+   * a description; for others it just returns the error code.\n+   *\n+   * @param code error code\n+   * @return a string description for error messages\n+   */\n+  def timelineErrorCodeToString(code: Int): String = {\n+    code match {\n+      case 0 => \"0: no error\"\n+      case 1 => \"No start time\"\n+      case 2 => \"IO Exception\"\n+      case 3 => \"System Filter Conflict\"\n+      case 4 => \"Access Denied\"\n+      case 5 => \"No Domain\"\n+      case 6 => \"Forbidden Relation\"\n+      case other: Int => s\"Error code $other\"\n+    }\n+  }\n+\n+  /**\n+   * Convert a timeline error response to a slightly more meaningful string.\n+   * @param error error\n+   * @return text for diagnostics\n+   */\n+  def describeError(error: TimelinePutError): String = {\n+    s\"Entity ID=${error.getEntityId()}; Entity type=${error.getEntityType}\" +\n+    s\" Error code ${error.getErrorCode}\" +\n+    s\": ${timelineErrorCodeToString(error.getErrorCode)}\"\n+  }\n+\n+  /**\n+   * Describe a put response by enumerating and describing all errors.\n+   * (if present. A null `errors` element is handled robustly).\n+   *\n+   * @param response response to describe\n+   * @return text for diagnostics\n+   */\n+  def describePutResponse(response: TimelinePutResponse) : String = {\n+    val responseErrs = response.getErrors\n+    if (responseErrs != null) {\n+      val errors = mutable.MutableList(s\"TimelinePutResponse with ${responseErrs.size()} errors\")\n+      for (err <- responseErrs.asScala) {\n+        errors += describeError(err)\n+      }\n+      errors.foldLeft(\"\")((buff, elt) => buff + \"\\n\" + elt)\n+    } else {\n+      s\"TimelinePutResponse with null error list\"\n+    }\n+  }\n+\n+  /**\n+   * This is used to highlight an undefined field.\n+   */\n+  val UNDEFINED_FIELD = \"Undefined\"\n+\n+  /**\n+   * Lookup a field in the `otherInfo` section of a [[TimelineEntity]].\n+   *\n+   * @param en entity\n+   * @param name field name\n+   * @return the value or the string [[UNDEFINED_FIELD]] if not\n+   * @throws Exception if the field is not found\n+   */\n+  def field(en: TimelineEntity, name: String) : Object = {\n+    fieldOption(en, name) match {\n+      case Some(v) => v\n+      case None => UNDEFINED_FIELD\n+    }"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "done\n",
    "commit": "99161a6a415d324ddfb1f4cfd0d066da58d4a2a8",
    "createdAt": "2015-10-29T14:09:03Z",
    "diffHunk": "@@ -0,0 +1,757 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.history.yarn\n+\n+import java.io.IOException\n+import java.net.{InetSocketAddress, NoRouteToHostException, URI, URL}\n+import java.text.DateFormat\n+import java.util.concurrent.atomic.AtomicLong\n+import java.util.{ArrayList => JArrayList, Collection => JCollection, Date, HashMap => JHashMap, Map => JMap}\n+import java.{lang, util}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse.TimelinePutError\n+import org.apache.hadoop.yarn.api.records.timeline.{TimelineEntity, TimelineEvent, TimelinePutResponse}\n+import org.apache.hadoop.yarn.api.records.{ApplicationAttemptId, ApplicationId}\n+import org.apache.hadoop.yarn.client.api.TimelineClient\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.apache.spark\n+import org.json4s.JsonAST.JObject\n+import org.json4s._\n+import org.json4s.jackson.JsonMethods._\n+\n+import org.apache.spark.deploy.history.yarn.YarnHistoryService._\n+import org.apache.spark.scheduler.{SparkListenerApplicationEnd, SparkListenerApplicationStart, SparkListenerEvent, SparkListenerExecutorAdded, SparkListenerExecutorRemoved, SparkListenerJobEnd, SparkListenerJobStart, SparkListenerStageCompleted, SparkListenerStageSubmitted}\n+import org.apache.spark.util.{JsonProtocol, Utils}\n+import org.apache.spark.{Logging, SparkContext}\n+\n+/**\n+ * Utility methods for timeline classes.\n+ */\n+private[spark] object YarnTimelineUtils extends Logging {\n+\n+  /**\n+   * What attempt ID to use as the attempt ID field (not the entity ID) when\n+   * there is no attempt info.\n+   */\n+  val SINGLE_ATTEMPT = \"1\"\n+\n+  /**\n+   * Exception text when there is no event info data to unmarshall.\n+   */\n+  val E_NO_EVENTINFO = \"No 'eventinfo' entry\"\n+\n+  /**\n+   * Exception text when there is event info entry in the timeline event, but it is empty.\n+   */\n+\n+  val E_EMPTY_EVENTINFO = \"Empty 'eventinfo' entry\"\n+\n+  /**\n+   * counter incremented on every spark event to timeline event creation,\n+   * so guaranteeing uniqueness of event IDs across a single application attempt\n+   * (which is implicitly, one per JVM).\n+   */\n+  val uid = new AtomicLong(System.currentTimeMillis())\n+\n+  /**\n+   * Converts a Java object to its equivalent json4s representation.\n+   */\n+  def toJValue(obj: Object): JValue = {\n+    obj match {\n+      case str: String => JString(str)\n+      case dbl: java.lang.Double => JDouble(dbl)\n+      case dec: java.math.BigDecimal => JDecimal(dec)\n+      case int: java.lang.Integer => JInt(BigInt(int))\n+      case long: java.lang.Long => JInt(BigInt(long))\n+      case bool: java.lang.Boolean => JBool(bool)\n+      case map: JMap[_, _] =>\n+        val jmap = map.asInstanceOf[JMap[String, Object]]\n+        JObject(jmap.entrySet().asScala.map { e => e.getKey -> toJValue(e.getValue) }.toList)\n+      case array: JCollection[_] =>\n+        JArray(array.asInstanceOf[JCollection[Object]].asScala.map(o => toJValue(o)).toList)\n+      case null => JNothing\n+    }\n+  }\n+\n+  /**\n+   * Converts a JValue into its Java equivalent.\n+   */\n+  def toJavaObject(v: JValue): Object = {\n+    v match {\n+      case JNothing => null\n+      case JNull => null\n+      case JString(s) => s\n+      case JDouble(num) => java.lang.Double.valueOf(num)\n+      case JDecimal(num) => num.bigDecimal\n+      case JInt(num) => java.lang.Long.valueOf(num.longValue())\n+      case JBool(value) => java.lang.Boolean.valueOf(value)\n+      case obj: JObject => toJavaMap(obj)\n+      case JArray(vals) =>\n+        val list = new JArrayList[Object]()\n+        vals.foreach(x => list.add(toJavaObject(x)))\n+        list\n+    }\n+  }\n+\n+  /**\n+   * Converts a json4s list of fields into a Java Map suitable for serialization by Jackson,\n+   * which is used by the ATS client library.\n+   */\n+  def toJavaMap(sourceObj: JObject): JHashMap[String, Object] = {\n+    val map = new JHashMap[String, Object]()\n+    sourceObj.obj.foreach { case (k, v) => map.put(k, toJavaObject(v)) }\n+    map\n+  }\n+\n+  /**\n+   * Convert a timeline event to a spark one. Includes some basic checks for validity of\n+   * the event payload.\n+   * @param event timeline event\n+   * @return an unmarshalled event\n+   */\n+  def toSparkEvent(event: TimelineEvent): SparkListenerEvent = {\n+    val info = event.getEventInfo\n+    if (info == null) {\n+      throw new IOException(E_NO_EVENTINFO)\n+    }\n+    if (info.size() == 0) {\n+      throw new IOException(E_EMPTY_EVENTINFO)\n+    }\n+    val payload = toJValue(info)\n+    def jsonToString: String = {\n+      val json = compact(render(payload))\n+      val limit = 256\n+      if (json.length < limit) {\n+        json\n+      } else {\n+         json.substring(0, limit) + \" ... }\"\n+      }\n+    }\n+    logDebug(s\"toSparkEvent payload is $jsonToString\")\n+    val eventField = payload \\ \"Event\"\n+    if (eventField == JNothing) {\n+      throw new IOException(s\"No 'Event' entry in $jsonToString\")\n+    }\n+\n+    // now the real unmarshalling\n+    try {\n+      JsonProtocol.sparkEventFromJson(payload)\n+    } catch {\n+      // failure in the marshalling; include payload in the message\n+      case ex: MappingException =>\n+        logDebug(s\"$ex while rendering $jsonToString\", ex)\n+        throw ex\n+    }\n+  }\n+\n+  /**\n+   * Convert a spark event to a timeline event\n+   * @param event handled spark event\n+   * @return a timeline event\n+   */\n+  def toTimelineEvent(event: SparkListenerEvent, timestamp: Long): TimelineEvent = {\n+    val tlEvent = new TimelineEvent()\n+    tlEvent.setEventType(Utils.getFormattedClassName(event)\n+        + \"-\" + YarnTimelineUtils.uid.incrementAndGet.toString)\n+    tlEvent.setTimestamp(timestamp)\n+    val kvMap = new JHashMap[String, Object]()\n+    val json = JsonProtocol.sparkEventToJson(event)\n+    val jObject = json.asInstanceOf[JObject]\n+    // the timeline event wants a map of java objects for Jackson to serialize\n+    val hashMap = toJavaMap(jObject)\n+    tlEvent.setEventInfo(hashMap)\n+    tlEvent\n+  }\n+\n+  /**\n+   * Describe the event for logging.\n+   *\n+   * @param event timeline event\n+   * @return a description\n+   */\n+  def describeEvent(event: TimelineEvent): String = {\n+    val sparkEventDetails = try { {\n+      toSparkEvent(event).toString\n+    }\n+    } catch {\n+      case _: MappingException =>\n+       \"(cannot convert event details to spark exception)\"\n+    }\n+    s\"${event.getEventType()} @ ${new Date(event.getTimestamp())}\" +\n+      s\"\\n    $sparkEventDetails\"\n+  }\n+\n+  /**\n+   * Create details of a timeline entity, by describing every event inside it.\n+   *\n+   * @param entity entity containing a possibly empty or null list of events\n+   * @return a list of event details, with a newline between each one\n+   */\n+  def eventDetails(entity: TimelineEntity): String = {\n+    val events = entity.getEvents\n+    if (events != null) {\n+      events.asScala.map(describeEvent).mkString(\"\\n\")\n+    } else {\n+      \"\"\n+    }\n+  }\n+\n+  /**\n+   * Describe a timeline entity.\n+   * @param entity entity\n+   * @return a string description.\n+   */\n+  def describeEntity(entity: TimelineEntity): String = {\n+    val events: util.List[TimelineEvent] = entity.getEvents\n+    val eventSummary = if (events != null) {\n+      s\"contains ${events.size()} event(s)\"\n+    } else {\n+      \"contains no events\"\n+    }\n+\n+    val domain = if (entity.getDomainId != null) s\" Domain ${entity.getDomainId}\" else \"\"\n+    val header = s\"${entity.getEntityType}/${entity.getEntityId} $domain\"\n+    try {\n+      events.asScala.map(describeEvent).mkString(\"\\n\")\n+      val otherInfo = entity.getOtherInfo.asScala.map {\n+        case (k, v) => s\" $k = $v;\"\n+      }.mkString(\"\\n\")\n+      s\"Timeline Entity \" + header +\n+          \" \" + otherInfo + \"\\n\" +\n+          \" started: \" + timeFieldToString(entity.getStartTime, \"start\") + \"\\n\" +\n+          \" \" + eventSummary\n+    } catch {\n+      case e: MappingException =>\n+        // failure to marshall/unmarshall; downgrade\n+        s\"Timeline Entity $header\"\n+    }\n+  }\n+\n+  /**\n+   * Convert a `java.lang.Long` reference to a string value, or, if the reference is null,\n+   * to text declaring that the named field is empty.\n+   *\n+   * @param time time reference\n+   * @param field field name for error message\n+   * @return a string to describe the field\n+   */\n+  def timeFieldToString(time: lang.Long, field: String): String = {\n+    if (time != null) {\n+      new Date(time).toString\n+    } else {\n+      s\"no $field time\"\n+    }\n+  }\n+\n+  /**\n+   * A verbose description of the entity which contains event details and info about\n+   * primary/secondary keys.\n+   *\n+   * @param entity timeline entity\n+   * @return a verbose description of the field\n+   */\n+  def describeEntityVerbose(entity: TimelineEntity): String = {\n+    val header = describeEntity(entity)\n+    val primaryFilters = entity.getPrimaryFilters.asScala.toMap\n+    var filterElements = \"\"\n+    for ((k, v) <- primaryFilters) {\n+      filterElements = filterElements +\n+        \" filter \" + k + \": [ \" + v.asScala.foldLeft(\"\")((s, o) => s + o.toString + \" \") + \"]\\n\"\n+    }\n+    val events = eventDetails(entity)\n+    header + \"\\n\" + filterElements + events\n+  }\n+\n+  /**\n+   * Split a comma separated String, filter out any empty items, and return a `Set` of strings.\n+   */\n+  def stringToSet(list: String): Set[String] = {\n+    list.split(',').map(_.trim).filter(!_.isEmpty).toSet\n+  }\n+\n+  /**\n+   * Try to get the event time off an event. Not all events have the required information.\n+   *\n+   * @param event event to process\n+   * @return the event time\n+   */\n+  def eventTime(event: SparkListenerEvent): Option[Long] = {\n+    event match {\n+      case evt: SparkListenerApplicationStart =>\n+        Some(evt.time)\n+      case evt: SparkListenerApplicationEnd =>\n+        Some(evt.time)\n+      case evt: SparkListenerJobStart =>\n+        Some(evt.time)\n+      case evt: SparkListenerJobEnd =>\n+        Some(evt.time)\n+      case evt: SparkListenerExecutorAdded =>\n+        Some(evt.time)\n+      case evt: SparkListenerExecutorRemoved =>\n+        Some(evt.time)\n+      case evt: SparkListenerStageSubmitted =>\n+        evt.stageInfo.submissionTime\n+      case evt: SparkListenerStageCompleted =>\n+        evt.stageInfo.completionTime\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+   * Create and start a timeline client, using the configuration context to\n+   * set up the binding.\n+   *\n+   * @param sparkContext spark context\n+   * @return the started instance\n+   */\n+  def createTimelineClient(sparkContext: SparkContext): TimelineClient = {\n+    val client = TimelineClient.createTimelineClient\n+    client.init(sparkContext.hadoopConfiguration)\n+    client.start()\n+    client\n+  }\n+\n+  /**\n+   * The path for the V1 ATS REST API.\n+   */\n+  val TIMELINE_REST_PATH = s\"/ws/v1/timeline/\"\n+\n+  /**\n+   * Build the URI to the base of the timeline web application\n+   * from the Hadoop context.\n+   *\n+   * Raises an exception if the address cannot be determined or is considered invalid from\n+   * a networking perspective.\n+   *\n+   * Does not perform any checks as to whether or not the timeline service is enabled\n+   * @param conf configuration\n+   * @return the URI to the timeline service.\n+   */\n+  def getTimelineEndpoint(conf: Configuration): URI = {\n+    val isHttps = YarnConfiguration.useHttps(conf)\n+    val address = if (isHttps) {\n+      conf.get(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS,\n+                YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_HTTPS_ADDRESS)\n+    } else {\n+      conf.get(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_ADDRESS,\n+                YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WEBAPP_ADDRESS)\n+    }\n+    val protocol = if (isHttps) \"https://\" else \"http://\"\n+    require(address != null, s\"No timeline service defined\")\n+    validateEndpoint(URI.create(s\"$protocol$address$TIMELINE_REST_PATH\"))\n+  }\n+\n+  /**\n+   * Create a URI to the history service. This uses the entity type of\n+   * [[YarnHistoryService#ENTITY_TYPE]] for spark application histories.\n+   * @param conf hadoop configuration to examine\n+   * @return\n+   */\n+  def timelineWebappUri(conf: Configuration): URI = {\n+    timelineWebappUri(conf, YarnHistoryService.SPARK_EVENT_ENTITY_TYPE)\n+  }\n+\n+  /**\n+   * Get the URI of a path under the timeline web UI.\n+   *\n+   * @param conf configuration\n+   * @param subpath path under the root web UI\n+   * @return a URI\n+   */\n+  def timelineWebappUri(conf: Configuration, subpath: String): URI = {\n+    val base = getTimelineEndpoint(conf)\n+    new URL(base.toURL, subpath).toURI\n+  }\n+\n+  /**\n+   * Check the service configuration to see if the timeline service is enabled.\n+   *\n+   * @return true if `YarnConfiguration.TIMELINE_SERVICE_ENABLED` is set.\n+   */\n+  def timelineServiceEnabled(conf: Configuration): Boolean = {\n+    conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED,\n+                    YarnConfiguration.DEFAULT_TIMELINE_SERVICE_ENABLED)\n+  }\n+\n+  /**\n+   * Get the URI to an application under the timeline\n+   * (this requires the applicationID to have been used to\n+   * publish entities there)\n+   * @param timelineUri timeline URI\n+   * @param appId App ID (really, the entityId used to publish)\n+   * @return the path\n+   */\n+  def applicationURI(timelineUri: URI, appId: String): URI = {\n+    require(appId != null && !appId.isEmpty, \"No application ID\")\n+    require(!appId.contains(\"/\"), s\"Illegal character '/' in $appId\")\n+    timelineUri.resolve(s\"${timelineUri.getPath()}/$appId\")\n+  }\n+\n+  /**\n+   * Map an error code to a string. For known codes, it returns\n+   * a description; for others it just returns the error code.\n+   *\n+   * @param code error code\n+   * @return a string description for error messages\n+   */\n+  def timelineErrorCodeToString(code: Int): String = {\n+    code match {\n+      case 0 => \"0: no error\"\n+      case 1 => \"No start time\"\n+      case 2 => \"IO Exception\"\n+      case 3 => \"System Filter Conflict\"\n+      case 4 => \"Access Denied\"\n+      case 5 => \"No Domain\"\n+      case 6 => \"Forbidden Relation\"\n+      case other: Int => s\"Error code $other\"\n+    }\n+  }\n+\n+  /**\n+   * Convert a timeline error response to a slightly more meaningful string.\n+   * @param error error\n+   * @return text for diagnostics\n+   */\n+  def describeError(error: TimelinePutError): String = {\n+    s\"Entity ID=${error.getEntityId()}; Entity type=${error.getEntityType}\" +\n+    s\" Error code ${error.getErrorCode}\" +\n+    s\": ${timelineErrorCodeToString(error.getErrorCode)}\"\n+  }\n+\n+  /**\n+   * Describe a put response by enumerating and describing all errors.\n+   * (if present. A null `errors` element is handled robustly).\n+   *\n+   * @param response response to describe\n+   * @return text for diagnostics\n+   */\n+  def describePutResponse(response: TimelinePutResponse) : String = {\n+    val responseErrs = response.getErrors\n+    if (responseErrs != null) {\n+      val errors = mutable.MutableList(s\"TimelinePutResponse with ${responseErrs.size()} errors\")\n+      for (err <- responseErrs.asScala) {\n+        errors += describeError(err)\n+      }\n+      errors.foldLeft(\"\")((buff, elt) => buff + \"\\n\" + elt)\n+    } else {\n+      s\"TimelinePutResponse with null error list\"\n+    }\n+  }\n+\n+  /**\n+   * This is used to highlight an undefined field.\n+   */\n+  val UNDEFINED_FIELD = \"Undefined\"\n+\n+  /**\n+   * Lookup a field in the `otherInfo` section of a [[TimelineEntity]].\n+   *\n+   * @param en entity\n+   * @param name field name\n+   * @return the value or the string [[UNDEFINED_FIELD]] if not\n+   * @throws Exception if the field is not found\n+   */\n+  def field(en: TimelineEntity, name: String) : Object = {\n+    fieldOption(en, name) match {\n+      case Some(v) => v\n+      case None => UNDEFINED_FIELD\n+    }"
  }],
  "prId": 8744
}]