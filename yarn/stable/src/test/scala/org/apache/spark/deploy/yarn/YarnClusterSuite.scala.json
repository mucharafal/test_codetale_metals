[{
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "Is this just making a copy of `sys.props`? Why not just do `toMap`?\n",
    "commit": "6d5b84e8b5987683591d8c07b3ff8557d9581871",
    "createdAt": "2014-09-12T20:19:55Z",
    "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import java.io.File\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.mutable.HashMap\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import org.scalatest.{BeforeAndAfterAll, FunSuite, Matchers}\n+\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.apache.hadoop.yarn.server.MiniYARNCluster\n+\n+import org.apache.spark.{Logging, SparkConf, SparkContext}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.util.Utils\n+\n+class YarnClusterSuite extends FunSuite with BeforeAndAfterAll with Matchers {\n+\n+  private val oldConf = new HashMap[String, String]()\n+  private var yarnCluster: MiniYARNCluster = _\n+  private var tempDir: File = _\n+  private var fakeSparkJar: File = _\n+\n+  override def beforeAll() {\n+    tempDir = Utils.createTempDir()\n+\n+    yarnCluster = new MiniYARNCluster(getClass().getName(), 1, 1, 1)\n+    yarnCluster.init(new YarnConfiguration())\n+    yarnCluster.start()\n+\n+    val sysProps = sys.props.map { case (k, v) => (k, v) }\n+    sysProps.foreach { case (k, v) =>"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "Actually, why not just take a snapshot of `sys.props` here and restore it later? Might be simpler\n",
    "commit": "6d5b84e8b5987683591d8c07b3ff8557d9581871",
    "createdAt": "2014-09-12T20:29:27Z",
    "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import java.io.File\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.mutable.HashMap\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import org.scalatest.{BeforeAndAfterAll, FunSuite, Matchers}\n+\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.apache.hadoop.yarn.server.MiniYARNCluster\n+\n+import org.apache.spark.{Logging, SparkConf, SparkContext}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.util.Utils\n+\n+class YarnClusterSuite extends FunSuite with BeforeAndAfterAll with Matchers {\n+\n+  private val oldConf = new HashMap[String, String]()\n+  private var yarnCluster: MiniYARNCluster = _\n+  private var tempDir: File = _\n+  private var fakeSparkJar: File = _\n+\n+  override def beforeAll() {\n+    tempDir = Utils.createTempDir()\n+\n+    yarnCluster = new MiniYARNCluster(getClass().getName(), 1, 1, 1)\n+    yarnCluster.init(new YarnConfiguration())\n+    yarnCluster.start()\n+\n+    val sysProps = sys.props.map { case (k, v) => (k, v) }\n+    sysProps.foreach { case (k, v) =>"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "I guess I was trying to be paranoid and clean up any \"spark.*\" options from sys.props before running these tests. That may not be the best idea (since I'd be cleaning up \"spark.test\" properties set by the build scripts), so I went with the copy instead.\n",
    "commit": "6d5b84e8b5987683591d8c07b3ff8557d9581871",
    "createdAt": "2014-09-15T17:52:44Z",
    "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import java.io.File\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.mutable.HashMap\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import org.scalatest.{BeforeAndAfterAll, FunSuite, Matchers}\n+\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.apache.hadoop.yarn.server.MiniYARNCluster\n+\n+import org.apache.spark.{Logging, SparkConf, SparkContext}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.util.Utils\n+\n+class YarnClusterSuite extends FunSuite with BeforeAndAfterAll with Matchers {\n+\n+  private val oldConf = new HashMap[String, String]()\n+  private var yarnCluster: MiniYARNCluster = _\n+  private var tempDir: File = _\n+  private var fakeSparkJar: File = _\n+\n+  override def beforeAll() {\n+    tempDir = Utils.createTempDir()\n+\n+    yarnCluster = new MiniYARNCluster(getClass().getName(), 1, 1, 1)\n+    yarnCluster.init(new YarnConfiguration())\n+    yarnCluster.start()\n+\n+    val sysProps = sys.props.map { case (k, v) => (k, v) }\n+    sysProps.foreach { case (k, v) =>"
  }],
  "prId": 2257
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "Nice. Can you also add backslashes\n",
    "commit": "6d5b84e8b5987683591d8c07b3ff8557d9581871",
    "createdAt": "2014-09-12T20:22:28Z",
    "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import java.io.File\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.mutable.HashMap\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import org.scalatest.{BeforeAndAfterAll, FunSuite, Matchers}\n+\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.apache.hadoop.yarn.server.MiniYARNCluster\n+\n+import org.apache.spark.{Logging, SparkConf, SparkContext}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.util.Utils\n+\n+class YarnClusterSuite extends FunSuite with BeforeAndAfterAll with Matchers {\n+\n+  private val oldConf = new HashMap[String, String]()\n+  private var yarnCluster: MiniYARNCluster = _\n+  private var tempDir: File = _\n+  private var fakeSparkJar: File = _\n+\n+  override def beforeAll() {\n+    tempDir = Utils.createTempDir()\n+\n+    yarnCluster = new MiniYARNCluster(getClass().getName(), 1, 1, 1)\n+    yarnCluster.init(new YarnConfiguration())\n+    yarnCluster.start()\n+\n+    val sysProps = sys.props.map { case (k, v) => (k, v) }\n+    sysProps.foreach { case (k, v) =>\n+      if (k.startsWith(\"spark.\")) {\n+        oldConf += (k -> v)\n+        sys.props -= k\n+      }\n+    }\n+\n+    yarnCluster.getConfig().foreach { e =>\n+      sys.props += (\"spark.hadoop.\" + e.getKey() -> e.getValue())\n+    }\n+\n+    fakeSparkJar = File.createTempFile(\"sparkJar\", null, tempDir)\n+    sys.props += (\"spark.yarn.jar\" -> (\"local:\" + fakeSparkJar.getAbsolutePath()))\n+    sys.props += (\"spark.executor.instances\" -> \"1\")\n+    sys.props += (\"spark.driver.extraClassPath\" -> sys.props(\"java.class.path\"))\n+    sys.props += (\"spark.executor.extraClassPath\" -> sys.props(\"java.class.path\"))\n+\n+    super.beforeAll()\n+  }\n+\n+  override def afterAll() {\n+    yarnCluster.stop()\n+\n+    val sysProps = sys.props.map { case (k, v) => (k, v) }\n+    sysProps.foreach { case (k, v) =>\n+      if (k.startsWith(\"spark.\")) {\n+        sys.props -= k\n+      }\n+    }\n+\n+    oldConf.foreach { case (k, v) => sys.props += (k -> v) }\n+\n+    super.afterAll()\n+  }\n+\n+  test(\"run Spark in yarn-client mode\") {\n+    var result = File.createTempFile(\"result\", null, tempDir)\n+    YarnClusterDriver.main(Array(\"yarn-client\", result.getAbsolutePath()))\n+    checkResult(result)\n+  }\n+\n+  test(\"run Spark in yarn-cluster mode\") {\n+    val main = YarnClusterDriver.getClass.getName().stripSuffix(\"$\")\n+    var result = File.createTempFile(\"result\", null, tempDir)\n+\n+    // The Client object will call System.exit() after the job is done, and we don't want\n+    // that because it messes up the scalatest monitoring. So replicate some of what main()\n+    // does here.\n+    val args = Array(\"--class\", main,\n+      \"--jar\", \"file:\" + fakeSparkJar.getAbsolutePath(),\n+      \"--arg\", \"yarn-cluster\",\n+      \"--arg\", result.getAbsolutePath(),\n+      \"--num-executors\", \"4\")\n+    val sparkConf = new SparkConf()\n+    val yarnConf = SparkHadoopUtil.get.newConfiguration(sparkConf)\n+    val clientArgs = new ClientArguments(args, sparkConf)\n+    new Client(clientArgs, yarnConf, sparkConf).run()\n+    checkResult(result)\n+  }\n+\n+  /**\n+   * This is a workaround for an issue with yarn-cluster mode: the Client class will not provide\n+   * any sort of error when the job process finishes successfully, but the job itself fails. So\n+   * the tests enforce that something is written to a file after everything is ok to indicate\n+   * that the job succeeded.\n+   */\n+  private def checkResult(result: File) = {\n+    var resultString = Files.toString(result, Charsets.UTF_8)\n+    resultString should be (\"success\")\n+  }\n+\n+}\n+\n+private object YarnClusterDriver extends Logging with Matchers {\n+\n+  def main(args: Array[String]) = {\n+    val sc = new SparkContext(new SparkConf().setMaster(args(0))\n+      .setAppName(\"yarn \\\"test app\\\" 'with quotes'\"))"
  }],
  "prId": 2257
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "This can just be `sys.props ++= oldConf` I think\n",
    "commit": "6d5b84e8b5987683591d8c07b3ff8557d9581871",
    "createdAt": "2014-09-12T20:26:30Z",
    "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import java.io.File\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.mutable.HashMap\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import org.scalatest.{BeforeAndAfterAll, FunSuite, Matchers}\n+\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.apache.hadoop.yarn.server.MiniYARNCluster\n+\n+import org.apache.spark.{Logging, SparkConf, SparkContext}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.util.Utils\n+\n+class YarnClusterSuite extends FunSuite with BeforeAndAfterAll with Matchers {\n+\n+  private val oldConf = new HashMap[String, String]()\n+  private var yarnCluster: MiniYARNCluster = _\n+  private var tempDir: File = _\n+  private var fakeSparkJar: File = _\n+\n+  override def beforeAll() {\n+    tempDir = Utils.createTempDir()\n+\n+    yarnCluster = new MiniYARNCluster(getClass().getName(), 1, 1, 1)\n+    yarnCluster.init(new YarnConfiguration())\n+    yarnCluster.start()\n+\n+    val sysProps = sys.props.map { case (k, v) => (k, v) }\n+    sysProps.foreach { case (k, v) =>\n+      if (k.startsWith(\"spark.\")) {\n+        oldConf += (k -> v)\n+        sys.props -= k\n+      }\n+    }\n+\n+    yarnCluster.getConfig().foreach { e =>\n+      sys.props += (\"spark.hadoop.\" + e.getKey() -> e.getValue())\n+    }\n+\n+    fakeSparkJar = File.createTempFile(\"sparkJar\", null, tempDir)\n+    sys.props += (\"spark.yarn.jar\" -> (\"local:\" + fakeSparkJar.getAbsolutePath()))\n+    sys.props += (\"spark.executor.instances\" -> \"1\")\n+    sys.props += (\"spark.driver.extraClassPath\" -> sys.props(\"java.class.path\"))\n+    sys.props += (\"spark.executor.extraClassPath\" -> sys.props(\"java.class.path\"))\n+\n+    super.beforeAll()\n+  }\n+\n+  override def afterAll() {\n+    yarnCluster.stop()\n+\n+    val sysProps = sys.props.map { case (k, v) => (k, v) }\n+    sysProps.foreach { case (k, v) =>\n+      if (k.startsWith(\"spark.\")) {\n+        sys.props -= k\n+      }\n+    }\n+\n+    oldConf.foreach { case (k, v) => sys.props += (k -> v) }"
  }],
  "prId": 2257
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "This whole chunk can be rewritten as:\n\n```\nsys.props.retain { case (k, _) => !k.startsWith(\"spark.\") }\n```\n",
    "commit": "6d5b84e8b5987683591d8c07b3ff8557d9581871",
    "createdAt": "2014-09-23T20:46:54Z",
    "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import java.io.File\n+\n+import scala.collection.JavaConversions._\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import org.scalatest.{BeforeAndAfterAll, FunSuite, Matchers}\n+\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.apache.hadoop.yarn.server.MiniYARNCluster\n+\n+import org.apache.spark.{Logging, SparkConf, SparkContext}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.util.Utils\n+\n+class YarnClusterSuite extends FunSuite with BeforeAndAfterAll with Matchers {\n+\n+  private var oldConf: Map[String, String] = _\n+  private var yarnCluster: MiniYARNCluster = _\n+  private var tempDir: File = _\n+  private var fakeSparkJar: File = _\n+\n+  override def beforeAll() {\n+    tempDir = Utils.createTempDir()\n+\n+    yarnCluster = new MiniYARNCluster(getClass().getName(), 1, 1, 1)\n+    yarnCluster.init(new YarnConfiguration())\n+    yarnCluster.start()\n+\n+    oldConf = sys.props.toMap\n+    yarnCluster.getConfig().foreach { e =>\n+      sys.props += (\"spark.hadoop.\" + e.getKey() -> e.getValue())\n+    }\n+\n+    fakeSparkJar = File.createTempFile(\"sparkJar\", null, tempDir)\n+    sys.props += (\"spark.yarn.jar\" -> (\"local:\" + fakeSparkJar.getAbsolutePath()))\n+    sys.props += (\"spark.executor.instances\" -> \"1\")\n+    sys.props += (\"spark.driver.extraClassPath\" -> sys.props(\"java.class.path\"))\n+    sys.props += (\"spark.executor.extraClassPath\" -> sys.props(\"java.class.path\"))\n+\n+    super.beforeAll()\n+  }\n+\n+  override def afterAll() {\n+    yarnCluster.stop()\n+\n+    val sysProps = sys.props.map { case (k, v) => (k, v) }\n+    sysProps.foreach { case (k, v) =>\n+      if (k.startsWith(\"spark.\")) {\n+        sys.props -= k\n+      }\n+    }"
  }],
  "prId": 2257
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "Does the map to identity here do anything?\n",
    "commit": "6d5b84e8b5987683591d8c07b3ff8557d9581871",
    "createdAt": "2014-09-23T20:49:16Z",
    "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import java.io.File\n+\n+import scala.collection.JavaConversions._\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import org.scalatest.{BeforeAndAfterAll, FunSuite, Matchers}\n+\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.apache.hadoop.yarn.server.MiniYARNCluster\n+\n+import org.apache.spark.{Logging, SparkConf, SparkContext}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.util.Utils\n+\n+class YarnClusterSuite extends FunSuite with BeforeAndAfterAll with Matchers {\n+\n+  private var oldConf: Map[String, String] = _\n+  private var yarnCluster: MiniYARNCluster = _\n+  private var tempDir: File = _\n+  private var fakeSparkJar: File = _\n+\n+  override def beforeAll() {\n+    tempDir = Utils.createTempDir()\n+\n+    yarnCluster = new MiniYARNCluster(getClass().getName(), 1, 1, 1)\n+    yarnCluster.init(new YarnConfiguration())\n+    yarnCluster.start()\n+\n+    oldConf = sys.props.toMap\n+    yarnCluster.getConfig().foreach { e =>\n+      sys.props += (\"spark.hadoop.\" + e.getKey() -> e.getValue())\n+    }\n+\n+    fakeSparkJar = File.createTempFile(\"sparkJar\", null, tempDir)\n+    sys.props += (\"spark.yarn.jar\" -> (\"local:\" + fakeSparkJar.getAbsolutePath()))\n+    sys.props += (\"spark.executor.instances\" -> \"1\")\n+    sys.props += (\"spark.driver.extraClassPath\" -> sys.props(\"java.class.path\"))\n+    sys.props += (\"spark.executor.extraClassPath\" -> sys.props(\"java.class.path\"))\n+\n+    super.beforeAll()\n+  }\n+\n+  override def afterAll() {\n+    yarnCluster.stop()\n+\n+    val sysProps = sys.props.map { case (k, v) => (k, v) }\n+    sysProps.foreach { case (k, v) =>\n+      if (k.startsWith(\"spark.\")) {\n+        sys.props -= k\n+      }\n+    }\n+\n+    sys.props ++= oldConf\n+\n+    super.afterAll()\n+  }\n+\n+  test(\"run Spark in yarn-client mode\") {\n+    var result = File.createTempFile(\"result\", null, tempDir)\n+    YarnClusterDriver.main(Array(\"yarn-client\", result.getAbsolutePath()))\n+    checkResult(result)\n+  }\n+\n+  test(\"run Spark in yarn-cluster mode\") {\n+    val main = YarnClusterDriver.getClass.getName().stripSuffix(\"$\")\n+    var result = File.createTempFile(\"result\", null, tempDir)\n+\n+    // The Client object will call System.exit() after the job is done, and we don't want\n+    // that because it messes up the scalatest monitoring. So replicate some of what main()\n+    // does here.\n+    val args = Array(\"--class\", main,\n+      \"--jar\", \"file:\" + fakeSparkJar.getAbsolutePath(),\n+      \"--arg\", \"yarn-cluster\",\n+      \"--arg\", result.getAbsolutePath(),\n+      \"--num-executors\", \"1\")\n+    val sparkConf = new SparkConf()\n+    val yarnConf = SparkHadoopUtil.get.newConfiguration(sparkConf)\n+    val clientArgs = new ClientArguments(args, sparkConf)\n+    new Client(clientArgs, yarnConf, sparkConf).run()\n+    checkResult(result)\n+  }\n+\n+  /**\n+   * This is a workaround for an issue with yarn-cluster mode: the Client class will not provide\n+   * any sort of error when the job process finishes successfully, but the job itself fails. So\n+   * the tests enforce that something is written to a file after everything is ok to indicate\n+   * that the job succeeded.\n+   */\n+  private def checkResult(result: File) = {\n+    var resultString = Files.toString(result, Charsets.UTF_8)\n+    resultString should be (\"success\")\n+  }\n+\n+}\n+\n+private object YarnClusterDriver extends Logging with Matchers {\n+\n+  def main(args: Array[String]) = {\n+    val sc = new SparkContext(new SparkConf().setMaster(args(0))\n+      .setAppName(\"yarn \\\"test app\\\" 'with quotes' and \\\\back\\\\slashes and $dollarSigns\"))\n+    val status = new File(args(1))\n+    var result = \"failure\"\n+    try {\n+      val data = sc.parallelize(1 to 4).map(i => i).collect().toSet"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "It was just a way to trigger an actual job. I guess I could do `parallelize(1 to 4, 4).collect()` to achieve the same thing.\n",
    "commit": "6d5b84e8b5987683591d8c07b3ff8557d9581871",
    "createdAt": "2014-09-23T20:58:27Z",
    "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import java.io.File\n+\n+import scala.collection.JavaConversions._\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import org.scalatest.{BeforeAndAfterAll, FunSuite, Matchers}\n+\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.apache.hadoop.yarn.server.MiniYARNCluster\n+\n+import org.apache.spark.{Logging, SparkConf, SparkContext}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.util.Utils\n+\n+class YarnClusterSuite extends FunSuite with BeforeAndAfterAll with Matchers {\n+\n+  private var oldConf: Map[String, String] = _\n+  private var yarnCluster: MiniYARNCluster = _\n+  private var tempDir: File = _\n+  private var fakeSparkJar: File = _\n+\n+  override def beforeAll() {\n+    tempDir = Utils.createTempDir()\n+\n+    yarnCluster = new MiniYARNCluster(getClass().getName(), 1, 1, 1)\n+    yarnCluster.init(new YarnConfiguration())\n+    yarnCluster.start()\n+\n+    oldConf = sys.props.toMap\n+    yarnCluster.getConfig().foreach { e =>\n+      sys.props += (\"spark.hadoop.\" + e.getKey() -> e.getValue())\n+    }\n+\n+    fakeSparkJar = File.createTempFile(\"sparkJar\", null, tempDir)\n+    sys.props += (\"spark.yarn.jar\" -> (\"local:\" + fakeSparkJar.getAbsolutePath()))\n+    sys.props += (\"spark.executor.instances\" -> \"1\")\n+    sys.props += (\"spark.driver.extraClassPath\" -> sys.props(\"java.class.path\"))\n+    sys.props += (\"spark.executor.extraClassPath\" -> sys.props(\"java.class.path\"))\n+\n+    super.beforeAll()\n+  }\n+\n+  override def afterAll() {\n+    yarnCluster.stop()\n+\n+    val sysProps = sys.props.map { case (k, v) => (k, v) }\n+    sysProps.foreach { case (k, v) =>\n+      if (k.startsWith(\"spark.\")) {\n+        sys.props -= k\n+      }\n+    }\n+\n+    sys.props ++= oldConf\n+\n+    super.afterAll()\n+  }\n+\n+  test(\"run Spark in yarn-client mode\") {\n+    var result = File.createTempFile(\"result\", null, tempDir)\n+    YarnClusterDriver.main(Array(\"yarn-client\", result.getAbsolutePath()))\n+    checkResult(result)\n+  }\n+\n+  test(\"run Spark in yarn-cluster mode\") {\n+    val main = YarnClusterDriver.getClass.getName().stripSuffix(\"$\")\n+    var result = File.createTempFile(\"result\", null, tempDir)\n+\n+    // The Client object will call System.exit() after the job is done, and we don't want\n+    // that because it messes up the scalatest monitoring. So replicate some of what main()\n+    // does here.\n+    val args = Array(\"--class\", main,\n+      \"--jar\", \"file:\" + fakeSparkJar.getAbsolutePath(),\n+      \"--arg\", \"yarn-cluster\",\n+      \"--arg\", result.getAbsolutePath(),\n+      \"--num-executors\", \"1\")\n+    val sparkConf = new SparkConf()\n+    val yarnConf = SparkHadoopUtil.get.newConfiguration(sparkConf)\n+    val clientArgs = new ClientArguments(args, sparkConf)\n+    new Client(clientArgs, yarnConf, sparkConf).run()\n+    checkResult(result)\n+  }\n+\n+  /**\n+   * This is a workaround for an issue with yarn-cluster mode: the Client class will not provide\n+   * any sort of error when the job process finishes successfully, but the job itself fails. So\n+   * the tests enforce that something is written to a file after everything is ok to indicate\n+   * that the job succeeded.\n+   */\n+  private def checkResult(result: File) = {\n+    var resultString = Files.toString(result, Charsets.UTF_8)\n+    resultString should be (\"success\")\n+  }\n+\n+}\n+\n+private object YarnClusterDriver extends Logging with Matchers {\n+\n+  def main(args: Array[String]) = {\n+    val sc = new SparkContext(new SparkConf().setMaster(args(0))\n+      .setAppName(\"yarn \\\"test app\\\" 'with quotes' and \\\\back\\\\slashes and $dollarSigns\"))\n+    val status = new File(args(1))\n+    var result = \"failure\"\n+    try {\n+      val data = sc.parallelize(1 to 4).map(i => i).collect().toSet"
  }],
  "prId": 2257
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "Can you add an arg check here that prints usage and exits if not enough args are provided?\n",
    "commit": "6d5b84e8b5987683591d8c07b3ff8557d9581871",
    "createdAt": "2014-09-23T20:50:44Z",
    "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import java.io.File\n+\n+import scala.collection.JavaConversions._\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import org.scalatest.{BeforeAndAfterAll, FunSuite, Matchers}\n+\n+import org.apache.hadoop.yarn.conf.YarnConfiguration\n+import org.apache.hadoop.yarn.server.MiniYARNCluster\n+\n+import org.apache.spark.{Logging, SparkConf, SparkContext}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.util.Utils\n+\n+class YarnClusterSuite extends FunSuite with BeforeAndAfterAll with Matchers {\n+\n+  private var oldConf: Map[String, String] = _\n+  private var yarnCluster: MiniYARNCluster = _\n+  private var tempDir: File = _\n+  private var fakeSparkJar: File = _\n+\n+  override def beforeAll() {\n+    tempDir = Utils.createTempDir()\n+\n+    yarnCluster = new MiniYARNCluster(getClass().getName(), 1, 1, 1)\n+    yarnCluster.init(new YarnConfiguration())\n+    yarnCluster.start()\n+\n+    oldConf = sys.props.toMap\n+    yarnCluster.getConfig().foreach { e =>\n+      sys.props += (\"spark.hadoop.\" + e.getKey() -> e.getValue())\n+    }\n+\n+    fakeSparkJar = File.createTempFile(\"sparkJar\", null, tempDir)\n+    sys.props += (\"spark.yarn.jar\" -> (\"local:\" + fakeSparkJar.getAbsolutePath()))\n+    sys.props += (\"spark.executor.instances\" -> \"1\")\n+    sys.props += (\"spark.driver.extraClassPath\" -> sys.props(\"java.class.path\"))\n+    sys.props += (\"spark.executor.extraClassPath\" -> sys.props(\"java.class.path\"))\n+\n+    super.beforeAll()\n+  }\n+\n+  override def afterAll() {\n+    yarnCluster.stop()\n+\n+    val sysProps = sys.props.map { case (k, v) => (k, v) }\n+    sysProps.foreach { case (k, v) =>\n+      if (k.startsWith(\"spark.\")) {\n+        sys.props -= k\n+      }\n+    }\n+\n+    sys.props ++= oldConf\n+\n+    super.afterAll()\n+  }\n+\n+  test(\"run Spark in yarn-client mode\") {\n+    var result = File.createTempFile(\"result\", null, tempDir)\n+    YarnClusterDriver.main(Array(\"yarn-client\", result.getAbsolutePath()))\n+    checkResult(result)\n+  }\n+\n+  test(\"run Spark in yarn-cluster mode\") {\n+    val main = YarnClusterDriver.getClass.getName().stripSuffix(\"$\")\n+    var result = File.createTempFile(\"result\", null, tempDir)\n+\n+    // The Client object will call System.exit() after the job is done, and we don't want\n+    // that because it messes up the scalatest monitoring. So replicate some of what main()\n+    // does here.\n+    val args = Array(\"--class\", main,\n+      \"--jar\", \"file:\" + fakeSparkJar.getAbsolutePath(),\n+      \"--arg\", \"yarn-cluster\",\n+      \"--arg\", result.getAbsolutePath(),\n+      \"--num-executors\", \"1\")\n+    val sparkConf = new SparkConf()\n+    val yarnConf = SparkHadoopUtil.get.newConfiguration(sparkConf)\n+    val clientArgs = new ClientArguments(args, sparkConf)\n+    new Client(clientArgs, yarnConf, sparkConf).run()\n+    checkResult(result)\n+  }\n+\n+  /**\n+   * This is a workaround for an issue with yarn-cluster mode: the Client class will not provide\n+   * any sort of error when the job process finishes successfully, but the job itself fails. So\n+   * the tests enforce that something is written to a file after everything is ok to indicate\n+   * that the job succeeded.\n+   */\n+  private def checkResult(result: File) = {\n+    var resultString = Files.toString(result, Charsets.UTF_8)\n+    resultString should be (\"success\")\n+  }\n+\n+}\n+\n+private object YarnClusterDriver extends Logging with Matchers {\n+\n+  def main(args: Array[String]) = {",
    "line": 129
  }],
  "prId": 2257
}]