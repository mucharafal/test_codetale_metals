[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Also, it's a better idea to use Hadoop's `ShutdownHookManager`. That allows you to make sure this runs before the HDFS shutdown hook, and avoid ugly exceptions (like \"file system closed\") showing up. See ApplicationMaster.scala, for example.\n",
    "commit": "c560c6ae1b479123e0ea570cbf93aa244ddcace2",
    "createdAt": "2014-08-29T16:22:05Z",
    "diffHunk": "@@ -36,4 +36,18 @@ private[spark] class YarnClientClusterScheduler(sc: SparkContext, conf: Configur\n     val host = Utils.parseHostPort(hostPort)._1\n     Option(YarnSparkHadoopUtil.lookupRack(conf, host))\n   }\n+\n+  override def postStartHook() {\n+    Runtime.getRuntime.addShutdownHook(new Thread with Logging {"
  }],
  "prId": 2001
}]