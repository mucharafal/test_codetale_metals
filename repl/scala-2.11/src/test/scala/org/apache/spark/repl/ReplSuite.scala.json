[{
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "Why you don't move this test?",
    "commit": "7dde745dbd5b629fb7cff598f417d00494bcbc00",
    "createdAt": "2017-05-05T22:22:22Z",
    "diffHunk": "@@ -373,52 +190,6 @@ class ReplSuite extends SparkFunSuite {\n     }\n   }\n \n-  test(\"collecting objects of class defined in repl\") {\n-    val output = runInterpreter(\"local[2]\",\n-      \"\"\"\n-        |case class Foo(i: Int)\n-        |val ret = sc.parallelize((1 to 100).map(Foo), 10).collect()\n-      \"\"\".stripMargin)\n-    assertDoesNotContain(\"error:\", output)\n-    assertDoesNotContain(\"Exception\", output)\n-    assertContains(\"ret: Array[Foo] = Array(Foo(1),\", output)\n-  }\n-\n-  test(\"collecting objects of class defined in repl - shuffling\") {\n-    val output = runInterpreter(\"local-cluster[1,1,1024]\",\n-      \"\"\"\n-        |case class Foo(i: Int)\n-        |val list = List((1, Foo(1)), (1, Foo(2)))\n-        |val ret = sc.parallelize(list).groupByKey().collect()\n-      \"\"\".stripMargin)\n-    assertDoesNotContain(\"error:\", output)\n-    assertDoesNotContain(\"Exception\", output)\n-    assertContains(\"ret: Array[(Int, Iterable[Foo])] = Array((1,\", output)\n-  }\n-\n-  test(\"replicating blocks of object with class defined in repl\") {\n-    val output = runInterpreter(\"local-cluster[2,1,1024]\",\n-      \"\"\"\n-        |val timeout = 60000 // 60 seconds\n-        |val start = System.currentTimeMillis\n-        |while(sc.getExecutorStorageStatus.size != 3 &&\n-        |    (System.currentTimeMillis - start) < timeout) {\n-        |  Thread.sleep(10)\n-        |}\n-        |if (System.currentTimeMillis - start >= timeout) {\n-        |  throw new java.util.concurrent.TimeoutException(\"Executors were not up in 60 seconds\")\n-        |}\n-        |import org.apache.spark.storage.StorageLevel._\n-        |case class Foo(i: Int)\n-        |val ret = sc.parallelize((1 to 100).map(Foo), 10).persist(MEMORY_AND_DISK_2)\n-        |ret.count()\n-        |sc.getExecutorStorageStatus.map(s => s.rddBlocksById(ret.id).size).sum\n-      \"\"\".stripMargin)\n-    assertDoesNotContain(\"error:\", output)\n-    assertDoesNotContain(\"Exception\", output)\n-    assertContains(\": Int = 20\", output)\n-  }\n-\n   test(\"line wrapper only initialized once when used as encoder outer scope\") {",
    "line": 262
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "this test only works in local mode...",
    "commit": "7dde745dbd5b629fb7cff598f417d00494bcbc00",
    "createdAt": "2017-05-08T08:50:52Z",
    "diffHunk": "@@ -373,52 +190,6 @@ class ReplSuite extends SparkFunSuite {\n     }\n   }\n \n-  test(\"collecting objects of class defined in repl\") {\n-    val output = runInterpreter(\"local[2]\",\n-      \"\"\"\n-        |case class Foo(i: Int)\n-        |val ret = sc.parallelize((1 to 100).map(Foo), 10).collect()\n-      \"\"\".stripMargin)\n-    assertDoesNotContain(\"error:\", output)\n-    assertDoesNotContain(\"Exception\", output)\n-    assertContains(\"ret: Array[Foo] = Array(Foo(1),\", output)\n-  }\n-\n-  test(\"collecting objects of class defined in repl - shuffling\") {\n-    val output = runInterpreter(\"local-cluster[1,1,1024]\",\n-      \"\"\"\n-        |case class Foo(i: Int)\n-        |val list = List((1, Foo(1)), (1, Foo(2)))\n-        |val ret = sc.parallelize(list).groupByKey().collect()\n-      \"\"\".stripMargin)\n-    assertDoesNotContain(\"error:\", output)\n-    assertDoesNotContain(\"Exception\", output)\n-    assertContains(\"ret: Array[(Int, Iterable[Foo])] = Array((1,\", output)\n-  }\n-\n-  test(\"replicating blocks of object with class defined in repl\") {\n-    val output = runInterpreter(\"local-cluster[2,1,1024]\",\n-      \"\"\"\n-        |val timeout = 60000 // 60 seconds\n-        |val start = System.currentTimeMillis\n-        |while(sc.getExecutorStorageStatus.size != 3 &&\n-        |    (System.currentTimeMillis - start) < timeout) {\n-        |  Thread.sleep(10)\n-        |}\n-        |if (System.currentTimeMillis - start >= timeout) {\n-        |  throw new java.util.concurrent.TimeoutException(\"Executors were not up in 60 seconds\")\n-        |}\n-        |import org.apache.spark.storage.StorageLevel._\n-        |case class Foo(i: Int)\n-        |val ret = sc.parallelize((1 to 100).map(Foo), 10).persist(MEMORY_AND_DISK_2)\n-        |ret.count()\n-        |sc.getExecutorStorageStatus.map(s => s.rddBlocksById(ret.id).size).sum\n-      \"\"\".stripMargin)\n-    assertDoesNotContain(\"error:\", output)\n-    assertDoesNotContain(\"Exception\", output)\n-    assertContains(\": Int = 20\", output)\n-  }\n-\n   test(\"line wrapper only initialized once when used as encoder outer scope\") {",
    "line": 262
  }],
  "prId": 17844
}]