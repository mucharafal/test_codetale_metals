[{
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "nit: it's better to use some more special string. E.g.,\r\n\r\n```\r\nval timestamp = System.currentTimeMillis()\r\nin.write((input + s\"\\nval _result_$timestamp = 1\\n\").getBytes)\r\nin.flush()\r\nval stopMessage = s\"_result_$timestamp: Int = 1\"\r\n```",
    "commit": "7dde745dbd5b629fb7cff598f417d00494bcbc00",
    "createdAt": "2017-05-05T18:28:08Z",
    "diffHunk": "@@ -0,0 +1,404 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.repl\n+\n+import java.io._\n+import java.net.URLClassLoader\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.commons.lang3.StringEscapeUtils\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A special test suite for REPL that all test cases share one REPL instance.\n+ */\n+class SingletonReplSuite extends SparkFunSuite {\n+\n+  private val out = new StringWriter()\n+  private val in = new PipedOutputStream()\n+  private var thread: Thread = _\n+\n+  private val CONF_EXECUTOR_CLASSPATH = \"spark.executor.extraClassPath\"\n+  private val oldExecutorClasspath = System.getProperty(CONF_EXECUTOR_CLASSPATH)\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+\n+    val cl = getClass.getClassLoader\n+    var paths = new ArrayBuffer[String]\n+    if (cl.isInstanceOf[URLClassLoader]) {\n+      val urlLoader = cl.asInstanceOf[URLClassLoader]\n+      for (url <- urlLoader.getURLs) {\n+        if (url.getProtocol == \"file\") {\n+          paths += url.getFile\n+        }\n+      }\n+    }\n+    val classpath = paths.map(new File(_).getAbsolutePath).mkString(File.pathSeparator)\n+\n+    System.setProperty(CONF_EXECUTOR_CLASSPATH, classpath)\n+    Main.conf.set(\"spark.master\", \"local-cluster[2,4,4096]\")\n+    val interp = new SparkILoop(\n+      new BufferedReader(new InputStreamReader(new PipedInputStream(in))),\n+      new PrintWriter(out))\n+\n+    // Forces to create new SparkContext\n+    Main.sparkContext = null\n+    Main.sparkSession = null\n+\n+    // Starts a new thread to run the REPL interpreter, so that we won't block.\n+    thread = new Thread(new Runnable {\n+      override def run(): Unit = Main.doMain(Array(\"-classpath\", classpath), interp)\n+    })\n+    thread.start()\n+\n+    waitUntil(() => out.toString.contains(\"Type :help for more information\"))\n+  }\n+\n+  override def afterAll(): Unit = {\n+    in.close()\n+    thread.join()\n+    if (oldExecutorClasspath != null) {\n+      System.setProperty(CONF_EXECUTOR_CLASSPATH, oldExecutorClasspath)\n+    } else {\n+      System.clearProperty(CONF_EXECUTOR_CLASSPATH)\n+    }\n+    super.afterAll()\n+  }\n+\n+  private def waitUntil(cond: () => Boolean): Unit = {\n+    var i = 0\n+    while (i < 100 && !cond()) {\n+      Thread.sleep(500)\n+      i += 1\n+    }\n+    if (i == 100) {\n+      throw new IllegalStateException(\"timeout after 50 seconds, current output: \" + out.toString)\n+    }\n+  }\n+\n+  def runInterpreter(input: String): String = {\n+    val currentOffset = out.getBuffer.length()\n+    // append a `val _result = 1` statement to the end of the given code, so that we can know what's\n+    // the final output of this code snippet and rely on it to wait until the output is ready.\n+    in.write((input + s\"\\nval _result = 1\\n\").getBytes)"
  }],
  "prId": 17844
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "Could you add a comment mentioning that the codes in `input` should not throw an exception?",
    "commit": "7dde745dbd5b629fb7cff598f417d00494bcbc00",
    "createdAt": "2017-05-05T22:06:04Z",
    "diffHunk": "@@ -0,0 +1,404 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.repl\n+\n+import java.io._\n+import java.net.URLClassLoader\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.commons.lang3.StringEscapeUtils\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A special test suite for REPL that all test cases share one REPL instance.\n+ */\n+class SingletonReplSuite extends SparkFunSuite {\n+\n+  private val out = new StringWriter()\n+  private val in = new PipedOutputStream()\n+  private var thread: Thread = _\n+\n+  private val CONF_EXECUTOR_CLASSPATH = \"spark.executor.extraClassPath\"\n+  private val oldExecutorClasspath = System.getProperty(CONF_EXECUTOR_CLASSPATH)\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+\n+    val cl = getClass.getClassLoader\n+    var paths = new ArrayBuffer[String]\n+    if (cl.isInstanceOf[URLClassLoader]) {\n+      val urlLoader = cl.asInstanceOf[URLClassLoader]\n+      for (url <- urlLoader.getURLs) {\n+        if (url.getProtocol == \"file\") {\n+          paths += url.getFile\n+        }\n+      }\n+    }\n+    val classpath = paths.map(new File(_).getAbsolutePath).mkString(File.pathSeparator)\n+\n+    System.setProperty(CONF_EXECUTOR_CLASSPATH, classpath)\n+    Main.conf.set(\"spark.master\", \"local-cluster[2,4,4096]\")\n+    val interp = new SparkILoop(\n+      new BufferedReader(new InputStreamReader(new PipedInputStream(in))),\n+      new PrintWriter(out))\n+\n+    // Forces to create new SparkContext\n+    Main.sparkContext = null\n+    Main.sparkSession = null\n+\n+    // Starts a new thread to run the REPL interpreter, so that we won't block.\n+    thread = new Thread(new Runnable {\n+      override def run(): Unit = Main.doMain(Array(\"-classpath\", classpath), interp)\n+    })\n+    thread.start()\n+\n+    waitUntil(() => out.toString.contains(\"Type :help for more information\"))\n+  }\n+\n+  override def afterAll(): Unit = {\n+    in.close()\n+    thread.join()\n+    if (oldExecutorClasspath != null) {\n+      System.setProperty(CONF_EXECUTOR_CLASSPATH, oldExecutorClasspath)\n+    } else {\n+      System.clearProperty(CONF_EXECUTOR_CLASSPATH)\n+    }\n+    super.afterAll()\n+  }\n+\n+  private def waitUntil(cond: () => Boolean): Unit = {\n+    var i = 0\n+    while (i < 100 && !cond()) {\n+      Thread.sleep(500)\n+      i += 1\n+    }\n+    if (i == 100) {\n+      throw new IllegalStateException(\"timeout after 50 seconds, current output: \" + out.toString)\n+    }\n+  }\n+\n+  def runInterpreter(input: String): String = {"
  }],
  "prId": 17844
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "You can use `eventually`\r\n```\r\n    eventually(timeout(50.seconds), interval(500.millis)) {\r\n      assert(cond(), \"current output: \" + out.toString)\r\n    }\r\n```",
    "commit": "7dde745dbd5b629fb7cff598f417d00494bcbc00",
    "createdAt": "2017-05-05T22:09:26Z",
    "diffHunk": "@@ -0,0 +1,404 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.repl\n+\n+import java.io._\n+import java.net.URLClassLoader\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.commons.lang3.StringEscapeUtils\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A special test suite for REPL that all test cases share one REPL instance.\n+ */\n+class SingletonReplSuite extends SparkFunSuite {\n+\n+  private val out = new StringWriter()\n+  private val in = new PipedOutputStream()\n+  private var thread: Thread = _\n+\n+  private val CONF_EXECUTOR_CLASSPATH = \"spark.executor.extraClassPath\"\n+  private val oldExecutorClasspath = System.getProperty(CONF_EXECUTOR_CLASSPATH)\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+\n+    val cl = getClass.getClassLoader\n+    var paths = new ArrayBuffer[String]\n+    if (cl.isInstanceOf[URLClassLoader]) {\n+      val urlLoader = cl.asInstanceOf[URLClassLoader]\n+      for (url <- urlLoader.getURLs) {\n+        if (url.getProtocol == \"file\") {\n+          paths += url.getFile\n+        }\n+      }\n+    }\n+    val classpath = paths.map(new File(_).getAbsolutePath).mkString(File.pathSeparator)\n+\n+    System.setProperty(CONF_EXECUTOR_CLASSPATH, classpath)\n+    Main.conf.set(\"spark.master\", \"local-cluster[2,4,4096]\")\n+    val interp = new SparkILoop(\n+      new BufferedReader(new InputStreamReader(new PipedInputStream(in))),\n+      new PrintWriter(out))\n+\n+    // Forces to create new SparkContext\n+    Main.sparkContext = null\n+    Main.sparkSession = null\n+\n+    // Starts a new thread to run the REPL interpreter, so that we won't block.\n+    thread = new Thread(new Runnable {\n+      override def run(): Unit = Main.doMain(Array(\"-classpath\", classpath), interp)\n+    })\n+    thread.start()\n+\n+    waitUntil(() => out.toString.contains(\"Type :help for more information\"))\n+  }\n+\n+  override def afterAll(): Unit = {\n+    in.close()\n+    thread.join()\n+    if (oldExecutorClasspath != null) {\n+      System.setProperty(CONF_EXECUTOR_CLASSPATH, oldExecutorClasspath)\n+    } else {\n+      System.clearProperty(CONF_EXECUTOR_CLASSPATH)\n+    }\n+    super.afterAll()\n+  }\n+\n+  private def waitUntil(cond: () => Boolean): Unit = {",
    "line": 88
  }],
  "prId": 17844
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "nit: I think it's safe to just use `local-cluster[2,1,1024]`.",
    "commit": "7dde745dbd5b629fb7cff598f417d00494bcbc00",
    "createdAt": "2017-05-05T22:25:49Z",
    "diffHunk": "@@ -0,0 +1,404 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.repl\n+\n+import java.io._\n+import java.net.URLClassLoader\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.commons.lang3.StringEscapeUtils\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A special test suite for REPL that all test cases share one REPL instance.\n+ */\n+class SingletonReplSuite extends SparkFunSuite {\n+\n+  private val out = new StringWriter()\n+  private val in = new PipedOutputStream()\n+  private var thread: Thread = _\n+\n+  private val CONF_EXECUTOR_CLASSPATH = \"spark.executor.extraClassPath\"\n+  private val oldExecutorClasspath = System.getProperty(CONF_EXECUTOR_CLASSPATH)\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+\n+    val cl = getClass.getClassLoader\n+    var paths = new ArrayBuffer[String]\n+    if (cl.isInstanceOf[URLClassLoader]) {\n+      val urlLoader = cl.asInstanceOf[URLClassLoader]\n+      for (url <- urlLoader.getURLs) {\n+        if (url.getProtocol == \"file\") {\n+          paths += url.getFile\n+        }\n+      }\n+    }\n+    val classpath = paths.map(new File(_).getAbsolutePath).mkString(File.pathSeparator)\n+\n+    System.setProperty(CONF_EXECUTOR_CLASSPATH, classpath)\n+    Main.conf.set(\"spark.master\", \"local-cluster[2,4,4096]\")"
  }],
  "prId": 17844
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "nit: please call `Thread.setDaemon(true)` in case that something is wrong and it prevents the process from exiting.",
    "commit": "7dde745dbd5b629fb7cff598f417d00494bcbc00",
    "createdAt": "2017-05-05T22:27:53Z",
    "diffHunk": "@@ -0,0 +1,404 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.repl\n+\n+import java.io._\n+import java.net.URLClassLoader\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.commons.lang3.StringEscapeUtils\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A special test suite for REPL that all test cases share one REPL instance.\n+ */\n+class SingletonReplSuite extends SparkFunSuite {\n+\n+  private val out = new StringWriter()\n+  private val in = new PipedOutputStream()\n+  private var thread: Thread = _\n+\n+  private val CONF_EXECUTOR_CLASSPATH = \"spark.executor.extraClassPath\"\n+  private val oldExecutorClasspath = System.getProperty(CONF_EXECUTOR_CLASSPATH)\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+\n+    val cl = getClass.getClassLoader\n+    var paths = new ArrayBuffer[String]\n+    if (cl.isInstanceOf[URLClassLoader]) {\n+      val urlLoader = cl.asInstanceOf[URLClassLoader]\n+      for (url <- urlLoader.getURLs) {\n+        if (url.getProtocol == \"file\") {\n+          paths += url.getFile\n+        }\n+      }\n+    }\n+    val classpath = paths.map(new File(_).getAbsolutePath).mkString(File.pathSeparator)\n+\n+    System.setProperty(CONF_EXECUTOR_CLASSPATH, classpath)\n+    Main.conf.set(\"spark.master\", \"local-cluster[2,4,4096]\")\n+    val interp = new SparkILoop(\n+      new BufferedReader(new InputStreamReader(new PipedInputStream(in))),\n+      new PrintWriter(out))\n+\n+    // Forces to create new SparkContext\n+    Main.sparkContext = null\n+    Main.sparkSession = null\n+\n+    // Starts a new thread to run the REPL interpreter, so that we won't block.\n+    thread = new Thread(new Runnable {",
    "line": 68
  }],
  "prId": 17844
}]