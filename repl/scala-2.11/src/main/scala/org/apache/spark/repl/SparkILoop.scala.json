[{
  "comments": [{
    "author": {
      "login": "felixcheung"
    },
    "body": "my concern is SparkILoop is used in a bunch of settings outside of spark and its shell/repl - sys.exit might not be ideal in some cases",
    "commit": "6d53ca024a5f88d7d3dcd41257c3de72aadd40b6",
    "createdAt": "2018-05-19T06:39:56Z",
    "diffHunk": "@@ -44,7 +44,14 @@ class SparkILoop(in0: Option[BufferedReader], out: JPrintWriter)\n     @transient val spark = if (org.apache.spark.repl.Main.sparkSession != null) {\n         org.apache.spark.repl.Main.sparkSession\n       } else {\n-        org.apache.spark.repl.Main.createSparkSession()\n+        try {\n+          org.apache.spark.repl.Main.createSparkSession()\n+        } catch {\n+          case e: Exception =>\n+            println(\"Failed to initialize Spark session:\")\n+            e.printStackTrace()\n+            sys.exit(1)"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "My usual response is \"this is not a public class\" (it's not in the public API docs), but let me see if it's easy to restrict the `sys.exit` to spark-shell invocations.",
    "commit": "6d53ca024a5f88d7d3dcd41257c3de72aadd40b6",
    "createdAt": "2018-05-21T16:56:04Z",
    "diffHunk": "@@ -44,7 +44,14 @@ class SparkILoop(in0: Option[BufferedReader], out: JPrintWriter)\n     @transient val spark = if (org.apache.spark.repl.Main.sparkSession != null) {\n         org.apache.spark.repl.Main.sparkSession\n       } else {\n-        org.apache.spark.repl.Main.createSparkSession()\n+        try {\n+          org.apache.spark.repl.Main.createSparkSession()\n+        } catch {\n+          case e: Exception =>\n+            println(\"Failed to initialize Spark session:\")\n+            e.printStackTrace()\n+            sys.exit(1)"
  }],
  "prId": 21368
}]