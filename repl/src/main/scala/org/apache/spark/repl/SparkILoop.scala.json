[{
  "comments": [{
    "author": {
      "login": "mateiz"
    },
    "body": "Why did you remove this here? Do they get added elsewhere?\n",
    "commit": "262c6a2cff8438b568e90fae5a1dc04baf656f0c",
    "createdAt": "2014-08-30T22:51:54Z",
    "diffHunk": "@@ -965,11 +966,9 @@ class SparkILoop(in0: Option[BufferedReader], protected val out: JPrintWriter,\n \n   def createSparkContext(): SparkContext = {\n     val execUri = System.getenv(\"SPARK_EXECUTOR_URI\")\n-    val jars = SparkILoop.getAddedJars\n     val conf = new SparkConf()\n       .setMaster(getMaster())\n       .setAppName(\"Spark shell\")\n-      .setJars(jars)"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "When you create a `SparkConf` it picks up `spark.jars` anyway, so we shouldn't set it again here. For Windows, `getAddedJars` actually does some post-processing on the jar paths, so if we set it again here then `sys.props(\"spark.jars\")` and `sc.getConf.get(\"spark.jars\")` will have inconsistent values.\n",
    "commit": "262c6a2cff8438b568e90fae5a1dc04baf656f0c",
    "createdAt": "2014-08-31T09:04:14Z",
    "diffHunk": "@@ -965,11 +966,9 @@ class SparkILoop(in0: Option[BufferedReader], protected val out: JPrintWriter,\n \n   def createSparkContext(): SparkContext = {\n     val execUri = System.getenv(\"SPARK_EXECUTOR_URI\")\n-    val jars = SparkILoop.getAddedJars\n     val conf = new SparkConf()\n       .setMaster(getMaster())\n       .setAppName(\"Spark shell\")\n-      .setJars(jars)"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "Though I need to fix the `ADD_JAR` case, which is currently ignored...\n",
    "commit": "262c6a2cff8438b568e90fae5a1dc04baf656f0c",
    "createdAt": "2014-08-31T09:07:13Z",
    "diffHunk": "@@ -965,11 +966,9 @@ class SparkILoop(in0: Option[BufferedReader], protected val out: JPrintWriter,\n \n   def createSparkContext(): SparkContext = {\n     val execUri = System.getenv(\"SPARK_EXECUTOR_URI\")\n-    val jars = SparkILoop.getAddedJars\n     val conf = new SparkConf()\n       .setMaster(getMaster())\n       .setAppName(\"Spark shell\")\n-      .setJars(jars)"
  }, {
    "author": {
      "login": "mateiz"
    },
    "body": "Maybe we should deprecate ADD_JAR if you can add JARs with `--jars` now. Not sure why we left it in there. But maybe we can complain in the shell script and just leave it out.\n",
    "commit": "262c6a2cff8438b568e90fae5a1dc04baf656f0c",
    "createdAt": "2014-08-31T20:58:32Z",
    "diffHunk": "@@ -965,11 +966,9 @@ class SparkILoop(in0: Option[BufferedReader], protected val out: JPrintWriter,\n \n   def createSparkContext(): SparkContext = {\n     val execUri = System.getenv(\"SPARK_EXECUTOR_URI\")\n-    val jars = SparkILoop.getAddedJars\n     val conf = new SparkConf()\n       .setMaster(getMaster())\n       .setAppName(\"Spark shell\")\n-      .setJars(jars)"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "Hm yeah. Actually for this PR I think I'll just modify only the jar paths added to the classpath of the shell, but not worry about changing the value of `spark.jars` when different things are set. Then we can deal with the `ADD_JAR` thing in a separate PR.\n",
    "commit": "262c6a2cff8438b568e90fae5a1dc04baf656f0c",
    "createdAt": "2014-09-01T23:51:48Z",
    "diffHunk": "@@ -965,11 +966,9 @@ class SparkILoop(in0: Option[BufferedReader], protected val out: JPrintWriter,\n \n   def createSparkContext(): SparkContext = {\n     val execUri = System.getenv(\"SPARK_EXECUTOR_URI\")\n-    val jars = SparkILoop.getAddedJars\n     val conf = new SparkConf()\n       .setMaster(getMaster())\n       .setAppName(\"Spark shell\")\n-      .setJars(jars)"
  }],
  "prId": 2211
}]