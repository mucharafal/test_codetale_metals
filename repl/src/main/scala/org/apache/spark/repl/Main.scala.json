[{
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "@vanzin, seem `e.printStackTrace()` is missing .. ?",
    "commit": "6d53ca024a5f88d7d3dcd41257c3de72aadd40b6",
    "createdAt": "2018-05-22T02:03:51Z",
    "diffHunk": "@@ -79,44 +81,50 @@ object Main extends Logging {\n   }\n \n   def createSparkSession(): SparkSession = {\n-    val execUri = System.getenv(\"SPARK_EXECUTOR_URI\")\n-    conf.setIfMissing(\"spark.app.name\", \"Spark shell\")\n-    // SparkContext will detect this configuration and register it with the RpcEnv's\n-    // file server, setting spark.repl.class.uri to the actual URI for executors to\n-    // use. This is sort of ugly but since executors are started as part of SparkContext\n-    // initialization in certain cases, there's an initialization order issue that prevents\n-    // this from being set after SparkContext is instantiated.\n-    conf.set(\"spark.repl.class.outputDir\", outputDir.getAbsolutePath())\n-    if (execUri != null) {\n-      conf.set(\"spark.executor.uri\", execUri)\n-    }\n-    if (System.getenv(\"SPARK_HOME\") != null) {\n-      conf.setSparkHome(System.getenv(\"SPARK_HOME\"))\n-    }\n+    try {\n+      val execUri = System.getenv(\"SPARK_EXECUTOR_URI\")\n+      conf.setIfMissing(\"spark.app.name\", \"Spark shell\")\n+      // SparkContext will detect this configuration and register it with the RpcEnv's\n+      // file server, setting spark.repl.class.uri to the actual URI for executors to\n+      // use. This is sort of ugly but since executors are started as part of SparkContext\n+      // initialization in certain cases, there's an initialization order issue that prevents\n+      // this from being set after SparkContext is instantiated.\n+      conf.set(\"spark.repl.class.outputDir\", outputDir.getAbsolutePath())\n+      if (execUri != null) {\n+        conf.set(\"spark.executor.uri\", execUri)\n+      }\n+      if (System.getenv(\"SPARK_HOME\") != null) {\n+        conf.setSparkHome(System.getenv(\"SPARK_HOME\"))\n+      }\n \n-    val builder = SparkSession.builder.config(conf)\n-    if (conf.get(CATALOG_IMPLEMENTATION.key, \"hive\").toLowerCase(Locale.ROOT) == \"hive\") {\n-      if (SparkSession.hiveClassesArePresent) {\n-        // In the case that the property is not set at all, builder's config\n-        // does not have this value set to 'hive' yet. The original default\n-        // behavior is that when there are hive classes, we use hive catalog.\n-        sparkSession = builder.enableHiveSupport().getOrCreate()\n-        logInfo(\"Created Spark session with Hive support\")\n+      val builder = SparkSession.builder.config(conf)\n+      if (conf.get(CATALOG_IMPLEMENTATION.key, \"hive\").toLowerCase(Locale.ROOT) == \"hive\") {\n+        if (SparkSession.hiveClassesArePresent) {\n+          // In the case that the property is not set at all, builder's config\n+          // does not have this value set to 'hive' yet. The original default\n+          // behavior is that when there are hive classes, we use hive catalog.\n+          sparkSession = builder.enableHiveSupport().getOrCreate()\n+          logInfo(\"Created Spark session with Hive support\")\n+        } else {\n+          // Need to change it back to 'in-memory' if no hive classes are found\n+          // in the case that the property is set to hive in spark-defaults.conf\n+          builder.config(CATALOG_IMPLEMENTATION.key, \"in-memory\")\n+          sparkSession = builder.getOrCreate()\n+          logInfo(\"Created Spark session\")\n+        }\n       } else {\n-        // Need to change it back to 'in-memory' if no hive classes are found\n-        // in the case that the property is set to hive in spark-defaults.conf\n-        builder.config(CATALOG_IMPLEMENTATION.key, \"in-memory\")\n+        // In the case that the property is set but not to 'hive', the internal\n+        // default is 'in-memory'. So the sparkSession will use in-memory catalog.\n         sparkSession = builder.getOrCreate()\n         logInfo(\"Created Spark session\")\n       }\n-    } else {\n-      // In the case that the property is set but not to 'hive', the internal\n-      // default is 'in-memory'. So the sparkSession will use in-memory catalog.\n-      sparkSession = builder.getOrCreate()\n-      logInfo(\"Created Spark session\")\n+      sparkContext = sparkSession.sparkContext\n+      sparkSession\n+    } catch {\n+      case e: Exception if isShellSession =>\n+        logError(\"Failed to initialize Spark session.\", e)",
    "line": 91
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "The exception is being printed as part of the `logError`.",
    "commit": "6d53ca024a5f88d7d3dcd41257c3de72aadd40b6",
    "createdAt": "2018-05-22T02:26:10Z",
    "diffHunk": "@@ -79,44 +81,50 @@ object Main extends Logging {\n   }\n \n   def createSparkSession(): SparkSession = {\n-    val execUri = System.getenv(\"SPARK_EXECUTOR_URI\")\n-    conf.setIfMissing(\"spark.app.name\", \"Spark shell\")\n-    // SparkContext will detect this configuration and register it with the RpcEnv's\n-    // file server, setting spark.repl.class.uri to the actual URI for executors to\n-    // use. This is sort of ugly but since executors are started as part of SparkContext\n-    // initialization in certain cases, there's an initialization order issue that prevents\n-    // this from being set after SparkContext is instantiated.\n-    conf.set(\"spark.repl.class.outputDir\", outputDir.getAbsolutePath())\n-    if (execUri != null) {\n-      conf.set(\"spark.executor.uri\", execUri)\n-    }\n-    if (System.getenv(\"SPARK_HOME\") != null) {\n-      conf.setSparkHome(System.getenv(\"SPARK_HOME\"))\n-    }\n+    try {\n+      val execUri = System.getenv(\"SPARK_EXECUTOR_URI\")\n+      conf.setIfMissing(\"spark.app.name\", \"Spark shell\")\n+      // SparkContext will detect this configuration and register it with the RpcEnv's\n+      // file server, setting spark.repl.class.uri to the actual URI for executors to\n+      // use. This is sort of ugly but since executors are started as part of SparkContext\n+      // initialization in certain cases, there's an initialization order issue that prevents\n+      // this from being set after SparkContext is instantiated.\n+      conf.set(\"spark.repl.class.outputDir\", outputDir.getAbsolutePath())\n+      if (execUri != null) {\n+        conf.set(\"spark.executor.uri\", execUri)\n+      }\n+      if (System.getenv(\"SPARK_HOME\") != null) {\n+        conf.setSparkHome(System.getenv(\"SPARK_HOME\"))\n+      }\n \n-    val builder = SparkSession.builder.config(conf)\n-    if (conf.get(CATALOG_IMPLEMENTATION.key, \"hive\").toLowerCase(Locale.ROOT) == \"hive\") {\n-      if (SparkSession.hiveClassesArePresent) {\n-        // In the case that the property is not set at all, builder's config\n-        // does not have this value set to 'hive' yet. The original default\n-        // behavior is that when there are hive classes, we use hive catalog.\n-        sparkSession = builder.enableHiveSupport().getOrCreate()\n-        logInfo(\"Created Spark session with Hive support\")\n+      val builder = SparkSession.builder.config(conf)\n+      if (conf.get(CATALOG_IMPLEMENTATION.key, \"hive\").toLowerCase(Locale.ROOT) == \"hive\") {\n+        if (SparkSession.hiveClassesArePresent) {\n+          // In the case that the property is not set at all, builder's config\n+          // does not have this value set to 'hive' yet. The original default\n+          // behavior is that when there are hive classes, we use hive catalog.\n+          sparkSession = builder.enableHiveSupport().getOrCreate()\n+          logInfo(\"Created Spark session with Hive support\")\n+        } else {\n+          // Need to change it back to 'in-memory' if no hive classes are found\n+          // in the case that the property is set to hive in spark-defaults.conf\n+          builder.config(CATALOG_IMPLEMENTATION.key, \"in-memory\")\n+          sparkSession = builder.getOrCreate()\n+          logInfo(\"Created Spark session\")\n+        }\n       } else {\n-        // Need to change it back to 'in-memory' if no hive classes are found\n-        // in the case that the property is set to hive in spark-defaults.conf\n-        builder.config(CATALOG_IMPLEMENTATION.key, \"in-memory\")\n+        // In the case that the property is set but not to 'hive', the internal\n+        // default is 'in-memory'. So the sparkSession will use in-memory catalog.\n         sparkSession = builder.getOrCreate()\n         logInfo(\"Created Spark session\")\n       }\n-    } else {\n-      // In the case that the property is set but not to 'hive', the internal\n-      // default is 'in-memory'. So the sparkSession will use in-memory catalog.\n-      sparkSession = builder.getOrCreate()\n-      logInfo(\"Created Spark session\")\n+      sparkContext = sparkSession.sparkContext\n+      sparkSession\n+    } catch {\n+      case e: Exception if isShellSession =>\n+        logError(\"Failed to initialize Spark session.\", e)",
    "line": 91
  }],
  "prId": 21368
}]