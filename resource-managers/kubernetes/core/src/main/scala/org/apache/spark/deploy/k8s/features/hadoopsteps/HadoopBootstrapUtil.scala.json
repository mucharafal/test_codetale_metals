[{
  "comments": [{
    "author": {
      "login": "suryag10"
    },
    "body": "This makes the \"/etc/\" mounted as read only and makes the driver pod not to spawn up on a spark submit. I guess we should use as below:\r\n          .withMountPath(KRB_FILE_DIR_PATH + \"/krb5.conf\")\r\n          .withSubPath(\"krb5.conf\")\r\n",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-08-28T17:43:20Z",
    "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features.hadoopsteps\n+\n+import java.io.File\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model.{ConfigMap, ConfigMapBuilder, ContainerBuilder, KeyToPathBuilder, PodBuilder}\n+\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkPod\n+import org.apache.spark.deploy.k8s.security.KubernetesHadoopDelegationTokenManager\n+\n+private[spark] object HadoopBootstrapUtil {\n+\n+   /**\n+    * Mounting the DT secret for both the Driver and the executors\n+    *\n+    * @param dtSecretName Name of the secret that stores the Delegation Token\n+    * @param dtSecretItemKey Name of the Item Key storing the Delegation Token\n+    * @param userName Name of the SparkUser to set SPARK_USER\n+    * @param fileLocation Location of the krb5 file\n+    * @param krb5ConfName Name of the ConfigMap for Krb5\n+    * @param pod Input pod to be appended to\n+    * @return a modified SparkPod\n+    */\n+  def bootstrapKerberosPod(\n+      dtSecretName: String,\n+      dtSecretItemKey: String,\n+      userName: String,\n+      fileLocation: String,\n+      krb5ConfName: String,\n+      pod: SparkPod) : SparkPod = {\n+      val krb5File = new File(fileLocation)\n+      val fileStringPath = krb5File.toPath.getFileName.toString\n+      val kerberizedPod = new PodBuilder(pod.pod)\n+        .editOrNewSpec()\n+          .addNewVolume()\n+            .withName(SPARK_APP_HADOOP_SECRET_VOLUME_NAME)\n+            .withNewSecret()\n+              .withSecretName(dtSecretName)\n+              .endSecret()\n+            .endVolume()\n+          .addNewVolume()\n+            .withName(KRB_FILE_VOLUME)\n+              .withNewConfigMap()\n+                .withName(krb5ConfName)\n+                .withItems(new KeyToPathBuilder()\n+                  .withKey(fileStringPath)\n+                  .withPath(fileStringPath)\n+                  .build())\n+                .endConfigMap()\n+              .endVolume()\n+        // TODO: (ifilonenko) make configurable PU(G)ID\n+          .editOrNewSecurityContext()\n+            .withRunAsUser(1000L)\n+            .withFsGroup(2000L)\n+            .endSecurityContext()\n+          .endSpec()\n+        .build()\n+      val kerberizedContainer = new ContainerBuilder(pod.container)\n+        .addNewVolumeMount()\n+          .withName(SPARK_APP_HADOOP_SECRET_VOLUME_NAME)\n+          .withMountPath(SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR)\n+          .endVolumeMount()\n+        .addNewVolumeMount()\n+          .withName(KRB_FILE_VOLUME)\n+          .withMountPath(KRB_FILE_DIR_PATH)"
  }, {
    "author": {
      "login": "ifilonenko"
    },
    "body": "Excellent catch as this was the reason it was unable to read the `/etc/` directory upon doing hadoopUGI login",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-09-04T16:25:10Z",
    "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features.hadoopsteps\n+\n+import java.io.File\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model.{ConfigMap, ConfigMapBuilder, ContainerBuilder, KeyToPathBuilder, PodBuilder}\n+\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkPod\n+import org.apache.spark.deploy.k8s.security.KubernetesHadoopDelegationTokenManager\n+\n+private[spark] object HadoopBootstrapUtil {\n+\n+   /**\n+    * Mounting the DT secret for both the Driver and the executors\n+    *\n+    * @param dtSecretName Name of the secret that stores the Delegation Token\n+    * @param dtSecretItemKey Name of the Item Key storing the Delegation Token\n+    * @param userName Name of the SparkUser to set SPARK_USER\n+    * @param fileLocation Location of the krb5 file\n+    * @param krb5ConfName Name of the ConfigMap for Krb5\n+    * @param pod Input pod to be appended to\n+    * @return a modified SparkPod\n+    */\n+  def bootstrapKerberosPod(\n+      dtSecretName: String,\n+      dtSecretItemKey: String,\n+      userName: String,\n+      fileLocation: String,\n+      krb5ConfName: String,\n+      pod: SparkPod) : SparkPod = {\n+      val krb5File = new File(fileLocation)\n+      val fileStringPath = krb5File.toPath.getFileName.toString\n+      val kerberizedPod = new PodBuilder(pod.pod)\n+        .editOrNewSpec()\n+          .addNewVolume()\n+            .withName(SPARK_APP_HADOOP_SECRET_VOLUME_NAME)\n+            .withNewSecret()\n+              .withSecretName(dtSecretName)\n+              .endSecret()\n+            .endVolume()\n+          .addNewVolume()\n+            .withName(KRB_FILE_VOLUME)\n+              .withNewConfigMap()\n+                .withName(krb5ConfName)\n+                .withItems(new KeyToPathBuilder()\n+                  .withKey(fileStringPath)\n+                  .withPath(fileStringPath)\n+                  .build())\n+                .endConfigMap()\n+              .endVolume()\n+        // TODO: (ifilonenko) make configurable PU(G)ID\n+          .editOrNewSecurityContext()\n+            .withRunAsUser(1000L)\n+            .withFsGroup(2000L)\n+            .endSecurityContext()\n+          .endSpec()\n+        .build()\n+      val kerberizedContainer = new ContainerBuilder(pod.container)\n+        .addNewVolumeMount()\n+          .withName(SPARK_APP_HADOOP_SECRET_VOLUME_NAME)\n+          .withMountPath(SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR)\n+          .endVolumeMount()\n+        .addNewVolumeMount()\n+          .withName(KRB_FILE_VOLUME)\n+          .withMountPath(KRB_FILE_DIR_PATH)"
  }, {
    "author": {
      "login": "skonto"
    },
    "body": "If it is read only why it cannot be read, am I missing something here? What are the permissions? Btw  if we plan to modify the configmap then there is a known limitation:  https://github.com/kubernetes/kubernetes/issues/50345.",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-09-04T18:26:15Z",
    "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features.hadoopsteps\n+\n+import java.io.File\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model.{ConfigMap, ConfigMapBuilder, ContainerBuilder, KeyToPathBuilder, PodBuilder}\n+\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkPod\n+import org.apache.spark.deploy.k8s.security.KubernetesHadoopDelegationTokenManager\n+\n+private[spark] object HadoopBootstrapUtil {\n+\n+   /**\n+    * Mounting the DT secret for both the Driver and the executors\n+    *\n+    * @param dtSecretName Name of the secret that stores the Delegation Token\n+    * @param dtSecretItemKey Name of the Item Key storing the Delegation Token\n+    * @param userName Name of the SparkUser to set SPARK_USER\n+    * @param fileLocation Location of the krb5 file\n+    * @param krb5ConfName Name of the ConfigMap for Krb5\n+    * @param pod Input pod to be appended to\n+    * @return a modified SparkPod\n+    */\n+  def bootstrapKerberosPod(\n+      dtSecretName: String,\n+      dtSecretItemKey: String,\n+      userName: String,\n+      fileLocation: String,\n+      krb5ConfName: String,\n+      pod: SparkPod) : SparkPod = {\n+      val krb5File = new File(fileLocation)\n+      val fileStringPath = krb5File.toPath.getFileName.toString\n+      val kerberizedPod = new PodBuilder(pod.pod)\n+        .editOrNewSpec()\n+          .addNewVolume()\n+            .withName(SPARK_APP_HADOOP_SECRET_VOLUME_NAME)\n+            .withNewSecret()\n+              .withSecretName(dtSecretName)\n+              .endSecret()\n+            .endVolume()\n+          .addNewVolume()\n+            .withName(KRB_FILE_VOLUME)\n+              .withNewConfigMap()\n+                .withName(krb5ConfName)\n+                .withItems(new KeyToPathBuilder()\n+                  .withKey(fileStringPath)\n+                  .withPath(fileStringPath)\n+                  .build())\n+                .endConfigMap()\n+              .endVolume()\n+        // TODO: (ifilonenko) make configurable PU(G)ID\n+          .editOrNewSecurityContext()\n+            .withRunAsUser(1000L)\n+            .withFsGroup(2000L)\n+            .endSecurityContext()\n+          .endSpec()\n+        .build()\n+      val kerberizedContainer = new ContainerBuilder(pod.container)\n+        .addNewVolumeMount()\n+          .withName(SPARK_APP_HADOOP_SECRET_VOLUME_NAME)\n+          .withMountPath(SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR)\n+          .endVolumeMount()\n+        .addNewVolumeMount()\n+          .withName(KRB_FILE_VOLUME)\n+          .withMountPath(KRB_FILE_DIR_PATH)"
  }, {
    "author": {
      "login": "suryag10"
    },
    "body": "Hi Stavros,\r\nThe main aim of the following step was to mount the krb5.conf file into /etc/ directory.\r\n        .addNewVolumeMount()\r\n          .withName(KRB_FILE_VOLUME)\r\n          .withMountPath(KRB_FILE_DIR_PATH) \r\nwhen the above is done all the contents which are actually  present inside the container in \"/etc/\" directory are lost and gets mounted only with the krb5.conf file with read permissions. As other contents are lost from \"/etc\" directory nothing works. As i had commented earlier as well, this makes the driver pod to fail (spawn fails) as well. To make things correct and mount only the krb5.conf file,   following should be done:\r\n.addNewVolumeMount()\r\n   .withMountPath(KRB_FILE_DIR_PATH + \"/krb5.conf\")\r\n   .withSubPath(\"krb5.conf\")\r\n\r\nWith this everything is perfect without any deletions from /etc/ directory. For krb5.conf file configmap, i dont think its going to change in run time. So this will not create any issue. Let me know, if you need more details on this. We can discuss further on this as well.\r\n\r\nRegards\r\nSurya",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-09-05T16:21:45Z",
    "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features.hadoopsteps\n+\n+import java.io.File\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model.{ConfigMap, ConfigMapBuilder, ContainerBuilder, KeyToPathBuilder, PodBuilder}\n+\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkPod\n+import org.apache.spark.deploy.k8s.security.KubernetesHadoopDelegationTokenManager\n+\n+private[spark] object HadoopBootstrapUtil {\n+\n+   /**\n+    * Mounting the DT secret for both the Driver and the executors\n+    *\n+    * @param dtSecretName Name of the secret that stores the Delegation Token\n+    * @param dtSecretItemKey Name of the Item Key storing the Delegation Token\n+    * @param userName Name of the SparkUser to set SPARK_USER\n+    * @param fileLocation Location of the krb5 file\n+    * @param krb5ConfName Name of the ConfigMap for Krb5\n+    * @param pod Input pod to be appended to\n+    * @return a modified SparkPod\n+    */\n+  def bootstrapKerberosPod(\n+      dtSecretName: String,\n+      dtSecretItemKey: String,\n+      userName: String,\n+      fileLocation: String,\n+      krb5ConfName: String,\n+      pod: SparkPod) : SparkPod = {\n+      val krb5File = new File(fileLocation)\n+      val fileStringPath = krb5File.toPath.getFileName.toString\n+      val kerberizedPod = new PodBuilder(pod.pod)\n+        .editOrNewSpec()\n+          .addNewVolume()\n+            .withName(SPARK_APP_HADOOP_SECRET_VOLUME_NAME)\n+            .withNewSecret()\n+              .withSecretName(dtSecretName)\n+              .endSecret()\n+            .endVolume()\n+          .addNewVolume()\n+            .withName(KRB_FILE_VOLUME)\n+              .withNewConfigMap()\n+                .withName(krb5ConfName)\n+                .withItems(new KeyToPathBuilder()\n+                  .withKey(fileStringPath)\n+                  .withPath(fileStringPath)\n+                  .build())\n+                .endConfigMap()\n+              .endVolume()\n+        // TODO: (ifilonenko) make configurable PU(G)ID\n+          .editOrNewSecurityContext()\n+            .withRunAsUser(1000L)\n+            .withFsGroup(2000L)\n+            .endSecurityContext()\n+          .endSpec()\n+        .build()\n+      val kerberizedContainer = new ContainerBuilder(pod.container)\n+        .addNewVolumeMount()\n+          .withName(SPARK_APP_HADOOP_SECRET_VOLUME_NAME)\n+          .withMountPath(SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR)\n+          .endVolumeMount()\n+        .addNewVolumeMount()\n+          .withName(KRB_FILE_VOLUME)\n+          .withMountPath(KRB_FILE_DIR_PATH)"
  }],
  "prId": 21669
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Whole body is indented too far.",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-09-05T21:27:56Z",
    "diffHunk": "@@ -0,0 +1,182 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features.hadoopsteps\n+\n+import java.io.File\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model.{ConfigMap, ConfigMapBuilder, ContainerBuilder, KeyToPathBuilder, PodBuilder}\n+\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkPod\n+import org.apache.spark.deploy.k8s.security.KubernetesHadoopDelegationTokenManager\n+\n+private[spark] object HadoopBootstrapUtil {\n+\n+   /**\n+    * Mounting the DT secret for both the Driver and the executors\n+    *\n+    * @param dtSecretName Name of the secret that stores the Delegation Token\n+    * @param dtSecretItemKey Name of the Item Key storing the Delegation Token\n+    * @param userName Name of the SparkUser to set SPARK_USER\n+    * @param fileLocation Location of the krb5 file\n+    * @param krb5ConfName Name of the ConfigMap for Krb5\n+    * @param pod Input pod to be appended to\n+    * @return a modified SparkPod\n+    */\n+  def bootstrapKerberosPod(\n+      dtSecretName: String,\n+      dtSecretItemKey: String,\n+      userName: String,\n+      fileLocation: String,\n+      krb5ConfName: String,\n+      pod: SparkPod) : SparkPod = {\n+      val krb5File = new File(fileLocation)"
  }],
  "prId": 21669
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "indent",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-09-05T21:28:27Z",
    "diffHunk": "@@ -0,0 +1,182 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features.hadoopsteps\n+\n+import java.io.File\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model.{ConfigMap, ConfigMapBuilder, ContainerBuilder, KeyToPathBuilder, PodBuilder}\n+\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkPod\n+import org.apache.spark.deploy.k8s.security.KubernetesHadoopDelegationTokenManager\n+\n+private[spark] object HadoopBootstrapUtil {\n+\n+   /**\n+    * Mounting the DT secret for both the Driver and the executors\n+    *\n+    * @param dtSecretName Name of the secret that stores the Delegation Token\n+    * @param dtSecretItemKey Name of the Item Key storing the Delegation Token\n+    * @param userName Name of the SparkUser to set SPARK_USER\n+    * @param fileLocation Location of the krb5 file\n+    * @param krb5ConfName Name of the ConfigMap for Krb5\n+    * @param pod Input pod to be appended to\n+    * @return a modified SparkPod\n+    */\n+  def bootstrapKerberosPod(\n+      dtSecretName: String,\n+      dtSecretItemKey: String,\n+      userName: String,\n+      fileLocation: String,\n+      krb5ConfName: String,\n+      pod: SparkPod) : SparkPod = {\n+      val krb5File = new File(fileLocation)\n+      val fileStringPath = krb5File.toPath.getFileName.toString\n+      val kerberizedPod = new PodBuilder(pod.pod)\n+        .editOrNewSpec()\n+          .addNewVolume()\n+            .withName(SPARK_APP_HADOOP_SECRET_VOLUME_NAME)\n+            .withNewSecret()\n+              .withSecretName(dtSecretName)\n+              .endSecret()\n+            .endVolume()\n+          .addNewVolume()\n+            .withName(KRB_FILE_VOLUME)\n+              .withNewConfigMap()\n+                .withName(krb5ConfName)\n+                .withItems(new KeyToPathBuilder()\n+                  .withKey(fileStringPath)\n+                  .withPath(fileStringPath)\n+                  .build())\n+                .endConfigMap()\n+              .endVolume()\n+          .endSpec()\n+        .build()\n+      val kerberizedContainer = new ContainerBuilder(pod.container)\n+        .addNewVolumeMount()\n+          .withName(SPARK_APP_HADOOP_SECRET_VOLUME_NAME)\n+          .withMountPath(SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR)\n+          .endVolumeMount()\n+        .addNewVolumeMount()\n+          .withName(KRB_FILE_VOLUME)\n+          .withMountPath(KRB_FILE_DIR_PATH + \"/krb5.conf\")\n+          .withSubPath(\"krb5.conf\")\n+          .endVolumeMount()\n+        .addNewEnv()\n+          .withName(ENV_HADOOP_TOKEN_FILE_LOCATION)\n+          .withValue(s\"$SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR/$dtSecretItemKey\")\n+          .endEnv()\n+        .addNewEnv()\n+          .withName(ENV_SPARK_USER)\n+          .withValue(userName)\n+          .endEnv()\n+        .build()\n+    SparkPod(kerberizedPod, kerberizedContainer)\n+  }\n+\n+   /**\n+    * setting ENV_SPARK_USER when HADOOP_FILES are detected\n+    *\n+    * @param sparkUserName Name of the SPARK_USER\n+    * @param pod Input pod to be appended to\n+    * @return a modified SparkPod\n+    */\n+  def bootstrapSparkUserPod(\n+     sparkUserName: String,\n+     pod: SparkPod) : SparkPod = {\n+     val envModifiedContainer = new ContainerBuilder(pod.container)\n+       .addNewEnv()"
  }],
  "prId": 21669
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "this is actually indented 3 spaces from the `def`...",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-09-05T21:29:14Z",
    "diffHunk": "@@ -0,0 +1,182 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features.hadoopsteps\n+\n+import java.io.File\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model.{ConfigMap, ConfigMapBuilder, ContainerBuilder, KeyToPathBuilder, PodBuilder}\n+\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkPod\n+import org.apache.spark.deploy.k8s.security.KubernetesHadoopDelegationTokenManager\n+\n+private[spark] object HadoopBootstrapUtil {\n+\n+   /**\n+    * Mounting the DT secret for both the Driver and the executors\n+    *\n+    * @param dtSecretName Name of the secret that stores the Delegation Token\n+    * @param dtSecretItemKey Name of the Item Key storing the Delegation Token\n+    * @param userName Name of the SparkUser to set SPARK_USER\n+    * @param fileLocation Location of the krb5 file\n+    * @param krb5ConfName Name of the ConfigMap for Krb5\n+    * @param pod Input pod to be appended to\n+    * @return a modified SparkPod\n+    */\n+  def bootstrapKerberosPod(\n+      dtSecretName: String,\n+      dtSecretItemKey: String,\n+      userName: String,\n+      fileLocation: String,\n+      krb5ConfName: String,\n+      pod: SparkPod) : SparkPod = {\n+      val krb5File = new File(fileLocation)\n+      val fileStringPath = krb5File.toPath.getFileName.toString\n+      val kerberizedPod = new PodBuilder(pod.pod)\n+        .editOrNewSpec()\n+          .addNewVolume()\n+            .withName(SPARK_APP_HADOOP_SECRET_VOLUME_NAME)\n+            .withNewSecret()\n+              .withSecretName(dtSecretName)\n+              .endSecret()\n+            .endVolume()\n+          .addNewVolume()\n+            .withName(KRB_FILE_VOLUME)\n+              .withNewConfigMap()\n+                .withName(krb5ConfName)\n+                .withItems(new KeyToPathBuilder()\n+                  .withKey(fileStringPath)\n+                  .withPath(fileStringPath)\n+                  .build())\n+                .endConfigMap()\n+              .endVolume()\n+          .endSpec()\n+        .build()\n+      val kerberizedContainer = new ContainerBuilder(pod.container)\n+        .addNewVolumeMount()\n+          .withName(SPARK_APP_HADOOP_SECRET_VOLUME_NAME)\n+          .withMountPath(SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR)\n+          .endVolumeMount()\n+        .addNewVolumeMount()\n+          .withName(KRB_FILE_VOLUME)\n+          .withMountPath(KRB_FILE_DIR_PATH + \"/krb5.conf\")\n+          .withSubPath(\"krb5.conf\")\n+          .endVolumeMount()\n+        .addNewEnv()\n+          .withName(ENV_HADOOP_TOKEN_FILE_LOCATION)\n+          .withValue(s\"$SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR/$dtSecretItemKey\")\n+          .endEnv()\n+        .addNewEnv()\n+          .withName(ENV_SPARK_USER)\n+          .withValue(userName)\n+          .endEnv()\n+        .build()\n+    SparkPod(kerberizedPod, kerberizedContainer)\n+  }\n+\n+   /**\n+    * setting ENV_SPARK_USER when HADOOP_FILES are detected\n+    *\n+    * @param sparkUserName Name of the SPARK_USER\n+    * @param pod Input pod to be appended to\n+    * @return a modified SparkPod\n+    */\n+  def bootstrapSparkUserPod(\n+     sparkUserName: String,"
  }],
  "prId": 21669
}, {
  "comments": [{
    "author": {
      "login": "liyinan926"
    },
    "body": "Instead of using `hadoopsteps`, you should probably use `hadooputils` as the package seems to contain only utility classes.",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-10-01T16:12:35Z",
    "diffHunk": "@@ -0,0 +1,182 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features.hadoopsteps"
  }],
  "prId": 21669
}, {
  "comments": [{
    "author": {
      "login": "liyinan926"
    },
    "body": "Wondering can we use the same secret to mount both the DT and `krb5.conf`? Currently this PR mounts `krb5.conf` using a ConfigMap to the default location `/etc/krb5.conf`. But it looks to me that `krb5.conf` should really be treated as credential information and probably better be bundled with the DT.",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-10-01T17:01:01Z",
    "diffHunk": "@@ -0,0 +1,182 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features.hadoopsteps\n+\n+import java.io.File\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model.{ConfigMap, ConfigMapBuilder, ContainerBuilder, KeyToPathBuilder, PodBuilder}\n+\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkPod\n+import org.apache.spark.deploy.k8s.security.KubernetesHadoopDelegationTokenManager\n+\n+private[spark] object HadoopBootstrapUtil {\n+\n+   /**\n+    * Mounting the DT secret for both the Driver and the executors\n+    *\n+    * @param dtSecretName Name of the secret that stores the Delegation Token\n+    * @param dtSecretItemKey Name of the Item Key storing the Delegation Token\n+    * @param userName Name of the SparkUser to set SPARK_USER\n+    * @param fileLocation Location of the krb5 file\n+    * @param krb5ConfName Name of the ConfigMap for Krb5\n+    * @param pod Input pod to be appended to\n+    * @return a modified SparkPod\n+    */\n+  def bootstrapKerberosPod(\n+    dtSecretName: String,\n+    dtSecretItemKey: String,\n+    userName: String,\n+    fileLocation: String,\n+    krb5ConfName: String,\n+    pod: SparkPod) : SparkPod = {\n+    val krb5File = new File(fileLocation)\n+    val fileStringPath = krb5File.toPath.getFileName.toString\n+    val kerberizedPod = new PodBuilder(pod.pod)\n+      .editOrNewSpec()\n+        .addNewVolume()\n+          .withName(SPARK_APP_HADOOP_SECRET_VOLUME_NAME)\n+          .withNewSecret()\n+            .withSecretName(dtSecretName)\n+            .endSecret()\n+          .endVolume()\n+        .addNewVolume()\n+          .withName(KRB_FILE_VOLUME)"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Another option that is agnostic to the above idea on using the same secret is: if you allow users to use a pre-existing secret to mount in the DT in option 3, you should also allow users to use a pre-existing ConfigMap carrying the `krb5.conf`. All 3 options of the current implementation require users to pass in the location of a submission local `krb5.conf`, which seems odd for option 3.  ",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-10-02T03:52:32Z",
    "diffHunk": "@@ -0,0 +1,182 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features.hadoopsteps\n+\n+import java.io.File\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model.{ConfigMap, ConfigMapBuilder, ContainerBuilder, KeyToPathBuilder, PodBuilder}\n+\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkPod\n+import org.apache.spark.deploy.k8s.security.KubernetesHadoopDelegationTokenManager\n+\n+private[spark] object HadoopBootstrapUtil {\n+\n+   /**\n+    * Mounting the DT secret for both the Driver and the executors\n+    *\n+    * @param dtSecretName Name of the secret that stores the Delegation Token\n+    * @param dtSecretItemKey Name of the Item Key storing the Delegation Token\n+    * @param userName Name of the SparkUser to set SPARK_USER\n+    * @param fileLocation Location of the krb5 file\n+    * @param krb5ConfName Name of the ConfigMap for Krb5\n+    * @param pod Input pod to be appended to\n+    * @return a modified SparkPod\n+    */\n+  def bootstrapKerberosPod(\n+    dtSecretName: String,\n+    dtSecretItemKey: String,\n+    userName: String,\n+    fileLocation: String,\n+    krb5ConfName: String,\n+    pod: SparkPod) : SparkPod = {\n+    val krb5File = new File(fileLocation)\n+    val fileStringPath = krb5File.toPath.getFileName.toString\n+    val kerberizedPod = new PodBuilder(pod.pod)\n+      .editOrNewSpec()\n+        .addNewVolume()\n+          .withName(SPARK_APP_HADOOP_SECRET_VOLUME_NAME)\n+          .withNewSecret()\n+            .withSecretName(dtSecretName)\n+            .endSecret()\n+          .endVolume()\n+        .addNewVolume()\n+          .withName(KRB_FILE_VOLUME)"
  }],
  "prId": 21669
}, {
  "comments": [{
    "author": {
      "login": "liyinan926"
    },
    "body": "The doc here is not accurate.",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-10-01T17:02:33Z",
    "diffHunk": "@@ -0,0 +1,182 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features.hadoopsteps\n+\n+import java.io.File\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model.{ConfigMap, ConfigMapBuilder, ContainerBuilder, KeyToPathBuilder, PodBuilder}\n+\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkPod\n+import org.apache.spark.deploy.k8s.security.KubernetesHadoopDelegationTokenManager\n+\n+private[spark] object HadoopBootstrapUtil {\n+\n+   /**\n+    * Mounting the DT secret for both the Driver and the executors\n+    *\n+    * @param dtSecretName Name of the secret that stores the Delegation Token\n+    * @param dtSecretItemKey Name of the Item Key storing the Delegation Token\n+    * @param userName Name of the SparkUser to set SPARK_USER\n+    * @param fileLocation Location of the krb5 file\n+    * @param krb5ConfName Name of the ConfigMap for Krb5\n+    * @param pod Input pod to be appended to\n+    * @return a modified SparkPod\n+    */\n+  def bootstrapKerberosPod(\n+    dtSecretName: String,\n+    dtSecretItemKey: String,\n+    userName: String,\n+    fileLocation: String,\n+    krb5ConfName: String,\n+    pod: SparkPod) : SparkPod = {\n+    val krb5File = new File(fileLocation)\n+    val fileStringPath = krb5File.toPath.getFileName.toString\n+    val kerberizedPod = new PodBuilder(pod.pod)\n+      .editOrNewSpec()\n+        .addNewVolume()\n+          .withName(SPARK_APP_HADOOP_SECRET_VOLUME_NAME)\n+          .withNewSecret()\n+            .withSecretName(dtSecretName)\n+            .endSecret()\n+          .endVolume()\n+        .addNewVolume()\n+          .withName(KRB_FILE_VOLUME)\n+            .withNewConfigMap()\n+              .withName(krb5ConfName)\n+              .withItems(new KeyToPathBuilder()\n+                .withKey(fileStringPath)\n+                .withPath(fileStringPath)\n+                .build())\n+              .endConfigMap()\n+            .endVolume()\n+        .endSpec()\n+      .build()\n+    val kerberizedContainer = new ContainerBuilder(pod.container)\n+      .addNewVolumeMount()\n+        .withName(SPARK_APP_HADOOP_SECRET_VOLUME_NAME)\n+        .withMountPath(SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR)\n+        .endVolumeMount()\n+      .addNewVolumeMount()\n+        .withName(KRB_FILE_VOLUME)\n+        .withMountPath(KRB_FILE_DIR_PATH + \"/krb5.conf\")\n+        .withSubPath(\"krb5.conf\")\n+        .endVolumeMount()\n+      .addNewEnv()\n+        .withName(ENV_HADOOP_TOKEN_FILE_LOCATION)\n+        .withValue(s\"$SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR/$dtSecretItemKey\")\n+        .endEnv()\n+      .addNewEnv()\n+        .withName(ENV_SPARK_USER)\n+        .withValue(userName)\n+        .endEnv()\n+      .build()\n+    SparkPod(kerberizedPod, kerberizedContainer)\n+  }\n+\n+   /**\n+    * setting ENV_SPARK_USER when HADOOP_FILES are detected\n+    *\n+    * @param sparkUserName Name of the SPARK_USER\n+    * @param pod Input pod to be appended to\n+    * @return a modified SparkPod\n+    */\n+  def bootstrapSparkUserPod(\n+    sparkUserName: String,\n+    pod: SparkPod) : SparkPod = {\n+    val envModifiedContainer = new ContainerBuilder(pod.container)\n+      .addNewEnv()\n+        .withName(ENV_SPARK_USER)\n+        .withValue(sparkUserName)\n+        .endEnv()\n+      .build()\n+    SparkPod(pod.pod, envModifiedContainer)\n+  }\n+\n+   /**\n+    * Bootstraping the container with ConfigMaps that store\n+    * Hadoop configuration files\n+    *\n+    * @param hadoopConfDir location of HADOOP_CONF_DIR\n+    * @param hadoopConfigMapName name of the configMap for HADOOP_CONF_DIR\n+    * @param kubeTokenManager KubernetesHadoopDelegationTokenManager\n+    * @param pod Input pod to be appended to\n+    * @return a modified SparkPod\n+    */\n+  def bootstrapHadoopConfDir(\n+    hadoopConfDir: String,\n+    hadoopConfigMapName: String,\n+    kubeTokenManager: KubernetesHadoopDelegationTokenManager,\n+    pod: SparkPod) : SparkPod = {\n+      val hadoopConfigFiles =\n+        kubeTokenManager.getHadoopConfFiles(hadoopConfDir)\n+      val keyPaths = hadoopConfigFiles.map { file =>\n+        val fileStringPath = file.toPath.getFileName.toString\n+        new KeyToPathBuilder()\n+          .withKey(fileStringPath)\n+          .withPath(fileStringPath)\n+          .build() }\n+\n+      val hadoopSupportedPod = new PodBuilder(pod.pod)\n+        .editSpec()\n+          .addNewVolume()\n+            .withName(HADOOP_FILE_VOLUME)\n+            .withNewConfigMap()\n+              .withName(hadoopConfigMapName)\n+              .withItems(keyPaths.asJava)\n+              .endConfigMap()\n+            .endVolume()\n+          .endSpec()\n+        .build()\n+\n+      val hadoopSupportedContainer = new ContainerBuilder(pod.container)\n+        .addNewVolumeMount()\n+          .withName(HADOOP_FILE_VOLUME)\n+          .withMountPath(HADOOP_CONF_DIR_PATH)\n+          .endVolumeMount()\n+        .addNewEnv()\n+          .withName(ENV_HADOOP_CONF_DIR)\n+          .withValue(HADOOP_CONF_DIR_PATH)\n+          .endEnv()\n+        .build()\n+      SparkPod(hadoopSupportedPod, hadoopSupportedContainer)\n+    }\n+     /**\n+      * bootstraping the container with ConfigMaps that store\n+      * Hadoop configuration files"
  }],
  "prId": 21669
}, {
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "Args can all be on the same line as the function declaration.",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-10-04T00:17:13Z",
    "diffHunk": "@@ -0,0 +1,182 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features.hadoopsteps\n+\n+import java.io.File\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model.{ConfigMap, ConfigMapBuilder, ContainerBuilder, KeyToPathBuilder, PodBuilder}\n+\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkPod\n+import org.apache.spark.deploy.k8s.security.KubernetesHadoopDelegationTokenManager\n+\n+private[spark] object HadoopBootstrapUtil {\n+\n+   /**\n+    * Mounting the DT secret for both the Driver and the executors\n+    *\n+    * @param dtSecretName Name of the secret that stores the Delegation Token\n+    * @param dtSecretItemKey Name of the Item Key storing the Delegation Token\n+    * @param userName Name of the SparkUser to set SPARK_USER\n+    * @param fileLocation Location of the krb5 file\n+    * @param krb5ConfName Name of the ConfigMap for Krb5\n+    * @param pod Input pod to be appended to\n+    * @return a modified SparkPod\n+    */\n+  def bootstrapKerberosPod(\n+    dtSecretName: String,\n+    dtSecretItemKey: String,\n+    userName: String,\n+    fileLocation: String,\n+    krb5ConfName: String,\n+    pod: SparkPod) : SparkPod = {\n+    val krb5File = new File(fileLocation)\n+    val fileStringPath = krb5File.toPath.getFileName.toString\n+    val kerberizedPod = new PodBuilder(pod.pod)\n+      .editOrNewSpec()\n+        .addNewVolume()\n+          .withName(SPARK_APP_HADOOP_SECRET_VOLUME_NAME)\n+          .withNewSecret()\n+            .withSecretName(dtSecretName)\n+            .endSecret()\n+          .endVolume()\n+        .addNewVolume()\n+          .withName(KRB_FILE_VOLUME)\n+            .withNewConfigMap()\n+              .withName(krb5ConfName)\n+              .withItems(new KeyToPathBuilder()\n+                .withKey(fileStringPath)\n+                .withPath(fileStringPath)\n+                .build())\n+              .endConfigMap()\n+            .endVolume()\n+        .endSpec()\n+      .build()\n+    val kerberizedContainer = new ContainerBuilder(pod.container)\n+      .addNewVolumeMount()\n+        .withName(SPARK_APP_HADOOP_SECRET_VOLUME_NAME)\n+        .withMountPath(SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR)\n+        .endVolumeMount()\n+      .addNewVolumeMount()\n+        .withName(KRB_FILE_VOLUME)\n+        .withMountPath(KRB_FILE_DIR_PATH + \"/krb5.conf\")\n+        .withSubPath(\"krb5.conf\")\n+        .endVolumeMount()\n+      .addNewEnv()\n+        .withName(ENV_HADOOP_TOKEN_FILE_LOCATION)\n+        .withValue(s\"$SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR/$dtSecretItemKey\")\n+        .endEnv()\n+      .addNewEnv()\n+        .withName(ENV_SPARK_USER)\n+        .withValue(userName)\n+        .endEnv()\n+      .build()\n+    SparkPod(kerberizedPod, kerberizedContainer)\n+  }\n+\n+   /**\n+    * setting ENV_SPARK_USER when HADOOP_FILES are detected\n+    *\n+    * @param sparkUserName Name of the SPARK_USER\n+    * @param pod Input pod to be appended to\n+    * @return a modified SparkPod\n+    */\n+  def bootstrapSparkUserPod(\n+    sparkUserName: String,"
  }],
  "prId": 21669
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "need an empty line, indentation is off.",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-10-05T18:22:35Z",
    "diffHunk": "@@ -0,0 +1,182 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features.hadoopsteps\n+\n+import java.io.File\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model.{ConfigMap, ConfigMapBuilder, ContainerBuilder, KeyToPathBuilder, PodBuilder}\n+\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkPod\n+import org.apache.spark.deploy.k8s.security.KubernetesHadoopDelegationTokenManager\n+\n+private[spark] object HadoopBootstrapUtil {\n+\n+   /**\n+    * Mounting the DT secret for both the Driver and the executors\n+    *\n+    * @param dtSecretName Name of the secret that stores the Delegation Token\n+    * @param dtSecretItemKey Name of the Item Key storing the Delegation Token\n+    * @param userName Name of the SparkUser to set SPARK_USER\n+    * @param fileLocation Location of the krb5 file\n+    * @param krb5ConfName Name of the ConfigMap for Krb5\n+    * @param pod Input pod to be appended to\n+    * @return a modified SparkPod\n+    */\n+  def bootstrapKerberosPod(\n+    dtSecretName: String,\n+    dtSecretItemKey: String,\n+    userName: String,\n+    fileLocation: String,\n+    krb5ConfName: String,\n+    pod: SparkPod) : SparkPod = {\n+    val krb5File = new File(fileLocation)\n+    val fileStringPath = krb5File.toPath.getFileName.toString\n+    val kerberizedPod = new PodBuilder(pod.pod)\n+      .editOrNewSpec()\n+        .addNewVolume()\n+          .withName(SPARK_APP_HADOOP_SECRET_VOLUME_NAME)\n+          .withNewSecret()\n+            .withSecretName(dtSecretName)\n+            .endSecret()\n+          .endVolume()\n+        .addNewVolume()\n+          .withName(KRB_FILE_VOLUME)\n+            .withNewConfigMap()\n+              .withName(krb5ConfName)\n+              .withItems(new KeyToPathBuilder()\n+                .withKey(fileStringPath)\n+                .withPath(fileStringPath)\n+                .build())\n+              .endConfigMap()\n+            .endVolume()\n+        .endSpec()\n+      .build()\n+    val kerberizedContainer = new ContainerBuilder(pod.container)\n+      .addNewVolumeMount()\n+        .withName(SPARK_APP_HADOOP_SECRET_VOLUME_NAME)\n+        .withMountPath(SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR)\n+        .endVolumeMount()\n+      .addNewVolumeMount()\n+        .withName(KRB_FILE_VOLUME)\n+        .withMountPath(KRB_FILE_DIR_PATH + \"/krb5.conf\")\n+        .withSubPath(\"krb5.conf\")\n+        .endVolumeMount()\n+      .addNewEnv()\n+        .withName(ENV_HADOOP_TOKEN_FILE_LOCATION)\n+        .withValue(s\"$SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR/$dtSecretItemKey\")\n+        .endEnv()\n+      .addNewEnv()\n+        .withName(ENV_SPARK_USER)\n+        .withValue(userName)\n+        .endEnv()\n+      .build()\n+    SparkPod(kerberizedPod, kerberizedContainer)\n+  }\n+\n+   /**\n+    * setting ENV_SPARK_USER when HADOOP_FILES are detected\n+    *\n+    * @param sparkUserName Name of the SPARK_USER\n+    * @param pod Input pod to be appended to\n+    * @return a modified SparkPod\n+    */\n+  def bootstrapSparkUserPod(\n+    sparkUserName: String,\n+    pod: SparkPod) : SparkPod = {\n+    val envModifiedContainer = new ContainerBuilder(pod.container)\n+      .addNewEnv()\n+        .withName(ENV_SPARK_USER)\n+        .withValue(sparkUserName)\n+        .endEnv()\n+      .build()\n+    SparkPod(pod.pod, envModifiedContainer)\n+  }\n+\n+   /**\n+    * Bootstraping the container with ConfigMaps that store\n+    * Hadoop configuration files\n+    *\n+    * @param hadoopConfDir location of HADOOP_CONF_DIR\n+    * @param hadoopConfigMapName name of the configMap for HADOOP_CONF_DIR\n+    * @param kubeTokenManager KubernetesHadoopDelegationTokenManager\n+    * @param pod Input pod to be appended to\n+    * @return a modified SparkPod\n+    */\n+  def bootstrapHadoopConfDir(\n+    hadoopConfDir: String,\n+    hadoopConfigMapName: String,\n+    kubeTokenManager: KubernetesHadoopDelegationTokenManager,\n+    pod: SparkPod) : SparkPod = {\n+      val hadoopConfigFiles =\n+        kubeTokenManager.getHadoopConfFiles(hadoopConfDir)\n+      val keyPaths = hadoopConfigFiles.map { file =>\n+        val fileStringPath = file.toPath.getFileName.toString\n+        new KeyToPathBuilder()\n+          .withKey(fileStringPath)\n+          .withPath(fileStringPath)\n+          .build() }\n+\n+      val hadoopSupportedPod = new PodBuilder(pod.pod)\n+        .editSpec()\n+          .addNewVolume()\n+            .withName(HADOOP_FILE_VOLUME)\n+            .withNewConfigMap()\n+              .withName(hadoopConfigMapName)\n+              .withItems(keyPaths.asJava)\n+              .endConfigMap()\n+            .endVolume()\n+          .endSpec()\n+        .build()\n+\n+      val hadoopSupportedContainer = new ContainerBuilder(pod.container)\n+        .addNewVolumeMount()\n+          .withName(HADOOP_FILE_VOLUME)\n+          .withMountPath(HADOOP_CONF_DIR_PATH)\n+          .endVolumeMount()\n+        .addNewEnv()\n+          .withName(ENV_HADOOP_CONF_DIR)\n+          .withValue(HADOOP_CONF_DIR_PATH)\n+          .endEnv()\n+        .build()\n+      SparkPod(hadoopSupportedPod, hadoopSupportedContainer)\n+    }\n+     /**"
  }],
  "prId": 21669
}]