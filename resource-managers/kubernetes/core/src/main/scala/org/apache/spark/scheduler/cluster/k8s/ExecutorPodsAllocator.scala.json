[{
  "comments": [{
    "author": {
      "login": "felixcheung"
    },
    "body": "trigerring -> trigering",
    "commit": "635326a88a19e21f7644dbc8a149aa2dd1cb917c",
    "createdAt": "2019-07-30T05:52:11Z",
    "diffHunk": "@@ -66,97 +66,167 @@ private[spark] class ExecutorPodsAllocator(\n   // snapshot yet. Mapped to the timestamp when they were created.\n   private val newlyCreatedExecutors = mutable.Map.empty[Long, Long]\n \n+  private val dynamicAllocationEnabled = Utils.isDynamicAllocationEnabled(conf)\n+\n+  private val hasPendingPods = new AtomicBoolean()\n+\n+  private var lastSnapshot = ExecutorPodsSnapshot(Nil)\n+\n   def start(applicationId: String): Unit = {\n     snapshotsStore.addSubscriber(podAllocationDelay) {\n       onNewSnapshots(applicationId, _)\n     }\n   }\n \n-  def setTotalExpectedExecutors(total: Int): Unit = totalExpectedExecutors.set(total)\n+  def setTotalExpectedExecutors(total: Int): Unit = {\n+    totalExpectedExecutors.set(total)\n+    if (!hasPendingPods.get()) {\n+      snapshotsStore.notifySubscribers()\n+    }\n+  }\n \n-  private def onNewSnapshots(applicationId: String, snapshots: Seq[ExecutorPodsSnapshot]): Unit = {\n+  private def onNewSnapshots(\n+      applicationId: String,\n+      snapshots: Seq[ExecutorPodsSnapshot]): Unit = synchronized {\n     newlyCreatedExecutors --= snapshots.flatMap(_.executorPods.keys)\n     // For all executors we've created against the API but have not seen in a snapshot\n     // yet - check the current time. If the current time has exceeded some threshold,\n     // assume that the pod was either never created (the API server never properly\n     // handled the creation request), or the API server created the pod but we missed\n     // both the creation and deletion events. In either case, delete the missing pod\n     // if possible, and mark such a pod to be rescheduled below.\n-    newlyCreatedExecutors.foreach { case (execId, timeCreated) =>\n-      val currentTime = clock.getTimeMillis()\n+    val currentTime = clock.getTimeMillis()\n+    val timedOut = newlyCreatedExecutors.flatMap { case (execId, timeCreated) =>\n       if (currentTime - timeCreated > podCreationTimeout) {\n-        logWarning(s\"Executor with id $execId was not detected in the Kubernetes\" +\n-          s\" cluster after $podCreationTimeout milliseconds despite the fact that a\" +\n-          \" previous allocation attempt tried to create it. The executor may have been\" +\n-          \" deleted but the application missed the deletion event.\")\n-\n-        if (shouldDeleteExecutors) {\n-          Utils.tryLogNonFatalError {\n-            kubernetesClient\n-              .pods()\n-              .withLabel(SPARK_APP_ID_LABEL, applicationId)\n-              .withLabel(SPARK_ROLE_LABEL, SPARK_POD_EXECUTOR_ROLE)\n-              .withLabel(SPARK_EXECUTOR_ID_LABEL, execId.toString)\n-              .delete()\n-          }\n-        }\n-        newlyCreatedExecutors -= execId\n+        Some(execId)\n       } else {\n         logDebug(s\"Executor with id $execId was not found in the Kubernetes cluster since it\" +\n           s\" was created ${currentTime - timeCreated} milliseconds ago.\")\n+        None\n+      }\n+    }\n+\n+    if (timedOut.nonEmpty) {\n+      logWarning(s\"Executors with ids ${timedOut.mkString(\",\")} were not detected in the\" +\n+        s\" Kubernetes cluster after $podCreationTimeout ms despite the fact that a previous\" +\n+        \" allocation attempt tried to create them. The executors may have been deleted but the\" +\n+        \" application missed the deletion event.\")\n+\n+      newlyCreatedExecutors --= timedOut\n+      if (shouldDeleteExecutors) {\n+        Utils.tryLogNonFatalError {\n+          kubernetesClient\n+            .pods()\n+            .withLabel(SPARK_APP_ID_LABEL, applicationId)\n+            .withLabel(SPARK_ROLE_LABEL, SPARK_POD_EXECUTOR_ROLE)\n+            .withLabelIn(SPARK_EXECUTOR_ID_LABEL, timedOut.toSeq.map(_.toString): _*)\n+            .delete()\n+        }\n       }\n     }\n \n     if (snapshots.nonEmpty) {\n-      // Only need to examine the cluster as of the latest snapshot, the \"current\" state, to see if\n-      // we need to allocate more executors or not.\n-      val latestSnapshot = snapshots.last\n-      val currentRunningExecutors = latestSnapshot.executorPods.values.count {\n-        case PodRunning(_) => true\n+      lastSnapshot = snapshots.last\n+    }\n+\n+    val currentRunningExecutors = lastSnapshot.executorPods.values.count {\n+      case PodRunning(_) => true\n+      case _ => false\n+    }\n+\n+    val currentPendingExecutors = lastSnapshot.executorPods\n+      .filter {\n+        case (_, PodPending(_)) => true\n         case _ => false\n       }\n-      val currentPendingExecutors = latestSnapshot.executorPods.values.count {\n-        case PodPending(_) => true\n-        case _ => false\n+      .map { case (id, _) => id }\n+\n+    if (snapshots.nonEmpty) {\n+      logDebug(s\"Pod allocation status: $currentRunningExecutors running, \" +\n+        s\"${currentPendingExecutors.size} pending, \" +\n+        s\"${newlyCreatedExecutors.size} unacknowledged.\")\n+    }\n+\n+    val currentTotalExpectedExecutors = totalExpectedExecutors.get\n+\n+    // This variable is used later to print some debug logs. It's updated when cleaning up\n+    // excess pod requests, since currentPendingExecutors is immutable.\n+    var knownPendingCount = currentPendingExecutors.size\n+\n+    // It's possible that we have outstanding pods that are outdated when dynamic allocation\n+    // decides to downscale the application. So check if we can release any pending pods early\n+    // instead of waiting for them to time out. Drop them first from the unacknowledged list,\n+    // then from the pending.\n+    //\n+    // TODO: with dynamic allocation off, handle edge cases if we end up with more running\n+    // executors than expected.\n+    val knownPodCount = currentRunningExecutors + currentPendingExecutors.size +\n+      newlyCreatedExecutors.size\n+    if (knownPodCount > currentTotalExpectedExecutors) {\n+      val excess = knownPodCount - currentTotalExpectedExecutors\n+      val knownPendingToDelete = currentPendingExecutors.take(excess - newlyCreatedExecutors.size)\n+      val toDelete = newlyCreatedExecutors.keys.take(excess).toList ++ knownPendingToDelete\n+\n+      if (toDelete.nonEmpty) {\n+        logInfo(s\"Deleting ${toDelete.size} excess pod requests (${toDelete.mkString(\",\")}).\")\n+        Utils.tryLogNonFatalError {\n+          kubernetesClient\n+            .pods()\n+            .withField(\"status.phase\", \"Pending\")\n+            .withLabel(SPARK_APP_ID_LABEL, applicationId)\n+            .withLabel(SPARK_ROLE_LABEL, SPARK_POD_EXECUTOR_ROLE)\n+            .withLabelIn(SPARK_EXECUTOR_ID_LABEL, toDelete.sorted.map(_.toString): _*)\n+            .delete()\n+          newlyCreatedExecutors --= toDelete\n+          knownPendingCount -= knownPendingToDelete.size\n+        }\n       }\n-      val currentTotalExpectedExecutors = totalExpectedExecutors.get\n-      logDebug(s\"Currently have $currentRunningExecutors running executors and\" +\n-        s\" $currentPendingExecutors pending executors. $newlyCreatedExecutors executors\" +\n-        s\" have been requested but are pending appearance in the cluster.\")\n-      if (newlyCreatedExecutors.isEmpty\n-        && currentPendingExecutors == 0\n+    }\n+\n+    if (newlyCreatedExecutors.isEmpty\n+        && currentPendingExecutors.isEmpty\n         && currentRunningExecutors < currentTotalExpectedExecutors) {\n-        val numExecutorsToAllocate = math.min(\n-          currentTotalExpectedExecutors - currentRunningExecutors, podAllocationSize)\n-        logInfo(s\"Going to request $numExecutorsToAllocate executors from Kubernetes.\")\n-        for ( _ <- 0 until numExecutorsToAllocate) {\n-          val newExecutorId = EXECUTOR_ID_COUNTER.incrementAndGet()\n-          val executorConf = KubernetesConf.createExecutorConf(\n-            conf,\n-            newExecutorId.toString,\n-            applicationId,\n-            driverPod)\n-          val executorPod = executorBuilder.buildFromFeatures(executorConf, secMgr,\n-            kubernetesClient)\n-          val podWithAttachedContainer = new PodBuilder(executorPod.pod)\n-            .editOrNewSpec()\n-            .addToContainers(executorPod.container)\n-            .endSpec()\n-            .build()\n-          kubernetesClient.pods().create(podWithAttachedContainer)\n-          newlyCreatedExecutors(newExecutorId) = clock.getTimeMillis()\n-          logDebug(s\"Requested executor with id $newExecutorId from Kubernetes.\")\n-        }\n-      } else if (currentRunningExecutors >= currentTotalExpectedExecutors) {\n-        // TODO handle edge cases if we end up with more running executors than expected.\n-        logDebug(\"Current number of running executors is equal to the number of requested\" +\n-          \" executors. Not scaling up further.\")\n-      } else if (newlyCreatedExecutors.nonEmpty || currentPendingExecutors != 0) {\n-        logDebug(s\"Still waiting for ${newlyCreatedExecutors.size + currentPendingExecutors}\" +\n-          s\" executors to begin running before requesting for more executors. # of executors in\" +\n-          s\" pending status in the cluster: $currentPendingExecutors. # of executors that we have\" +\n-          s\" created but we have not observed as being present in the cluster yet:\" +\n-          s\" ${newlyCreatedExecutors.size}.\")\n+      val numExecutorsToAllocate = math.min(\n+        currentTotalExpectedExecutors - currentRunningExecutors, podAllocationSize)\n+      logInfo(s\"Going to request $numExecutorsToAllocate executors from Kubernetes.\")\n+      for ( _ <- 0 until numExecutorsToAllocate) {\n+        val newExecutorId = EXECUTOR_ID_COUNTER.incrementAndGet()\n+        val executorConf = KubernetesConf.createExecutorConf(\n+          conf,\n+          newExecutorId.toString,\n+          applicationId,\n+          driverPod)\n+        val executorPod = executorBuilder.buildFromFeatures(executorConf, secMgr,\n+          kubernetesClient)\n+        val podWithAttachedContainer = new PodBuilder(executorPod.pod)\n+          .editOrNewSpec()\n+          .addToContainers(executorPod.container)\n+          .endSpec()\n+          .build()\n+        kubernetesClient.pods().create(podWithAttachedContainer)\n+        newlyCreatedExecutors(newExecutorId) = clock.getTimeMillis()\n+        logDebug(s\"Requested executor with id $newExecutorId from Kubernetes.\")\n+      }\n+    }\n+\n+    // Update the flag that helps the setTotalExpectedExecutors() callback avoid trigerring this"
  }],
  "prId": 25236
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "minor: on a quick read, the naming of these variables is a bit confusing on whether its a list of execs or just a count -- would be nice to have the counts consistently use `...Count` or `num...`",
    "commit": "635326a88a19e21f7644dbc8a149aa2dd1cb917c",
    "createdAt": "2019-08-01T14:33:50Z",
    "diffHunk": "@@ -66,97 +66,167 @@ private[spark] class ExecutorPodsAllocator(\n   // snapshot yet. Mapped to the timestamp when they were created.\n   private val newlyCreatedExecutors = mutable.Map.empty[Long, Long]\n \n+  private val dynamicAllocationEnabled = Utils.isDynamicAllocationEnabled(conf)\n+\n+  private val hasPendingPods = new AtomicBoolean()\n+\n+  private var lastSnapshot = ExecutorPodsSnapshot(Nil)\n+\n   def start(applicationId: String): Unit = {\n     snapshotsStore.addSubscriber(podAllocationDelay) {\n       onNewSnapshots(applicationId, _)\n     }\n   }\n \n-  def setTotalExpectedExecutors(total: Int): Unit = totalExpectedExecutors.set(total)\n+  def setTotalExpectedExecutors(total: Int): Unit = {\n+    totalExpectedExecutors.set(total)\n+    if (!hasPendingPods.get()) {\n+      snapshotsStore.notifySubscribers()\n+    }\n+  }\n \n-  private def onNewSnapshots(applicationId: String, snapshots: Seq[ExecutorPodsSnapshot]): Unit = {\n+  private def onNewSnapshots(\n+      applicationId: String,\n+      snapshots: Seq[ExecutorPodsSnapshot]): Unit = synchronized {\n     newlyCreatedExecutors --= snapshots.flatMap(_.executorPods.keys)\n     // For all executors we've created against the API but have not seen in a snapshot\n     // yet - check the current time. If the current time has exceeded some threshold,\n     // assume that the pod was either never created (the API server never properly\n     // handled the creation request), or the API server created the pod but we missed\n     // both the creation and deletion events. In either case, delete the missing pod\n     // if possible, and mark such a pod to be rescheduled below.\n-    newlyCreatedExecutors.foreach { case (execId, timeCreated) =>\n-      val currentTime = clock.getTimeMillis()\n+    val currentTime = clock.getTimeMillis()\n+    val timedOut = newlyCreatedExecutors.flatMap { case (execId, timeCreated) =>\n       if (currentTime - timeCreated > podCreationTimeout) {\n-        logWarning(s\"Executor with id $execId was not detected in the Kubernetes\" +\n-          s\" cluster after $podCreationTimeout milliseconds despite the fact that a\" +\n-          \" previous allocation attempt tried to create it. The executor may have been\" +\n-          \" deleted but the application missed the deletion event.\")\n-\n-        if (shouldDeleteExecutors) {\n-          Utils.tryLogNonFatalError {\n-            kubernetesClient\n-              .pods()\n-              .withLabel(SPARK_APP_ID_LABEL, applicationId)\n-              .withLabel(SPARK_ROLE_LABEL, SPARK_POD_EXECUTOR_ROLE)\n-              .withLabel(SPARK_EXECUTOR_ID_LABEL, execId.toString)\n-              .delete()\n-          }\n-        }\n-        newlyCreatedExecutors -= execId\n+        Some(execId)\n       } else {\n         logDebug(s\"Executor with id $execId was not found in the Kubernetes cluster since it\" +\n           s\" was created ${currentTime - timeCreated} milliseconds ago.\")\n+        None\n+      }\n+    }\n+\n+    if (timedOut.nonEmpty) {\n+      logWarning(s\"Executors with ids ${timedOut.mkString(\",\")} were not detected in the\" +\n+        s\" Kubernetes cluster after $podCreationTimeout ms despite the fact that a previous\" +\n+        \" allocation attempt tried to create them. The executors may have been deleted but the\" +\n+        \" application missed the deletion event.\")\n+\n+      newlyCreatedExecutors --= timedOut\n+      if (shouldDeleteExecutors) {\n+        Utils.tryLogNonFatalError {\n+          kubernetesClient\n+            .pods()\n+            .withLabel(SPARK_APP_ID_LABEL, applicationId)\n+            .withLabel(SPARK_ROLE_LABEL, SPARK_POD_EXECUTOR_ROLE)\n+            .withLabelIn(SPARK_EXECUTOR_ID_LABEL, timedOut.toSeq.map(_.toString): _*)\n+            .delete()\n+        }\n       }\n     }\n \n     if (snapshots.nonEmpty) {\n-      // Only need to examine the cluster as of the latest snapshot, the \"current\" state, to see if\n-      // we need to allocate more executors or not.\n-      val latestSnapshot = snapshots.last\n-      val currentRunningExecutors = latestSnapshot.executorPods.values.count {\n-        case PodRunning(_) => true\n+      lastSnapshot = snapshots.last\n+    }\n+\n+    val currentRunningExecutors = lastSnapshot.executorPods.values.count {"
  }],
  "prId": 25236
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "does sorting matter here?  don't see it mentioned on the k8s api, and you're not doing it above.",
    "commit": "635326a88a19e21f7644dbc8a149aa2dd1cb917c",
    "createdAt": "2019-08-01T14:42:29Z",
    "diffHunk": "@@ -66,97 +66,167 @@ private[spark] class ExecutorPodsAllocator(\n   // snapshot yet. Mapped to the timestamp when they were created.\n   private val newlyCreatedExecutors = mutable.Map.empty[Long, Long]\n \n+  private val dynamicAllocationEnabled = Utils.isDynamicAllocationEnabled(conf)\n+\n+  private val hasPendingPods = new AtomicBoolean()\n+\n+  private var lastSnapshot = ExecutorPodsSnapshot(Nil)\n+\n   def start(applicationId: String): Unit = {\n     snapshotsStore.addSubscriber(podAllocationDelay) {\n       onNewSnapshots(applicationId, _)\n     }\n   }\n \n-  def setTotalExpectedExecutors(total: Int): Unit = totalExpectedExecutors.set(total)\n+  def setTotalExpectedExecutors(total: Int): Unit = {\n+    totalExpectedExecutors.set(total)\n+    if (!hasPendingPods.get()) {\n+      snapshotsStore.notifySubscribers()\n+    }\n+  }\n \n-  private def onNewSnapshots(applicationId: String, snapshots: Seq[ExecutorPodsSnapshot]): Unit = {\n+  private def onNewSnapshots(\n+      applicationId: String,\n+      snapshots: Seq[ExecutorPodsSnapshot]): Unit = synchronized {\n     newlyCreatedExecutors --= snapshots.flatMap(_.executorPods.keys)\n     // For all executors we've created against the API but have not seen in a snapshot\n     // yet - check the current time. If the current time has exceeded some threshold,\n     // assume that the pod was either never created (the API server never properly\n     // handled the creation request), or the API server created the pod but we missed\n     // both the creation and deletion events. In either case, delete the missing pod\n     // if possible, and mark such a pod to be rescheduled below.\n-    newlyCreatedExecutors.foreach { case (execId, timeCreated) =>\n-      val currentTime = clock.getTimeMillis()\n+    val currentTime = clock.getTimeMillis()\n+    val timedOut = newlyCreatedExecutors.flatMap { case (execId, timeCreated) =>\n       if (currentTime - timeCreated > podCreationTimeout) {\n-        logWarning(s\"Executor with id $execId was not detected in the Kubernetes\" +\n-          s\" cluster after $podCreationTimeout milliseconds despite the fact that a\" +\n-          \" previous allocation attempt tried to create it. The executor may have been\" +\n-          \" deleted but the application missed the deletion event.\")\n-\n-        if (shouldDeleteExecutors) {\n-          Utils.tryLogNonFatalError {\n-            kubernetesClient\n-              .pods()\n-              .withLabel(SPARK_APP_ID_LABEL, applicationId)\n-              .withLabel(SPARK_ROLE_LABEL, SPARK_POD_EXECUTOR_ROLE)\n-              .withLabel(SPARK_EXECUTOR_ID_LABEL, execId.toString)\n-              .delete()\n-          }\n-        }\n-        newlyCreatedExecutors -= execId\n+        Some(execId)\n       } else {\n         logDebug(s\"Executor with id $execId was not found in the Kubernetes cluster since it\" +\n           s\" was created ${currentTime - timeCreated} milliseconds ago.\")\n+        None\n+      }\n+    }\n+\n+    if (timedOut.nonEmpty) {\n+      logWarning(s\"Executors with ids ${timedOut.mkString(\",\")} were not detected in the\" +\n+        s\" Kubernetes cluster after $podCreationTimeout ms despite the fact that a previous\" +\n+        \" allocation attempt tried to create them. The executors may have been deleted but the\" +\n+        \" application missed the deletion event.\")\n+\n+      newlyCreatedExecutors --= timedOut\n+      if (shouldDeleteExecutors) {\n+        Utils.tryLogNonFatalError {\n+          kubernetesClient\n+            .pods()\n+            .withLabel(SPARK_APP_ID_LABEL, applicationId)\n+            .withLabel(SPARK_ROLE_LABEL, SPARK_POD_EXECUTOR_ROLE)\n+            .withLabelIn(SPARK_EXECUTOR_ID_LABEL, timedOut.toSeq.map(_.toString): _*)\n+            .delete()\n+        }\n       }\n     }\n \n     if (snapshots.nonEmpty) {\n-      // Only need to examine the cluster as of the latest snapshot, the \"current\" state, to see if\n-      // we need to allocate more executors or not.\n-      val latestSnapshot = snapshots.last\n-      val currentRunningExecutors = latestSnapshot.executorPods.values.count {\n-        case PodRunning(_) => true\n+      lastSnapshot = snapshots.last\n+    }\n+\n+    val currentRunningExecutors = lastSnapshot.executorPods.values.count {\n+      case PodRunning(_) => true\n+      case _ => false\n+    }\n+\n+    val currentPendingExecutors = lastSnapshot.executorPods\n+      .filter {\n+        case (_, PodPending(_)) => true\n         case _ => false\n       }\n-      val currentPendingExecutors = latestSnapshot.executorPods.values.count {\n-        case PodPending(_) => true\n-        case _ => false\n+      .map { case (id, _) => id }\n+\n+    if (snapshots.nonEmpty) {\n+      logDebug(s\"Pod allocation status: $currentRunningExecutors running, \" +\n+        s\"${currentPendingExecutors.size} pending, \" +\n+        s\"${newlyCreatedExecutors.size} unacknowledged.\")\n+    }\n+\n+    val currentTotalExpectedExecutors = totalExpectedExecutors.get\n+\n+    // This variable is used later to print some debug logs. It's updated when cleaning up\n+    // excess pod requests, since currentPendingExecutors is immutable.\n+    var knownPendingCount = currentPendingExecutors.size\n+\n+    // It's possible that we have outstanding pods that are outdated when dynamic allocation\n+    // decides to downscale the application. So check if we can release any pending pods early\n+    // instead of waiting for them to time out. Drop them first from the unacknowledged list,\n+    // then from the pending.\n+    //\n+    // TODO: with dynamic allocation off, handle edge cases if we end up with more running\n+    // executors than expected.\n+    val knownPodCount = currentRunningExecutors + currentPendingExecutors.size +\n+      newlyCreatedExecutors.size\n+    if (knownPodCount > currentTotalExpectedExecutors) {\n+      val excess = knownPodCount - currentTotalExpectedExecutors\n+      val knownPendingToDelete = currentPendingExecutors.take(excess - newlyCreatedExecutors.size)\n+      val toDelete = newlyCreatedExecutors.keys.take(excess).toList ++ knownPendingToDelete\n+\n+      if (toDelete.nonEmpty) {\n+        logInfo(s\"Deleting ${toDelete.size} excess pod requests (${toDelete.mkString(\",\")}).\")\n+        Utils.tryLogNonFatalError {\n+          kubernetesClient\n+            .pods()\n+            .withField(\"status.phase\", \"Pending\")\n+            .withLabel(SPARK_APP_ID_LABEL, applicationId)\n+            .withLabel(SPARK_ROLE_LABEL, SPARK_POD_EXECUTOR_ROLE)\n+            .withLabelIn(SPARK_EXECUTOR_ID_LABEL, toDelete.sorted.map(_.toString): _*)",
    "line": 178
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Normally it doesn't matter, but it matters when using mocks in the tests (since this is a varargs call, not a parameter that takes a `Set`).",
    "commit": "635326a88a19e21f7644dbc8a149aa2dd1cb917c",
    "createdAt": "2019-08-01T20:53:35Z",
    "diffHunk": "@@ -66,97 +66,167 @@ private[spark] class ExecutorPodsAllocator(\n   // snapshot yet. Mapped to the timestamp when they were created.\n   private val newlyCreatedExecutors = mutable.Map.empty[Long, Long]\n \n+  private val dynamicAllocationEnabled = Utils.isDynamicAllocationEnabled(conf)\n+\n+  private val hasPendingPods = new AtomicBoolean()\n+\n+  private var lastSnapshot = ExecutorPodsSnapshot(Nil)\n+\n   def start(applicationId: String): Unit = {\n     snapshotsStore.addSubscriber(podAllocationDelay) {\n       onNewSnapshots(applicationId, _)\n     }\n   }\n \n-  def setTotalExpectedExecutors(total: Int): Unit = totalExpectedExecutors.set(total)\n+  def setTotalExpectedExecutors(total: Int): Unit = {\n+    totalExpectedExecutors.set(total)\n+    if (!hasPendingPods.get()) {\n+      snapshotsStore.notifySubscribers()\n+    }\n+  }\n \n-  private def onNewSnapshots(applicationId: String, snapshots: Seq[ExecutorPodsSnapshot]): Unit = {\n+  private def onNewSnapshots(\n+      applicationId: String,\n+      snapshots: Seq[ExecutorPodsSnapshot]): Unit = synchronized {\n     newlyCreatedExecutors --= snapshots.flatMap(_.executorPods.keys)\n     // For all executors we've created against the API but have not seen in a snapshot\n     // yet - check the current time. If the current time has exceeded some threshold,\n     // assume that the pod was either never created (the API server never properly\n     // handled the creation request), or the API server created the pod but we missed\n     // both the creation and deletion events. In either case, delete the missing pod\n     // if possible, and mark such a pod to be rescheduled below.\n-    newlyCreatedExecutors.foreach { case (execId, timeCreated) =>\n-      val currentTime = clock.getTimeMillis()\n+    val currentTime = clock.getTimeMillis()\n+    val timedOut = newlyCreatedExecutors.flatMap { case (execId, timeCreated) =>\n       if (currentTime - timeCreated > podCreationTimeout) {\n-        logWarning(s\"Executor with id $execId was not detected in the Kubernetes\" +\n-          s\" cluster after $podCreationTimeout milliseconds despite the fact that a\" +\n-          \" previous allocation attempt tried to create it. The executor may have been\" +\n-          \" deleted but the application missed the deletion event.\")\n-\n-        if (shouldDeleteExecutors) {\n-          Utils.tryLogNonFatalError {\n-            kubernetesClient\n-              .pods()\n-              .withLabel(SPARK_APP_ID_LABEL, applicationId)\n-              .withLabel(SPARK_ROLE_LABEL, SPARK_POD_EXECUTOR_ROLE)\n-              .withLabel(SPARK_EXECUTOR_ID_LABEL, execId.toString)\n-              .delete()\n-          }\n-        }\n-        newlyCreatedExecutors -= execId\n+        Some(execId)\n       } else {\n         logDebug(s\"Executor with id $execId was not found in the Kubernetes cluster since it\" +\n           s\" was created ${currentTime - timeCreated} milliseconds ago.\")\n+        None\n+      }\n+    }\n+\n+    if (timedOut.nonEmpty) {\n+      logWarning(s\"Executors with ids ${timedOut.mkString(\",\")} were not detected in the\" +\n+        s\" Kubernetes cluster after $podCreationTimeout ms despite the fact that a previous\" +\n+        \" allocation attempt tried to create them. The executors may have been deleted but the\" +\n+        \" application missed the deletion event.\")\n+\n+      newlyCreatedExecutors --= timedOut\n+      if (shouldDeleteExecutors) {\n+        Utils.tryLogNonFatalError {\n+          kubernetesClient\n+            .pods()\n+            .withLabel(SPARK_APP_ID_LABEL, applicationId)\n+            .withLabel(SPARK_ROLE_LABEL, SPARK_POD_EXECUTOR_ROLE)\n+            .withLabelIn(SPARK_EXECUTOR_ID_LABEL, timedOut.toSeq.map(_.toString): _*)\n+            .delete()\n+        }\n       }\n     }\n \n     if (snapshots.nonEmpty) {\n-      // Only need to examine the cluster as of the latest snapshot, the \"current\" state, to see if\n-      // we need to allocate more executors or not.\n-      val latestSnapshot = snapshots.last\n-      val currentRunningExecutors = latestSnapshot.executorPods.values.count {\n-        case PodRunning(_) => true\n+      lastSnapshot = snapshots.last\n+    }\n+\n+    val currentRunningExecutors = lastSnapshot.executorPods.values.count {\n+      case PodRunning(_) => true\n+      case _ => false\n+    }\n+\n+    val currentPendingExecutors = lastSnapshot.executorPods\n+      .filter {\n+        case (_, PodPending(_)) => true\n         case _ => false\n       }\n-      val currentPendingExecutors = latestSnapshot.executorPods.values.count {\n-        case PodPending(_) => true\n-        case _ => false\n+      .map { case (id, _) => id }\n+\n+    if (snapshots.nonEmpty) {\n+      logDebug(s\"Pod allocation status: $currentRunningExecutors running, \" +\n+        s\"${currentPendingExecutors.size} pending, \" +\n+        s\"${newlyCreatedExecutors.size} unacknowledged.\")\n+    }\n+\n+    val currentTotalExpectedExecutors = totalExpectedExecutors.get\n+\n+    // This variable is used later to print some debug logs. It's updated when cleaning up\n+    // excess pod requests, since currentPendingExecutors is immutable.\n+    var knownPendingCount = currentPendingExecutors.size\n+\n+    // It's possible that we have outstanding pods that are outdated when dynamic allocation\n+    // decides to downscale the application. So check if we can release any pending pods early\n+    // instead of waiting for them to time out. Drop them first from the unacknowledged list,\n+    // then from the pending.\n+    //\n+    // TODO: with dynamic allocation off, handle edge cases if we end up with more running\n+    // executors than expected.\n+    val knownPodCount = currentRunningExecutors + currentPendingExecutors.size +\n+      newlyCreatedExecutors.size\n+    if (knownPodCount > currentTotalExpectedExecutors) {\n+      val excess = knownPodCount - currentTotalExpectedExecutors\n+      val knownPendingToDelete = currentPendingExecutors.take(excess - newlyCreatedExecutors.size)\n+      val toDelete = newlyCreatedExecutors.keys.take(excess).toList ++ knownPendingToDelete\n+\n+      if (toDelete.nonEmpty) {\n+        logInfo(s\"Deleting ${toDelete.size} excess pod requests (${toDelete.mkString(\",\")}).\")\n+        Utils.tryLogNonFatalError {\n+          kubernetesClient\n+            .pods()\n+            .withField(\"status.phase\", \"Pending\")\n+            .withLabel(SPARK_APP_ID_LABEL, applicationId)\n+            .withLabel(SPARK_ROLE_LABEL, SPARK_POD_EXECUTOR_ROLE)\n+            .withLabelIn(SPARK_EXECUTOR_ID_LABEL, toDelete.sorted.map(_.toString): _*)",
    "line": 178
  }],
  "prId": 25236
}]