[{
  "comments": [{
    "author": {
      "login": "mridulm"
    },
    "body": "nit: `SPARK_ROLE_LABEL` also ?",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-27T07:44:56Z",
    "diffHunk": "@@ -0,0 +1,84 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.steps.{BaseDriverConfigurationStep, DriverConfigurationStep, DriverKubernetesCredentialsStep, DriverServiceBootstrapStep}\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * Constructs the complete list of driver configuration steps to run to deploy the Spark driver.\n+ */\n+private[spark] class DriverConfigurationStepsOrchestrator(\n+    namespace: String,\n+    kubernetesAppId: String,\n+    launchTime: Long,\n+    mainAppResource: MainAppResource,\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) {\n+\n+  // The resource name prefix is derived from the application name, making it easy to connect the\n+  // names of the Kubernetes resources from e.g. kubectl or the Kubernetes dashboard to the\n+  // application the user submitted. However, we can't use the application name in the label, as\n+  // label values are considerably restrictive, e.g. must be no longer than 63 characters in\n+  // length. So we generate a separate identifier for the app ID itself, and bookkeeping that\n+  // requires finding \"all pods for this application\" should use the kubernetesAppId.\n+  private val kubernetesResourceNamePrefix =\n+      s\"$appName-$launchTime\".toLowerCase.replaceAll(\"\\\\.\", \"-\")\n+  private val dockerImagePullPolicy = submissionSparkConf.get(DOCKER_IMAGE_PULL_POLICY)\n+\n+  def getAllConfigurationSteps(): Seq[DriverConfigurationStep] = {\n+    val driverCustomLabels = ConfigurationUtils.parsePrefixedKeyValuePairs(\n+      submissionSparkConf,\n+      KUBERNETES_DRIVER_LABEL_PREFIX)\n+    require(!driverCustomLabels.contains(SPARK_APP_ID_LABEL), \"Label with key \" +",
    "line": 60
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Yes, done.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-28T00:46:03Z",
    "diffHunk": "@@ -0,0 +1,84 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.steps.{BaseDriverConfigurationStep, DriverConfigurationStep, DriverKubernetesCredentialsStep, DriverServiceBootstrapStep}\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * Constructs the complete list of driver configuration steps to run to deploy the Spark driver.\n+ */\n+private[spark] class DriverConfigurationStepsOrchestrator(\n+    namespace: String,\n+    kubernetesAppId: String,\n+    launchTime: Long,\n+    mainAppResource: MainAppResource,\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) {\n+\n+  // The resource name prefix is derived from the application name, making it easy to connect the\n+  // names of the Kubernetes resources from e.g. kubectl or the Kubernetes dashboard to the\n+  // application the user submitted. However, we can't use the application name in the label, as\n+  // label values are considerably restrictive, e.g. must be no longer than 63 characters in\n+  // length. So we generate a separate identifier for the app ID itself, and bookkeeping that\n+  // requires finding \"all pods for this application\" should use the kubernetesAppId.\n+  private val kubernetesResourceNamePrefix =\n+      s\"$appName-$launchTime\".toLowerCase.replaceAll(\"\\\\.\", \"-\")\n+  private val dockerImagePullPolicy = submissionSparkConf.get(DOCKER_IMAGE_PULL_POLICY)\n+\n+  def getAllConfigurationSteps(): Seq[DriverConfigurationStep] = {\n+    val driverCustomLabels = ConfigurationUtils.parsePrefixedKeyValuePairs(\n+      submissionSparkConf,\n+      KUBERNETES_DRIVER_LABEL_PREFIX)\n+    require(!driverCustomLabels.contains(SPARK_APP_ID_LABEL), \"Label with key \" +",
    "line": 60
  }],
  "prId": 19717
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Is this required to be unique? If so, this could use some more uniqueness (e.g. using a UUID instead of the \"launch time\").",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-04T19:45:26Z",
    "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.steps._\n+import org.apache.spark.launcher.SparkLauncher\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * Constructs the complete list of driver configuration steps to run to deploy the Spark driver.\n+ */\n+private[spark] class DriverConfigurationStepsOrchestrator(\n+    namespace: String,\n+    kubernetesAppId: String,\n+    launchTime: Long,\n+    mainAppResource: Option[MainAppResource],\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) {\n+\n+  // The resource name prefix is derived from the application name, making it easy to connect the\n+  // names of the Kubernetes resources from e.g. kubectl or the Kubernetes dashboard to the\n+  // application the user submitted. However, we can't use the application name in the label, as\n+  // label values are considerably restrictive, e.g. must be no longer than 63 characters in\n+  // length. So we generate a separate identifier for the app ID itself, and bookkeeping that\n+  // requires finding \"all pods for this application\" should use the kubernetesAppId.\n+  private val kubernetesResourceNamePrefix ="
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Yes, it needs to be unique. Changed to use `UUID.nameUUIDFromBytes` based on `launchTime`. @mccheah any concern about this?",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-04T23:07:00Z",
    "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.steps._\n+import org.apache.spark.launcher.SparkLauncher\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * Constructs the complete list of driver configuration steps to run to deploy the Spark driver.\n+ */\n+private[spark] class DriverConfigurationStepsOrchestrator(\n+    namespace: String,\n+    kubernetesAppId: String,\n+    launchTime: Long,\n+    mainAppResource: Option[MainAppResource],\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) {\n+\n+  // The resource name prefix is derived from the application name, making it easy to connect the\n+  // names of the Kubernetes resources from e.g. kubectl or the Kubernetes dashboard to the\n+  // application the user submitted. However, we can't use the application name in the label, as\n+  // label values are considerably restrictive, e.g. must be no longer than 63 characters in\n+  // length. So we generate a separate identifier for the app ID itself, and bookkeeping that\n+  // requires finding \"all pods for this application\" should use the kubernetesAppId.\n+  private val kubernetesResourceNamePrefix ="
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "UUIDs are a bit long which leads to name length problems since Kubernetes requires some names to be < 63 characters. I would agree that a UUID would be better if we can get away with it.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-05T00:19:12Z",
    "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.steps._\n+import org.apache.spark.launcher.SparkLauncher\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * Constructs the complete list of driver configuration steps to run to deploy the Spark driver.\n+ */\n+private[spark] class DriverConfigurationStepsOrchestrator(\n+    namespace: String,\n+    kubernetesAppId: String,\n+    launchTime: Long,\n+    mainAppResource: Option[MainAppResource],\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) {\n+\n+  // The resource name prefix is derived from the application name, making it easy to connect the\n+  // names of the Kubernetes resources from e.g. kubectl or the Kubernetes dashboard to the\n+  // application the user submitted. However, we can't use the application name in the label, as\n+  // label values are considerably restrictive, e.g. must be no longer than 63 characters in\n+  // length. So we generate a separate identifier for the app ID itself, and bookkeeping that\n+  // requires finding \"all pods for this application\" should use the kubernetesAppId.\n+  private val kubernetesResourceNamePrefix ="
  }, {
    "author": {
      "login": "mridulm"
    },
    "body": "I cant see rest of the context of this conversation - but UUID will be 36 odd bytes, well within the 63 char limit ?",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-05T02:39:25Z",
    "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.steps._\n+import org.apache.spark.launcher.SparkLauncher\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * Constructs the complete list of driver configuration steps to run to deploy the Spark driver.\n+ */\n+private[spark] class DriverConfigurationStepsOrchestrator(\n+    namespace: String,\n+    kubernetesAppId: String,\n+    launchTime: Long,\n+    mainAppResource: Option[MainAppResource],\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) {\n+\n+  // The resource name prefix is derived from the application name, making it easy to connect the\n+  // names of the Kubernetes resources from e.g. kubectl or the Kubernetes dashboard to the\n+  // application the user submitted. However, we can't use the application name in the label, as\n+  // label values are considerably restrictive, e.g. must be no longer than 63 characters in\n+  // length. So we generate a separate identifier for the app ID itself, and bookkeeping that\n+  // requires finding \"all pods for this application\" should use the kubernetesAppId.\n+  private val kubernetesResourceNamePrefix ="
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "I think the biggest constraint here is labels, which cannot be longer than 63 characters. API object names have a limit of 253 characters. Are we using the prefix in any label?",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-05T03:00:53Z",
    "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.steps._\n+import org.apache.spark.launcher.SparkLauncher\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * Constructs the complete list of driver configuration steps to run to deploy the Spark driver.\n+ */\n+private[spark] class DriverConfigurationStepsOrchestrator(\n+    namespace: String,\n+    kubernetesAppId: String,\n+    launchTime: Long,\n+    mainAppResource: Option[MainAppResource],\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) {\n+\n+  // The resource name prefix is derived from the application name, making it easy to connect the\n+  // names of the Kubernetes resources from e.g. kubectl or the Kubernetes dashboard to the\n+  // application the user submitted. However, we can't use the application name in the label, as\n+  // label values are considerably restrictive, e.g. must be no longer than 63 characters in\n+  // length. So we generate a separate identifier for the app ID itself, and bookkeeping that\n+  // requires finding \"all pods for this application\" should use the kubernetesAppId.\n+  private val kubernetesResourceNamePrefix ="
  }],
  "prId": 19717
}, {
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "Nit: think `None` reads better.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-05T00:30:46Z",
    "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.steps._\n+import org.apache.spark.launcher.SparkLauncher\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * Constructs the complete list of driver configuration steps to run to deploy the Spark driver.\n+ */\n+private[spark] class DriverConfigurationStepsOrchestrator(\n+    namespace: String,\n+    kubernetesAppId: String,\n+    launchTime: Long,\n+    mainAppResource: Option[MainAppResource],\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) {\n+\n+  // The resource name prefix is derived from the application name, making it easy to connect the\n+  // names of the Kubernetes resources from e.g. kubectl or the Kubernetes dashboard to the\n+  // application the user submitted. However, we can't use the application name in the label, as\n+  // label values are considerably restrictive, e.g. must be no longer than 63 characters in\n+  // length. So we generate a separate identifier for the app ID itself, and bookkeeping that\n+  // requires finding \"all pods for this application\" should use the kubernetesAppId.\n+  private val kubernetesResourceNamePrefix =\n+      s\"$appName-$launchTime\".toLowerCase.replaceAll(\"\\\\.\", \"-\")\n+  private val dockerImagePullPolicy = submissionSparkConf.get(DOCKER_IMAGE_PULL_POLICY)\n+  private val jarsDownloadPath = submissionSparkConf.get(JARS_DOWNLOAD_LOCATION)\n+  private val filesDownloadPath = submissionSparkConf.get(FILES_DOWNLOAD_LOCATION)\n+\n+  def getAllConfigurationSteps(): Seq[DriverConfigurationStep] = {\n+    val driverCustomLabels = ConfigurationUtils.parsePrefixedKeyValuePairs(\n+      submissionSparkConf,\n+      KUBERNETES_DRIVER_LABEL_PREFIX)\n+    require(!driverCustomLabels.contains(SPARK_APP_ID_LABEL), \"Label with key \" +\n+      s\"$SPARK_APP_ID_LABEL is not allowed as it is reserved for Spark bookkeeping \" +\n+      \"operations.\")\n+    require(!driverCustomLabels.contains(SPARK_ROLE_LABEL), \"Label with key \" +\n+      s\"$SPARK_ROLE_LABEL is not allowed as it is reserved for Spark bookkeeping \" +\n+      \"operations.\")\n+\n+    val allDriverLabels = driverCustomLabels ++ Map(\n+      SPARK_APP_ID_LABEL -> kubernetesAppId,\n+      SPARK_ROLE_LABEL -> SPARK_POD_DRIVER_ROLE)\n+\n+    val initialSubmissionStep = new BaseDriverConfigurationStep(\n+      kubernetesAppId,\n+      kubernetesResourceNamePrefix,\n+      allDriverLabels,\n+      dockerImagePullPolicy,\n+      appName,\n+      mainClass,\n+      appArgs,\n+      submissionSparkConf)\n+\n+    val driverAddressStep = new DriverServiceBootstrapStep(\n+      kubernetesResourceNamePrefix,\n+      allDriverLabels,\n+      submissionSparkConf,\n+      new SystemClock)\n+\n+    val kubernetesCredentialsStep = new DriverKubernetesCredentialsStep(\n+      submissionSparkConf, kubernetesResourceNamePrefix)\n+\n+    val additionalMainAppJar = if (mainAppResource.nonEmpty) {\n+       val mayBeResource = mainAppResource.get match {\n+        case JavaMainAppResource(resource) if resource != SparkLauncher.NO_RESOURCE =>\n+          Some(resource)\n+        case _ => Option.empty"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Done.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-05T02:42:16Z",
    "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.steps._\n+import org.apache.spark.launcher.SparkLauncher\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * Constructs the complete list of driver configuration steps to run to deploy the Spark driver.\n+ */\n+private[spark] class DriverConfigurationStepsOrchestrator(\n+    namespace: String,\n+    kubernetesAppId: String,\n+    launchTime: Long,\n+    mainAppResource: Option[MainAppResource],\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) {\n+\n+  // The resource name prefix is derived from the application name, making it easy to connect the\n+  // names of the Kubernetes resources from e.g. kubectl or the Kubernetes dashboard to the\n+  // application the user submitted. However, we can't use the application name in the label, as\n+  // label values are considerably restrictive, e.g. must be no longer than 63 characters in\n+  // length. So we generate a separate identifier for the app ID itself, and bookkeeping that\n+  // requires finding \"all pods for this application\" should use the kubernetesAppId.\n+  private val kubernetesResourceNamePrefix =\n+      s\"$appName-$launchTime\".toLowerCase.replaceAll(\"\\\\.\", \"-\")\n+  private val dockerImagePullPolicy = submissionSparkConf.get(DOCKER_IMAGE_PULL_POLICY)\n+  private val jarsDownloadPath = submissionSparkConf.get(JARS_DOWNLOAD_LOCATION)\n+  private val filesDownloadPath = submissionSparkConf.get(FILES_DOWNLOAD_LOCATION)\n+\n+  def getAllConfigurationSteps(): Seq[DriverConfigurationStep] = {\n+    val driverCustomLabels = ConfigurationUtils.parsePrefixedKeyValuePairs(\n+      submissionSparkConf,\n+      KUBERNETES_DRIVER_LABEL_PREFIX)\n+    require(!driverCustomLabels.contains(SPARK_APP_ID_LABEL), \"Label with key \" +\n+      s\"$SPARK_APP_ID_LABEL is not allowed as it is reserved for Spark bookkeeping \" +\n+      \"operations.\")\n+    require(!driverCustomLabels.contains(SPARK_ROLE_LABEL), \"Label with key \" +\n+      s\"$SPARK_ROLE_LABEL is not allowed as it is reserved for Spark bookkeeping \" +\n+      \"operations.\")\n+\n+    val allDriverLabels = driverCustomLabels ++ Map(\n+      SPARK_APP_ID_LABEL -> kubernetesAppId,\n+      SPARK_ROLE_LABEL -> SPARK_POD_DRIVER_ROLE)\n+\n+    val initialSubmissionStep = new BaseDriverConfigurationStep(\n+      kubernetesAppId,\n+      kubernetesResourceNamePrefix,\n+      allDriverLabels,\n+      dockerImagePullPolicy,\n+      appName,\n+      mainClass,\n+      appArgs,\n+      submissionSparkConf)\n+\n+    val driverAddressStep = new DriverServiceBootstrapStep(\n+      kubernetesResourceNamePrefix,\n+      allDriverLabels,\n+      submissionSparkConf,\n+      new SystemClock)\n+\n+    val kubernetesCredentialsStep = new DriverKubernetesCredentialsStep(\n+      submissionSparkConf, kubernetesResourceNamePrefix)\n+\n+    val additionalMainAppJar = if (mainAppResource.nonEmpty) {\n+       val mayBeResource = mainAppResource.get match {\n+        case JavaMainAppResource(resource) if resource != SparkLauncher.NO_RESOURCE =>\n+          Some(resource)\n+        case _ => Option.empty"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Doesn't look done.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-08T18:52:25Z",
    "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.steps._\n+import org.apache.spark.launcher.SparkLauncher\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * Constructs the complete list of driver configuration steps to run to deploy the Spark driver.\n+ */\n+private[spark] class DriverConfigurationStepsOrchestrator(\n+    namespace: String,\n+    kubernetesAppId: String,\n+    launchTime: Long,\n+    mainAppResource: Option[MainAppResource],\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) {\n+\n+  // The resource name prefix is derived from the application name, making it easy to connect the\n+  // names of the Kubernetes resources from e.g. kubectl or the Kubernetes dashboard to the\n+  // application the user submitted. However, we can't use the application name in the label, as\n+  // label values are considerably restrictive, e.g. must be no longer than 63 characters in\n+  // length. So we generate a separate identifier for the app ID itself, and bookkeeping that\n+  // requires finding \"all pods for this application\" should use the kubernetesAppId.\n+  private val kubernetesResourceNamePrefix =\n+      s\"$appName-$launchTime\".toLowerCase.replaceAll(\"\\\\.\", \"-\")\n+  private val dockerImagePullPolicy = submissionSparkConf.get(DOCKER_IMAGE_PULL_POLICY)\n+  private val jarsDownloadPath = submissionSparkConf.get(JARS_DOWNLOAD_LOCATION)\n+  private val filesDownloadPath = submissionSparkConf.get(FILES_DOWNLOAD_LOCATION)\n+\n+  def getAllConfigurationSteps(): Seq[DriverConfigurationStep] = {\n+    val driverCustomLabels = ConfigurationUtils.parsePrefixedKeyValuePairs(\n+      submissionSparkConf,\n+      KUBERNETES_DRIVER_LABEL_PREFIX)\n+    require(!driverCustomLabels.contains(SPARK_APP_ID_LABEL), \"Label with key \" +\n+      s\"$SPARK_APP_ID_LABEL is not allowed as it is reserved for Spark bookkeeping \" +\n+      \"operations.\")\n+    require(!driverCustomLabels.contains(SPARK_ROLE_LABEL), \"Label with key \" +\n+      s\"$SPARK_ROLE_LABEL is not allowed as it is reserved for Spark bookkeeping \" +\n+      \"operations.\")\n+\n+    val allDriverLabels = driverCustomLabels ++ Map(\n+      SPARK_APP_ID_LABEL -> kubernetesAppId,\n+      SPARK_ROLE_LABEL -> SPARK_POD_DRIVER_ROLE)\n+\n+    val initialSubmissionStep = new BaseDriverConfigurationStep(\n+      kubernetesAppId,\n+      kubernetesResourceNamePrefix,\n+      allDriverLabels,\n+      dockerImagePullPolicy,\n+      appName,\n+      mainClass,\n+      appArgs,\n+      submissionSparkConf)\n+\n+    val driverAddressStep = new DriverServiceBootstrapStep(\n+      kubernetesResourceNamePrefix,\n+      allDriverLabels,\n+      submissionSparkConf,\n+      new SystemClock)\n+\n+    val kubernetesCredentialsStep = new DriverKubernetesCredentialsStep(\n+      submissionSparkConf, kubernetesResourceNamePrefix)\n+\n+    val additionalMainAppJar = if (mainAppResource.nonEmpty) {\n+       val mayBeResource = mainAppResource.get match {\n+        case JavaMainAppResource(resource) if resource != SparkLauncher.NO_RESOURCE =>\n+          Some(resource)\n+        case _ => Option.empty"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "I don't follow. The code still says `Option.empty` when Matt asked it to be replaced with `None`.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-08T23:10:07Z",
    "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.steps._\n+import org.apache.spark.launcher.SparkLauncher\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * Constructs the complete list of driver configuration steps to run to deploy the Spark driver.\n+ */\n+private[spark] class DriverConfigurationStepsOrchestrator(\n+    namespace: String,\n+    kubernetesAppId: String,\n+    launchTime: Long,\n+    mainAppResource: Option[MainAppResource],\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) {\n+\n+  // The resource name prefix is derived from the application name, making it easy to connect the\n+  // names of the Kubernetes resources from e.g. kubectl or the Kubernetes dashboard to the\n+  // application the user submitted. However, we can't use the application name in the label, as\n+  // label values are considerably restrictive, e.g. must be no longer than 63 characters in\n+  // length. So we generate a separate identifier for the app ID itself, and bookkeeping that\n+  // requires finding \"all pods for this application\" should use the kubernetesAppId.\n+  private val kubernetesResourceNamePrefix =\n+      s\"$appName-$launchTime\".toLowerCase.replaceAll(\"\\\\.\", \"-\")\n+  private val dockerImagePullPolicy = submissionSparkConf.get(DOCKER_IMAGE_PULL_POLICY)\n+  private val jarsDownloadPath = submissionSparkConf.get(JARS_DOWNLOAD_LOCATION)\n+  private val filesDownloadPath = submissionSparkConf.get(FILES_DOWNLOAD_LOCATION)\n+\n+  def getAllConfigurationSteps(): Seq[DriverConfigurationStep] = {\n+    val driverCustomLabels = ConfigurationUtils.parsePrefixedKeyValuePairs(\n+      submissionSparkConf,\n+      KUBERNETES_DRIVER_LABEL_PREFIX)\n+    require(!driverCustomLabels.contains(SPARK_APP_ID_LABEL), \"Label with key \" +\n+      s\"$SPARK_APP_ID_LABEL is not allowed as it is reserved for Spark bookkeeping \" +\n+      \"operations.\")\n+    require(!driverCustomLabels.contains(SPARK_ROLE_LABEL), \"Label with key \" +\n+      s\"$SPARK_ROLE_LABEL is not allowed as it is reserved for Spark bookkeeping \" +\n+      \"operations.\")\n+\n+    val allDriverLabels = driverCustomLabels ++ Map(\n+      SPARK_APP_ID_LABEL -> kubernetesAppId,\n+      SPARK_ROLE_LABEL -> SPARK_POD_DRIVER_ROLE)\n+\n+    val initialSubmissionStep = new BaseDriverConfigurationStep(\n+      kubernetesAppId,\n+      kubernetesResourceNamePrefix,\n+      allDriverLabels,\n+      dockerImagePullPolicy,\n+      appName,\n+      mainClass,\n+      appArgs,\n+      submissionSparkConf)\n+\n+    val driverAddressStep = new DriverServiceBootstrapStep(\n+      kubernetesResourceNamePrefix,\n+      allDriverLabels,\n+      submissionSparkConf,\n+      new SystemClock)\n+\n+    val kubernetesCredentialsStep = new DriverKubernetesCredentialsStep(\n+      submissionSparkConf, kubernetesResourceNamePrefix)\n+\n+    val additionalMainAppJar = if (mainAppResource.nonEmpty) {\n+       val mayBeResource = mainAppResource.get match {\n+        case JavaMainAppResource(resource) if resource != SparkLauncher.NO_RESOURCE =>\n+          Some(resource)\n+        case _ => Option.empty"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Ah, I may have forgotten to actually make the change. Anyway, It's done now https://github.com/apache/spark/pull/19717/commits/7d2b30373b2e4d8d5311e10c3f9a62a2d900d568#diff-3d4589124c45f78dcd516e2d61294b70.  ",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-08T23:24:21Z",
    "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.steps._\n+import org.apache.spark.launcher.SparkLauncher\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * Constructs the complete list of driver configuration steps to run to deploy the Spark driver.\n+ */\n+private[spark] class DriverConfigurationStepsOrchestrator(\n+    namespace: String,\n+    kubernetesAppId: String,\n+    launchTime: Long,\n+    mainAppResource: Option[MainAppResource],\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) {\n+\n+  // The resource name prefix is derived from the application name, making it easy to connect the\n+  // names of the Kubernetes resources from e.g. kubectl or the Kubernetes dashboard to the\n+  // application the user submitted. However, we can't use the application name in the label, as\n+  // label values are considerably restrictive, e.g. must be no longer than 63 characters in\n+  // length. So we generate a separate identifier for the app ID itself, and bookkeeping that\n+  // requires finding \"all pods for this application\" should use the kubernetesAppId.\n+  private val kubernetesResourceNamePrefix =\n+      s\"$appName-$launchTime\".toLowerCase.replaceAll(\"\\\\.\", \"-\")\n+  private val dockerImagePullPolicy = submissionSparkConf.get(DOCKER_IMAGE_PULL_POLICY)\n+  private val jarsDownloadPath = submissionSparkConf.get(JARS_DOWNLOAD_LOCATION)\n+  private val filesDownloadPath = submissionSparkConf.get(FILES_DOWNLOAD_LOCATION)\n+\n+  def getAllConfigurationSteps(): Seq[DriverConfigurationStep] = {\n+    val driverCustomLabels = ConfigurationUtils.parsePrefixedKeyValuePairs(\n+      submissionSparkConf,\n+      KUBERNETES_DRIVER_LABEL_PREFIX)\n+    require(!driverCustomLabels.contains(SPARK_APP_ID_LABEL), \"Label with key \" +\n+      s\"$SPARK_APP_ID_LABEL is not allowed as it is reserved for Spark bookkeeping \" +\n+      \"operations.\")\n+    require(!driverCustomLabels.contains(SPARK_ROLE_LABEL), \"Label with key \" +\n+      s\"$SPARK_ROLE_LABEL is not allowed as it is reserved for Spark bookkeeping \" +\n+      \"operations.\")\n+\n+    val allDriverLabels = driverCustomLabels ++ Map(\n+      SPARK_APP_ID_LABEL -> kubernetesAppId,\n+      SPARK_ROLE_LABEL -> SPARK_POD_DRIVER_ROLE)\n+\n+    val initialSubmissionStep = new BaseDriverConfigurationStep(\n+      kubernetesAppId,\n+      kubernetesResourceNamePrefix,\n+      allDriverLabels,\n+      dockerImagePullPolicy,\n+      appName,\n+      mainClass,\n+      appArgs,\n+      submissionSparkConf)\n+\n+    val driverAddressStep = new DriverServiceBootstrapStep(\n+      kubernetesResourceNamePrefix,\n+      allDriverLabels,\n+      submissionSparkConf,\n+      new SystemClock)\n+\n+    val kubernetesCredentialsStep = new DriverKubernetesCredentialsStep(\n+      submissionSparkConf, kubernetesResourceNamePrefix)\n+\n+    val additionalMainAppJar = if (mainAppResource.nonEmpty) {\n+       val mayBeResource = mainAppResource.get match {\n+        case JavaMainAppResource(resource) if resource != SparkLauncher.NO_RESOURCE =>\n+          Some(resource)\n+        case _ => Option.empty"
  }],
  "prId": 19717
}, {
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "Nit: think `None` reads better.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-05T00:30:56Z",
    "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.steps._\n+import org.apache.spark.launcher.SparkLauncher\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * Constructs the complete list of driver configuration steps to run to deploy the Spark driver.\n+ */\n+private[spark] class DriverConfigurationStepsOrchestrator(\n+    namespace: String,\n+    kubernetesAppId: String,\n+    launchTime: Long,\n+    mainAppResource: Option[MainAppResource],\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) {\n+\n+  // The resource name prefix is derived from the application name, making it easy to connect the\n+  // names of the Kubernetes resources from e.g. kubectl or the Kubernetes dashboard to the\n+  // application the user submitted. However, we can't use the application name in the label, as\n+  // label values are considerably restrictive, e.g. must be no longer than 63 characters in\n+  // length. So we generate a separate identifier for the app ID itself, and bookkeeping that\n+  // requires finding \"all pods for this application\" should use the kubernetesAppId.\n+  private val kubernetesResourceNamePrefix =\n+      s\"$appName-$launchTime\".toLowerCase.replaceAll(\"\\\\.\", \"-\")\n+  private val dockerImagePullPolicy = submissionSparkConf.get(DOCKER_IMAGE_PULL_POLICY)\n+  private val jarsDownloadPath = submissionSparkConf.get(JARS_DOWNLOAD_LOCATION)\n+  private val filesDownloadPath = submissionSparkConf.get(FILES_DOWNLOAD_LOCATION)\n+\n+  def getAllConfigurationSteps(): Seq[DriverConfigurationStep] = {\n+    val driverCustomLabels = ConfigurationUtils.parsePrefixedKeyValuePairs(\n+      submissionSparkConf,\n+      KUBERNETES_DRIVER_LABEL_PREFIX)\n+    require(!driverCustomLabels.contains(SPARK_APP_ID_LABEL), \"Label with key \" +\n+      s\"$SPARK_APP_ID_LABEL is not allowed as it is reserved for Spark bookkeeping \" +\n+      \"operations.\")\n+    require(!driverCustomLabels.contains(SPARK_ROLE_LABEL), \"Label with key \" +\n+      s\"$SPARK_ROLE_LABEL is not allowed as it is reserved for Spark bookkeeping \" +\n+      \"operations.\")\n+\n+    val allDriverLabels = driverCustomLabels ++ Map(\n+      SPARK_APP_ID_LABEL -> kubernetesAppId,\n+      SPARK_ROLE_LABEL -> SPARK_POD_DRIVER_ROLE)\n+\n+    val initialSubmissionStep = new BaseDriverConfigurationStep(\n+      kubernetesAppId,\n+      kubernetesResourceNamePrefix,\n+      allDriverLabels,\n+      dockerImagePullPolicy,\n+      appName,\n+      mainClass,\n+      appArgs,\n+      submissionSparkConf)\n+\n+    val driverAddressStep = new DriverServiceBootstrapStep(\n+      kubernetesResourceNamePrefix,\n+      allDriverLabels,\n+      submissionSparkConf,\n+      new SystemClock)\n+\n+    val kubernetesCredentialsStep = new DriverKubernetesCredentialsStep(\n+      submissionSparkConf, kubernetesResourceNamePrefix)\n+\n+    val additionalMainAppJar = if (mainAppResource.nonEmpty) {\n+       val mayBeResource = mainAppResource.get match {\n+        case JavaMainAppResource(resource) if resource != SparkLauncher.NO_RESOURCE =>\n+          Some(resource)\n+        case _ => Option.empty\n+      }\n+      mayBeResource\n+    } else {\n+      Option.empty"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Done.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-05T02:42:20Z",
    "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.steps._\n+import org.apache.spark.launcher.SparkLauncher\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * Constructs the complete list of driver configuration steps to run to deploy the Spark driver.\n+ */\n+private[spark] class DriverConfigurationStepsOrchestrator(\n+    namespace: String,\n+    kubernetesAppId: String,\n+    launchTime: Long,\n+    mainAppResource: Option[MainAppResource],\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) {\n+\n+  // The resource name prefix is derived from the application name, making it easy to connect the\n+  // names of the Kubernetes resources from e.g. kubectl or the Kubernetes dashboard to the\n+  // application the user submitted. However, we can't use the application name in the label, as\n+  // label values are considerably restrictive, e.g. must be no longer than 63 characters in\n+  // length. So we generate a separate identifier for the app ID itself, and bookkeeping that\n+  // requires finding \"all pods for this application\" should use the kubernetesAppId.\n+  private val kubernetesResourceNamePrefix =\n+      s\"$appName-$launchTime\".toLowerCase.replaceAll(\"\\\\.\", \"-\")\n+  private val dockerImagePullPolicy = submissionSparkConf.get(DOCKER_IMAGE_PULL_POLICY)\n+  private val jarsDownloadPath = submissionSparkConf.get(JARS_DOWNLOAD_LOCATION)\n+  private val filesDownloadPath = submissionSparkConf.get(FILES_DOWNLOAD_LOCATION)\n+\n+  def getAllConfigurationSteps(): Seq[DriverConfigurationStep] = {\n+    val driverCustomLabels = ConfigurationUtils.parsePrefixedKeyValuePairs(\n+      submissionSparkConf,\n+      KUBERNETES_DRIVER_LABEL_PREFIX)\n+    require(!driverCustomLabels.contains(SPARK_APP_ID_LABEL), \"Label with key \" +\n+      s\"$SPARK_APP_ID_LABEL is not allowed as it is reserved for Spark bookkeeping \" +\n+      \"operations.\")\n+    require(!driverCustomLabels.contains(SPARK_ROLE_LABEL), \"Label with key \" +\n+      s\"$SPARK_ROLE_LABEL is not allowed as it is reserved for Spark bookkeeping \" +\n+      \"operations.\")\n+\n+    val allDriverLabels = driverCustomLabels ++ Map(\n+      SPARK_APP_ID_LABEL -> kubernetesAppId,\n+      SPARK_ROLE_LABEL -> SPARK_POD_DRIVER_ROLE)\n+\n+    val initialSubmissionStep = new BaseDriverConfigurationStep(\n+      kubernetesAppId,\n+      kubernetesResourceNamePrefix,\n+      allDriverLabels,\n+      dockerImagePullPolicy,\n+      appName,\n+      mainClass,\n+      appArgs,\n+      submissionSparkConf)\n+\n+    val driverAddressStep = new DriverServiceBootstrapStep(\n+      kubernetesResourceNamePrefix,\n+      allDriverLabels,\n+      submissionSparkConf,\n+      new SystemClock)\n+\n+    val kubernetesCredentialsStep = new DriverKubernetesCredentialsStep(\n+      submissionSparkConf, kubernetesResourceNamePrefix)\n+\n+    val additionalMainAppJar = if (mainAppResource.nonEmpty) {\n+       val mayBeResource = mainAppResource.get match {\n+        case JavaMainAppResource(resource) if resource != SparkLauncher.NO_RESOURCE =>\n+          Some(resource)\n+        case _ => Option.empty\n+      }\n+      mayBeResource\n+    } else {\n+      Option.empty"
  }],
  "prId": 19717
}, {
  "comments": [{
    "author": {
      "login": "jerryshao"
    },
    "body": "Do we support jars/files on distributed file systems or Http/https?",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-05T08:33:43Z",
    "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.UUID\n+\n+import com.google.common.primitives.Longs\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.steps._\n+import org.apache.spark.launcher.SparkLauncher\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * Constructs the complete list of driver configuration steps to run to deploy the Spark driver.\n+ */\n+private[spark] class DriverConfigurationStepsOrchestrator(\n+    namespace: String,\n+    kubernetesAppId: String,\n+    launchTime: Long,\n+    mainAppResource: Option[MainAppResource],\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) {\n+\n+  // The resource name prefix is derived from the application name, making it easy to connect the\n+  // names of the Kubernetes resources from e.g. kubectl or the Kubernetes dashboard to the\n+  // application the user submitted. However, we can't use the application name in the label, as\n+  // label values are considerably restrictive, e.g. must be no longer than 63 characters in\n+  // length. So we generate a separate identifier for the app ID itself, and bookkeeping that\n+  // requires finding \"all pods for this application\" should use the kubernetesAppId.\n+  private val kubernetesResourceNamePrefix = {\n+    val uuid = UUID.nameUUIDFromBytes(Longs.toByteArray(launchTime))\n+    s\"$appName-$uuid\".toLowerCase.replaceAll(\"\\\\.\", \"-\")\n+  }\n+\n+  private val dockerImagePullPolicy = submissionSparkConf.get(DOCKER_IMAGE_PULL_POLICY)\n+  private val jarsDownloadPath = submissionSparkConf.get(JARS_DOWNLOAD_LOCATION)\n+  private val filesDownloadPath = submissionSparkConf.get(FILES_DOWNLOAD_LOCATION)\n+\n+  def getAllConfigurationSteps(): Seq[DriverConfigurationStep] = {\n+    val driverCustomLabels = ConfigurationUtils.parsePrefixedKeyValuePairs(\n+      submissionSparkConf,\n+      KUBERNETES_DRIVER_LABEL_PREFIX)\n+    require(!driverCustomLabels.contains(SPARK_APP_ID_LABEL), \"Label with key \" +\n+      s\"$SPARK_APP_ID_LABEL is not allowed as it is reserved for Spark bookkeeping \" +\n+      \"operations.\")\n+    require(!driverCustomLabels.contains(SPARK_ROLE_LABEL), \"Label with key \" +\n+      s\"$SPARK_ROLE_LABEL is not allowed as it is reserved for Spark bookkeeping \" +\n+      \"operations.\")\n+\n+    val allDriverLabels = driverCustomLabels ++ Map(\n+      SPARK_APP_ID_LABEL -> kubernetesAppId,\n+      SPARK_ROLE_LABEL -> SPARK_POD_DRIVER_ROLE)\n+\n+    val initialSubmissionStep = new BaseDriverConfigurationStep(\n+      kubernetesAppId,\n+      kubernetesResourceNamePrefix,\n+      allDriverLabels,\n+      dockerImagePullPolicy,\n+      appName,\n+      mainClass,\n+      appArgs,\n+      submissionSparkConf)\n+\n+    val driverAddressStep = new DriverServiceBootstrapStep(\n+      kubernetesResourceNamePrefix,\n+      allDriverLabels,\n+      submissionSparkConf,\n+      new SystemClock)\n+\n+    val kubernetesCredentialsStep = new DriverKubernetesCredentialsStep(\n+      submissionSparkConf, kubernetesResourceNamePrefix)\n+\n+    val additionalMainAppJar = if (mainAppResource.nonEmpty) {\n+       val mayBeResource = mainAppResource.get match {\n+        case JavaMainAppResource(resource) if resource != SparkLauncher.NO_RESOURCE =>\n+          Some(resource)\n+        case _ => Option.empty\n+      }\n+      mayBeResource\n+    } else {\n+      None\n+    }\n+\n+    val sparkJars = submissionSparkConf.getOption(\"spark.jars\")",
    "line": 101
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "In our fork, we use Kubernetes init-container col-located with the driver/executor containers for downloading remote dependencies, e.g., from http/https endpoints. The init-container will be introduced in a subsequent PR.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-05T16:47:59Z",
    "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.UUID\n+\n+import com.google.common.primitives.Longs\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.steps._\n+import org.apache.spark.launcher.SparkLauncher\n+import org.apache.spark.util.SystemClock\n+\n+/**\n+ * Constructs the complete list of driver configuration steps to run to deploy the Spark driver.\n+ */\n+private[spark] class DriverConfigurationStepsOrchestrator(\n+    namespace: String,\n+    kubernetesAppId: String,\n+    launchTime: Long,\n+    mainAppResource: Option[MainAppResource],\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) {\n+\n+  // The resource name prefix is derived from the application name, making it easy to connect the\n+  // names of the Kubernetes resources from e.g. kubectl or the Kubernetes dashboard to the\n+  // application the user submitted. However, we can't use the application name in the label, as\n+  // label values are considerably restrictive, e.g. must be no longer than 63 characters in\n+  // length. So we generate a separate identifier for the app ID itself, and bookkeeping that\n+  // requires finding \"all pods for this application\" should use the kubernetesAppId.\n+  private val kubernetesResourceNamePrefix = {\n+    val uuid = UUID.nameUUIDFromBytes(Longs.toByteArray(launchTime))\n+    s\"$appName-$uuid\".toLowerCase.replaceAll(\"\\\\.\", \"-\")\n+  }\n+\n+  private val dockerImagePullPolicy = submissionSparkConf.get(DOCKER_IMAGE_PULL_POLICY)\n+  private val jarsDownloadPath = submissionSparkConf.get(JARS_DOWNLOAD_LOCATION)\n+  private val filesDownloadPath = submissionSparkConf.get(FILES_DOWNLOAD_LOCATION)\n+\n+  def getAllConfigurationSteps(): Seq[DriverConfigurationStep] = {\n+    val driverCustomLabels = ConfigurationUtils.parsePrefixedKeyValuePairs(\n+      submissionSparkConf,\n+      KUBERNETES_DRIVER_LABEL_PREFIX)\n+    require(!driverCustomLabels.contains(SPARK_APP_ID_LABEL), \"Label with key \" +\n+      s\"$SPARK_APP_ID_LABEL is not allowed as it is reserved for Spark bookkeeping \" +\n+      \"operations.\")\n+    require(!driverCustomLabels.contains(SPARK_ROLE_LABEL), \"Label with key \" +\n+      s\"$SPARK_ROLE_LABEL is not allowed as it is reserved for Spark bookkeeping \" +\n+      \"operations.\")\n+\n+    val allDriverLabels = driverCustomLabels ++ Map(\n+      SPARK_APP_ID_LABEL -> kubernetesAppId,\n+      SPARK_ROLE_LABEL -> SPARK_POD_DRIVER_ROLE)\n+\n+    val initialSubmissionStep = new BaseDriverConfigurationStep(\n+      kubernetesAppId,\n+      kubernetesResourceNamePrefix,\n+      allDriverLabels,\n+      dockerImagePullPolicy,\n+      appName,\n+      mainClass,\n+      appArgs,\n+      submissionSparkConf)\n+\n+    val driverAddressStep = new DriverServiceBootstrapStep(\n+      kubernetesResourceNamePrefix,\n+      allDriverLabels,\n+      submissionSparkConf,\n+      new SystemClock)\n+\n+    val kubernetesCredentialsStep = new DriverKubernetesCredentialsStep(\n+      submissionSparkConf, kubernetesResourceNamePrefix)\n+\n+    val additionalMainAppJar = if (mainAppResource.nonEmpty) {\n+       val mayBeResource = mainAppResource.get match {\n+        case JavaMainAppResource(resource) if resource != SparkLauncher.NO_RESOURCE =>\n+          Some(resource)\n+        case _ => Option.empty\n+      }\n+      mayBeResource\n+    } else {\n+      None\n+    }\n+\n+    val sparkJars = submissionSparkConf.getOption(\"spark.jars\")",
    "line": 101
  }],
  "prId": 19717
}]