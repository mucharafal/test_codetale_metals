[{
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "Another case where the previous approach was insufficient. If Spark gets an executor connected to it that is not in the current snapshot, then we should probably get rid of it. Would specifically like feedback on if this is done correctly @foxish @liyinan926.",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-06-08T01:43:50Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import com.google.common.cache.Cache\n+import io.fabric8.kubernetes.api.model.Pod\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsLifecycleManager(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    snapshotsStore: ExecutorPodsSnapshotsStore,\n+    // Use a best-effort to track which executors have been removed already. It's not generally\n+    // job-breaking if we remove executors more than once but it's ideal if we make an attempt\n+    // to avoid doing so. Expire cache entries so that this data structure doesn't grow beyond\n+    // bounds.\n+    removedExecutorsCache: Cache[java.lang.Long, java.lang.Long]) {\n+\n+  import ExecutorPodsLifecycleManager._\n+\n+  private val eventProcessingInterval = conf.get(KUBERNETES_EXECUTOR_EVENT_PROCESSING_INTERVAL)\n+\n+  def start(schedulerBackend: KubernetesClusterSchedulerBackend): Unit = {\n+    snapshotsStore.addSubscriber(eventProcessingInterval) {\n+      onNextSnapshot(schedulerBackend, _)\n+    }\n+  }\n+\n+  private def onNextSnapshot(\n+      schedulerBackend: KubernetesClusterSchedulerBackend,\n+      snapshot: ExecutorPodsSnapshot): Unit = {\n+    val execIdsRemovedInThisRound = mutable.HashSet.empty[Long]\n+    snapshot.executorPods.foreach { case (execId, state) =>\n+      state match {\n+        case PodDeleted(pod) =>\n+          removeExecutorFromSpark(schedulerBackend, pod, execId)\n+          execIdsRemovedInThisRound += execId\n+        case errorOrSucceeded @ (PodFailed(_) | PodSucceeded(_)) =>\n+          removeExecutorFromK8s(errorOrSucceeded.pod)\n+          removeExecutorFromSpark(schedulerBackend, errorOrSucceeded.pod, execId)\n+          execIdsRemovedInThisRound += execId\n+        case _ =>\n+      }\n+    }\n+\n+    // Reconcile the case where Spark claims to know about an executor but the corresponding pod\n+    // is missing from the cluster. This would occur if we miss a deletion event and the pod\n+    // transitions immediately from running io absent.\n+    (schedulerBackend.getExecutorIds().map(_.toLong).toSet"
  }],
  "prId": 21366
}, {
  "comments": [{
    "author": {
      "login": "dvogelbacher"
    },
    "body": "this method is duplicated in `ExecutorPodsSnapshot.scala`, can we get rid of it here?",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-06-08T21:03:02Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import com.google.common.cache.Cache\n+import io.fabric8.kubernetes.api.model.Pod\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsLifecycleManager(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    snapshotsStore: ExecutorPodsSnapshotsStore,\n+    // Use a best-effort to track which executors have been removed already. It's not generally\n+    // job-breaking if we remove executors more than once but it's ideal if we make an attempt\n+    // to avoid doing so. Expire cache entries so that this data structure doesn't grow beyond\n+    // bounds.\n+    removedExecutorsCache: Cache[java.lang.Long, java.lang.Long]) {\n+\n+  import ExecutorPodsLifecycleManager._\n+\n+  private val eventProcessingInterval = conf.get(KUBERNETES_EXECUTOR_EVENT_PROCESSING_INTERVAL)\n+\n+  def start(schedulerBackend: KubernetesClusterSchedulerBackend): Unit = {\n+    snapshotsStore.addSubscriber(eventProcessingInterval) {\n+      onNextSnapshot(schedulerBackend, _)\n+    }\n+  }\n+\n+  private def onNextSnapshot(\n+      schedulerBackend: KubernetesClusterSchedulerBackend,\n+      snapshot: ExecutorPodsSnapshot): Unit = {\n+    val execIdsRemovedInThisRound = mutable.HashSet.empty[Long]\n+    snapshot.executorPods.foreach { case (execId, state) =>\n+      state match {\n+        case PodDeleted(pod) =>\n+          removeExecutorFromSpark(schedulerBackend, pod, execId)\n+          execIdsRemovedInThisRound += execId\n+        case errorOrSucceeded @ (PodFailed(_) | PodSucceeded(_)) =>\n+          removeExecutorFromK8s(errorOrSucceeded.pod)\n+          removeExecutorFromSpark(schedulerBackend, errorOrSucceeded.pod, execId)\n+          execIdsRemovedInThisRound += execId\n+        case _ =>\n+      }\n+    }\n+\n+    // Reconcile the case where Spark claims to know about an executor but the corresponding pod\n+    // is missing from the cluster. This would occur if we miss a deletion event and the pod\n+    // transitions immediately from running io absent.\n+    (schedulerBackend.getExecutorIds().map(_.toLong).toSet\n+      -- snapshot.executorPods.keySet\n+      -- execIdsRemovedInThisRound).foreach { missingExecutorId =>\n+      if (removedExecutorsCache.getIfPresent(missingExecutorId) == null) {\n+        val exitReason = ExecutorExited(\n+          UNKNOWN_EXIT_CODE,\n+          exitCausedByApp = false,\n+          s\"The executor with ID $missingExecutorId was not found in the cluster but we didn't\" +\n+            s\" get a reason why. Marking the executor as failed. The executor may have been\" +\n+            s\" deleted but the driver missed the deletion event.\")\n+        schedulerBackend.doRemoveExecutor(missingExecutorId.toString, exitReason)\n+      }\n+    }\n+  }\n+\n+  private def removeExecutorFromK8s(updatedPod: Pod): Unit = {\n+    // If deletion failed on a previous try, we can try again if resync informs us the pod\n+    // is still around.\n+    // Delete as best attempt - duplicate deletes will throw an exception but the end state\n+    // of getting rid of the pod is what matters.\n+    Utils.tryLogNonFatalError {\n+      kubernetesClient\n+        .pods()\n+        .withName(updatedPod.getMetadata.getName)\n+        .delete()\n+    }\n+  }\n+\n+  private def removeExecutorFromSpark(\n+      schedulerBackend: KubernetesClusterSchedulerBackend,\n+      pod: Pod,\n+      execId: Long): Unit = {\n+    if (removedExecutorsCache.getIfPresent(execId) == null) {\n+      removedExecutorsCache.put(execId, execId)\n+      val exitReason = findExitReason(pod, execId)\n+      schedulerBackend.doRemoveExecutor(execId.toString, exitReason)\n+    }\n+  }\n+\n+  private def findExitReason(pod: Pod, execId: Long): ExecutorExited = {\n+    val exitCode = findExitCode(pod)\n+    val (exitCausedByApp, exitMessage) = if (isDeleted(pod)) {\n+      (false, s\"The executor with id $execId was deleted by a user or the framework.\")\n+    } else {\n+      val msg = exitReasonMessage(pod, execId, exitCode)\n+      (true, msg)\n+    }\n+    ExecutorExited(exitCode, exitCausedByApp, exitMessage)\n+  }\n+\n+  private def exitReasonMessage(pod: Pod, execId: Long, exitCode: Int) = {\n+    s\"\"\"\n+       |The executor with id $execId exited with exit code $exitCode.\n+       |The API gave the following brief reason: ${pod.getStatus.getReason}\n+       |The API gave the following message: ${pod.getStatus.getMessage}\n+       |The API gave the following container statuses:\n+       |\n+       |${pod.getStatus.getContainerStatuses.asScala.map(_.toString).mkString(\"\\n===\\n\")}\n+      \"\"\".stripMargin\n+  }\n+\n+  private def isDeleted(pod: Pod): Boolean = pod.getMetadata.getDeletionTimestamp != null"
  }],
  "prId": 21366
}, {
  "comments": [{
    "author": {
      "login": "skonto"
    },
    "body": "s/succeeded@PodSucceeded(pod)/succeeded@PodSucceeded(_)",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-06-14T10:54:14Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import com.google.common.cache.Cache\n+import io.fabric8.kubernetes.api.model.Pod\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsLifecycleManager(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    snapshotsStore: ExecutorPodsSnapshotsStore,\n+    // Use a best-effort to track which executors have been removed already. It's not generally\n+    // job-breaking if we remove executors more than once but it's ideal if we make an attempt\n+    // to avoid doing so. Expire cache entries so that this data structure doesn't grow beyond\n+    // bounds.\n+    removedExecutorsCache: Cache[java.lang.Long, java.lang.Long]) {\n+\n+  import ExecutorPodsLifecycleManager._\n+\n+  private val eventProcessingInterval = conf.get(KUBERNETES_EXECUTOR_EVENT_PROCESSING_INTERVAL)\n+\n+  def start(schedulerBackend: KubernetesClusterSchedulerBackend): Unit = {\n+    snapshotsStore.addSubscriber(eventProcessingInterval) {\n+      onNewSnapshots(schedulerBackend, _)\n+    }\n+  }\n+\n+  private def onNewSnapshots(\n+      schedulerBackend: KubernetesClusterSchedulerBackend,\n+      snapshots: Seq[ExecutorPodsSnapshot]): Unit = {\n+    val execIdsRemovedInThisRound = mutable.HashSet.empty[Long]\n+    snapshots.foreach { snapshot =>\n+      snapshot.executorPods.foreach { case (execId, state) =>\n+        state match {\n+          case deleted@PodDeleted(pod) =>"
  }],
  "prId": 21366
}, {
  "comments": [{
    "author": {
      "login": "skonto"
    },
    "body": "same as above.",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-06-14T10:54:29Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import com.google.common.cache.Cache\n+import io.fabric8.kubernetes.api.model.Pod\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsLifecycleManager(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    snapshotsStore: ExecutorPodsSnapshotsStore,\n+    // Use a best-effort to track which executors have been removed already. It's not generally\n+    // job-breaking if we remove executors more than once but it's ideal if we make an attempt\n+    // to avoid doing so. Expire cache entries so that this data structure doesn't grow beyond\n+    // bounds.\n+    removedExecutorsCache: Cache[java.lang.Long, java.lang.Long]) {\n+\n+  import ExecutorPodsLifecycleManager._\n+\n+  private val eventProcessingInterval = conf.get(KUBERNETES_EXECUTOR_EVENT_PROCESSING_INTERVAL)\n+\n+  def start(schedulerBackend: KubernetesClusterSchedulerBackend): Unit = {\n+    snapshotsStore.addSubscriber(eventProcessingInterval) {\n+      onNewSnapshots(schedulerBackend, _)\n+    }\n+  }\n+\n+  private def onNewSnapshots(\n+      schedulerBackend: KubernetesClusterSchedulerBackend,\n+      snapshots: Seq[ExecutorPodsSnapshot]): Unit = {\n+    val execIdsRemovedInThisRound = mutable.HashSet.empty[Long]\n+    snapshots.foreach { snapshot =>\n+      snapshot.executorPods.foreach { case (execId, state) =>\n+        state match {\n+          case deleted@PodDeleted(pod) =>\n+            removeExecutorFromSpark(schedulerBackend, deleted, execId)\n+            execIdsRemovedInThisRound += execId\n+          case failed@PodFailed(pod) =>"
  }],
  "prId": 21366
}, {
  "comments": [{
    "author": {
      "login": "skonto"
    },
    "body": "same as above.",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-06-14T10:54:41Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import com.google.common.cache.Cache\n+import io.fabric8.kubernetes.api.model.Pod\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsLifecycleManager(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    snapshotsStore: ExecutorPodsSnapshotsStore,\n+    // Use a best-effort to track which executors have been removed already. It's not generally\n+    // job-breaking if we remove executors more than once but it's ideal if we make an attempt\n+    // to avoid doing so. Expire cache entries so that this data structure doesn't grow beyond\n+    // bounds.\n+    removedExecutorsCache: Cache[java.lang.Long, java.lang.Long]) {\n+\n+  import ExecutorPodsLifecycleManager._\n+\n+  private val eventProcessingInterval = conf.get(KUBERNETES_EXECUTOR_EVENT_PROCESSING_INTERVAL)\n+\n+  def start(schedulerBackend: KubernetesClusterSchedulerBackend): Unit = {\n+    snapshotsStore.addSubscriber(eventProcessingInterval) {\n+      onNewSnapshots(schedulerBackend, _)\n+    }\n+  }\n+\n+  private def onNewSnapshots(\n+      schedulerBackend: KubernetesClusterSchedulerBackend,\n+      snapshots: Seq[ExecutorPodsSnapshot]): Unit = {\n+    val execIdsRemovedInThisRound = mutable.HashSet.empty[Long]\n+    snapshots.foreach { snapshot =>\n+      snapshot.executorPods.foreach { case (execId, state) =>\n+        state match {\n+          case deleted@PodDeleted(pod) =>\n+            removeExecutorFromSpark(schedulerBackend, deleted, execId)\n+            execIdsRemovedInThisRound += execId\n+          case failed@PodFailed(pod) =>\n+            onFinalNonDeletedState(failed, execId, schedulerBackend, execIdsRemovedInThisRound)\n+          case succeeded@PodSucceeded(pod) =>"
  }],
  "prId": 21366
}]