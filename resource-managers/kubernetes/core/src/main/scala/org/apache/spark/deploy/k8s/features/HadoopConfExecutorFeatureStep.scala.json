[{
  "comments": [{
    "author": {
      "login": "liyinan926"
    },
    "body": "Looks like you can do `conf. getOption`.",
    "commit": "516ae6816a8521e944fa7471f2c069dbfc93ecfe",
    "createdAt": "2018-11-19T20:42:46Z",
    "diffHunk": "@@ -28,21 +26,15 @@ import org.apache.spark.internal.Logging\n  * containing Hadoop config files mounted as volumes and an ENV variable\n  * pointed to the mounted file directory.\n  */\n-private[spark] class HadoopConfExecutorFeatureStep(\n-    kubernetesConf: KubernetesConf[KubernetesExecutorSpecificConf])\n+private[spark] class HadoopConfExecutorFeatureStep(conf: KubernetesExecutorConf)\n   extends KubernetesFeatureConfigStep with Logging {\n \n   override def configurePod(pod: SparkPod): SparkPod = {\n-    val sparkConf = kubernetesConf.sparkConf\n-    val hadoopConfDirCMapName = sparkConf.getOption(HADOOP_CONFIG_MAP_NAME)\n+    val hadoopConfDirCMapName = conf.sparkConf.getOption(HADOOP_CONFIG_MAP_NAME)"
  }],
  "prId": 22959
}]