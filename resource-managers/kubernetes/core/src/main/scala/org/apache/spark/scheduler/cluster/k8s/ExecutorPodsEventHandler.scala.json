[{
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "believes an executor is running*",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-05-18T21:40:29Z",
    "diffHunk": "@@ -0,0 +1,212 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{Future, LinkedBlockingQueue, ScheduledExecutorService, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicInteger, AtomicLong}\n+\n+import gnu.trove.list.array.TLongArrayList\n+import gnu.trove.set.hash.TLongHashSet\n+import io.fabric8.kubernetes.api.model.{Pod, PodBuilder}\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesConf\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsEventHandler(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    eventProcessorExecutor: ScheduledExecutorService) {\n+\n+  import ExecutorPodsEventHandler._\n+\n+  private val EXECUTOR_ID_COUNTER = new AtomicLong(0L)\n+\n+  private val totalExpectedExecutors = new AtomicInteger(0)\n+\n+  private val eventQueue = new LinkedBlockingQueue[Seq[Pod]]()\n+\n+  private val podAllocationSize = conf.get(KUBERNETES_ALLOCATION_BATCH_SIZE)\n+\n+  private val kubernetesDriverPodName = conf\n+    .get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(throw new SparkException(\"Must specify the driver pod name\"))\n+\n+  private val driverPod = kubernetesClient.pods()\n+    .withName(kubernetesDriverPodName)\n+    .get()\n+\n+  // Use sets of ids instead of counters to be able to handle duplicate events.\n+\n+  // Executor IDs that have been requested from Kubernetes but are not running yet.\n+  private val pendingExecutors = new TLongHashSet()\n+\n+  // We could use CoarseGrainedSchedulerBackend#totalRegisteredExecutors here for tallying the\n+  // executors that are running. But, here we choose instead to maintain all state within this\n+  // class from the persecptive of the k8s API. Therefore whether or not this scheduler loop\n+  // believes a scheduler is running is dictated by the K8s API rather than Spark's RPC events."
  }],
  "prId": 21366
}, {
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "Not sure if this is 100% accurate - the pod may be evicted by the Kubernetes API if the pod misbehaves, so we should introspect if Kubernetes kicked out the pod because the pod itself did something wrong or if the pod was just deleted by a user or by this Spark application.",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-05-18T21:48:37Z",
    "diffHunk": "@@ -0,0 +1,212 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{Future, LinkedBlockingQueue, ScheduledExecutorService, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicInteger, AtomicLong}\n+\n+import gnu.trove.list.array.TLongArrayList\n+import gnu.trove.set.hash.TLongHashSet\n+import io.fabric8.kubernetes.api.model.{Pod, PodBuilder}\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesConf\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsEventHandler(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    eventProcessorExecutor: ScheduledExecutorService) {\n+\n+  import ExecutorPodsEventHandler._\n+\n+  private val EXECUTOR_ID_COUNTER = new AtomicLong(0L)\n+\n+  private val totalExpectedExecutors = new AtomicInteger(0)\n+\n+  private val eventQueue = new LinkedBlockingQueue[Seq[Pod]]()\n+\n+  private val podAllocationSize = conf.get(KUBERNETES_ALLOCATION_BATCH_SIZE)\n+\n+  private val kubernetesDriverPodName = conf\n+    .get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(throw new SparkException(\"Must specify the driver pod name\"))\n+\n+  private val driverPod = kubernetesClient.pods()\n+    .withName(kubernetesDriverPodName)\n+    .get()\n+\n+  // Use sets of ids instead of counters to be able to handle duplicate events.\n+\n+  // Executor IDs that have been requested from Kubernetes but are not running yet.\n+  private val pendingExecutors = new TLongHashSet()\n+\n+  // We could use CoarseGrainedSchedulerBackend#totalRegisteredExecutors here for tallying the\n+  // executors that are running. But, here we choose instead to maintain all state within this\n+  // class from the persecptive of the k8s API. Therefore whether or not this scheduler loop\n+  // believes a scheduler is running is dictated by the K8s API rather than Spark's RPC events.\n+  // We may need to consider where these perspectives may differ and which perspective should\n+  // take precedence.\n+  private val runningExecutors = new TLongHashSet()\n+\n+  private var eventProcessorFuture: Future[_] = _\n+\n+  def start(applicationId: String, schedulerBackend: KubernetesClusterSchedulerBackend): Unit = {\n+    require(eventProcessorFuture == null, \"Cannot start event processing twice.\")\n+    val eventProcessor = new Runnable {\n+      override def run(): Unit = processEvents(applicationId, schedulerBackend)\n+    }\n+    eventProcessorFuture = eventProcessorExecutor.scheduleWithFixedDelay(\n+      eventProcessor, 0L, 5L, TimeUnit.SECONDS)\n+  }\n+\n+  def stop(): Unit = {\n+    if (eventProcessorFuture != null) {\n+      eventProcessorFuture.cancel(true)\n+      eventProcessorFuture = null\n+    }\n+  }\n+\n+  private def processEvents(\n+      applicationId: String, schedulerBackend: KubernetesClusterSchedulerBackend) {\n+    val currentEvents = new java.util.ArrayList[Seq[Pod]](eventQueue.size())\n+      eventQueue.size())\n+    eventQueue.drainTo(currentEvents)\n+    currentEvents.asScala.flatten.foreach { updatedPod =>\n+      val execId = updatedPod.getMetadata.getLabels.get(SPARK_EXECUTOR_ID_LABEL).toLong\n+      val podPhase = updatedPod.getStatus.getPhase.toLowerCase\n+      if (isDeleted(updatedPod)) {\n+        removeExecutorFromSpark(schedulerBackend, updatedPod, execId)\n+      } else {\n+        updatedPod.getStatus.getPhase.toLowerCase match {\n+          case \"running\" =>\n+            // If clause is for resililence to out of order operations - executor must be\n+            // pending and first reach running. Without this check you may e.g. process a\n+            // deletion event followed by some arbitrary modification event - we want the\n+            // deletion event to \"stick\".\n+            if (pendingExecutors.contains(execId)) {\n+              pendingExecutors.remove(execId)\n+              runningExecutors.add(execId)\n+            }\n+          // TODO (SPARK-24135) - handle more classes of errors\n+          case \"error\" | \"failed\" | \"succeeded\" =>\n+            removeExecutorFromSpark(schedulerBackend, updatedPod, execId)\n+            // If deletion failed on a previous try, we can try again if resync informs us the pod\n+            // is still around.\n+            // Delete as best attempt - duplicate deletes will throw an exception but the end state\n+            // of getting rid of the pod is what matters.\n+            if (!isDeleted(updatedPod)) {\n+              Utils.tryLogNonFatalError {\n+                kubernetesClient\n+                  .pods()\n+                  .withName(updatedPod.getMetadata.getName)\n+                  .delete()\n+              }\n+            }\n+        }\n+      }\n+    }\n+\n+    val currentRunningExecutors = runningExecutors.size\n+    val currentTotalExpectedExecutors = totalExpectedExecutors.get\n+    if (pendingExecutors.isEmpty && currentRunningExecutors < currentTotalExpectedExecutors) {\n+      val numExecutorsToAllocate = math.min(\n+        currentTotalExpectedExecutors - currentRunningExecutors, podAllocationSize)\n+      val newExecutorIds = new TLongArrayList()\n+      val podsToAllocate = mutable.Buffer.empty[Pod]\n+      for (_ <- 0 until numExecutorsToAllocate) {\n+        val newExecutorId = EXECUTOR_ID_COUNTER.incrementAndGet()\n+        val executorConf = KubernetesConf.createExecutorConf(\n+          conf,\n+          newExecutorId.toString,\n+          applicationId,\n+          driverPod)\n+        val executorPod = executorBuilder.buildFromFeatures(executorConf)\n+        val podWithAttachedContainer = new PodBuilder(executorPod.pod)\n+          .editOrNewSpec()\n+          .addToContainers(executorPod.container)\n+          .endSpec()\n+          .build()\n+        newExecutorIds.add(newExecutorId)\n+      }\n+      kubernetesClient.pods().create(podsToAllocate: _*)\n+      pendingExecutors.addAll(newExecutorIds)\n+    }\n+  }\n+\n+  def sendUpdatedPodMetadata(updatedPod: Pod): Unit = {\n+    eventQueue.add(Seq(updatedPod))\n+  }\n+\n+  def sendUpdatedPodMetadata(updatedPods: Iterable[Pod]): Unit = {\n+    eventQueue.add(updatedPods.toSeq)\n+  }\n+\n+  def setTotalExpectedExecutors(newTotal: Int): Unit = totalExpectedExecutors.set(newTotal)\n+\n+  private def removeExecutorFromSpark(\n+      schedulerBackend: KubernetesClusterSchedulerBackend,\n+      updatedPod: Pod,\n+      execId: Long): Unit = {\n+    // Avoid removing twice from Spark's perspective.\n+    if (pendingExecutors.contains(execId) || runningExecutors.contains(execId)) {\n+      pendingExecutors.remove(execId)\n+      runningExecutors.remove(execId)\n+      val exitReason = findExitReason(updatedPod, execId)\n+      schedulerBackend.doRemoveExecutor(execId.toString, exitReason)\n+    }\n+  }\n+\n+  private def findExitReason(pod: Pod, execId: Long): ExecutorExited = {\n+    val exitCode = findExitCode(pod)\n+    val (exitCausedByApp, exitMessage) = if (isDeleted(pod)) {"
  }],
  "prId": 21366
}, {
  "comments": [{
    "author": {
      "login": "liyinan926"
    },
    "body": "The `eventProcessorExecutor` should be shutdown properly. Or you can use https://google.github.io/guava/releases/17.0/api/docs/com/google/common/util/concurrent/MoreExecutors.html#getExitingScheduledExecutorService(java.util.concurrent.ScheduledThreadPoolExecutor) to avoid having to manually shutdown it.",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-05-23T18:18:35Z",
    "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{Future, LinkedBlockingQueue, ScheduledExecutorService, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicInteger, AtomicLong}\n+\n+import io.fabric8.kubernetes.api.model.{Pod, PodBuilder}\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsEventHandler(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    eventProcessorExecutor: ScheduledExecutorService) extends Logging {\n+\n+  import ExecutorPodsEventHandler._\n+\n+  private val EXECUTOR_ID_COUNTER = new AtomicLong(0L)\n+\n+  private val totalExpectedExecutors = new AtomicInteger(0)\n+\n+  private val eventQueue = new LinkedBlockingQueue[Seq[Pod]]()\n+\n+  private val podAllocationSize = conf.get(KUBERNETES_ALLOCATION_BATCH_SIZE)\n+\n+  private val podAllocationDelay = conf.get(KUBERNETES_ALLOCATION_BATCH_DELAY)\n+\n+  private val kubernetesDriverPodName = conf\n+    .get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(throw new SparkException(\"Must specify the driver pod name\"))\n+\n+  private val driverPod = kubernetesClient.pods()\n+    .withName(kubernetesDriverPodName)\n+    .get()\n+\n+  // Use sets of ids instead of counters to be able to handle duplicate events.\n+\n+  // Executor IDs that have been requested from Kubernetes but are not running yet.\n+  private val pendingExecutors = mutable.Set.empty[Long]\n+\n+  // We could use CoarseGrainedSchedulerBackend#totalRegisteredExecutors here for tallying the\n+  // executors that are running. But, here we choose instead to maintain all state within this\n+  // class from the persecptive of the k8s API. Therefore whether or not this scheduler loop\n+  // believes an executor is running is dictated by the K8s API rather than Spark's RPC events.\n+  // We may need to consider where these perspectives may differ and which perspective should\n+  // take precedence.\n+  private val runningExecutors = mutable.Set.empty[Long]\n+\n+  private var eventProcessorFuture: Future[_] = _\n+\n+  def start(applicationId: String, schedulerBackend: KubernetesClusterSchedulerBackend): Unit = {\n+    require(eventProcessorFuture == null, \"Cannot start event processing twice.\")\n+    logInfo(s\"Starting Kubernetes executor pods event handler for application with\" +\n+      s\" id $applicationId.\")\n+    val eventProcessor = new Runnable {\n+      override def run(): Unit = {\n+        Utils.tryLogNonFatalError {\n+          processEvents(applicationId, schedulerBackend)\n+        }\n+      }\n+    }\n+    eventProcessorFuture = eventProcessorExecutor.scheduleWithFixedDelay("
  }],
  "prId": 21366
}, {
  "comments": [{
    "author": {
      "login": "liyinan926"
    },
    "body": "Why do you need to drain the queue? Can you just dequeue and process events one by one sequentially?",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-05-23T18:19:58Z",
    "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{Future, LinkedBlockingQueue, ScheduledExecutorService, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicInteger, AtomicLong}\n+\n+import io.fabric8.kubernetes.api.model.{Pod, PodBuilder}\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsEventHandler(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    eventProcessorExecutor: ScheduledExecutorService) extends Logging {\n+\n+  import ExecutorPodsEventHandler._\n+\n+  private val EXECUTOR_ID_COUNTER = new AtomicLong(0L)\n+\n+  private val totalExpectedExecutors = new AtomicInteger(0)\n+\n+  private val eventQueue = new LinkedBlockingQueue[Seq[Pod]]()\n+\n+  private val podAllocationSize = conf.get(KUBERNETES_ALLOCATION_BATCH_SIZE)\n+\n+  private val podAllocationDelay = conf.get(KUBERNETES_ALLOCATION_BATCH_DELAY)\n+\n+  private val kubernetesDriverPodName = conf\n+    .get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(throw new SparkException(\"Must specify the driver pod name\"))\n+\n+  private val driverPod = kubernetesClient.pods()\n+    .withName(kubernetesDriverPodName)\n+    .get()\n+\n+  // Use sets of ids instead of counters to be able to handle duplicate events.\n+\n+  // Executor IDs that have been requested from Kubernetes but are not running yet.\n+  private val pendingExecutors = mutable.Set.empty[Long]\n+\n+  // We could use CoarseGrainedSchedulerBackend#totalRegisteredExecutors here for tallying the\n+  // executors that are running. But, here we choose instead to maintain all state within this\n+  // class from the persecptive of the k8s API. Therefore whether or not this scheduler loop\n+  // believes an executor is running is dictated by the K8s API rather than Spark's RPC events.\n+  // We may need to consider where these perspectives may differ and which perspective should\n+  // take precedence.\n+  private val runningExecutors = mutable.Set.empty[Long]\n+\n+  private var eventProcessorFuture: Future[_] = _\n+\n+  def start(applicationId: String, schedulerBackend: KubernetesClusterSchedulerBackend): Unit = {\n+    require(eventProcessorFuture == null, \"Cannot start event processing twice.\")\n+    logInfo(s\"Starting Kubernetes executor pods event handler for application with\" +\n+      s\" id $applicationId.\")\n+    val eventProcessor = new Runnable {\n+      override def run(): Unit = {\n+        Utils.tryLogNonFatalError {\n+          processEvents(applicationId, schedulerBackend)\n+        }\n+      }\n+    }\n+    eventProcessorFuture = eventProcessorExecutor.scheduleWithFixedDelay(\n+      eventProcessor, 0L, podAllocationDelay, TimeUnit.MILLISECONDS)\n+  }\n+\n+  def stop(): Unit = {\n+    if (eventProcessorFuture != null) {\n+      eventProcessorFuture.cancel(true)\n+      eventProcessorFuture = null\n+    }\n+  }\n+\n+  private def processEvents(\n+      applicationId: String, schedulerBackend: KubernetesClusterSchedulerBackend) {\n+    val currentEvents = new java.util.ArrayList[Seq[Pod]](eventQueue.size())\n+    eventQueue.drainTo(currentEvents)"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "During a single round you probably want to process as many events as possible. Processing a single event per round causes an event at position N to need to wait N time units before it can be processed. Another way to do this is to have a loop that calls `poll` repeatedly until the queue is empty. What's nice about `drainTo` is that we lock the queue from write access while pulling all the events out, which is important so that a single pass doesn't potentially process events forever without getting to the actual executor deployment logic afterwards (granted if we move executor allocation to a separate schedule this becomes a non-factor). Aside from that, the documentation for blocking queues states that `drainTo` can be more efficient than repeated `poll()`s: https://docs.oracle.com/javase/8/docs/api/?java/util/concurrent/BlockingQueue.html",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-05-23T19:13:08Z",
    "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{Future, LinkedBlockingQueue, ScheduledExecutorService, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicInteger, AtomicLong}\n+\n+import io.fabric8.kubernetes.api.model.{Pod, PodBuilder}\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsEventHandler(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    eventProcessorExecutor: ScheduledExecutorService) extends Logging {\n+\n+  import ExecutorPodsEventHandler._\n+\n+  private val EXECUTOR_ID_COUNTER = new AtomicLong(0L)\n+\n+  private val totalExpectedExecutors = new AtomicInteger(0)\n+\n+  private val eventQueue = new LinkedBlockingQueue[Seq[Pod]]()\n+\n+  private val podAllocationSize = conf.get(KUBERNETES_ALLOCATION_BATCH_SIZE)\n+\n+  private val podAllocationDelay = conf.get(KUBERNETES_ALLOCATION_BATCH_DELAY)\n+\n+  private val kubernetesDriverPodName = conf\n+    .get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(throw new SparkException(\"Must specify the driver pod name\"))\n+\n+  private val driverPod = kubernetesClient.pods()\n+    .withName(kubernetesDriverPodName)\n+    .get()\n+\n+  // Use sets of ids instead of counters to be able to handle duplicate events.\n+\n+  // Executor IDs that have been requested from Kubernetes but are not running yet.\n+  private val pendingExecutors = mutable.Set.empty[Long]\n+\n+  // We could use CoarseGrainedSchedulerBackend#totalRegisteredExecutors here for tallying the\n+  // executors that are running. But, here we choose instead to maintain all state within this\n+  // class from the persecptive of the k8s API. Therefore whether or not this scheduler loop\n+  // believes an executor is running is dictated by the K8s API rather than Spark's RPC events.\n+  // We may need to consider where these perspectives may differ and which perspective should\n+  // take precedence.\n+  private val runningExecutors = mutable.Set.empty[Long]\n+\n+  private var eventProcessorFuture: Future[_] = _\n+\n+  def start(applicationId: String, schedulerBackend: KubernetesClusterSchedulerBackend): Unit = {\n+    require(eventProcessorFuture == null, \"Cannot start event processing twice.\")\n+    logInfo(s\"Starting Kubernetes executor pods event handler for application with\" +\n+      s\" id $applicationId.\")\n+    val eventProcessor = new Runnable {\n+      override def run(): Unit = {\n+        Utils.tryLogNonFatalError {\n+          processEvents(applicationId, schedulerBackend)\n+        }\n+      }\n+    }\n+    eventProcessorFuture = eventProcessorExecutor.scheduleWithFixedDelay(\n+      eventProcessor, 0L, podAllocationDelay, TimeUnit.MILLISECONDS)\n+  }\n+\n+  def stop(): Unit = {\n+    if (eventProcessorFuture != null) {\n+      eventProcessorFuture.cancel(true)\n+      eventProcessorFuture = null\n+    }\n+  }\n+\n+  private def processEvents(\n+      applicationId: String, schedulerBackend: KubernetesClusterSchedulerBackend) {\n+    val currentEvents = new java.util.ArrayList[Seq[Pod]](eventQueue.size())\n+    eventQueue.drainTo(currentEvents)"
  }],
  "prId": 21366
}, {
  "comments": [{
    "author": {
      "login": "liyinan926"
    },
    "body": "I think you can move the code for creating new executors to its own executor service that can actually run concurrently with the executor service for processing pod events. This way you can use independent schedules for the two. I think this is less confusing and allows pod event processing to run with a higher frequency than pod allocation. ",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-05-23T18:30:53Z",
    "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{Future, LinkedBlockingQueue, ScheduledExecutorService, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicInteger, AtomicLong}\n+\n+import io.fabric8.kubernetes.api.model.{Pod, PodBuilder}\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsEventHandler(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    eventProcessorExecutor: ScheduledExecutorService) extends Logging {\n+\n+  import ExecutorPodsEventHandler._\n+\n+  private val EXECUTOR_ID_COUNTER = new AtomicLong(0L)\n+\n+  private val totalExpectedExecutors = new AtomicInteger(0)\n+\n+  private val eventQueue = new LinkedBlockingQueue[Seq[Pod]]()\n+\n+  private val podAllocationSize = conf.get(KUBERNETES_ALLOCATION_BATCH_SIZE)\n+\n+  private val podAllocationDelay = conf.get(KUBERNETES_ALLOCATION_BATCH_DELAY)\n+\n+  private val kubernetesDriverPodName = conf\n+    .get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(throw new SparkException(\"Must specify the driver pod name\"))\n+\n+  private val driverPod = kubernetesClient.pods()\n+    .withName(kubernetesDriverPodName)\n+    .get()\n+\n+  // Use sets of ids instead of counters to be able to handle duplicate events.\n+\n+  // Executor IDs that have been requested from Kubernetes but are not running yet.\n+  private val pendingExecutors = mutable.Set.empty[Long]\n+\n+  // We could use CoarseGrainedSchedulerBackend#totalRegisteredExecutors here for tallying the\n+  // executors that are running. But, here we choose instead to maintain all state within this\n+  // class from the persecptive of the k8s API. Therefore whether or not this scheduler loop\n+  // believes an executor is running is dictated by the K8s API rather than Spark's RPC events.\n+  // We may need to consider where these perspectives may differ and which perspective should\n+  // take precedence.\n+  private val runningExecutors = mutable.Set.empty[Long]\n+\n+  private var eventProcessorFuture: Future[_] = _\n+\n+  def start(applicationId: String, schedulerBackend: KubernetesClusterSchedulerBackend): Unit = {\n+    require(eventProcessorFuture == null, \"Cannot start event processing twice.\")\n+    logInfo(s\"Starting Kubernetes executor pods event handler for application with\" +\n+      s\" id $applicationId.\")\n+    val eventProcessor = new Runnable {\n+      override def run(): Unit = {\n+        Utils.tryLogNonFatalError {\n+          processEvents(applicationId, schedulerBackend)\n+        }\n+      }\n+    }\n+    eventProcessorFuture = eventProcessorExecutor.scheduleWithFixedDelay(\n+      eventProcessor, 0L, podAllocationDelay, TimeUnit.MILLISECONDS)\n+  }\n+\n+  def stop(): Unit = {\n+    if (eventProcessorFuture != null) {\n+      eventProcessorFuture.cancel(true)\n+      eventProcessorFuture = null\n+    }\n+  }\n+\n+  private def processEvents(\n+      applicationId: String, schedulerBackend: KubernetesClusterSchedulerBackend) {\n+    val currentEvents = new java.util.ArrayList[Seq[Pod]](eventQueue.size())\n+    eventQueue.drainTo(currentEvents)\n+    currentEvents.asScala.flatten.foreach { updatedPod =>\n+      val execId = updatedPod.getMetadata.getLabels.get(SPARK_EXECUTOR_ID_LABEL).toLong\n+      val podPhase = updatedPod.getStatus.getPhase.toLowerCase\n+      if (isDeleted(updatedPod)) {\n+        removeExecutorFromSpark(schedulerBackend, updatedPod, execId)\n+      } else {\n+        updatedPod.getStatus.getPhase.toLowerCase match {\n+          case \"running\" =>\n+            // If clause is for resililence to out of order operations - executor must be\n+            // pending and first reach running. Without this check you may e.g. process a\n+            // deletion event followed by some arbitrary modification event - we want the\n+            // deletion event to \"stick\".\n+            if (pendingExecutors.contains(execId)) {\n+              pendingExecutors.remove(execId)\n+              runningExecutors.add(execId)\n+            }\n+          // TODO (SPARK-24135) - handle more classes of errors\n+          case \"error\" | \"failed\" | \"succeeded\" =>\n+            // If deletion failed on a previous try, we can try again if resync informs us the pod\n+            // is still around.\n+            // Delete as best attempt - duplicate deletes will throw an exception but the end state\n+            // of getting rid of the pod is what matters.\n+            if (!isDeleted(updatedPod)) {\n+              Utils.tryLogNonFatalError {\n+                kubernetesClient\n+                  .pods()\n+                  .withName(updatedPod.getMetadata.getName)\n+                  .delete()\n+              }\n+            }\n+            removeExecutorFromSpark(schedulerBackend, updatedPod, execId)\n+        }\n+      }\n+    }\n+\n+    val currentRunningExecutors = runningExecutors.size"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "I thought about this, and prefer the current way because of the ease of reasoning about synchronization. One of the downsides with the previous approach was the need to synchronize state and reason about thread safety in multiple places. With a separate executor service that reads/writes from the `pendingExecutorIds` and `runningExecutorIds` field, we'd have to reason about how the two executors would share about these data structures with the right thread safety model.\r\n\r\nBut what do you think, would the concurrency complexity be manageable and worth the upside of having separate schedules?",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-05-23T19:05:29Z",
    "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{Future, LinkedBlockingQueue, ScheduledExecutorService, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicInteger, AtomicLong}\n+\n+import io.fabric8.kubernetes.api.model.{Pod, PodBuilder}\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsEventHandler(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    eventProcessorExecutor: ScheduledExecutorService) extends Logging {\n+\n+  import ExecutorPodsEventHandler._\n+\n+  private val EXECUTOR_ID_COUNTER = new AtomicLong(0L)\n+\n+  private val totalExpectedExecutors = new AtomicInteger(0)\n+\n+  private val eventQueue = new LinkedBlockingQueue[Seq[Pod]]()\n+\n+  private val podAllocationSize = conf.get(KUBERNETES_ALLOCATION_BATCH_SIZE)\n+\n+  private val podAllocationDelay = conf.get(KUBERNETES_ALLOCATION_BATCH_DELAY)\n+\n+  private val kubernetesDriverPodName = conf\n+    .get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(throw new SparkException(\"Must specify the driver pod name\"))\n+\n+  private val driverPod = kubernetesClient.pods()\n+    .withName(kubernetesDriverPodName)\n+    .get()\n+\n+  // Use sets of ids instead of counters to be able to handle duplicate events.\n+\n+  // Executor IDs that have been requested from Kubernetes but are not running yet.\n+  private val pendingExecutors = mutable.Set.empty[Long]\n+\n+  // We could use CoarseGrainedSchedulerBackend#totalRegisteredExecutors here for tallying the\n+  // executors that are running. But, here we choose instead to maintain all state within this\n+  // class from the persecptive of the k8s API. Therefore whether or not this scheduler loop\n+  // believes an executor is running is dictated by the K8s API rather than Spark's RPC events.\n+  // We may need to consider where these perspectives may differ and which perspective should\n+  // take precedence.\n+  private val runningExecutors = mutable.Set.empty[Long]\n+\n+  private var eventProcessorFuture: Future[_] = _\n+\n+  def start(applicationId: String, schedulerBackend: KubernetesClusterSchedulerBackend): Unit = {\n+    require(eventProcessorFuture == null, \"Cannot start event processing twice.\")\n+    logInfo(s\"Starting Kubernetes executor pods event handler for application with\" +\n+      s\" id $applicationId.\")\n+    val eventProcessor = new Runnable {\n+      override def run(): Unit = {\n+        Utils.tryLogNonFatalError {\n+          processEvents(applicationId, schedulerBackend)\n+        }\n+      }\n+    }\n+    eventProcessorFuture = eventProcessorExecutor.scheduleWithFixedDelay(\n+      eventProcessor, 0L, podAllocationDelay, TimeUnit.MILLISECONDS)\n+  }\n+\n+  def stop(): Unit = {\n+    if (eventProcessorFuture != null) {\n+      eventProcessorFuture.cancel(true)\n+      eventProcessorFuture = null\n+    }\n+  }\n+\n+  private def processEvents(\n+      applicationId: String, schedulerBackend: KubernetesClusterSchedulerBackend) {\n+    val currentEvents = new java.util.ArrayList[Seq[Pod]](eventQueue.size())\n+    eventQueue.drainTo(currentEvents)\n+    currentEvents.asScala.flatten.foreach { updatedPod =>\n+      val execId = updatedPod.getMetadata.getLabels.get(SPARK_EXECUTOR_ID_LABEL).toLong\n+      val podPhase = updatedPod.getStatus.getPhase.toLowerCase\n+      if (isDeleted(updatedPod)) {\n+        removeExecutorFromSpark(schedulerBackend, updatedPod, execId)\n+      } else {\n+        updatedPod.getStatus.getPhase.toLowerCase match {\n+          case \"running\" =>\n+            // If clause is for resililence to out of order operations - executor must be\n+            // pending and first reach running. Without this check you may e.g. process a\n+            // deletion event followed by some arbitrary modification event - we want the\n+            // deletion event to \"stick\".\n+            if (pendingExecutors.contains(execId)) {\n+              pendingExecutors.remove(execId)\n+              runningExecutors.add(execId)\n+            }\n+          // TODO (SPARK-24135) - handle more classes of errors\n+          case \"error\" | \"failed\" | \"succeeded\" =>\n+            // If deletion failed on a previous try, we can try again if resync informs us the pod\n+            // is still around.\n+            // Delete as best attempt - duplicate deletes will throw an exception but the end state\n+            // of getting rid of the pod is what matters.\n+            if (!isDeleted(updatedPod)) {\n+              Utils.tryLogNonFatalError {\n+                kubernetesClient\n+                  .pods()\n+                  .withName(updatedPod.getMetadata.getName)\n+                  .delete()\n+              }\n+            }\n+            removeExecutorFromSpark(schedulerBackend, updatedPod, execId)\n+        }\n+      }\n+    }\n+\n+    val currentRunningExecutors = runningExecutors.size"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "Furthermore I don't think it's just a matter of using thread safe data structures and access to the hash sets, but one also has to think carefully about ordering and atomicity, etc.",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-05-23T19:13:57Z",
    "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{Future, LinkedBlockingQueue, ScheduledExecutorService, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicInteger, AtomicLong}\n+\n+import io.fabric8.kubernetes.api.model.{Pod, PodBuilder}\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsEventHandler(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    eventProcessorExecutor: ScheduledExecutorService) extends Logging {\n+\n+  import ExecutorPodsEventHandler._\n+\n+  private val EXECUTOR_ID_COUNTER = new AtomicLong(0L)\n+\n+  private val totalExpectedExecutors = new AtomicInteger(0)\n+\n+  private val eventQueue = new LinkedBlockingQueue[Seq[Pod]]()\n+\n+  private val podAllocationSize = conf.get(KUBERNETES_ALLOCATION_BATCH_SIZE)\n+\n+  private val podAllocationDelay = conf.get(KUBERNETES_ALLOCATION_BATCH_DELAY)\n+\n+  private val kubernetesDriverPodName = conf\n+    .get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(throw new SparkException(\"Must specify the driver pod name\"))\n+\n+  private val driverPod = kubernetesClient.pods()\n+    .withName(kubernetesDriverPodName)\n+    .get()\n+\n+  // Use sets of ids instead of counters to be able to handle duplicate events.\n+\n+  // Executor IDs that have been requested from Kubernetes but are not running yet.\n+  private val pendingExecutors = mutable.Set.empty[Long]\n+\n+  // We could use CoarseGrainedSchedulerBackend#totalRegisteredExecutors here for tallying the\n+  // executors that are running. But, here we choose instead to maintain all state within this\n+  // class from the persecptive of the k8s API. Therefore whether or not this scheduler loop\n+  // believes an executor is running is dictated by the K8s API rather than Spark's RPC events.\n+  // We may need to consider where these perspectives may differ and which perspective should\n+  // take precedence.\n+  private val runningExecutors = mutable.Set.empty[Long]\n+\n+  private var eventProcessorFuture: Future[_] = _\n+\n+  def start(applicationId: String, schedulerBackend: KubernetesClusterSchedulerBackend): Unit = {\n+    require(eventProcessorFuture == null, \"Cannot start event processing twice.\")\n+    logInfo(s\"Starting Kubernetes executor pods event handler for application with\" +\n+      s\" id $applicationId.\")\n+    val eventProcessor = new Runnable {\n+      override def run(): Unit = {\n+        Utils.tryLogNonFatalError {\n+          processEvents(applicationId, schedulerBackend)\n+        }\n+      }\n+    }\n+    eventProcessorFuture = eventProcessorExecutor.scheduleWithFixedDelay(\n+      eventProcessor, 0L, podAllocationDelay, TimeUnit.MILLISECONDS)\n+  }\n+\n+  def stop(): Unit = {\n+    if (eventProcessorFuture != null) {\n+      eventProcessorFuture.cancel(true)\n+      eventProcessorFuture = null\n+    }\n+  }\n+\n+  private def processEvents(\n+      applicationId: String, schedulerBackend: KubernetesClusterSchedulerBackend) {\n+    val currentEvents = new java.util.ArrayList[Seq[Pod]](eventQueue.size())\n+    eventQueue.drainTo(currentEvents)\n+    currentEvents.asScala.flatten.foreach { updatedPod =>\n+      val execId = updatedPod.getMetadata.getLabels.get(SPARK_EXECUTOR_ID_LABEL).toLong\n+      val podPhase = updatedPod.getStatus.getPhase.toLowerCase\n+      if (isDeleted(updatedPod)) {\n+        removeExecutorFromSpark(schedulerBackend, updatedPod, execId)\n+      } else {\n+        updatedPod.getStatus.getPhase.toLowerCase match {\n+          case \"running\" =>\n+            // If clause is for resililence to out of order operations - executor must be\n+            // pending and first reach running. Without this check you may e.g. process a\n+            // deletion event followed by some arbitrary modification event - we want the\n+            // deletion event to \"stick\".\n+            if (pendingExecutors.contains(execId)) {\n+              pendingExecutors.remove(execId)\n+              runningExecutors.add(execId)\n+            }\n+          // TODO (SPARK-24135) - handle more classes of errors\n+          case \"error\" | \"failed\" | \"succeeded\" =>\n+            // If deletion failed on a previous try, we can try again if resync informs us the pod\n+            // is still around.\n+            // Delete as best attempt - duplicate deletes will throw an exception but the end state\n+            // of getting rid of the pod is what matters.\n+            if (!isDeleted(updatedPod)) {\n+              Utils.tryLogNonFatalError {\n+                kubernetesClient\n+                  .pods()\n+                  .withName(updatedPod.getMetadata.getName)\n+                  .delete()\n+              }\n+            }\n+            removeExecutorFromSpark(schedulerBackend, updatedPod, execId)\n+        }\n+      }\n+    }\n+\n+    val currentRunningExecutors = runningExecutors.size"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "I think the concurrency complexity is totally manageable. The idiom of Kubernetes controllers is you should not be worrying about ordering of events, but instead should be level-triggered. I don't think it makes much sense to use the pod allocation delay as the interval for both executor pod allocation and executor pod event processing. If you feel strongly that the current way is better, I 'm fine with that. But in this case, I think we need to rename of the configuration property for pod allocation interval to something that with the keyword `resync` in it and rename this method to `syncExecutorPods`. Also it's better to at least separate code in this method into two sub-methods.",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-05-23T19:30:49Z",
    "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{Future, LinkedBlockingQueue, ScheduledExecutorService, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicInteger, AtomicLong}\n+\n+import io.fabric8.kubernetes.api.model.{Pod, PodBuilder}\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsEventHandler(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    eventProcessorExecutor: ScheduledExecutorService) extends Logging {\n+\n+  import ExecutorPodsEventHandler._\n+\n+  private val EXECUTOR_ID_COUNTER = new AtomicLong(0L)\n+\n+  private val totalExpectedExecutors = new AtomicInteger(0)\n+\n+  private val eventQueue = new LinkedBlockingQueue[Seq[Pod]]()\n+\n+  private val podAllocationSize = conf.get(KUBERNETES_ALLOCATION_BATCH_SIZE)\n+\n+  private val podAllocationDelay = conf.get(KUBERNETES_ALLOCATION_BATCH_DELAY)\n+\n+  private val kubernetesDriverPodName = conf\n+    .get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(throw new SparkException(\"Must specify the driver pod name\"))\n+\n+  private val driverPod = kubernetesClient.pods()\n+    .withName(kubernetesDriverPodName)\n+    .get()\n+\n+  // Use sets of ids instead of counters to be able to handle duplicate events.\n+\n+  // Executor IDs that have been requested from Kubernetes but are not running yet.\n+  private val pendingExecutors = mutable.Set.empty[Long]\n+\n+  // We could use CoarseGrainedSchedulerBackend#totalRegisteredExecutors here for tallying the\n+  // executors that are running. But, here we choose instead to maintain all state within this\n+  // class from the persecptive of the k8s API. Therefore whether or not this scheduler loop\n+  // believes an executor is running is dictated by the K8s API rather than Spark's RPC events.\n+  // We may need to consider where these perspectives may differ and which perspective should\n+  // take precedence.\n+  private val runningExecutors = mutable.Set.empty[Long]\n+\n+  private var eventProcessorFuture: Future[_] = _\n+\n+  def start(applicationId: String, schedulerBackend: KubernetesClusterSchedulerBackend): Unit = {\n+    require(eventProcessorFuture == null, \"Cannot start event processing twice.\")\n+    logInfo(s\"Starting Kubernetes executor pods event handler for application with\" +\n+      s\" id $applicationId.\")\n+    val eventProcessor = new Runnable {\n+      override def run(): Unit = {\n+        Utils.tryLogNonFatalError {\n+          processEvents(applicationId, schedulerBackend)\n+        }\n+      }\n+    }\n+    eventProcessorFuture = eventProcessorExecutor.scheduleWithFixedDelay(\n+      eventProcessor, 0L, podAllocationDelay, TimeUnit.MILLISECONDS)\n+  }\n+\n+  def stop(): Unit = {\n+    if (eventProcessorFuture != null) {\n+      eventProcessorFuture.cancel(true)\n+      eventProcessorFuture = null\n+    }\n+  }\n+\n+  private def processEvents(\n+      applicationId: String, schedulerBackend: KubernetesClusterSchedulerBackend) {\n+    val currentEvents = new java.util.ArrayList[Seq[Pod]](eventQueue.size())\n+    eventQueue.drainTo(currentEvents)\n+    currentEvents.asScala.flatten.foreach { updatedPod =>\n+      val execId = updatedPod.getMetadata.getLabels.get(SPARK_EXECUTOR_ID_LABEL).toLong\n+      val podPhase = updatedPod.getStatus.getPhase.toLowerCase\n+      if (isDeleted(updatedPod)) {\n+        removeExecutorFromSpark(schedulerBackend, updatedPod, execId)\n+      } else {\n+        updatedPod.getStatus.getPhase.toLowerCase match {\n+          case \"running\" =>\n+            // If clause is for resililence to out of order operations - executor must be\n+            // pending and first reach running. Without this check you may e.g. process a\n+            // deletion event followed by some arbitrary modification event - we want the\n+            // deletion event to \"stick\".\n+            if (pendingExecutors.contains(execId)) {\n+              pendingExecutors.remove(execId)\n+              runningExecutors.add(execId)\n+            }\n+          // TODO (SPARK-24135) - handle more classes of errors\n+          case \"error\" | \"failed\" | \"succeeded\" =>\n+            // If deletion failed on a previous try, we can try again if resync informs us the pod\n+            // is still around.\n+            // Delete as best attempt - duplicate deletes will throw an exception but the end state\n+            // of getting rid of the pod is what matters.\n+            if (!isDeleted(updatedPod)) {\n+              Utils.tryLogNonFatalError {\n+                kubernetesClient\n+                  .pods()\n+                  .withName(updatedPod.getMetadata.getName)\n+                  .delete()\n+              }\n+            }\n+            removeExecutorFromSpark(schedulerBackend, updatedPod, execId)\n+        }\n+      }\n+    }\n+\n+    val currentRunningExecutors = runningExecutors.size"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "I like the idea of not having this method be a single large method and not having this class need to deal with everything. I'll try a patch that separates out the executor construction from the event handling, but, once that patch is out please review it carefully to ensure that the concurrency semantics are correct.",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-05-23T20:22:03Z",
    "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{Future, LinkedBlockingQueue, ScheduledExecutorService, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicInteger, AtomicLong}\n+\n+import io.fabric8.kubernetes.api.model.{Pod, PodBuilder}\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsEventHandler(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    eventProcessorExecutor: ScheduledExecutorService) extends Logging {\n+\n+  import ExecutorPodsEventHandler._\n+\n+  private val EXECUTOR_ID_COUNTER = new AtomicLong(0L)\n+\n+  private val totalExpectedExecutors = new AtomicInteger(0)\n+\n+  private val eventQueue = new LinkedBlockingQueue[Seq[Pod]]()\n+\n+  private val podAllocationSize = conf.get(KUBERNETES_ALLOCATION_BATCH_SIZE)\n+\n+  private val podAllocationDelay = conf.get(KUBERNETES_ALLOCATION_BATCH_DELAY)\n+\n+  private val kubernetesDriverPodName = conf\n+    .get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(throw new SparkException(\"Must specify the driver pod name\"))\n+\n+  private val driverPod = kubernetesClient.pods()\n+    .withName(kubernetesDriverPodName)\n+    .get()\n+\n+  // Use sets of ids instead of counters to be able to handle duplicate events.\n+\n+  // Executor IDs that have been requested from Kubernetes but are not running yet.\n+  private val pendingExecutors = mutable.Set.empty[Long]\n+\n+  // We could use CoarseGrainedSchedulerBackend#totalRegisteredExecutors here for tallying the\n+  // executors that are running. But, here we choose instead to maintain all state within this\n+  // class from the persecptive of the k8s API. Therefore whether or not this scheduler loop\n+  // believes an executor is running is dictated by the K8s API rather than Spark's RPC events.\n+  // We may need to consider where these perspectives may differ and which perspective should\n+  // take precedence.\n+  private val runningExecutors = mutable.Set.empty[Long]\n+\n+  private var eventProcessorFuture: Future[_] = _\n+\n+  def start(applicationId: String, schedulerBackend: KubernetesClusterSchedulerBackend): Unit = {\n+    require(eventProcessorFuture == null, \"Cannot start event processing twice.\")\n+    logInfo(s\"Starting Kubernetes executor pods event handler for application with\" +\n+      s\" id $applicationId.\")\n+    val eventProcessor = new Runnable {\n+      override def run(): Unit = {\n+        Utils.tryLogNonFatalError {\n+          processEvents(applicationId, schedulerBackend)\n+        }\n+      }\n+    }\n+    eventProcessorFuture = eventProcessorExecutor.scheduleWithFixedDelay(\n+      eventProcessor, 0L, podAllocationDelay, TimeUnit.MILLISECONDS)\n+  }\n+\n+  def stop(): Unit = {\n+    if (eventProcessorFuture != null) {\n+      eventProcessorFuture.cancel(true)\n+      eventProcessorFuture = null\n+    }\n+  }\n+\n+  private def processEvents(\n+      applicationId: String, schedulerBackend: KubernetesClusterSchedulerBackend) {\n+    val currentEvents = new java.util.ArrayList[Seq[Pod]](eventQueue.size())\n+    eventQueue.drainTo(currentEvents)\n+    currentEvents.asScala.flatten.foreach { updatedPod =>\n+      val execId = updatedPod.getMetadata.getLabels.get(SPARK_EXECUTOR_ID_LABEL).toLong\n+      val podPhase = updatedPod.getStatus.getPhase.toLowerCase\n+      if (isDeleted(updatedPod)) {\n+        removeExecutorFromSpark(schedulerBackend, updatedPod, execId)\n+      } else {\n+        updatedPod.getStatus.getPhase.toLowerCase match {\n+          case \"running\" =>\n+            // If clause is for resililence to out of order operations - executor must be\n+            // pending and first reach running. Without this check you may e.g. process a\n+            // deletion event followed by some arbitrary modification event - we want the\n+            // deletion event to \"stick\".\n+            if (pendingExecutors.contains(execId)) {\n+              pendingExecutors.remove(execId)\n+              runningExecutors.add(execId)\n+            }\n+          // TODO (SPARK-24135) - handle more classes of errors\n+          case \"error\" | \"failed\" | \"succeeded\" =>\n+            // If deletion failed on a previous try, we can try again if resync informs us the pod\n+            // is still around.\n+            // Delete as best attempt - duplicate deletes will throw an exception but the end state\n+            // of getting rid of the pod is what matters.\n+            if (!isDeleted(updatedPod)) {\n+              Utils.tryLogNonFatalError {\n+                kubernetesClient\n+                  .pods()\n+                  .withName(updatedPod.getMetadata.getName)\n+                  .delete()\n+              }\n+            }\n+            removeExecutorFromSpark(schedulerBackend, updatedPod, execId)\n+        }\n+      }\n+    }\n+\n+    val currentRunningExecutors = runningExecutors.size"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "Had another idea - you can denormalize the state, meaning each event processor module maintains its own copy of the perceived state. We get the modularization and separate scheduling, without having to worry about thread safety. The amount of state being duplicated is quite small. I'll propose the idea and would appreciate feedback once it's in.",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-05-23T20:34:13Z",
    "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{Future, LinkedBlockingQueue, ScheduledExecutorService, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicInteger, AtomicLong}\n+\n+import io.fabric8.kubernetes.api.model.{Pod, PodBuilder}\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsEventHandler(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    eventProcessorExecutor: ScheduledExecutorService) extends Logging {\n+\n+  import ExecutorPodsEventHandler._\n+\n+  private val EXECUTOR_ID_COUNTER = new AtomicLong(0L)\n+\n+  private val totalExpectedExecutors = new AtomicInteger(0)\n+\n+  private val eventQueue = new LinkedBlockingQueue[Seq[Pod]]()\n+\n+  private val podAllocationSize = conf.get(KUBERNETES_ALLOCATION_BATCH_SIZE)\n+\n+  private val podAllocationDelay = conf.get(KUBERNETES_ALLOCATION_BATCH_DELAY)\n+\n+  private val kubernetesDriverPodName = conf\n+    .get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(throw new SparkException(\"Must specify the driver pod name\"))\n+\n+  private val driverPod = kubernetesClient.pods()\n+    .withName(kubernetesDriverPodName)\n+    .get()\n+\n+  // Use sets of ids instead of counters to be able to handle duplicate events.\n+\n+  // Executor IDs that have been requested from Kubernetes but are not running yet.\n+  private val pendingExecutors = mutable.Set.empty[Long]\n+\n+  // We could use CoarseGrainedSchedulerBackend#totalRegisteredExecutors here for tallying the\n+  // executors that are running. But, here we choose instead to maintain all state within this\n+  // class from the persecptive of the k8s API. Therefore whether or not this scheduler loop\n+  // believes an executor is running is dictated by the K8s API rather than Spark's RPC events.\n+  // We may need to consider where these perspectives may differ and which perspective should\n+  // take precedence.\n+  private val runningExecutors = mutable.Set.empty[Long]\n+\n+  private var eventProcessorFuture: Future[_] = _\n+\n+  def start(applicationId: String, schedulerBackend: KubernetesClusterSchedulerBackend): Unit = {\n+    require(eventProcessorFuture == null, \"Cannot start event processing twice.\")\n+    logInfo(s\"Starting Kubernetes executor pods event handler for application with\" +\n+      s\" id $applicationId.\")\n+    val eventProcessor = new Runnable {\n+      override def run(): Unit = {\n+        Utils.tryLogNonFatalError {\n+          processEvents(applicationId, schedulerBackend)\n+        }\n+      }\n+    }\n+    eventProcessorFuture = eventProcessorExecutor.scheduleWithFixedDelay(\n+      eventProcessor, 0L, podAllocationDelay, TimeUnit.MILLISECONDS)\n+  }\n+\n+  def stop(): Unit = {\n+    if (eventProcessorFuture != null) {\n+      eventProcessorFuture.cancel(true)\n+      eventProcessorFuture = null\n+    }\n+  }\n+\n+  private def processEvents(\n+      applicationId: String, schedulerBackend: KubernetesClusterSchedulerBackend) {\n+    val currentEvents = new java.util.ArrayList[Seq[Pod]](eventQueue.size())\n+    eventQueue.drainTo(currentEvents)\n+    currentEvents.asScala.flatten.foreach { updatedPod =>\n+      val execId = updatedPod.getMetadata.getLabels.get(SPARK_EXECUTOR_ID_LABEL).toLong\n+      val podPhase = updatedPod.getStatus.getPhase.toLowerCase\n+      if (isDeleted(updatedPod)) {\n+        removeExecutorFromSpark(schedulerBackend, updatedPod, execId)\n+      } else {\n+        updatedPod.getStatus.getPhase.toLowerCase match {\n+          case \"running\" =>\n+            // If clause is for resililence to out of order operations - executor must be\n+            // pending and first reach running. Without this check you may e.g. process a\n+            // deletion event followed by some arbitrary modification event - we want the\n+            // deletion event to \"stick\".\n+            if (pendingExecutors.contains(execId)) {\n+              pendingExecutors.remove(execId)\n+              runningExecutors.add(execId)\n+            }\n+          // TODO (SPARK-24135) - handle more classes of errors\n+          case \"error\" | \"failed\" | \"succeeded\" =>\n+            // If deletion failed on a previous try, we can try again if resync informs us the pod\n+            // is still around.\n+            // Delete as best attempt - duplicate deletes will throw an exception but the end state\n+            // of getting rid of the pod is what matters.\n+            if (!isDeleted(updatedPod)) {\n+              Utils.tryLogNonFatalError {\n+                kubernetesClient\n+                  .pods()\n+                  .withName(updatedPod.getMetadata.getName)\n+                  .delete()\n+              }\n+            }\n+            removeExecutorFromSpark(schedulerBackend, updatedPod, execId)\n+        }\n+      }\n+    }\n+\n+    val currentRunningExecutors = runningExecutors.size"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Sounds good to me.",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-05-23T21:03:31Z",
    "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{Future, LinkedBlockingQueue, ScheduledExecutorService, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicInteger, AtomicLong}\n+\n+import io.fabric8.kubernetes.api.model.{Pod, PodBuilder}\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsEventHandler(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    eventProcessorExecutor: ScheduledExecutorService) extends Logging {\n+\n+  import ExecutorPodsEventHandler._\n+\n+  private val EXECUTOR_ID_COUNTER = new AtomicLong(0L)\n+\n+  private val totalExpectedExecutors = new AtomicInteger(0)\n+\n+  private val eventQueue = new LinkedBlockingQueue[Seq[Pod]]()\n+\n+  private val podAllocationSize = conf.get(KUBERNETES_ALLOCATION_BATCH_SIZE)\n+\n+  private val podAllocationDelay = conf.get(KUBERNETES_ALLOCATION_BATCH_DELAY)\n+\n+  private val kubernetesDriverPodName = conf\n+    .get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(throw new SparkException(\"Must specify the driver pod name\"))\n+\n+  private val driverPod = kubernetesClient.pods()\n+    .withName(kubernetesDriverPodName)\n+    .get()\n+\n+  // Use sets of ids instead of counters to be able to handle duplicate events.\n+\n+  // Executor IDs that have been requested from Kubernetes but are not running yet.\n+  private val pendingExecutors = mutable.Set.empty[Long]\n+\n+  // We could use CoarseGrainedSchedulerBackend#totalRegisteredExecutors here for tallying the\n+  // executors that are running. But, here we choose instead to maintain all state within this\n+  // class from the persecptive of the k8s API. Therefore whether or not this scheduler loop\n+  // believes an executor is running is dictated by the K8s API rather than Spark's RPC events.\n+  // We may need to consider where these perspectives may differ and which perspective should\n+  // take precedence.\n+  private val runningExecutors = mutable.Set.empty[Long]\n+\n+  private var eventProcessorFuture: Future[_] = _\n+\n+  def start(applicationId: String, schedulerBackend: KubernetesClusterSchedulerBackend): Unit = {\n+    require(eventProcessorFuture == null, \"Cannot start event processing twice.\")\n+    logInfo(s\"Starting Kubernetes executor pods event handler for application with\" +\n+      s\" id $applicationId.\")\n+    val eventProcessor = new Runnable {\n+      override def run(): Unit = {\n+        Utils.tryLogNonFatalError {\n+          processEvents(applicationId, schedulerBackend)\n+        }\n+      }\n+    }\n+    eventProcessorFuture = eventProcessorExecutor.scheduleWithFixedDelay(\n+      eventProcessor, 0L, podAllocationDelay, TimeUnit.MILLISECONDS)\n+  }\n+\n+  def stop(): Unit = {\n+    if (eventProcessorFuture != null) {\n+      eventProcessorFuture.cancel(true)\n+      eventProcessorFuture = null\n+    }\n+  }\n+\n+  private def processEvents(\n+      applicationId: String, schedulerBackend: KubernetesClusterSchedulerBackend) {\n+    val currentEvents = new java.util.ArrayList[Seq[Pod]](eventQueue.size())\n+    eventQueue.drainTo(currentEvents)\n+    currentEvents.asScala.flatten.foreach { updatedPod =>\n+      val execId = updatedPod.getMetadata.getLabels.get(SPARK_EXECUTOR_ID_LABEL).toLong\n+      val podPhase = updatedPod.getStatus.getPhase.toLowerCase\n+      if (isDeleted(updatedPod)) {\n+        removeExecutorFromSpark(schedulerBackend, updatedPod, execId)\n+      } else {\n+        updatedPod.getStatus.getPhase.toLowerCase match {\n+          case \"running\" =>\n+            // If clause is for resililence to out of order operations - executor must be\n+            // pending and first reach running. Without this check you may e.g. process a\n+            // deletion event followed by some arbitrary modification event - we want the\n+            // deletion event to \"stick\".\n+            if (pendingExecutors.contains(execId)) {\n+              pendingExecutors.remove(execId)\n+              runningExecutors.add(execId)\n+            }\n+          // TODO (SPARK-24135) - handle more classes of errors\n+          case \"error\" | \"failed\" | \"succeeded\" =>\n+            // If deletion failed on a previous try, we can try again if resync informs us the pod\n+            // is still around.\n+            // Delete as best attempt - duplicate deletes will throw an exception but the end state\n+            // of getting rid of the pod is what matters.\n+            if (!isDeleted(updatedPod)) {\n+              Utils.tryLogNonFatalError {\n+                kubernetesClient\n+                  .pods()\n+                  .withName(updatedPod.getMetadata.getName)\n+                  .delete()\n+              }\n+            }\n+            removeExecutorFromSpark(schedulerBackend, updatedPod, execId)\n+        }\n+      }\n+    }\n+\n+    val currentRunningExecutors = runningExecutors.size"
  }],
  "prId": 21366
}, {
  "comments": [{
    "author": {
      "login": "liyinan926"
    },
    "body": "Instead of storing a `Seq` of events, how about storing events directly, and calling `eventQueue.add` for each pod here? Then you don't need to call `flatten` when processing the events.",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-05-23T18:35:39Z",
    "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{Future, LinkedBlockingQueue, ScheduledExecutorService, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicInteger, AtomicLong}\n+\n+import io.fabric8.kubernetes.api.model.{Pod, PodBuilder}\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsEventHandler(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    eventProcessorExecutor: ScheduledExecutorService) extends Logging {\n+\n+  import ExecutorPodsEventHandler._\n+\n+  private val EXECUTOR_ID_COUNTER = new AtomicLong(0L)\n+\n+  private val totalExpectedExecutors = new AtomicInteger(0)\n+\n+  private val eventQueue = new LinkedBlockingQueue[Seq[Pod]]()\n+\n+  private val podAllocationSize = conf.get(KUBERNETES_ALLOCATION_BATCH_SIZE)\n+\n+  private val podAllocationDelay = conf.get(KUBERNETES_ALLOCATION_BATCH_DELAY)\n+\n+  private val kubernetesDriverPodName = conf\n+    .get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(throw new SparkException(\"Must specify the driver pod name\"))\n+\n+  private val driverPod = kubernetesClient.pods()\n+    .withName(kubernetesDriverPodName)\n+    .get()\n+\n+  // Use sets of ids instead of counters to be able to handle duplicate events.\n+\n+  // Executor IDs that have been requested from Kubernetes but are not running yet.\n+  private val pendingExecutors = mutable.Set.empty[Long]\n+\n+  // We could use CoarseGrainedSchedulerBackend#totalRegisteredExecutors here for tallying the\n+  // executors that are running. But, here we choose instead to maintain all state within this\n+  // class from the persecptive of the k8s API. Therefore whether or not this scheduler loop\n+  // believes an executor is running is dictated by the K8s API rather than Spark's RPC events.\n+  // We may need to consider where these perspectives may differ and which perspective should\n+  // take precedence.\n+  private val runningExecutors = mutable.Set.empty[Long]\n+\n+  private var eventProcessorFuture: Future[_] = _\n+\n+  def start(applicationId: String, schedulerBackend: KubernetesClusterSchedulerBackend): Unit = {\n+    require(eventProcessorFuture == null, \"Cannot start event processing twice.\")\n+    logInfo(s\"Starting Kubernetes executor pods event handler for application with\" +\n+      s\" id $applicationId.\")\n+    val eventProcessor = new Runnable {\n+      override def run(): Unit = {\n+        Utils.tryLogNonFatalError {\n+          processEvents(applicationId, schedulerBackend)\n+        }\n+      }\n+    }\n+    eventProcessorFuture = eventProcessorExecutor.scheduleWithFixedDelay(\n+      eventProcessor, 0L, podAllocationDelay, TimeUnit.MILLISECONDS)\n+  }\n+\n+  def stop(): Unit = {\n+    if (eventProcessorFuture != null) {\n+      eventProcessorFuture.cancel(true)\n+      eventProcessorFuture = null\n+    }\n+  }\n+\n+  private def processEvents(\n+      applicationId: String, schedulerBackend: KubernetesClusterSchedulerBackend) {\n+    val currentEvents = new java.util.ArrayList[Seq[Pod]](eventQueue.size())\n+    eventQueue.drainTo(currentEvents)\n+    currentEvents.asScala.flatten.foreach { updatedPod =>\n+      val execId = updatedPod.getMetadata.getLabels.get(SPARK_EXECUTOR_ID_LABEL).toLong\n+      val podPhase = updatedPod.getStatus.getPhase.toLowerCase\n+      if (isDeleted(updatedPod)) {\n+        removeExecutorFromSpark(schedulerBackend, updatedPod, execId)\n+      } else {\n+        updatedPod.getStatus.getPhase.toLowerCase match {\n+          case \"running\" =>\n+            // If clause is for resililence to out of order operations - executor must be\n+            // pending and first reach running. Without this check you may e.g. process a\n+            // deletion event followed by some arbitrary modification event - we want the\n+            // deletion event to \"stick\".\n+            if (pendingExecutors.contains(execId)) {\n+              pendingExecutors.remove(execId)\n+              runningExecutors.add(execId)\n+            }\n+          // TODO (SPARK-24135) - handle more classes of errors\n+          case \"error\" | \"failed\" | \"succeeded\" =>\n+            // If deletion failed on a previous try, we can try again if resync informs us the pod\n+            // is still around.\n+            // Delete as best attempt - duplicate deletes will throw an exception but the end state\n+            // of getting rid of the pod is what matters.\n+            if (!isDeleted(updatedPod)) {\n+              Utils.tryLogNonFatalError {\n+                kubernetesClient\n+                  .pods()\n+                  .withName(updatedPod.getMetadata.getName)\n+                  .delete()\n+              }\n+            }\n+            removeExecutorFromSpark(schedulerBackend, updatedPod, execId)\n+        }\n+      }\n+    }\n+\n+    val currentRunningExecutors = runningExecutors.size\n+    val currentTotalExpectedExecutors = totalExpectedExecutors.get\n+    if (pendingExecutors.isEmpty && currentRunningExecutors < currentTotalExpectedExecutors) {\n+      val numExecutorsToAllocate = math.min(\n+        currentTotalExpectedExecutors - currentRunningExecutors, podAllocationSize)\n+      logInfo(s\"Going to request $numExecutorsToAllocate executors from Kubernetes.\")\n+      val newExecutorIds = mutable.Buffer.empty[Long]\n+      val podsToAllocate = mutable.Buffer.empty[Pod]\n+      for ( _ <- 0 until numExecutorsToAllocate) {\n+        val newExecutorId = EXECUTOR_ID_COUNTER.incrementAndGet()\n+        val executorConf = KubernetesConf.createExecutorConf(\n+          conf,\n+          newExecutorId.toString,\n+          applicationId,\n+          driverPod)\n+        val executorPod = executorBuilder.buildFromFeatures(executorConf)\n+        val podWithAttachedContainer = new PodBuilder(executorPod.pod)\n+          .editOrNewSpec()\n+          .addToContainers(executorPod.container)\n+          .endSpec()\n+          .build()\n+        kubernetesClient.pods().create(podWithAttachedContainer)\n+        pendingExecutors += newExecutorId\n+      }\n+    } else if (currentRunningExecutors == currentTotalExpectedExecutors) {\n+      logDebug(\"Current number of running executors is equal to the number of requested\" +\n+        \" executors. Not scaling up further.\")\n+    } else if (pendingExecutors.nonEmpty) {\n+      logInfo(s\"Still waiting for ${pendingExecutors.size} executors to begin running before\" +\n+        s\" requesting for more executors.\")\n+    }\n+  }\n+\n+  def sendUpdatedPodMetadata(updatedPod: Pod): Unit = {\n+    eventQueue.add(Seq(updatedPod))\n+  }\n+\n+  def sendUpdatedPodMetadata(updatedPods: Iterable[Pod]): Unit = {\n+    eventQueue.add(updatedPods.toSeq)"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "Thought about this, I prefer the idea of the polling executor being able to add all of its pods that it saw in a round as an atomic operation. But as long as we're handling out of order events properly perhaps the difference is negligible.",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-05-23T19:08:00Z",
    "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{Future, LinkedBlockingQueue, ScheduledExecutorService, TimeUnit}\n+import java.util.concurrent.atomic.{AtomicInteger, AtomicLong}\n+\n+import io.fabric8.kubernetes.api.model.{Pod, PodBuilder}\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsEventHandler(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    eventProcessorExecutor: ScheduledExecutorService) extends Logging {\n+\n+  import ExecutorPodsEventHandler._\n+\n+  private val EXECUTOR_ID_COUNTER = new AtomicLong(0L)\n+\n+  private val totalExpectedExecutors = new AtomicInteger(0)\n+\n+  private val eventQueue = new LinkedBlockingQueue[Seq[Pod]]()\n+\n+  private val podAllocationSize = conf.get(KUBERNETES_ALLOCATION_BATCH_SIZE)\n+\n+  private val podAllocationDelay = conf.get(KUBERNETES_ALLOCATION_BATCH_DELAY)\n+\n+  private val kubernetesDriverPodName = conf\n+    .get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(throw new SparkException(\"Must specify the driver pod name\"))\n+\n+  private val driverPod = kubernetesClient.pods()\n+    .withName(kubernetesDriverPodName)\n+    .get()\n+\n+  // Use sets of ids instead of counters to be able to handle duplicate events.\n+\n+  // Executor IDs that have been requested from Kubernetes but are not running yet.\n+  private val pendingExecutors = mutable.Set.empty[Long]\n+\n+  // We could use CoarseGrainedSchedulerBackend#totalRegisteredExecutors here for tallying the\n+  // executors that are running. But, here we choose instead to maintain all state within this\n+  // class from the persecptive of the k8s API. Therefore whether or not this scheduler loop\n+  // believes an executor is running is dictated by the K8s API rather than Spark's RPC events.\n+  // We may need to consider where these perspectives may differ and which perspective should\n+  // take precedence.\n+  private val runningExecutors = mutable.Set.empty[Long]\n+\n+  private var eventProcessorFuture: Future[_] = _\n+\n+  def start(applicationId: String, schedulerBackend: KubernetesClusterSchedulerBackend): Unit = {\n+    require(eventProcessorFuture == null, \"Cannot start event processing twice.\")\n+    logInfo(s\"Starting Kubernetes executor pods event handler for application with\" +\n+      s\" id $applicationId.\")\n+    val eventProcessor = new Runnable {\n+      override def run(): Unit = {\n+        Utils.tryLogNonFatalError {\n+          processEvents(applicationId, schedulerBackend)\n+        }\n+      }\n+    }\n+    eventProcessorFuture = eventProcessorExecutor.scheduleWithFixedDelay(\n+      eventProcessor, 0L, podAllocationDelay, TimeUnit.MILLISECONDS)\n+  }\n+\n+  def stop(): Unit = {\n+    if (eventProcessorFuture != null) {\n+      eventProcessorFuture.cancel(true)\n+      eventProcessorFuture = null\n+    }\n+  }\n+\n+  private def processEvents(\n+      applicationId: String, schedulerBackend: KubernetesClusterSchedulerBackend) {\n+    val currentEvents = new java.util.ArrayList[Seq[Pod]](eventQueue.size())\n+    eventQueue.drainTo(currentEvents)\n+    currentEvents.asScala.flatten.foreach { updatedPod =>\n+      val execId = updatedPod.getMetadata.getLabels.get(SPARK_EXECUTOR_ID_LABEL).toLong\n+      val podPhase = updatedPod.getStatus.getPhase.toLowerCase\n+      if (isDeleted(updatedPod)) {\n+        removeExecutorFromSpark(schedulerBackend, updatedPod, execId)\n+      } else {\n+        updatedPod.getStatus.getPhase.toLowerCase match {\n+          case \"running\" =>\n+            // If clause is for resililence to out of order operations - executor must be\n+            // pending and first reach running. Without this check you may e.g. process a\n+            // deletion event followed by some arbitrary modification event - we want the\n+            // deletion event to \"stick\".\n+            if (pendingExecutors.contains(execId)) {\n+              pendingExecutors.remove(execId)\n+              runningExecutors.add(execId)\n+            }\n+          // TODO (SPARK-24135) - handle more classes of errors\n+          case \"error\" | \"failed\" | \"succeeded\" =>\n+            // If deletion failed on a previous try, we can try again if resync informs us the pod\n+            // is still around.\n+            // Delete as best attempt - duplicate deletes will throw an exception but the end state\n+            // of getting rid of the pod is what matters.\n+            if (!isDeleted(updatedPod)) {\n+              Utils.tryLogNonFatalError {\n+                kubernetesClient\n+                  .pods()\n+                  .withName(updatedPod.getMetadata.getName)\n+                  .delete()\n+              }\n+            }\n+            removeExecutorFromSpark(schedulerBackend, updatedPod, execId)\n+        }\n+      }\n+    }\n+\n+    val currentRunningExecutors = runningExecutors.size\n+    val currentTotalExpectedExecutors = totalExpectedExecutors.get\n+    if (pendingExecutors.isEmpty && currentRunningExecutors < currentTotalExpectedExecutors) {\n+      val numExecutorsToAllocate = math.min(\n+        currentTotalExpectedExecutors - currentRunningExecutors, podAllocationSize)\n+      logInfo(s\"Going to request $numExecutorsToAllocate executors from Kubernetes.\")\n+      val newExecutorIds = mutable.Buffer.empty[Long]\n+      val podsToAllocate = mutable.Buffer.empty[Pod]\n+      for ( _ <- 0 until numExecutorsToAllocate) {\n+        val newExecutorId = EXECUTOR_ID_COUNTER.incrementAndGet()\n+        val executorConf = KubernetesConf.createExecutorConf(\n+          conf,\n+          newExecutorId.toString,\n+          applicationId,\n+          driverPod)\n+        val executorPod = executorBuilder.buildFromFeatures(executorConf)\n+        val podWithAttachedContainer = new PodBuilder(executorPod.pod)\n+          .editOrNewSpec()\n+          .addToContainers(executorPod.container)\n+          .endSpec()\n+          .build()\n+        kubernetesClient.pods().create(podWithAttachedContainer)\n+        pendingExecutors += newExecutorId\n+      }\n+    } else if (currentRunningExecutors == currentTotalExpectedExecutors) {\n+      logDebug(\"Current number of running executors is equal to the number of requested\" +\n+        \" executors. Not scaling up further.\")\n+    } else if (pendingExecutors.nonEmpty) {\n+      logInfo(s\"Still waiting for ${pendingExecutors.size} executors to begin running before\" +\n+        s\" requesting for more executors.\")\n+    }\n+  }\n+\n+  def sendUpdatedPodMetadata(updatedPod: Pod): Unit = {\n+    eventQueue.add(Seq(updatedPod))\n+  }\n+\n+  def sendUpdatedPodMetadata(updatedPods: Iterable[Pod]): Unit = {\n+    eventQueue.add(updatedPods.toSeq)"
  }],
  "prId": 21366
}]