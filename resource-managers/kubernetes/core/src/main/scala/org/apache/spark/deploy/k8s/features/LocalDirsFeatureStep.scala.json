[{
  "comments": [{
    "author": {
      "login": "ifilonenko"
    },
    "body": "Checking current volumes in a feature step isn't consistent with the additive design of the feature builder pattern. @mccheah to comment",
    "commit": "90a5ae5e8529e546b14fd9e7f9ae8a21b648ed8e",
    "createdAt": "2018-08-29T17:31:25Z",
    "diffHunk": "@@ -37,41 +40,99 @@ private[spark] class LocalDirsFeatureStep(\n     .orElse(conf.getOption(\"spark.local.dir\"))\n     .getOrElse(defaultLocalDir)\n     .split(\",\")\n+  private val useLocalDirTmpFs = conf.get(KUBERNETES_LOCAL_DIRS_TMPFS)\n \n   override def configurePod(pod: SparkPod): SparkPod = {\n     val localDirVolumes = resolvedLocalDirs\n       .zipWithIndex\n       .map { case (localDir, index) =>\n-        new VolumeBuilder()\n-          .withName(s\"spark-local-dir-${index + 1}\")\n-          .withNewEmptyDir()\n-          .endEmptyDir()\n-          .build()\n+        val name = s\"spark-local-dir-${index + 1}\"\n+        // To allow customisation of local dirs backing volumes we should avoid creating\n+        // emptyDir volumes if the volume is already defined in the pod spec\n+        hasVolume(pod, name) match {\n+          case true =>\n+            // For pre-existing volume definitions just re-use the volume\n+            pod.pod.getSpec().getVolumes().asScala.find(v => v.getName.equals(name)).get\n+          case false =>\n+            // Create new emptyDir volume\n+            new VolumeBuilder()\n+              .withName(name)\n+              .withNewEmptyDir()\n+                .withMedium(useLocalDirTmpFs match {\n+                  case true => \"Memory\" // Use tmpfs\n+                  case false => null    // Default - use nodes backing storage\n+                })\n+              .endEmptyDir()\n+              .build()\n+        }\n       }\n+\n     val localDirVolumeMounts = localDirVolumes\n       .zip(resolvedLocalDirs)\n       .map { case (localDirVolume, localDirPath) =>\n-        new VolumeMountBuilder()\n-          .withName(localDirVolume.getName)\n-          .withMountPath(localDirPath)\n-          .build()\n+        hasVolumeMount(pod, localDirVolume.getName, localDirPath) match {\n+          case true =>\n+            // For pre-existing volume mounts just re-use the mount\n+            pod.container.getVolumeMounts().asScala\n+              .find(m => m.getName.equals(localDirVolume.getName)\n+                         && m.getMountPath.equals(localDirPath))\n+              .get\n+          case false =>\n+            // Create new volume mount\n+            new VolumeMountBuilder()\n+              .withName (localDirVolume.getName)\n+              .withMountPath (localDirPath)\n+              .build()\n+        }\n+      }\n+\n+    // Check for conflicting volume mounts\n+    for (m: VolumeMount <- localDirVolumeMounts) {\n+      if (hasConflictingVolumeMount(pod, m.getName, m.getMountPath).size > 0) {\n+        throw new SparkException(s\"Conflicting volume mounts defined, pod template attempted to \" +\n+          \"mount SPARK_LOCAL_DIRS volume ${m.getName} multiple times or at an alternative path \" +\n+          \"then the expected ${m.getPath}\")\n       }\n+    }\n+\n     val podWithLocalDirVolumes = new PodBuilder(pod.pod)\n       .editSpec()\n-        .addToVolumes(localDirVolumes: _*)\n+         // Don't want to re-add volumes that already existed in the incoming spec\n+         // as duplicate definitions will lead to K8S API errors\n+        .addToVolumes(localDirVolumes.filter(v => !hasVolume(pod, v.getName)): _*)",
    "line": 87
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "All of this conflicting volume mount and conflicting volumes seems out of place here. If we're anticipating using the pod template file, keep in mind that the pod template feature is specifically not designed to do any validation. What kinds of errors are we hoping to avoid by doing the deduplication here?",
    "commit": "90a5ae5e8529e546b14fd9e7f9ae8a21b648ed8e",
    "createdAt": "2018-08-29T17:42:01Z",
    "diffHunk": "@@ -37,41 +40,99 @@ private[spark] class LocalDirsFeatureStep(\n     .orElse(conf.getOption(\"spark.local.dir\"))\n     .getOrElse(defaultLocalDir)\n     .split(\",\")\n+  private val useLocalDirTmpFs = conf.get(KUBERNETES_LOCAL_DIRS_TMPFS)\n \n   override def configurePod(pod: SparkPod): SparkPod = {\n     val localDirVolumes = resolvedLocalDirs\n       .zipWithIndex\n       .map { case (localDir, index) =>\n-        new VolumeBuilder()\n-          .withName(s\"spark-local-dir-${index + 1}\")\n-          .withNewEmptyDir()\n-          .endEmptyDir()\n-          .build()\n+        val name = s\"spark-local-dir-${index + 1}\"\n+        // To allow customisation of local dirs backing volumes we should avoid creating\n+        // emptyDir volumes if the volume is already defined in the pod spec\n+        hasVolume(pod, name) match {\n+          case true =>\n+            // For pre-existing volume definitions just re-use the volume\n+            pod.pod.getSpec().getVolumes().asScala.find(v => v.getName.equals(name)).get\n+          case false =>\n+            // Create new emptyDir volume\n+            new VolumeBuilder()\n+              .withName(name)\n+              .withNewEmptyDir()\n+                .withMedium(useLocalDirTmpFs match {\n+                  case true => \"Memory\" // Use tmpfs\n+                  case false => null    // Default - use nodes backing storage\n+                })\n+              .endEmptyDir()\n+              .build()\n+        }\n       }\n+\n     val localDirVolumeMounts = localDirVolumes\n       .zip(resolvedLocalDirs)\n       .map { case (localDirVolume, localDirPath) =>\n-        new VolumeMountBuilder()\n-          .withName(localDirVolume.getName)\n-          .withMountPath(localDirPath)\n-          .build()\n+        hasVolumeMount(pod, localDirVolume.getName, localDirPath) match {\n+          case true =>\n+            // For pre-existing volume mounts just re-use the mount\n+            pod.container.getVolumeMounts().asScala\n+              .find(m => m.getName.equals(localDirVolume.getName)\n+                         && m.getMountPath.equals(localDirPath))\n+              .get\n+          case false =>\n+            // Create new volume mount\n+            new VolumeMountBuilder()\n+              .withName (localDirVolume.getName)\n+              .withMountPath (localDirPath)\n+              .build()\n+        }\n+      }\n+\n+    // Check for conflicting volume mounts\n+    for (m: VolumeMount <- localDirVolumeMounts) {\n+      if (hasConflictingVolumeMount(pod, m.getName, m.getMountPath).size > 0) {\n+        throw new SparkException(s\"Conflicting volume mounts defined, pod template attempted to \" +\n+          \"mount SPARK_LOCAL_DIRS volume ${m.getName} multiple times or at an alternative path \" +\n+          \"then the expected ${m.getPath}\")\n       }\n+    }\n+\n     val podWithLocalDirVolumes = new PodBuilder(pod.pod)\n       .editSpec()\n-        .addToVolumes(localDirVolumes: _*)\n+         // Don't want to re-add volumes that already existed in the incoming spec\n+         // as duplicate definitions will lead to K8S API errors\n+        .addToVolumes(localDirVolumes.filter(v => !hasVolume(pod, v.getName)): _*)",
    "line": 87
  }, {
    "author": {
      "login": "rvesse"
    },
    "body": "The implementation is still additive in that it will add to existing elements in the pod spec as needed but respect what is already present.\r\n\r\nIf your pod spec contains duplicate volumes/volume mounts then K8S will reject it as invalid e.g.\r\n\r\n```\r\nThe Pod \"rvesse-test\" is invalid: spec.volumes[1].name: Duplicate value: \"spark-local-dirs-1\"\r\n```\r\n\r\nTherefore it is necessary to explicitly avoid duplicating things already present in the template\r\n\r\nIf the aim is to replace adding further config options with the pod template feature then the existing builders do need to be more intelligent in what they do to avoid generating invalid pod specs.  This is regardless of whether the template feature is opinionated about validation, even if the template feature doesn't do validation, Spark code itself should be ensuring that it generates valid specs as far as it is able to.  Obviously it can't detect every possible invalid spec that it might generate if the templates aren't being validated but it can avoid introducing easily avoidable invalid specs.",
    "commit": "90a5ae5e8529e546b14fd9e7f9ae8a21b648ed8e",
    "createdAt": "2018-08-30T09:00:36Z",
    "diffHunk": "@@ -37,41 +40,99 @@ private[spark] class LocalDirsFeatureStep(\n     .orElse(conf.getOption(\"spark.local.dir\"))\n     .getOrElse(defaultLocalDir)\n     .split(\",\")\n+  private val useLocalDirTmpFs = conf.get(KUBERNETES_LOCAL_DIRS_TMPFS)\n \n   override def configurePod(pod: SparkPod): SparkPod = {\n     val localDirVolumes = resolvedLocalDirs\n       .zipWithIndex\n       .map { case (localDir, index) =>\n-        new VolumeBuilder()\n-          .withName(s\"spark-local-dir-${index + 1}\")\n-          .withNewEmptyDir()\n-          .endEmptyDir()\n-          .build()\n+        val name = s\"spark-local-dir-${index + 1}\"\n+        // To allow customisation of local dirs backing volumes we should avoid creating\n+        // emptyDir volumes if the volume is already defined in the pod spec\n+        hasVolume(pod, name) match {\n+          case true =>\n+            // For pre-existing volume definitions just re-use the volume\n+            pod.pod.getSpec().getVolumes().asScala.find(v => v.getName.equals(name)).get\n+          case false =>\n+            // Create new emptyDir volume\n+            new VolumeBuilder()\n+              .withName(name)\n+              .withNewEmptyDir()\n+                .withMedium(useLocalDirTmpFs match {\n+                  case true => \"Memory\" // Use tmpfs\n+                  case false => null    // Default - use nodes backing storage\n+                })\n+              .endEmptyDir()\n+              .build()\n+        }\n       }\n+\n     val localDirVolumeMounts = localDirVolumes\n       .zip(resolvedLocalDirs)\n       .map { case (localDirVolume, localDirPath) =>\n-        new VolumeMountBuilder()\n-          .withName(localDirVolume.getName)\n-          .withMountPath(localDirPath)\n-          .build()\n+        hasVolumeMount(pod, localDirVolume.getName, localDirPath) match {\n+          case true =>\n+            // For pre-existing volume mounts just re-use the mount\n+            pod.container.getVolumeMounts().asScala\n+              .find(m => m.getName.equals(localDirVolume.getName)\n+                         && m.getMountPath.equals(localDirPath))\n+              .get\n+          case false =>\n+            // Create new volume mount\n+            new VolumeMountBuilder()\n+              .withName (localDirVolume.getName)\n+              .withMountPath (localDirPath)\n+              .build()\n+        }\n+      }\n+\n+    // Check for conflicting volume mounts\n+    for (m: VolumeMount <- localDirVolumeMounts) {\n+      if (hasConflictingVolumeMount(pod, m.getName, m.getMountPath).size > 0) {\n+        throw new SparkException(s\"Conflicting volume mounts defined, pod template attempted to \" +\n+          \"mount SPARK_LOCAL_DIRS volume ${m.getName} multiple times or at an alternative path \" +\n+          \"then the expected ${m.getPath}\")\n       }\n+    }\n+\n     val podWithLocalDirVolumes = new PodBuilder(pod.pod)\n       .editSpec()\n-        .addToVolumes(localDirVolumes: _*)\n+         // Don't want to re-add volumes that already existed in the incoming spec\n+         // as duplicate definitions will lead to K8S API errors\n+        .addToVolumes(localDirVolumes.filter(v => !hasVolume(pod, v.getName)): _*)",
    "line": 87
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "> This is regardless of whether the template feature is opinionated about validation, even if the template feature doesn't do validation, Spark code itself should be ensuring that it generates valid specs as far as it is able to.\r\n\r\nThis is a stance that as far I'm aware, we specifically chose not to take in the pod template feature. If one is using the pod template feature then Spark won't provide any guarantees that the pod it makes will be well-formed. When spark submit deploys the pod to the cluster the API will return a clear enough error informing the user to make the appropriate corrections to their pod template.\r\n\r\n@onursatici I just checked the pod template files PR, I didn't see this specifically called out - should this be documented?",
    "commit": "90a5ae5e8529e546b14fd9e7f9ae8a21b648ed8e",
    "createdAt": "2018-08-30T17:00:44Z",
    "diffHunk": "@@ -37,41 +40,99 @@ private[spark] class LocalDirsFeatureStep(\n     .orElse(conf.getOption(\"spark.local.dir\"))\n     .getOrElse(defaultLocalDir)\n     .split(\",\")\n+  private val useLocalDirTmpFs = conf.get(KUBERNETES_LOCAL_DIRS_TMPFS)\n \n   override def configurePod(pod: SparkPod): SparkPod = {\n     val localDirVolumes = resolvedLocalDirs\n       .zipWithIndex\n       .map { case (localDir, index) =>\n-        new VolumeBuilder()\n-          .withName(s\"spark-local-dir-${index + 1}\")\n-          .withNewEmptyDir()\n-          .endEmptyDir()\n-          .build()\n+        val name = s\"spark-local-dir-${index + 1}\"\n+        // To allow customisation of local dirs backing volumes we should avoid creating\n+        // emptyDir volumes if the volume is already defined in the pod spec\n+        hasVolume(pod, name) match {\n+          case true =>\n+            // For pre-existing volume definitions just re-use the volume\n+            pod.pod.getSpec().getVolumes().asScala.find(v => v.getName.equals(name)).get\n+          case false =>\n+            // Create new emptyDir volume\n+            new VolumeBuilder()\n+              .withName(name)\n+              .withNewEmptyDir()\n+                .withMedium(useLocalDirTmpFs match {\n+                  case true => \"Memory\" // Use tmpfs\n+                  case false => null    // Default - use nodes backing storage\n+                })\n+              .endEmptyDir()\n+              .build()\n+        }\n       }\n+\n     val localDirVolumeMounts = localDirVolumes\n       .zip(resolvedLocalDirs)\n       .map { case (localDirVolume, localDirPath) =>\n-        new VolumeMountBuilder()\n-          .withName(localDirVolume.getName)\n-          .withMountPath(localDirPath)\n-          .build()\n+        hasVolumeMount(pod, localDirVolume.getName, localDirPath) match {\n+          case true =>\n+            // For pre-existing volume mounts just re-use the mount\n+            pod.container.getVolumeMounts().asScala\n+              .find(m => m.getName.equals(localDirVolume.getName)\n+                         && m.getMountPath.equals(localDirPath))\n+              .get\n+          case false =>\n+            // Create new volume mount\n+            new VolumeMountBuilder()\n+              .withName (localDirVolume.getName)\n+              .withMountPath (localDirPath)\n+              .build()\n+        }\n+      }\n+\n+    // Check for conflicting volume mounts\n+    for (m: VolumeMount <- localDirVolumeMounts) {\n+      if (hasConflictingVolumeMount(pod, m.getName, m.getMountPath).size > 0) {\n+        throw new SparkException(s\"Conflicting volume mounts defined, pod template attempted to \" +\n+          \"mount SPARK_LOCAL_DIRS volume ${m.getName} multiple times or at an alternative path \" +\n+          \"then the expected ${m.getPath}\")\n       }\n+    }\n+\n     val podWithLocalDirVolumes = new PodBuilder(pod.pod)\n       .editSpec()\n-        .addToVolumes(localDirVolumes: _*)\n+         // Don't want to re-add volumes that already existed in the incoming spec\n+         // as duplicate definitions will lead to K8S API errors\n+        .addToVolumes(localDirVolumes.filter(v => !hasVolume(pod, v.getName)): _*)",
    "line": 87
  }, {
    "author": {
      "login": "onursatici"
    },
    "body": "@mccheah yeap we should document that, will add",
    "commit": "90a5ae5e8529e546b14fd9e7f9ae8a21b648ed8e",
    "createdAt": "2018-08-30T22:20:08Z",
    "diffHunk": "@@ -37,41 +40,99 @@ private[spark] class LocalDirsFeatureStep(\n     .orElse(conf.getOption(\"spark.local.dir\"))\n     .getOrElse(defaultLocalDir)\n     .split(\",\")\n+  private val useLocalDirTmpFs = conf.get(KUBERNETES_LOCAL_DIRS_TMPFS)\n \n   override def configurePod(pod: SparkPod): SparkPod = {\n     val localDirVolumes = resolvedLocalDirs\n       .zipWithIndex\n       .map { case (localDir, index) =>\n-        new VolumeBuilder()\n-          .withName(s\"spark-local-dir-${index + 1}\")\n-          .withNewEmptyDir()\n-          .endEmptyDir()\n-          .build()\n+        val name = s\"spark-local-dir-${index + 1}\"\n+        // To allow customisation of local dirs backing volumes we should avoid creating\n+        // emptyDir volumes if the volume is already defined in the pod spec\n+        hasVolume(pod, name) match {\n+          case true =>\n+            // For pre-existing volume definitions just re-use the volume\n+            pod.pod.getSpec().getVolumes().asScala.find(v => v.getName.equals(name)).get\n+          case false =>\n+            // Create new emptyDir volume\n+            new VolumeBuilder()\n+              .withName(name)\n+              .withNewEmptyDir()\n+                .withMedium(useLocalDirTmpFs match {\n+                  case true => \"Memory\" // Use tmpfs\n+                  case false => null    // Default - use nodes backing storage\n+                })\n+              .endEmptyDir()\n+              .build()\n+        }\n       }\n+\n     val localDirVolumeMounts = localDirVolumes\n       .zip(resolvedLocalDirs)\n       .map { case (localDirVolume, localDirPath) =>\n-        new VolumeMountBuilder()\n-          .withName(localDirVolume.getName)\n-          .withMountPath(localDirPath)\n-          .build()\n+        hasVolumeMount(pod, localDirVolume.getName, localDirPath) match {\n+          case true =>\n+            // For pre-existing volume mounts just re-use the mount\n+            pod.container.getVolumeMounts().asScala\n+              .find(m => m.getName.equals(localDirVolume.getName)\n+                         && m.getMountPath.equals(localDirPath))\n+              .get\n+          case false =>\n+            // Create new volume mount\n+            new VolumeMountBuilder()\n+              .withName (localDirVolume.getName)\n+              .withMountPath (localDirPath)\n+              .build()\n+        }\n+      }\n+\n+    // Check for conflicting volume mounts\n+    for (m: VolumeMount <- localDirVolumeMounts) {\n+      if (hasConflictingVolumeMount(pod, m.getName, m.getMountPath).size > 0) {\n+        throw new SparkException(s\"Conflicting volume mounts defined, pod template attempted to \" +\n+          \"mount SPARK_LOCAL_DIRS volume ${m.getName} multiple times or at an alternative path \" +\n+          \"then the expected ${m.getPath}\")\n       }\n+    }\n+\n     val podWithLocalDirVolumes = new PodBuilder(pod.pod)\n       .editSpec()\n-        .addToVolumes(localDirVolumes: _*)\n+         // Don't want to re-add volumes that already existed in the incoming spec\n+         // as duplicate definitions will lead to K8S API errors\n+        .addToVolumes(localDirVolumes.filter(v => !hasVolume(pod, v.getName)): _*)",
    "line": 87
  }, {
    "author": {
      "login": "rvesse"
    },
    "body": "> This is a stance that as far I'm aware, we specifically chose not to take in the pod template feature. If one is using the pod template feature then Spark won't provide any guarantees that the pod it makes will be well-formed. When spark submit deploys the pod to the cluster the API will return a clear enough error informing the user to make the appropriate corrections to their pod template.\r\n\r\nSure, but we still need to be realistic about how the template feature will be used.  It is supposed to enable power users to customise the pods for their environments.  If there is an area like this where there is a clear use case to allow customisation we should be enabling that rather than saying sorry we're going to generate invalid pods regardless.  Obviously the power user is assuming the risk of creating a pod template that meaningfully combines with Sparks generated pod to yield a valid runtime environment.\r\n\r\nClearly my stance here is controversial and likely needs a broader discussion on the dev list.\r\n\r\nI can reduce this PR to just the config to enable `tmpfs` backed `emptyDir` volumes if that is preferred?",
    "commit": "90a5ae5e8529e546b14fd9e7f9ae8a21b648ed8e",
    "createdAt": "2018-08-31T08:21:13Z",
    "diffHunk": "@@ -37,41 +40,99 @@ private[spark] class LocalDirsFeatureStep(\n     .orElse(conf.getOption(\"spark.local.dir\"))\n     .getOrElse(defaultLocalDir)\n     .split(\",\")\n+  private val useLocalDirTmpFs = conf.get(KUBERNETES_LOCAL_DIRS_TMPFS)\n \n   override def configurePod(pod: SparkPod): SparkPod = {\n     val localDirVolumes = resolvedLocalDirs\n       .zipWithIndex\n       .map { case (localDir, index) =>\n-        new VolumeBuilder()\n-          .withName(s\"spark-local-dir-${index + 1}\")\n-          .withNewEmptyDir()\n-          .endEmptyDir()\n-          .build()\n+        val name = s\"spark-local-dir-${index + 1}\"\n+        // To allow customisation of local dirs backing volumes we should avoid creating\n+        // emptyDir volumes if the volume is already defined in the pod spec\n+        hasVolume(pod, name) match {\n+          case true =>\n+            // For pre-existing volume definitions just re-use the volume\n+            pod.pod.getSpec().getVolumes().asScala.find(v => v.getName.equals(name)).get\n+          case false =>\n+            // Create new emptyDir volume\n+            new VolumeBuilder()\n+              .withName(name)\n+              .withNewEmptyDir()\n+                .withMedium(useLocalDirTmpFs match {\n+                  case true => \"Memory\" // Use tmpfs\n+                  case false => null    // Default - use nodes backing storage\n+                })\n+              .endEmptyDir()\n+              .build()\n+        }\n       }\n+\n     val localDirVolumeMounts = localDirVolumes\n       .zip(resolvedLocalDirs)\n       .map { case (localDirVolume, localDirPath) =>\n-        new VolumeMountBuilder()\n-          .withName(localDirVolume.getName)\n-          .withMountPath(localDirPath)\n-          .build()\n+        hasVolumeMount(pod, localDirVolume.getName, localDirPath) match {\n+          case true =>\n+            // For pre-existing volume mounts just re-use the mount\n+            pod.container.getVolumeMounts().asScala\n+              .find(m => m.getName.equals(localDirVolume.getName)\n+                         && m.getMountPath.equals(localDirPath))\n+              .get\n+          case false =>\n+            // Create new volume mount\n+            new VolumeMountBuilder()\n+              .withName (localDirVolume.getName)\n+              .withMountPath (localDirPath)\n+              .build()\n+        }\n+      }\n+\n+    // Check for conflicting volume mounts\n+    for (m: VolumeMount <- localDirVolumeMounts) {\n+      if (hasConflictingVolumeMount(pod, m.getName, m.getMountPath).size > 0) {\n+        throw new SparkException(s\"Conflicting volume mounts defined, pod template attempted to \" +\n+          \"mount SPARK_LOCAL_DIRS volume ${m.getName} multiple times or at an alternative path \" +\n+          \"then the expected ${m.getPath}\")\n       }\n+    }\n+\n     val podWithLocalDirVolumes = new PodBuilder(pod.pod)\n       .editSpec()\n-        .addToVolumes(localDirVolumes: _*)\n+         // Don't want to re-add volumes that already existed in the incoming spec\n+         // as duplicate definitions will lead to K8S API errors\n+        .addToVolumes(localDirVolumes.filter(v => !hasVolume(pod, v.getName)): _*)",
    "line": 87
  }, {
    "author": {
      "login": "ifilonenko"
    },
    "body": "Yeah, that might be better @rvesse ",
    "commit": "90a5ae5e8529e546b14fd9e7f9ae8a21b648ed8e",
    "createdAt": "2018-08-31T15:41:07Z",
    "diffHunk": "@@ -37,41 +40,99 @@ private[spark] class LocalDirsFeatureStep(\n     .orElse(conf.getOption(\"spark.local.dir\"))\n     .getOrElse(defaultLocalDir)\n     .split(\",\")\n+  private val useLocalDirTmpFs = conf.get(KUBERNETES_LOCAL_DIRS_TMPFS)\n \n   override def configurePod(pod: SparkPod): SparkPod = {\n     val localDirVolumes = resolvedLocalDirs\n       .zipWithIndex\n       .map { case (localDir, index) =>\n-        new VolumeBuilder()\n-          .withName(s\"spark-local-dir-${index + 1}\")\n-          .withNewEmptyDir()\n-          .endEmptyDir()\n-          .build()\n+        val name = s\"spark-local-dir-${index + 1}\"\n+        // To allow customisation of local dirs backing volumes we should avoid creating\n+        // emptyDir volumes if the volume is already defined in the pod spec\n+        hasVolume(pod, name) match {\n+          case true =>\n+            // For pre-existing volume definitions just re-use the volume\n+            pod.pod.getSpec().getVolumes().asScala.find(v => v.getName.equals(name)).get\n+          case false =>\n+            // Create new emptyDir volume\n+            new VolumeBuilder()\n+              .withName(name)\n+              .withNewEmptyDir()\n+                .withMedium(useLocalDirTmpFs match {\n+                  case true => \"Memory\" // Use tmpfs\n+                  case false => null    // Default - use nodes backing storage\n+                })\n+              .endEmptyDir()\n+              .build()\n+        }\n       }\n+\n     val localDirVolumeMounts = localDirVolumes\n       .zip(resolvedLocalDirs)\n       .map { case (localDirVolume, localDirPath) =>\n-        new VolumeMountBuilder()\n-          .withName(localDirVolume.getName)\n-          .withMountPath(localDirPath)\n-          .build()\n+        hasVolumeMount(pod, localDirVolume.getName, localDirPath) match {\n+          case true =>\n+            // For pre-existing volume mounts just re-use the mount\n+            pod.container.getVolumeMounts().asScala\n+              .find(m => m.getName.equals(localDirVolume.getName)\n+                         && m.getMountPath.equals(localDirPath))\n+              .get\n+          case false =>\n+            // Create new volume mount\n+            new VolumeMountBuilder()\n+              .withName (localDirVolume.getName)\n+              .withMountPath (localDirPath)\n+              .build()\n+        }\n+      }\n+\n+    // Check for conflicting volume mounts\n+    for (m: VolumeMount <- localDirVolumeMounts) {\n+      if (hasConflictingVolumeMount(pod, m.getName, m.getMountPath).size > 0) {\n+        throw new SparkException(s\"Conflicting volume mounts defined, pod template attempted to \" +\n+          \"mount SPARK_LOCAL_DIRS volume ${m.getName} multiple times or at an alternative path \" +\n+          \"then the expected ${m.getPath}\")\n       }\n+    }\n+\n     val podWithLocalDirVolumes = new PodBuilder(pod.pod)\n       .editSpec()\n-        .addToVolumes(localDirVolumes: _*)\n+         // Don't want to re-add volumes that already existed in the incoming spec\n+         // as duplicate definitions will lead to K8S API errors\n+        .addToVolumes(localDirVolumes.filter(v => !hasVolume(pod, v.getName)): _*)",
    "line": 87
  }, {
    "author": {
      "login": "rvesse"
    },
    "body": "Ok, will do that Monday\r\n\r\nFYI I notice @onursatici has now made some similar tweaks in his latest commit - https://github.com/apache/spark/pull/22146/commits/a4fde0cdc4dc5b64fd3f888244656371eb76f837 - notice several feature steps there now have `editOrNewX()` or `addToX()` so that they combine with rather than overriding the template",
    "commit": "90a5ae5e8529e546b14fd9e7f9ae8a21b648ed8e",
    "createdAt": "2018-08-31T17:01:24Z",
    "diffHunk": "@@ -37,41 +40,99 @@ private[spark] class LocalDirsFeatureStep(\n     .orElse(conf.getOption(\"spark.local.dir\"))\n     .getOrElse(defaultLocalDir)\n     .split(\",\")\n+  private val useLocalDirTmpFs = conf.get(KUBERNETES_LOCAL_DIRS_TMPFS)\n \n   override def configurePod(pod: SparkPod): SparkPod = {\n     val localDirVolumes = resolvedLocalDirs\n       .zipWithIndex\n       .map { case (localDir, index) =>\n-        new VolumeBuilder()\n-          .withName(s\"spark-local-dir-${index + 1}\")\n-          .withNewEmptyDir()\n-          .endEmptyDir()\n-          .build()\n+        val name = s\"spark-local-dir-${index + 1}\"\n+        // To allow customisation of local dirs backing volumes we should avoid creating\n+        // emptyDir volumes if the volume is already defined in the pod spec\n+        hasVolume(pod, name) match {\n+          case true =>\n+            // For pre-existing volume definitions just re-use the volume\n+            pod.pod.getSpec().getVolumes().asScala.find(v => v.getName.equals(name)).get\n+          case false =>\n+            // Create new emptyDir volume\n+            new VolumeBuilder()\n+              .withName(name)\n+              .withNewEmptyDir()\n+                .withMedium(useLocalDirTmpFs match {\n+                  case true => \"Memory\" // Use tmpfs\n+                  case false => null    // Default - use nodes backing storage\n+                })\n+              .endEmptyDir()\n+              .build()\n+        }\n       }\n+\n     val localDirVolumeMounts = localDirVolumes\n       .zip(resolvedLocalDirs)\n       .map { case (localDirVolume, localDirPath) =>\n-        new VolumeMountBuilder()\n-          .withName(localDirVolume.getName)\n-          .withMountPath(localDirPath)\n-          .build()\n+        hasVolumeMount(pod, localDirVolume.getName, localDirPath) match {\n+          case true =>\n+            // For pre-existing volume mounts just re-use the mount\n+            pod.container.getVolumeMounts().asScala\n+              .find(m => m.getName.equals(localDirVolume.getName)\n+                         && m.getMountPath.equals(localDirPath))\n+              .get\n+          case false =>\n+            // Create new volume mount\n+            new VolumeMountBuilder()\n+              .withName (localDirVolume.getName)\n+              .withMountPath (localDirPath)\n+              .build()\n+        }\n+      }\n+\n+    // Check for conflicting volume mounts\n+    for (m: VolumeMount <- localDirVolumeMounts) {\n+      if (hasConflictingVolumeMount(pod, m.getName, m.getMountPath).size > 0) {\n+        throw new SparkException(s\"Conflicting volume mounts defined, pod template attempted to \" +\n+          \"mount SPARK_LOCAL_DIRS volume ${m.getName} multiple times or at an alternative path \" +\n+          \"then the expected ${m.getPath}\")\n       }\n+    }\n+\n     val podWithLocalDirVolumes = new PodBuilder(pod.pod)\n       .editSpec()\n-        .addToVolumes(localDirVolumes: _*)\n+         // Don't want to re-add volumes that already existed in the incoming spec\n+         // as duplicate definitions will lead to K8S API errors\n+        .addToVolumes(localDirVolumes.filter(v => !hasVolume(pod, v.getName)): _*)",
    "line": 87
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "This is different in that we're looking for specific volumes that have been set up by previous feature steps or outside logic. Preferably every step is self-contained in that it doesn't have to look up specific values set by previous steps.\r\n\r\nFor example this logic would break if we applied the templating after this step, or if a different step after this one added the volumes that are being looked up here.\r\n\r\nWhereas `editOrNew` and `addTo...` at worst only change the ordering on some of the fields depending on when the step is invoked in the sequence.",
    "commit": "90a5ae5e8529e546b14fd9e7f9ae8a21b648ed8e",
    "createdAt": "2018-08-31T17:16:52Z",
    "diffHunk": "@@ -37,41 +40,99 @@ private[spark] class LocalDirsFeatureStep(\n     .orElse(conf.getOption(\"spark.local.dir\"))\n     .getOrElse(defaultLocalDir)\n     .split(\",\")\n+  private val useLocalDirTmpFs = conf.get(KUBERNETES_LOCAL_DIRS_TMPFS)\n \n   override def configurePod(pod: SparkPod): SparkPod = {\n     val localDirVolumes = resolvedLocalDirs\n       .zipWithIndex\n       .map { case (localDir, index) =>\n-        new VolumeBuilder()\n-          .withName(s\"spark-local-dir-${index + 1}\")\n-          .withNewEmptyDir()\n-          .endEmptyDir()\n-          .build()\n+        val name = s\"spark-local-dir-${index + 1}\"\n+        // To allow customisation of local dirs backing volumes we should avoid creating\n+        // emptyDir volumes if the volume is already defined in the pod spec\n+        hasVolume(pod, name) match {\n+          case true =>\n+            // For pre-existing volume definitions just re-use the volume\n+            pod.pod.getSpec().getVolumes().asScala.find(v => v.getName.equals(name)).get\n+          case false =>\n+            // Create new emptyDir volume\n+            new VolumeBuilder()\n+              .withName(name)\n+              .withNewEmptyDir()\n+                .withMedium(useLocalDirTmpFs match {\n+                  case true => \"Memory\" // Use tmpfs\n+                  case false => null    // Default - use nodes backing storage\n+                })\n+              .endEmptyDir()\n+              .build()\n+        }\n       }\n+\n     val localDirVolumeMounts = localDirVolumes\n       .zip(resolvedLocalDirs)\n       .map { case (localDirVolume, localDirPath) =>\n-        new VolumeMountBuilder()\n-          .withName(localDirVolume.getName)\n-          .withMountPath(localDirPath)\n-          .build()\n+        hasVolumeMount(pod, localDirVolume.getName, localDirPath) match {\n+          case true =>\n+            // For pre-existing volume mounts just re-use the mount\n+            pod.container.getVolumeMounts().asScala\n+              .find(m => m.getName.equals(localDirVolume.getName)\n+                         && m.getMountPath.equals(localDirPath))\n+              .get\n+          case false =>\n+            // Create new volume mount\n+            new VolumeMountBuilder()\n+              .withName (localDirVolume.getName)\n+              .withMountPath (localDirPath)\n+              .build()\n+        }\n+      }\n+\n+    // Check for conflicting volume mounts\n+    for (m: VolumeMount <- localDirVolumeMounts) {\n+      if (hasConflictingVolumeMount(pod, m.getName, m.getMountPath).size > 0) {\n+        throw new SparkException(s\"Conflicting volume mounts defined, pod template attempted to \" +\n+          \"mount SPARK_LOCAL_DIRS volume ${m.getName} multiple times or at an alternative path \" +\n+          \"then the expected ${m.getPath}\")\n       }\n+    }\n+\n     val podWithLocalDirVolumes = new PodBuilder(pod.pod)\n       .editSpec()\n-        .addToVolumes(localDirVolumes: _*)\n+         // Don't want to re-add volumes that already existed in the incoming spec\n+         // as duplicate definitions will lead to K8S API errors\n+        .addToVolumes(localDirVolumes.filter(v => !hasVolume(pod, v.getName)): _*)",
    "line": 87
  }, {
    "author": {
      "login": "rvesse"
    },
    "body": "@mccheah @ifilonenko OK, I have opened PR #22323 with just the `tmpfs` enabling changes",
    "commit": "90a5ae5e8529e546b14fd9e7f9ae8a21b648ed8e",
    "createdAt": "2018-09-03T09:15:27Z",
    "diffHunk": "@@ -37,41 +40,99 @@ private[spark] class LocalDirsFeatureStep(\n     .orElse(conf.getOption(\"spark.local.dir\"))\n     .getOrElse(defaultLocalDir)\n     .split(\",\")\n+  private val useLocalDirTmpFs = conf.get(KUBERNETES_LOCAL_DIRS_TMPFS)\n \n   override def configurePod(pod: SparkPod): SparkPod = {\n     val localDirVolumes = resolvedLocalDirs\n       .zipWithIndex\n       .map { case (localDir, index) =>\n-        new VolumeBuilder()\n-          .withName(s\"spark-local-dir-${index + 1}\")\n-          .withNewEmptyDir()\n-          .endEmptyDir()\n-          .build()\n+        val name = s\"spark-local-dir-${index + 1}\"\n+        // To allow customisation of local dirs backing volumes we should avoid creating\n+        // emptyDir volumes if the volume is already defined in the pod spec\n+        hasVolume(pod, name) match {\n+          case true =>\n+            // For pre-existing volume definitions just re-use the volume\n+            pod.pod.getSpec().getVolumes().asScala.find(v => v.getName.equals(name)).get\n+          case false =>\n+            // Create new emptyDir volume\n+            new VolumeBuilder()\n+              .withName(name)\n+              .withNewEmptyDir()\n+                .withMedium(useLocalDirTmpFs match {\n+                  case true => \"Memory\" // Use tmpfs\n+                  case false => null    // Default - use nodes backing storage\n+                })\n+              .endEmptyDir()\n+              .build()\n+        }\n       }\n+\n     val localDirVolumeMounts = localDirVolumes\n       .zip(resolvedLocalDirs)\n       .map { case (localDirVolume, localDirPath) =>\n-        new VolumeMountBuilder()\n-          .withName(localDirVolume.getName)\n-          .withMountPath(localDirPath)\n-          .build()\n+        hasVolumeMount(pod, localDirVolume.getName, localDirPath) match {\n+          case true =>\n+            // For pre-existing volume mounts just re-use the mount\n+            pod.container.getVolumeMounts().asScala\n+              .find(m => m.getName.equals(localDirVolume.getName)\n+                         && m.getMountPath.equals(localDirPath))\n+              .get\n+          case false =>\n+            // Create new volume mount\n+            new VolumeMountBuilder()\n+              .withName (localDirVolume.getName)\n+              .withMountPath (localDirPath)\n+              .build()\n+        }\n+      }\n+\n+    // Check for conflicting volume mounts\n+    for (m: VolumeMount <- localDirVolumeMounts) {\n+      if (hasConflictingVolumeMount(pod, m.getName, m.getMountPath).size > 0) {\n+        throw new SparkException(s\"Conflicting volume mounts defined, pod template attempted to \" +\n+          \"mount SPARK_LOCAL_DIRS volume ${m.getName} multiple times or at an alternative path \" +\n+          \"then the expected ${m.getPath}\")\n       }\n+    }\n+\n     val podWithLocalDirVolumes = new PodBuilder(pod.pod)\n       .editSpec()\n-        .addToVolumes(localDirVolumes: _*)\n+         // Don't want to re-add volumes that already existed in the incoming spec\n+         // as duplicate definitions will lead to K8S API errors\n+        .addToVolumes(localDirVolumes.filter(v => !hasVolume(pod, v.getName)): _*)",
    "line": 87
  }],
  "prId": 22256
}]