[{
  "comments": [{
    "author": {
      "login": "liyinan926"
    },
    "body": "Can we put the default implementation in `object ExecutorPodsLifecycleEventHandler`? I'm not sure if syntax-wise if that's feasible, but it's pretty hard to read here. ",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-05-25T16:58:50Z",
    "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.TimeUnit\n+\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import io.fabric8.kubernetes.api.model.Pod\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsLifecycleEventHandler(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    podsEventQueue: ExecutorPodsEventQueue,\n+    // Use a best-effort to track which executors have been removed already. It's not generally\n+    // job-breaking if we remove executors more than once but it's ideal if we make an attempt\n+    // to avoid doing so. Expire cache entries so that this data structure doesn't grow beyond\n+    // bounds.\n+    removedExecutorsCache: Cache[java.lang.Long, java.lang.Long] ="
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Also why do we need this? It doesn't look like it's really used for anything.",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-05-25T17:01:05Z",
    "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.TimeUnit\n+\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import io.fabric8.kubernetes.api.model.Pod\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsLifecycleEventHandler(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    podsEventQueue: ExecutorPodsEventQueue,\n+    // Use a best-effort to track which executors have been removed already. It's not generally\n+    // job-breaking if we remove executors more than once but it's ideal if we make an attempt\n+    // to avoid doing so. Expire cache entries so that this data structure doesn't grow beyond\n+    // bounds.\n+    removedExecutorsCache: Cache[java.lang.Long, java.lang.Long] ="
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "For usage, see line 87. Avoid removing the same executor from Spark's management components twice.\r\n\r\nWe can inject this from `KubernetesClusterManager` if you believe that's easier to read.",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-05-25T17:36:21Z",
    "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.TimeUnit\n+\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import io.fabric8.kubernetes.api.model.Pod\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsLifecycleEventHandler(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    podsEventQueue: ExecutorPodsEventQueue,\n+    // Use a best-effort to track which executors have been removed already. It's not generally\n+    // job-breaking if we remove executors more than once but it's ideal if we make an attempt\n+    // to avoid doing so. Expire cache entries so that this data structure doesn't grow beyond\n+    // bounds.\n+    removedExecutorsCache: Cache[java.lang.Long, java.lang.Long] ="
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Oh yes didn't see it. ",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-05-25T17:40:09Z",
    "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.TimeUnit\n+\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import io.fabric8.kubernetes.api.model.Pod\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsLifecycleEventHandler(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    podsEventQueue: ExecutorPodsEventQueue,\n+    // Use a best-effort to track which executors have been removed already. It's not generally\n+    // job-breaking if we remove executors more than once but it's ideal if we make an attempt\n+    // to avoid doing so. Expire cache entries so that this data structure doesn't grow beyond\n+    // bounds.\n+    removedExecutorsCache: Cache[java.lang.Long, java.lang.Long] ="
  }],
  "prId": 21366
}, {
  "comments": [{
    "author": {
      "login": "felixcheung"
    },
    "body": "add a `case _ =>` for now till SPARK-24135?",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-05-27T01:04:31Z",
    "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import io.fabric8.kubernetes.api.model.Pod\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsLifecycleEventHandler(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    podsEventQueue: ExecutorPodsEventQueue,\n+    // Use a best-effort to track which executors have been removed already. It's not generally\n+    // job-breaking if we remove executors more than once but it's ideal if we make an attempt\n+    // to avoid doing so. Expire cache entries so that this data structure doesn't grow beyond\n+    // bounds.\n+    removedExecutorsCache: Cache[java.lang.Long, java.lang.Long]) {\n+\n+  import ExecutorPodsLifecycleEventHandler._\n+\n+  private val eventProcessingInterval = conf.get(KUBERNETES_EXECUTOR_EVENT_PROCESSING_INTERVAL)\n+\n+  def start(schedulerBackend: KubernetesClusterSchedulerBackend): Unit = {\n+    podsEventQueue.addSubscriber(eventProcessingInterval) { updatedPods =>\n+      updatedPods.foreach { updatedPod =>\n+        processUpdatedPod(schedulerBackend, updatedPod)\n+      }\n+    }\n+  }\n+\n+  private def processUpdatedPod(\n+      schedulerBackend: KubernetesClusterSchedulerBackend, updatedPod: Pod) = {\n+    val execId = updatedPod.getMetadata.getLabels.get(SPARK_EXECUTOR_ID_LABEL).toLong\n+    if (isDeleted(updatedPod)) {\n+      removeExecutorFromSpark(schedulerBackend, updatedPod, execId)\n+    } else {\n+      updatedPod.getStatus.getPhase.toLowerCase match {\n+        // TODO (SPARK-24135) - handle more classes of errors\n+        case \"error\" | \"failed\" | \"succeeded\" =>\n+          // If deletion failed on a previous try, we can try again if resync informs us the pod\n+          // is still around.\n+          // Delete as best attempt - duplicate deletes will throw an exception but the end state\n+          // of getting rid of the pod is what matters.\n+          Utils.tryLogNonFatalError {\n+            kubernetesClient\n+              .pods()\n+              .withName(updatedPod.getMetadata.getName)\n+              .delete()\n+          }\n+          removeExecutorFromSpark(schedulerBackend, updatedPod, execId)\n+      }"
  }],
  "prId": 21366
}, {
  "comments": [{
    "author": {
      "login": "foxish"
    },
    "body": "Why not just `removeExecutor`?",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-05-30T18:13:07Z",
    "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import io.fabric8.kubernetes.api.model.Pod\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsLifecycleEventHandler(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    podsEventQueue: ExecutorPodsEventQueue,\n+    // Use a best-effort to track which executors have been removed already. It's not generally\n+    // job-breaking if we remove executors more than once but it's ideal if we make an attempt\n+    // to avoid doing so. Expire cache entries so that this data structure doesn't grow beyond\n+    // bounds.\n+    removedExecutorsCache: Cache[java.lang.Long, java.lang.Long]) {\n+\n+  import ExecutorPodsLifecycleEventHandler._\n+\n+  private val eventProcessingInterval = conf.get(KUBERNETES_EXECUTOR_EVENT_PROCESSING_INTERVAL)\n+\n+  def start(schedulerBackend: KubernetesClusterSchedulerBackend): Unit = {\n+    podsEventQueue.addSubscriber(eventProcessingInterval) { updatedPods =>\n+      updatedPods.foreach { updatedPod =>\n+        processUpdatedPod(schedulerBackend, updatedPod)\n+      }\n+    }\n+  }\n+\n+  private def processUpdatedPod(\n+      schedulerBackend: KubernetesClusterSchedulerBackend, updatedPod: Pod) = {\n+    val execId = updatedPod.getMetadata.getLabels.get(SPARK_EXECUTOR_ID_LABEL).toLong\n+    if (isDeleted(updatedPod)) {\n+      removeExecutorFromSpark(schedulerBackend, updatedPod, execId)\n+    } else {\n+      updatedPod.getStatus.getPhase.toLowerCase match {\n+        // TODO (SPARK-24135) - handle more classes of errors\n+        case \"error\" | \"failed\" | \"succeeded\" =>\n+          // If deletion failed on a previous try, we can try again if resync informs us the pod\n+          // is still around.\n+          // Delete as best attempt - duplicate deletes will throw an exception but the end state\n+          // of getting rid of the pod is what matters.\n+          Utils.tryLogNonFatalError {\n+            kubernetesClient\n+              .pods()\n+              .withName(updatedPod.getMetadata.getName)\n+              .delete()\n+          }\n+          removeExecutorFromSpark(schedulerBackend, updatedPod, execId)\n+        case _ =>\n+      }\n+    }\n+  }\n+\n+  private def removeExecutorFromSpark("
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "There's deleting the executor from k8s and then removing the executor from Spark's knowledge. Think it's helpful to distinguish the two. We could have an equivalent helper method `removeExecutorFromK8s`.",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-05-30T20:38:32Z",
    "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import io.fabric8.kubernetes.api.model.Pod\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsLifecycleEventHandler(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    podsEventQueue: ExecutorPodsEventQueue,\n+    // Use a best-effort to track which executors have been removed already. It's not generally\n+    // job-breaking if we remove executors more than once but it's ideal if we make an attempt\n+    // to avoid doing so. Expire cache entries so that this data structure doesn't grow beyond\n+    // bounds.\n+    removedExecutorsCache: Cache[java.lang.Long, java.lang.Long]) {\n+\n+  import ExecutorPodsLifecycleEventHandler._\n+\n+  private val eventProcessingInterval = conf.get(KUBERNETES_EXECUTOR_EVENT_PROCESSING_INTERVAL)\n+\n+  def start(schedulerBackend: KubernetesClusterSchedulerBackend): Unit = {\n+    podsEventQueue.addSubscriber(eventProcessingInterval) { updatedPods =>\n+      updatedPods.foreach { updatedPod =>\n+        processUpdatedPod(schedulerBackend, updatedPod)\n+      }\n+    }\n+  }\n+\n+  private def processUpdatedPod(\n+      schedulerBackend: KubernetesClusterSchedulerBackend, updatedPod: Pod) = {\n+    val execId = updatedPod.getMetadata.getLabels.get(SPARK_EXECUTOR_ID_LABEL).toLong\n+    if (isDeleted(updatedPod)) {\n+      removeExecutorFromSpark(schedulerBackend, updatedPod, execId)\n+    } else {\n+      updatedPod.getStatus.getPhase.toLowerCase match {\n+        // TODO (SPARK-24135) - handle more classes of errors\n+        case \"error\" | \"failed\" | \"succeeded\" =>\n+          // If deletion failed on a previous try, we can try again if resync informs us the pod\n+          // is still around.\n+          // Delete as best attempt - duplicate deletes will throw an exception but the end state\n+          // of getting rid of the pod is what matters.\n+          Utils.tryLogNonFatalError {\n+            kubernetesClient\n+              .pods()\n+              .withName(updatedPod.getMetadata.getName)\n+              .delete()\n+          }\n+          removeExecutorFromSpark(schedulerBackend, updatedPod, execId)\n+        case _ =>\n+      }\n+    }\n+  }\n+\n+  private def removeExecutorFromSpark("
  }],
  "prId": 21366
}, {
  "comments": [{
    "author": {
      "login": "dvogelbacher"
    },
    "body": "same as above, \"error\" doesn't seem to be a phase",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-05-31T18:04:39Z",
    "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import io.fabric8.kubernetes.api.model.Pod\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsLifecycleEventHandler(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    podsEventQueue: ExecutorPodsEventQueue,\n+    // Use a best-effort to track which executors have been removed already. It's not generally\n+    // job-breaking if we remove executors more than once but it's ideal if we make an attempt\n+    // to avoid doing so. Expire cache entries so that this data structure doesn't grow beyond\n+    // bounds.\n+    removedExecutorsCache: Cache[java.lang.Long, java.lang.Long]) {\n+\n+  import ExecutorPodsLifecycleEventHandler._\n+\n+  private val eventProcessingInterval = conf.get(KUBERNETES_EXECUTOR_EVENT_PROCESSING_INTERVAL)\n+\n+  def start(schedulerBackend: KubernetesClusterSchedulerBackend): Unit = {\n+    podsEventQueue.addSubscriber(eventProcessingInterval) { updatedPods =>\n+      updatedPods.foreach { updatedPod =>\n+        processUpdatedPod(schedulerBackend, updatedPod)\n+      }\n+    }\n+  }\n+\n+  private def processUpdatedPod(\n+      schedulerBackend: KubernetesClusterSchedulerBackend, updatedPod: Pod) = {\n+    val execId = updatedPod.getMetadata.getLabels.get(SPARK_EXECUTOR_ID_LABEL).toLong\n+    if (isDeleted(updatedPod)) {\n+      removeExecutorFromSpark(schedulerBackend, updatedPod, execId)\n+    } else {\n+      updatedPod.getStatus.getPhase.toLowerCase match {\n+        // TODO (SPARK-24135) - handle more classes of errors\n+        case \"error\" | \"failed\" | \"succeeded\" =>"
  }, {
    "author": {
      "login": "dvogelbacher"
    },
    "body": "btw, I just tried testing a pod with an init error locally and doing `kubectl get -o=yaml pod {name} | grep phase` gives me `phase: Failed`. So I think the Init errors of `SPARK-24135` will be handled by this as well. ",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-05-31T18:47:17Z",
    "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import io.fabric8.kubernetes.api.model.Pod\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsLifecycleEventHandler(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    podsEventQueue: ExecutorPodsEventQueue,\n+    // Use a best-effort to track which executors have been removed already. It's not generally\n+    // job-breaking if we remove executors more than once but it's ideal if we make an attempt\n+    // to avoid doing so. Expire cache entries so that this data structure doesn't grow beyond\n+    // bounds.\n+    removedExecutorsCache: Cache[java.lang.Long, java.lang.Long]) {\n+\n+  import ExecutorPodsLifecycleEventHandler._\n+\n+  private val eventProcessingInterval = conf.get(KUBERNETES_EXECUTOR_EVENT_PROCESSING_INTERVAL)\n+\n+  def start(schedulerBackend: KubernetesClusterSchedulerBackend): Unit = {\n+    podsEventQueue.addSubscriber(eventProcessingInterval) { updatedPods =>\n+      updatedPods.foreach { updatedPod =>\n+        processUpdatedPod(schedulerBackend, updatedPod)\n+      }\n+    }\n+  }\n+\n+  private def processUpdatedPod(\n+      schedulerBackend: KubernetesClusterSchedulerBackend, updatedPod: Pod) = {\n+    val execId = updatedPod.getMetadata.getLabels.get(SPARK_EXECUTOR_ID_LABEL).toLong\n+    if (isDeleted(updatedPod)) {\n+      removeExecutorFromSpark(schedulerBackend, updatedPod, execId)\n+    } else {\n+      updatedPod.getStatus.getPhase.toLowerCase match {\n+        // TODO (SPARK-24135) - handle more classes of errors\n+        case \"error\" | \"failed\" | \"succeeded\" =>"
  }],
  "prId": 21366
}, {
  "comments": [{
    "author": {
      "login": "dvogelbacher"
    },
    "body": "we don't check for this in `ExecutorPodsAllocator`. So it could happen that we remove an executor here but not there. ",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-05-31T18:04:43Z",
    "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import io.fabric8.kubernetes.api.model.Pod\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsLifecycleEventHandler(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    podsEventQueue: ExecutorPodsEventQueue,\n+    // Use a best-effort to track which executors have been removed already. It's not generally\n+    // job-breaking if we remove executors more than once but it's ideal if we make an attempt\n+    // to avoid doing so. Expire cache entries so that this data structure doesn't grow beyond\n+    // bounds.\n+    removedExecutorsCache: Cache[java.lang.Long, java.lang.Long]) {\n+\n+  import ExecutorPodsLifecycleEventHandler._\n+\n+  private val eventProcessingInterval = conf.get(KUBERNETES_EXECUTOR_EVENT_PROCESSING_INTERVAL)\n+\n+  def start(schedulerBackend: KubernetesClusterSchedulerBackend): Unit = {\n+    podsEventQueue.addSubscriber(eventProcessingInterval) { updatedPods =>\n+      updatedPods.foreach { updatedPod =>\n+        processUpdatedPod(schedulerBackend, updatedPod)\n+      }\n+    }\n+  }\n+\n+  private def processUpdatedPod(\n+      schedulerBackend: KubernetesClusterSchedulerBackend, updatedPod: Pod) = {\n+    val execId = updatedPod.getMetadata.getLabels.get(SPARK_EXECUTOR_ID_LABEL).toLong\n+    if (isDeleted(updatedPod)) {\n+      removeExecutorFromSpark(schedulerBackend, updatedPod, execId)"
  }, {
    "author": {
      "login": "dvogelbacher"
    },
    "body": "also both classes are very similar and have duplicated code. Can we factor out the commonalities? We could have a common base class that does the matching of pod phase and then calls the appropriate abstract method `handleFailurePhase`, `handleRunningPhase`, ..., which needs to be overwritten in the subclass.",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-05-31T18:17:03Z",
    "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import io.fabric8.kubernetes.api.model.Pod\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsLifecycleEventHandler(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    podsEventQueue: ExecutorPodsEventQueue,\n+    // Use a best-effort to track which executors have been removed already. It's not generally\n+    // job-breaking if we remove executors more than once but it's ideal if we make an attempt\n+    // to avoid doing so. Expire cache entries so that this data structure doesn't grow beyond\n+    // bounds.\n+    removedExecutorsCache: Cache[java.lang.Long, java.lang.Long]) {\n+\n+  import ExecutorPodsLifecycleEventHandler._\n+\n+  private val eventProcessingInterval = conf.get(KUBERNETES_EXECUTOR_EVENT_PROCESSING_INTERVAL)\n+\n+  def start(schedulerBackend: KubernetesClusterSchedulerBackend): Unit = {\n+    podsEventQueue.addSubscriber(eventProcessingInterval) { updatedPods =>\n+      updatedPods.foreach { updatedPod =>\n+        processUpdatedPod(schedulerBackend, updatedPod)\n+      }\n+    }\n+  }\n+\n+  private def processUpdatedPod(\n+      schedulerBackend: KubernetesClusterSchedulerBackend, updatedPod: Pod) = {\n+    val execId = updatedPod.getMetadata.getLabels.get(SPARK_EXECUTOR_ID_LABEL).toLong\n+    if (isDeleted(updatedPod)) {\n+      removeExecutorFromSpark(schedulerBackend, updatedPod, execId)"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "It's not entirely analogous but there are parts that can be shared. The pods allocator has a \"post-batch\" processing task which can run even on empty batches (particularly important for the first empty batch to request the first round of executors). But, I have some ideas for shared code that can be posted shortly.",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-06-01T00:18:19Z",
    "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import io.fabric8.kubernetes.api.model.Pod\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsLifecycleEventHandler(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    podsEventQueue: ExecutorPodsEventQueue,\n+    // Use a best-effort to track which executors have been removed already. It's not generally\n+    // job-breaking if we remove executors more than once but it's ideal if we make an attempt\n+    // to avoid doing so. Expire cache entries so that this data structure doesn't grow beyond\n+    // bounds.\n+    removedExecutorsCache: Cache[java.lang.Long, java.lang.Long]) {\n+\n+  import ExecutorPodsLifecycleEventHandler._\n+\n+  private val eventProcessingInterval = conf.get(KUBERNETES_EXECUTOR_EVENT_PROCESSING_INTERVAL)\n+\n+  def start(schedulerBackend: KubernetesClusterSchedulerBackend): Unit = {\n+    podsEventQueue.addSubscriber(eventProcessingInterval) { updatedPods =>\n+      updatedPods.foreach { updatedPod =>\n+        processUpdatedPod(schedulerBackend, updatedPod)\n+      }\n+    }\n+  }\n+\n+  private def processUpdatedPod(\n+      schedulerBackend: KubernetesClusterSchedulerBackend, updatedPod: Pod) = {\n+    val execId = updatedPod.getMetadata.getLabels.get(SPARK_EXECUTOR_ID_LABEL).toLong\n+    if (isDeleted(updatedPod)) {\n+      removeExecutorFromSpark(schedulerBackend, updatedPod, execId)"
  }],
  "prId": 21366
}, {
  "comments": [{
    "author": {
      "login": "dvogelbacher"
    },
    "body": "nit: this is called `processUpdatedPods` in `ExecutorPodsAllocator` and does the `updatedPods.foreach ` in the method itself. Can we make this consistent?",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-05-31T18:13:48Z",
    "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import com.google.common.cache.{Cache, CacheBuilder}\n+import io.fabric8.kubernetes.api.model.Pod\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.scheduler.ExecutorExited\n+import org.apache.spark.util.Utils\n+\n+private[spark] class ExecutorPodsLifecycleEventHandler(\n+    conf: SparkConf,\n+    executorBuilder: KubernetesExecutorBuilder,\n+    kubernetesClient: KubernetesClient,\n+    podsEventQueue: ExecutorPodsEventQueue,\n+    // Use a best-effort to track which executors have been removed already. It's not generally\n+    // job-breaking if we remove executors more than once but it's ideal if we make an attempt\n+    // to avoid doing so. Expire cache entries so that this data structure doesn't grow beyond\n+    // bounds.\n+    removedExecutorsCache: Cache[java.lang.Long, java.lang.Long]) {\n+\n+  import ExecutorPodsLifecycleEventHandler._\n+\n+  private val eventProcessingInterval = conf.get(KUBERNETES_EXECUTOR_EVENT_PROCESSING_INTERVAL)\n+\n+  def start(schedulerBackend: KubernetesClusterSchedulerBackend): Unit = {\n+    podsEventQueue.addSubscriber(eventProcessingInterval) { updatedPods =>\n+      updatedPods.foreach { updatedPod =>\n+        processUpdatedPod(schedulerBackend, updatedPod)\n+      }\n+    }\n+  }\n+\n+  private def processUpdatedPod("
  }],
  "prId": 21366
}]