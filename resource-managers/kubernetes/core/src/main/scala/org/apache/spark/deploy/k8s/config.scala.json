[{
  "comments": [{
    "author": {
      "login": "felixcheung"
    },
    "body": "nit: extra space at the end of string here and L102",
    "commit": "b85cfc4038c8de9340b78d10edf88ab76dd90ba3",
    "createdAt": "2017-10-25T18:09:31Z",
    "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s\n+\n+import org.apache.spark.{SPARK_VERSION => sparkVersion}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.ConfigBuilder\n+import org.apache.spark.network.util.ByteUnit\n+\n+package object config extends Logging {\n+\n+  private[spark] val KUBERNETES_NAMESPACE =\n+    ConfigBuilder(\"spark.kubernetes.namespace\")\n+      .doc(\"The namespace that will be used for running the driver and executor pods. When using\" +\n+        \" spark-submit in cluster mode, this can also be passed to spark-submit via the\" +\n+        \" --kubernetes-namespace command line argument.\")\n+      .stringConf\n+      .createWithDefault(\"default\")\n+\n+  private[spark] val EXECUTOR_DOCKER_IMAGE =\n+    ConfigBuilder(\"spark.kubernetes.executor.docker.image\")\n+      .doc(\"Docker image to use for the executors. Specify this using the standard Docker tag\" +\n+        \" format.\")\n+      .stringConf\n+      .createWithDefault(s\"spark-executor:$sparkVersion\")\n+\n+  private[spark] val DOCKER_IMAGE_PULL_POLICY =\n+    ConfigBuilder(\"spark.kubernetes.docker.image.pullPolicy\")\n+      .doc(\"Docker image pull policy when pulling any docker image in Kubernetes integration\")\n+      .stringConf\n+      .createWithDefault(\"IfNotPresent\")\n+\n+\n+  private[spark] val APISERVER_AUTH_DRIVER_CONF_PREFIX =\n+      \"spark.kubernetes.authenticate.driver\"\n+  private[spark] val APISERVER_AUTH_DRIVER_MOUNTED_CONF_PREFIX =\n+      \"spark.kubernetes.authenticate.driver.mounted\"\n+  private[spark] val OAUTH_TOKEN_CONF_SUFFIX = \"oauthToken\"\n+  private[spark] val OAUTH_TOKEN_FILE_CONF_SUFFIX = \"oauthTokenFile\"\n+  private[spark] val CLIENT_KEY_FILE_CONF_SUFFIX = \"clientKeyFile\"\n+  private[spark] val CLIENT_CERT_FILE_CONF_SUFFIX = \"clientCertFile\"\n+  private[spark] val CA_CERT_FILE_CONF_SUFFIX = \"caCertFile\"\n+\n+  private[spark] val KUBERNETES_SERVICE_ACCOUNT_NAME =\n+    ConfigBuilder(s\"$APISERVER_AUTH_DRIVER_CONF_PREFIX.serviceAccountName\")\n+      .doc(\"Service account that is used when running the driver pod. The driver pod uses\" +\n+        \" this service account when requesting executor pods from the API server. If specific\" +\n+        \" credentials are given for the driver pod to use, the driver will favor\" +\n+        \" using those credentials instead.\")\n+      .stringConf\n+      .createOptional\n+\n+  // Note that while we set a default for this when we start up the\n+  // scheduler, the specific default value is dynamically determined\n+  // based on the executor memory.\n+  private[spark] val KUBERNETES_EXECUTOR_MEMORY_OVERHEAD =\n+    ConfigBuilder(\"spark.kubernetes.executor.memoryOverhead\")\n+      .doc(\"The amount of off-heap memory (in megabytes) to be allocated per executor. This\" +\n+        \" is memory that accounts for things like VM overheads, interned strings, other native\" +\n+        \" overheads, etc. This tends to grow with the executor size. (typically 6-10%).\")\n+      .bytesConf(ByteUnit.MiB)\n+      .createOptional\n+\n+  private[spark] val KUBERNETES_EXECUTOR_LABEL_PREFIX = \"spark.kubernetes.executor.label.\"\n+  private[spark] val KUBERNETES_EXECUTOR_ANNOTATION_PREFIX = \"spark.kubernetes.executor.annotation.\"\n+\n+  private[spark] val KUBERNETES_DRIVER_POD_NAME =\n+    ConfigBuilder(\"spark.kubernetes.driver.pod.name\")\n+      .doc(\"Name of the driver pod.\")\n+      .stringConf\n+      .createOptional\n+\n+  private[spark] val KUBERNETES_EXECUTOR_POD_NAME_PREFIX =\n+    ConfigBuilder(\"spark.kubernetes.executor.podNamePrefix\")\n+      .doc(\"Prefix to use in front of the executor pod names.\")\n+      .internal()\n+      .stringConf\n+      .createWithDefault(\"spark\")\n+\n+  private[spark] val KUBERNETES_ALLOCATION_BATCH_SIZE =\n+    ConfigBuilder(\"spark.kubernetes.allocation.batch.size\")\n+      .doc(\"Number of pods to launch at once in each round of dynamic allocation. \")"
  }],
  "prId": 19468
}, {
  "comments": [{
    "author": {
      "login": "felixcheung"
    },
    "body": "nit no need for `s\"` here.\r\nalso perhaps this is more useful as Info instead of Debug",
    "commit": "b85cfc4038c8de9340b78d10edf88ab76dd90ba3",
    "createdAt": "2017-10-25T18:11:03Z",
    "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s\n+\n+import org.apache.spark.{SPARK_VERSION => sparkVersion}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.ConfigBuilder\n+import org.apache.spark.network.util.ByteUnit\n+\n+package object config extends Logging {\n+\n+  private[spark] val KUBERNETES_NAMESPACE =\n+    ConfigBuilder(\"spark.kubernetes.namespace\")\n+      .doc(\"The namespace that will be used for running the driver and executor pods. When using\" +\n+        \" spark-submit in cluster mode, this can also be passed to spark-submit via the\" +\n+        \" --kubernetes-namespace command line argument.\")\n+      .stringConf\n+      .createWithDefault(\"default\")\n+\n+  private[spark] val EXECUTOR_DOCKER_IMAGE =\n+    ConfigBuilder(\"spark.kubernetes.executor.docker.image\")\n+      .doc(\"Docker image to use for the executors. Specify this using the standard Docker tag\" +\n+        \" format.\")\n+      .stringConf\n+      .createWithDefault(s\"spark-executor:$sparkVersion\")\n+\n+  private[spark] val DOCKER_IMAGE_PULL_POLICY =\n+    ConfigBuilder(\"spark.kubernetes.docker.image.pullPolicy\")\n+      .doc(\"Docker image pull policy when pulling any docker image in Kubernetes integration\")\n+      .stringConf\n+      .createWithDefault(\"IfNotPresent\")\n+\n+\n+  private[spark] val APISERVER_AUTH_DRIVER_CONF_PREFIX =\n+      \"spark.kubernetes.authenticate.driver\"\n+  private[spark] val APISERVER_AUTH_DRIVER_MOUNTED_CONF_PREFIX =\n+      \"spark.kubernetes.authenticate.driver.mounted\"\n+  private[spark] val OAUTH_TOKEN_CONF_SUFFIX = \"oauthToken\"\n+  private[spark] val OAUTH_TOKEN_FILE_CONF_SUFFIX = \"oauthTokenFile\"\n+  private[spark] val CLIENT_KEY_FILE_CONF_SUFFIX = \"clientKeyFile\"\n+  private[spark] val CLIENT_CERT_FILE_CONF_SUFFIX = \"clientCertFile\"\n+  private[spark] val CA_CERT_FILE_CONF_SUFFIX = \"caCertFile\"\n+\n+  private[spark] val KUBERNETES_SERVICE_ACCOUNT_NAME =\n+    ConfigBuilder(s\"$APISERVER_AUTH_DRIVER_CONF_PREFIX.serviceAccountName\")\n+      .doc(\"Service account that is used when running the driver pod. The driver pod uses\" +\n+        \" this service account when requesting executor pods from the API server. If specific\" +\n+        \" credentials are given for the driver pod to use, the driver will favor\" +\n+        \" using those credentials instead.\")\n+      .stringConf\n+      .createOptional\n+\n+  // Note that while we set a default for this when we start up the\n+  // scheduler, the specific default value is dynamically determined\n+  // based on the executor memory.\n+  private[spark] val KUBERNETES_EXECUTOR_MEMORY_OVERHEAD =\n+    ConfigBuilder(\"spark.kubernetes.executor.memoryOverhead\")\n+      .doc(\"The amount of off-heap memory (in megabytes) to be allocated per executor. This\" +\n+        \" is memory that accounts for things like VM overheads, interned strings, other native\" +\n+        \" overheads, etc. This tends to grow with the executor size. (typically 6-10%).\")\n+      .bytesConf(ByteUnit.MiB)\n+      .createOptional\n+\n+  private[spark] val KUBERNETES_EXECUTOR_LABEL_PREFIX = \"spark.kubernetes.executor.label.\"\n+  private[spark] val KUBERNETES_EXECUTOR_ANNOTATION_PREFIX = \"spark.kubernetes.executor.annotation.\"\n+\n+  private[spark] val KUBERNETES_DRIVER_POD_NAME =\n+    ConfigBuilder(\"spark.kubernetes.driver.pod.name\")\n+      .doc(\"Name of the driver pod.\")\n+      .stringConf\n+      .createOptional\n+\n+  private[spark] val KUBERNETES_EXECUTOR_POD_NAME_PREFIX =\n+    ConfigBuilder(\"spark.kubernetes.executor.podNamePrefix\")\n+      .doc(\"Prefix to use in front of the executor pod names.\")\n+      .internal()\n+      .stringConf\n+      .createWithDefault(\"spark\")\n+\n+  private[spark] val KUBERNETES_ALLOCATION_BATCH_SIZE =\n+    ConfigBuilder(\"spark.kubernetes.allocation.batch.size\")\n+      .doc(\"Number of pods to launch at once in each round of dynamic allocation. \")\n+      .intConf\n+      .createWithDefault(5)\n+\n+  private[spark] val KUBERNETES_ALLOCATION_BATCH_DELAY =\n+    ConfigBuilder(\"spark.kubernetes.allocation.batch.delay\")\n+      .doc(\"Number of seconds to wait between each round of executor allocation. \")\n+      .longConf\n+      .createWithDefault(1)\n+\n+  private[spark] val INIT_CONTAINER_JARS_DOWNLOAD_LOCATION =\n+    ConfigBuilder(\"spark.kubernetes.mountdependencies.jarsDownloadDir\")\n+      .doc(\"Location to download jars to in the driver and executors. When using\" +\n+        \" spark-submit, this directory must be empty and will be mounted as an empty directory\" +\n+        \" volume on the driver and executor pod.\")\n+      .stringConf\n+      .createWithDefault(\"/var/spark-data/spark-jars\")\n+\n+  private[spark] val KUBERNETES_EXECUTOR_LIMIT_CORES =\n+    ConfigBuilder(\"spark.kubernetes.executor.limit.cores\")\n+      .doc(\"Specify the hard cpu limit for a single executor pod\")\n+      .stringConf\n+      .createOptional\n+\n+  private[spark] val KUBERNETES_NODE_SELECTOR_PREFIX = \"spark.kubernetes.node.selector.\"\n+\n+  private[spark] def resolveK8sMaster(rawMasterString: String): String = {\n+    if (!rawMasterString.startsWith(\"k8s://\")) {\n+      throw new IllegalArgumentException(\"Master URL should start with k8s:// in Kubernetes mode.\")\n+    }\n+    val masterWithoutK8sPrefix = rawMasterString.replaceFirst(\"k8s://\", \"\")\n+    if (masterWithoutK8sPrefix.startsWith(\"http://\")\n+      || masterWithoutK8sPrefix.startsWith(\"https://\")) {\n+      masterWithoutK8sPrefix\n+    } else {\n+      val resolvedURL = s\"https://$masterWithoutK8sPrefix\"\n+      logDebug(s\"No scheme specified for kubernetes master URL, so defaulting to https. Resolved\" +"
  }],
  "prId": 19468
}, {
  "comments": [{
    "author": {
      "login": "mridulm"
    },
    "body": "I am probably missing something here - who populates this directory ?\r\nI did not see any writes to this path and it is expected to be empty by definition above.\r\n\r\nAlso, if I want additional jars to be included in my driver/executor classpath which I provision in my docker image, is that supported ? (I initially assumed this path was for supporting that usecase as well ...)",
    "commit": "b85cfc4038c8de9340b78d10edf88ab76dd90ba3",
    "createdAt": "2017-10-25T19:33:18Z",
    "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s\n+\n+import org.apache.spark.{SPARK_VERSION => sparkVersion}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.ConfigBuilder\n+import org.apache.spark.network.util.ByteUnit\n+\n+package object config extends Logging {\n+\n+  private[spark] val KUBERNETES_NAMESPACE =\n+    ConfigBuilder(\"spark.kubernetes.namespace\")\n+      .doc(\"The namespace that will be used for running the driver and executor pods. When using\" +\n+        \" spark-submit in cluster mode, this can also be passed to spark-submit via the\" +\n+        \" --kubernetes-namespace command line argument.\")\n+      .stringConf\n+      .createWithDefault(\"default\")\n+\n+  private[spark] val EXECUTOR_DOCKER_IMAGE =\n+    ConfigBuilder(\"spark.kubernetes.executor.docker.image\")\n+      .doc(\"Docker image to use for the executors. Specify this using the standard Docker tag\" +\n+        \" format.\")\n+      .stringConf\n+      .createWithDefault(s\"spark-executor:$sparkVersion\")\n+\n+  private[spark] val DOCKER_IMAGE_PULL_POLICY =\n+    ConfigBuilder(\"spark.kubernetes.docker.image.pullPolicy\")\n+      .doc(\"Docker image pull policy when pulling any docker image in Kubernetes integration\")\n+      .stringConf\n+      .createWithDefault(\"IfNotPresent\")\n+\n+\n+  private[spark] val APISERVER_AUTH_DRIVER_CONF_PREFIX =\n+      \"spark.kubernetes.authenticate.driver\"\n+  private[spark] val APISERVER_AUTH_DRIVER_MOUNTED_CONF_PREFIX =\n+      \"spark.kubernetes.authenticate.driver.mounted\"\n+  private[spark] val OAUTH_TOKEN_CONF_SUFFIX = \"oauthToken\"\n+  private[spark] val OAUTH_TOKEN_FILE_CONF_SUFFIX = \"oauthTokenFile\"\n+  private[spark] val CLIENT_KEY_FILE_CONF_SUFFIX = \"clientKeyFile\"\n+  private[spark] val CLIENT_CERT_FILE_CONF_SUFFIX = \"clientCertFile\"\n+  private[spark] val CA_CERT_FILE_CONF_SUFFIX = \"caCertFile\"\n+\n+  private[spark] val KUBERNETES_SERVICE_ACCOUNT_NAME =\n+    ConfigBuilder(s\"$APISERVER_AUTH_DRIVER_CONF_PREFIX.serviceAccountName\")\n+      .doc(\"Service account that is used when running the driver pod. The driver pod uses\" +\n+        \" this service account when requesting executor pods from the API server. If specific\" +\n+        \" credentials are given for the driver pod to use, the driver will favor\" +\n+        \" using those credentials instead.\")\n+      .stringConf\n+      .createOptional\n+\n+  // Note that while we set a default for this when we start up the\n+  // scheduler, the specific default value is dynamically determined\n+  // based on the executor memory.\n+  private[spark] val KUBERNETES_EXECUTOR_MEMORY_OVERHEAD =\n+    ConfigBuilder(\"spark.kubernetes.executor.memoryOverhead\")\n+      .doc(\"The amount of off-heap memory (in megabytes) to be allocated per executor. This\" +\n+        \" is memory that accounts for things like VM overheads, interned strings, other native\" +\n+        \" overheads, etc. This tends to grow with the executor size. (typically 6-10%).\")\n+      .bytesConf(ByteUnit.MiB)\n+      .createOptional\n+\n+  private[spark] val KUBERNETES_EXECUTOR_LABEL_PREFIX = \"spark.kubernetes.executor.label.\"\n+  private[spark] val KUBERNETES_EXECUTOR_ANNOTATION_PREFIX = \"spark.kubernetes.executor.annotation.\"\n+\n+  private[spark] val KUBERNETES_DRIVER_POD_NAME =\n+    ConfigBuilder(\"spark.kubernetes.driver.pod.name\")\n+      .doc(\"Name of the driver pod.\")\n+      .stringConf\n+      .createOptional\n+\n+  private[spark] val KUBERNETES_EXECUTOR_POD_NAME_PREFIX =\n+    ConfigBuilder(\"spark.kubernetes.executor.podNamePrefix\")\n+      .doc(\"Prefix to use in front of the executor pod names.\")\n+      .internal()\n+      .stringConf\n+      .createWithDefault(\"spark\")\n+\n+  private[spark] val KUBERNETES_ALLOCATION_BATCH_SIZE =\n+    ConfigBuilder(\"spark.kubernetes.allocation.batch.size\")\n+      .doc(\"Number of pods to launch at once in each round of dynamic allocation. \")\n+      .intConf\n+      .createWithDefault(5)\n+\n+  private[spark] val KUBERNETES_ALLOCATION_BATCH_DELAY =\n+    ConfigBuilder(\"spark.kubernetes.allocation.batch.delay\")\n+      .doc(\"Number of seconds to wait between each round of executor allocation. \")\n+      .longConf\n+      .createWithDefault(1)\n+\n+  private[spark] val INIT_CONTAINER_JARS_DOWNLOAD_LOCATION =\n+    ConfigBuilder(\"spark.kubernetes.mountdependencies.jarsDownloadDir\")\n+      .doc(\"Location to download jars to in the driver and executors. When using\" +\n+        \" spark-submit, this directory must be empty and will be mounted as an empty directory\" +\n+        \" volume on the driver and executor pod.\")\n+      .stringConf\n+      .createWithDefault(\"/var/spark-data/spark-jars\")"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "Hm, this should be removed until we introduce the concept of the jar download init container in a later push.",
    "commit": "b85cfc4038c8de9340b78d10edf88ab76dd90ba3",
    "createdAt": "2017-10-25T22:07:57Z",
    "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s\n+\n+import org.apache.spark.{SPARK_VERSION => sparkVersion}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.ConfigBuilder\n+import org.apache.spark.network.util.ByteUnit\n+\n+package object config extends Logging {\n+\n+  private[spark] val KUBERNETES_NAMESPACE =\n+    ConfigBuilder(\"spark.kubernetes.namespace\")\n+      .doc(\"The namespace that will be used for running the driver and executor pods. When using\" +\n+        \" spark-submit in cluster mode, this can also be passed to spark-submit via the\" +\n+        \" --kubernetes-namespace command line argument.\")\n+      .stringConf\n+      .createWithDefault(\"default\")\n+\n+  private[spark] val EXECUTOR_DOCKER_IMAGE =\n+    ConfigBuilder(\"spark.kubernetes.executor.docker.image\")\n+      .doc(\"Docker image to use for the executors. Specify this using the standard Docker tag\" +\n+        \" format.\")\n+      .stringConf\n+      .createWithDefault(s\"spark-executor:$sparkVersion\")\n+\n+  private[spark] val DOCKER_IMAGE_PULL_POLICY =\n+    ConfigBuilder(\"spark.kubernetes.docker.image.pullPolicy\")\n+      .doc(\"Docker image pull policy when pulling any docker image in Kubernetes integration\")\n+      .stringConf\n+      .createWithDefault(\"IfNotPresent\")\n+\n+\n+  private[spark] val APISERVER_AUTH_DRIVER_CONF_PREFIX =\n+      \"spark.kubernetes.authenticate.driver\"\n+  private[spark] val APISERVER_AUTH_DRIVER_MOUNTED_CONF_PREFIX =\n+      \"spark.kubernetes.authenticate.driver.mounted\"\n+  private[spark] val OAUTH_TOKEN_CONF_SUFFIX = \"oauthToken\"\n+  private[spark] val OAUTH_TOKEN_FILE_CONF_SUFFIX = \"oauthTokenFile\"\n+  private[spark] val CLIENT_KEY_FILE_CONF_SUFFIX = \"clientKeyFile\"\n+  private[spark] val CLIENT_CERT_FILE_CONF_SUFFIX = \"clientCertFile\"\n+  private[spark] val CA_CERT_FILE_CONF_SUFFIX = \"caCertFile\"\n+\n+  private[spark] val KUBERNETES_SERVICE_ACCOUNT_NAME =\n+    ConfigBuilder(s\"$APISERVER_AUTH_DRIVER_CONF_PREFIX.serviceAccountName\")\n+      .doc(\"Service account that is used when running the driver pod. The driver pod uses\" +\n+        \" this service account when requesting executor pods from the API server. If specific\" +\n+        \" credentials are given for the driver pod to use, the driver will favor\" +\n+        \" using those credentials instead.\")\n+      .stringConf\n+      .createOptional\n+\n+  // Note that while we set a default for this when we start up the\n+  // scheduler, the specific default value is dynamically determined\n+  // based on the executor memory.\n+  private[spark] val KUBERNETES_EXECUTOR_MEMORY_OVERHEAD =\n+    ConfigBuilder(\"spark.kubernetes.executor.memoryOverhead\")\n+      .doc(\"The amount of off-heap memory (in megabytes) to be allocated per executor. This\" +\n+        \" is memory that accounts for things like VM overheads, interned strings, other native\" +\n+        \" overheads, etc. This tends to grow with the executor size. (typically 6-10%).\")\n+      .bytesConf(ByteUnit.MiB)\n+      .createOptional\n+\n+  private[spark] val KUBERNETES_EXECUTOR_LABEL_PREFIX = \"spark.kubernetes.executor.label.\"\n+  private[spark] val KUBERNETES_EXECUTOR_ANNOTATION_PREFIX = \"spark.kubernetes.executor.annotation.\"\n+\n+  private[spark] val KUBERNETES_DRIVER_POD_NAME =\n+    ConfigBuilder(\"spark.kubernetes.driver.pod.name\")\n+      .doc(\"Name of the driver pod.\")\n+      .stringConf\n+      .createOptional\n+\n+  private[spark] val KUBERNETES_EXECUTOR_POD_NAME_PREFIX =\n+    ConfigBuilder(\"spark.kubernetes.executor.podNamePrefix\")\n+      .doc(\"Prefix to use in front of the executor pod names.\")\n+      .internal()\n+      .stringConf\n+      .createWithDefault(\"spark\")\n+\n+  private[spark] val KUBERNETES_ALLOCATION_BATCH_SIZE =\n+    ConfigBuilder(\"spark.kubernetes.allocation.batch.size\")\n+      .doc(\"Number of pods to launch at once in each round of dynamic allocation. \")\n+      .intConf\n+      .createWithDefault(5)\n+\n+  private[spark] val KUBERNETES_ALLOCATION_BATCH_DELAY =\n+    ConfigBuilder(\"spark.kubernetes.allocation.batch.delay\")\n+      .doc(\"Number of seconds to wait between each round of executor allocation. \")\n+      .longConf\n+      .createWithDefault(1)\n+\n+  private[spark] val INIT_CONTAINER_JARS_DOWNLOAD_LOCATION =\n+    ConfigBuilder(\"spark.kubernetes.mountdependencies.jarsDownloadDir\")\n+      .doc(\"Location to download jars to in the driver and executors. When using\" +\n+        \" spark-submit, this directory must be empty and will be mounted as an empty directory\" +\n+        \" volume on the driver and executor pod.\")\n+      .stringConf\n+      .createWithDefault(\"/var/spark-data/spark-jars\")"
  }],
  "prId": 19468
}, {
  "comments": [{
    "author": {
      "login": "mridulm"
    },
    "body": "While this does conform to uri spec, it does look fairly weird (`k8s://https://..`) : not to mention, any normalization of the uri will make it invalid.\r\nWondering if it makes more sense to namespace it better.\r\n\r\nAlso, nothing seems to be using it currently ?",
    "commit": "b85cfc4038c8de9340b78d10edf88ab76dd90ba3",
    "createdAt": "2017-10-25T19:36:17Z",
    "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s\n+\n+import org.apache.spark.{SPARK_VERSION => sparkVersion}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.ConfigBuilder\n+import org.apache.spark.network.util.ByteUnit\n+\n+package object config extends Logging {\n+\n+  private[spark] val KUBERNETES_NAMESPACE =\n+    ConfigBuilder(\"spark.kubernetes.namespace\")\n+      .doc(\"The namespace that will be used for running the driver and executor pods. When using\" +\n+        \" spark-submit in cluster mode, this can also be passed to spark-submit via the\" +\n+        \" --kubernetes-namespace command line argument.\")\n+      .stringConf\n+      .createWithDefault(\"default\")\n+\n+  private[spark] val EXECUTOR_DOCKER_IMAGE =\n+    ConfigBuilder(\"spark.kubernetes.executor.docker.image\")\n+      .doc(\"Docker image to use for the executors. Specify this using the standard Docker tag\" +\n+        \" format.\")\n+      .stringConf\n+      .createWithDefault(s\"spark-executor:$sparkVersion\")\n+\n+  private[spark] val DOCKER_IMAGE_PULL_POLICY =\n+    ConfigBuilder(\"spark.kubernetes.docker.image.pullPolicy\")\n+      .doc(\"Docker image pull policy when pulling any docker image in Kubernetes integration\")\n+      .stringConf\n+      .createWithDefault(\"IfNotPresent\")\n+\n+\n+  private[spark] val APISERVER_AUTH_DRIVER_CONF_PREFIX =\n+      \"spark.kubernetes.authenticate.driver\"\n+  private[spark] val APISERVER_AUTH_DRIVER_MOUNTED_CONF_PREFIX =\n+      \"spark.kubernetes.authenticate.driver.mounted\"\n+  private[spark] val OAUTH_TOKEN_CONF_SUFFIX = \"oauthToken\"\n+  private[spark] val OAUTH_TOKEN_FILE_CONF_SUFFIX = \"oauthTokenFile\"\n+  private[spark] val CLIENT_KEY_FILE_CONF_SUFFIX = \"clientKeyFile\"\n+  private[spark] val CLIENT_CERT_FILE_CONF_SUFFIX = \"clientCertFile\"\n+  private[spark] val CA_CERT_FILE_CONF_SUFFIX = \"caCertFile\"\n+\n+  private[spark] val KUBERNETES_SERVICE_ACCOUNT_NAME =\n+    ConfigBuilder(s\"$APISERVER_AUTH_DRIVER_CONF_PREFIX.serviceAccountName\")\n+      .doc(\"Service account that is used when running the driver pod. The driver pod uses\" +\n+        \" this service account when requesting executor pods from the API server. If specific\" +\n+        \" credentials are given for the driver pod to use, the driver will favor\" +\n+        \" using those credentials instead.\")\n+      .stringConf\n+      .createOptional\n+\n+  // Note that while we set a default for this when we start up the\n+  // scheduler, the specific default value is dynamically determined\n+  // based on the executor memory.\n+  private[spark] val KUBERNETES_EXECUTOR_MEMORY_OVERHEAD =\n+    ConfigBuilder(\"spark.kubernetes.executor.memoryOverhead\")\n+      .doc(\"The amount of off-heap memory (in megabytes) to be allocated per executor. This\" +\n+        \" is memory that accounts for things like VM overheads, interned strings, other native\" +\n+        \" overheads, etc. This tends to grow with the executor size. (typically 6-10%).\")\n+      .bytesConf(ByteUnit.MiB)\n+      .createOptional\n+\n+  private[spark] val KUBERNETES_EXECUTOR_LABEL_PREFIX = \"spark.kubernetes.executor.label.\"\n+  private[spark] val KUBERNETES_EXECUTOR_ANNOTATION_PREFIX = \"spark.kubernetes.executor.annotation.\"\n+\n+  private[spark] val KUBERNETES_DRIVER_POD_NAME =\n+    ConfigBuilder(\"spark.kubernetes.driver.pod.name\")\n+      .doc(\"Name of the driver pod.\")\n+      .stringConf\n+      .createOptional\n+\n+  private[spark] val KUBERNETES_EXECUTOR_POD_NAME_PREFIX =\n+    ConfigBuilder(\"spark.kubernetes.executor.podNamePrefix\")\n+      .doc(\"Prefix to use in front of the executor pod names.\")\n+      .internal()\n+      .stringConf\n+      .createWithDefault(\"spark\")\n+\n+  private[spark] val KUBERNETES_ALLOCATION_BATCH_SIZE =\n+    ConfigBuilder(\"spark.kubernetes.allocation.batch.size\")\n+      .doc(\"Number of pods to launch at once in each round of dynamic allocation. \")\n+      .intConf\n+      .createWithDefault(5)\n+\n+  private[spark] val KUBERNETES_ALLOCATION_BATCH_DELAY =\n+    ConfigBuilder(\"spark.kubernetes.allocation.batch.delay\")\n+      .doc(\"Number of seconds to wait between each round of executor allocation. \")\n+      .longConf\n+      .createWithDefault(1)\n+\n+  private[spark] val INIT_CONTAINER_JARS_DOWNLOAD_LOCATION =\n+    ConfigBuilder(\"spark.kubernetes.mountdependencies.jarsDownloadDir\")\n+      .doc(\"Location to download jars to in the driver and executors. When using\" +\n+        \" spark-submit, this directory must be empty and will be mounted as an empty directory\" +\n+        \" volume on the driver and executor pod.\")\n+      .stringConf\n+      .createWithDefault(\"/var/spark-data/spark-jars\")\n+\n+  private[spark] val KUBERNETES_EXECUTOR_LIMIT_CORES =\n+    ConfigBuilder(\"spark.kubernetes.executor.limit.cores\")\n+      .doc(\"Specify the hard cpu limit for a single executor pod\")\n+      .stringConf\n+      .createOptional\n+\n+  private[spark] val KUBERNETES_NODE_SELECTOR_PREFIX = \"spark.kubernetes.node.selector.\"\n+\n+  private[spark] def resolveK8sMaster(rawMasterString: String): String = {\n+    if (!rawMasterString.startsWith(\"k8s://\")) {\n+      throw new IllegalArgumentException(\"Master URL should start with k8s:// in Kubernetes mode.\")\n+    }\n+    val masterWithoutK8sPrefix = rawMasterString.replaceFirst(\"k8s://\", \"\")\n+    if (masterWithoutK8sPrefix.startsWith(\"http://\")\n+      || masterWithoutK8sPrefix.startsWith(\"https://\")) {"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "Yeah we can remove this method for this commit and introduce it when we use it in a future commit. `scheme1://scheme2://host` is used for Mesos with Zookeeper, so it's not entirely without precedent.",
    "commit": "b85cfc4038c8de9340b78d10edf88ab76dd90ba3",
    "createdAt": "2017-10-25T22:08:53Z",
    "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s\n+\n+import org.apache.spark.{SPARK_VERSION => sparkVersion}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.ConfigBuilder\n+import org.apache.spark.network.util.ByteUnit\n+\n+package object config extends Logging {\n+\n+  private[spark] val KUBERNETES_NAMESPACE =\n+    ConfigBuilder(\"spark.kubernetes.namespace\")\n+      .doc(\"The namespace that will be used for running the driver and executor pods. When using\" +\n+        \" spark-submit in cluster mode, this can also be passed to spark-submit via the\" +\n+        \" --kubernetes-namespace command line argument.\")\n+      .stringConf\n+      .createWithDefault(\"default\")\n+\n+  private[spark] val EXECUTOR_DOCKER_IMAGE =\n+    ConfigBuilder(\"spark.kubernetes.executor.docker.image\")\n+      .doc(\"Docker image to use for the executors. Specify this using the standard Docker tag\" +\n+        \" format.\")\n+      .stringConf\n+      .createWithDefault(s\"spark-executor:$sparkVersion\")\n+\n+  private[spark] val DOCKER_IMAGE_PULL_POLICY =\n+    ConfigBuilder(\"spark.kubernetes.docker.image.pullPolicy\")\n+      .doc(\"Docker image pull policy when pulling any docker image in Kubernetes integration\")\n+      .stringConf\n+      .createWithDefault(\"IfNotPresent\")\n+\n+\n+  private[spark] val APISERVER_AUTH_DRIVER_CONF_PREFIX =\n+      \"spark.kubernetes.authenticate.driver\"\n+  private[spark] val APISERVER_AUTH_DRIVER_MOUNTED_CONF_PREFIX =\n+      \"spark.kubernetes.authenticate.driver.mounted\"\n+  private[spark] val OAUTH_TOKEN_CONF_SUFFIX = \"oauthToken\"\n+  private[spark] val OAUTH_TOKEN_FILE_CONF_SUFFIX = \"oauthTokenFile\"\n+  private[spark] val CLIENT_KEY_FILE_CONF_SUFFIX = \"clientKeyFile\"\n+  private[spark] val CLIENT_CERT_FILE_CONF_SUFFIX = \"clientCertFile\"\n+  private[spark] val CA_CERT_FILE_CONF_SUFFIX = \"caCertFile\"\n+\n+  private[spark] val KUBERNETES_SERVICE_ACCOUNT_NAME =\n+    ConfigBuilder(s\"$APISERVER_AUTH_DRIVER_CONF_PREFIX.serviceAccountName\")\n+      .doc(\"Service account that is used when running the driver pod. The driver pod uses\" +\n+        \" this service account when requesting executor pods from the API server. If specific\" +\n+        \" credentials are given for the driver pod to use, the driver will favor\" +\n+        \" using those credentials instead.\")\n+      .stringConf\n+      .createOptional\n+\n+  // Note that while we set a default for this when we start up the\n+  // scheduler, the specific default value is dynamically determined\n+  // based on the executor memory.\n+  private[spark] val KUBERNETES_EXECUTOR_MEMORY_OVERHEAD =\n+    ConfigBuilder(\"spark.kubernetes.executor.memoryOverhead\")\n+      .doc(\"The amount of off-heap memory (in megabytes) to be allocated per executor. This\" +\n+        \" is memory that accounts for things like VM overheads, interned strings, other native\" +\n+        \" overheads, etc. This tends to grow with the executor size. (typically 6-10%).\")\n+      .bytesConf(ByteUnit.MiB)\n+      .createOptional\n+\n+  private[spark] val KUBERNETES_EXECUTOR_LABEL_PREFIX = \"spark.kubernetes.executor.label.\"\n+  private[spark] val KUBERNETES_EXECUTOR_ANNOTATION_PREFIX = \"spark.kubernetes.executor.annotation.\"\n+\n+  private[spark] val KUBERNETES_DRIVER_POD_NAME =\n+    ConfigBuilder(\"spark.kubernetes.driver.pod.name\")\n+      .doc(\"Name of the driver pod.\")\n+      .stringConf\n+      .createOptional\n+\n+  private[spark] val KUBERNETES_EXECUTOR_POD_NAME_PREFIX =\n+    ConfigBuilder(\"spark.kubernetes.executor.podNamePrefix\")\n+      .doc(\"Prefix to use in front of the executor pod names.\")\n+      .internal()\n+      .stringConf\n+      .createWithDefault(\"spark\")\n+\n+  private[spark] val KUBERNETES_ALLOCATION_BATCH_SIZE =\n+    ConfigBuilder(\"spark.kubernetes.allocation.batch.size\")\n+      .doc(\"Number of pods to launch at once in each round of dynamic allocation. \")\n+      .intConf\n+      .createWithDefault(5)\n+\n+  private[spark] val KUBERNETES_ALLOCATION_BATCH_DELAY =\n+    ConfigBuilder(\"spark.kubernetes.allocation.batch.delay\")\n+      .doc(\"Number of seconds to wait between each round of executor allocation. \")\n+      .longConf\n+      .createWithDefault(1)\n+\n+  private[spark] val INIT_CONTAINER_JARS_DOWNLOAD_LOCATION =\n+    ConfigBuilder(\"spark.kubernetes.mountdependencies.jarsDownloadDir\")\n+      .doc(\"Location to download jars to in the driver and executors. When using\" +\n+        \" spark-submit, this directory must be empty and will be mounted as an empty directory\" +\n+        \" volume on the driver and executor pod.\")\n+      .stringConf\n+      .createWithDefault(\"/var/spark-data/spark-jars\")\n+\n+  private[spark] val KUBERNETES_EXECUTOR_LIMIT_CORES =\n+    ConfigBuilder(\"spark.kubernetes.executor.limit.cores\")\n+      .doc(\"Specify the hard cpu limit for a single executor pod\")\n+      .stringConf\n+      .createOptional\n+\n+  private[spark] val KUBERNETES_NODE_SELECTOR_PREFIX = \"spark.kubernetes.node.selector.\"\n+\n+  private[spark] def resolveK8sMaster(rawMasterString: String): String = {\n+    if (!rawMasterString.startsWith(\"k8s://\")) {\n+      throw new IllegalArgumentException(\"Master URL should start with k8s:// in Kubernetes mode.\")\n+    }\n+    val masterWithoutK8sPrefix = rawMasterString.replaceFirst(\"k8s://\", \"\")\n+    if (masterWithoutK8sPrefix.startsWith(\"http://\")\n+      || masterWithoutK8sPrefix.startsWith(\"https://\")) {"
  }, {
    "author": {
      "login": "mridulm"
    },
    "body": "Indeed, you are right - that kind of sucks.",
    "commit": "b85cfc4038c8de9340b78d10edf88ab76dd90ba3",
    "createdAt": "2017-10-25T23:32:50Z",
    "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s\n+\n+import org.apache.spark.{SPARK_VERSION => sparkVersion}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.ConfigBuilder\n+import org.apache.spark.network.util.ByteUnit\n+\n+package object config extends Logging {\n+\n+  private[spark] val KUBERNETES_NAMESPACE =\n+    ConfigBuilder(\"spark.kubernetes.namespace\")\n+      .doc(\"The namespace that will be used for running the driver and executor pods. When using\" +\n+        \" spark-submit in cluster mode, this can also be passed to spark-submit via the\" +\n+        \" --kubernetes-namespace command line argument.\")\n+      .stringConf\n+      .createWithDefault(\"default\")\n+\n+  private[spark] val EXECUTOR_DOCKER_IMAGE =\n+    ConfigBuilder(\"spark.kubernetes.executor.docker.image\")\n+      .doc(\"Docker image to use for the executors. Specify this using the standard Docker tag\" +\n+        \" format.\")\n+      .stringConf\n+      .createWithDefault(s\"spark-executor:$sparkVersion\")\n+\n+  private[spark] val DOCKER_IMAGE_PULL_POLICY =\n+    ConfigBuilder(\"spark.kubernetes.docker.image.pullPolicy\")\n+      .doc(\"Docker image pull policy when pulling any docker image in Kubernetes integration\")\n+      .stringConf\n+      .createWithDefault(\"IfNotPresent\")\n+\n+\n+  private[spark] val APISERVER_AUTH_DRIVER_CONF_PREFIX =\n+      \"spark.kubernetes.authenticate.driver\"\n+  private[spark] val APISERVER_AUTH_DRIVER_MOUNTED_CONF_PREFIX =\n+      \"spark.kubernetes.authenticate.driver.mounted\"\n+  private[spark] val OAUTH_TOKEN_CONF_SUFFIX = \"oauthToken\"\n+  private[spark] val OAUTH_TOKEN_FILE_CONF_SUFFIX = \"oauthTokenFile\"\n+  private[spark] val CLIENT_KEY_FILE_CONF_SUFFIX = \"clientKeyFile\"\n+  private[spark] val CLIENT_CERT_FILE_CONF_SUFFIX = \"clientCertFile\"\n+  private[spark] val CA_CERT_FILE_CONF_SUFFIX = \"caCertFile\"\n+\n+  private[spark] val KUBERNETES_SERVICE_ACCOUNT_NAME =\n+    ConfigBuilder(s\"$APISERVER_AUTH_DRIVER_CONF_PREFIX.serviceAccountName\")\n+      .doc(\"Service account that is used when running the driver pod. The driver pod uses\" +\n+        \" this service account when requesting executor pods from the API server. If specific\" +\n+        \" credentials are given for the driver pod to use, the driver will favor\" +\n+        \" using those credentials instead.\")\n+      .stringConf\n+      .createOptional\n+\n+  // Note that while we set a default for this when we start up the\n+  // scheduler, the specific default value is dynamically determined\n+  // based on the executor memory.\n+  private[spark] val KUBERNETES_EXECUTOR_MEMORY_OVERHEAD =\n+    ConfigBuilder(\"spark.kubernetes.executor.memoryOverhead\")\n+      .doc(\"The amount of off-heap memory (in megabytes) to be allocated per executor. This\" +\n+        \" is memory that accounts for things like VM overheads, interned strings, other native\" +\n+        \" overheads, etc. This tends to grow with the executor size. (typically 6-10%).\")\n+      .bytesConf(ByteUnit.MiB)\n+      .createOptional\n+\n+  private[spark] val KUBERNETES_EXECUTOR_LABEL_PREFIX = \"spark.kubernetes.executor.label.\"\n+  private[spark] val KUBERNETES_EXECUTOR_ANNOTATION_PREFIX = \"spark.kubernetes.executor.annotation.\"\n+\n+  private[spark] val KUBERNETES_DRIVER_POD_NAME =\n+    ConfigBuilder(\"spark.kubernetes.driver.pod.name\")\n+      .doc(\"Name of the driver pod.\")\n+      .stringConf\n+      .createOptional\n+\n+  private[spark] val KUBERNETES_EXECUTOR_POD_NAME_PREFIX =\n+    ConfigBuilder(\"spark.kubernetes.executor.podNamePrefix\")\n+      .doc(\"Prefix to use in front of the executor pod names.\")\n+      .internal()\n+      .stringConf\n+      .createWithDefault(\"spark\")\n+\n+  private[spark] val KUBERNETES_ALLOCATION_BATCH_SIZE =\n+    ConfigBuilder(\"spark.kubernetes.allocation.batch.size\")\n+      .doc(\"Number of pods to launch at once in each round of dynamic allocation. \")\n+      .intConf\n+      .createWithDefault(5)\n+\n+  private[spark] val KUBERNETES_ALLOCATION_BATCH_DELAY =\n+    ConfigBuilder(\"spark.kubernetes.allocation.batch.delay\")\n+      .doc(\"Number of seconds to wait between each round of executor allocation. \")\n+      .longConf\n+      .createWithDefault(1)\n+\n+  private[spark] val INIT_CONTAINER_JARS_DOWNLOAD_LOCATION =\n+    ConfigBuilder(\"spark.kubernetes.mountdependencies.jarsDownloadDir\")\n+      .doc(\"Location to download jars to in the driver and executors. When using\" +\n+        \" spark-submit, this directory must be empty and will be mounted as an empty directory\" +\n+        \" volume on the driver and executor pod.\")\n+      .stringConf\n+      .createWithDefault(\"/var/spark-data/spark-jars\")\n+\n+  private[spark] val KUBERNETES_EXECUTOR_LIMIT_CORES =\n+    ConfigBuilder(\"spark.kubernetes.executor.limit.cores\")\n+      .doc(\"Specify the hard cpu limit for a single executor pod\")\n+      .stringConf\n+      .createOptional\n+\n+  private[spark] val KUBERNETES_NODE_SELECTOR_PREFIX = \"spark.kubernetes.node.selector.\"\n+\n+  private[spark] def resolveK8sMaster(rawMasterString: String): String = {\n+    if (!rawMasterString.startsWith(\"k8s://\")) {\n+      throw new IllegalArgumentException(\"Master URL should start with k8s:// in Kubernetes mode.\")\n+    }\n+    val masterWithoutK8sPrefix = rawMasterString.replaceFirst(\"k8s://\", \"\")\n+    if (masterWithoutK8sPrefix.startsWith(\"http://\")\n+      || masterWithoutK8sPrefix.startsWith(\"https://\")) {"
  }],
  "prId": 19468
}, {
  "comments": [{
    "author": {
      "login": "jiangxb1987"
    },
    "body": "nit: duplicated empty lines.",
    "commit": "b85cfc4038c8de9340b78d10edf88ab76dd90ba3",
    "createdAt": "2017-11-13T10:16:36Z",
    "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s\n+\n+import org.apache.spark.{SPARK_VERSION => sparkVersion}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.ConfigBuilder\n+import org.apache.spark.network.util.ByteUnit\n+\n+package object config extends Logging {\n+\n+  private[spark] val KUBERNETES_NAMESPACE =\n+    ConfigBuilder(\"spark.kubernetes.namespace\")\n+      .doc(\"The namespace that will be used for running the driver and executor pods. When using\" +\n+        \" spark-submit in cluster mode, this can also be passed to spark-submit via the\" +\n+        \" --kubernetes-namespace command line argument.\")\n+      .stringConf\n+      .createWithDefault(\"default\")\n+\n+  private[spark] val EXECUTOR_DOCKER_IMAGE =\n+    ConfigBuilder(\"spark.kubernetes.executor.docker.image\")\n+      .doc(\"Docker image to use for the executors. Specify this using the standard Docker tag\" +\n+        \" format.\")\n+      .stringConf\n+      .createWithDefault(s\"spark-executor:$sparkVersion\")\n+\n+  private[spark] val DOCKER_IMAGE_PULL_POLICY =\n+    ConfigBuilder(\"spark.kubernetes.docker.image.pullPolicy\")\n+      .doc(\"Docker image pull policy when pulling any docker image in Kubernetes integration\")\n+      .stringConf\n+      .createWithDefault(\"IfNotPresent\")\n+"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Removed.",
    "commit": "b85cfc4038c8de9340b78d10edf88ab76dd90ba3",
    "createdAt": "2017-11-13T18:27:22Z",
    "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s\n+\n+import org.apache.spark.{SPARK_VERSION => sparkVersion}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.ConfigBuilder\n+import org.apache.spark.network.util.ByteUnit\n+\n+package object config extends Logging {\n+\n+  private[spark] val KUBERNETES_NAMESPACE =\n+    ConfigBuilder(\"spark.kubernetes.namespace\")\n+      .doc(\"The namespace that will be used for running the driver and executor pods. When using\" +\n+        \" spark-submit in cluster mode, this can also be passed to spark-submit via the\" +\n+        \" --kubernetes-namespace command line argument.\")\n+      .stringConf\n+      .createWithDefault(\"default\")\n+\n+  private[spark] val EXECUTOR_DOCKER_IMAGE =\n+    ConfigBuilder(\"spark.kubernetes.executor.docker.image\")\n+      .doc(\"Docker image to use for the executors. Specify this using the standard Docker tag\" +\n+        \" format.\")\n+      .stringConf\n+      .createWithDefault(s\"spark-executor:$sparkVersion\")\n+\n+  private[spark] val DOCKER_IMAGE_PULL_POLICY =\n+    ConfigBuilder(\"spark.kubernetes.docker.image.pullPolicy\")\n+      .doc(\"Docker image pull policy when pulling any docker image in Kubernetes integration\")\n+      .stringConf\n+      .createWithDefault(\"IfNotPresent\")\n+"
  }],
  "prId": 19468
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "FYI, you can have a plain `private[spark] object config` now which should remove the need for declaring `private[spark]` in each declared field...",
    "commit": "b85cfc4038c8de9340b78d10edf88ab76dd90ba3",
    "createdAt": "2017-11-14T00:06:47Z",
    "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s\n+\n+import org.apache.spark.{SPARK_VERSION => sparkVersion}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.ConfigBuilder\n+import org.apache.spark.network.util.ByteUnit\n+\n+package object config extends Logging {"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Done.",
    "commit": "b85cfc4038c8de9340b78d10edf88ab76dd90ba3",
    "createdAt": "2017-11-14T05:42:24Z",
    "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s\n+\n+import org.apache.spark.{SPARK_VERSION => sparkVersion}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.ConfigBuilder\n+import org.apache.spark.network.util.ByteUnit\n+\n+package object config extends Logging {"
  }],
  "prId": 19468
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`$SPARK_VERSION` does not work?",
    "commit": "b85cfc4038c8de9340b78d10edf88ab76dd90ba3",
    "createdAt": "2017-11-14T00:11:56Z",
    "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s\n+\n+import org.apache.spark.{SPARK_VERSION => sparkVersion}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.ConfigBuilder\n+import org.apache.spark.network.util.ByteUnit\n+\n+package object config extends Logging {\n+\n+  private[spark] val KUBERNETES_NAMESPACE =\n+    ConfigBuilder(\"spark.kubernetes.namespace\")\n+      .doc(\"The namespace that will be used for running the driver and executor pods. When using\" +\n+        \" spark-submit in cluster mode, this can also be passed to spark-submit via the\" +\n+        \" --kubernetes-namespace command line argument.\")\n+      .stringConf\n+      .createWithDefault(\"default\")\n+\n+  private[spark] val EXECUTOR_DOCKER_IMAGE =\n+    ConfigBuilder(\"spark.kubernetes.executor.docker.image\")\n+      .doc(\"Docker image to use for the executors. Specify this using the standard Docker tag\" +\n+        \" format.\")\n+      .stringConf\n+      .createWithDefault(s\"spark-executor:$sparkVersion\")"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Done.",
    "commit": "b85cfc4038c8de9340b78d10edf88ab76dd90ba3",
    "createdAt": "2017-11-14T05:42:21Z",
    "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s\n+\n+import org.apache.spark.{SPARK_VERSION => sparkVersion}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config.ConfigBuilder\n+import org.apache.spark.network.util.ByteUnit\n+\n+package object config extends Logging {\n+\n+  private[spark] val KUBERNETES_NAMESPACE =\n+    ConfigBuilder(\"spark.kubernetes.namespace\")\n+      .doc(\"The namespace that will be used for running the driver and executor pods. When using\" +\n+        \" spark-submit in cluster mode, this can also be passed to spark-submit via the\" +\n+        \" --kubernetes-namespace command line argument.\")\n+      .stringConf\n+      .createWithDefault(\"default\")\n+\n+  private[spark] val EXECUTOR_DOCKER_IMAGE =\n+    ConfigBuilder(\"spark.kubernetes.executor.docker.image\")\n+      .doc(\"Docker image to use for the executors. Specify this using the standard Docker tag\" +\n+        \" format.\")\n+      .stringConf\n+      .createWithDefault(s\"spark-executor:$sparkVersion\")"
  }],
  "prId": 19468
}]