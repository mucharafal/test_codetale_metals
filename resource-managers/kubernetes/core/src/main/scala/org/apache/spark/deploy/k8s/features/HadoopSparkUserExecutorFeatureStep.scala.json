[{
  "comments": [{
    "author": {
      "login": "liyinan926"
    },
    "body": "Ditto.",
    "commit": "516ae6816a8521e944fa7471f2c069dbfc93ecfe",
    "createdAt": "2018-11-19T20:42:58Z",
    "diffHunk": "@@ -16,28 +16,19 @@\n  */\n package org.apache.spark.deploy.k8s.features\n \n-import io.fabric8.kubernetes.api.model.HasMetadata\n-\n-import org.apache.spark.deploy.k8s.{KubernetesConf, SparkPod}\n+import org.apache.spark.deploy.k8s.{KubernetesExecutorConf, SparkPod}\n import org.apache.spark.deploy.k8s.Constants._\n-import org.apache.spark.deploy.k8s.KubernetesExecutorSpecificConf\n import org.apache.spark.deploy.k8s.features.hadooputils.HadoopBootstrapUtil\n-import org.apache.spark.internal.Logging\n \n /**\n  * This step is responsible for setting ENV_SPARK_USER when HADOOP_FILES are detected\n  * however, this step would not be run if Kerberos is enabled, as Kerberos sets SPARK_USER\n  */\n-private[spark] class HadoopSparkUserExecutorFeatureStep(\n-    kubernetesConf: KubernetesConf[KubernetesExecutorSpecificConf])\n-  extends KubernetesFeatureConfigStep with Logging {\n+private[spark] class HadoopSparkUserExecutorFeatureStep(conf: KubernetesExecutorConf)\n+  extends KubernetesFeatureConfigStep {\n \n   override def configurePod(pod: SparkPod): SparkPod = {\n-    val sparkUserName = kubernetesConf.sparkConf.get(KERBEROS_SPARK_USER_NAME)\n+    val sparkUserName = conf.sparkConf.get(KERBEROS_SPARK_USER_NAME)"
  }],
  "prId": 22959
}]