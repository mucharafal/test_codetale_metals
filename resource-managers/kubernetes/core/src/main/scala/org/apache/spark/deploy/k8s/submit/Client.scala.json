[{
  "comments": [{
    "author": {
      "login": "mridulm"
    },
    "body": "As with #19468 I am not sure how this is being used in driver.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-27T01:59:50Z",
    "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ClientArguments(\n+     mainAppResource: MainAppResource,\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.Buffer.empty[String]\n+\n+    args.sliding(2, 2).toList.collect {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainAppResource.isDefined,\n+      \"Main app resource must be defined by either --primary-py-file or --primary-java-resource.\")\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource.get,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }\n+\n+    val resolvedDriverJavaOpts = currentDriverSpec\n+      .driverSparkConf\n+      // Remove this as the options are instead extracted and set individually below using\n+      // environment variables with prefix SPARK_JAVA_OPT_.\n+      .remove(org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+      .getAll\n+      .map {\n+        case (confKey, confValue) => s\"-D$confKey=$confValue\"\n+      } ++ driverJavaOptions.map(Utils.splitCommandString).getOrElse(Seq.empty)\n+    val driverJavaOptsEnvs: Seq[EnvVar] = resolvedDriverJavaOpts.zipWithIndex.map {\n+      case (option, index) => new EnvVarBuilder()\n+        .withName(s\"$ENV_JAVA_OPT_PREFIX$index\")\n+        .withValue(option)\n+        .build()\n+    }"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "It's used in the driver Docker file included in this PR. See https://github.com/apache-spark-on-k8s/spark/blob/60234a29846955b8a6e8cb6fbb1dc35da3c3b4f2/resource-managers/kubernetes/docker/src/main/dockerfiles/driver/Dockerfile.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-27T16:41:53Z",
    "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ClientArguments(\n+     mainAppResource: MainAppResource,\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.Buffer.empty[String]\n+\n+    args.sliding(2, 2).toList.collect {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainAppResource.isDefined,\n+      \"Main app resource must be defined by either --primary-py-file or --primary-java-resource.\")\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource.get,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }\n+\n+    val resolvedDriverJavaOpts = currentDriverSpec\n+      .driverSparkConf\n+      // Remove this as the options are instead extracted and set individually below using\n+      // environment variables with prefix SPARK_JAVA_OPT_.\n+      .remove(org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+      .getAll\n+      .map {\n+        case (confKey, confValue) => s\"-D$confKey=$confValue\"\n+      } ++ driverJavaOptions.map(Utils.splitCommandString).getOrElse(Seq.empty)\n+    val driverJavaOptsEnvs: Seq[EnvVar] = resolvedDriverJavaOpts.zipWithIndex.map {\n+      case (option, index) => new EnvVarBuilder()\n+        .withName(s\"$ENV_JAVA_OPT_PREFIX$index\")\n+        .withValue(option)\n+        .build()\n+    }"
  }, {
    "author": {
      "login": "mridulm"
    },
    "body": "Thanks ! Not sure how I missed this one when I searched ... something messed up in Intellij",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-27T18:28:17Z",
    "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ClientArguments(\n+     mainAppResource: MainAppResource,\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.Buffer.empty[String]\n+\n+    args.sliding(2, 2).toList.collect {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainAppResource.isDefined,\n+      \"Main app resource must be defined by either --primary-py-file or --primary-java-resource.\")\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource.get,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }\n+\n+    val resolvedDriverJavaOpts = currentDriverSpec\n+      .driverSparkConf\n+      // Remove this as the options are instead extracted and set individually below using\n+      // environment variables with prefix SPARK_JAVA_OPT_.\n+      .remove(org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+      .getAll\n+      .map {\n+        case (confKey, confValue) => s\"-D$confKey=$confValue\"\n+      } ++ driverJavaOptions.map(Utils.splitCommandString).getOrElse(Seq.empty)\n+    val driverJavaOptsEnvs: Seq[EnvVar] = resolvedDriverJavaOpts.zipWithIndex.map {\n+      case (option, index) => new EnvVarBuilder()\n+        .withName(s\"$ENV_JAVA_OPT_PREFIX$index\")\n+        .withValue(option)\n+        .build()\n+    }"
  }],
  "prId": 19717
}, {
  "comments": [{
    "author": {
      "login": "mridulm"
    },
    "body": "mainAppResource need not be valid here",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-27T03:02:45Z",
    "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ClientArguments(\n+     mainAppResource: MainAppResource,\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.Buffer.empty[String]\n+\n+    args.sliding(2, 2).toList.collect {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainAppResource.isDefined,\n+      \"Main app resource must be defined by either --primary-py-file or --primary-java-resource.\")\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Wondering why?",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-28T00:45:51Z",
    "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ClientArguments(\n+     mainAppResource: MainAppResource,\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.Buffer.empty[String]\n+\n+    args.sliding(2, 2).toList.collect {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainAppResource.isDefined,\n+      \"Main app resource must be defined by either --primary-py-file or --primary-java-resource.\")\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")"
  }, {
    "author": {
      "login": "mridulm"
    },
    "body": "For case of spark examples, spark thrift server, etc - there is no main app resource - the bits are bundled as part of spark itself.\r\nYou can take a look at how yarn or mesos handles this case : I would assume something similar should suffice (look for usage of SparkLauncher.NO_RESOURCE)",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-28T01:49:54Z",
    "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ClientArguments(\n+     mainAppResource: MainAppResource,\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.Buffer.empty[String]\n+\n+    args.sliding(2, 2).toList.collect {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainAppResource.isDefined,\n+      \"Main app resource must be defined by either --primary-py-file or --primary-java-resource.\")\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Got it, removed the check.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-28T16:34:08Z",
    "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ClientArguments(\n+     mainAppResource: MainAppResource,\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.Buffer.empty[String]\n+\n+    args.sliding(2, 2).toList.collect {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainAppResource.isDefined,\n+      \"Main app resource must be defined by either --primary-py-file or --primary-java-resource.\")\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")"
  }],
  "prId": 19717
}, {
  "comments": [{
    "author": {
      "login": "jiangxb1987"
    },
    "body": "nit: add comments to describe the usage of this class, and the meaning of each params.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-27T14:54:39Z",
    "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ClientArguments("
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Done.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-28T00:46:30Z",
    "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ClientArguments("
  }],
  "prId": 19717
}, {
  "comments": [{
    "author": {
      "login": "jiangxb1987"
    },
    "body": "Have we already support `--primary-py-file` here?",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-27T14:56:20Z",
    "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ClientArguments(\n+     mainAppResource: MainAppResource,\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.Buffer.empty[String]\n+\n+    args.sliding(2, 2).toList.collect {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainAppResource.isDefined,\n+      \"Main app resource must be defined by either --primary-py-file or --primary-java-resource.\")"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "No, not yet. Removed.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-28T00:46:42Z",
    "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ClientArguments(\n+     mainAppResource: MainAppResource,\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.Buffer.empty[String]\n+\n+    args.sliding(2, 2).toList.collect {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainAppResource.isDefined,\n+      \"Main app resource must be defined by either --primary-py-file or --primary-java-resource.\")"
  }],
  "prId": 19717
}, {
  "comments": [{
    "author": {
      "login": "jiangxb1987"
    },
    "body": "nit: add comments to describe the usage of this class, and the meaning of each params.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-27T14:57:46Z",
    "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ClientArguments(\n+     mainAppResource: MainAppResource,\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.Buffer.empty[String]\n+\n+    args.sliding(2, 2).toList.collect {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainAppResource.isDefined,\n+      \"Main app resource must be defined by either --primary-py-file or --primary-java-resource.\")\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource.get,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+private[spark] class Client("
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Done.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-28T00:46:46Z",
    "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ClientArguments(\n+     mainAppResource: MainAppResource,\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.Buffer.empty[String]\n+\n+    args.sliding(2, 2).toList.collect {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainAppResource.isDefined,\n+      \"Main app resource must be defined by either --primary-py-file or --primary-java-resource.\")\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource.get,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+private[spark] class Client("
  }],
  "prId": 19717
}, {
  "comments": [{
    "author": {
      "login": "jiangxb1987"
    },
    "body": "Is this `resolvedDriverJavaOpts` or `resolvedDriverOpts`?",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-27T15:02:00Z",
    "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ClientArguments(\n+     mainAppResource: MainAppResource,\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.Buffer.empty[String]\n+\n+    args.sliding(2, 2).toList.collect {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainAppResource.isDefined,\n+      \"Main app resource must be defined by either --primary-py-file or --primary-java-resource.\")\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource.get,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }\n+\n+    val resolvedDriverJavaOpts = currentDriverSpec"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "`resolvedDriverJavaOpts`.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-28T00:46:49Z",
    "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+private[spark] case class ClientArguments(\n+     mainAppResource: MainAppResource,\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.Buffer.empty[String]\n+\n+    args.sliding(2, 2).toList.collect {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainAppResource.isDefined,\n+      \"Main app resource must be defined by either --primary-py-file or --primary-java-resource.\")\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource.get,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }\n+\n+    val resolvedDriverJavaOpts = currentDriverSpec"
  }],
  "prId": 19717
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "nit: `foreach` instead of `collect`?",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-28T06:21:27Z",
    "diffHunk": "@@ -0,0 +1,236 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: MainAppResource,\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.Buffer.empty[String]\n+\n+    args.sliding(2, 2).toList.collect {"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Done.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-28T16:34:40Z",
    "diffHunk": "@@ -0,0 +1,236 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: MainAppResource,\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.Buffer.empty[String]\n+\n+    args.sliding(2, 2).toList.collect {"
  }],
  "prId": 19717
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "nit: can we use `ArrayBuffer` explicitly?",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-28T06:22:37Z",
    "diffHunk": "@@ -0,0 +1,236 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: MainAppResource,\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.Buffer.empty[String]"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Done.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-28T16:34:38Z",
    "diffHunk": "@@ -0,0 +1,236 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: MainAppResource,\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.Buffer.empty[String]"
  }],
  "prId": 19717
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "Maybe we can use `foldLeft` here?\r\nI'm not sure which is preferable for other reviewers, though.\r\n\r\n```scala\r\nval currentDriverSpec =\r\n  submissionSteps.foldLeft(KubernetesDriverSpec.initialSpec(submissionSparkConf)) {\r\n    (spec, step) => step.configureDriver(spec)\r\n  }\r\n```",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-28T06:34:45Z",
    "diffHunk": "@@ -0,0 +1,236 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: MainAppResource,\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.Buffer.empty[String]\n+\n+    args.sliding(2, 2).toList.collect {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainAppResource.isDefined,\n+      \"Main app resource must be defined by --primary-java-resource.\")\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource.get,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+/**\n+ * Submits a Spark application to run on Kubernetes by creating the driver pod and starting a\n+ * watcher that monitors and logs the application status. Waits for the application to terminate if\n+ * spark.kubernetes.submission.waitAppCompletion is true.\n+ *\n+ * @param submissionSteps steps that collectively configure the driver\n+ * @param submissionSparkConf the submission client Spark configuration\n+ * @param kubernetesClient the client to talk to the Kubernetes API server\n+ * @param waitForAppCompletion a flag indicating whether the client should wait for the application\n+ *                             to complete\n+ * @param appName the application name\n+ * @param loggingPodStatusWatcher a watcher that monitors and logs the application status\n+ */\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "I feel that the current syntax is more readable and intuitive.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-28T16:05:01Z",
    "diffHunk": "@@ -0,0 +1,236 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: MainAppResource,\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.Buffer.empty[String]\n+\n+    args.sliding(2, 2).toList.collect {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainAppResource.isDefined,\n+      \"Main app resource must be defined by --primary-java-resource.\")\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource.get,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+/**\n+ * Submits a Spark application to run on Kubernetes by creating the driver pod and starting a\n+ * watcher that monitors and logs the application status. Waits for the application to terminate if\n+ * spark.kubernetes.submission.waitAppCompletion is true.\n+ *\n+ * @param submissionSteps steps that collectively configure the driver\n+ * @param submissionSparkConf the submission client Spark configuration\n+ * @param kubernetesClient the client to talk to the Kubernetes API server\n+ * @param waitForAppCompletion a flag indicating whether the client should wait for the application\n+ *                             to complete\n+ * @param appName the application name\n+ * @param loggingPodStatusWatcher a watcher that monitors and logs the application status\n+ */\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "Other reviews have stated that `foldLeft` is to be avoided.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-28T18:34:15Z",
    "diffHunk": "@@ -0,0 +1,236 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: MainAppResource,\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.Buffer.empty[String]\n+\n+    args.sliding(2, 2).toList.collect {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainAppResource.isDefined,\n+      \"Main app resource must be defined by --primary-java-resource.\")\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource.get,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+/**\n+ * Submits a Spark application to run on Kubernetes by creating the driver pod and starting a\n+ * watcher that monitors and logs the application status. Waits for the application to terminate if\n+ * spark.kubernetes.submission.waitAppCompletion is true.\n+ *\n+ * @param submissionSteps steps that collectively configure the driver\n+ * @param submissionSparkConf the submission client Spark configuration\n+ * @param kubernetesClient the client to talk to the Kubernetes API server\n+ * @param waitForAppCompletion a flag indicating whether the client should wait for the application\n+ *                             to complete\n+ * @param appName the application name\n+ * @param loggingPodStatusWatcher a watcher that monitors and logs the application status\n+ */\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }"
  }, {
    "author": {
      "login": "ueshin"
    },
    "body": "Oh, I see, thanks.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-29T01:30:23Z",
    "diffHunk": "@@ -0,0 +1,236 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: MainAppResource,\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.Buffer.empty[String]\n+\n+    args.sliding(2, 2).toList.collect {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainAppResource.isDefined,\n+      \"Main app resource must be defined by --primary-java-resource.\")\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource.get,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+/**\n+ * Submits a Spark application to run on Kubernetes by creating the driver pod and starting a\n+ * watcher that monitors and logs the application status. Waits for the application to terminate if\n+ * spark.kubernetes.submission.waitAppCompletion is true.\n+ *\n+ * @param submissionSteps steps that collectively configure the driver\n+ * @param submissionSparkConf the submission client Spark configuration\n+ * @param kubernetesClient the client to talk to the Kubernetes API server\n+ * @param waitForAppCompletion a flag indicating whether the client should wait for the application\n+ *                             to complete\n+ * @param appName the application name\n+ * @param loggingPodStatusWatcher a watcher that monitors and logs the application status\n+ */\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }"
  }],
  "prId": 19717
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "nit: `new EnvVarBuilder()` in the next line.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-28T06:42:37Z",
    "diffHunk": "@@ -0,0 +1,236 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: MainAppResource,\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.Buffer.empty[String]\n+\n+    args.sliding(2, 2).toList.collect {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainAppResource.isDefined,\n+      \"Main app resource must be defined by --primary-java-resource.\")\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource.get,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+/**\n+ * Submits a Spark application to run on Kubernetes by creating the driver pod and starting a\n+ * watcher that monitors and logs the application status. Waits for the application to terminate if\n+ * spark.kubernetes.submission.waitAppCompletion is true.\n+ *\n+ * @param submissionSteps steps that collectively configure the driver\n+ * @param submissionSparkConf the submission client Spark configuration\n+ * @param kubernetesClient the client to talk to the Kubernetes API server\n+ * @param waitForAppCompletion a flag indicating whether the client should wait for the application\n+ *                             to complete\n+ * @param appName the application name\n+ * @param loggingPodStatusWatcher a watcher that monitors and logs the application status\n+ */\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }\n+\n+    val resolvedDriverJavaOpts = currentDriverSpec\n+      .driverSparkConf\n+      // Remove this as the options are instead extracted and set individually below using\n+      // environment variables with prefix SPARK_JAVA_OPT_.\n+      .remove(org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+      .getAll\n+      .map {\n+        case (confKey, confValue) => s\"-D$confKey=$confValue\"\n+      } ++ driverJavaOptions.map(Utils.splitCommandString).getOrElse(Seq.empty)\n+    val driverJavaOptsEnvs: Seq[EnvVar] = resolvedDriverJavaOpts.zipWithIndex.map {\n+      case (option, index) => new EnvVarBuilder()"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Done.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-28T16:34:45Z",
    "diffHunk": "@@ -0,0 +1,236 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: MainAppResource,\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.Buffer.empty[String]\n+\n+    args.sliding(2, 2).toList.collect {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainAppResource.isDefined,\n+      \"Main app resource must be defined by --primary-java-resource.\")\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource.get,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+/**\n+ * Submits a Spark application to run on Kubernetes by creating the driver pod and starting a\n+ * watcher that monitors and logs the application status. Waits for the application to terminate if\n+ * spark.kubernetes.submission.waitAppCompletion is true.\n+ *\n+ * @param submissionSteps steps that collectively configure the driver\n+ * @param submissionSparkConf the submission client Spark configuration\n+ * @param kubernetesClient the client to talk to the Kubernetes API server\n+ * @param waitForAppCompletion a flag indicating whether the client should wait for the application\n+ *                             to complete\n+ * @param appName the application name\n+ * @param loggingPodStatusWatcher a watcher that monitors and logs the application status\n+ */\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }\n+\n+    val resolvedDriverJavaOpts = currentDriverSpec\n+      .driverSparkConf\n+      // Remove this as the options are instead extracted and set individually below using\n+      // environment variables with prefix SPARK_JAVA_OPT_.\n+      .remove(org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+      .getAll\n+      .map {\n+        case (confKey, confValue) => s\"-D$confKey=$confValue\"\n+      } ++ driverJavaOptions.map(Utils.splitCommandString).getOrElse(Seq.empty)\n+    val driverJavaOptsEnvs: Seq[EnvVar] = resolvedDriverJavaOpts.zipWithIndex.map {\n+      case (option, index) => new EnvVarBuilder()"
  }],
  "prId": 19717
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "nit: indent, or we can put `kubernetesClient =>` at the end of the previous line.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-28T06:59:16Z",
    "diffHunk": "@@ -0,0 +1,236 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: MainAppResource,\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.Buffer.empty[String]\n+\n+    args.sliding(2, 2).toList.collect {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainAppResource.isDefined,\n+      \"Main app resource must be defined by --primary-java-resource.\")\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource.get,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+/**\n+ * Submits a Spark application to run on Kubernetes by creating the driver pod and starting a\n+ * watcher that monitors and logs the application status. Waits for the application to terminate if\n+ * spark.kubernetes.submission.waitAppCompletion is true.\n+ *\n+ * @param submissionSteps steps that collectively configure the driver\n+ * @param submissionSparkConf the submission client Spark configuration\n+ * @param kubernetesClient the client to talk to the Kubernetes API server\n+ * @param waitForAppCompletion a flag indicating whether the client should wait for the application\n+ *                             to complete\n+ * @param appName the application name\n+ * @param loggingPodStatusWatcher a watcher that monitors and logs the application status\n+ */\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }\n+\n+    val resolvedDriverJavaOpts = currentDriverSpec\n+      .driverSparkConf\n+      // Remove this as the options are instead extracted and set individually below using\n+      // environment variables with prefix SPARK_JAVA_OPT_.\n+      .remove(org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+      .getAll\n+      .map {\n+        case (confKey, confValue) => s\"-D$confKey=$confValue\"\n+      } ++ driverJavaOptions.map(Utils.splitCommandString).getOrElse(Seq.empty)\n+    val driverJavaOptsEnvs: Seq[EnvVar] = resolvedDriverJavaOpts.zipWithIndex.map {\n+      case (option, index) => new EnvVarBuilder()\n+        .withName(s\"$ENV_JAVA_OPT_PREFIX$index\")\n+        .withValue(option)\n+        .build()\n+    }\n+\n+    val resolvedDriverContainer = new ContainerBuilder(currentDriverSpec.driverContainer)\n+      .addAllToEnv(driverJavaOptsEnvs.asJava)\n+      .build()\n+    val resolvedDriverPod = new PodBuilder(currentDriverSpec.driverPod)\n+      .editSpec()\n+        .addToContainers(resolvedDriverContainer)\n+        .endSpec()\n+      .build()\n+\n+    Utils.tryWithResource(\n+        kubernetesClient\n+          .pods()\n+          .withName(resolvedDriverPod.getMetadata.getName)\n+          .watch(loggingPodStatusWatcher)) { _ =>\n+      val createdDriverPod = kubernetesClient.pods().create(resolvedDriverPod)\n+      try {\n+        if (currentDriverSpec.otherKubernetesResources.nonEmpty) {\n+          val otherKubernetesResources = currentDriverSpec.otherKubernetesResources\n+          addDriverOwnerReference(createdDriverPod, otherKubernetesResources)\n+          kubernetesClient.resourceList(otherKubernetesResources: _*).createOrReplace()\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          kubernetesClient.pods().delete(createdDriverPod)\n+          throw e\n+      }\n+\n+      if (waitForAppCompletion) {\n+        logInfo(s\"Waiting for application $appName to finish...\")\n+        loggingPodStatusWatcher.awaitCompletion()\n+        logInfo(s\"Application $appName finished.\")\n+      } else {\n+        logInfo(s\"Deployed Spark application $appName into Kubernetes.\")\n+      }\n+    }\n+  }\n+\n+  // Add a OwnerReference to the given resources making the driver pod an owner of them so when\n+  // the driver pod is deleted, the resources are garbage collected.\n+  private def addDriverOwnerReference(driverPod: Pod, resources: Seq[HasMetadata]): Unit = {\n+    val driverPodOwnerReference = new OwnerReferenceBuilder()\n+      .withName(driverPod.getMetadata.getName)\n+      .withApiVersion(driverPod.getApiVersion)\n+      .withUid(driverPod.getMetadata.getUid)\n+      .withKind(driverPod.getKind)\n+      .withController(true)\n+      .build()\n+    resources.foreach { resource =>\n+      val originalMetadata = resource.getMetadata\n+      originalMetadata.setOwnerReferences(Collections.singletonList(driverPodOwnerReference))\n+    }\n+  }\n+}\n+\n+/**\n+ * Main class and entry point of application submission in KUBERNETES mode.\n+ */\n+private[spark] object Client extends SparkApplication {\n+\n+  override def start(args: Array[String], conf: SparkConf): Unit = {\n+    val parsedArguments = ClientArguments.fromCommandLineArgs(args)\n+    run(parsedArguments, conf)\n+  }\n+\n+  private def run(clientArguments: ClientArguments, sparkConf: SparkConf): Unit = {\n+    val namespace = sparkConf.get(KUBERNETES_NAMESPACE)\n+    val kubernetesAppId = s\"spark-${UUID.randomUUID().toString.replaceAll(\"-\", \"\")}\"\n+    val launchTime = System.currentTimeMillis()\n+    val waitForAppCompletion = sparkConf.get(WAIT_FOR_APP_COMPLETION)\n+    val appName = sparkConf.getOption(\"spark.app.name\").getOrElse(\"spark\")\n+    val master = getK8sMasterUrl(sparkConf.get(\"spark.master\"))\n+    val loggingInterval = Option(sparkConf.get(REPORT_INTERVAL)).filter(_ => waitForAppCompletion)\n+\n+    val loggingPodStatusWatcher = new LoggingPodStatusWatcherImpl(\n+      kubernetesAppId, loggingInterval)\n+\n+    val configurationStepsOrchestrator = new DriverConfigurationStepsOrchestrator(\n+      namespace,\n+      kubernetesAppId,\n+      launchTime,\n+      clientArguments.mainAppResource,\n+      appName,\n+      clientArguments.mainClass,\n+      clientArguments.driverArgs,\n+      sparkConf)\n+\n+    Utils.tryWithResource(KubernetesClientFactory.createKubernetesClient(\n+      master,\n+      Some(namespace),\n+      KUBERNETES_AUTH_SUBMISSION_CONF_PREFIX,\n+      sparkConf,\n+      None,\n+      None)) {\n+      kubernetesClient =>"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Done.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-28T16:34:48Z",
    "diffHunk": "@@ -0,0 +1,236 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: MainAppResource,\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.Buffer.empty[String]\n+\n+    args.sliding(2, 2).toList.collect {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainAppResource.isDefined,\n+      \"Main app resource must be defined by --primary-java-resource.\")\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource.get,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+/**\n+ * Submits a Spark application to run on Kubernetes by creating the driver pod and starting a\n+ * watcher that monitors and logs the application status. Waits for the application to terminate if\n+ * spark.kubernetes.submission.waitAppCompletion is true.\n+ *\n+ * @param submissionSteps steps that collectively configure the driver\n+ * @param submissionSparkConf the submission client Spark configuration\n+ * @param kubernetesClient the client to talk to the Kubernetes API server\n+ * @param waitForAppCompletion a flag indicating whether the client should wait for the application\n+ *                             to complete\n+ * @param appName the application name\n+ * @param loggingPodStatusWatcher a watcher that monitors and logs the application status\n+ */\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }\n+\n+    val resolvedDriverJavaOpts = currentDriverSpec\n+      .driverSparkConf\n+      // Remove this as the options are instead extracted and set individually below using\n+      // environment variables with prefix SPARK_JAVA_OPT_.\n+      .remove(org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+      .getAll\n+      .map {\n+        case (confKey, confValue) => s\"-D$confKey=$confValue\"\n+      } ++ driverJavaOptions.map(Utils.splitCommandString).getOrElse(Seq.empty)\n+    val driverJavaOptsEnvs: Seq[EnvVar] = resolvedDriverJavaOpts.zipWithIndex.map {\n+      case (option, index) => new EnvVarBuilder()\n+        .withName(s\"$ENV_JAVA_OPT_PREFIX$index\")\n+        .withValue(option)\n+        .build()\n+    }\n+\n+    val resolvedDriverContainer = new ContainerBuilder(currentDriverSpec.driverContainer)\n+      .addAllToEnv(driverJavaOptsEnvs.asJava)\n+      .build()\n+    val resolvedDriverPod = new PodBuilder(currentDriverSpec.driverPod)\n+      .editSpec()\n+        .addToContainers(resolvedDriverContainer)\n+        .endSpec()\n+      .build()\n+\n+    Utils.tryWithResource(\n+        kubernetesClient\n+          .pods()\n+          .withName(resolvedDriverPod.getMetadata.getName)\n+          .watch(loggingPodStatusWatcher)) { _ =>\n+      val createdDriverPod = kubernetesClient.pods().create(resolvedDriverPod)\n+      try {\n+        if (currentDriverSpec.otherKubernetesResources.nonEmpty) {\n+          val otherKubernetesResources = currentDriverSpec.otherKubernetesResources\n+          addDriverOwnerReference(createdDriverPod, otherKubernetesResources)\n+          kubernetesClient.resourceList(otherKubernetesResources: _*).createOrReplace()\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          kubernetesClient.pods().delete(createdDriverPod)\n+          throw e\n+      }\n+\n+      if (waitForAppCompletion) {\n+        logInfo(s\"Waiting for application $appName to finish...\")\n+        loggingPodStatusWatcher.awaitCompletion()\n+        logInfo(s\"Application $appName finished.\")\n+      } else {\n+        logInfo(s\"Deployed Spark application $appName into Kubernetes.\")\n+      }\n+    }\n+  }\n+\n+  // Add a OwnerReference to the given resources making the driver pod an owner of them so when\n+  // the driver pod is deleted, the resources are garbage collected.\n+  private def addDriverOwnerReference(driverPod: Pod, resources: Seq[HasMetadata]): Unit = {\n+    val driverPodOwnerReference = new OwnerReferenceBuilder()\n+      .withName(driverPod.getMetadata.getName)\n+      .withApiVersion(driverPod.getApiVersion)\n+      .withUid(driverPod.getMetadata.getUid)\n+      .withKind(driverPod.getKind)\n+      .withController(true)\n+      .build()\n+    resources.foreach { resource =>\n+      val originalMetadata = resource.getMetadata\n+      originalMetadata.setOwnerReferences(Collections.singletonList(driverPodOwnerReference))\n+    }\n+  }\n+}\n+\n+/**\n+ * Main class and entry point of application submission in KUBERNETES mode.\n+ */\n+private[spark] object Client extends SparkApplication {\n+\n+  override def start(args: Array[String], conf: SparkConf): Unit = {\n+    val parsedArguments = ClientArguments.fromCommandLineArgs(args)\n+    run(parsedArguments, conf)\n+  }\n+\n+  private def run(clientArguments: ClientArguments, sparkConf: SparkConf): Unit = {\n+    val namespace = sparkConf.get(KUBERNETES_NAMESPACE)\n+    val kubernetesAppId = s\"spark-${UUID.randomUUID().toString.replaceAll(\"-\", \"\")}\"\n+    val launchTime = System.currentTimeMillis()\n+    val waitForAppCompletion = sparkConf.get(WAIT_FOR_APP_COMPLETION)\n+    val appName = sparkConf.getOption(\"spark.app.name\").getOrElse(\"spark\")\n+    val master = getK8sMasterUrl(sparkConf.get(\"spark.master\"))\n+    val loggingInterval = Option(sparkConf.get(REPORT_INTERVAL)).filter(_ => waitForAppCompletion)\n+\n+    val loggingPodStatusWatcher = new LoggingPodStatusWatcherImpl(\n+      kubernetesAppId, loggingInterval)\n+\n+    val configurationStepsOrchestrator = new DriverConfigurationStepsOrchestrator(\n+      namespace,\n+      kubernetesAppId,\n+      launchTime,\n+      clientArguments.mainAppResource,\n+      appName,\n+      clientArguments.mainClass,\n+      clientArguments.driverArgs,\n+      sparkConf)\n+\n+    Utils.tryWithResource(KubernetesClientFactory.createKubernetesClient(\n+      master,\n+      Some(namespace),\n+      KUBERNETES_AUTH_SUBMISSION_CONF_PREFIX,\n+      sparkConf,\n+      None,\n+      None)) {\n+      kubernetesClient =>"
  }],
  "prId": 19717
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "Don't we need to check if `mainAppResource` is defined or not before here? Otherwise some exception will be thrown if it is not defined.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-30T09:27:30Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: MainAppResource,\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.ArrayBuffer.empty[String]\n+\n+    args.sliding(2, 2).toList.foreach {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource.get,"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "As @mridulm pointed above, `mainAppResource` may be empty in cases of spark examples, Thrift server, etc.  `--primary-java-resource` is set only if `mainAppResource != SparkLauncher.NO_RESOURCE` in `SparkSubmit`. So it could be empty here. I changed `ClientArguments` to take an `Option[MainAppResource]` instead.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-30T17:20:53Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: MainAppResource,\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.ArrayBuffer.empty[String]\n+\n+    args.sliding(2, 2).toList.foreach {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource.get,"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Also added the missing step `DependencyResolutionStep` that is essential for supporting `local://` dependencies (jars and files).",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-30T18:18:12Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: MainAppResource,\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.ArrayBuffer.empty[String]\n+\n+    args.sliding(2, 2).toList.foreach {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource.get,"
  }],
  "prId": 19717
}, {
  "comments": [{
    "author": {
      "login": "felixcheung"
    },
    "body": "just want to check: NonFatal but delete & rethrow?",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-02T18:33:52Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkKubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource if any\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: Option[MainAppResource],\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.ArrayBuffer.empty[String]\n+\n+    args.sliding(2, 2).toList.foreach {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+/**\n+ * Submits a Spark application to run on Kubernetes by creating the driver pod and starting a\n+ * watcher that monitors and logs the application status. Waits for the application to terminate if\n+ * spark.kubernetes.submission.waitAppCompletion is true.\n+ *\n+ * @param submissionSteps steps that collectively configure the driver\n+ * @param submissionSparkConf the submission client Spark configuration\n+ * @param kubernetesClient the client to talk to the Kubernetes API server\n+ * @param waitForAppCompletion a flag indicating whether the client should wait for the application\n+ *                             to complete\n+ * @param appName the application name\n+ * @param loggingPodStatusWatcher a watcher that monitors and logs the application status\n+ */\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }\n+\n+    val resolvedDriverJavaOpts = currentDriverSpec\n+      .driverSparkConf\n+      // Remove this as the options are instead extracted and set individually below using\n+      // environment variables with prefix SPARK_JAVA_OPT_.\n+      .remove(org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+      .getAll\n+      .map {\n+        case (confKey, confValue) => s\"-D$confKey=$confValue\"\n+      } ++ driverJavaOptions.map(Utils.splitCommandString).getOrElse(Seq.empty)\n+    val driverJavaOptsEnvs: Seq[EnvVar] = resolvedDriverJavaOpts.zipWithIndex.map {\n+      case (option, index) =>\n+        new EnvVarBuilder()\n+          .withName(s\"$ENV_JAVA_OPT_PREFIX$index\")\n+          .withValue(option)\n+          .build()\n+    }\n+\n+    val resolvedDriverContainer = new ContainerBuilder(currentDriverSpec.driverContainer)\n+      .addAllToEnv(driverJavaOptsEnvs.asJava)\n+      .build()\n+    val resolvedDriverPod = new PodBuilder(currentDriverSpec.driverPod)\n+      .editSpec()\n+        .addToContainers(resolvedDriverContainer)\n+        .endSpec()\n+      .build()\n+\n+    Utils.tryWithResource(\n+        kubernetesClient\n+          .pods()\n+          .withName(resolvedDriverPod.getMetadata.getName)\n+          .watch(loggingPodStatusWatcher)) { _ =>\n+      val createdDriverPod = kubernetesClient.pods().create(resolvedDriverPod)\n+      try {\n+        if (currentDriverSpec.otherKubernetesResources.nonEmpty) {\n+          val otherKubernetesResources = currentDriverSpec.otherKubernetesResources\n+          addDriverOwnerReference(createdDriverPod, otherKubernetesResources)\n+          kubernetesClient.resourceList(otherKubernetesResources: _*).createOrReplace()\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          kubernetesClient.pods().delete(createdDriverPod)\n+          throw e"
  }, {
    "author": {
      "login": "felixcheung"
    },
    "body": "also should it delete if it is fatal?",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-02T18:36:27Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkKubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource if any\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: Option[MainAppResource],\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.ArrayBuffer.empty[String]\n+\n+    args.sliding(2, 2).toList.foreach {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+/**\n+ * Submits a Spark application to run on Kubernetes by creating the driver pod and starting a\n+ * watcher that monitors and logs the application status. Waits for the application to terminate if\n+ * spark.kubernetes.submission.waitAppCompletion is true.\n+ *\n+ * @param submissionSteps steps that collectively configure the driver\n+ * @param submissionSparkConf the submission client Spark configuration\n+ * @param kubernetesClient the client to talk to the Kubernetes API server\n+ * @param waitForAppCompletion a flag indicating whether the client should wait for the application\n+ *                             to complete\n+ * @param appName the application name\n+ * @param loggingPodStatusWatcher a watcher that monitors and logs the application status\n+ */\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }\n+\n+    val resolvedDriverJavaOpts = currentDriverSpec\n+      .driverSparkConf\n+      // Remove this as the options are instead extracted and set individually below using\n+      // environment variables with prefix SPARK_JAVA_OPT_.\n+      .remove(org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+      .getAll\n+      .map {\n+        case (confKey, confValue) => s\"-D$confKey=$confValue\"\n+      } ++ driverJavaOptions.map(Utils.splitCommandString).getOrElse(Seq.empty)\n+    val driverJavaOptsEnvs: Seq[EnvVar] = resolvedDriverJavaOpts.zipWithIndex.map {\n+      case (option, index) =>\n+        new EnvVarBuilder()\n+          .withName(s\"$ENV_JAVA_OPT_PREFIX$index\")\n+          .withValue(option)\n+          .build()\n+    }\n+\n+    val resolvedDriverContainer = new ContainerBuilder(currentDriverSpec.driverContainer)\n+      .addAllToEnv(driverJavaOptsEnvs.asJava)\n+      .build()\n+    val resolvedDriverPod = new PodBuilder(currentDriverSpec.driverPod)\n+      .editSpec()\n+        .addToContainers(resolvedDriverContainer)\n+        .endSpec()\n+      .build()\n+\n+    Utils.tryWithResource(\n+        kubernetesClient\n+          .pods()\n+          .withName(resolvedDriverPod.getMetadata.getName)\n+          .watch(loggingPodStatusWatcher)) { _ =>\n+      val createdDriverPod = kubernetesClient.pods().create(resolvedDriverPod)\n+      try {\n+        if (currentDriverSpec.otherKubernetesResources.nonEmpty) {\n+          val otherKubernetesResources = currentDriverSpec.otherKubernetesResources\n+          addDriverOwnerReference(createdDriverPod, otherKubernetesResources)\n+          kubernetesClient.resourceList(otherKubernetesResources: _*).createOrReplace()\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          kubernetesClient.pods().delete(createdDriverPod)\n+          throw e"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Yes, good catch. It should delete the pod regardlessly.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-04T06:24:36Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkKubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource if any\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: Option[MainAppResource],\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.ArrayBuffer.empty[String]\n+\n+    args.sliding(2, 2).toList.foreach {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+/**\n+ * Submits a Spark application to run on Kubernetes by creating the driver pod and starting a\n+ * watcher that monitors and logs the application status. Waits for the application to terminate if\n+ * spark.kubernetes.submission.waitAppCompletion is true.\n+ *\n+ * @param submissionSteps steps that collectively configure the driver\n+ * @param submissionSparkConf the submission client Spark configuration\n+ * @param kubernetesClient the client to talk to the Kubernetes API server\n+ * @param waitForAppCompletion a flag indicating whether the client should wait for the application\n+ *                             to complete\n+ * @param appName the application name\n+ * @param loggingPodStatusWatcher a watcher that monitors and logs the application status\n+ */\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }\n+\n+    val resolvedDriverJavaOpts = currentDriverSpec\n+      .driverSparkConf\n+      // Remove this as the options are instead extracted and set individually below using\n+      // environment variables with prefix SPARK_JAVA_OPT_.\n+      .remove(org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+      .getAll\n+      .map {\n+        case (confKey, confValue) => s\"-D$confKey=$confValue\"\n+      } ++ driverJavaOptions.map(Utils.splitCommandString).getOrElse(Seq.empty)\n+    val driverJavaOptsEnvs: Seq[EnvVar] = resolvedDriverJavaOpts.zipWithIndex.map {\n+      case (option, index) =>\n+        new EnvVarBuilder()\n+          .withName(s\"$ENV_JAVA_OPT_PREFIX$index\")\n+          .withValue(option)\n+          .build()\n+    }\n+\n+    val resolvedDriverContainer = new ContainerBuilder(currentDriverSpec.driverContainer)\n+      .addAllToEnv(driverJavaOptsEnvs.asJava)\n+      .build()\n+    val resolvedDriverPod = new PodBuilder(currentDriverSpec.driverPod)\n+      .editSpec()\n+        .addToContainers(resolvedDriverContainer)\n+        .endSpec()\n+      .build()\n+\n+    Utils.tryWithResource(\n+        kubernetesClient\n+          .pods()\n+          .withName(resolvedDriverPod.getMetadata.getName)\n+          .watch(loggingPodStatusWatcher)) { _ =>\n+      val createdDriverPod = kubernetesClient.pods().create(resolvedDriverPod)\n+      try {\n+        if (currentDriverSpec.otherKubernetesResources.nonEmpty) {\n+          val otherKubernetesResources = currentDriverSpec.otherKubernetesResources\n+          addDriverOwnerReference(createdDriverPod, otherKubernetesResources)\n+          kubernetesClient.resourceList(otherKubernetesResources: _*).createOrReplace()\n+        }\n+      } catch {\n+        case NonFatal(e) =>\n+          kubernetesClient.pods().delete(createdDriverPod)\n+          throw e"
  }],
  "prId": 19717
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Kind of an odd way to write `if (waitForAppCompletion) Some(...) else None`.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-04T19:42:46Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkKubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource if any\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: Option[MainAppResource],\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.ArrayBuffer.empty[String]\n+\n+    args.sliding(2, 2).toList.foreach {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+/**\n+ * Submits a Spark application to run on Kubernetes by creating the driver pod and starting a\n+ * watcher that monitors and logs the application status. Waits for the application to terminate if\n+ * spark.kubernetes.submission.waitAppCompletion is true.\n+ *\n+ * @param submissionSteps steps that collectively configure the driver\n+ * @param submissionSparkConf the submission client Spark configuration\n+ * @param kubernetesClient the client to talk to the Kubernetes API server\n+ * @param waitForAppCompletion a flag indicating whether the client should wait for the application\n+ *                             to complete\n+ * @param appName the application name\n+ * @param loggingPodStatusWatcher a watcher that monitors and logs the application status\n+ */\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }\n+\n+    val resolvedDriverJavaOpts = currentDriverSpec\n+      .driverSparkConf\n+      // Remove this as the options are instead extracted and set individually below using\n+      // environment variables with prefix SPARK_JAVA_OPT_.\n+      .remove(org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+      .getAll\n+      .map {\n+        case (confKey, confValue) => s\"-D$confKey=$confValue\"\n+      } ++ driverJavaOptions.map(Utils.splitCommandString).getOrElse(Seq.empty)\n+    val driverJavaOptsEnvs: Seq[EnvVar] = resolvedDriverJavaOpts.zipWithIndex.map {\n+      case (option, index) =>\n+        new EnvVarBuilder()\n+          .withName(s\"$ENV_JAVA_OPT_PREFIX$index\")\n+          .withValue(option)\n+          .build()\n+    }\n+\n+    val resolvedDriverContainer = new ContainerBuilder(currentDriverSpec.driverContainer)\n+      .addAllToEnv(driverJavaOptsEnvs.asJava)\n+      .build()\n+    val resolvedDriverPod = new PodBuilder(currentDriverSpec.driverPod)\n+      .editSpec()\n+        .addToContainers(resolvedDriverContainer)\n+        .endSpec()\n+      .build()\n+\n+    Utils.tryWithResource(\n+        kubernetesClient\n+          .pods()\n+          .withName(resolvedDriverPod.getMetadata.getName)\n+          .watch(loggingPodStatusWatcher)) { _ =>\n+      val createdDriverPod = kubernetesClient.pods().create(resolvedDriverPod)\n+      try {\n+        if (currentDriverSpec.otherKubernetesResources.nonEmpty) {\n+          val otherKubernetesResources = currentDriverSpec.otherKubernetesResources\n+          addDriverOwnerReference(createdDriverPod, otherKubernetesResources)\n+          kubernetesClient.resourceList(otherKubernetesResources: _*).createOrReplace()\n+        }\n+      } catch {\n+        case e: Throwable =>\n+          kubernetesClient.pods().delete(createdDriverPod)\n+          throw e\n+      }\n+\n+      if (waitForAppCompletion) {\n+        logInfo(s\"Waiting for application $appName to finish...\")\n+        loggingPodStatusWatcher.awaitCompletion()\n+        logInfo(s\"Application $appName finished.\")\n+      } else {\n+        logInfo(s\"Deployed Spark application $appName into Kubernetes.\")\n+      }\n+    }\n+  }\n+\n+  // Add a OwnerReference to the given resources making the driver pod an owner of them so when\n+  // the driver pod is deleted, the resources are garbage collected.\n+  private def addDriverOwnerReference(driverPod: Pod, resources: Seq[HasMetadata]): Unit = {\n+    val driverPodOwnerReference = new OwnerReferenceBuilder()\n+      .withName(driverPod.getMetadata.getName)\n+      .withApiVersion(driverPod.getApiVersion)\n+      .withUid(driverPod.getMetadata.getUid)\n+      .withKind(driverPod.getKind)\n+      .withController(true)\n+      .build()\n+    resources.foreach { resource =>\n+      val originalMetadata = resource.getMetadata\n+      originalMetadata.setOwnerReferences(Collections.singletonList(driverPodOwnerReference))\n+    }\n+  }\n+}\n+\n+/**\n+ * Main class and entry point of application submission in KUBERNETES mode.\n+ */\n+private[spark] object Client extends SparkApplication {\n+\n+  override def start(args: Array[String], conf: SparkConf): Unit = {\n+    val parsedArguments = ClientArguments.fromCommandLineArgs(args)\n+    run(parsedArguments, conf)\n+  }\n+\n+  private def run(clientArguments: ClientArguments, sparkConf: SparkConf): Unit = {\n+    val namespace = sparkConf.get(KUBERNETES_NAMESPACE)\n+    val kubernetesAppId = s\"spark-${UUID.randomUUID().toString.replaceAll(\"-\", \"\")}\"\n+    val launchTime = System.currentTimeMillis()\n+    val waitForAppCompletion = sparkConf.get(WAIT_FOR_APP_COMPLETION)\n+    val appName = sparkConf.getOption(\"spark.app.name\").getOrElse(\"spark\")\n+    // The master URL has been checked for validity already in SparkSubmit.\n+    val master = sparkConf.get(\"spark.master\")\n+    val loggingInterval = Option(sparkConf.get(REPORT_INTERVAL)).filter(_ => waitForAppCompletion)"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Done.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-04T21:38:55Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkKubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource if any\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: Option[MainAppResource],\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.ArrayBuffer.empty[String]\n+\n+    args.sliding(2, 2).toList.foreach {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+/**\n+ * Submits a Spark application to run on Kubernetes by creating the driver pod and starting a\n+ * watcher that monitors and logs the application status. Waits for the application to terminate if\n+ * spark.kubernetes.submission.waitAppCompletion is true.\n+ *\n+ * @param submissionSteps steps that collectively configure the driver\n+ * @param submissionSparkConf the submission client Spark configuration\n+ * @param kubernetesClient the client to talk to the Kubernetes API server\n+ * @param waitForAppCompletion a flag indicating whether the client should wait for the application\n+ *                             to complete\n+ * @param appName the application name\n+ * @param loggingPodStatusWatcher a watcher that monitors and logs the application status\n+ */\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }\n+\n+    val resolvedDriverJavaOpts = currentDriverSpec\n+      .driverSparkConf\n+      // Remove this as the options are instead extracted and set individually below using\n+      // environment variables with prefix SPARK_JAVA_OPT_.\n+      .remove(org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+      .getAll\n+      .map {\n+        case (confKey, confValue) => s\"-D$confKey=$confValue\"\n+      } ++ driverJavaOptions.map(Utils.splitCommandString).getOrElse(Seq.empty)\n+    val driverJavaOptsEnvs: Seq[EnvVar] = resolvedDriverJavaOpts.zipWithIndex.map {\n+      case (option, index) =>\n+        new EnvVarBuilder()\n+          .withName(s\"$ENV_JAVA_OPT_PREFIX$index\")\n+          .withValue(option)\n+          .build()\n+    }\n+\n+    val resolvedDriverContainer = new ContainerBuilder(currentDriverSpec.driverContainer)\n+      .addAllToEnv(driverJavaOptsEnvs.asJava)\n+      .build()\n+    val resolvedDriverPod = new PodBuilder(currentDriverSpec.driverPod)\n+      .editSpec()\n+        .addToContainers(resolvedDriverContainer)\n+        .endSpec()\n+      .build()\n+\n+    Utils.tryWithResource(\n+        kubernetesClient\n+          .pods()\n+          .withName(resolvedDriverPod.getMetadata.getName)\n+          .watch(loggingPodStatusWatcher)) { _ =>\n+      val createdDriverPod = kubernetesClient.pods().create(resolvedDriverPod)\n+      try {\n+        if (currentDriverSpec.otherKubernetesResources.nonEmpty) {\n+          val otherKubernetesResources = currentDriverSpec.otherKubernetesResources\n+          addDriverOwnerReference(createdDriverPod, otherKubernetesResources)\n+          kubernetesClient.resourceList(otherKubernetesResources: _*).createOrReplace()\n+        }\n+      } catch {\n+        case e: Throwable =>\n+          kubernetesClient.pods().delete(createdDriverPod)\n+          throw e\n+      }\n+\n+      if (waitForAppCompletion) {\n+        logInfo(s\"Waiting for application $appName to finish...\")\n+        loggingPodStatusWatcher.awaitCompletion()\n+        logInfo(s\"Application $appName finished.\")\n+      } else {\n+        logInfo(s\"Deployed Spark application $appName into Kubernetes.\")\n+      }\n+    }\n+  }\n+\n+  // Add a OwnerReference to the given resources making the driver pod an owner of them so when\n+  // the driver pod is deleted, the resources are garbage collected.\n+  private def addDriverOwnerReference(driverPod: Pod, resources: Seq[HasMetadata]): Unit = {\n+    val driverPodOwnerReference = new OwnerReferenceBuilder()\n+      .withName(driverPod.getMetadata.getName)\n+      .withApiVersion(driverPod.getApiVersion)\n+      .withUid(driverPod.getMetadata.getUid)\n+      .withKind(driverPod.getKind)\n+      .withController(true)\n+      .build()\n+    resources.foreach { resource =>\n+      val originalMetadata = resource.getMetadata\n+      originalMetadata.setOwnerReferences(Collections.singletonList(driverPodOwnerReference))\n+    }\n+  }\n+}\n+\n+/**\n+ * Main class and entry point of application submission in KUBERNETES mode.\n+ */\n+private[spark] object Client extends SparkApplication {\n+\n+  override def start(args: Array[String], conf: SparkConf): Unit = {\n+    val parsedArguments = ClientArguments.fromCommandLineArgs(args)\n+    run(parsedArguments, conf)\n+  }\n+\n+  private def run(clientArguments: ClientArguments, sparkConf: SparkConf): Unit = {\n+    val namespace = sparkConf.get(KUBERNETES_NAMESPACE)\n+    val kubernetesAppId = s\"spark-${UUID.randomUUID().toString.replaceAll(\"-\", \"\")}\"\n+    val launchTime = System.currentTimeMillis()\n+    val waitForAppCompletion = sparkConf.get(WAIT_FOR_APP_COMPLETION)\n+    val appName = sparkConf.getOption(\"spark.app.name\").getOrElse(\"spark\")\n+    // The master URL has been checked for validity already in SparkSubmit.\n+    val master = sparkConf.get(\"spark.master\")\n+    val loggingInterval = Option(sparkConf.get(REPORT_INTERVAL)).filter(_ => waitForAppCompletion)"
  }],
  "prId": 19717
}, {
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "In the interest of reducing the complexity of this particular change, I think it's fine to remove the logging pod status watcher for now and introduce it in a follow up commit. We can make a simpler version here. For example, we can have a simpler watcher that checks the pod ends up in the running state before some timeout. We can also explicitly not support waiting for app completion.\r\n\r\nBasically I'm curious as to how simple we can make this to minimize the amount of new code we're introducing in the first push. The tradeoff of course is that we're expiicitly limiting functionality we introduce in 2.3.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-05T00:14:55Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkKubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource if any\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: Option[MainAppResource],\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.ArrayBuffer.empty[String]\n+\n+    args.sliding(2, 2).toList.foreach {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+/**\n+ * Submits a Spark application to run on Kubernetes by creating the driver pod and starting a\n+ * watcher that monitors and logs the application status. Waits for the application to terminate if\n+ * spark.kubernetes.submission.waitAppCompletion is true.\n+ *\n+ * @param submissionSteps steps that collectively configure the driver\n+ * @param submissionSparkConf the submission client Spark configuration\n+ * @param kubernetesClient the client to talk to the Kubernetes API server\n+ * @param waitForAppCompletion a flag indicating whether the client should wait for the application\n+ *                             to complete\n+ * @param appName the application name\n+ * @param loggingPodStatusWatcher a watcher that monitors and logs the application status\n+ */\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }\n+\n+    val resolvedDriverJavaOpts = currentDriverSpec\n+      .driverSparkConf\n+      // Remove this as the options are instead extracted and set individually below using\n+      // environment variables with prefix SPARK_JAVA_OPT_.\n+      .remove(org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+      .getAll\n+      .map {\n+        case (confKey, confValue) => s\"-D$confKey=$confValue\"\n+      } ++ driverJavaOptions.map(Utils.splitCommandString).getOrElse(Seq.empty)\n+    val driverJavaOptsEnvs: Seq[EnvVar] = resolvedDriverJavaOpts.zipWithIndex.map {\n+      case (option, index) =>\n+        new EnvVarBuilder()\n+          .withName(s\"$ENV_JAVA_OPT_PREFIX$index\")\n+          .withValue(option)\n+          .build()\n+    }\n+\n+    val resolvedDriverContainer = new ContainerBuilder(currentDriverSpec.driverContainer)\n+      .addAllToEnv(driverJavaOptsEnvs.asJava)\n+      .build()\n+    val resolvedDriverPod = new PodBuilder(currentDriverSpec.driverPod)\n+      .editSpec()\n+        .addToContainers(resolvedDriverContainer)\n+        .endSpec()\n+      .build()\n+\n+    Utils.tryWithResource(\n+        kubernetesClient\n+          .pods()\n+          .withName(resolvedDriverPod.getMetadata.getName)\n+          .watch(loggingPodStatusWatcher)) { _ =>"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "I personally don't think we can reduce much complexity as what are included in this PR and the first one form a MVP that support the most basic use cases: allowing users to submit Spark applications to a Kubernetes cluster with `local://` dependencies baked into images. Also given that this has already gone through several rounds of reviews by different reviewers, we are probably at a better position if we keep the current form. The next PR most probably will be for integration tests so we get more confident on the health of the MVP.  ",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-05T02:42:06Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkKubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource if any\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: Option[MainAppResource],\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.ArrayBuffer.empty[String]\n+\n+    args.sliding(2, 2).toList.foreach {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+/**\n+ * Submits a Spark application to run on Kubernetes by creating the driver pod and starting a\n+ * watcher that monitors and logs the application status. Waits for the application to terminate if\n+ * spark.kubernetes.submission.waitAppCompletion is true.\n+ *\n+ * @param submissionSteps steps that collectively configure the driver\n+ * @param submissionSparkConf the submission client Spark configuration\n+ * @param kubernetesClient the client to talk to the Kubernetes API server\n+ * @param waitForAppCompletion a flag indicating whether the client should wait for the application\n+ *                             to complete\n+ * @param appName the application name\n+ * @param loggingPodStatusWatcher a watcher that monitors and logs the application status\n+ */\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }\n+\n+    val resolvedDriverJavaOpts = currentDriverSpec\n+      .driverSparkConf\n+      // Remove this as the options are instead extracted and set individually below using\n+      // environment variables with prefix SPARK_JAVA_OPT_.\n+      .remove(org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+      .getAll\n+      .map {\n+        case (confKey, confValue) => s\"-D$confKey=$confValue\"\n+      } ++ driverJavaOptions.map(Utils.splitCommandString).getOrElse(Seq.empty)\n+    val driverJavaOptsEnvs: Seq[EnvVar] = resolvedDriverJavaOpts.zipWithIndex.map {\n+      case (option, index) =>\n+        new EnvVarBuilder()\n+          .withName(s\"$ENV_JAVA_OPT_PREFIX$index\")\n+          .withValue(option)\n+          .build()\n+    }\n+\n+    val resolvedDriverContainer = new ContainerBuilder(currentDriverSpec.driverContainer)\n+      .addAllToEnv(driverJavaOptsEnvs.asJava)\n+      .build()\n+    val resolvedDriverPod = new PodBuilder(currentDriverSpec.driverPod)\n+      .editSpec()\n+        .addToContainers(resolvedDriverContainer)\n+        .endSpec()\n+      .build()\n+\n+    Utils.tryWithResource(\n+        kubernetesClient\n+          .pods()\n+          .withName(resolvedDriverPod.getMetadata.getName)\n+          .watch(loggingPodStatusWatcher)) { _ =>"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "What are your thoughts of breaking up the change in how I suggest in https://github.com/apache/spark/pull/19717#pullrequestreview-81034043? We can have some PRs that create unusable code but the combination of all of said PRs create the final product.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-05T18:45:05Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkKubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource if any\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: Option[MainAppResource],\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.ArrayBuffer.empty[String]\n+\n+    args.sliding(2, 2).toList.foreach {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+/**\n+ * Submits a Spark application to run on Kubernetes by creating the driver pod and starting a\n+ * watcher that monitors and logs the application status. Waits for the application to terminate if\n+ * spark.kubernetes.submission.waitAppCompletion is true.\n+ *\n+ * @param submissionSteps steps that collectively configure the driver\n+ * @param submissionSparkConf the submission client Spark configuration\n+ * @param kubernetesClient the client to talk to the Kubernetes API server\n+ * @param waitForAppCompletion a flag indicating whether the client should wait for the application\n+ *                             to complete\n+ * @param appName the application name\n+ * @param loggingPodStatusWatcher a watcher that monitors and logs the application status\n+ */\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }\n+\n+    val resolvedDriverJavaOpts = currentDriverSpec\n+      .driverSparkConf\n+      // Remove this as the options are instead extracted and set individually below using\n+      // environment variables with prefix SPARK_JAVA_OPT_.\n+      .remove(org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+      .getAll\n+      .map {\n+        case (confKey, confValue) => s\"-D$confKey=$confValue\"\n+      } ++ driverJavaOptions.map(Utils.splitCommandString).getOrElse(Seq.empty)\n+    val driverJavaOptsEnvs: Seq[EnvVar] = resolvedDriverJavaOpts.zipWithIndex.map {\n+      case (option, index) =>\n+        new EnvVarBuilder()\n+          .withName(s\"$ENV_JAVA_OPT_PREFIX$index\")\n+          .withValue(option)\n+          .build()\n+    }\n+\n+    val resolvedDriverContainer = new ContainerBuilder(currentDriverSpec.driverContainer)\n+      .addAllToEnv(driverJavaOptsEnvs.asJava)\n+      .build()\n+    val resolvedDriverPod = new PodBuilder(currentDriverSpec.driverPod)\n+      .editSpec()\n+        .addToContainers(resolvedDriverContainer)\n+        .endSpec()\n+      .build()\n+\n+    Utils.tryWithResource(\n+        kubernetesClient\n+          .pods()\n+          .withName(resolvedDriverPod.getMetadata.getName)\n+          .watch(loggingPodStatusWatcher)) { _ =>"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Given the current state of this PR, I don't think it's far from being OK to merge. Breaking this up into multiple ones likely will not make the time to merge shorter, and possibly even make them harder to review because of lack of contexts in a single PR. IMO, I think we should keep this PR in its current form and dedicate the next one to integration tests. @felixcheung @foxish WDYT? ",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-05T18:53:02Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkKubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource if any\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: Option[MainAppResource],\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.ArrayBuffer.empty[String]\n+\n+    args.sliding(2, 2).toList.foreach {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+/**\n+ * Submits a Spark application to run on Kubernetes by creating the driver pod and starting a\n+ * watcher that monitors and logs the application status. Waits for the application to terminate if\n+ * spark.kubernetes.submission.waitAppCompletion is true.\n+ *\n+ * @param submissionSteps steps that collectively configure the driver\n+ * @param submissionSparkConf the submission client Spark configuration\n+ * @param kubernetesClient the client to talk to the Kubernetes API server\n+ * @param waitForAppCompletion a flag indicating whether the client should wait for the application\n+ *                             to complete\n+ * @param appName the application name\n+ * @param loggingPodStatusWatcher a watcher that monitors and logs the application status\n+ */\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }\n+\n+    val resolvedDriverJavaOpts = currentDriverSpec\n+      .driverSparkConf\n+      // Remove this as the options are instead extracted and set individually below using\n+      // environment variables with prefix SPARK_JAVA_OPT_.\n+      .remove(org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+      .getAll\n+      .map {\n+        case (confKey, confValue) => s\"-D$confKey=$confValue\"\n+      } ++ driverJavaOptions.map(Utils.splitCommandString).getOrElse(Seq.empty)\n+    val driverJavaOptsEnvs: Seq[EnvVar] = resolvedDriverJavaOpts.zipWithIndex.map {\n+      case (option, index) =>\n+        new EnvVarBuilder()\n+          .withName(s\"$ENV_JAVA_OPT_PREFIX$index\")\n+          .withValue(option)\n+          .build()\n+    }\n+\n+    val resolvedDriverContainer = new ContainerBuilder(currentDriverSpec.driverContainer)\n+      .addAllToEnv(driverJavaOptsEnvs.asJava)\n+      .build()\n+    val resolvedDriverPod = new PodBuilder(currentDriverSpec.driverPod)\n+      .editSpec()\n+        .addToContainers(resolvedDriverContainer)\n+        .endSpec()\n+      .build()\n+\n+    Utils.tryWithResource(\n+        kubernetesClient\n+          .pods()\n+          .withName(resolvedDriverPod.getMetadata.getName)\n+          .watch(loggingPodStatusWatcher)) { _ =>"
  }],
  "prId": 19717
}, {
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "I'm still unclear as to whether using a `scala.util.Try` is more idiomatic than `try...catch` for situations like these. @vanzin for thoughts.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-05T00:16:00Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkKubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource if any\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: Option[MainAppResource],\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.ArrayBuffer.empty[String]\n+\n+    args.sliding(2, 2).toList.foreach {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+/**\n+ * Submits a Spark application to run on Kubernetes by creating the driver pod and starting a\n+ * watcher that monitors and logs the application status. Waits for the application to terminate if\n+ * spark.kubernetes.submission.waitAppCompletion is true.\n+ *\n+ * @param submissionSteps steps that collectively configure the driver\n+ * @param submissionSparkConf the submission client Spark configuration\n+ * @param kubernetesClient the client to talk to the Kubernetes API server\n+ * @param waitForAppCompletion a flag indicating whether the client should wait for the application\n+ *                             to complete\n+ * @param appName the application name\n+ * @param loggingPodStatusWatcher a watcher that monitors and logs the application status\n+ */\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }\n+\n+    val resolvedDriverJavaOpts = currentDriverSpec\n+      .driverSparkConf\n+      // Remove this as the options are instead extracted and set individually below using\n+      // environment variables with prefix SPARK_JAVA_OPT_.\n+      .remove(org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+      .getAll\n+      .map {\n+        case (confKey, confValue) => s\"-D$confKey=$confValue\"\n+      } ++ driverJavaOptions.map(Utils.splitCommandString).getOrElse(Seq.empty)\n+    val driverJavaOptsEnvs: Seq[EnvVar] = resolvedDriverJavaOpts.zipWithIndex.map {\n+      case (option, index) =>\n+        new EnvVarBuilder()\n+          .withName(s\"$ENV_JAVA_OPT_PREFIX$index\")\n+          .withValue(option)\n+          .build()\n+    }\n+\n+    val resolvedDriverContainer = new ContainerBuilder(currentDriverSpec.driverContainer)\n+      .addAllToEnv(driverJavaOptsEnvs.asJava)\n+      .build()\n+    val resolvedDriverPod = new PodBuilder(currentDriverSpec.driverPod)\n+      .editSpec()\n+        .addToContainers(resolvedDriverContainer)\n+        .endSpec()\n+      .build()\n+\n+    Utils.tryWithResource(\n+        kubernetesClient\n+          .pods()\n+          .withName(resolvedDriverPod.getMetadata.getName)\n+          .watch(loggingPodStatusWatcher)) { _ =>\n+      val createdDriverPod = kubernetesClient.pods().create(resolvedDriverPod)\n+      try {"
  }, {
    "author": {
      "login": "mridulm"
    },
    "body": "We make more use of try than Try in spark codebase.\r\nAn unscientific test:\r\n```\r\n$ find . -type f -name '*.scala' | grep -v Suite | xargs grep Try | wc -l\r\n     269\r\n$ find . -type f -name '*.scala' | grep -v Suite | xargs grep try | wc -l\r\n    2506\r\n```\r\n",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-05T02:47:04Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkKubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource if any\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: Option[MainAppResource],\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.ArrayBuffer.empty[String]\n+\n+    args.sliding(2, 2).toList.foreach {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+/**\n+ * Submits a Spark application to run on Kubernetes by creating the driver pod and starting a\n+ * watcher that monitors and logs the application status. Waits for the application to terminate if\n+ * spark.kubernetes.submission.waitAppCompletion is true.\n+ *\n+ * @param submissionSteps steps that collectively configure the driver\n+ * @param submissionSparkConf the submission client Spark configuration\n+ * @param kubernetesClient the client to talk to the Kubernetes API server\n+ * @param waitForAppCompletion a flag indicating whether the client should wait for the application\n+ *                             to complete\n+ * @param appName the application name\n+ * @param loggingPodStatusWatcher a watcher that monitors and logs the application status\n+ */\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }\n+\n+    val resolvedDriverJavaOpts = currentDriverSpec\n+      .driverSparkConf\n+      // Remove this as the options are instead extracted and set individually below using\n+      // environment variables with prefix SPARK_JAVA_OPT_.\n+      .remove(org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+      .getAll\n+      .map {\n+        case (confKey, confValue) => s\"-D$confKey=$confValue\"\n+      } ++ driverJavaOptions.map(Utils.splitCommandString).getOrElse(Seq.empty)\n+    val driverJavaOptsEnvs: Seq[EnvVar] = resolvedDriverJavaOpts.zipWithIndex.map {\n+      case (option, index) =>\n+        new EnvVarBuilder()\n+          .withName(s\"$ENV_JAVA_OPT_PREFIX$index\")\n+          .withValue(option)\n+          .build()\n+    }\n+\n+    val resolvedDriverContainer = new ContainerBuilder(currentDriverSpec.driverContainer)\n+      .addAllToEnv(driverJavaOptsEnvs.asJava)\n+      .build()\n+    val resolvedDriverPod = new PodBuilder(currentDriverSpec.driverPod)\n+      .editSpec()\n+        .addToContainers(resolvedDriverContainer)\n+        .endSpec()\n+      .build()\n+\n+    Utils.tryWithResource(\n+        kubernetesClient\n+          .pods()\n+          .withName(resolvedDriverPod.getMetadata.getName)\n+          .watch(loggingPodStatusWatcher)) { _ =>\n+      val createdDriverPod = kubernetesClient.pods().create(resolvedDriverPod)\n+      try {"
  }],
  "prId": 19717
}, {
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "Check for `nonfatal` instead of `Throwable` I think.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-05T00:16:15Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkKubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource if any\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: Option[MainAppResource],\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.ArrayBuffer.empty[String]\n+\n+    args.sliding(2, 2).toList.foreach {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+/**\n+ * Submits a Spark application to run on Kubernetes by creating the driver pod and starting a\n+ * watcher that monitors and logs the application status. Waits for the application to terminate if\n+ * spark.kubernetes.submission.waitAppCompletion is true.\n+ *\n+ * @param submissionSteps steps that collectively configure the driver\n+ * @param submissionSparkConf the submission client Spark configuration\n+ * @param kubernetesClient the client to talk to the Kubernetes API server\n+ * @param waitForAppCompletion a flag indicating whether the client should wait for the application\n+ *                             to complete\n+ * @param appName the application name\n+ * @param loggingPodStatusWatcher a watcher that monitors and logs the application status\n+ */\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }\n+\n+    val resolvedDriverJavaOpts = currentDriverSpec\n+      .driverSparkConf\n+      // Remove this as the options are instead extracted and set individually below using\n+      // environment variables with prefix SPARK_JAVA_OPT_.\n+      .remove(org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+      .getAll\n+      .map {\n+        case (confKey, confValue) => s\"-D$confKey=$confValue\"\n+      } ++ driverJavaOptions.map(Utils.splitCommandString).getOrElse(Seq.empty)\n+    val driverJavaOptsEnvs: Seq[EnvVar] = resolvedDriverJavaOpts.zipWithIndex.map {\n+      case (option, index) =>\n+        new EnvVarBuilder()\n+          .withName(s\"$ENV_JAVA_OPT_PREFIX$index\")\n+          .withValue(option)\n+          .build()\n+    }\n+\n+    val resolvedDriverContainer = new ContainerBuilder(currentDriverSpec.driverContainer)\n+      .addAllToEnv(driverJavaOptsEnvs.asJava)\n+      .build()\n+    val resolvedDriverPod = new PodBuilder(currentDriverSpec.driverPod)\n+      .editSpec()\n+        .addToContainers(resolvedDriverContainer)\n+        .endSpec()\n+      .build()\n+\n+    Utils.tryWithResource(\n+        kubernetesClient\n+          .pods()\n+          .withName(resolvedDriverPod.getMetadata.getName)\n+          .watch(loggingPodStatusWatcher)) { _ =>\n+      val createdDriverPod = kubernetesClient.pods().create(resolvedDriverPod)\n+      try {\n+        if (currentDriverSpec.otherKubernetesResources.nonEmpty) {\n+          val otherKubernetesResources = currentDriverSpec.otherKubernetesResources\n+          addDriverOwnerReference(createdDriverPod, otherKubernetesResources)\n+          kubernetesClient.resourceList(otherKubernetesResources: _*).createOrReplace()\n+        }\n+      } catch {\n+        case e: Throwable =>"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "I think we want to make sure the driver pod gets deleted regardlessly.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-05T02:23:35Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkKubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource if any\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: Option[MainAppResource],\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.ArrayBuffer.empty[String]\n+\n+    args.sliding(2, 2).toList.foreach {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+/**\n+ * Submits a Spark application to run on Kubernetes by creating the driver pod and starting a\n+ * watcher that monitors and logs the application status. Waits for the application to terminate if\n+ * spark.kubernetes.submission.waitAppCompletion is true.\n+ *\n+ * @param submissionSteps steps that collectively configure the driver\n+ * @param submissionSparkConf the submission client Spark configuration\n+ * @param kubernetesClient the client to talk to the Kubernetes API server\n+ * @param waitForAppCompletion a flag indicating whether the client should wait for the application\n+ *                             to complete\n+ * @param appName the application name\n+ * @param loggingPodStatusWatcher a watcher that monitors and logs the application status\n+ */\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }\n+\n+    val resolvedDriverJavaOpts = currentDriverSpec\n+      .driverSparkConf\n+      // Remove this as the options are instead extracted and set individually below using\n+      // environment variables with prefix SPARK_JAVA_OPT_.\n+      .remove(org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+      .getAll\n+      .map {\n+        case (confKey, confValue) => s\"-D$confKey=$confValue\"\n+      } ++ driverJavaOptions.map(Utils.splitCommandString).getOrElse(Seq.empty)\n+    val driverJavaOptsEnvs: Seq[EnvVar] = resolvedDriverJavaOpts.zipWithIndex.map {\n+      case (option, index) =>\n+        new EnvVarBuilder()\n+          .withName(s\"$ENV_JAVA_OPT_PREFIX$index\")\n+          .withValue(option)\n+          .build()\n+    }\n+\n+    val resolvedDriverContainer = new ContainerBuilder(currentDriverSpec.driverContainer)\n+      .addAllToEnv(driverJavaOptsEnvs.asJava)\n+      .build()\n+    val resolvedDriverPod = new PodBuilder(currentDriverSpec.driverPod)\n+      .editSpec()\n+        .addToContainers(resolvedDriverContainer)\n+        .endSpec()\n+      .build()\n+\n+    Utils.tryWithResource(\n+        kubernetesClient\n+          .pods()\n+          .withName(resolvedDriverPod.getMetadata.getName)\n+          .watch(loggingPodStatusWatcher)) { _ =>\n+      val createdDriverPod = kubernetesClient.pods().create(resolvedDriverPod)\n+      try {\n+        if (currentDriverSpec.otherKubernetesResources.nonEmpty) {\n+          val otherKubernetesResources = currentDriverSpec.otherKubernetesResources\n+          addDriverOwnerReference(createdDriverPod, otherKubernetesResources)\n+          kubernetesClient.resourceList(otherKubernetesResources: _*).createOrReplace()\n+        }\n+      } catch {\n+        case e: Throwable =>"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "We don't want to be attempting to do anything on `OutOfMemmoryError` though for example. Perhaps we can register a shutdown hook that deletes the pod, and remove the shutdown hook after we successfully create everything. See `ShutdownHookManager`. But I think just catching `NonFatal` will deal with the majority use case.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-05T18:43:34Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkKubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource if any\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: Option[MainAppResource],\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.ArrayBuffer.empty[String]\n+\n+    args.sliding(2, 2).toList.foreach {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+/**\n+ * Submits a Spark application to run on Kubernetes by creating the driver pod and starting a\n+ * watcher that monitors and logs the application status. Waits for the application to terminate if\n+ * spark.kubernetes.submission.waitAppCompletion is true.\n+ *\n+ * @param submissionSteps steps that collectively configure the driver\n+ * @param submissionSparkConf the submission client Spark configuration\n+ * @param kubernetesClient the client to talk to the Kubernetes API server\n+ * @param waitForAppCompletion a flag indicating whether the client should wait for the application\n+ *                             to complete\n+ * @param appName the application name\n+ * @param loggingPodStatusWatcher a watcher that monitors and logs the application status\n+ */\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }\n+\n+    val resolvedDriverJavaOpts = currentDriverSpec\n+      .driverSparkConf\n+      // Remove this as the options are instead extracted and set individually below using\n+      // environment variables with prefix SPARK_JAVA_OPT_.\n+      .remove(org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+      .getAll\n+      .map {\n+        case (confKey, confValue) => s\"-D$confKey=$confValue\"\n+      } ++ driverJavaOptions.map(Utils.splitCommandString).getOrElse(Seq.empty)\n+    val driverJavaOptsEnvs: Seq[EnvVar] = resolvedDriverJavaOpts.zipWithIndex.map {\n+      case (option, index) =>\n+        new EnvVarBuilder()\n+          .withName(s\"$ENV_JAVA_OPT_PREFIX$index\")\n+          .withValue(option)\n+          .build()\n+    }\n+\n+    val resolvedDriverContainer = new ContainerBuilder(currentDriverSpec.driverContainer)\n+      .addAllToEnv(driverJavaOptsEnvs.asJava)\n+      .build()\n+    val resolvedDriverPod = new PodBuilder(currentDriverSpec.driverPod)\n+      .editSpec()\n+        .addToContainers(resolvedDriverContainer)\n+        .endSpec()\n+      .build()\n+\n+    Utils.tryWithResource(\n+        kubernetesClient\n+          .pods()\n+          .withName(resolvedDriverPod.getMetadata.getName)\n+          .watch(loggingPodStatusWatcher)) { _ =>\n+      val createdDriverPod = kubernetesClient.pods().create(resolvedDriverPod)\n+      try {\n+        if (currentDriverSpec.otherKubernetesResources.nonEmpty) {\n+          val otherKubernetesResources = currentDriverSpec.otherKubernetesResources\n+          addDriverOwnerReference(createdDriverPod, otherKubernetesResources)\n+          kubernetesClient.resourceList(otherKubernetesResources: _*).createOrReplace()\n+        }\n+      } catch {\n+        case e: Throwable =>"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Makes sense to me. @felixcheung WDYT about catching only `NonFatal`?",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-05T19:39:13Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkKubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource if any\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: Option[MainAppResource],\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.ArrayBuffer.empty[String]\n+\n+    args.sliding(2, 2).toList.foreach {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+/**\n+ * Submits a Spark application to run on Kubernetes by creating the driver pod and starting a\n+ * watcher that monitors and logs the application status. Waits for the application to terminate if\n+ * spark.kubernetes.submission.waitAppCompletion is true.\n+ *\n+ * @param submissionSteps steps that collectively configure the driver\n+ * @param submissionSparkConf the submission client Spark configuration\n+ * @param kubernetesClient the client to talk to the Kubernetes API server\n+ * @param waitForAppCompletion a flag indicating whether the client should wait for the application\n+ *                             to complete\n+ * @param appName the application name\n+ * @param loggingPodStatusWatcher a watcher that monitors and logs the application status\n+ */\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }\n+\n+    val resolvedDriverJavaOpts = currentDriverSpec\n+      .driverSparkConf\n+      // Remove this as the options are instead extracted and set individually below using\n+      // environment variables with prefix SPARK_JAVA_OPT_.\n+      .remove(org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+      .getAll\n+      .map {\n+        case (confKey, confValue) => s\"-D$confKey=$confValue\"\n+      } ++ driverJavaOptions.map(Utils.splitCommandString).getOrElse(Seq.empty)\n+    val driverJavaOptsEnvs: Seq[EnvVar] = resolvedDriverJavaOpts.zipWithIndex.map {\n+      case (option, index) =>\n+        new EnvVarBuilder()\n+          .withName(s\"$ENV_JAVA_OPT_PREFIX$index\")\n+          .withValue(option)\n+          .build()\n+    }\n+\n+    val resolvedDriverContainer = new ContainerBuilder(currentDriverSpec.driverContainer)\n+      .addAllToEnv(driverJavaOptsEnvs.asJava)\n+      .build()\n+    val resolvedDriverPod = new PodBuilder(currentDriverSpec.driverPod)\n+      .editSpec()\n+        .addToContainers(resolvedDriverContainer)\n+        .endSpec()\n+      .build()\n+\n+    Utils.tryWithResource(\n+        kubernetesClient\n+          .pods()\n+          .withName(resolvedDriverPod.getMetadata.getName)\n+          .watch(loggingPodStatusWatcher)) { _ =>\n+      val createdDriverPod = kubernetesClient.pods().create(resolvedDriverPod)\n+      try {\n+        if (currentDriverSpec.otherKubernetesResources.nonEmpty) {\n+          val otherKubernetesResources = currentDriverSpec.otherKubernetesResources\n+          addDriverOwnerReference(createdDriverPod, otherKubernetesResources)\n+          kubernetesClient.resourceList(otherKubernetesResources: _*).createOrReplace()\n+        }\n+      } catch {\n+        case e: Throwable =>"
  }],
  "prId": 19717
}, {
  "comments": [{
    "author": {
      "login": "jerryshao"
    },
    "body": "Is it an application Id for Spark k8s application? Does K8s have a convention for app id? Normally like YARN, it is timestamp based, which is easy for sorting, retrieving, I'm not sure how k8s treat this?\r\n\r\nFrom my understanding of the code, seems like k8s doesn't have app id related concept, so it is up to Spark to define here? Is it posssible to change to timestamp based like standalone, local and yarn deploy?",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-05T08:26:14Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkKubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource if any\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: Option[MainAppResource],\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.ArrayBuffer.empty[String]\n+\n+    args.sliding(2, 2).toList.foreach {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+/**\n+ * Submits a Spark application to run on Kubernetes by creating the driver pod and starting a\n+ * watcher that monitors and logs the application status. Waits for the application to terminate if\n+ * spark.kubernetes.submission.waitAppCompletion is true.\n+ *\n+ * @param submissionSteps steps that collectively configure the driver\n+ * @param submissionSparkConf the submission client Spark configuration\n+ * @param kubernetesClient the client to talk to the Kubernetes API server\n+ * @param waitForAppCompletion a flag indicating whether the client should wait for the application\n+ *                             to complete\n+ * @param appName the application name\n+ * @param loggingPodStatusWatcher a watcher that monitors and logs the application status\n+ */\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }\n+\n+    val resolvedDriverJavaOpts = currentDriverSpec\n+      .driverSparkConf\n+      // Remove this as the options are instead extracted and set individually below using\n+      // environment variables with prefix SPARK_JAVA_OPT_.\n+      .remove(org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+      .getAll\n+      .map {\n+        case (confKey, confValue) => s\"-D$confKey=$confValue\"\n+      } ++ driverJavaOptions.map(Utils.splitCommandString).getOrElse(Seq.empty)\n+    val driverJavaOptsEnvs: Seq[EnvVar] = resolvedDriverJavaOpts.zipWithIndex.map {\n+      case (option, index) =>\n+        new EnvVarBuilder()\n+          .withName(s\"$ENV_JAVA_OPT_PREFIX$index\")\n+          .withValue(option)\n+          .build()\n+    }\n+\n+    val resolvedDriverContainer = new ContainerBuilder(currentDriverSpec.driverContainer)\n+      .addAllToEnv(driverJavaOptsEnvs.asJava)\n+      .build()\n+    val resolvedDriverPod = new PodBuilder(currentDriverSpec.driverPod)\n+      .editSpec()\n+        .addToContainers(resolvedDriverContainer)\n+        .endSpec()\n+      .build()\n+\n+    Utils.tryWithResource(\n+        kubernetesClient\n+          .pods()\n+          .withName(resolvedDriverPod.getMetadata.getName)\n+          .watch(loggingPodStatusWatcher)) { _ =>\n+      val createdDriverPod = kubernetesClient.pods().create(resolvedDriverPod)\n+      try {\n+        if (currentDriverSpec.otherKubernetesResources.nonEmpty) {\n+          val otherKubernetesResources = currentDriverSpec.otherKubernetesResources\n+          addDriverOwnerReference(createdDriverPod, otherKubernetesResources)\n+          kubernetesClient.resourceList(otherKubernetesResources: _*).createOrReplace()\n+        }\n+      } catch {\n+        case e: Throwable =>\n+          kubernetesClient.pods().delete(createdDriverPod)\n+          throw e\n+      }\n+\n+      if (waitForAppCompletion) {\n+        logInfo(s\"Waiting for application $appName to finish...\")\n+        loggingPodStatusWatcher.awaitCompletion()\n+        logInfo(s\"Application $appName finished.\")\n+      } else {\n+        logInfo(s\"Deployed Spark application $appName into Kubernetes.\")\n+      }\n+    }\n+  }\n+\n+  // Add a OwnerReference to the given resources making the driver pod an owner of them so when\n+  // the driver pod is deleted, the resources are garbage collected.\n+  private def addDriverOwnerReference(driverPod: Pod, resources: Seq[HasMetadata]): Unit = {\n+    val driverPodOwnerReference = new OwnerReferenceBuilder()\n+      .withName(driverPod.getMetadata.getName)\n+      .withApiVersion(driverPod.getApiVersion)\n+      .withUid(driverPod.getMetadata.getUid)\n+      .withKind(driverPod.getKind)\n+      .withController(true)\n+      .build()\n+    resources.foreach { resource =>\n+      val originalMetadata = resource.getMetadata\n+      originalMetadata.setOwnerReferences(Collections.singletonList(driverPodOwnerReference))\n+    }\n+  }\n+}\n+\n+/**\n+ * Main class and entry point of application submission in KUBERNETES mode.\n+ */\n+private[spark] object Client extends SparkApplication {\n+\n+  override def start(args: Array[String], conf: SparkConf): Unit = {\n+    val parsedArguments = ClientArguments.fromCommandLineArgs(args)\n+    run(parsedArguments, conf)\n+  }\n+\n+  private def run(clientArguments: ClientArguments, sparkConf: SparkConf): Unit = {\n+    val namespace = sparkConf.get(KUBERNETES_NAMESPACE)\n+    val kubernetesAppId = s\"spark-${UUID.randomUUID().toString.replaceAll(\"-\", \"\")}\""
  }, {
    "author": {
      "login": "foxish"
    },
    "body": "> Does K8s have a convention for app id?\r\n\r\nK8s does not. It does have the notion of UUIDs for individual objects, but those are associated with each pod, and not common to an entire application. For representation, we will be introducing a [CRD](https://kubernetes.io/docs/concepts/api-extension/custom-resources/) in the future. That will be how we track individual spark apps in future (post 2.3). That CRD should be able to accommodate all the details, such as timestamp, and status - so, keeping the appID (and driver pod names) collision free using UUIDs made sense.\r\n",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-05T14:12:36Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkKubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource if any\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: Option[MainAppResource],\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.ArrayBuffer.empty[String]\n+\n+    args.sliding(2, 2).toList.foreach {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+/**\n+ * Submits a Spark application to run on Kubernetes by creating the driver pod and starting a\n+ * watcher that monitors and logs the application status. Waits for the application to terminate if\n+ * spark.kubernetes.submission.waitAppCompletion is true.\n+ *\n+ * @param submissionSteps steps that collectively configure the driver\n+ * @param submissionSparkConf the submission client Spark configuration\n+ * @param kubernetesClient the client to talk to the Kubernetes API server\n+ * @param waitForAppCompletion a flag indicating whether the client should wait for the application\n+ *                             to complete\n+ * @param appName the application name\n+ * @param loggingPodStatusWatcher a watcher that monitors and logs the application status\n+ */\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }\n+\n+    val resolvedDriverJavaOpts = currentDriverSpec\n+      .driverSparkConf\n+      // Remove this as the options are instead extracted and set individually below using\n+      // environment variables with prefix SPARK_JAVA_OPT_.\n+      .remove(org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+      .getAll\n+      .map {\n+        case (confKey, confValue) => s\"-D$confKey=$confValue\"\n+      } ++ driverJavaOptions.map(Utils.splitCommandString).getOrElse(Seq.empty)\n+    val driverJavaOptsEnvs: Seq[EnvVar] = resolvedDriverJavaOpts.zipWithIndex.map {\n+      case (option, index) =>\n+        new EnvVarBuilder()\n+          .withName(s\"$ENV_JAVA_OPT_PREFIX$index\")\n+          .withValue(option)\n+          .build()\n+    }\n+\n+    val resolvedDriverContainer = new ContainerBuilder(currentDriverSpec.driverContainer)\n+      .addAllToEnv(driverJavaOptsEnvs.asJava)\n+      .build()\n+    val resolvedDriverPod = new PodBuilder(currentDriverSpec.driverPod)\n+      .editSpec()\n+        .addToContainers(resolvedDriverContainer)\n+        .endSpec()\n+      .build()\n+\n+    Utils.tryWithResource(\n+        kubernetesClient\n+          .pods()\n+          .withName(resolvedDriverPod.getMetadata.getName)\n+          .watch(loggingPodStatusWatcher)) { _ =>\n+      val createdDriverPod = kubernetesClient.pods().create(resolvedDriverPod)\n+      try {\n+        if (currentDriverSpec.otherKubernetesResources.nonEmpty) {\n+          val otherKubernetesResources = currentDriverSpec.otherKubernetesResources\n+          addDriverOwnerReference(createdDriverPod, otherKubernetesResources)\n+          kubernetesClient.resourceList(otherKubernetesResources: _*).createOrReplace()\n+        }\n+      } catch {\n+        case e: Throwable =>\n+          kubernetesClient.pods().delete(createdDriverPod)\n+          throw e\n+      }\n+\n+      if (waitForAppCompletion) {\n+        logInfo(s\"Waiting for application $appName to finish...\")\n+        loggingPodStatusWatcher.awaitCompletion()\n+        logInfo(s\"Application $appName finished.\")\n+      } else {\n+        logInfo(s\"Deployed Spark application $appName into Kubernetes.\")\n+      }\n+    }\n+  }\n+\n+  // Add a OwnerReference to the given resources making the driver pod an owner of them so when\n+  // the driver pod is deleted, the resources are garbage collected.\n+  private def addDriverOwnerReference(driverPod: Pod, resources: Seq[HasMetadata]): Unit = {\n+    val driverPodOwnerReference = new OwnerReferenceBuilder()\n+      .withName(driverPod.getMetadata.getName)\n+      .withApiVersion(driverPod.getApiVersion)\n+      .withUid(driverPod.getMetadata.getUid)\n+      .withKind(driverPod.getKind)\n+      .withController(true)\n+      .build()\n+    resources.foreach { resource =>\n+      val originalMetadata = resource.getMetadata\n+      originalMetadata.setOwnerReferences(Collections.singletonList(driverPodOwnerReference))\n+    }\n+  }\n+}\n+\n+/**\n+ * Main class and entry point of application submission in KUBERNETES mode.\n+ */\n+private[spark] object Client extends SparkApplication {\n+\n+  override def start(args: Array[String], conf: SparkConf): Unit = {\n+    val parsedArguments = ClientArguments.fromCommandLineArgs(args)\n+    run(parsedArguments, conf)\n+  }\n+\n+  private def run(clientArguments: ClientArguments, sparkConf: SparkConf): Unit = {\n+    val namespace = sparkConf.get(KUBERNETES_NAMESPACE)\n+    val kubernetesAppId = s\"spark-${UUID.randomUUID().toString.replaceAll(\"-\", \"\")}\""
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "Kubernetes is different from YARN in the sense that the API server expects one to provide their own names for things that are unique in their namespace. However we're coming up with the unique object names for the user since a single spark-submit command generates multiple kinds of objects (secrets, service, pod) that may end up being difficult for the user to reason about in terms of uniqueness.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-05T18:42:03Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkKubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource if any\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: Option[MainAppResource],\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.ArrayBuffer.empty[String]\n+\n+    args.sliding(2, 2).toList.foreach {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+/**\n+ * Submits a Spark application to run on Kubernetes by creating the driver pod and starting a\n+ * watcher that monitors and logs the application status. Waits for the application to terminate if\n+ * spark.kubernetes.submission.waitAppCompletion is true.\n+ *\n+ * @param submissionSteps steps that collectively configure the driver\n+ * @param submissionSparkConf the submission client Spark configuration\n+ * @param kubernetesClient the client to talk to the Kubernetes API server\n+ * @param waitForAppCompletion a flag indicating whether the client should wait for the application\n+ *                             to complete\n+ * @param appName the application name\n+ * @param loggingPodStatusWatcher a watcher that monitors and logs the application status\n+ */\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }\n+\n+    val resolvedDriverJavaOpts = currentDriverSpec\n+      .driverSparkConf\n+      // Remove this as the options are instead extracted and set individually below using\n+      // environment variables with prefix SPARK_JAVA_OPT_.\n+      .remove(org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+      .getAll\n+      .map {\n+        case (confKey, confValue) => s\"-D$confKey=$confValue\"\n+      } ++ driverJavaOptions.map(Utils.splitCommandString).getOrElse(Seq.empty)\n+    val driverJavaOptsEnvs: Seq[EnvVar] = resolvedDriverJavaOpts.zipWithIndex.map {\n+      case (option, index) =>\n+        new EnvVarBuilder()\n+          .withName(s\"$ENV_JAVA_OPT_PREFIX$index\")\n+          .withValue(option)\n+          .build()\n+    }\n+\n+    val resolvedDriverContainer = new ContainerBuilder(currentDriverSpec.driverContainer)\n+      .addAllToEnv(driverJavaOptsEnvs.asJava)\n+      .build()\n+    val resolvedDriverPod = new PodBuilder(currentDriverSpec.driverPod)\n+      .editSpec()\n+        .addToContainers(resolvedDriverContainer)\n+        .endSpec()\n+      .build()\n+\n+    Utils.tryWithResource(\n+        kubernetesClient\n+          .pods()\n+          .withName(resolvedDriverPod.getMetadata.getName)\n+          .watch(loggingPodStatusWatcher)) { _ =>\n+      val createdDriverPod = kubernetesClient.pods().create(resolvedDriverPod)\n+      try {\n+        if (currentDriverSpec.otherKubernetesResources.nonEmpty) {\n+          val otherKubernetesResources = currentDriverSpec.otherKubernetesResources\n+          addDriverOwnerReference(createdDriverPod, otherKubernetesResources)\n+          kubernetesClient.resourceList(otherKubernetesResources: _*).createOrReplace()\n+        }\n+      } catch {\n+        case e: Throwable =>\n+          kubernetesClient.pods().delete(createdDriverPod)\n+          throw e\n+      }\n+\n+      if (waitForAppCompletion) {\n+        logInfo(s\"Waiting for application $appName to finish...\")\n+        loggingPodStatusWatcher.awaitCompletion()\n+        logInfo(s\"Application $appName finished.\")\n+      } else {\n+        logInfo(s\"Deployed Spark application $appName into Kubernetes.\")\n+      }\n+    }\n+  }\n+\n+  // Add a OwnerReference to the given resources making the driver pod an owner of them so when\n+  // the driver pod is deleted, the resources are garbage collected.\n+  private def addDriverOwnerReference(driverPod: Pod, resources: Seq[HasMetadata]): Unit = {\n+    val driverPodOwnerReference = new OwnerReferenceBuilder()\n+      .withName(driverPod.getMetadata.getName)\n+      .withApiVersion(driverPod.getApiVersion)\n+      .withUid(driverPod.getMetadata.getUid)\n+      .withKind(driverPod.getKind)\n+      .withController(true)\n+      .build()\n+    resources.foreach { resource =>\n+      val originalMetadata = resource.getMetadata\n+      originalMetadata.setOwnerReferences(Collections.singletonList(driverPodOwnerReference))\n+    }\n+  }\n+}\n+\n+/**\n+ * Main class and entry point of application submission in KUBERNETES mode.\n+ */\n+private[spark] object Client extends SparkApplication {\n+\n+  override def start(args: Array[String], conf: SparkConf): Unit = {\n+    val parsedArguments = ClientArguments.fromCommandLineArgs(args)\n+    run(parsedArguments, conf)\n+  }\n+\n+  private def run(clientArguments: ClientArguments, sparkConf: SparkConf): Unit = {\n+    val namespace = sparkConf.get(KUBERNETES_NAMESPACE)\n+    val kubernetesAppId = s\"spark-${UUID.randomUUID().toString.replaceAll(\"-\", \"\")}\""
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "Thanks for the explanation. \r\n\r\nMy hunch is that if we can make this app-id more meaningful, like timestamp-xxx, which will be easier for user to distinguish. Using UUID though achieves uniqueness, seems not so intuitive.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-06T01:28:57Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkKubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource if any\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: Option[MainAppResource],\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.ArrayBuffer.empty[String]\n+\n+    args.sliding(2, 2).toList.foreach {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+/**\n+ * Submits a Spark application to run on Kubernetes by creating the driver pod and starting a\n+ * watcher that monitors and logs the application status. Waits for the application to terminate if\n+ * spark.kubernetes.submission.waitAppCompletion is true.\n+ *\n+ * @param submissionSteps steps that collectively configure the driver\n+ * @param submissionSparkConf the submission client Spark configuration\n+ * @param kubernetesClient the client to talk to the Kubernetes API server\n+ * @param waitForAppCompletion a flag indicating whether the client should wait for the application\n+ *                             to complete\n+ * @param appName the application name\n+ * @param loggingPodStatusWatcher a watcher that monitors and logs the application status\n+ */\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }\n+\n+    val resolvedDriverJavaOpts = currentDriverSpec\n+      .driverSparkConf\n+      // Remove this as the options are instead extracted and set individually below using\n+      // environment variables with prefix SPARK_JAVA_OPT_.\n+      .remove(org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+      .getAll\n+      .map {\n+        case (confKey, confValue) => s\"-D$confKey=$confValue\"\n+      } ++ driverJavaOptions.map(Utils.splitCommandString).getOrElse(Seq.empty)\n+    val driverJavaOptsEnvs: Seq[EnvVar] = resolvedDriverJavaOpts.zipWithIndex.map {\n+      case (option, index) =>\n+        new EnvVarBuilder()\n+          .withName(s\"$ENV_JAVA_OPT_PREFIX$index\")\n+          .withValue(option)\n+          .build()\n+    }\n+\n+    val resolvedDriverContainer = new ContainerBuilder(currentDriverSpec.driverContainer)\n+      .addAllToEnv(driverJavaOptsEnvs.asJava)\n+      .build()\n+    val resolvedDriverPod = new PodBuilder(currentDriverSpec.driverPod)\n+      .editSpec()\n+        .addToContainers(resolvedDriverContainer)\n+        .endSpec()\n+      .build()\n+\n+    Utils.tryWithResource(\n+        kubernetesClient\n+          .pods()\n+          .withName(resolvedDriverPod.getMetadata.getName)\n+          .watch(loggingPodStatusWatcher)) { _ =>\n+      val createdDriverPod = kubernetesClient.pods().create(resolvedDriverPod)\n+      try {\n+        if (currentDriverSpec.otherKubernetesResources.nonEmpty) {\n+          val otherKubernetesResources = currentDriverSpec.otherKubernetesResources\n+          addDriverOwnerReference(createdDriverPod, otherKubernetesResources)\n+          kubernetesClient.resourceList(otherKubernetesResources: _*).createOrReplace()\n+        }\n+      } catch {\n+        case e: Throwable =>\n+          kubernetesClient.pods().delete(createdDriverPod)\n+          throw e\n+      }\n+\n+      if (waitForAppCompletion) {\n+        logInfo(s\"Waiting for application $appName to finish...\")\n+        loggingPodStatusWatcher.awaitCompletion()\n+        logInfo(s\"Application $appName finished.\")\n+      } else {\n+        logInfo(s\"Deployed Spark application $appName into Kubernetes.\")\n+      }\n+    }\n+  }\n+\n+  // Add a OwnerReference to the given resources making the driver pod an owner of them so when\n+  // the driver pod is deleted, the resources are garbage collected.\n+  private def addDriverOwnerReference(driverPod: Pod, resources: Seq[HasMetadata]): Unit = {\n+    val driverPodOwnerReference = new OwnerReferenceBuilder()\n+      .withName(driverPod.getMetadata.getName)\n+      .withApiVersion(driverPod.getApiVersion)\n+      .withUid(driverPod.getMetadata.getUid)\n+      .withKind(driverPod.getKind)\n+      .withController(true)\n+      .build()\n+    resources.foreach { resource =>\n+      val originalMetadata = resource.getMetadata\n+      originalMetadata.setOwnerReferences(Collections.singletonList(driverPodOwnerReference))\n+    }\n+  }\n+}\n+\n+/**\n+ * Main class and entry point of application submission in KUBERNETES mode.\n+ */\n+private[spark] object Client extends SparkApplication {\n+\n+  override def start(args: Array[String], conf: SparkConf): Unit = {\n+    val parsedArguments = ClientArguments.fromCommandLineArgs(args)\n+    run(parsedArguments, conf)\n+  }\n+\n+  private def run(clientArguments: ClientArguments, sparkConf: SparkConf): Unit = {\n+    val namespace = sparkConf.get(KUBERNETES_NAMESPACE)\n+    val kubernetesAppId = s\"spark-${UUID.randomUUID().toString.replaceAll(\"-\", \"\")}\""
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "The app id is actually not meant for users to see and use so being meaningful is not important. It's used as a label selector to group k8s API resources belonging to the same application. The resource name prefix, on the other hand, does have the application name in it so users know which application the resources belong to.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-06T05:14:16Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit\n+\n+import java.util.{Collections, UUID}\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import io.fabric8.kubernetes.api.model._\n+import io.fabric8.kubernetes.client.KubernetesClient\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkApplication\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkKubernetesClientFactory\n+import org.apache.spark.deploy.k8s.submit.steps.DriverConfigurationStep\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * Encapsulates arguments to the submission client.\n+ *\n+ * @param mainAppResource the main application resource if any\n+ * @param mainClass the main class of the application to run\n+ * @param driverArgs arguments to the driver\n+ */\n+private[spark] case class ClientArguments(\n+     mainAppResource: Option[MainAppResource],\n+     mainClass: String,\n+     driverArgs: Array[String])\n+\n+private[spark] object ClientArguments {\n+\n+  def fromCommandLineArgs(args: Array[String]): ClientArguments = {\n+    var mainAppResource: Option[MainAppResource] = None\n+    var mainClass: Option[String] = None\n+    val driverArgs = mutable.ArrayBuffer.empty[String]\n+\n+    args.sliding(2, 2).toList.foreach {\n+      case Array(\"--primary-java-resource\", primaryJavaResource: String) =>\n+        mainAppResource = Some(JavaMainAppResource(primaryJavaResource))\n+      case Array(\"--main-class\", clazz: String) =>\n+        mainClass = Some(clazz)\n+      case Array(\"--arg\", arg: String) =>\n+        driverArgs += arg\n+      case other =>\n+        val invalid = other.mkString(\" \")\n+        throw new RuntimeException(s\"Unknown arguments: $invalid\")\n+    }\n+\n+    require(mainClass.isDefined, \"Main class must be specified via --main-class\")\n+\n+    ClientArguments(\n+      mainAppResource,\n+      mainClass.get,\n+      driverArgs.toArray)\n+  }\n+}\n+\n+/**\n+ * Submits a Spark application to run on Kubernetes by creating the driver pod and starting a\n+ * watcher that monitors and logs the application status. Waits for the application to terminate if\n+ * spark.kubernetes.submission.waitAppCompletion is true.\n+ *\n+ * @param submissionSteps steps that collectively configure the driver\n+ * @param submissionSparkConf the submission client Spark configuration\n+ * @param kubernetesClient the client to talk to the Kubernetes API server\n+ * @param waitForAppCompletion a flag indicating whether the client should wait for the application\n+ *                             to complete\n+ * @param appName the application name\n+ * @param loggingPodStatusWatcher a watcher that monitors and logs the application status\n+ */\n+private[spark] class Client(\n+    submissionSteps: Seq[DriverConfigurationStep],\n+    submissionSparkConf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    waitForAppCompletion: Boolean,\n+    appName: String,\n+    loggingPodStatusWatcher: LoggingPodStatusWatcher) extends Logging {\n+\n+  private val driverJavaOptions = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+\n+   /**\n+    * Run command that initializes a DriverSpec that will be updated after each\n+    * DriverConfigurationStep in the sequence that is passed in. The final KubernetesDriverSpec\n+    * will be used to build the Driver Container, Driver Pod, and Kubernetes Resources\n+    */\n+  def run(): Unit = {\n+    var currentDriverSpec = KubernetesDriverSpec.initialSpec(submissionSparkConf)\n+    // submissionSteps contain steps necessary to take, to resolve varying\n+    // client arguments that are passed in, created by orchestrator\n+    for (nextStep <- submissionSteps) {\n+      currentDriverSpec = nextStep.configureDriver(currentDriverSpec)\n+    }\n+\n+    val resolvedDriverJavaOpts = currentDriverSpec\n+      .driverSparkConf\n+      // Remove this as the options are instead extracted and set individually below using\n+      // environment variables with prefix SPARK_JAVA_OPT_.\n+      .remove(org.apache.spark.internal.config.DRIVER_JAVA_OPTIONS)\n+      .getAll\n+      .map {\n+        case (confKey, confValue) => s\"-D$confKey=$confValue\"\n+      } ++ driverJavaOptions.map(Utils.splitCommandString).getOrElse(Seq.empty)\n+    val driverJavaOptsEnvs: Seq[EnvVar] = resolvedDriverJavaOpts.zipWithIndex.map {\n+      case (option, index) =>\n+        new EnvVarBuilder()\n+          .withName(s\"$ENV_JAVA_OPT_PREFIX$index\")\n+          .withValue(option)\n+          .build()\n+    }\n+\n+    val resolvedDriverContainer = new ContainerBuilder(currentDriverSpec.driverContainer)\n+      .addAllToEnv(driverJavaOptsEnvs.asJava)\n+      .build()\n+    val resolvedDriverPod = new PodBuilder(currentDriverSpec.driverPod)\n+      .editSpec()\n+        .addToContainers(resolvedDriverContainer)\n+        .endSpec()\n+      .build()\n+\n+    Utils.tryWithResource(\n+        kubernetesClient\n+          .pods()\n+          .withName(resolvedDriverPod.getMetadata.getName)\n+          .watch(loggingPodStatusWatcher)) { _ =>\n+      val createdDriverPod = kubernetesClient.pods().create(resolvedDriverPod)\n+      try {\n+        if (currentDriverSpec.otherKubernetesResources.nonEmpty) {\n+          val otherKubernetesResources = currentDriverSpec.otherKubernetesResources\n+          addDriverOwnerReference(createdDriverPod, otherKubernetesResources)\n+          kubernetesClient.resourceList(otherKubernetesResources: _*).createOrReplace()\n+        }\n+      } catch {\n+        case e: Throwable =>\n+          kubernetesClient.pods().delete(createdDriverPod)\n+          throw e\n+      }\n+\n+      if (waitForAppCompletion) {\n+        logInfo(s\"Waiting for application $appName to finish...\")\n+        loggingPodStatusWatcher.awaitCompletion()\n+        logInfo(s\"Application $appName finished.\")\n+      } else {\n+        logInfo(s\"Deployed Spark application $appName into Kubernetes.\")\n+      }\n+    }\n+  }\n+\n+  // Add a OwnerReference to the given resources making the driver pod an owner of them so when\n+  // the driver pod is deleted, the resources are garbage collected.\n+  private def addDriverOwnerReference(driverPod: Pod, resources: Seq[HasMetadata]): Unit = {\n+    val driverPodOwnerReference = new OwnerReferenceBuilder()\n+      .withName(driverPod.getMetadata.getName)\n+      .withApiVersion(driverPod.getApiVersion)\n+      .withUid(driverPod.getMetadata.getUid)\n+      .withKind(driverPod.getKind)\n+      .withController(true)\n+      .build()\n+    resources.foreach { resource =>\n+      val originalMetadata = resource.getMetadata\n+      originalMetadata.setOwnerReferences(Collections.singletonList(driverPodOwnerReference))\n+    }\n+  }\n+}\n+\n+/**\n+ * Main class and entry point of application submission in KUBERNETES mode.\n+ */\n+private[spark] object Client extends SparkApplication {\n+\n+  override def start(args: Array[String], conf: SparkConf): Unit = {\n+    val parsedArguments = ClientArguments.fromCommandLineArgs(args)\n+    run(parsedArguments, conf)\n+  }\n+\n+  private def run(clientArguments: ClientArguments, sparkConf: SparkConf): Unit = {\n+    val namespace = sparkConf.get(KUBERNETES_NAMESPACE)\n+    val kubernetesAppId = s\"spark-${UUID.randomUUID().toString.replaceAll(\"-\", \"\")}\""
  }],
  "prId": 19717
}]