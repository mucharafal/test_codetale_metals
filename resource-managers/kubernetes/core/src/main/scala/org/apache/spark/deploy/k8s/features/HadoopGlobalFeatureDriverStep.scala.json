[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: params in their own line are double-indented. Also happens in other places.",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-09-05T21:21:27Z",
    "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features\n+\n+import scala.collection.JavaConverters._\n+\n+import io.fabric8.kubernetes.api.model.{ConfigMapBuilder, ContainerBuilder, HasMetadata, PodBuilder}\n+\n+import org.apache.spark.deploy.k8s.{KubernetesConf, SparkPod}\n+import org.apache.spark.deploy.k8s.Config.{KUBERNETES_KERBEROS_KRB5_FILE, KUBERNETES_KERBEROS_PROXY_USER}\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesDriverSpecificConf\n+import org.apache.spark.deploy.k8s.features.hadoopsteps.{HadoopBootstrapUtil, HadoopConfigSpec, HadoopConfigurationStep}\n+import org.apache.spark.internal.Logging\n+\n+ /**\n+  * This is the main method that runs the hadoopConfigurationSteps defined\n+  * by the HadoopStepsOrchestrator. These steps are run to modify the\n+  * SparkPod and Kubernetes Resources using the additive method of the feature steps\n+  */\n+private[spark] class HadoopGlobalFeatureDriverStep(\n+  kubernetesConf: KubernetesConf[KubernetesDriverSpecificConf])"
  }],
  "prId": 21669
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "fits in previous line.",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-09-05T21:21:48Z",
    "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features\n+\n+import scala.collection.JavaConverters._\n+\n+import io.fabric8.kubernetes.api.model.{ConfigMapBuilder, ContainerBuilder, HasMetadata, PodBuilder}\n+\n+import org.apache.spark.deploy.k8s.{KubernetesConf, SparkPod}\n+import org.apache.spark.deploy.k8s.Config.{KUBERNETES_KERBEROS_KRB5_FILE, KUBERNETES_KERBEROS_PROXY_USER}\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesDriverSpecificConf\n+import org.apache.spark.deploy.k8s.features.hadoopsteps.{HadoopBootstrapUtil, HadoopConfigSpec, HadoopConfigurationStep}\n+import org.apache.spark.internal.Logging\n+\n+ /**\n+  * This is the main method that runs the hadoopConfigurationSteps defined\n+  * by the HadoopStepsOrchestrator. These steps are run to modify the\n+  * SparkPod and Kubernetes Resources using the additive method of the feature steps\n+  */\n+private[spark] class HadoopGlobalFeatureDriverStep(\n+  kubernetesConf: KubernetesConf[KubernetesDriverSpecificConf])\n+  extends KubernetesFeatureConfigStep with Logging {\n+   private val hadoopTestOrchestrator =\n+     kubernetesConf.getHadoopStepsOrchestrator"
  }],
  "prId": 21669
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "keep condition in previous line, break only the message if it doesn't fit.",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-09-05T21:22:08Z",
    "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features\n+\n+import scala.collection.JavaConverters._\n+\n+import io.fabric8.kubernetes.api.model.{ConfigMapBuilder, ContainerBuilder, HasMetadata, PodBuilder}\n+\n+import org.apache.spark.deploy.k8s.{KubernetesConf, SparkPod}\n+import org.apache.spark.deploy.k8s.Config.{KUBERNETES_KERBEROS_KRB5_FILE, KUBERNETES_KERBEROS_PROXY_USER}\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesDriverSpecificConf\n+import org.apache.spark.deploy.k8s.features.hadoopsteps.{HadoopBootstrapUtil, HadoopConfigSpec, HadoopConfigurationStep}\n+import org.apache.spark.internal.Logging\n+\n+ /**\n+  * This is the main method that runs the hadoopConfigurationSteps defined\n+  * by the HadoopStepsOrchestrator. These steps are run to modify the\n+  * SparkPod and Kubernetes Resources using the additive method of the feature steps\n+  */\n+private[spark] class HadoopGlobalFeatureDriverStep(\n+  kubernetesConf: KubernetesConf[KubernetesDriverSpecificConf])\n+  extends KubernetesFeatureConfigStep with Logging {\n+   private val hadoopTestOrchestrator =\n+     kubernetesConf.getHadoopStepsOrchestrator\n+   require(kubernetesConf.hadoopConfDir.isDefined &&\n+     hadoopTestOrchestrator.isDefined, \"Ensure that HADOOP_CONF_DIR is defined\")"
  }],
  "prId": 21669
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "All this seems to just say \"Runs the configuration steps defined by HadoopStepsOrchestrator\" which is a lot shorter.",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-09-05T21:23:33Z",
    "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features\n+\n+import scala.collection.JavaConverters._\n+\n+import io.fabric8.kubernetes.api.model.{ConfigMapBuilder, ContainerBuilder, HasMetadata, PodBuilder}\n+\n+import org.apache.spark.deploy.k8s.{KubernetesConf, SparkPod}\n+import org.apache.spark.deploy.k8s.Config.{KUBERNETES_KERBEROS_KRB5_FILE, KUBERNETES_KERBEROS_PROXY_USER}\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesDriverSpecificConf\n+import org.apache.spark.deploy.k8s.features.hadoopsteps.{HadoopBootstrapUtil, HadoopConfigSpec, HadoopConfigurationStep}\n+import org.apache.spark.internal.Logging\n+\n+ /**\n+  * This is the main method that runs the hadoopConfigurationSteps defined"
  }],
  "prId": 21669
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: more indent",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-09-05T21:25:23Z",
    "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features\n+\n+import scala.collection.JavaConverters._\n+\n+import io.fabric8.kubernetes.api.model.{ConfigMapBuilder, ContainerBuilder, HasMetadata, PodBuilder}\n+\n+import org.apache.spark.deploy.k8s.{KubernetesConf, SparkPod}\n+import org.apache.spark.deploy.k8s.Config.{KUBERNETES_KERBEROS_KRB5_FILE, KUBERNETES_KERBEROS_PROXY_USER}\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesDriverSpecificConf\n+import org.apache.spark.deploy.k8s.features.hadoopsteps.{HadoopBootstrapUtil, HadoopConfigSpec, HadoopConfigurationStep}\n+import org.apache.spark.internal.Logging\n+\n+ /**\n+  * This is the main method that runs the hadoopConfigurationSteps defined\n+  * by the HadoopStepsOrchestrator. These steps are run to modify the\n+  * SparkPod and Kubernetes Resources using the additive method of the feature steps\n+  */\n+private[spark] class HadoopGlobalFeatureDriverStep(\n+  kubernetesConf: KubernetesConf[KubernetesDriverSpecificConf])\n+  extends KubernetesFeatureConfigStep with Logging {\n+   private val hadoopTestOrchestrator =\n+     kubernetesConf.getHadoopStepsOrchestrator\n+   require(kubernetesConf.hadoopConfDir.isDefined &&\n+     hadoopTestOrchestrator.isDefined, \"Ensure that HADOOP_CONF_DIR is defined\")\n+   private val hadoopSteps =\n+     hadoopTestOrchestrator\n+       .map(hto => hto.getHadoopSteps(kubernetesConf.getTokenManager))\n+     .getOrElse(Seq.empty[HadoopConfigurationStep])\n+\n+   var currentHadoopSpec = HadoopConfigSpec(\n+     configMapProperties = Map.empty[String, String],\n+     dtSecret = None,\n+     dtSecretName = KERBEROS_DELEGEGATION_TOKEN_SECRET_NAME,\n+     dtSecretItemKey = None,\n+     jobUserName = None)\n+\n+   for (nextStep <- hadoopSteps) {\n+     currentHadoopSpec = nextStep.configureHadoopSpec(currentHadoopSpec)\n+   }\n+\n+  override def configurePod(pod: SparkPod): SparkPod = {\n+    val hadoopBasedSparkPod = HadoopBootstrapUtil.bootstrapHadoopConfDir(\n+      kubernetesConf.hadoopConfDir.get,\n+      kubernetesConf.getHadoopConfigMapName,\n+      kubernetesConf.getTokenManager,\n+      pod)\n+\n+    val maybeKerberosModification =\n+      for {\n+        secretItemKey <- currentHadoopSpec.dtSecretItemKey\n+        userName <- currentHadoopSpec.jobUserName\n+        krb5fileLocation <- kubernetesConf.get(KUBERNETES_KERBEROS_KRB5_FILE)\n+      } yield {\n+        HadoopBootstrapUtil.bootstrapKerberosPod(\n+          currentHadoopSpec.dtSecretName,\n+          secretItemKey,\n+          userName,\n+          krb5fileLocation,\n+          kubernetesConf.getKRBConfigMapName,\n+          hadoopBasedSparkPod)\n+      }\n+    maybeKerberosModification.getOrElse(\n+      HadoopBootstrapUtil.bootstrapSparkUserPod(\n+        kubernetesConf.getTokenManager.getCurrentUser.getShortUserName,\n+        hadoopBasedSparkPod))\n+  }\n+\n+  override def getAdditionalPodSystemProperties(): Map[String, String] = {\n+    val maybeKerberosConfValues =\n+      for {\n+        secretItemKey <- currentHadoopSpec.dtSecretItemKey\n+        userName <- currentHadoopSpec.jobUserName\n+      } yield {\n+        Map(KERBEROS_KEYTAB_SECRET_NAME -> currentHadoopSpec.dtSecretName,\n+          KERBEROS_KEYTAB_SECRET_KEY -> secretItemKey,\n+          KERBEROS_SPARK_USER_NAME -> userName)\n+    }"
  }],
  "prId": 21669
}, {
  "comments": [{
    "author": {
      "login": "liyinan926"
    },
    "body": "I would suggest passing in the user name so it's easier to unit-test this.",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-10-01T16:14:02Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features\n+\n+import java.io.File\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model.{ConfigMapBuilder, HasMetadata}\n+\n+import org.apache.spark.deploy.k8s.{KubernetesConf, KubernetesUtils, SparkPod}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesDriverSpecificConf\n+import org.apache.spark.deploy.k8s.features.hadoopsteps._\n+import org.apache.spark.internal.Logging\n+\n+ /**\n+  * Runs the necessary Hadoop-based logic based on Kerberos configs and the presence of the\n+  * HADOOP_CONF_DIR. This runs various bootstrap methods defined in HadoopBootstrapUtil.\n+  */\n+private[spark] class HadoopGlobalFeatureDriverStep(\n+    kubernetesConf: KubernetesConf[KubernetesDriverSpecificConf])\n+    extends KubernetesFeatureConfigStep with Logging {\n+\n+    private val conf = kubernetesConf.sparkConf\n+    private val maybePrincipal = conf.get(org.apache.spark.internal.config.PRINCIPAL)\n+    private val maybeKeytab = conf.get(org.apache.spark.internal.config.KEYTAB)\n+    private val maybeExistingSecretName = conf.get(KUBERNETES_KERBEROS_DT_SECRET_NAME)\n+    private val maybeExistingSecretItemKey =\n+      conf.get(KUBERNETES_KERBEROS_DT_SECRET_ITEM_KEY)\n+    private val kubeTokenManager = kubernetesConf.tokenManager\n+    private val isKerberosEnabled = kubeTokenManager.isSecurityEnabled\n+\n+    require(maybeKeytab.forall( _ => isKerberosEnabled ),\n+      \"You must enable Kerberos support if you are specifying a Kerberos Keytab\")\n+\n+    require(maybeExistingSecretName.forall( _ => isKerberosEnabled ),\n+      \"You must enable Kerberos support if you are specifying a Kerberos Secret\")\n+\n+    KubernetesUtils.requireBothOrNeitherDefined(\n+      maybeKeytab,\n+      maybePrincipal,\n+      \"If a Kerberos principal is specified you must also specify a Kerberos keytab\",\n+      \"If a Kerberos keytab is specified you must also specify a Kerberos principal\")\n+\n+    KubernetesUtils.requireBothOrNeitherDefined(\n+      maybeExistingSecretName,\n+      maybeExistingSecretItemKey,\n+      \"If a secret data item-key where the data of the Kerberos Delegation Token is specified\" +\n+        \" you must also specify the name of the secret\",\n+      \"If a secret storing a Kerberos Delegation Token is specified you must also\" +\n+        \" specify the item-key where the data is stored\")\n+\n+    require(kubernetesConf.hadoopConfDir.isDefined, \"Ensure that HADOOP_CONF_DIR is defined\")\n+    private val hadoopConfDir = kubernetesConf.hadoopConfDir.get\n+    private val hadoopConfigurationFiles = kubeTokenManager.getHadoopConfFiles(hadoopConfDir)\n+\n+    // Either use pre-existing secret or login to create new Secret with DT stored within\n+    private val hadoopSpec: Option[KerberosConfigSpec] = (for {\n+      secretName <- maybeExistingSecretName\n+      secretItemKey <- maybeExistingSecretItemKey\n+    } yield {\n+      KerberosConfigSpec(\n+         dtSecret = None,\n+         dtSecretName = secretName,\n+         dtSecretItemKey = secretItemKey,\n+         jobUserName = kubeTokenManager.getCurrentUser.getShortUserName)"
  }],
  "prId": 21669
}, {
  "comments": [{
    "author": {
      "login": "liyinan926"
    },
    "body": "It doesn't look like this is really used in the class except in some checks.",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-10-01T16:31:43Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features\n+\n+import java.io.File\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model.{ConfigMapBuilder, HasMetadata}\n+\n+import org.apache.spark.deploy.k8s.{KubernetesConf, KubernetesUtils, SparkPod}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesDriverSpecificConf\n+import org.apache.spark.deploy.k8s.features.hadoopsteps._\n+import org.apache.spark.internal.Logging\n+\n+ /**\n+  * Runs the necessary Hadoop-based logic based on Kerberos configs and the presence of the\n+  * HADOOP_CONF_DIR. This runs various bootstrap methods defined in HadoopBootstrapUtil.\n+  */\n+private[spark] class HadoopGlobalFeatureDriverStep(\n+    kubernetesConf: KubernetesConf[KubernetesDriverSpecificConf])\n+    extends KubernetesFeatureConfigStep with Logging {\n+\n+    private val conf = kubernetesConf.sparkConf\n+    private val maybePrincipal = conf.get(org.apache.spark.internal.config.PRINCIPAL)\n+    private val maybeKeytab = conf.get(org.apache.spark.internal.config.KEYTAB)"
  }, {
    "author": {
      "login": "ifilonenko"
    },
    "body": "Correct. For the purpose of giving checks to the user",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-10-01T19:53:55Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features\n+\n+import java.io.File\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model.{ConfigMapBuilder, HasMetadata}\n+\n+import org.apache.spark.deploy.k8s.{KubernetesConf, KubernetesUtils, SparkPod}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesDriverSpecificConf\n+import org.apache.spark.deploy.k8s.features.hadoopsteps._\n+import org.apache.spark.internal.Logging\n+\n+ /**\n+  * Runs the necessary Hadoop-based logic based on Kerberos configs and the presence of the\n+  * HADOOP_CONF_DIR. This runs various bootstrap methods defined in HadoopBootstrapUtil.\n+  */\n+private[spark] class HadoopGlobalFeatureDriverStep(\n+    kubernetesConf: KubernetesConf[KubernetesDriverSpecificConf])\n+    extends KubernetesFeatureConfigStep with Logging {\n+\n+    private val conf = kubernetesConf.sparkConf\n+    private val maybePrincipal = conf.get(org.apache.spark.internal.config.PRINCIPAL)\n+    private val maybeKeytab = conf.get(org.apache.spark.internal.config.KEYTAB)"
  }],
  "prId": 21669
}, {
  "comments": [{
    "author": {
      "login": "liyinan926"
    },
    "body": "Can we just call this `KerberosConfFeatureDriverStep` to be consistent with the one for the executor?",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-10-01T16:52:30Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features\n+\n+import java.io.File\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model.{ConfigMapBuilder, HasMetadata}\n+\n+import org.apache.spark.deploy.k8s.{KubernetesConf, KubernetesUtils, SparkPod}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesDriverSpecificConf\n+import org.apache.spark.deploy.k8s.features.hadoopsteps._\n+import org.apache.spark.internal.Logging\n+\n+ /**\n+  * Runs the necessary Hadoop-based logic based on Kerberos configs and the presence of the\n+  * HADOOP_CONF_DIR. This runs various bootstrap methods defined in HadoopBootstrapUtil.\n+  */\n+private[spark] class HadoopGlobalFeatureDriverStep("
  }],
  "prId": 21669
}, {
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "Can bundle secret name and secret key in a case class so that for... yield isn't necessary.",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-10-01T21:01:53Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features\n+\n+import java.io.File\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model.{ConfigMapBuilder, HasMetadata}\n+\n+import org.apache.spark.deploy.k8s.{KubernetesConf, KubernetesUtils, SparkPod}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesDriverSpecificConf\n+import org.apache.spark.deploy.k8s.features.hadoopsteps._\n+import org.apache.spark.internal.Logging\n+\n+ /**\n+  * Runs the necessary Hadoop-based logic based on Kerberos configs and the presence of the\n+  * HADOOP_CONF_DIR. This runs various bootstrap methods defined in HadoopBootstrapUtil.\n+  */\n+private[spark] class HadoopGlobalFeatureDriverStep(\n+    kubernetesConf: KubernetesConf[KubernetesDriverSpecificConf])\n+    extends KubernetesFeatureConfigStep with Logging {\n+\n+    private val conf = kubernetesConf.sparkConf\n+    private val maybePrincipal = conf.get(org.apache.spark.internal.config.PRINCIPAL)\n+    private val maybeKeytab = conf.get(org.apache.spark.internal.config.KEYTAB)\n+    private val maybeExistingSecretName = conf.get(KUBERNETES_KERBEROS_DT_SECRET_NAME)\n+    private val maybeExistingSecretItemKey =\n+      conf.get(KUBERNETES_KERBEROS_DT_SECRET_ITEM_KEY)\n+    private val kubeTokenManager = kubernetesConf.tokenManager\n+    private val isKerberosEnabled = kubeTokenManager.isSecurityEnabled\n+\n+    require(maybeKeytab.forall( _ => isKerberosEnabled ),\n+      \"You must enable Kerberos support if you are specifying a Kerberos Keytab\")\n+\n+    require(maybeExistingSecretName.forall( _ => isKerberosEnabled ),\n+      \"You must enable Kerberos support if you are specifying a Kerberos Secret\")\n+\n+    KubernetesUtils.requireBothOrNeitherDefined(\n+      maybeKeytab,\n+      maybePrincipal,\n+      \"If a Kerberos principal is specified you must also specify a Kerberos keytab\",\n+      \"If a Kerberos keytab is specified you must also specify a Kerberos principal\")\n+\n+    KubernetesUtils.requireBothOrNeitherDefined(\n+      maybeExistingSecretName,\n+      maybeExistingSecretItemKey,\n+      \"If a secret data item-key where the data of the Kerberos Delegation Token is specified\" +\n+        \" you must also specify the name of the secret\",\n+      \"If a secret storing a Kerberos Delegation Token is specified you must also\" +\n+        \" specify the item-key where the data is stored\")\n+\n+    require(kubernetesConf.hadoopConfDir.isDefined, \"Ensure that HADOOP_CONF_DIR is defined\")\n+    private val hadoopConfDir = kubernetesConf.hadoopConfDir.get\n+    private val hadoopConfigurationFiles = kubeTokenManager.getHadoopConfFiles(hadoopConfDir)\n+\n+    // Either use pre-existing secret or login to create new Secret with DT stored within\n+    private val hadoopSpec: Option[KerberosConfigSpec] = (for {"
  }, {
    "author": {
      "login": "ifilonenko"
    },
    "body": "In this specific case, the `for..yield` is quite clear IMHO. I think it is easier to parse. \r\nI would prefer to leave it.",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-10-07T02:14:07Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features\n+\n+import java.io.File\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model.{ConfigMapBuilder, HasMetadata}\n+\n+import org.apache.spark.deploy.k8s.{KubernetesConf, KubernetesUtils, SparkPod}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesDriverSpecificConf\n+import org.apache.spark.deploy.k8s.features.hadoopsteps._\n+import org.apache.spark.internal.Logging\n+\n+ /**\n+  * Runs the necessary Hadoop-based logic based on Kerberos configs and the presence of the\n+  * HADOOP_CONF_DIR. This runs various bootstrap methods defined in HadoopBootstrapUtil.\n+  */\n+private[spark] class HadoopGlobalFeatureDriverStep(\n+    kubernetesConf: KubernetesConf[KubernetesDriverSpecificConf])\n+    extends KubernetesFeatureConfigStep with Logging {\n+\n+    private val conf = kubernetesConf.sparkConf\n+    private val maybePrincipal = conf.get(org.apache.spark.internal.config.PRINCIPAL)\n+    private val maybeKeytab = conf.get(org.apache.spark.internal.config.KEYTAB)\n+    private val maybeExistingSecretName = conf.get(KUBERNETES_KERBEROS_DT_SECRET_NAME)\n+    private val maybeExistingSecretItemKey =\n+      conf.get(KUBERNETES_KERBEROS_DT_SECRET_ITEM_KEY)\n+    private val kubeTokenManager = kubernetesConf.tokenManager\n+    private val isKerberosEnabled = kubeTokenManager.isSecurityEnabled\n+\n+    require(maybeKeytab.forall( _ => isKerberosEnabled ),\n+      \"You must enable Kerberos support if you are specifying a Kerberos Keytab\")\n+\n+    require(maybeExistingSecretName.forall( _ => isKerberosEnabled ),\n+      \"You must enable Kerberos support if you are specifying a Kerberos Secret\")\n+\n+    KubernetesUtils.requireBothOrNeitherDefined(\n+      maybeKeytab,\n+      maybePrincipal,\n+      \"If a Kerberos principal is specified you must also specify a Kerberos keytab\",\n+      \"If a Kerberos keytab is specified you must also specify a Kerberos principal\")\n+\n+    KubernetesUtils.requireBothOrNeitherDefined(\n+      maybeExistingSecretName,\n+      maybeExistingSecretItemKey,\n+      \"If a secret data item-key where the data of the Kerberos Delegation Token is specified\" +\n+        \" you must also specify the name of the secret\",\n+      \"If a secret storing a Kerberos Delegation Token is specified you must also\" +\n+        \" specify the item-key where the data is stored\")\n+\n+    require(kubernetesConf.hadoopConfDir.isDefined, \"Ensure that HADOOP_CONF_DIR is defined\")\n+    private val hadoopConfDir = kubernetesConf.hadoopConfDir.get\n+    private val hadoopConfigurationFiles = kubeTokenManager.getHadoopConfFiles(hadoopConfDir)\n+\n+    // Either use pre-existing secret or login to create new Secret with DT stored within\n+    private val hadoopSpec: Option[KerberosConfigSpec] = (for {"
  }],
  "prId": 21669
}, {
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "Can we avoid for... yield - again can use a tuple since both objects have to be present for any of this stuff to work.",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-10-01T21:02:34Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features\n+\n+import java.io.File\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model.{ConfigMapBuilder, HasMetadata}\n+\n+import org.apache.spark.deploy.k8s.{KubernetesConf, KubernetesUtils, SparkPod}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesDriverSpecificConf\n+import org.apache.spark.deploy.k8s.features.hadoopsteps._\n+import org.apache.spark.internal.Logging\n+\n+ /**\n+  * Runs the necessary Hadoop-based logic based on Kerberos configs and the presence of the\n+  * HADOOP_CONF_DIR. This runs various bootstrap methods defined in HadoopBootstrapUtil.\n+  */\n+private[spark] class HadoopGlobalFeatureDriverStep(\n+    kubernetesConf: KubernetesConf[KubernetesDriverSpecificConf])\n+    extends KubernetesFeatureConfigStep with Logging {\n+\n+    private val conf = kubernetesConf.sparkConf\n+    private val maybePrincipal = conf.get(org.apache.spark.internal.config.PRINCIPAL)\n+    private val maybeKeytab = conf.get(org.apache.spark.internal.config.KEYTAB)\n+    private val maybeExistingSecretName = conf.get(KUBERNETES_KERBEROS_DT_SECRET_NAME)\n+    private val maybeExistingSecretItemKey =\n+      conf.get(KUBERNETES_KERBEROS_DT_SECRET_ITEM_KEY)\n+    private val kubeTokenManager = kubernetesConf.tokenManager\n+    private val isKerberosEnabled = kubeTokenManager.isSecurityEnabled\n+\n+    require(maybeKeytab.forall( _ => isKerberosEnabled ),\n+      \"You must enable Kerberos support if you are specifying a Kerberos Keytab\")\n+\n+    require(maybeExistingSecretName.forall( _ => isKerberosEnabled ),\n+      \"You must enable Kerberos support if you are specifying a Kerberos Secret\")\n+\n+    KubernetesUtils.requireBothOrNeitherDefined(\n+      maybeKeytab,\n+      maybePrincipal,\n+      \"If a Kerberos principal is specified you must also specify a Kerberos keytab\",\n+      \"If a Kerberos keytab is specified you must also specify a Kerberos principal\")\n+\n+    KubernetesUtils.requireBothOrNeitherDefined(\n+      maybeExistingSecretName,\n+      maybeExistingSecretItemKey,\n+      \"If a secret data item-key where the data of the Kerberos Delegation Token is specified\" +\n+        \" you must also specify the name of the secret\",\n+      \"If a secret storing a Kerberos Delegation Token is specified you must also\" +\n+        \" specify the item-key where the data is stored\")\n+\n+    require(kubernetesConf.hadoopConfDir.isDefined, \"Ensure that HADOOP_CONF_DIR is defined\")\n+    private val hadoopConfDir = kubernetesConf.hadoopConfDir.get\n+    private val hadoopConfigurationFiles = kubeTokenManager.getHadoopConfFiles(hadoopConfDir)\n+\n+    // Either use pre-existing secret or login to create new Secret with DT stored within\n+    private val hadoopSpec: Option[KerberosConfigSpec] = (for {\n+      secretName <- maybeExistingSecretName\n+      secretItemKey <- maybeExistingSecretItemKey\n+    } yield {\n+      KerberosConfigSpec(\n+         dtSecret = None,\n+         dtSecretName = secretName,\n+         dtSecretItemKey = secretItemKey,\n+         jobUserName = kubeTokenManager.getCurrentUser.getShortUserName)\n+    }).orElse(\n+      if (isKerberosEnabled) {\n+         Some(HadoopKerberosLogin.buildSpec(\n+             conf,\n+             kubernetesConf.appResourceNamePrefix,\n+             kubeTokenManager))\n+       } else None )\n+\n+    override def configurePod(pod: SparkPod): SparkPod = {\n+      val hadoopBasedSparkPod = HadoopBootstrapUtil.bootstrapHadoopConfDir(\n+        hadoopConfDir,\n+        kubernetesConf.hadoopConfigMapName,\n+        kubeTokenManager,\n+        pod)\n+      (for {\n+        hSpec <- hadoopSpec"
  }],
  "prId": 21669
}, {
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "Specifically here you can do `hSpec.dtSecret.filter( _ => hadoopSpec.isDefined)`",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-10-04T00:10:09Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features\n+\n+import java.io.File\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model.{ConfigMapBuilder, HasMetadata}\n+\n+import org.apache.spark.deploy.k8s.{KubernetesConf, KubernetesUtils, SparkPod}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesDriverSpecificConf\n+import org.apache.spark.deploy.k8s.features.hadoopsteps._\n+import org.apache.spark.internal.Logging\n+\n+ /**\n+  * Runs the necessary Hadoop-based logic based on Kerberos configs and the presence of the\n+  * HADOOP_CONF_DIR. This runs various bootstrap methods defined in HadoopBootstrapUtil.\n+  */\n+private[spark] class HadoopGlobalFeatureDriverStep(\n+    kubernetesConf: KubernetesConf[KubernetesDriverSpecificConf])\n+    extends KubernetesFeatureConfigStep with Logging {\n+\n+    private val conf = kubernetesConf.sparkConf\n+    private val maybePrincipal = conf.get(org.apache.spark.internal.config.PRINCIPAL)\n+    private val maybeKeytab = conf.get(org.apache.spark.internal.config.KEYTAB)\n+    private val maybeExistingSecretName = conf.get(KUBERNETES_KERBEROS_DT_SECRET_NAME)\n+    private val maybeExistingSecretItemKey =\n+      conf.get(KUBERNETES_KERBEROS_DT_SECRET_ITEM_KEY)\n+    private val kubeTokenManager = kubernetesConf.tokenManager\n+    private val isKerberosEnabled = kubeTokenManager.isSecurityEnabled\n+\n+    require(maybeKeytab.forall( _ => isKerberosEnabled ),\n+      \"You must enable Kerberos support if you are specifying a Kerberos Keytab\")\n+\n+    require(maybeExistingSecretName.forall( _ => isKerberosEnabled ),\n+      \"You must enable Kerberos support if you are specifying a Kerberos Secret\")\n+\n+    KubernetesUtils.requireBothOrNeitherDefined(\n+      maybeKeytab,\n+      maybePrincipal,\n+      \"If a Kerberos principal is specified you must also specify a Kerberos keytab\",\n+      \"If a Kerberos keytab is specified you must also specify a Kerberos principal\")\n+\n+    KubernetesUtils.requireBothOrNeitherDefined(\n+      maybeExistingSecretName,\n+      maybeExistingSecretItemKey,\n+      \"If a secret data item-key where the data of the Kerberos Delegation Token is specified\" +\n+        \" you must also specify the name of the secret\",\n+      \"If a secret storing a Kerberos Delegation Token is specified you must also\" +\n+        \" specify the item-key where the data is stored\")\n+\n+    require(kubernetesConf.hadoopConfDir.isDefined, \"Ensure that HADOOP_CONF_DIR is defined\")\n+    private val hadoopConfDir = kubernetesConf.hadoopConfDir.get\n+    private val hadoopConfigurationFiles = kubeTokenManager.getHadoopConfFiles(hadoopConfDir)\n+\n+    // Either use pre-existing secret or login to create new Secret with DT stored within\n+    private val hadoopSpec: Option[KerberosConfigSpec] = (for {\n+      secretName <- maybeExistingSecretName\n+      secretItemKey <- maybeExistingSecretItemKey\n+    } yield {\n+      KerberosConfigSpec(\n+         dtSecret = None,\n+         dtSecretName = secretName,\n+         dtSecretItemKey = secretItemKey,\n+         jobUserName = kubeTokenManager.getCurrentUser.getShortUserName)\n+    }).orElse(\n+      if (isKerberosEnabled) {\n+         Some(HadoopKerberosLogin.buildSpec(\n+             conf,\n+             kubernetesConf.appResourceNamePrefix,\n+             kubeTokenManager))\n+       } else None )\n+\n+    override def configurePod(pod: SparkPod): SparkPod = {\n+      val hadoopBasedSparkPod = HadoopBootstrapUtil.bootstrapHadoopConfDir(\n+        hadoopConfDir,\n+        kubernetesConf.hadoopConfigMapName,\n+        kubeTokenManager,\n+        pod)\n+      (for {\n+        hSpec <- hadoopSpec\n+        krb5fileLocation <- kubernetesConf.get(KUBERNETES_KERBEROS_KRB5_FILE)\n+      } yield {\n+        HadoopBootstrapUtil.bootstrapKerberosPod(\n+            hSpec.dtSecretName,\n+            hSpec.dtSecretItemKey,\n+            hSpec.jobUserName,\n+            krb5fileLocation,\n+            kubernetesConf.kRBConfigMapName,\n+            hadoopBasedSparkPod)\n+      }).getOrElse(\n+        HadoopBootstrapUtil.bootstrapSparkUserPod(\n+          kubeTokenManager.getCurrentUser.getShortUserName,\n+          hadoopBasedSparkPod))\n+    }\n+\n+    override def getAdditionalPodSystemProperties(): Map[String, String] = {\n+      val resolvedConfValues = hadoopSpec.map{ hSpec =>\n+         Map(KERBEROS_KEYTAB_SECRET_NAME -> hSpec.dtSecretName,\n+            KERBEROS_KEYTAB_SECRET_KEY -> hSpec.dtSecretItemKey,\n+            KERBEROS_SPARK_USER_NAME -> hSpec.jobUserName)\n+      }.getOrElse(\n+          Map(KERBEROS_SPARK_USER_NAME ->\n+            kubernetesConf.tokenManager.getCurrentUser.getShortUserName))\n+      Map(HADOOP_CONFIG_MAP_SPARK_CONF_NAME -> kubernetesConf.hadoopConfigMapName,\n+          HADOOP_CONF_DIR_LOC -> kubernetesConf.hadoopConfDir.get) ++ resolvedConfValues\n+    }\n+\n+    override def getAdditionalKubernetesResources(): Seq[HasMetadata] = {\n+      val krb5ConfigMap = kubernetesConf.get(KUBERNETES_KERBEROS_KRB5_FILE)\n+        .map(fileLocation => HadoopBootstrapUtil.buildkrb5ConfigMap(\n+          kubernetesConf.kRBConfigMapName,\n+          fileLocation))\n+      val kerberosDTSecret = for {"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "In fact here you can just do `val dtSecret = hadoopSpec.map(_.dtSecret)`\r\n\r\nMost places where I've seen `for...yield` here can be replaced with `option.map { ... }`.",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-10-05T18:19:53Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features\n+\n+import java.io.File\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model.{ConfigMapBuilder, HasMetadata}\n+\n+import org.apache.spark.deploy.k8s.{KubernetesConf, KubernetesUtils, SparkPod}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesDriverSpecificConf\n+import org.apache.spark.deploy.k8s.features.hadoopsteps._\n+import org.apache.spark.internal.Logging\n+\n+ /**\n+  * Runs the necessary Hadoop-based logic based on Kerberos configs and the presence of the\n+  * HADOOP_CONF_DIR. This runs various bootstrap methods defined in HadoopBootstrapUtil.\n+  */\n+private[spark] class HadoopGlobalFeatureDriverStep(\n+    kubernetesConf: KubernetesConf[KubernetesDriverSpecificConf])\n+    extends KubernetesFeatureConfigStep with Logging {\n+\n+    private val conf = kubernetesConf.sparkConf\n+    private val maybePrincipal = conf.get(org.apache.spark.internal.config.PRINCIPAL)\n+    private val maybeKeytab = conf.get(org.apache.spark.internal.config.KEYTAB)\n+    private val maybeExistingSecretName = conf.get(KUBERNETES_KERBEROS_DT_SECRET_NAME)\n+    private val maybeExistingSecretItemKey =\n+      conf.get(KUBERNETES_KERBEROS_DT_SECRET_ITEM_KEY)\n+    private val kubeTokenManager = kubernetesConf.tokenManager\n+    private val isKerberosEnabled = kubeTokenManager.isSecurityEnabled\n+\n+    require(maybeKeytab.forall( _ => isKerberosEnabled ),\n+      \"You must enable Kerberos support if you are specifying a Kerberos Keytab\")\n+\n+    require(maybeExistingSecretName.forall( _ => isKerberosEnabled ),\n+      \"You must enable Kerberos support if you are specifying a Kerberos Secret\")\n+\n+    KubernetesUtils.requireBothOrNeitherDefined(\n+      maybeKeytab,\n+      maybePrincipal,\n+      \"If a Kerberos principal is specified you must also specify a Kerberos keytab\",\n+      \"If a Kerberos keytab is specified you must also specify a Kerberos principal\")\n+\n+    KubernetesUtils.requireBothOrNeitherDefined(\n+      maybeExistingSecretName,\n+      maybeExistingSecretItemKey,\n+      \"If a secret data item-key where the data of the Kerberos Delegation Token is specified\" +\n+        \" you must also specify the name of the secret\",\n+      \"If a secret storing a Kerberos Delegation Token is specified you must also\" +\n+        \" specify the item-key where the data is stored\")\n+\n+    require(kubernetesConf.hadoopConfDir.isDefined, \"Ensure that HADOOP_CONF_DIR is defined\")\n+    private val hadoopConfDir = kubernetesConf.hadoopConfDir.get\n+    private val hadoopConfigurationFiles = kubeTokenManager.getHadoopConfFiles(hadoopConfDir)\n+\n+    // Either use pre-existing secret or login to create new Secret with DT stored within\n+    private val hadoopSpec: Option[KerberosConfigSpec] = (for {\n+      secretName <- maybeExistingSecretName\n+      secretItemKey <- maybeExistingSecretItemKey\n+    } yield {\n+      KerberosConfigSpec(\n+         dtSecret = None,\n+         dtSecretName = secretName,\n+         dtSecretItemKey = secretItemKey,\n+         jobUserName = kubeTokenManager.getCurrentUser.getShortUserName)\n+    }).orElse(\n+      if (isKerberosEnabled) {\n+         Some(HadoopKerberosLogin.buildSpec(\n+             conf,\n+             kubernetesConf.appResourceNamePrefix,\n+             kubeTokenManager))\n+       } else None )\n+\n+    override def configurePod(pod: SparkPod): SparkPod = {\n+      val hadoopBasedSparkPod = HadoopBootstrapUtil.bootstrapHadoopConfDir(\n+        hadoopConfDir,\n+        kubernetesConf.hadoopConfigMapName,\n+        kubeTokenManager,\n+        pod)\n+      (for {\n+        hSpec <- hadoopSpec\n+        krb5fileLocation <- kubernetesConf.get(KUBERNETES_KERBEROS_KRB5_FILE)\n+      } yield {\n+        HadoopBootstrapUtil.bootstrapKerberosPod(\n+            hSpec.dtSecretName,\n+            hSpec.dtSecretItemKey,\n+            hSpec.jobUserName,\n+            krb5fileLocation,\n+            kubernetesConf.kRBConfigMapName,\n+            hadoopBasedSparkPod)\n+      }).getOrElse(\n+        HadoopBootstrapUtil.bootstrapSparkUserPod(\n+          kubeTokenManager.getCurrentUser.getShortUserName,\n+          hadoopBasedSparkPod))\n+    }\n+\n+    override def getAdditionalPodSystemProperties(): Map[String, String] = {\n+      val resolvedConfValues = hadoopSpec.map{ hSpec =>\n+         Map(KERBEROS_KEYTAB_SECRET_NAME -> hSpec.dtSecretName,\n+            KERBEROS_KEYTAB_SECRET_KEY -> hSpec.dtSecretItemKey,\n+            KERBEROS_SPARK_USER_NAME -> hSpec.jobUserName)\n+      }.getOrElse(\n+          Map(KERBEROS_SPARK_USER_NAME ->\n+            kubernetesConf.tokenManager.getCurrentUser.getShortUserName))\n+      Map(HADOOP_CONFIG_MAP_SPARK_CONF_NAME -> kubernetesConf.hadoopConfigMapName,\n+          HADOOP_CONF_DIR_LOC -> kubernetesConf.hadoopConfDir.get) ++ resolvedConfValues\n+    }\n+\n+    override def getAdditionalKubernetesResources(): Seq[HasMetadata] = {\n+      val krb5ConfigMap = kubernetesConf.get(KUBERNETES_KERBEROS_KRB5_FILE)\n+        .map(fileLocation => HadoopBootstrapUtil.buildkrb5ConfigMap(\n+          kubernetesConf.kRBConfigMapName,\n+          fileLocation))\n+      val kerberosDTSecret = for {"
  }, {
    "author": {
      "login": "ifilonenko"
    },
    "body": "The above recommendations give a `Option[Option[Secret]]`. I was aware of this, but the below resolution of `kerberosDTSecret.toSeq` would not be resolved correctly with the above recommendations, as it is looking for `Option[Secret]`. As such, I think the `for...yield` is the correct way of approaching this and is idiomatic in Scala.",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-10-07T02:16:35Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features\n+\n+import java.io.File\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model.{ConfigMapBuilder, HasMetadata}\n+\n+import org.apache.spark.deploy.k8s.{KubernetesConf, KubernetesUtils, SparkPod}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesDriverSpecificConf\n+import org.apache.spark.deploy.k8s.features.hadoopsteps._\n+import org.apache.spark.internal.Logging\n+\n+ /**\n+  * Runs the necessary Hadoop-based logic based on Kerberos configs and the presence of the\n+  * HADOOP_CONF_DIR. This runs various bootstrap methods defined in HadoopBootstrapUtil.\n+  */\n+private[spark] class HadoopGlobalFeatureDriverStep(\n+    kubernetesConf: KubernetesConf[KubernetesDriverSpecificConf])\n+    extends KubernetesFeatureConfigStep with Logging {\n+\n+    private val conf = kubernetesConf.sparkConf\n+    private val maybePrincipal = conf.get(org.apache.spark.internal.config.PRINCIPAL)\n+    private val maybeKeytab = conf.get(org.apache.spark.internal.config.KEYTAB)\n+    private val maybeExistingSecretName = conf.get(KUBERNETES_KERBEROS_DT_SECRET_NAME)\n+    private val maybeExistingSecretItemKey =\n+      conf.get(KUBERNETES_KERBEROS_DT_SECRET_ITEM_KEY)\n+    private val kubeTokenManager = kubernetesConf.tokenManager\n+    private val isKerberosEnabled = kubeTokenManager.isSecurityEnabled\n+\n+    require(maybeKeytab.forall( _ => isKerberosEnabled ),\n+      \"You must enable Kerberos support if you are specifying a Kerberos Keytab\")\n+\n+    require(maybeExistingSecretName.forall( _ => isKerberosEnabled ),\n+      \"You must enable Kerberos support if you are specifying a Kerberos Secret\")\n+\n+    KubernetesUtils.requireBothOrNeitherDefined(\n+      maybeKeytab,\n+      maybePrincipal,\n+      \"If a Kerberos principal is specified you must also specify a Kerberos keytab\",\n+      \"If a Kerberos keytab is specified you must also specify a Kerberos principal\")\n+\n+    KubernetesUtils.requireBothOrNeitherDefined(\n+      maybeExistingSecretName,\n+      maybeExistingSecretItemKey,\n+      \"If a secret data item-key where the data of the Kerberos Delegation Token is specified\" +\n+        \" you must also specify the name of the secret\",\n+      \"If a secret storing a Kerberos Delegation Token is specified you must also\" +\n+        \" specify the item-key where the data is stored\")\n+\n+    require(kubernetesConf.hadoopConfDir.isDefined, \"Ensure that HADOOP_CONF_DIR is defined\")\n+    private val hadoopConfDir = kubernetesConf.hadoopConfDir.get\n+    private val hadoopConfigurationFiles = kubeTokenManager.getHadoopConfFiles(hadoopConfDir)\n+\n+    // Either use pre-existing secret or login to create new Secret with DT stored within\n+    private val hadoopSpec: Option[KerberosConfigSpec] = (for {\n+      secretName <- maybeExistingSecretName\n+      secretItemKey <- maybeExistingSecretItemKey\n+    } yield {\n+      KerberosConfigSpec(\n+         dtSecret = None,\n+         dtSecretName = secretName,\n+         dtSecretItemKey = secretItemKey,\n+         jobUserName = kubeTokenManager.getCurrentUser.getShortUserName)\n+    }).orElse(\n+      if (isKerberosEnabled) {\n+         Some(HadoopKerberosLogin.buildSpec(\n+             conf,\n+             kubernetesConf.appResourceNamePrefix,\n+             kubeTokenManager))\n+       } else None )\n+\n+    override def configurePod(pod: SparkPod): SparkPod = {\n+      val hadoopBasedSparkPod = HadoopBootstrapUtil.bootstrapHadoopConfDir(\n+        hadoopConfDir,\n+        kubernetesConf.hadoopConfigMapName,\n+        kubeTokenManager,\n+        pod)\n+      (for {\n+        hSpec <- hadoopSpec\n+        krb5fileLocation <- kubernetesConf.get(KUBERNETES_KERBEROS_KRB5_FILE)\n+      } yield {\n+        HadoopBootstrapUtil.bootstrapKerberosPod(\n+            hSpec.dtSecretName,\n+            hSpec.dtSecretItemKey,\n+            hSpec.jobUserName,\n+            krb5fileLocation,\n+            kubernetesConf.kRBConfigMapName,\n+            hadoopBasedSparkPod)\n+      }).getOrElse(\n+        HadoopBootstrapUtil.bootstrapSparkUserPod(\n+          kubeTokenManager.getCurrentUser.getShortUserName,\n+          hadoopBasedSparkPod))\n+    }\n+\n+    override def getAdditionalPodSystemProperties(): Map[String, String] = {\n+      val resolvedConfValues = hadoopSpec.map{ hSpec =>\n+         Map(KERBEROS_KEYTAB_SECRET_NAME -> hSpec.dtSecretName,\n+            KERBEROS_KEYTAB_SECRET_KEY -> hSpec.dtSecretItemKey,\n+            KERBEROS_SPARK_USER_NAME -> hSpec.jobUserName)\n+      }.getOrElse(\n+          Map(KERBEROS_SPARK_USER_NAME ->\n+            kubernetesConf.tokenManager.getCurrentUser.getShortUserName))\n+      Map(HADOOP_CONFIG_MAP_SPARK_CONF_NAME -> kubernetesConf.hadoopConfigMapName,\n+          HADOOP_CONF_DIR_LOC -> kubernetesConf.hadoopConfDir.get) ++ resolvedConfValues\n+    }\n+\n+    override def getAdditionalKubernetesResources(): Seq[HasMetadata] = {\n+      val krb5ConfigMap = kubernetesConf.get(KUBERNETES_KERBEROS_KRB5_FILE)\n+        .map(fileLocation => HadoopBootstrapUtil.buildkrb5ConfigMap(\n+          kubernetesConf.kRBConfigMapName,\n+          fileLocation))\n+      val kerberosDTSecret = for {"
  }],
  "prId": 21669
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "I prefer `!maybeKeytab.isDefined || isKerberosEnabled`. Also below.",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-10-05T18:13:58Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features\n+\n+import java.io.File\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model.{ConfigMapBuilder, HasMetadata}\n+\n+import org.apache.spark.deploy.k8s.{KubernetesConf, KubernetesUtils, SparkPod}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesDriverSpecificConf\n+import org.apache.spark.deploy.k8s.features.hadoopsteps._\n+import org.apache.spark.internal.Logging\n+\n+ /**\n+  * Runs the necessary Hadoop-based logic based on Kerberos configs and the presence of the\n+  * HADOOP_CONF_DIR. This runs various bootstrap methods defined in HadoopBootstrapUtil.\n+  */\n+private[spark] class HadoopGlobalFeatureDriverStep(\n+    kubernetesConf: KubernetesConf[KubernetesDriverSpecificConf])\n+    extends KubernetesFeatureConfigStep with Logging {\n+\n+    private val conf = kubernetesConf.sparkConf\n+    private val maybePrincipal = conf.get(org.apache.spark.internal.config.PRINCIPAL)\n+    private val maybeKeytab = conf.get(org.apache.spark.internal.config.KEYTAB)\n+    private val maybeExistingSecretName = conf.get(KUBERNETES_KERBEROS_DT_SECRET_NAME)\n+    private val maybeExistingSecretItemKey =\n+      conf.get(KUBERNETES_KERBEROS_DT_SECRET_ITEM_KEY)\n+    private val kubeTokenManager = kubernetesConf.tokenManager\n+    private val isKerberosEnabled = kubeTokenManager.isSecurityEnabled\n+\n+    require(maybeKeytab.forall( _ => isKerberosEnabled ),"
  }],
  "prId": 21669
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: style is `.map { foo => bar }` (also in other places)",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-10-05T18:16:53Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features\n+\n+import java.io.File\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model.{ConfigMapBuilder, HasMetadata}\n+\n+import org.apache.spark.deploy.k8s.{KubernetesConf, KubernetesUtils, SparkPod}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesDriverSpecificConf\n+import org.apache.spark.deploy.k8s.features.hadoopsteps._\n+import org.apache.spark.internal.Logging\n+\n+ /**\n+  * Runs the necessary Hadoop-based logic based on Kerberos configs and the presence of the\n+  * HADOOP_CONF_DIR. This runs various bootstrap methods defined in HadoopBootstrapUtil.\n+  */\n+private[spark] class HadoopGlobalFeatureDriverStep(\n+    kubernetesConf: KubernetesConf[KubernetesDriverSpecificConf])\n+    extends KubernetesFeatureConfigStep with Logging {\n+\n+    private val conf = kubernetesConf.sparkConf\n+    private val maybePrincipal = conf.get(org.apache.spark.internal.config.PRINCIPAL)\n+    private val maybeKeytab = conf.get(org.apache.spark.internal.config.KEYTAB)\n+    private val maybeExistingSecretName = conf.get(KUBERNETES_KERBEROS_DT_SECRET_NAME)\n+    private val maybeExistingSecretItemKey =\n+      conf.get(KUBERNETES_KERBEROS_DT_SECRET_ITEM_KEY)\n+    private val kubeTokenManager = kubernetesConf.tokenManager\n+    private val isKerberosEnabled = kubeTokenManager.isSecurityEnabled\n+\n+    require(maybeKeytab.forall( _ => isKerberosEnabled ),\n+      \"You must enable Kerberos support if you are specifying a Kerberos Keytab\")\n+\n+    require(maybeExistingSecretName.forall( _ => isKerberosEnabled ),\n+      \"You must enable Kerberos support if you are specifying a Kerberos Secret\")\n+\n+    KubernetesUtils.requireBothOrNeitherDefined(\n+      maybeKeytab,\n+      maybePrincipal,\n+      \"If a Kerberos principal is specified you must also specify a Kerberos keytab\",\n+      \"If a Kerberos keytab is specified you must also specify a Kerberos principal\")\n+\n+    KubernetesUtils.requireBothOrNeitherDefined(\n+      maybeExistingSecretName,\n+      maybeExistingSecretItemKey,\n+      \"If a secret data item-key where the data of the Kerberos Delegation Token is specified\" +\n+        \" you must also specify the name of the secret\",\n+      \"If a secret storing a Kerberos Delegation Token is specified you must also\" +\n+        \" specify the item-key where the data is stored\")\n+\n+    require(kubernetesConf.hadoopConfDir.isDefined, \"Ensure that HADOOP_CONF_DIR is defined\")\n+    private val hadoopConfDir = kubernetesConf.hadoopConfDir.get\n+    private val hadoopConfigurationFiles = kubeTokenManager.getHadoopConfFiles(hadoopConfDir)\n+\n+    // Either use pre-existing secret or login to create new Secret with DT stored within\n+    private val hadoopSpec: Option[KerberosConfigSpec] = (for {\n+      secretName <- maybeExistingSecretName\n+      secretItemKey <- maybeExistingSecretItemKey\n+    } yield {\n+      KerberosConfigSpec(\n+         dtSecret = None,\n+         dtSecretName = secretName,\n+         dtSecretItemKey = secretItemKey,\n+         jobUserName = kubeTokenManager.getCurrentUser.getShortUserName)\n+    }).orElse(\n+      if (isKerberosEnabled) {\n+         Some(HadoopKerberosLogin.buildSpec(\n+             conf,\n+             kubernetesConf.appResourceNamePrefix,\n+             kubeTokenManager))\n+       } else None )\n+\n+    override def configurePod(pod: SparkPod): SparkPod = {\n+      val hadoopBasedSparkPod = HadoopBootstrapUtil.bootstrapHadoopConfDir(\n+        hadoopConfDir,\n+        kubernetesConf.hadoopConfigMapName,\n+        kubeTokenManager,\n+        pod)\n+      (for {\n+        hSpec <- hadoopSpec\n+        krb5fileLocation <- kubernetesConf.get(KUBERNETES_KERBEROS_KRB5_FILE)\n+      } yield {\n+        HadoopBootstrapUtil.bootstrapKerberosPod(\n+            hSpec.dtSecretName,\n+            hSpec.dtSecretItemKey,\n+            hSpec.jobUserName,\n+            krb5fileLocation,\n+            kubernetesConf.kRBConfigMapName,\n+            hadoopBasedSparkPod)\n+      }).getOrElse(\n+        HadoopBootstrapUtil.bootstrapSparkUserPod(\n+          kubeTokenManager.getCurrentUser.getShortUserName,\n+          hadoopBasedSparkPod))\n+    }\n+\n+    override def getAdditionalPodSystemProperties(): Map[String, String] = {\n+      val resolvedConfValues = hadoopSpec.map{ hSpec =>\n+         Map(KERBEROS_KEYTAB_SECRET_NAME -> hSpec.dtSecretName,\n+            KERBEROS_KEYTAB_SECRET_KEY -> hSpec.dtSecretItemKey,\n+            KERBEROS_SPARK_USER_NAME -> hSpec.jobUserName)\n+      }.getOrElse(\n+          Map(KERBEROS_SPARK_USER_NAME ->\n+            kubernetesConf.tokenManager.getCurrentUser.getShortUserName))\n+      Map(HADOOP_CONFIG_MAP_SPARK_CONF_NAME -> kubernetesConf.hadoopConfigMapName,\n+          HADOOP_CONF_DIR_LOC -> kubernetesConf.hadoopConfDir.get) ++ resolvedConfValues\n+    }\n+\n+    override def getAdditionalKubernetesResources(): Seq[HasMetadata] = {\n+      val krb5ConfigMap = kubernetesConf.get(KUBERNETES_KERBEROS_KRB5_FILE)\n+        .map(fileLocation => HadoopBootstrapUtil.buildkrb5ConfigMap("
  }],
  "prId": 21669
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "space before `{`\r\n\r\n(I thought the style checker complained about that kind of thing...)",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-10-05T18:18:55Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features\n+\n+import java.io.File\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.base.Charsets\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model.{ConfigMapBuilder, HasMetadata}\n+\n+import org.apache.spark.deploy.k8s.{KubernetesConf, KubernetesUtils, SparkPod}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.KubernetesDriverSpecificConf\n+import org.apache.spark.deploy.k8s.features.hadoopsteps._\n+import org.apache.spark.internal.Logging\n+\n+ /**\n+  * Runs the necessary Hadoop-based logic based on Kerberos configs and the presence of the\n+  * HADOOP_CONF_DIR. This runs various bootstrap methods defined in HadoopBootstrapUtil.\n+  */\n+private[spark] class HadoopGlobalFeatureDriverStep(\n+    kubernetesConf: KubernetesConf[KubernetesDriverSpecificConf])\n+    extends KubernetesFeatureConfigStep with Logging {\n+\n+    private val conf = kubernetesConf.sparkConf\n+    private val maybePrincipal = conf.get(org.apache.spark.internal.config.PRINCIPAL)\n+    private val maybeKeytab = conf.get(org.apache.spark.internal.config.KEYTAB)\n+    private val maybeExistingSecretName = conf.get(KUBERNETES_KERBEROS_DT_SECRET_NAME)\n+    private val maybeExistingSecretItemKey =\n+      conf.get(KUBERNETES_KERBEROS_DT_SECRET_ITEM_KEY)\n+    private val kubeTokenManager = kubernetesConf.tokenManager\n+    private val isKerberosEnabled = kubeTokenManager.isSecurityEnabled\n+\n+    require(maybeKeytab.forall( _ => isKerberosEnabled ),\n+      \"You must enable Kerberos support if you are specifying a Kerberos Keytab\")\n+\n+    require(maybeExistingSecretName.forall( _ => isKerberosEnabled ),\n+      \"You must enable Kerberos support if you are specifying a Kerberos Secret\")\n+\n+    KubernetesUtils.requireBothOrNeitherDefined(\n+      maybeKeytab,\n+      maybePrincipal,\n+      \"If a Kerberos principal is specified you must also specify a Kerberos keytab\",\n+      \"If a Kerberos keytab is specified you must also specify a Kerberos principal\")\n+\n+    KubernetesUtils.requireBothOrNeitherDefined(\n+      maybeExistingSecretName,\n+      maybeExistingSecretItemKey,\n+      \"If a secret data item-key where the data of the Kerberos Delegation Token is specified\" +\n+        \" you must also specify the name of the secret\",\n+      \"If a secret storing a Kerberos Delegation Token is specified you must also\" +\n+        \" specify the item-key where the data is stored\")\n+\n+    require(kubernetesConf.hadoopConfDir.isDefined, \"Ensure that HADOOP_CONF_DIR is defined\")\n+    private val hadoopConfDir = kubernetesConf.hadoopConfDir.get\n+    private val hadoopConfigurationFiles = kubeTokenManager.getHadoopConfFiles(hadoopConfDir)\n+\n+    // Either use pre-existing secret or login to create new Secret with DT stored within\n+    private val hadoopSpec: Option[KerberosConfigSpec] = (for {\n+      secretName <- maybeExistingSecretName\n+      secretItemKey <- maybeExistingSecretItemKey\n+    } yield {\n+      KerberosConfigSpec(\n+         dtSecret = None,\n+         dtSecretName = secretName,\n+         dtSecretItemKey = secretItemKey,\n+         jobUserName = kubeTokenManager.getCurrentUser.getShortUserName)\n+    }).orElse(\n+      if (isKerberosEnabled) {\n+         Some(HadoopKerberosLogin.buildSpec(\n+             conf,\n+             kubernetesConf.appResourceNamePrefix,\n+             kubeTokenManager))\n+       } else None )\n+\n+    override def configurePod(pod: SparkPod): SparkPod = {\n+      val hadoopBasedSparkPod = HadoopBootstrapUtil.bootstrapHadoopConfDir(\n+        hadoopConfDir,\n+        kubernetesConf.hadoopConfigMapName,\n+        kubeTokenManager,\n+        pod)\n+      (for {\n+        hSpec <- hadoopSpec\n+        krb5fileLocation <- kubernetesConf.get(KUBERNETES_KERBEROS_KRB5_FILE)\n+      } yield {\n+        HadoopBootstrapUtil.bootstrapKerberosPod(\n+            hSpec.dtSecretName,\n+            hSpec.dtSecretItemKey,\n+            hSpec.jobUserName,\n+            krb5fileLocation,\n+            kubernetesConf.kRBConfigMapName,\n+            hadoopBasedSparkPod)\n+      }).getOrElse(\n+        HadoopBootstrapUtil.bootstrapSparkUserPod(\n+          kubeTokenManager.getCurrentUser.getShortUserName,\n+          hadoopBasedSparkPod))\n+    }\n+\n+    override def getAdditionalPodSystemProperties(): Map[String, String] = {\n+      val resolvedConfValues = hadoopSpec.map{ hSpec =>"
  }],
  "prId": 21669
}]