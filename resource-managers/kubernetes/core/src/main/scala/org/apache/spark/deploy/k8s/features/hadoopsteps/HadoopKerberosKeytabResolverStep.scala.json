[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`SparkSubmit` already logs in the user if a keytab is provided, so you could reuse that. Only issue is that it uses the existing configs which have \"yarn\" in their name.\r\n\r\nI think it would be better to create common names for the principal and keytab configs, and deprecate the YARN-specific ones. Then you can just use `UGI.getCurrentUser` here. You could even simplify the logic below (no need for `doAs` in that case).",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-09-05T21:33:44Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features.hadoopsteps\n+\n+import java.io._\n+import java.security.PrivilegedExceptionAction\n+\n+import scala.collection.JavaConverters._\n+\n+import io.fabric8.kubernetes.api.model.SecretBuilder\n+import org.apache.commons.codec.binary.Base64\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.security.KubernetesHadoopDelegationTokenManager\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.Logging\n+\n+ /**\n+  * This step does all the heavy lifting for Delegation Token logic. This step\n+  * assumes that the job user has either specified a principal and keytab or ran\n+  * $kinit before running spark-submit. With a TGT stored locally, by running\n+  * UGI.getCurrentUser you are able to obtain the current user, alternatively\n+  * you can run UGI.loginUserFromKeytabAndReturnUGI and by running .doAs run\n+  * as the logged into user instead of the current user. With the Job User principal\n+  * you then retrieve the delegation token from the NameNode and store values in\n+  * DelegationToken. Lastly, the class puts the data into a secret. All this is\n+  * appended to the current HadoopSpec which in turn will append to the current\n+  * DriverSpec.\n+  */\n+private[spark] class HadoopKerberosKeytabResolverStep(\n+    submissionSparkConf: SparkConf,\n+    kubernetesResourceNamePrefix : String,\n+    maybePrincipal: Option[String],\n+    maybeKeytab: Option[File],\n+    maybeRenewerPrincipal: Option[String],\n+    tokenManager: KubernetesHadoopDelegationTokenManager)\n+   extends HadoopConfigurationStep with Logging {\n+\n+    override def configureHadoopSpec(hSpec: HadoopConfigSpec): HadoopConfigSpec = {\n+      val hadoopConf = SparkHadoopUtil.get.newConfiguration(submissionSparkConf)\n+      if (!tokenManager.isSecurityEnabled) {\n+        throw new SparkException(\"Hadoop not configured with Kerberos\")\n+      }\n+      val maybeJobUserUGI ="
  }, {
    "author": {
      "login": "ifilonenko"
    },
    "body": "Shouldn't the `yarn` config change exist in a different PR or should I include it in here? \r\nI agree that it would be super helpful. ",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-09-06T01:09:10Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features.hadoopsteps\n+\n+import java.io._\n+import java.security.PrivilegedExceptionAction\n+\n+import scala.collection.JavaConverters._\n+\n+import io.fabric8.kubernetes.api.model.SecretBuilder\n+import org.apache.commons.codec.binary.Base64\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.security.KubernetesHadoopDelegationTokenManager\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.Logging\n+\n+ /**\n+  * This step does all the heavy lifting for Delegation Token logic. This step\n+  * assumes that the job user has either specified a principal and keytab or ran\n+  * $kinit before running spark-submit. With a TGT stored locally, by running\n+  * UGI.getCurrentUser you are able to obtain the current user, alternatively\n+  * you can run UGI.loginUserFromKeytabAndReturnUGI and by running .doAs run\n+  * as the logged into user instead of the current user. With the Job User principal\n+  * you then retrieve the delegation token from the NameNode and store values in\n+  * DelegationToken. Lastly, the class puts the data into a secret. All this is\n+  * appended to the current HadoopSpec which in turn will append to the current\n+  * DriverSpec.\n+  */\n+private[spark] class HadoopKerberosKeytabResolverStep(\n+    submissionSparkConf: SparkConf,\n+    kubernetesResourceNamePrefix : String,\n+    maybePrincipal: Option[String],\n+    maybeKeytab: Option[File],\n+    maybeRenewerPrincipal: Option[String],\n+    tokenManager: KubernetesHadoopDelegationTokenManager)\n+   extends HadoopConfigurationStep with Logging {\n+\n+    override def configureHadoopSpec(hSpec: HadoopConfigSpec): HadoopConfigSpec = {\n+      val hadoopConf = SparkHadoopUtil.get.newConfiguration(submissionSparkConf)\n+      if (!tokenManager.isSecurityEnabled) {\n+        throw new SparkException(\"Hadoop not configured with Kerberos\")\n+      }\n+      val maybeJobUserUGI ="
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "It would be nice to have it in a separate PR. It should be pretty trivial, so most likely would be merged before this one if you open it.",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-09-06T17:41:39Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features.hadoopsteps\n+\n+import java.io._\n+import java.security.PrivilegedExceptionAction\n+\n+import scala.collection.JavaConverters._\n+\n+import io.fabric8.kubernetes.api.model.SecretBuilder\n+import org.apache.commons.codec.binary.Base64\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.security.KubernetesHadoopDelegationTokenManager\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.Logging\n+\n+ /**\n+  * This step does all the heavy lifting for Delegation Token logic. This step\n+  * assumes that the job user has either specified a principal and keytab or ran\n+  * $kinit before running spark-submit. With a TGT stored locally, by running\n+  * UGI.getCurrentUser you are able to obtain the current user, alternatively\n+  * you can run UGI.loginUserFromKeytabAndReturnUGI and by running .doAs run\n+  * as the logged into user instead of the current user. With the Job User principal\n+  * you then retrieve the delegation token from the NameNode and store values in\n+  * DelegationToken. Lastly, the class puts the data into a secret. All this is\n+  * appended to the current HadoopSpec which in turn will append to the current\n+  * DriverSpec.\n+  */\n+private[spark] class HadoopKerberosKeytabResolverStep(\n+    submissionSparkConf: SparkConf,\n+    kubernetesResourceNamePrefix : String,\n+    maybePrincipal: Option[String],\n+    maybeKeytab: Option[File],\n+    maybeRenewerPrincipal: Option[String],\n+    tokenManager: KubernetesHadoopDelegationTokenManager)\n+   extends HadoopConfigurationStep with Logging {\n+\n+    override def configureHadoopSpec(hSpec: HadoopConfigSpec): HadoopConfigSpec = {\n+      val hadoopConf = SparkHadoopUtil.get.newConfiguration(submissionSparkConf)\n+      if (!tokenManager.isSecurityEnabled) {\n+        throw new SparkException(\"Hadoop not configured with Kerberos\")\n+      }\n+      val maybeJobUserUGI ="
  }, {
    "author": {
      "login": "ifilonenko"
    },
    "body": "Then this PR will depend on: https://github.com/apache/spark/pull/22362 to be merged.  Good catch on the SparkSubmit as this should simplify the jobUser retrieval with just `UserGroupInformation.getCurrentUser`",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-09-07T22:37:45Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features.hadoopsteps\n+\n+import java.io._\n+import java.security.PrivilegedExceptionAction\n+\n+import scala.collection.JavaConverters._\n+\n+import io.fabric8.kubernetes.api.model.SecretBuilder\n+import org.apache.commons.codec.binary.Base64\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.security.KubernetesHadoopDelegationTokenManager\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.Logging\n+\n+ /**\n+  * This step does all the heavy lifting for Delegation Token logic. This step\n+  * assumes that the job user has either specified a principal and keytab or ran\n+  * $kinit before running spark-submit. With a TGT stored locally, by running\n+  * UGI.getCurrentUser you are able to obtain the current user, alternatively\n+  * you can run UGI.loginUserFromKeytabAndReturnUGI and by running .doAs run\n+  * as the logged into user instead of the current user. With the Job User principal\n+  * you then retrieve the delegation token from the NameNode and store values in\n+  * DelegationToken. Lastly, the class puts the data into a secret. All this is\n+  * appended to the current HadoopSpec which in turn will append to the current\n+  * DriverSpec.\n+  */\n+private[spark] class HadoopKerberosKeytabResolverStep(\n+    submissionSparkConf: SparkConf,\n+    kubernetesResourceNamePrefix : String,\n+    maybePrincipal: Option[String],\n+    maybeKeytab: Option[File],\n+    maybeRenewerPrincipal: Option[String],\n+    tokenManager: KubernetesHadoopDelegationTokenManager)\n+   extends HadoopConfigurationStep with Logging {\n+\n+    override def configureHadoopSpec(hSpec: HadoopConfigSpec): HadoopConfigSpec = {\n+      val hadoopConf = SparkHadoopUtil.get.newConfiguration(submissionSparkConf)\n+      if (!tokenManager.isSecurityEnabled) {\n+        throw new SparkException(\"Hadoop not configured with Kerberos\")\n+      }\n+      val maybeJobUserUGI ="
  }],
  "prId": 21669
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Just use `require`?",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-09-05T21:36:40Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features.hadoopsteps\n+\n+import java.io._\n+import java.security.PrivilegedExceptionAction\n+\n+import scala.collection.JavaConverters._\n+\n+import io.fabric8.kubernetes.api.model.SecretBuilder\n+import org.apache.commons.codec.binary.Base64\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.security.KubernetesHadoopDelegationTokenManager\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.Logging\n+\n+ /**\n+  * This step does all the heavy lifting for Delegation Token logic. This step\n+  * assumes that the job user has either specified a principal and keytab or ran\n+  * $kinit before running spark-submit. With a TGT stored locally, by running\n+  * UGI.getCurrentUser you are able to obtain the current user, alternatively\n+  * you can run UGI.loginUserFromKeytabAndReturnUGI and by running .doAs run\n+  * as the logged into user instead of the current user. With the Job User principal\n+  * you then retrieve the delegation token from the NameNode and store values in\n+  * DelegationToken. Lastly, the class puts the data into a secret. All this is\n+  * appended to the current HadoopSpec which in turn will append to the current\n+  * DriverSpec.\n+  */\n+private[spark] class HadoopKerberosKeytabResolverStep(\n+    submissionSparkConf: SparkConf,\n+    kubernetesResourceNamePrefix : String,\n+    maybePrincipal: Option[String],\n+    maybeKeytab: Option[File],\n+    maybeRenewerPrincipal: Option[String],\n+    tokenManager: KubernetesHadoopDelegationTokenManager)\n+   extends HadoopConfigurationStep with Logging {\n+\n+    override def configureHadoopSpec(hSpec: HadoopConfigSpec): HadoopConfigSpec = {\n+      val hadoopConf = SparkHadoopUtil.get.newConfiguration(submissionSparkConf)\n+      if (!tokenManager.isSecurityEnabled) {\n+        throw new SparkException(\"Hadoop not configured with Kerberos\")\n+      }\n+      val maybeJobUserUGI =\n+        for {\n+          principal <- maybePrincipal\n+          keytab <- maybeKeytab\n+        } yield {\n+          logDebug(\"Logged into KDC with keytab using Job User UGI\")\n+          tokenManager.loginUserFromKeytabAndReturnUGI(\n+            principal,\n+            keytab.toURI.toString)\n+        }\n+      // In the case that keytab is not specified we will read from Local Ticket Cache\n+      val jobUserUGI = maybeJobUserUGI.getOrElse(tokenManager.getCurrentUser)\n+      val originalCredentials = jobUserUGI.getCredentials\n+      // It is necessary to run as jobUserUGI because logged in user != Current User\n+      val (tokenData, renewalInterval) = jobUserUGI.doAs(\n+        new PrivilegedExceptionAction[(Array[Byte], Long)] {\n+        override def run(): (Array[Byte], Long) = {\n+          val hadoopTokenManager: HadoopDelegationTokenManager =\n+            new HadoopDelegationTokenManager(submissionSparkConf, hadoopConf)\n+          tokenManager.getDelegationTokens(\n+            originalCredentials,\n+            submissionSparkConf,\n+            hadoopConf,\n+            hadoopTokenManager)\n+        }})\n+      if (tokenData.isEmpty) throw new SparkException(s\"Did not obtain any delegation tokens\")"
  }],
  "prId": 21669
}]