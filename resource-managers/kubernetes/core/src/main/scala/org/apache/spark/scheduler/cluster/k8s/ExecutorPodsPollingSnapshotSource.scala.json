[{
  "comments": [{
    "author": {
      "login": "skonto"
    },
    "body": "The are a number of such calls, are we sure they will be executed in any scenario like an exception?\r\nAre the stop calls bound to some shutdown hook? Is this covered by RX-java? \r\n",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-06-12T09:04:03Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{Future, ScheduledExecutorService, TimeUnit}\n+\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.util.ThreadUtils\n+\n+private[spark] class ExecutorPodsPollingSnapshotSource(\n+    conf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    snapshotsStore: ExecutorPodsSnapshotsStore,\n+    pollingExecutor: ScheduledExecutorService) {\n+\n+  private val pollingInterval = conf.get(KUBERNETES_EXECUTOR_API_POLLING_INTERVAL)\n+\n+  private var pollingFuture: Future[_] = _\n+\n+  def start(applicationId: String): Unit = {\n+    require(pollingFuture == null, \"Cannot start polling more than once.\")\n+    pollingFuture = pollingExecutor.scheduleWithFixedDelay(\n+      new PollRunnable(applicationId), pollingInterval, pollingInterval, TimeUnit.MILLISECONDS)\n+  }\n+\n+  def stop(): Unit = {\n+    if (pollingFuture != null) {\n+      pollingFuture.cancel(true)\n+      pollingFuture = null\n+    }\n+    ThreadUtils.shutdown(pollingExecutor)",
    "line": 52
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "There's a shutdown hook that closes the Spark Context and all its dependent components, including the `KubernetesClusterSchedulerBackend`. In `KubernetesClusterSchedulerBackend` we shut down all of these components. We also shut them down with handling caught exceptions. Also all of these thread pools are daemon thread pools so they should shut themselves down on JVM exit.",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-06-12T18:14:03Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{Future, ScheduledExecutorService, TimeUnit}\n+\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.util.ThreadUtils\n+\n+private[spark] class ExecutorPodsPollingSnapshotSource(\n+    conf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    snapshotsStore: ExecutorPodsSnapshotsStore,\n+    pollingExecutor: ScheduledExecutorService) {\n+\n+  private val pollingInterval = conf.get(KUBERNETES_EXECUTOR_API_POLLING_INTERVAL)\n+\n+  private var pollingFuture: Future[_] = _\n+\n+  def start(applicationId: String): Unit = {\n+    require(pollingFuture == null, \"Cannot start polling more than once.\")\n+    pollingFuture = pollingExecutor.scheduleWithFixedDelay(\n+      new PollRunnable(applicationId), pollingInterval, pollingInterval, TimeUnit.MILLISECONDS)\n+  }\n+\n+  def stop(): Unit = {\n+    if (pollingFuture != null) {\n+      pollingFuture.cancel(true)\n+      pollingFuture = null\n+    }\n+    ThreadUtils.shutdown(pollingExecutor)",
    "line": 52
  }, {
    "author": {
      "login": "skonto"
    },
    "body": "Ok just wanted to verify it.",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-06-13T11:00:04Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{Future, ScheduledExecutorService, TimeUnit}\n+\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.util.ThreadUtils\n+\n+private[spark] class ExecutorPodsPollingSnapshotSource(\n+    conf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    snapshotsStore: ExecutorPodsSnapshotsStore,\n+    pollingExecutor: ScheduledExecutorService) {\n+\n+  private val pollingInterval = conf.get(KUBERNETES_EXECUTOR_API_POLLING_INTERVAL)\n+\n+  private var pollingFuture: Future[_] = _\n+\n+  def start(applicationId: String): Unit = {\n+    require(pollingFuture == null, \"Cannot start polling more than once.\")\n+    pollingFuture = pollingExecutor.scheduleWithFixedDelay(\n+      new PollRunnable(applicationId), pollingInterval, pollingInterval, TimeUnit.MILLISECONDS)\n+  }\n+\n+  def stop(): Unit = {\n+    if (pollingFuture != null) {\n+      pollingFuture.cancel(true)\n+      pollingFuture = null\n+    }\n+    ThreadUtils.shutdown(pollingExecutor)",
    "line": 52
  }],
  "prId": 21366
}, {
  "comments": [{
    "author": {
      "login": "skonto"
    },
    "body": "Do you start with an empty state to trigger executor creation at the very beginning when the driver starts?",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-06-14T11:43:41Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{Future, ScheduledExecutorService, TimeUnit}\n+\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.util.ThreadUtils\n+\n+private[spark] class ExecutorPodsPollingSnapshotSource(\n+    conf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    snapshotsStore: ExecutorPodsSnapshotsStore,\n+    pollingExecutor: ScheduledExecutorService) {\n+\n+  private val pollingInterval = conf.get(KUBERNETES_EXECUTOR_API_POLLING_INTERVAL)\n+\n+  private var pollingFuture: Future[_] = _\n+\n+  def start(applicationId: String): Unit = {\n+    require(pollingFuture == null, \"Cannot start polling more than once.\")\n+    pollingFuture = pollingExecutor.scheduleWithFixedDelay(\n+      new PollRunnable(applicationId), pollingInterval, pollingInterval, TimeUnit.MILLISECONDS)\n+  }\n+\n+  def stop(): Unit = {\n+    if (pollingFuture != null) {\n+      pollingFuture.cancel(true)\n+      pollingFuture = null\n+    }\n+    ThreadUtils.shutdown(pollingExecutor)\n+  }\n+\n+  private class PollRunnable(applicationId: String) extends Runnable {\n+    override def run(): Unit = {\n+      snapshotsStore.replaceSnapshot(kubernetesClient"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "Not strictly why that's done here but a side-effect I suppose. Really the snapshots store should push an initial empty snapshot to all subscribers when it starts, and the unit tests do check for that - it's the responsibility of the snapshots store.",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-06-14T17:45:33Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{Future, ScheduledExecutorService, TimeUnit}\n+\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.util.ThreadUtils\n+\n+private[spark] class ExecutorPodsPollingSnapshotSource(\n+    conf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    snapshotsStore: ExecutorPodsSnapshotsStore,\n+    pollingExecutor: ScheduledExecutorService) {\n+\n+  private val pollingInterval = conf.get(KUBERNETES_EXECUTOR_API_POLLING_INTERVAL)\n+\n+  private var pollingFuture: Future[_] = _\n+\n+  def start(applicationId: String): Unit = {\n+    require(pollingFuture == null, \"Cannot start polling more than once.\")\n+    pollingFuture = pollingExecutor.scheduleWithFixedDelay(\n+      new PollRunnable(applicationId), pollingInterval, pollingInterval, TimeUnit.MILLISECONDS)\n+  }\n+\n+  def stop(): Unit = {\n+    if (pollingFuture != null) {\n+      pollingFuture.cancel(true)\n+      pollingFuture = null\n+    }\n+    ThreadUtils.shutdown(pollingExecutor)\n+  }\n+\n+  private class PollRunnable(applicationId: String) extends Runnable {\n+    override def run(): Unit = {\n+      snapshotsStore.replaceSnapshot(kubernetesClient"
  }, {
    "author": {
      "login": "skonto"
    },
    "body": "Yes you need to trigger the initial creation of executors somehow and yes I saw that in the tests, my only concern is that this should be explicit not implicit to make code more obvious anyway.",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-06-14T18:55:59Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{Future, ScheduledExecutorService, TimeUnit}\n+\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.util.ThreadUtils\n+\n+private[spark] class ExecutorPodsPollingSnapshotSource(\n+    conf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    snapshotsStore: ExecutorPodsSnapshotsStore,\n+    pollingExecutor: ScheduledExecutorService) {\n+\n+  private val pollingInterval = conf.get(KUBERNETES_EXECUTOR_API_POLLING_INTERVAL)\n+\n+  private var pollingFuture: Future[_] = _\n+\n+  def start(applicationId: String): Unit = {\n+    require(pollingFuture == null, \"Cannot start polling more than once.\")\n+    pollingFuture = pollingExecutor.scheduleWithFixedDelay(\n+      new PollRunnable(applicationId), pollingInterval, pollingInterval, TimeUnit.MILLISECONDS)\n+  }\n+\n+  def stop(): Unit = {\n+    if (pollingFuture != null) {\n+      pollingFuture.cancel(true)\n+      pollingFuture = null\n+    }\n+    ThreadUtils.shutdown(pollingExecutor)\n+  }\n+\n+  private class PollRunnable(applicationId: String) extends Runnable {\n+    override def run(): Unit = {\n+      snapshotsStore.replaceSnapshot(kubernetesClient"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "I see - I think what we actually want is `ExecutorPodsSnapshotStoreImpl` to initialize the subscriber with its current snapshot. That creates the semantics where the new subscriber will first receive the most up to date state immediately.",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-06-14T19:23:39Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{Future, ScheduledExecutorService, TimeUnit}\n+\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.util.ThreadUtils\n+\n+private[spark] class ExecutorPodsPollingSnapshotSource(\n+    conf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    snapshotsStore: ExecutorPodsSnapshotsStore,\n+    pollingExecutor: ScheduledExecutorService) {\n+\n+  private val pollingInterval = conf.get(KUBERNETES_EXECUTOR_API_POLLING_INTERVAL)\n+\n+  private var pollingFuture: Future[_] = _\n+\n+  def start(applicationId: String): Unit = {\n+    require(pollingFuture == null, \"Cannot start polling more than once.\")\n+    pollingFuture = pollingExecutor.scheduleWithFixedDelay(\n+      new PollRunnable(applicationId), pollingInterval, pollingInterval, TimeUnit.MILLISECONDS)\n+  }\n+\n+  def stop(): Unit = {\n+    if (pollingFuture != null) {\n+      pollingFuture.cancel(true)\n+      pollingFuture = null\n+    }\n+    ThreadUtils.shutdown(pollingExecutor)\n+  }\n+\n+  private class PollRunnable(applicationId: String) extends Runnable {\n+    override def run(): Unit = {\n+      snapshotsStore.replaceSnapshot(kubernetesClient"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "And though we don't allow for this right now, the above would allow subscribers to be added midway through to receive the most recent snapshot immediately. But again we don't do this right now - we setup all subscribers on startup before starting pushing snapshots.",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-06-14T20:33:19Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{Future, ScheduledExecutorService, TimeUnit}\n+\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.util.ThreadUtils\n+\n+private[spark] class ExecutorPodsPollingSnapshotSource(\n+    conf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    snapshotsStore: ExecutorPodsSnapshotsStore,\n+    pollingExecutor: ScheduledExecutorService) {\n+\n+  private val pollingInterval = conf.get(KUBERNETES_EXECUTOR_API_POLLING_INTERVAL)\n+\n+  private var pollingFuture: Future[_] = _\n+\n+  def start(applicationId: String): Unit = {\n+    require(pollingFuture == null, \"Cannot start polling more than once.\")\n+    pollingFuture = pollingExecutor.scheduleWithFixedDelay(\n+      new PollRunnable(applicationId), pollingInterval, pollingInterval, TimeUnit.MILLISECONDS)\n+  }\n+\n+  def stop(): Unit = {\n+    if (pollingFuture != null) {\n+      pollingFuture.cancel(true)\n+      pollingFuture = null\n+    }\n+    ThreadUtils.shutdown(pollingExecutor)\n+  }\n+\n+  private class PollRunnable(applicationId: String) extends Runnable {\n+    override def run(): Unit = {\n+      snapshotsStore.replaceSnapshot(kubernetesClient"
  }, {
    "author": {
      "login": "skonto"
    },
    "body": "You could add some comment saying this is where we create executors and by what way.\r\nI mean on mesos you start executors when you get offers from agents and that is straightforward and makes sense. Here you want to start them ASAP, you have no restrictions,  so then you can send Spark tasks to them right? ",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-06-14T20:45:44Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{Future, ScheduledExecutorService, TimeUnit}\n+\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.util.ThreadUtils\n+\n+private[spark] class ExecutorPodsPollingSnapshotSource(\n+    conf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    snapshotsStore: ExecutorPodsSnapshotsStore,\n+    pollingExecutor: ScheduledExecutorService) {\n+\n+  private val pollingInterval = conf.get(KUBERNETES_EXECUTOR_API_POLLING_INTERVAL)\n+\n+  private var pollingFuture: Future[_] = _\n+\n+  def start(applicationId: String): Unit = {\n+    require(pollingFuture == null, \"Cannot start polling more than once.\")\n+    pollingFuture = pollingExecutor.scheduleWithFixedDelay(\n+      new PollRunnable(applicationId), pollingInterval, pollingInterval, TimeUnit.MILLISECONDS)\n+  }\n+\n+  def stop(): Unit = {\n+    if (pollingFuture != null) {\n+      pollingFuture.cancel(true)\n+      pollingFuture = null\n+    }\n+    ThreadUtils.shutdown(pollingExecutor)\n+  }\n+\n+  private class PollRunnable(applicationId: String) extends Runnable {\n+    override def run(): Unit = {\n+      snapshotsStore.replaceSnapshot(kubernetesClient"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "But polling isn't where we start to create executors - that's done on the subscriber rounds. Polling here populates the snapshots store, but processing the snapshots happens on the subscriber thread(s). Furthermore with the scheme proposed above you never have to even poll for snapshots once before we begin requesting executors, because the pods allocator subscriber will trigger immediately with an empty snapshot.",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-06-14T20:51:59Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{Future, ScheduledExecutorService, TimeUnit}\n+\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.util.ThreadUtils\n+\n+private[spark] class ExecutorPodsPollingSnapshotSource(\n+    conf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    snapshotsStore: ExecutorPodsSnapshotsStore,\n+    pollingExecutor: ScheduledExecutorService) {\n+\n+  private val pollingInterval = conf.get(KUBERNETES_EXECUTOR_API_POLLING_INTERVAL)\n+\n+  private var pollingFuture: Future[_] = _\n+\n+  def start(applicationId: String): Unit = {\n+    require(pollingFuture == null, \"Cannot start polling more than once.\")\n+    pollingFuture = pollingExecutor.scheduleWithFixedDelay(\n+      new PollRunnable(applicationId), pollingInterval, pollingInterval, TimeUnit.MILLISECONDS)\n+  }\n+\n+  def stop(): Unit = {\n+    if (pollingFuture != null) {\n+      pollingFuture.cancel(true)\n+      pollingFuture = null\n+    }\n+    ThreadUtils.shutdown(pollingExecutor)\n+  }\n+\n+  private class PollRunnable(applicationId: String) extends Runnable {\n+    override def run(): Unit = {\n+      snapshotsStore.replaceSnapshot(kubernetesClient"
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "For example if we changed the `initialDelay` here to stall before the first snapshots sync then with the above scheme we'd still try to request executors immediately, because the subscriber thread kicks off an allocation round immediately.",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-06-14T20:53:15Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{Future, ScheduledExecutorService, TimeUnit}\n+\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.util.ThreadUtils\n+\n+private[spark] class ExecutorPodsPollingSnapshotSource(\n+    conf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    snapshotsStore: ExecutorPodsSnapshotsStore,\n+    pollingExecutor: ScheduledExecutorService) {\n+\n+  private val pollingInterval = conf.get(KUBERNETES_EXECUTOR_API_POLLING_INTERVAL)\n+\n+  private var pollingFuture: Future[_] = _\n+\n+  def start(applicationId: String): Unit = {\n+    require(pollingFuture == null, \"Cannot start polling more than once.\")\n+    pollingFuture = pollingExecutor.scheduleWithFixedDelay(\n+      new PollRunnable(applicationId), pollingInterval, pollingInterval, TimeUnit.MILLISECONDS)\n+  }\n+\n+  def stop(): Unit = {\n+    if (pollingFuture != null) {\n+      pollingFuture.cancel(true)\n+      pollingFuture = null\n+    }\n+    ThreadUtils.shutdown(pollingExecutor)\n+  }\n+\n+  private class PollRunnable(applicationId: String) extends Runnable {\n+    override def run(): Unit = {\n+      snapshotsStore.replaceSnapshot(kubernetesClient"
  }, {
    "author": {
      "login": "skonto"
    },
    "body": "Yeah I see. I guess this is done by ExecutorPodsAllocator as a subscriber when it gets the empty snapshot. ",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-06-14T21:00:32Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{Future, ScheduledExecutorService, TimeUnit}\n+\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.util.ThreadUtils\n+\n+private[spark] class ExecutorPodsPollingSnapshotSource(\n+    conf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    snapshotsStore: ExecutorPodsSnapshotsStore,\n+    pollingExecutor: ScheduledExecutorService) {\n+\n+  private val pollingInterval = conf.get(KUBERNETES_EXECUTOR_API_POLLING_INTERVAL)\n+\n+  private var pollingFuture: Future[_] = _\n+\n+  def start(applicationId: String): Unit = {\n+    require(pollingFuture == null, \"Cannot start polling more than once.\")\n+    pollingFuture = pollingExecutor.scheduleWithFixedDelay(\n+      new PollRunnable(applicationId), pollingInterval, pollingInterval, TimeUnit.MILLISECONDS)\n+  }\n+\n+  def stop(): Unit = {\n+    if (pollingFuture != null) {\n+      pollingFuture.cancel(true)\n+      pollingFuture = null\n+    }\n+    ThreadUtils.shutdown(pollingExecutor)\n+  }\n+\n+  private class PollRunnable(applicationId: String) extends Runnable {\n+    override def run(): Unit = {\n+      snapshotsStore.replaceSnapshot(kubernetesClient"
  }],
  "prId": 21366
}, {
  "comments": [{
    "author": {
      "login": "skonto"
    },
    "body": "Could you add some debug logging here. In general it would be good to be able to trace what is happening in case of a an issue with debug mode, this applies to all classes introduced for both watching and polling.",
    "commit": "1a99dceeb9dfbfc58e26885c290461cbf37a5428",
    "createdAt": "2018-06-14T12:25:07Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{Future, ScheduledExecutorService, TimeUnit}\n+\n+import io.fabric8.kubernetes.client.KubernetesClient\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.util.ThreadUtils\n+\n+private[spark] class ExecutorPodsPollingSnapshotSource(\n+    conf: SparkConf,\n+    kubernetesClient: KubernetesClient,\n+    snapshotsStore: ExecutorPodsSnapshotsStore,\n+    pollingExecutor: ScheduledExecutorService) {"
  }],
  "prId": 21366
}]