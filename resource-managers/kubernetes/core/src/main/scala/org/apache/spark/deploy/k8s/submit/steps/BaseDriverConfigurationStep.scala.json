[{
  "comments": [{
    "author": {
      "login": "mridulm"
    },
    "body": "How does this handle escaping, etc ?\r\nIf user specified arguments which have whitespace within, this will break ?\r\nExample: `appArgs = Array(\"a 1\", \"a\\t2\")` and so on",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-27T08:38:43Z",
    "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit.steps\n+\n+import scala.collection.JavaConverters._\n+\n+import io.fabric8.kubernetes.api.model.{ContainerBuilder, EnvVarBuilder, EnvVarSourceBuilder, PodBuilder, QuantityBuilder}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.KubernetesDriverSpec\n+\n+/**\n+ * Represents the initial setup required for the driver.\n+ */\n+private[spark] class BaseDriverConfigurationStep(\n+    kubernetesAppId: String,\n+    kubernetesResourceNamePrefix: String,\n+    driverLabels: Map[String, String],\n+    dockerImagePullPolicy: String,\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) extends DriverConfigurationStep {\n+\n+  private val kubernetesDriverPodName = submissionSparkConf.get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(s\"$kubernetesResourceNamePrefix-driver\")\n+\n+  private val driverExtraClasspath = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_CLASS_PATH)\n+\n+  private val driverDockerImage = submissionSparkConf.get(DRIVER_DOCKER_IMAGE)\n+\n+  // CPU settings\n+  private val driverCpuCores = submissionSparkConf.getOption(\"spark.driver.cores\").getOrElse(\"1\")\n+  private val driverLimitCores = submissionSparkConf.get(KUBERNETES_DRIVER_LIMIT_CORES)\n+\n+  // Memory settings\n+  private val driverMemoryMiB = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_MEMORY)\n+  private val driverMemoryString = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_MEMORY.key,\n+    org.apache.spark.internal.config.DRIVER_MEMORY.defaultValueString)\n+  private val memoryOverheadMiB = submissionSparkConf\n+    .get(KUBERNETES_DRIVER_MEMORY_OVERHEAD)\n+    .getOrElse(math.max((MEMORY_OVERHEAD_FACTOR * driverMemoryMiB).toInt,\n+      MEMORY_OVERHEAD_MIN_MIB))\n+  private val driverContainerMemoryWithOverheadMiB = driverMemoryMiB + memoryOverheadMiB\n+\n+  override def configureDriver(\n+      driverSpec: KubernetesDriverSpec): KubernetesDriverSpec = {\n+    val driverExtraClasspathEnv = driverExtraClasspath.map { classPath =>\n+      new EnvVarBuilder()\n+        .withName(ENV_SUBMIT_EXTRA_CLASSPATH)\n+        .withValue(classPath)\n+        .build()\n+    }\n+\n+    val driverCustomAnnotations = ConfigurationUtils\n+      .parsePrefixedKeyValuePairs(\n+        submissionSparkConf,\n+        KUBERNETES_DRIVER_ANNOTATION_PREFIX)\n+    require(!driverCustomAnnotations.contains(SPARK_APP_NAME_ANNOTATION),\n+      s\"Annotation with key $SPARK_APP_NAME_ANNOTATION is not allowed as it is reserved for\" +\n+        \" Spark bookkeeping operations.\")\n+\n+    val driverCustomEnvs = submissionSparkConf.getAllWithPrefix(KUBERNETES_DRIVER_ENV_KEY).toSeq\n+      .map(env => new EnvVarBuilder()\n+        .withName(env._1)\n+        .withValue(env._2)\n+        .build())\n+\n+    val allDriverAnnotations = driverCustomAnnotations ++ Map(SPARK_APP_NAME_ANNOTATION -> appName)\n+\n+    val nodeSelector = ConfigurationUtils.parsePrefixedKeyValuePairs(\n+      submissionSparkConf, KUBERNETES_NODE_SELECTOR_PREFIX)\n+\n+    val driverCpuQuantity = new QuantityBuilder(false)\n+      .withAmount(driverCpuCores)\n+      .build()\n+    val driverMemoryQuantity = new QuantityBuilder(false)\n+      .withAmount(s\"${driverMemoryMiB}Mi\")\n+      .build()\n+    val driverMemoryLimitQuantity = new QuantityBuilder(false)\n+      .withAmount(s\"${driverContainerMemoryWithOverheadMiB}Mi\")\n+      .build()\n+    val maybeCpuLimitQuantity = driverLimitCores.map { limitCores =>\n+      (\"cpu\", new QuantityBuilder(false).withAmount(limitCores).build())\n+    }\n+\n+    val driverContainer = new ContainerBuilder(driverSpec.driverContainer)\n+      .withName(DRIVER_CONTAINER_NAME)\n+      .withImage(driverDockerImage)\n+      .withImagePullPolicy(dockerImagePullPolicy)\n+      .addAllToEnv(driverCustomEnvs.asJava)\n+      .addToEnv(driverExtraClasspathEnv.toSeq: _*)\n+      .addNewEnv()\n+        .withName(ENV_DRIVER_MEMORY)\n+        .withValue(driverMemoryString)\n+        .endEnv()\n+      .addNewEnv()\n+        .withName(ENV_DRIVER_MAIN_CLASS)\n+        .withValue(mainClass)\n+        .endEnv()\n+      .addNewEnv()\n+        .withName(ENV_DRIVER_ARGS)\n+        .withValue(appArgs.mkString(\" \"))"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Good catch. Added `\\\"\\\"` around each argument.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-30T18:55:29Z",
    "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit.steps\n+\n+import scala.collection.JavaConverters._\n+\n+import io.fabric8.kubernetes.api.model.{ContainerBuilder, EnvVarBuilder, EnvVarSourceBuilder, PodBuilder, QuantityBuilder}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.KubernetesDriverSpec\n+\n+/**\n+ * Represents the initial setup required for the driver.\n+ */\n+private[spark] class BaseDriverConfigurationStep(\n+    kubernetesAppId: String,\n+    kubernetesResourceNamePrefix: String,\n+    driverLabels: Map[String, String],\n+    dockerImagePullPolicy: String,\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) extends DriverConfigurationStep {\n+\n+  private val kubernetesDriverPodName = submissionSparkConf.get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(s\"$kubernetesResourceNamePrefix-driver\")\n+\n+  private val driverExtraClasspath = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_CLASS_PATH)\n+\n+  private val driverDockerImage = submissionSparkConf.get(DRIVER_DOCKER_IMAGE)\n+\n+  // CPU settings\n+  private val driverCpuCores = submissionSparkConf.getOption(\"spark.driver.cores\").getOrElse(\"1\")\n+  private val driverLimitCores = submissionSparkConf.get(KUBERNETES_DRIVER_LIMIT_CORES)\n+\n+  // Memory settings\n+  private val driverMemoryMiB = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_MEMORY)\n+  private val driverMemoryString = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_MEMORY.key,\n+    org.apache.spark.internal.config.DRIVER_MEMORY.defaultValueString)\n+  private val memoryOverheadMiB = submissionSparkConf\n+    .get(KUBERNETES_DRIVER_MEMORY_OVERHEAD)\n+    .getOrElse(math.max((MEMORY_OVERHEAD_FACTOR * driverMemoryMiB).toInt,\n+      MEMORY_OVERHEAD_MIN_MIB))\n+  private val driverContainerMemoryWithOverheadMiB = driverMemoryMiB + memoryOverheadMiB\n+\n+  override def configureDriver(\n+      driverSpec: KubernetesDriverSpec): KubernetesDriverSpec = {\n+    val driverExtraClasspathEnv = driverExtraClasspath.map { classPath =>\n+      new EnvVarBuilder()\n+        .withName(ENV_SUBMIT_EXTRA_CLASSPATH)\n+        .withValue(classPath)\n+        .build()\n+    }\n+\n+    val driverCustomAnnotations = ConfigurationUtils\n+      .parsePrefixedKeyValuePairs(\n+        submissionSparkConf,\n+        KUBERNETES_DRIVER_ANNOTATION_PREFIX)\n+    require(!driverCustomAnnotations.contains(SPARK_APP_NAME_ANNOTATION),\n+      s\"Annotation with key $SPARK_APP_NAME_ANNOTATION is not allowed as it is reserved for\" +\n+        \" Spark bookkeeping operations.\")\n+\n+    val driverCustomEnvs = submissionSparkConf.getAllWithPrefix(KUBERNETES_DRIVER_ENV_KEY).toSeq\n+      .map(env => new EnvVarBuilder()\n+        .withName(env._1)\n+        .withValue(env._2)\n+        .build())\n+\n+    val allDriverAnnotations = driverCustomAnnotations ++ Map(SPARK_APP_NAME_ANNOTATION -> appName)\n+\n+    val nodeSelector = ConfigurationUtils.parsePrefixedKeyValuePairs(\n+      submissionSparkConf, KUBERNETES_NODE_SELECTOR_PREFIX)\n+\n+    val driverCpuQuantity = new QuantityBuilder(false)\n+      .withAmount(driverCpuCores)\n+      .build()\n+    val driverMemoryQuantity = new QuantityBuilder(false)\n+      .withAmount(s\"${driverMemoryMiB}Mi\")\n+      .build()\n+    val driverMemoryLimitQuantity = new QuantityBuilder(false)\n+      .withAmount(s\"${driverContainerMemoryWithOverheadMiB}Mi\")\n+      .build()\n+    val maybeCpuLimitQuantity = driverLimitCores.map { limitCores =>\n+      (\"cpu\", new QuantityBuilder(false).withAmount(limitCores).build())\n+    }\n+\n+    val driverContainer = new ContainerBuilder(driverSpec.driverContainer)\n+      .withName(DRIVER_CONTAINER_NAME)\n+      .withImage(driverDockerImage)\n+      .withImagePullPolicy(dockerImagePullPolicy)\n+      .addAllToEnv(driverCustomEnvs.asJava)\n+      .addToEnv(driverExtraClasspathEnv.toSeq: _*)\n+      .addNewEnv()\n+        .withName(ENV_DRIVER_MEMORY)\n+        .withValue(driverMemoryString)\n+        .endEnv()\n+      .addNewEnv()\n+        .withName(ENV_DRIVER_MAIN_CLASS)\n+        .withValue(mainClass)\n+        .endEnv()\n+      .addNewEnv()\n+        .withName(ENV_DRIVER_ARGS)\n+        .withValue(appArgs.mkString(\" \"))"
  }],
  "prId": 19717
}, {
  "comments": [{
    "author": {
      "login": "jiangxb1987"
    },
    "body": "nit: `new EnvVarBuilder()` in the next line",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-27T15:16:37Z",
    "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit.steps\n+\n+import scala.collection.JavaConverters._\n+\n+import io.fabric8.kubernetes.api.model.{ContainerBuilder, EnvVarBuilder, EnvVarSourceBuilder, PodBuilder, QuantityBuilder}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.KubernetesDriverSpec\n+\n+/**\n+ * Represents the initial setup required for the driver.\n+ */\n+private[spark] class BaseDriverConfigurationStep(\n+    kubernetesAppId: String,\n+    kubernetesResourceNamePrefix: String,\n+    driverLabels: Map[String, String],\n+    dockerImagePullPolicy: String,\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) extends DriverConfigurationStep {\n+\n+  private val kubernetesDriverPodName = submissionSparkConf.get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(s\"$kubernetesResourceNamePrefix-driver\")\n+\n+  private val driverExtraClasspath = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_CLASS_PATH)\n+\n+  private val driverDockerImage = submissionSparkConf.get(DRIVER_DOCKER_IMAGE)\n+\n+  // CPU settings\n+  private val driverCpuCores = submissionSparkConf.getOption(\"spark.driver.cores\").getOrElse(\"1\")\n+  private val driverLimitCores = submissionSparkConf.get(KUBERNETES_DRIVER_LIMIT_CORES)\n+\n+  // Memory settings\n+  private val driverMemoryMiB = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_MEMORY)\n+  private val driverMemoryString = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_MEMORY.key,\n+    org.apache.spark.internal.config.DRIVER_MEMORY.defaultValueString)\n+  private val memoryOverheadMiB = submissionSparkConf\n+    .get(KUBERNETES_DRIVER_MEMORY_OVERHEAD)\n+    .getOrElse(math.max((MEMORY_OVERHEAD_FACTOR * driverMemoryMiB).toInt,\n+      MEMORY_OVERHEAD_MIN_MIB))\n+  private val driverContainerMemoryWithOverheadMiB = driverMemoryMiB + memoryOverheadMiB\n+\n+  override def configureDriver(\n+      driverSpec: KubernetesDriverSpec): KubernetesDriverSpec = {\n+    val driverExtraClasspathEnv = driverExtraClasspath.map { classPath =>\n+      new EnvVarBuilder()\n+        .withName(ENV_SUBMIT_EXTRA_CLASSPATH)\n+        .withValue(classPath)\n+        .build()\n+    }\n+\n+    val driverCustomAnnotations = ConfigurationUtils\n+      .parsePrefixedKeyValuePairs(\n+        submissionSparkConf,\n+        KUBERNETES_DRIVER_ANNOTATION_PREFIX)\n+    require(!driverCustomAnnotations.contains(SPARK_APP_NAME_ANNOTATION),\n+      s\"Annotation with key $SPARK_APP_NAME_ANNOTATION is not allowed as it is reserved for\" +\n+        \" Spark bookkeeping operations.\")\n+\n+    val driverCustomEnvs = submissionSparkConf.getAllWithPrefix(KUBERNETES_DRIVER_ENV_KEY).toSeq\n+      .map(env => new EnvVarBuilder()"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Done.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-28T00:46:57Z",
    "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit.steps\n+\n+import scala.collection.JavaConverters._\n+\n+import io.fabric8.kubernetes.api.model.{ContainerBuilder, EnvVarBuilder, EnvVarSourceBuilder, PodBuilder, QuantityBuilder}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.KubernetesDriverSpec\n+\n+/**\n+ * Represents the initial setup required for the driver.\n+ */\n+private[spark] class BaseDriverConfigurationStep(\n+    kubernetesAppId: String,\n+    kubernetesResourceNamePrefix: String,\n+    driverLabels: Map[String, String],\n+    dockerImagePullPolicy: String,\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) extends DriverConfigurationStep {\n+\n+  private val kubernetesDriverPodName = submissionSparkConf.get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(s\"$kubernetesResourceNamePrefix-driver\")\n+\n+  private val driverExtraClasspath = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_CLASS_PATH)\n+\n+  private val driverDockerImage = submissionSparkConf.get(DRIVER_DOCKER_IMAGE)\n+\n+  // CPU settings\n+  private val driverCpuCores = submissionSparkConf.getOption(\"spark.driver.cores\").getOrElse(\"1\")\n+  private val driverLimitCores = submissionSparkConf.get(KUBERNETES_DRIVER_LIMIT_CORES)\n+\n+  // Memory settings\n+  private val driverMemoryMiB = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_MEMORY)\n+  private val driverMemoryString = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_MEMORY.key,\n+    org.apache.spark.internal.config.DRIVER_MEMORY.defaultValueString)\n+  private val memoryOverheadMiB = submissionSparkConf\n+    .get(KUBERNETES_DRIVER_MEMORY_OVERHEAD)\n+    .getOrElse(math.max((MEMORY_OVERHEAD_FACTOR * driverMemoryMiB).toInt,\n+      MEMORY_OVERHEAD_MIN_MIB))\n+  private val driverContainerMemoryWithOverheadMiB = driverMemoryMiB + memoryOverheadMiB\n+\n+  override def configureDriver(\n+      driverSpec: KubernetesDriverSpec): KubernetesDriverSpec = {\n+    val driverExtraClasspathEnv = driverExtraClasspath.map { classPath =>\n+      new EnvVarBuilder()\n+        .withName(ENV_SUBMIT_EXTRA_CLASSPATH)\n+        .withValue(classPath)\n+        .build()\n+    }\n+\n+    val driverCustomAnnotations = ConfigurationUtils\n+      .parsePrefixedKeyValuePairs(\n+        submissionSparkConf,\n+        KUBERNETES_DRIVER_ANNOTATION_PREFIX)\n+    require(!driverCustomAnnotations.contains(SPARK_APP_NAME_ANNOTATION),\n+      s\"Annotation with key $SPARK_APP_NAME_ANNOTATION is not allowed as it is reserved for\" +\n+        \" Spark bookkeeping operations.\")\n+\n+    val driverCustomEnvs = submissionSparkConf.getAllWithPrefix(KUBERNETES_DRIVER_ENV_KEY).toSeq\n+      .map(env => new EnvVarBuilder()"
  }],
  "prId": 19717
}, {
  "comments": [{
    "author": {
      "login": "jerryshao"
    },
    "body": "Did you add support of this configuration \"spark.driver.userClassPathFirst\" and \"spark.driver.userClassPathFirst\"? Sorry I cannot find it.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-28T07:18:50Z",
    "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit.steps\n+\n+import scala.collection.JavaConverters._\n+\n+import io.fabric8.kubernetes.api.model.{ContainerBuilder, EnvVarBuilder, EnvVarSourceBuilder, PodBuilder, QuantityBuilder}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.KubernetesDriverSpec\n+\n+/**\n+ * Represents the initial setup required for the driver.\n+ */\n+private[spark] class BaseDriverConfigurationStep(\n+    kubernetesAppId: String,\n+    kubernetesResourceNamePrefix: String,\n+    driverLabels: Map[String, String],\n+    dockerImagePullPolicy: String,\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) extends DriverConfigurationStep {\n+\n+  private val kubernetesDriverPodName = submissionSparkConf.get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(s\"$kubernetesResourceNamePrefix-driver\")\n+\n+  private val driverExtraClasspath = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_CLASS_PATH)\n+\n+  private val driverDockerImage = submissionSparkConf.get(DRIVER_DOCKER_IMAGE)\n+\n+  // CPU settings\n+  private val driverCpuCores = submissionSparkConf.getOption(\"spark.driver.cores\").getOrElse(\"1\")\n+  private val driverLimitCores = submissionSparkConf.get(KUBERNETES_DRIVER_LIMIT_CORES)\n+\n+  // Memory settings\n+  private val driverMemoryMiB = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_MEMORY)\n+  private val driverMemoryString = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_MEMORY.key,\n+    org.apache.spark.internal.config.DRIVER_MEMORY.defaultValueString)\n+  private val memoryOverheadMiB = submissionSparkConf\n+    .get(KUBERNETES_DRIVER_MEMORY_OVERHEAD)\n+    .getOrElse(math.max((MEMORY_OVERHEAD_FACTOR * driverMemoryMiB).toInt,\n+      MEMORY_OVERHEAD_MIN_MIB))\n+  private val driverContainerMemoryWithOverheadMiB = driverMemoryMiB + memoryOverheadMiB\n+\n+  override def configureDriver(\n+      driverSpec: KubernetesDriverSpec): KubernetesDriverSpec = {\n+    val driverExtraClasspathEnv = driverExtraClasspath.map { classPath =>\n+      new EnvVarBuilder()\n+        .withName(ENV_SUBMIT_EXTRA_CLASSPATH)\n+        .withValue(classPath)\n+        .build()\n+    }",
    "line": 75
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "No, I don't think so. @mccheah to confirm.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-28T16:20:51Z",
    "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit.steps\n+\n+import scala.collection.JavaConverters._\n+\n+import io.fabric8.kubernetes.api.model.{ContainerBuilder, EnvVarBuilder, EnvVarSourceBuilder, PodBuilder, QuantityBuilder}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.KubernetesDriverSpec\n+\n+/**\n+ * Represents the initial setup required for the driver.\n+ */\n+private[spark] class BaseDriverConfigurationStep(\n+    kubernetesAppId: String,\n+    kubernetesResourceNamePrefix: String,\n+    driverLabels: Map[String, String],\n+    dockerImagePullPolicy: String,\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) extends DriverConfigurationStep {\n+\n+  private val kubernetesDriverPodName = submissionSparkConf.get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(s\"$kubernetesResourceNamePrefix-driver\")\n+\n+  private val driverExtraClasspath = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_CLASS_PATH)\n+\n+  private val driverDockerImage = submissionSparkConf.get(DRIVER_DOCKER_IMAGE)\n+\n+  // CPU settings\n+  private val driverCpuCores = submissionSparkConf.getOption(\"spark.driver.cores\").getOrElse(\"1\")\n+  private val driverLimitCores = submissionSparkConf.get(KUBERNETES_DRIVER_LIMIT_CORES)\n+\n+  // Memory settings\n+  private val driverMemoryMiB = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_MEMORY)\n+  private val driverMemoryString = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_MEMORY.key,\n+    org.apache.spark.internal.config.DRIVER_MEMORY.defaultValueString)\n+  private val memoryOverheadMiB = submissionSparkConf\n+    .get(KUBERNETES_DRIVER_MEMORY_OVERHEAD)\n+    .getOrElse(math.max((MEMORY_OVERHEAD_FACTOR * driverMemoryMiB).toInt,\n+      MEMORY_OVERHEAD_MIN_MIB))\n+  private val driverContainerMemoryWithOverheadMiB = driverMemoryMiB + memoryOverheadMiB\n+\n+  override def configureDriver(\n+      driverSpec: KubernetesDriverSpec): KubernetesDriverSpec = {\n+    val driverExtraClasspathEnv = driverExtraClasspath.map { classPath =>\n+      new EnvVarBuilder()\n+        .withName(ENV_SUBMIT_EXTRA_CLASSPATH)\n+        .withValue(classPath)\n+        .build()\n+    }",
    "line": 75
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "We don't. It can be supported in the future. However, we also expect many of the use cases that normally would be handled by these properties to be manageable by building custom Docker images and letting those define the artifacts that are on the classpath.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-11-28T18:30:21Z",
    "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit.steps\n+\n+import scala.collection.JavaConverters._\n+\n+import io.fabric8.kubernetes.api.model.{ContainerBuilder, EnvVarBuilder, EnvVarSourceBuilder, PodBuilder, QuantityBuilder}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.KubernetesDriverSpec\n+\n+/**\n+ * Represents the initial setup required for the driver.\n+ */\n+private[spark] class BaseDriverConfigurationStep(\n+    kubernetesAppId: String,\n+    kubernetesResourceNamePrefix: String,\n+    driverLabels: Map[String, String],\n+    dockerImagePullPolicy: String,\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) extends DriverConfigurationStep {\n+\n+  private val kubernetesDriverPodName = submissionSparkConf.get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(s\"$kubernetesResourceNamePrefix-driver\")\n+\n+  private val driverExtraClasspath = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_CLASS_PATH)\n+\n+  private val driverDockerImage = submissionSparkConf.get(DRIVER_DOCKER_IMAGE)\n+\n+  // CPU settings\n+  private val driverCpuCores = submissionSparkConf.getOption(\"spark.driver.cores\").getOrElse(\"1\")\n+  private val driverLimitCores = submissionSparkConf.get(KUBERNETES_DRIVER_LIMIT_CORES)\n+\n+  // Memory settings\n+  private val driverMemoryMiB = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_MEMORY)\n+  private val driverMemoryString = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_MEMORY.key,\n+    org.apache.spark.internal.config.DRIVER_MEMORY.defaultValueString)\n+  private val memoryOverheadMiB = submissionSparkConf\n+    .get(KUBERNETES_DRIVER_MEMORY_OVERHEAD)\n+    .getOrElse(math.max((MEMORY_OVERHEAD_FACTOR * driverMemoryMiB).toInt,\n+      MEMORY_OVERHEAD_MIN_MIB))\n+  private val driverContainerMemoryWithOverheadMiB = driverMemoryMiB + memoryOverheadMiB\n+\n+  override def configureDriver(\n+      driverSpec: KubernetesDriverSpec): KubernetesDriverSpec = {\n+    val driverExtraClasspathEnv = driverExtraClasspath.map { classPath =>\n+      new EnvVarBuilder()\n+        .withName(ENV_SUBMIT_EXTRA_CLASSPATH)\n+        .withValue(classPath)\n+        .build()\n+    }",
    "line": 75
  }, {
    "author": {
      "login": "mridulm"
    },
    "body": "custom docker images ftw !",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-05T02:40:25Z",
    "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit.steps\n+\n+import scala.collection.JavaConverters._\n+\n+import io.fabric8.kubernetes.api.model.{ContainerBuilder, EnvVarBuilder, EnvVarSourceBuilder, PodBuilder, QuantityBuilder}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.KubernetesDriverSpec\n+\n+/**\n+ * Represents the initial setup required for the driver.\n+ */\n+private[spark] class BaseDriverConfigurationStep(\n+    kubernetesAppId: String,\n+    kubernetesResourceNamePrefix: String,\n+    driverLabels: Map[String, String],\n+    dockerImagePullPolicy: String,\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) extends DriverConfigurationStep {\n+\n+  private val kubernetesDriverPodName = submissionSparkConf.get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(s\"$kubernetesResourceNamePrefix-driver\")\n+\n+  private val driverExtraClasspath = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_CLASS_PATH)\n+\n+  private val driverDockerImage = submissionSparkConf.get(DRIVER_DOCKER_IMAGE)\n+\n+  // CPU settings\n+  private val driverCpuCores = submissionSparkConf.getOption(\"spark.driver.cores\").getOrElse(\"1\")\n+  private val driverLimitCores = submissionSparkConf.get(KUBERNETES_DRIVER_LIMIT_CORES)\n+\n+  // Memory settings\n+  private val driverMemoryMiB = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_MEMORY)\n+  private val driverMemoryString = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_MEMORY.key,\n+    org.apache.spark.internal.config.DRIVER_MEMORY.defaultValueString)\n+  private val memoryOverheadMiB = submissionSparkConf\n+    .get(KUBERNETES_DRIVER_MEMORY_OVERHEAD)\n+    .getOrElse(math.max((MEMORY_OVERHEAD_FACTOR * driverMemoryMiB).toInt,\n+      MEMORY_OVERHEAD_MIN_MIB))\n+  private val driverContainerMemoryWithOverheadMiB = driverMemoryMiB + memoryOverheadMiB\n+\n+  override def configureDriver(\n+      driverSpec: KubernetesDriverSpec): KubernetesDriverSpec = {\n+    val driverExtraClasspathEnv = driverExtraClasspath.map { classPath =>\n+      new EnvVarBuilder()\n+        .withName(ENV_SUBMIT_EXTRA_CLASSPATH)\n+        .withValue(classPath)\n+        .build()\n+    }",
    "line": 75
  }],
  "prId": 19717
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Why use the full path all over the place instead of importing?",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-04T19:59:45Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit.steps\n+\n+import scala.collection.JavaConverters._\n+\n+import io.fabric8.kubernetes.api.model.{ContainerBuilder, EnvVarBuilder, EnvVarSourceBuilder, PodBuilder, QuantityBuilder}\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.KubernetesDriverSpec\n+\n+/**\n+ * Represents the initial setup required for the driver.\n+ */\n+private[spark] class BaseDriverConfigurationStep(\n+    kubernetesAppId: String,\n+    kubernetesResourceNamePrefix: String,\n+    driverLabels: Map[String, String],\n+    dockerImagePullPolicy: String,\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) extends DriverConfigurationStep {\n+\n+  private val kubernetesDriverPodName = submissionSparkConf.get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(s\"$kubernetesResourceNamePrefix-driver\")\n+\n+  private val driverExtraClasspath = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_CLASS_PATH)"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Done.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-04T21:41:10Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit.steps\n+\n+import scala.collection.JavaConverters._\n+\n+import io.fabric8.kubernetes.api.model.{ContainerBuilder, EnvVarBuilder, EnvVarSourceBuilder, PodBuilder, QuantityBuilder}\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.KubernetesDriverSpec\n+\n+/**\n+ * Represents the initial setup required for the driver.\n+ */\n+private[spark] class BaseDriverConfigurationStep(\n+    kubernetesAppId: String,\n+    kubernetesResourceNamePrefix: String,\n+    driverLabels: Map[String, String],\n+    dockerImagePullPolicy: String,\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) extends DriverConfigurationStep {\n+\n+  private val kubernetesDriverPodName = submissionSparkConf.get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(s\"$kubernetesResourceNamePrefix-driver\")\n+\n+  private val driverExtraClasspath = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_CLASS_PATH)"
  }],
  "prId": 19717
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "You could move the `DRIVER_CORES` config from the YARN module to core.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-04T20:01:02Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit.steps\n+\n+import scala.collection.JavaConverters._\n+\n+import io.fabric8.kubernetes.api.model.{ContainerBuilder, EnvVarBuilder, EnvVarSourceBuilder, PodBuilder, QuantityBuilder}\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.KubernetesDriverSpec\n+\n+/**\n+ * Represents the initial setup required for the driver.\n+ */\n+private[spark] class BaseDriverConfigurationStep(\n+    kubernetesAppId: String,\n+    kubernetesResourceNamePrefix: String,\n+    driverLabels: Map[String, String],\n+    dockerImagePullPolicy: String,\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) extends DriverConfigurationStep {\n+\n+  private val kubernetesDriverPodName = submissionSparkConf.get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(s\"$kubernetesResourceNamePrefix-driver\")\n+\n+  private val driverExtraClasspath = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_CLASS_PATH)\n+\n+  private val driverDockerImage = submissionSparkConf\n+    .get(DRIVER_DOCKER_IMAGE)\n+    .getOrElse(throw new SparkException(\"Must specify the driver Docker image\"))\n+\n+  // CPU settings\n+  private val driverCpuCores = submissionSparkConf.getOption(\"spark.driver.cores\").getOrElse(\"1\")",
    "line": 54
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Agreed. But it seems `spark.driver.cores` is used in a lot of places. I think it needs a separate PR to union all of them. It also worths pointing out that the value of `spark.driver.cores` is used to set CPU request, and in Kubernetes this can be fractional, e.g., `0.1` or `100m`. `DRIVER_CORES` in yarn, however, only accepts integer values.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-04T22:53:06Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit.steps\n+\n+import scala.collection.JavaConverters._\n+\n+import io.fabric8.kubernetes.api.model.{ContainerBuilder, EnvVarBuilder, EnvVarSourceBuilder, PodBuilder, QuantityBuilder}\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.KubernetesDriverSpec\n+\n+/**\n+ * Represents the initial setup required for the driver.\n+ */\n+private[spark] class BaseDriverConfigurationStep(\n+    kubernetesAppId: String,\n+    kubernetesResourceNamePrefix: String,\n+    driverLabels: Map[String, String],\n+    dockerImagePullPolicy: String,\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) extends DriverConfigurationStep {\n+\n+  private val kubernetesDriverPodName = submissionSparkConf.get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(s\"$kubernetesResourceNamePrefix-driver\")\n+\n+  private val driverExtraClasspath = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_CLASS_PATH)\n+\n+  private val driverDockerImage = submissionSparkConf\n+    .get(DRIVER_DOCKER_IMAGE)\n+    .getOrElse(throw new SparkException(\"Must specify the driver Docker image\"))\n+\n+  // CPU settings\n+  private val driverCpuCores = submissionSparkConf.getOption(\"spark.driver.cores\").getOrElse(\"1\")",
    "line": 54
  }],
  "prId": 19717
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Another config that could use some re-factoring so that YARN and k8s use the same logic.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-04T20:01:51Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit.steps\n+\n+import scala.collection.JavaConverters._\n+\n+import io.fabric8.kubernetes.api.model.{ContainerBuilder, EnvVarBuilder, EnvVarSourceBuilder, PodBuilder, QuantityBuilder}\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.KubernetesDriverSpec\n+\n+/**\n+ * Represents the initial setup required for the driver.\n+ */\n+private[spark] class BaseDriverConfigurationStep(\n+    kubernetesAppId: String,\n+    kubernetesResourceNamePrefix: String,\n+    driverLabels: Map[String, String],\n+    dockerImagePullPolicy: String,\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) extends DriverConfigurationStep {\n+\n+  private val kubernetesDriverPodName = submissionSparkConf.get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(s\"$kubernetesResourceNamePrefix-driver\")\n+\n+  private val driverExtraClasspath = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_CLASS_PATH)\n+\n+  private val driverDockerImage = submissionSparkConf\n+    .get(DRIVER_DOCKER_IMAGE)\n+    .getOrElse(throw new SparkException(\"Must specify the driver Docker image\"))\n+\n+  // CPU settings\n+  private val driverCpuCores = submissionSparkConf.getOption(\"spark.driver.cores\").getOrElse(\"1\")\n+  private val driverLimitCores = submissionSparkConf.get(KUBERNETES_DRIVER_LIMIT_CORES)\n+\n+  // Memory settings\n+  private val driverMemoryMiB = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_MEMORY)\n+  private val driverMemoryString = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_MEMORY.key,\n+    org.apache.spark.internal.config.DRIVER_MEMORY.defaultValueString)\n+  private val memoryOverheadMiB = submissionSparkConf"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Merged both into `spark.driver.memoryOverhead` and used it in both yarn and k8s.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-04T22:56:44Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit.steps\n+\n+import scala.collection.JavaConverters._\n+\n+import io.fabric8.kubernetes.api.model.{ContainerBuilder, EnvVarBuilder, EnvVarSourceBuilder, PodBuilder, QuantityBuilder}\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.KubernetesDriverSpec\n+\n+/**\n+ * Represents the initial setup required for the driver.\n+ */\n+private[spark] class BaseDriverConfigurationStep(\n+    kubernetesAppId: String,\n+    kubernetesResourceNamePrefix: String,\n+    driverLabels: Map[String, String],\n+    dockerImagePullPolicy: String,\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) extends DriverConfigurationStep {\n+\n+  private val kubernetesDriverPodName = submissionSparkConf.get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(s\"$kubernetesResourceNamePrefix-driver\")\n+\n+  private val driverExtraClasspath = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_CLASS_PATH)\n+\n+  private val driverDockerImage = submissionSparkConf\n+    .get(DRIVER_DOCKER_IMAGE)\n+    .getOrElse(throw new SparkException(\"Must specify the driver Docker image\"))\n+\n+  // CPU settings\n+  private val driverCpuCores = submissionSparkConf.getOption(\"spark.driver.cores\").getOrElse(\"1\")\n+  private val driverLimitCores = submissionSparkConf.get(KUBERNETES_DRIVER_LIMIT_CORES)\n+\n+  // Memory settings\n+  private val driverMemoryMiB = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_MEMORY)\n+  private val driverMemoryString = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_MEMORY.key,\n+    org.apache.spark.internal.config.DRIVER_MEMORY.defaultValueString)\n+  private val memoryOverheadMiB = submissionSparkConf"
  }],
  "prId": 19717
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Fits in the previous line.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-04T20:02:35Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit.steps\n+\n+import scala.collection.JavaConverters._\n+\n+import io.fabric8.kubernetes.api.model.{ContainerBuilder, EnvVarBuilder, EnvVarSourceBuilder, PodBuilder, QuantityBuilder}\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.KubernetesDriverSpec\n+\n+/**\n+ * Represents the initial setup required for the driver.\n+ */\n+private[spark] class BaseDriverConfigurationStep(\n+    kubernetesAppId: String,\n+    kubernetesResourceNamePrefix: String,\n+    driverLabels: Map[String, String],\n+    dockerImagePullPolicy: String,\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) extends DriverConfigurationStep {\n+\n+  private val kubernetesDriverPodName = submissionSparkConf.get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(s\"$kubernetesResourceNamePrefix-driver\")\n+\n+  private val driverExtraClasspath = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_CLASS_PATH)\n+\n+  private val driverDockerImage = submissionSparkConf\n+    .get(DRIVER_DOCKER_IMAGE)\n+    .getOrElse(throw new SparkException(\"Must specify the driver Docker image\"))\n+\n+  // CPU settings\n+  private val driverCpuCores = submissionSparkConf.getOption(\"spark.driver.cores\").getOrElse(\"1\")\n+  private val driverLimitCores = submissionSparkConf.get(KUBERNETES_DRIVER_LIMIT_CORES)\n+\n+  // Memory settings\n+  private val driverMemoryMiB = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_MEMORY)\n+  private val driverMemoryString = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_MEMORY.key,\n+    org.apache.spark.internal.config.DRIVER_MEMORY.defaultValueString)\n+  private val memoryOverheadMiB = submissionSparkConf\n+    .get(KUBERNETES_DRIVER_MEMORY_OVERHEAD)\n+    .getOrElse(math.max((MEMORY_OVERHEAD_FACTOR * driverMemoryMiB).toInt,\n+      MEMORY_OVERHEAD_MIN_MIB))\n+  private val driverContainerMemoryWithOverheadMiB = driverMemoryMiB + memoryOverheadMiB\n+\n+  override def configureDriver(\n+      driverSpec: KubernetesDriverSpec): KubernetesDriverSpec = {"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Done.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-04T21:42:07Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit.steps\n+\n+import scala.collection.JavaConverters._\n+\n+import io.fabric8.kubernetes.api.model.{ContainerBuilder, EnvVarBuilder, EnvVarSourceBuilder, PodBuilder, QuantityBuilder}\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.KubernetesDriverSpec\n+\n+/**\n+ * Represents the initial setup required for the driver.\n+ */\n+private[spark] class BaseDriverConfigurationStep(\n+    kubernetesAppId: String,\n+    kubernetesResourceNamePrefix: String,\n+    driverLabels: Map[String, String],\n+    dockerImagePullPolicy: String,\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) extends DriverConfigurationStep {\n+\n+  private val kubernetesDriverPodName = submissionSparkConf.get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(s\"$kubernetesResourceNamePrefix-driver\")\n+\n+  private val driverExtraClasspath = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_CLASS_PATH)\n+\n+  private val driverDockerImage = submissionSparkConf\n+    .get(DRIVER_DOCKER_IMAGE)\n+    .getOrElse(throw new SparkException(\"Must specify the driver Docker image\"))\n+\n+  // CPU settings\n+  private val driverCpuCores = submissionSparkConf.getOption(\"spark.driver.cores\").getOrElse(\"1\")\n+  private val driverLimitCores = submissionSparkConf.get(KUBERNETES_DRIVER_LIMIT_CORES)\n+\n+  // Memory settings\n+  private val driverMemoryMiB = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_MEMORY)\n+  private val driverMemoryString = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_MEMORY.key,\n+    org.apache.spark.internal.config.DRIVER_MEMORY.defaultValueString)\n+  private val memoryOverheadMiB = submissionSparkConf\n+    .get(KUBERNETES_DRIVER_MEMORY_OVERHEAD)\n+    .getOrElse(math.max((MEMORY_OVERHEAD_FACTOR * driverMemoryMiB).toInt,\n+      MEMORY_OVERHEAD_MIN_MIB))\n+  private val driverContainerMemoryWithOverheadMiB = driverMemoryMiB + memoryOverheadMiB\n+\n+  override def configureDriver(\n+      driverSpec: KubernetesDriverSpec): KubernetesDriverSpec = {"
  }],
  "prId": 19717
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`.map { env =>`",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-04T20:03:09Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit.steps\n+\n+import scala.collection.JavaConverters._\n+\n+import io.fabric8.kubernetes.api.model.{ContainerBuilder, EnvVarBuilder, EnvVarSourceBuilder, PodBuilder, QuantityBuilder}\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.KubernetesDriverSpec\n+\n+/**\n+ * Represents the initial setup required for the driver.\n+ */\n+private[spark] class BaseDriverConfigurationStep(\n+    kubernetesAppId: String,\n+    kubernetesResourceNamePrefix: String,\n+    driverLabels: Map[String, String],\n+    dockerImagePullPolicy: String,\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) extends DriverConfigurationStep {\n+\n+  private val kubernetesDriverPodName = submissionSparkConf.get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(s\"$kubernetesResourceNamePrefix-driver\")\n+\n+  private val driverExtraClasspath = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_CLASS_PATH)\n+\n+  private val driverDockerImage = submissionSparkConf\n+    .get(DRIVER_DOCKER_IMAGE)\n+    .getOrElse(throw new SparkException(\"Must specify the driver Docker image\"))\n+\n+  // CPU settings\n+  private val driverCpuCores = submissionSparkConf.getOption(\"spark.driver.cores\").getOrElse(\"1\")\n+  private val driverLimitCores = submissionSparkConf.get(KUBERNETES_DRIVER_LIMIT_CORES)\n+\n+  // Memory settings\n+  private val driverMemoryMiB = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_MEMORY)\n+  private val driverMemoryString = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_MEMORY.key,\n+    org.apache.spark.internal.config.DRIVER_MEMORY.defaultValueString)\n+  private val memoryOverheadMiB = submissionSparkConf\n+    .get(KUBERNETES_DRIVER_MEMORY_OVERHEAD)\n+    .getOrElse(math.max((MEMORY_OVERHEAD_FACTOR * driverMemoryMiB).toInt,\n+      MEMORY_OVERHEAD_MIN_MIB))\n+  private val driverContainerMemoryWithOverheadMiB = driverMemoryMiB + memoryOverheadMiB\n+\n+  override def configureDriver(\n+      driverSpec: KubernetesDriverSpec): KubernetesDriverSpec = {\n+    val driverExtraClasspathEnv = driverExtraClasspath.map { classPath =>\n+      new EnvVarBuilder()\n+        .withName(ENV_SUBMIT_EXTRA_CLASSPATH)\n+        .withValue(classPath)\n+        .build()\n+    }\n+\n+    val driverCustomAnnotations = ConfigurationUtils\n+      .parsePrefixedKeyValuePairs(\n+        submissionSparkConf,\n+        KUBERNETES_DRIVER_ANNOTATION_PREFIX)\n+    require(!driverCustomAnnotations.contains(SPARK_APP_NAME_ANNOTATION),\n+      s\"Annotation with key $SPARK_APP_NAME_ANNOTATION is not allowed as it is reserved for\" +\n+        \" Spark bookkeeping operations.\")\n+\n+    val driverCustomEnvs = submissionSparkConf.getAllWithPrefix(KUBERNETES_DRIVER_ENV_KEY).toSeq\n+      .map(env =>"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Done.",
    "commit": "cbcd30ea0a5eb75ed831f50707bc97035c0a3371",
    "createdAt": "2017-12-04T22:56:48Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.submit.steps\n+\n+import scala.collection.JavaConverters._\n+\n+import io.fabric8.kubernetes.api.model.{ContainerBuilder, EnvVarBuilder, EnvVarSourceBuilder, PodBuilder, QuantityBuilder}\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.ConfigurationUtils\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.KubernetesDriverSpec\n+\n+/**\n+ * Represents the initial setup required for the driver.\n+ */\n+private[spark] class BaseDriverConfigurationStep(\n+    kubernetesAppId: String,\n+    kubernetesResourceNamePrefix: String,\n+    driverLabels: Map[String, String],\n+    dockerImagePullPolicy: String,\n+    appName: String,\n+    mainClass: String,\n+    appArgs: Array[String],\n+    submissionSparkConf: SparkConf) extends DriverConfigurationStep {\n+\n+  private val kubernetesDriverPodName = submissionSparkConf.get(KUBERNETES_DRIVER_POD_NAME)\n+    .getOrElse(s\"$kubernetesResourceNamePrefix-driver\")\n+\n+  private val driverExtraClasspath = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_CLASS_PATH)\n+\n+  private val driverDockerImage = submissionSparkConf\n+    .get(DRIVER_DOCKER_IMAGE)\n+    .getOrElse(throw new SparkException(\"Must specify the driver Docker image\"))\n+\n+  // CPU settings\n+  private val driverCpuCores = submissionSparkConf.getOption(\"spark.driver.cores\").getOrElse(\"1\")\n+  private val driverLimitCores = submissionSparkConf.get(KUBERNETES_DRIVER_LIMIT_CORES)\n+\n+  // Memory settings\n+  private val driverMemoryMiB = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_MEMORY)\n+  private val driverMemoryString = submissionSparkConf.get(\n+    org.apache.spark.internal.config.DRIVER_MEMORY.key,\n+    org.apache.spark.internal.config.DRIVER_MEMORY.defaultValueString)\n+  private val memoryOverheadMiB = submissionSparkConf\n+    .get(KUBERNETES_DRIVER_MEMORY_OVERHEAD)\n+    .getOrElse(math.max((MEMORY_OVERHEAD_FACTOR * driverMemoryMiB).toInt,\n+      MEMORY_OVERHEAD_MIN_MIB))\n+  private val driverContainerMemoryWithOverheadMiB = driverMemoryMiB + memoryOverheadMiB\n+\n+  override def configureDriver(\n+      driverSpec: KubernetesDriverSpec): KubernetesDriverSpec = {\n+    val driverExtraClasspathEnv = driverExtraClasspath.map { classPath =>\n+      new EnvVarBuilder()\n+        .withName(ENV_SUBMIT_EXTRA_CLASSPATH)\n+        .withValue(classPath)\n+        .build()\n+    }\n+\n+    val driverCustomAnnotations = ConfigurationUtils\n+      .parsePrefixedKeyValuePairs(\n+        submissionSparkConf,\n+        KUBERNETES_DRIVER_ANNOTATION_PREFIX)\n+    require(!driverCustomAnnotations.contains(SPARK_APP_NAME_ANNOTATION),\n+      s\"Annotation with key $SPARK_APP_NAME_ANNOTATION is not allowed as it is reserved for\" +\n+        \" Spark bookkeeping operations.\")\n+\n+    val driverCustomEnvs = submissionSparkConf.getAllWithPrefix(KUBERNETES_DRIVER_ENV_KEY).toSeq\n+      .map(env =>"
  }],
  "prId": 19717
}]