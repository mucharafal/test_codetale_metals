[{
  "comments": [{
    "author": {
      "login": "ifilonenko"
    },
    "body": "Shouldn't we check the contents of the token via some assert, later in the unit test?",
    "commit": "ccb39560298a5e54f144b8ba2a43d950289ccf34",
    "createdAt": "2018-11-14T22:44:00Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features\n+\n+import java.io.File\n+import java.nio.charset.StandardCharsets.UTF_8\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model.{ConfigMap, Secret}\n+import org.mockito.Mockito._\n+\n+import org.apache.spark.{SparkConf, SparkFunSuite}\n+import org.apache.spark.deploy.k8s._\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.JavaMainAppResource\n+import org.apache.spark.internal.config._\n+import org.apache.spark.util.Utils\n+\n+class KerberosConfDriverFeatureStepSuite extends SparkFunSuite {\n+\n+  import KubernetesFeaturesTestUtils._\n+  import SecretVolumeUtils._\n+\n+  private val tmpDir = Utils.createTempDir()\n+\n+  test(\"mount krb5 config map if defined\") {\n+    val configMap = \"testConfigMap\"\n+    val step = createStep(\n+      new SparkConf(false).set(KUBERNETES_KERBEROS_KRB5_CONFIG_MAP, configMap))\n+\n+    checkPodForKrbConf(step.configurePod(SparkPod.initialPod()), configMap)\n+    assert(step.getAdditionalPodSystemProperties().isEmpty)\n+    assert(filter[ConfigMap](step.getAdditionalKubernetesResources()).isEmpty)\n+  }\n+\n+  test(\"create krb5.conf config map if local config provided\") {\n+    val krbConf = File.createTempFile(\"krb5\", \".conf\", tmpDir)\n+    Files.write(\"some data\", krbConf, UTF_8)\n+\n+    val sparkConf = new SparkConf(false)\n+      .set(KUBERNETES_KERBEROS_KRB5_FILE, krbConf.getAbsolutePath())\n+    val step = createStep(sparkConf)\n+\n+    val confMap = filter[ConfigMap](step.getAdditionalKubernetesResources()).head\n+    assert(confMap.getData().keySet().asScala === Set(krbConf.getName()))\n+\n+    checkPodForKrbConf(step.configurePod(SparkPod.initialPod()), confMap.getMetadata().getName())\n+    assert(step.getAdditionalPodSystemProperties().isEmpty)\n+  }\n+\n+  test(\"create keytab secret if client keytab file used\") {\n+    val keytab = File.createTempFile(\"keytab\", \".bin\", tmpDir)\n+    Files.write(\"some data\", keytab, UTF_8)\n+\n+    val sparkConf = new SparkConf(false)\n+      .set(KEYTAB, keytab.getAbsolutePath())\n+      .set(PRINCIPAL, \"alice\")\n+    val step = createStep(sparkConf)\n+\n+    val pod = step.configurePod(SparkPod.initialPod())\n+    assert(podHasVolume(pod.pod, KERBEROS_KEYTAB_VOLUME))\n+    assert(containerHasVolume(pod.container, KERBEROS_KEYTAB_VOLUME, KERBEROS_KEYTAB_MOUNT_POINT))\n+\n+    assert(step.getAdditionalPodSystemProperties().keys === Set(KEYTAB.key))\n+\n+    val secret = filter[Secret](step.getAdditionalKubernetesResources()).head\n+    assert(secret.getData().keySet().asScala === Set(keytab.getName()))\n+  }\n+\n+  test(\"do nothing if container-local keytab used\") {\n+    val sparkConf = new SparkConf(false)\n+      .set(KEYTAB, \"local:/my.keytab\")\n+      .set(PRINCIPAL, \"alice\")\n+    val step = createStep(sparkConf)\n+\n+    val initial = SparkPod.initialPod()\n+    assert(step.configurePod(initial) === initial)\n+    assert(step.getAdditionalPodSystemProperties().isEmpty)\n+    assert(step.getAdditionalKubernetesResources().isEmpty)\n+  }\n+\n+  test(\"mount delegation tokens if provided\") {\n+    val dtSecret = \"tokenSecret\"\n+    val sparkConf = new SparkConf(false)\n+      .set(KUBERNETES_KERBEROS_DT_SECRET_NAME, dtSecret)\n+      .set(KUBERNETES_KERBEROS_DT_SECRET_ITEM_KEY, \"dtokens\")\n+    val step = createStep(sparkConf)\n+\n+    checkPodForTokens(step.configurePod(SparkPod.initialPod()), dtSecret)\n+    assert(step.getAdditionalPodSystemProperties().isEmpty)\n+    assert(step.getAdditionalKubernetesResources().isEmpty)\n+  }\n+\n+  test(\"create delegation tokens if needed\") {\n+    val step = spy(createStep(new SparkConf(false)))\n+    doReturn(Array[Byte](0x4, 0x2)).when(step).createDelegationTokens()"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Seems redundant, but why not.",
    "commit": "ccb39560298a5e54f144b8ba2a43d950289ccf34",
    "createdAt": "2018-11-14T22:47:09Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features\n+\n+import java.io.File\n+import java.nio.charset.StandardCharsets.UTF_8\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model.{ConfigMap, Secret}\n+import org.mockito.Mockito._\n+\n+import org.apache.spark.{SparkConf, SparkFunSuite}\n+import org.apache.spark.deploy.k8s._\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.JavaMainAppResource\n+import org.apache.spark.internal.config._\n+import org.apache.spark.util.Utils\n+\n+class KerberosConfDriverFeatureStepSuite extends SparkFunSuite {\n+\n+  import KubernetesFeaturesTestUtils._\n+  import SecretVolumeUtils._\n+\n+  private val tmpDir = Utils.createTempDir()\n+\n+  test(\"mount krb5 config map if defined\") {\n+    val configMap = \"testConfigMap\"\n+    val step = createStep(\n+      new SparkConf(false).set(KUBERNETES_KERBEROS_KRB5_CONFIG_MAP, configMap))\n+\n+    checkPodForKrbConf(step.configurePod(SparkPod.initialPod()), configMap)\n+    assert(step.getAdditionalPodSystemProperties().isEmpty)\n+    assert(filter[ConfigMap](step.getAdditionalKubernetesResources()).isEmpty)\n+  }\n+\n+  test(\"create krb5.conf config map if local config provided\") {\n+    val krbConf = File.createTempFile(\"krb5\", \".conf\", tmpDir)\n+    Files.write(\"some data\", krbConf, UTF_8)\n+\n+    val sparkConf = new SparkConf(false)\n+      .set(KUBERNETES_KERBEROS_KRB5_FILE, krbConf.getAbsolutePath())\n+    val step = createStep(sparkConf)\n+\n+    val confMap = filter[ConfigMap](step.getAdditionalKubernetesResources()).head\n+    assert(confMap.getData().keySet().asScala === Set(krbConf.getName()))\n+\n+    checkPodForKrbConf(step.configurePod(SparkPod.initialPod()), confMap.getMetadata().getName())\n+    assert(step.getAdditionalPodSystemProperties().isEmpty)\n+  }\n+\n+  test(\"create keytab secret if client keytab file used\") {\n+    val keytab = File.createTempFile(\"keytab\", \".bin\", tmpDir)\n+    Files.write(\"some data\", keytab, UTF_8)\n+\n+    val sparkConf = new SparkConf(false)\n+      .set(KEYTAB, keytab.getAbsolutePath())\n+      .set(PRINCIPAL, \"alice\")\n+    val step = createStep(sparkConf)\n+\n+    val pod = step.configurePod(SparkPod.initialPod())\n+    assert(podHasVolume(pod.pod, KERBEROS_KEYTAB_VOLUME))\n+    assert(containerHasVolume(pod.container, KERBEROS_KEYTAB_VOLUME, KERBEROS_KEYTAB_MOUNT_POINT))\n+\n+    assert(step.getAdditionalPodSystemProperties().keys === Set(KEYTAB.key))\n+\n+    val secret = filter[Secret](step.getAdditionalKubernetesResources()).head\n+    assert(secret.getData().keySet().asScala === Set(keytab.getName()))\n+  }\n+\n+  test(\"do nothing if container-local keytab used\") {\n+    val sparkConf = new SparkConf(false)\n+      .set(KEYTAB, \"local:/my.keytab\")\n+      .set(PRINCIPAL, \"alice\")\n+    val step = createStep(sparkConf)\n+\n+    val initial = SparkPod.initialPod()\n+    assert(step.configurePod(initial) === initial)\n+    assert(step.getAdditionalPodSystemProperties().isEmpty)\n+    assert(step.getAdditionalKubernetesResources().isEmpty)\n+  }\n+\n+  test(\"mount delegation tokens if provided\") {\n+    val dtSecret = \"tokenSecret\"\n+    val sparkConf = new SparkConf(false)\n+      .set(KUBERNETES_KERBEROS_DT_SECRET_NAME, dtSecret)\n+      .set(KUBERNETES_KERBEROS_DT_SECRET_ITEM_KEY, \"dtokens\")\n+    val step = createStep(sparkConf)\n+\n+    checkPodForTokens(step.configurePod(SparkPod.initialPod()), dtSecret)\n+    assert(step.getAdditionalPodSystemProperties().isEmpty)\n+    assert(step.getAdditionalKubernetesResources().isEmpty)\n+  }\n+\n+  test(\"create delegation tokens if needed\") {\n+    val step = spy(createStep(new SparkConf(false)))\n+    doReturn(Array[Byte](0x4, 0x2)).when(step).createDelegationTokens()"
  }],
  "prId": 22911
}, {
  "comments": [{
    "author": {
      "login": "ifilonenko"
    },
    "body": "The reason I am particular about using mocks is in the case that the internal logic changes (i.e. we remove using `SparkHadoopUtil` in future iterations, or whatnot.) But tbh, I am not hard-set on this as I see your point, and such changes aren't probable surrounding this code. So I guess I am fine with it. ",
    "commit": "ccb39560298a5e54f144b8ba2a43d950289ccf34",
    "createdAt": "2018-12-04T17:50:38Z",
    "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features\n+\n+import java.io.File\n+import java.nio.charset.StandardCharsets.UTF_8\n+import java.security.PrivilegedExceptionAction\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model.{ConfigMap, Secret}\n+import org.apache.commons.codec.binary.Base64\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.security.{Credentials, UserGroupInformation}\n+\n+import org.apache.spark.{SparkConf, SparkFunSuite}\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.k8s._\n+import org.apache.spark.deploy.k8s.Config._\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.submit.JavaMainAppResource\n+import org.apache.spark.internal.config._\n+import org.apache.spark.util.Utils\n+\n+class KerberosConfDriverFeatureStepSuite extends SparkFunSuite {\n+\n+  import KubernetesFeaturesTestUtils._\n+  import SecretVolumeUtils._\n+\n+  private val tmpDir = Utils.createTempDir()\n+\n+  test(\"mount krb5 config map if defined\") {\n+    val configMap = \"testConfigMap\"\n+    val step = createStep(\n+      new SparkConf(false).set(KUBERNETES_KERBEROS_KRB5_CONFIG_MAP, configMap))\n+\n+    checkPodForKrbConf(step.configurePod(SparkPod.initialPod()), configMap)\n+    assert(step.getAdditionalPodSystemProperties().isEmpty)\n+    assert(filter[ConfigMap](step.getAdditionalKubernetesResources()).isEmpty)\n+  }\n+\n+  test(\"create krb5.conf config map if local config provided\") {\n+    val krbConf = File.createTempFile(\"krb5\", \".conf\", tmpDir)\n+    Files.write(\"some data\", krbConf, UTF_8)\n+\n+    val sparkConf = new SparkConf(false)\n+      .set(KUBERNETES_KERBEROS_KRB5_FILE, krbConf.getAbsolutePath())\n+    val step = createStep(sparkConf)\n+\n+    val confMap = filter[ConfigMap](step.getAdditionalKubernetesResources()).head\n+    assert(confMap.getData().keySet().asScala === Set(krbConf.getName()))\n+\n+    checkPodForKrbConf(step.configurePod(SparkPod.initialPod()), confMap.getMetadata().getName())\n+    assert(step.getAdditionalPodSystemProperties().isEmpty)\n+  }\n+\n+  test(\"create keytab secret if client keytab file used\") {\n+    val keytab = File.createTempFile(\"keytab\", \".bin\", tmpDir)\n+    Files.write(\"some data\", keytab, UTF_8)\n+\n+    val sparkConf = new SparkConf(false)\n+      .set(KEYTAB, keytab.getAbsolutePath())\n+      .set(PRINCIPAL, \"alice\")\n+    val step = createStep(sparkConf)\n+\n+    val pod = step.configurePod(SparkPod.initialPod())\n+    assert(podHasVolume(pod.pod, KERBEROS_KEYTAB_VOLUME))\n+    assert(containerHasVolume(pod.container, KERBEROS_KEYTAB_VOLUME, KERBEROS_KEYTAB_MOUNT_POINT))\n+\n+    assert(step.getAdditionalPodSystemProperties().keys === Set(KEYTAB.key))\n+\n+    val secret = filter[Secret](step.getAdditionalKubernetesResources()).head\n+    assert(secret.getData().keySet().asScala === Set(keytab.getName()))\n+  }\n+\n+  test(\"do nothing if container-local keytab used\") {\n+    val sparkConf = new SparkConf(false)\n+      .set(KEYTAB, \"local:/my.keytab\")\n+      .set(PRINCIPAL, \"alice\")\n+    val step = createStep(sparkConf)\n+\n+    val initial = SparkPod.initialPod()\n+    assert(step.configurePod(initial) === initial)\n+    assert(step.getAdditionalPodSystemProperties().isEmpty)\n+    assert(step.getAdditionalKubernetesResources().isEmpty)\n+  }\n+\n+  test(\"mount delegation tokens if provided\") {\n+    val dtSecret = \"tokenSecret\"\n+    val sparkConf = new SparkConf(false)\n+      .set(KUBERNETES_KERBEROS_DT_SECRET_NAME, dtSecret)\n+      .set(KUBERNETES_KERBEROS_DT_SECRET_ITEM_KEY, \"dtokens\")\n+    val step = createStep(sparkConf)\n+\n+    checkPodForTokens(step.configurePod(SparkPod.initialPod()), dtSecret)\n+    assert(step.getAdditionalPodSystemProperties().isEmpty)\n+    assert(step.getAdditionalKubernetesResources().isEmpty)\n+  }\n+\n+  test(\"create delegation tokens if needed\") {\n+    // Since HadoopDelegationTokenManager does not create any tokens without proper configs and\n+    // services, start with a test user that already has some tokens that will just be piped\n+    // through to the driver.\n+    val testUser = UserGroupInformation.createUserForTesting(\"k8s\", Array())\n+    testUser.doAs(new PrivilegedExceptionAction[Unit]() {\n+      override def run(): Unit = {\n+        val creds = testUser.getCredentials()\n+        creds.addSecretKey(new Text(\"K8S_TEST_KEY\"), Array[Byte](0x4, 0x2))\n+        testUser.addCredentials(creds)\n+\n+        val tokens = SparkHadoopUtil.get.serialize(creds)",
    "line": 126
  }],
  "prId": 22911
}]