[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "labeled",
    "commit": "b85cfc4038c8de9340b78d10edf88ab76dd90ba3",
    "createdAt": "2017-10-17T21:23:22Z",
    "diffHunk": "@@ -0,0 +1,378 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{ExecutorService, ScheduledExecutorService, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.concurrent.Future\n+\n+import io.fabric8.kubernetes.api.model.{DoneablePod, Pod, PodBuilder, PodList}\n+import io.fabric8.kubernetes.client.{KubernetesClient, Watch, Watcher}\n+import io.fabric8.kubernetes.client.Watcher.Action\n+import io.fabric8.kubernetes.client.dsl.{FilterWatchListDeletable, MixedOperation, NonNamespaceOperation, PodResource}\n+import org.mockito.{AdditionalAnswers, ArgumentCaptor, Mock, MockitoAnnotations}\n+import org.mockito.Matchers.{any, eq => mockitoEq}\n+import org.mockito.Mockito.{mock => _, _}\n+import org.scalatest.BeforeAndAfter\n+import org.scalatest.mock.MockitoSugar._\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.deploy.k8s.config._\n+import org.apache.spark.deploy.k8s.constants._\n+import org.apache.spark.rpc._\n+import org.apache.spark.scheduler.{ExecutorExited, LiveListenerBus, SlaveLost, TaskSchedulerImpl}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.{RegisterExecutor, RemoveExecutor}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+private[spark] class KubernetesClusterSchedulerBackendSuite\n+    extends SparkFunSuite with BeforeAndAfter {\n+\n+  private val APP_ID = \"test-spark-app\"\n+  private val DRIVER_POD_NAME = \"spark-driver-pod\"\n+  private val NAMESPACE = \"test-namespace\"\n+  private val SPARK_DRIVER_HOST = \"localhost\"\n+  private val SPARK_DRIVER_PORT = 7077\n+  private val POD_ALLOCATION_INTERVAL = 60L\n+  private val DRIVER_URL = RpcEndpointAddress(\n+      SPARK_DRIVER_HOST, SPARK_DRIVER_PORT, CoarseGrainedSchedulerBackend.ENDPOINT_NAME).toString\n+  private val FIRST_EXECUTOR_POD = new PodBuilder()\n+    .withNewMetadata()\n+      .withName(\"pod1\")\n+      .endMetadata()\n+    .withNewSpec()\n+      .withNodeName(\"node1\")\n+      .endSpec()\n+    .withNewStatus()\n+      .withHostIP(\"192.168.99.100\")\n+      .endStatus()\n+    .build()\n+  private val SECOND_EXECUTOR_POD = new PodBuilder()\n+    .withNewMetadata()\n+      .withName(\"pod2\")\n+      .endMetadata()\n+    .withNewSpec()\n+      .withNodeName(\"node2\")\n+      .endSpec()\n+    .withNewStatus()\n+      .withHostIP(\"192.168.99.101\")\n+      .endStatus()\n+    .build()\n+\n+  private type PODS = MixedOperation[Pod, PodList, DoneablePod, PodResource[Pod, DoneablePod]]\n+  private type LABELLED_PODS = FilterWatchListDeletable["
  }],
  "prId": 19468
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "shorter test name?",
    "commit": "b85cfc4038c8de9340b78d10edf88ab76dd90ba3",
    "createdAt": "2017-10-17T21:24:19Z",
    "diffHunk": "@@ -0,0 +1,378 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{ExecutorService, ScheduledExecutorService, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.concurrent.Future\n+\n+import io.fabric8.kubernetes.api.model.{DoneablePod, Pod, PodBuilder, PodList}\n+import io.fabric8.kubernetes.client.{KubernetesClient, Watch, Watcher}\n+import io.fabric8.kubernetes.client.Watcher.Action\n+import io.fabric8.kubernetes.client.dsl.{FilterWatchListDeletable, MixedOperation, NonNamespaceOperation, PodResource}\n+import org.mockito.{AdditionalAnswers, ArgumentCaptor, Mock, MockitoAnnotations}\n+import org.mockito.Matchers.{any, eq => mockitoEq}\n+import org.mockito.Mockito.{mock => _, _}\n+import org.scalatest.BeforeAndAfter\n+import org.scalatest.mock.MockitoSugar._\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.deploy.k8s.config._\n+import org.apache.spark.deploy.k8s.constants._\n+import org.apache.spark.rpc._\n+import org.apache.spark.scheduler.{ExecutorExited, LiveListenerBus, SlaveLost, TaskSchedulerImpl}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.{RegisterExecutor, RemoveExecutor}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+private[spark] class KubernetesClusterSchedulerBackendSuite\n+    extends SparkFunSuite with BeforeAndAfter {\n+\n+  private val APP_ID = \"test-spark-app\"\n+  private val DRIVER_POD_NAME = \"spark-driver-pod\"\n+  private val NAMESPACE = \"test-namespace\"\n+  private val SPARK_DRIVER_HOST = \"localhost\"\n+  private val SPARK_DRIVER_PORT = 7077\n+  private val POD_ALLOCATION_INTERVAL = 60L\n+  private val DRIVER_URL = RpcEndpointAddress(\n+      SPARK_DRIVER_HOST, SPARK_DRIVER_PORT, CoarseGrainedSchedulerBackend.ENDPOINT_NAME).toString\n+  private val FIRST_EXECUTOR_POD = new PodBuilder()\n+    .withNewMetadata()\n+      .withName(\"pod1\")\n+      .endMetadata()\n+    .withNewSpec()\n+      .withNodeName(\"node1\")\n+      .endSpec()\n+    .withNewStatus()\n+      .withHostIP(\"192.168.99.100\")\n+      .endStatus()\n+    .build()\n+  private val SECOND_EXECUTOR_POD = new PodBuilder()\n+    .withNewMetadata()\n+      .withName(\"pod2\")\n+      .endMetadata()\n+    .withNewSpec()\n+      .withNodeName(\"node2\")\n+      .endSpec()\n+    .withNewStatus()\n+      .withHostIP(\"192.168.99.101\")\n+      .endStatus()\n+    .build()\n+\n+  private type PODS = MixedOperation[Pod, PodList, DoneablePod, PodResource[Pod, DoneablePod]]\n+  private type LABELLED_PODS = FilterWatchListDeletable[\n+      Pod, PodList, java.lang.Boolean, Watch, Watcher[Pod]]\n+  private type IN_NAMESPACE_PODS = NonNamespaceOperation[\n+      Pod, PodList, DoneablePod, PodResource[Pod, DoneablePod]]\n+\n+  @Mock\n+  private var sparkContext: SparkContext = _\n+\n+  @Mock\n+  private var listenerBus: LiveListenerBus = _\n+\n+  @Mock\n+  private var taskSchedulerImpl: TaskSchedulerImpl = _\n+\n+  @Mock\n+  private var allocatorExecutor: ScheduledExecutorService = _\n+\n+  @Mock\n+  private var requestExecutorsService: ExecutorService = _\n+\n+  @Mock\n+  private var executorPodFactory: ExecutorPodFactory = _\n+\n+  @Mock\n+  private var kubernetesClient: KubernetesClient = _\n+\n+  @Mock\n+  private var podOperations: PODS = _\n+\n+  @Mock\n+  private var podsWithLabelOperations: LABELLED_PODS = _\n+\n+  @Mock\n+  private var podsInNamespace: IN_NAMESPACE_PODS = _\n+\n+  @Mock\n+  private var podsWithDriverName: PodResource[Pod, DoneablePod] = _\n+\n+  @Mock\n+  private var rpcEnv: RpcEnv = _\n+\n+  @Mock\n+  private var driverEndpointRef: RpcEndpointRef = _\n+\n+  @Mock\n+  private var executorPodsWatch: Watch = _\n+\n+  private var sparkConf: SparkConf = _\n+  private var executorPodsWatcherArgument: ArgumentCaptor[Watcher[Pod]] = _\n+  private var allocatorRunnable: ArgumentCaptor[Runnable] = _\n+  private var requestExecutorRunnable: ArgumentCaptor[Runnable] = _\n+  private var driverEndpoint: ArgumentCaptor[RpcEndpoint] = _\n+\n+  private val driverPod = new PodBuilder()\n+    .withNewMetadata()\n+      .withName(DRIVER_POD_NAME)\n+      .addToLabels(SPARK_APP_ID_LABEL, APP_ID)\n+      .addToLabels(SPARK_ROLE_LABEL, SPARK_POD_DRIVER_ROLE)\n+      .endMetadata()\n+    .build()\n+\n+  before {\n+    MockitoAnnotations.initMocks(this)\n+    sparkConf = new SparkConf()\n+        .set(\"spark.app.id\", APP_ID)\n+        .set(KUBERNETES_DRIVER_POD_NAME, DRIVER_POD_NAME)\n+        .set(KUBERNETES_NAMESPACE, NAMESPACE)\n+        .set(\"spark.driver.host\", SPARK_DRIVER_HOST)\n+        .set(\"spark.driver.port\", SPARK_DRIVER_PORT.toString)\n+        .set(KUBERNETES_ALLOCATION_BATCH_DELAY, POD_ALLOCATION_INTERVAL)\n+    executorPodsWatcherArgument = ArgumentCaptor.forClass(classOf[Watcher[Pod]])\n+    allocatorRunnable = ArgumentCaptor.forClass(classOf[Runnable])\n+    requestExecutorRunnable = ArgumentCaptor.forClass(classOf[Runnable])\n+    driverEndpoint = ArgumentCaptor.forClass(classOf[RpcEndpoint])\n+    when(sparkContext.conf).thenReturn(sparkConf)\n+    when(sparkContext.listenerBus).thenReturn(listenerBus)\n+    when(taskSchedulerImpl.sc).thenReturn(sparkContext)\n+    when(kubernetesClient.pods()).thenReturn(podOperations)\n+    when(podOperations.withLabel(SPARK_APP_ID_LABEL, APP_ID)).thenReturn(podsWithLabelOperations)\n+    when(podsWithLabelOperations.watch(executorPodsWatcherArgument.capture()))\n+        .thenReturn(executorPodsWatch)\n+    when(podOperations.inNamespace(NAMESPACE)).thenReturn(podsInNamespace)\n+    when(podsInNamespace.withName(DRIVER_POD_NAME)).thenReturn(podsWithDriverName)\n+    when(podsWithDriverName.get()).thenReturn(driverPod)\n+    when(allocatorExecutor.scheduleWithFixedDelay(\n+        allocatorRunnable.capture(),\n+        mockitoEq(0L),\n+        mockitoEq(POD_ALLOCATION_INTERVAL),\n+        mockitoEq(TimeUnit.SECONDS))).thenReturn(null)\n+    // Creating Futures in Scala backed by a Java executor service resolves to running\n+    // ExecutorService#execute (as opposed to submit)\n+    doNothing().when(requestExecutorsService).execute(requestExecutorRunnable.capture())\n+    when(rpcEnv.setupEndpoint(\n+        mockitoEq(CoarseGrainedSchedulerBackend.ENDPOINT_NAME), driverEndpoint.capture()))\n+        .thenReturn(driverEndpointRef)\n+    when(driverEndpointRef.ask[Boolean]\n+      (any(classOf[Any]))\n+      (any())).thenReturn(mock[Future[Boolean]])\n+  }\n+\n+  test(\"Basic lifecycle expectations when starting and stopping the scheduler.\") {\n+    val scheduler = newSchedulerBackend()\n+    scheduler.start()\n+    assert(executorPodsWatcherArgument.getValue != null)\n+    assert(allocatorRunnable.getValue != null)\n+    scheduler.stop()\n+    verify(executorPodsWatch).close()\n+  }\n+\n+  test(\"Static allocation should request executors upon first allocator run.\") {\n+    sparkConf\n+        .set(KUBERNETES_ALLOCATION_BATCH_SIZE, 2)\n+        .set(org.apache.spark.internal.config.EXECUTOR_INSTANCES, 2)\n+    val scheduler = newSchedulerBackend()\n+    scheduler.start()\n+    requestExecutorRunnable.getValue.run()\n+    expectPodCreationWithId(1, FIRST_EXECUTOR_POD)\n+    expectPodCreationWithId(2, SECOND_EXECUTOR_POD)\n+    when(podOperations.create(any(classOf[Pod]))).thenAnswer(AdditionalAnswers.returnsFirstArg())\n+    allocatorRunnable.getValue.run()\n+    verify(podOperations).create(FIRST_EXECUTOR_POD)\n+    verify(podOperations).create(SECOND_EXECUTOR_POD)\n+  }\n+\n+  test(\"Killing executors deletes the executor pods\") {\n+    sparkConf\n+        .set(KUBERNETES_ALLOCATION_BATCH_SIZE, 2)\n+        .set(org.apache.spark.internal.config.EXECUTOR_INSTANCES, 2)\n+    val scheduler = newSchedulerBackend()\n+    scheduler.start()\n+    requestExecutorRunnable.getValue.run()\n+    expectPodCreationWithId(1, FIRST_EXECUTOR_POD)\n+    expectPodCreationWithId(2, SECOND_EXECUTOR_POD)\n+    when(podOperations.create(any(classOf[Pod])))\n+        .thenAnswer(AdditionalAnswers.returnsFirstArg())\n+    allocatorRunnable.getValue.run()\n+    scheduler.doKillExecutors(Seq(\"2\"))\n+    requestExecutorRunnable.getAllValues.asScala.last.run()\n+    verify(podOperations).delete(SECOND_EXECUTOR_POD)\n+    verify(podOperations, never()).delete(FIRST_EXECUTOR_POD)\n+  }\n+\n+  test(\"Executors should be requested in batches.\") {\n+    sparkConf\n+        .set(KUBERNETES_ALLOCATION_BATCH_SIZE, 1)\n+        .set(org.apache.spark.internal.config.EXECUTOR_INSTANCES, 2)\n+    val scheduler = newSchedulerBackend()\n+    scheduler.start()\n+    requestExecutorRunnable.getValue.run()\n+    when(podOperations.create(any(classOf[Pod])))\n+      .thenAnswer(AdditionalAnswers.returnsFirstArg())\n+    expectPodCreationWithId(1, FIRST_EXECUTOR_POD)\n+    expectPodCreationWithId(2, SECOND_EXECUTOR_POD)\n+    allocatorRunnable.getValue.run()\n+    verify(podOperations).create(FIRST_EXECUTOR_POD)\n+    verify(podOperations, never()).create(SECOND_EXECUTOR_POD)\n+    val registerFirstExecutorMessage = RegisterExecutor(\n+        \"1\", mock[RpcEndpointRef], \"localhost\", 1, Map.empty[String, String])\n+    when(taskSchedulerImpl.resourceOffers(any())).thenReturn(Seq.empty)\n+    driverEndpoint.getValue.receiveAndReply(mock[RpcCallContext])\n+        .apply(registerFirstExecutorMessage)\n+    allocatorRunnable.getValue.run()\n+    verify(podOperations).create(SECOND_EXECUTOR_POD)\n+  }\n+\n+  test(\"Deleting executors and then running an allocator pass after finding the loss reason\" +"
  }],
  "prId": 19468
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "name is too long, try to make them fit in one line.",
    "commit": "b85cfc4038c8de9340b78d10edf88ab76dd90ba3",
    "createdAt": "2017-10-17T21:24:52Z",
    "diffHunk": "@@ -0,0 +1,378 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{ExecutorService, ScheduledExecutorService, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.concurrent.Future\n+\n+import io.fabric8.kubernetes.api.model.{DoneablePod, Pod, PodBuilder, PodList}\n+import io.fabric8.kubernetes.client.{KubernetesClient, Watch, Watcher}\n+import io.fabric8.kubernetes.client.Watcher.Action\n+import io.fabric8.kubernetes.client.dsl.{FilterWatchListDeletable, MixedOperation, NonNamespaceOperation, PodResource}\n+import org.mockito.{AdditionalAnswers, ArgumentCaptor, Mock, MockitoAnnotations}\n+import org.mockito.Matchers.{any, eq => mockitoEq}\n+import org.mockito.Mockito.{mock => _, _}\n+import org.scalatest.BeforeAndAfter\n+import org.scalatest.mock.MockitoSugar._\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.deploy.k8s.config._\n+import org.apache.spark.deploy.k8s.constants._\n+import org.apache.spark.rpc._\n+import org.apache.spark.scheduler.{ExecutorExited, LiveListenerBus, SlaveLost, TaskSchedulerImpl}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.{RegisterExecutor, RemoveExecutor}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+\n+private[spark] class KubernetesClusterSchedulerBackendSuite\n+    extends SparkFunSuite with BeforeAndAfter {\n+\n+  private val APP_ID = \"test-spark-app\"\n+  private val DRIVER_POD_NAME = \"spark-driver-pod\"\n+  private val NAMESPACE = \"test-namespace\"\n+  private val SPARK_DRIVER_HOST = \"localhost\"\n+  private val SPARK_DRIVER_PORT = 7077\n+  private val POD_ALLOCATION_INTERVAL = 60L\n+  private val DRIVER_URL = RpcEndpointAddress(\n+      SPARK_DRIVER_HOST, SPARK_DRIVER_PORT, CoarseGrainedSchedulerBackend.ENDPOINT_NAME).toString\n+  private val FIRST_EXECUTOR_POD = new PodBuilder()\n+    .withNewMetadata()\n+      .withName(\"pod1\")\n+      .endMetadata()\n+    .withNewSpec()\n+      .withNodeName(\"node1\")\n+      .endSpec()\n+    .withNewStatus()\n+      .withHostIP(\"192.168.99.100\")\n+      .endStatus()\n+    .build()\n+  private val SECOND_EXECUTOR_POD = new PodBuilder()\n+    .withNewMetadata()\n+      .withName(\"pod2\")\n+      .endMetadata()\n+    .withNewSpec()\n+      .withNodeName(\"node2\")\n+      .endSpec()\n+    .withNewStatus()\n+      .withHostIP(\"192.168.99.101\")\n+      .endStatus()\n+    .build()\n+\n+  private type PODS = MixedOperation[Pod, PodList, DoneablePod, PodResource[Pod, DoneablePod]]\n+  private type LABELLED_PODS = FilterWatchListDeletable[\n+      Pod, PodList, java.lang.Boolean, Watch, Watcher[Pod]]\n+  private type IN_NAMESPACE_PODS = NonNamespaceOperation[\n+      Pod, PodList, DoneablePod, PodResource[Pod, DoneablePod]]\n+\n+  @Mock\n+  private var sparkContext: SparkContext = _\n+\n+  @Mock\n+  private var listenerBus: LiveListenerBus = _\n+\n+  @Mock\n+  private var taskSchedulerImpl: TaskSchedulerImpl = _\n+\n+  @Mock\n+  private var allocatorExecutor: ScheduledExecutorService = _\n+\n+  @Mock\n+  private var requestExecutorsService: ExecutorService = _\n+\n+  @Mock\n+  private var executorPodFactory: ExecutorPodFactory = _\n+\n+  @Mock\n+  private var kubernetesClient: KubernetesClient = _\n+\n+  @Mock\n+  private var podOperations: PODS = _\n+\n+  @Mock\n+  private var podsWithLabelOperations: LABELLED_PODS = _\n+\n+  @Mock\n+  private var podsInNamespace: IN_NAMESPACE_PODS = _\n+\n+  @Mock\n+  private var podsWithDriverName: PodResource[Pod, DoneablePod] = _\n+\n+  @Mock\n+  private var rpcEnv: RpcEnv = _\n+\n+  @Mock\n+  private var driverEndpointRef: RpcEndpointRef = _\n+\n+  @Mock\n+  private var executorPodsWatch: Watch = _\n+\n+  private var sparkConf: SparkConf = _\n+  private var executorPodsWatcherArgument: ArgumentCaptor[Watcher[Pod]] = _\n+  private var allocatorRunnable: ArgumentCaptor[Runnable] = _\n+  private var requestExecutorRunnable: ArgumentCaptor[Runnable] = _\n+  private var driverEndpoint: ArgumentCaptor[RpcEndpoint] = _\n+\n+  private val driverPod = new PodBuilder()\n+    .withNewMetadata()\n+      .withName(DRIVER_POD_NAME)\n+      .addToLabels(SPARK_APP_ID_LABEL, APP_ID)\n+      .addToLabels(SPARK_ROLE_LABEL, SPARK_POD_DRIVER_ROLE)\n+      .endMetadata()\n+    .build()\n+\n+  before {\n+    MockitoAnnotations.initMocks(this)\n+    sparkConf = new SparkConf()\n+        .set(\"spark.app.id\", APP_ID)\n+        .set(KUBERNETES_DRIVER_POD_NAME, DRIVER_POD_NAME)\n+        .set(KUBERNETES_NAMESPACE, NAMESPACE)\n+        .set(\"spark.driver.host\", SPARK_DRIVER_HOST)\n+        .set(\"spark.driver.port\", SPARK_DRIVER_PORT.toString)\n+        .set(KUBERNETES_ALLOCATION_BATCH_DELAY, POD_ALLOCATION_INTERVAL)\n+    executorPodsWatcherArgument = ArgumentCaptor.forClass(classOf[Watcher[Pod]])\n+    allocatorRunnable = ArgumentCaptor.forClass(classOf[Runnable])\n+    requestExecutorRunnable = ArgumentCaptor.forClass(classOf[Runnable])\n+    driverEndpoint = ArgumentCaptor.forClass(classOf[RpcEndpoint])\n+    when(sparkContext.conf).thenReturn(sparkConf)\n+    when(sparkContext.listenerBus).thenReturn(listenerBus)\n+    when(taskSchedulerImpl.sc).thenReturn(sparkContext)\n+    when(kubernetesClient.pods()).thenReturn(podOperations)\n+    when(podOperations.withLabel(SPARK_APP_ID_LABEL, APP_ID)).thenReturn(podsWithLabelOperations)\n+    when(podsWithLabelOperations.watch(executorPodsWatcherArgument.capture()))\n+        .thenReturn(executorPodsWatch)\n+    when(podOperations.inNamespace(NAMESPACE)).thenReturn(podsInNamespace)\n+    when(podsInNamespace.withName(DRIVER_POD_NAME)).thenReturn(podsWithDriverName)\n+    when(podsWithDriverName.get()).thenReturn(driverPod)\n+    when(allocatorExecutor.scheduleWithFixedDelay(\n+        allocatorRunnable.capture(),\n+        mockitoEq(0L),\n+        mockitoEq(POD_ALLOCATION_INTERVAL),\n+        mockitoEq(TimeUnit.SECONDS))).thenReturn(null)\n+    // Creating Futures in Scala backed by a Java executor service resolves to running\n+    // ExecutorService#execute (as opposed to submit)\n+    doNothing().when(requestExecutorsService).execute(requestExecutorRunnable.capture())\n+    when(rpcEnv.setupEndpoint(\n+        mockitoEq(CoarseGrainedSchedulerBackend.ENDPOINT_NAME), driverEndpoint.capture()))\n+        .thenReturn(driverEndpointRef)\n+    when(driverEndpointRef.ask[Boolean]\n+      (any(classOf[Any]))\n+      (any())).thenReturn(mock[Future[Boolean]])\n+  }\n+\n+  test(\"Basic lifecycle expectations when starting and stopping the scheduler.\") {\n+    val scheduler = newSchedulerBackend()\n+    scheduler.start()\n+    assert(executorPodsWatcherArgument.getValue != null)\n+    assert(allocatorRunnable.getValue != null)\n+    scheduler.stop()\n+    verify(executorPodsWatch).close()\n+  }\n+\n+  test(\"Static allocation should request executors upon first allocator run.\") {\n+    sparkConf\n+        .set(KUBERNETES_ALLOCATION_BATCH_SIZE, 2)\n+        .set(org.apache.spark.internal.config.EXECUTOR_INSTANCES, 2)\n+    val scheduler = newSchedulerBackend()\n+    scheduler.start()\n+    requestExecutorRunnable.getValue.run()\n+    expectPodCreationWithId(1, FIRST_EXECUTOR_POD)\n+    expectPodCreationWithId(2, SECOND_EXECUTOR_POD)\n+    when(podOperations.create(any(classOf[Pod]))).thenAnswer(AdditionalAnswers.returnsFirstArg())\n+    allocatorRunnable.getValue.run()\n+    verify(podOperations).create(FIRST_EXECUTOR_POD)\n+    verify(podOperations).create(SECOND_EXECUTOR_POD)\n+  }\n+\n+  test(\"Killing executors deletes the executor pods\") {\n+    sparkConf\n+        .set(KUBERNETES_ALLOCATION_BATCH_SIZE, 2)\n+        .set(org.apache.spark.internal.config.EXECUTOR_INSTANCES, 2)\n+    val scheduler = newSchedulerBackend()\n+    scheduler.start()\n+    requestExecutorRunnable.getValue.run()\n+    expectPodCreationWithId(1, FIRST_EXECUTOR_POD)\n+    expectPodCreationWithId(2, SECOND_EXECUTOR_POD)\n+    when(podOperations.create(any(classOf[Pod])))\n+        .thenAnswer(AdditionalAnswers.returnsFirstArg())\n+    allocatorRunnable.getValue.run()\n+    scheduler.doKillExecutors(Seq(\"2\"))\n+    requestExecutorRunnable.getAllValues.asScala.last.run()\n+    verify(podOperations).delete(SECOND_EXECUTOR_POD)\n+    verify(podOperations, never()).delete(FIRST_EXECUTOR_POD)\n+  }\n+\n+  test(\"Executors should be requested in batches.\") {\n+    sparkConf\n+        .set(KUBERNETES_ALLOCATION_BATCH_SIZE, 1)\n+        .set(org.apache.spark.internal.config.EXECUTOR_INSTANCES, 2)\n+    val scheduler = newSchedulerBackend()\n+    scheduler.start()\n+    requestExecutorRunnable.getValue.run()\n+    when(podOperations.create(any(classOf[Pod])))\n+      .thenAnswer(AdditionalAnswers.returnsFirstArg())\n+    expectPodCreationWithId(1, FIRST_EXECUTOR_POD)\n+    expectPodCreationWithId(2, SECOND_EXECUTOR_POD)\n+    allocatorRunnable.getValue.run()\n+    verify(podOperations).create(FIRST_EXECUTOR_POD)\n+    verify(podOperations, never()).create(SECOND_EXECUTOR_POD)\n+    val registerFirstExecutorMessage = RegisterExecutor(\n+        \"1\", mock[RpcEndpointRef], \"localhost\", 1, Map.empty[String, String])\n+    when(taskSchedulerImpl.resourceOffers(any())).thenReturn(Seq.empty)\n+    driverEndpoint.getValue.receiveAndReply(mock[RpcCallContext])\n+        .apply(registerFirstExecutorMessage)\n+    allocatorRunnable.getValue.run()\n+    verify(podOperations).create(SECOND_EXECUTOR_POD)\n+  }\n+\n+  test(\"Deleting executors and then running an allocator pass after finding the loss reason\" +\n+      \" should only delete the pod once.\") {\n+    sparkConf\n+        .set(KUBERNETES_ALLOCATION_BATCH_SIZE, 1)\n+        .set(org.apache.spark.internal.config.EXECUTOR_INSTANCES, 1)\n+    val scheduler = newSchedulerBackend()\n+    scheduler.start()\n+    requestExecutorRunnable.getValue.run()\n+    when(podOperations.create(any(classOf[Pod])))\n+        .thenAnswer(AdditionalAnswers.returnsFirstArg())\n+    expectPodCreationWithId(1, FIRST_EXECUTOR_POD)\n+    allocatorRunnable.getValue.run()\n+    val executorEndpointRef = mock[RpcEndpointRef]\n+    when(executorEndpointRef.address).thenReturn(RpcAddress(\"pod.example.com\", 9000))\n+    val registerFirstExecutorMessage = RegisterExecutor(\n+        \"1\", executorEndpointRef, \"localhost:9000\", 1, Map.empty[String, String])\n+    when(taskSchedulerImpl.resourceOffers(any())).thenReturn(Seq.empty)\n+    driverEndpoint.getValue.receiveAndReply(mock[RpcCallContext])\n+        .apply(registerFirstExecutorMessage)\n+    scheduler.doRequestTotalExecutors(0)\n+    requestExecutorRunnable.getAllValues.asScala.last.run()\n+    scheduler.doKillExecutors(Seq(\"1\"))\n+    requestExecutorRunnable.getAllValues.asScala.last.run()\n+    verify(podOperations, times(1)).delete(FIRST_EXECUTOR_POD)\n+    driverEndpoint.getValue.onDisconnected(executorEndpointRef.address)\n+\n+    val exitedPod = exitPod(FIRST_EXECUTOR_POD, 0)\n+    executorPodsWatcherArgument.getValue.eventReceived(Action.DELETED, exitedPod)\n+    allocatorRunnable.getValue.run()\n+    verify(podOperations, times(1)).delete(FIRST_EXECUTOR_POD)\n+    verify(driverEndpointRef, times(1)).ask[Boolean](\n+        RemoveExecutor(\"1\", ExecutorExited(\n+            0,\n+            exitCausedByApp = false,\n+            s\"Container in pod ${exitedPod.getMetadata.getName} exited from\" +\n+              s\" explicit termination request.\")))\n+  }\n+\n+  test(\"Executors that disconnect from application errors are noted as exits caused by app.\") {\n+    sparkConf\n+        .set(KUBERNETES_ALLOCATION_BATCH_SIZE, 1)\n+        .set(org.apache.spark.internal.config.EXECUTOR_INSTANCES, 1)\n+    val scheduler = newSchedulerBackend()\n+    scheduler.start()\n+    expectPodCreationWithId(1, FIRST_EXECUTOR_POD)\n+    when(podOperations.create(any(classOf[Pod]))).thenAnswer(AdditionalAnswers.returnsFirstArg())\n+    requestExecutorRunnable.getValue.run()\n+    allocatorRunnable.getValue.run()\n+    val executorEndpointRef = mock[RpcEndpointRef]\n+    when(executorEndpointRef.address).thenReturn(RpcAddress(\"pod.example.com\", 9000))\n+    val registerFirstExecutorMessage = RegisterExecutor(\n+        \"1\", executorEndpointRef, \"localhost:9000\", 1, Map.empty[String, String])\n+    when(taskSchedulerImpl.resourceOffers(any())).thenReturn(Seq.empty)\n+    driverEndpoint.getValue.receiveAndReply(mock[RpcCallContext])\n+        .apply(registerFirstExecutorMessage)\n+    driverEndpoint.getValue.onDisconnected(executorEndpointRef.address)\n+    executorPodsWatcherArgument.getValue.eventReceived(\n+        Action.ERROR, exitPod(FIRST_EXECUTOR_POD, 1))\n+\n+    expectPodCreationWithId(2, SECOND_EXECUTOR_POD)\n+    scheduler.doRequestTotalExecutors(1)\n+    requestExecutorRunnable.getValue.run()\n+    allocatorRunnable.getAllValues.asScala.last.run()\n+    verify(driverEndpointRef).ask[Boolean](\n+        RemoveExecutor(\"1\", ExecutorExited(\n+            1,\n+            exitCausedByApp = true,\n+            s\"Pod ${FIRST_EXECUTOR_POD.getMetadata.getName}'s executor container exited with\" +\n+              \" exit status code 1.\")))\n+    verify(podOperations, never()).delete(FIRST_EXECUTOR_POD)\n+  }\n+\n+  test(\"Executors should only try to get the loss reason a number of times before giving up and\" +"
  }],
  "prId": 19468
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "ditto.",
    "commit": "b85cfc4038c8de9340b78d10edf88ab76dd90ba3",
    "createdAt": "2017-11-21T09:27:40Z",
    "diffHunk": "@@ -0,0 +1,439 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{ExecutorService, ScheduledExecutorService, TimeUnit}\n+\n+import io.fabric8.kubernetes.api.model.{DoneablePod, Pod, PodBuilder, PodList}\n+import io.fabric8.kubernetes.client.{KubernetesClient, Watch, Watcher}\n+import io.fabric8.kubernetes.client.Watcher.Action\n+import io.fabric8.kubernetes.client.dsl.{FilterWatchListDeletable, MixedOperation, NonNamespaceOperation, PodResource}\n+import org.mockito.{AdditionalAnswers, ArgumentCaptor, Mock, MockitoAnnotations}\n+import org.mockito.Matchers.{any, eq => mockitoEq}\n+import org.mockito.Mockito.{doNothing, never, times, verify, when}\n+import org.scalatest.BeforeAndAfter\n+import org.scalatest.mock.MockitoSugar._\n+import scala.collection.JavaConverters._\n+import scala.concurrent.Future\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.deploy.k8s.config._\n+import org.apache.spark.deploy.k8s.constants._\n+import org.apache.spark.rpc._\n+import org.apache.spark.scheduler.{ExecutorExited, LiveListenerBus, SlaveLost, TaskSchedulerImpl}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.{RegisterExecutor, RemoveExecutor}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+import org.apache.spark.util.ThreadUtils\n+\n+class KubernetesClusterSchedulerBackendSuite\n+    extends SparkFunSuite with BeforeAndAfter {\n+\n+  private val APP_ID = \"test-spark-app\"\n+  private val DRIVER_POD_NAME = \"spark-driver-pod\"\n+  private val NAMESPACE = \"test-namespace\"\n+  private val SPARK_DRIVER_HOST = \"localhost\"\n+  private val SPARK_DRIVER_PORT = 7077\n+  private val POD_ALLOCATION_INTERVAL = 60L\n+  private val DRIVER_URL = RpcEndpointAddress(\n+      SPARK_DRIVER_HOST, SPARK_DRIVER_PORT, CoarseGrainedSchedulerBackend.ENDPOINT_NAME).toString"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Done.",
    "commit": "b85cfc4038c8de9340b78d10edf88ab76dd90ba3",
    "createdAt": "2017-11-21T19:12:35Z",
    "diffHunk": "@@ -0,0 +1,439 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{ExecutorService, ScheduledExecutorService, TimeUnit}\n+\n+import io.fabric8.kubernetes.api.model.{DoneablePod, Pod, PodBuilder, PodList}\n+import io.fabric8.kubernetes.client.{KubernetesClient, Watch, Watcher}\n+import io.fabric8.kubernetes.client.Watcher.Action\n+import io.fabric8.kubernetes.client.dsl.{FilterWatchListDeletable, MixedOperation, NonNamespaceOperation, PodResource}\n+import org.mockito.{AdditionalAnswers, ArgumentCaptor, Mock, MockitoAnnotations}\n+import org.mockito.Matchers.{any, eq => mockitoEq}\n+import org.mockito.Mockito.{doNothing, never, times, verify, when}\n+import org.scalatest.BeforeAndAfter\n+import org.scalatest.mock.MockitoSugar._\n+import scala.collection.JavaConverters._\n+import scala.concurrent.Future\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.deploy.k8s.config._\n+import org.apache.spark.deploy.k8s.constants._\n+import org.apache.spark.rpc._\n+import org.apache.spark.scheduler.{ExecutorExited, LiveListenerBus, SlaveLost, TaskSchedulerImpl}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.{RegisterExecutor, RemoveExecutor}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+import org.apache.spark.util.ThreadUtils\n+\n+class KubernetesClusterSchedulerBackendSuite\n+    extends SparkFunSuite with BeforeAndAfter {\n+\n+  private val APP_ID = \"test-spark-app\"\n+  private val DRIVER_POD_NAME = \"spark-driver-pod\"\n+  private val NAMESPACE = \"test-namespace\"\n+  private val SPARK_DRIVER_HOST = \"localhost\"\n+  private val SPARK_DRIVER_PORT = 7077\n+  private val POD_ALLOCATION_INTERVAL = 60L\n+  private val DRIVER_URL = RpcEndpointAddress(\n+      SPARK_DRIVER_HOST, SPARK_DRIVER_PORT, CoarseGrainedSchedulerBackend.ENDPOINT_NAME).toString"
  }],
  "prId": 19468
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "ditto.",
    "commit": "b85cfc4038c8de9340b78d10edf88ab76dd90ba3",
    "createdAt": "2017-11-21T09:27:51Z",
    "diffHunk": "@@ -0,0 +1,439 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{ExecutorService, ScheduledExecutorService, TimeUnit}\n+\n+import io.fabric8.kubernetes.api.model.{DoneablePod, Pod, PodBuilder, PodList}\n+import io.fabric8.kubernetes.client.{KubernetesClient, Watch, Watcher}\n+import io.fabric8.kubernetes.client.Watcher.Action\n+import io.fabric8.kubernetes.client.dsl.{FilterWatchListDeletable, MixedOperation, NonNamespaceOperation, PodResource}\n+import org.mockito.{AdditionalAnswers, ArgumentCaptor, Mock, MockitoAnnotations}\n+import org.mockito.Matchers.{any, eq => mockitoEq}\n+import org.mockito.Mockito.{doNothing, never, times, verify, when}\n+import org.scalatest.BeforeAndAfter\n+import org.scalatest.mock.MockitoSugar._\n+import scala.collection.JavaConverters._\n+import scala.concurrent.Future\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.deploy.k8s.config._\n+import org.apache.spark.deploy.k8s.constants._\n+import org.apache.spark.rpc._\n+import org.apache.spark.scheduler.{ExecutorExited, LiveListenerBus, SlaveLost, TaskSchedulerImpl}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.{RegisterExecutor, RemoveExecutor}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+import org.apache.spark.util.ThreadUtils\n+\n+class KubernetesClusterSchedulerBackendSuite\n+    extends SparkFunSuite with BeforeAndAfter {\n+\n+  private val APP_ID = \"test-spark-app\"\n+  private val DRIVER_POD_NAME = \"spark-driver-pod\"\n+  private val NAMESPACE = \"test-namespace\"\n+  private val SPARK_DRIVER_HOST = \"localhost\"\n+  private val SPARK_DRIVER_PORT = 7077\n+  private val POD_ALLOCATION_INTERVAL = 60L\n+  private val DRIVER_URL = RpcEndpointAddress(\n+      SPARK_DRIVER_HOST, SPARK_DRIVER_PORT, CoarseGrainedSchedulerBackend.ENDPOINT_NAME).toString\n+  private val FIRST_EXECUTOR_POD = new PodBuilder()\n+    .withNewMetadata()\n+      .withName(\"pod1\")\n+      .endMetadata()\n+    .withNewSpec()\n+      .withNodeName(\"node1\")\n+      .endSpec()\n+    .withNewStatus()\n+      .withHostIP(\"192.168.99.100\")\n+      .endStatus()\n+    .build()\n+  private val SECOND_EXECUTOR_POD = new PodBuilder()\n+    .withNewMetadata()\n+      .withName(\"pod2\")\n+      .endMetadata()\n+    .withNewSpec()\n+      .withNodeName(\"node2\")\n+      .endSpec()\n+    .withNewStatus()\n+      .withHostIP(\"192.168.99.101\")\n+      .endStatus()\n+    .build()\n+\n+  private type PODS = MixedOperation[Pod, PodList, DoneablePod, PodResource[Pod, DoneablePod]]\n+  private type LABELED_PODS = FilterWatchListDeletable[\n+      Pod, PodList, java.lang.Boolean, Watch, Watcher[Pod]]"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Done.",
    "commit": "b85cfc4038c8de9340b78d10edf88ab76dd90ba3",
    "createdAt": "2017-11-21T19:12:38Z",
    "diffHunk": "@@ -0,0 +1,439 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{ExecutorService, ScheduledExecutorService, TimeUnit}\n+\n+import io.fabric8.kubernetes.api.model.{DoneablePod, Pod, PodBuilder, PodList}\n+import io.fabric8.kubernetes.client.{KubernetesClient, Watch, Watcher}\n+import io.fabric8.kubernetes.client.Watcher.Action\n+import io.fabric8.kubernetes.client.dsl.{FilterWatchListDeletable, MixedOperation, NonNamespaceOperation, PodResource}\n+import org.mockito.{AdditionalAnswers, ArgumentCaptor, Mock, MockitoAnnotations}\n+import org.mockito.Matchers.{any, eq => mockitoEq}\n+import org.mockito.Mockito.{doNothing, never, times, verify, when}\n+import org.scalatest.BeforeAndAfter\n+import org.scalatest.mock.MockitoSugar._\n+import scala.collection.JavaConverters._\n+import scala.concurrent.Future\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.deploy.k8s.config._\n+import org.apache.spark.deploy.k8s.constants._\n+import org.apache.spark.rpc._\n+import org.apache.spark.scheduler.{ExecutorExited, LiveListenerBus, SlaveLost, TaskSchedulerImpl}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.{RegisterExecutor, RemoveExecutor}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+import org.apache.spark.util.ThreadUtils\n+\n+class KubernetesClusterSchedulerBackendSuite\n+    extends SparkFunSuite with BeforeAndAfter {\n+\n+  private val APP_ID = \"test-spark-app\"\n+  private val DRIVER_POD_NAME = \"spark-driver-pod\"\n+  private val NAMESPACE = \"test-namespace\"\n+  private val SPARK_DRIVER_HOST = \"localhost\"\n+  private val SPARK_DRIVER_PORT = 7077\n+  private val POD_ALLOCATION_INTERVAL = 60L\n+  private val DRIVER_URL = RpcEndpointAddress(\n+      SPARK_DRIVER_HOST, SPARK_DRIVER_PORT, CoarseGrainedSchedulerBackend.ENDPOINT_NAME).toString\n+  private val FIRST_EXECUTOR_POD = new PodBuilder()\n+    .withNewMetadata()\n+      .withName(\"pod1\")\n+      .endMetadata()\n+    .withNewSpec()\n+      .withNodeName(\"node1\")\n+      .endSpec()\n+    .withNewStatus()\n+      .withHostIP(\"192.168.99.100\")\n+      .endStatus()\n+    .build()\n+  private val SECOND_EXECUTOR_POD = new PodBuilder()\n+    .withNewMetadata()\n+      .withName(\"pod2\")\n+      .endMetadata()\n+    .withNewSpec()\n+      .withNodeName(\"node2\")\n+      .endSpec()\n+    .withNewStatus()\n+      .withHostIP(\"192.168.99.101\")\n+      .endStatus()\n+    .build()\n+\n+  private type PODS = MixedOperation[Pod, PodList, DoneablePod, PodResource[Pod, DoneablePod]]\n+  private type LABELED_PODS = FilterWatchListDeletable[\n+      Pod, PodList, java.lang.Boolean, Watch, Watcher[Pod]]"
  }],
  "prId": 19468
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "ditto.",
    "commit": "b85cfc4038c8de9340b78d10edf88ab76dd90ba3",
    "createdAt": "2017-11-21T09:27:59Z",
    "diffHunk": "@@ -0,0 +1,439 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{ExecutorService, ScheduledExecutorService, TimeUnit}\n+\n+import io.fabric8.kubernetes.api.model.{DoneablePod, Pod, PodBuilder, PodList}\n+import io.fabric8.kubernetes.client.{KubernetesClient, Watch, Watcher}\n+import io.fabric8.kubernetes.client.Watcher.Action\n+import io.fabric8.kubernetes.client.dsl.{FilterWatchListDeletable, MixedOperation, NonNamespaceOperation, PodResource}\n+import org.mockito.{AdditionalAnswers, ArgumentCaptor, Mock, MockitoAnnotations}\n+import org.mockito.Matchers.{any, eq => mockitoEq}\n+import org.mockito.Mockito.{doNothing, never, times, verify, when}\n+import org.scalatest.BeforeAndAfter\n+import org.scalatest.mock.MockitoSugar._\n+import scala.collection.JavaConverters._\n+import scala.concurrent.Future\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.deploy.k8s.config._\n+import org.apache.spark.deploy.k8s.constants._\n+import org.apache.spark.rpc._\n+import org.apache.spark.scheduler.{ExecutorExited, LiveListenerBus, SlaveLost, TaskSchedulerImpl}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.{RegisterExecutor, RemoveExecutor}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+import org.apache.spark.util.ThreadUtils\n+\n+class KubernetesClusterSchedulerBackendSuite\n+    extends SparkFunSuite with BeforeAndAfter {\n+\n+  private val APP_ID = \"test-spark-app\"\n+  private val DRIVER_POD_NAME = \"spark-driver-pod\"\n+  private val NAMESPACE = \"test-namespace\"\n+  private val SPARK_DRIVER_HOST = \"localhost\"\n+  private val SPARK_DRIVER_PORT = 7077\n+  private val POD_ALLOCATION_INTERVAL = 60L\n+  private val DRIVER_URL = RpcEndpointAddress(\n+      SPARK_DRIVER_HOST, SPARK_DRIVER_PORT, CoarseGrainedSchedulerBackend.ENDPOINT_NAME).toString\n+  private val FIRST_EXECUTOR_POD = new PodBuilder()\n+    .withNewMetadata()\n+      .withName(\"pod1\")\n+      .endMetadata()\n+    .withNewSpec()\n+      .withNodeName(\"node1\")\n+      .endSpec()\n+    .withNewStatus()\n+      .withHostIP(\"192.168.99.100\")\n+      .endStatus()\n+    .build()\n+  private val SECOND_EXECUTOR_POD = new PodBuilder()\n+    .withNewMetadata()\n+      .withName(\"pod2\")\n+      .endMetadata()\n+    .withNewSpec()\n+      .withNodeName(\"node2\")\n+      .endSpec()\n+    .withNewStatus()\n+      .withHostIP(\"192.168.99.101\")\n+      .endStatus()\n+    .build()\n+\n+  private type PODS = MixedOperation[Pod, PodList, DoneablePod, PodResource[Pod, DoneablePod]]\n+  private type LABELED_PODS = FilterWatchListDeletable[\n+      Pod, PodList, java.lang.Boolean, Watch, Watcher[Pod]]\n+  private type IN_NAMESPACE_PODS = NonNamespaceOperation[\n+      Pod, PodList, DoneablePod, PodResource[Pod, DoneablePod]]"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Done.",
    "commit": "b85cfc4038c8de9340b78d10edf88ab76dd90ba3",
    "createdAt": "2017-11-21T19:12:41Z",
    "diffHunk": "@@ -0,0 +1,439 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{ExecutorService, ScheduledExecutorService, TimeUnit}\n+\n+import io.fabric8.kubernetes.api.model.{DoneablePod, Pod, PodBuilder, PodList}\n+import io.fabric8.kubernetes.client.{KubernetesClient, Watch, Watcher}\n+import io.fabric8.kubernetes.client.Watcher.Action\n+import io.fabric8.kubernetes.client.dsl.{FilterWatchListDeletable, MixedOperation, NonNamespaceOperation, PodResource}\n+import org.mockito.{AdditionalAnswers, ArgumentCaptor, Mock, MockitoAnnotations}\n+import org.mockito.Matchers.{any, eq => mockitoEq}\n+import org.mockito.Mockito.{doNothing, never, times, verify, when}\n+import org.scalatest.BeforeAndAfter\n+import org.scalatest.mock.MockitoSugar._\n+import scala.collection.JavaConverters._\n+import scala.concurrent.Future\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.deploy.k8s.config._\n+import org.apache.spark.deploy.k8s.constants._\n+import org.apache.spark.rpc._\n+import org.apache.spark.scheduler.{ExecutorExited, LiveListenerBus, SlaveLost, TaskSchedulerImpl}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.{RegisterExecutor, RemoveExecutor}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+import org.apache.spark.util.ThreadUtils\n+\n+class KubernetesClusterSchedulerBackendSuite\n+    extends SparkFunSuite with BeforeAndAfter {\n+\n+  private val APP_ID = \"test-spark-app\"\n+  private val DRIVER_POD_NAME = \"spark-driver-pod\"\n+  private val NAMESPACE = \"test-namespace\"\n+  private val SPARK_DRIVER_HOST = \"localhost\"\n+  private val SPARK_DRIVER_PORT = 7077\n+  private val POD_ALLOCATION_INTERVAL = 60L\n+  private val DRIVER_URL = RpcEndpointAddress(\n+      SPARK_DRIVER_HOST, SPARK_DRIVER_PORT, CoarseGrainedSchedulerBackend.ENDPOINT_NAME).toString\n+  private val FIRST_EXECUTOR_POD = new PodBuilder()\n+    .withNewMetadata()\n+      .withName(\"pod1\")\n+      .endMetadata()\n+    .withNewSpec()\n+      .withNodeName(\"node1\")\n+      .endSpec()\n+    .withNewStatus()\n+      .withHostIP(\"192.168.99.100\")\n+      .endStatus()\n+    .build()\n+  private val SECOND_EXECUTOR_POD = new PodBuilder()\n+    .withNewMetadata()\n+      .withName(\"pod2\")\n+      .endMetadata()\n+    .withNewSpec()\n+      .withNodeName(\"node2\")\n+      .endSpec()\n+    .withNewStatus()\n+      .withHostIP(\"192.168.99.101\")\n+      .endStatus()\n+    .build()\n+\n+  private type PODS = MixedOperation[Pod, PodList, DoneablePod, PodResource[Pod, DoneablePod]]\n+  private type LABELED_PODS = FilterWatchListDeletable[\n+      Pod, PodList, java.lang.Boolean, Watch, Watcher[Pod]]\n+  private type IN_NAMESPACE_PODS = NonNamespaceOperation[\n+      Pod, PodList, DoneablePod, PodResource[Pod, DoneablePod]]"
  }],
  "prId": 19468
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "nit: indent",
    "commit": "b85cfc4038c8de9340b78d10edf88ab76dd90ba3",
    "createdAt": "2017-11-21T09:28:52Z",
    "diffHunk": "@@ -0,0 +1,439 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{ExecutorService, ScheduledExecutorService, TimeUnit}\n+\n+import io.fabric8.kubernetes.api.model.{DoneablePod, Pod, PodBuilder, PodList}\n+import io.fabric8.kubernetes.client.{KubernetesClient, Watch, Watcher}\n+import io.fabric8.kubernetes.client.Watcher.Action\n+import io.fabric8.kubernetes.client.dsl.{FilterWatchListDeletable, MixedOperation, NonNamespaceOperation, PodResource}\n+import org.mockito.{AdditionalAnswers, ArgumentCaptor, Mock, MockitoAnnotations}\n+import org.mockito.Matchers.{any, eq => mockitoEq}\n+import org.mockito.Mockito.{doNothing, never, times, verify, when}\n+import org.scalatest.BeforeAndAfter\n+import org.scalatest.mock.MockitoSugar._\n+import scala.collection.JavaConverters._\n+import scala.concurrent.Future\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.deploy.k8s.config._\n+import org.apache.spark.deploy.k8s.constants._\n+import org.apache.spark.rpc._\n+import org.apache.spark.scheduler.{ExecutorExited, LiveListenerBus, SlaveLost, TaskSchedulerImpl}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.{RegisterExecutor, RemoveExecutor}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+import org.apache.spark.util.ThreadUtils\n+\n+class KubernetesClusterSchedulerBackendSuite\n+    extends SparkFunSuite with BeforeAndAfter {"
  }, {
    "author": {
      "login": "liyinan926"
    },
    "body": "Done.",
    "commit": "b85cfc4038c8de9340b78d10edf88ab76dd90ba3",
    "createdAt": "2017-11-21T19:12:34Z",
    "diffHunk": "@@ -0,0 +1,439 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.scheduler.cluster.k8s\n+\n+import java.util.concurrent.{ExecutorService, ScheduledExecutorService, TimeUnit}\n+\n+import io.fabric8.kubernetes.api.model.{DoneablePod, Pod, PodBuilder, PodList}\n+import io.fabric8.kubernetes.client.{KubernetesClient, Watch, Watcher}\n+import io.fabric8.kubernetes.client.Watcher.Action\n+import io.fabric8.kubernetes.client.dsl.{FilterWatchListDeletable, MixedOperation, NonNamespaceOperation, PodResource}\n+import org.mockito.{AdditionalAnswers, ArgumentCaptor, Mock, MockitoAnnotations}\n+import org.mockito.Matchers.{any, eq => mockitoEq}\n+import org.mockito.Mockito.{doNothing, never, times, verify, when}\n+import org.scalatest.BeforeAndAfter\n+import org.scalatest.mock.MockitoSugar._\n+import scala.collection.JavaConverters._\n+import scala.concurrent.Future\n+\n+import org.apache.spark.{SparkConf, SparkContext, SparkFunSuite}\n+import org.apache.spark.deploy.k8s.config._\n+import org.apache.spark.deploy.k8s.constants._\n+import org.apache.spark.rpc._\n+import org.apache.spark.scheduler.{ExecutorExited, LiveListenerBus, SlaveLost, TaskSchedulerImpl}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.{RegisterExecutor, RemoveExecutor}\n+import org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend\n+import org.apache.spark.util.ThreadUtils\n+\n+class KubernetesClusterSchedulerBackendSuite\n+    extends SparkFunSuite with BeforeAndAfter {"
  }],
  "prId": 19468
}]