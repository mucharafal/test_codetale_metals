[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Use `File.createTempFile`.",
    "commit": "f2948a0622f6f6d34834ba4525e573cc81b4ed7b",
    "createdAt": "2018-11-01T22:31:37Z",
    "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features.hadooputils\n+\n+import java.io.File\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model._\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkPod\n+import org.apache.spark.deploy.k8s.features.KubernetesFeaturesTestUtils._\n+import org.apache.spark.util.Utils\n+\n+class HadoopBootstrapUtilSuite extends SparkFunSuite with BeforeAndAfter{\n+  private val sparkPod = SparkPod.initialPod()\n+  private val hadoopBootstrapUtil = new HadoopBootstrapUtil\n+  private var tmpDir: File = _\n+  private var tmpFile: File = _\n+\n+  before {\n+    tmpDir = Utils.createTempDir()\n+    tmpFile = createTempFile(tmpDir, \"contents\")\n+  }\n+\n+  after {\n+    tmpFile.delete()\n+    tmpDir.delete()\n+  }\n+\n+  test(\"Testing bootstrapKerberosPod with file location of krb5\") {\n+    val dtSecretName = \"EXAMPLE_SECRET_NAME\"\n+    val dtSecretItemKey = \"EXAMPLE_ITEM_KEY\"\n+    val userName = \"SPARK_USER_NAME\"\n+    val fileLocation = Some(tmpFile.getAbsolutePath)\n+    val stringPath = tmpFile.toPath.getFileName.toString\n+    val newKrb5ConfName = Some(\"/etc/krb5.conf\")\n+    val resultingPod = hadoopBootstrapUtil.bootstrapKerberosPod(\n+      dtSecretName,\n+      dtSecretItemKey,\n+      userName,\n+      fileLocation,\n+      newKrb5ConfName,\n+      None,\n+      sparkPod)\n+    val expectedVolumes = Seq(\n+      new VolumeBuilder()\n+        .withName(KRB_FILE_VOLUME)\n+        .withNewConfigMap()\n+          .withName(newKrb5ConfName.get)\n+          .withItems(new KeyToPathBuilder()\n+            .withKey(stringPath)\n+            .withPath(stringPath)\n+            .build())\n+        .endConfigMap()\n+        .build(),\n+      new VolumeBuilder()\n+        .withName(SPARK_APP_HADOOP_SECRET_VOLUME_NAME)\n+        .withNewSecret()\n+          .withSecretName(dtSecretName)\n+          .endSecret()\n+        .build()\n+    )\n+    assert(podHasVolumes(resultingPod.pod, expectedVolumes))\n+    assert(containerHasEnvVars(resultingPod.container, Map(\n+      ENV_HADOOP_TOKEN_FILE_LOCATION -> s\"$SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR/$dtSecretItemKey\",\n+      ENV_SPARK_USER -> userName)\n+    ))\n+    assert(containerHasVolumeMounts(resultingPod.container, Map(\n+      SPARK_APP_HADOOP_SECRET_VOLUME_NAME -> SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR,\n+      KRB_FILE_VOLUME -> (KRB_FILE_DIR_PATH + \"/krb5.conf\"))\n+    ))\n+  }\n+\n+  test(\"Testing bootstrapKerberosPod with configMap of krb5\") {\n+    val dtSecretName = \"EXAMPLE_SECRET_NAME\"\n+    val dtSecretItemKey = \"EXAMPLE_ITEM_KEY\"\n+    val userName = \"SPARK_USER_NAME\"\n+    val existingKrb5ConfName = Some(\"krb5CMap\")\n+    val resultingPod = hadoopBootstrapUtil.bootstrapKerberosPod(\n+      dtSecretName,\n+      dtSecretItemKey,\n+      userName,\n+      None,\n+      None,\n+      existingKrb5ConfName,\n+      sparkPod)\n+    val expectedVolumes = Seq(\n+      new VolumeBuilder()\n+        .withName(KRB_FILE_VOLUME)\n+        .withNewConfigMap()\n+          .withName(existingKrb5ConfName.get)\n+          .endConfigMap()\n+        .build(),\n+      new VolumeBuilder()\n+        .withName(SPARK_APP_HADOOP_SECRET_VOLUME_NAME)\n+        .withNewSecret()\n+          .withSecretName(dtSecretName)\n+          .endSecret()\n+        .build()\n+    )\n+    assert(podHasVolumes(resultingPod.pod, expectedVolumes))\n+    assert(containerHasEnvVars(resultingPod.container, Map(\n+      ENV_HADOOP_TOKEN_FILE_LOCATION -> s\"$SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR/$dtSecretItemKey\",\n+      ENV_SPARK_USER -> userName)\n+    ))\n+    assert(containerHasVolumeMounts(resultingPod.container, Map(\n+      SPARK_APP_HADOOP_SECRET_VOLUME_NAME -> SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR)\n+    ))\n+  }\n+\n+  test(\"Testing bootstrapSparkUserPod\") {\n+    val userName = \"SPARK_USER_NAME\"\n+    val resultingPod = hadoopBootstrapUtil.bootstrapSparkUserPod(userName, sparkPod)\n+    assert(containerHasEnvVars(resultingPod.container, Map(ENV_SPARK_USER -> userName)))\n+  }\n+\n+  test(\"Testing bootstrapHadoopConfDir with fileLocation HADOOP_CONF\") {\n+    val hadoopConfDir = Some(tmpDir.getAbsolutePath)\n+    val stringPath = tmpFile.toPath.getFileName.toString\n+    val newHadoopConfigMapName = Some(\"hconfMapName\")\n+    val resultingPod = hadoopBootstrapUtil.bootstrapHadoopConfDir(\n+      hadoopConfDir,\n+      newHadoopConfigMapName,\n+      None,\n+      sparkPod\n+    )\n+    assert(containerHasVolumeMounts(resultingPod.container, Map(\n+      HADOOP_FILE_VOLUME -> HADOOP_CONF_DIR_PATH)\n+    ))\n+    val expectedVolumes = Seq(\n+      new VolumeBuilder()\n+        .withName(HADOOP_FILE_VOLUME)\n+        .withNewConfigMap()\n+          .withName(newHadoopConfigMapName.get)\n+          .withItems(new KeyToPathBuilder()\n+            .withKey(stringPath)\n+            .withPath(stringPath)\n+            .build())\n+          .endConfigMap()\n+        .build()\n+    )\n+\n+    assert(podHasVolumes(resultingPod.pod, expectedVolumes))\n+    assert(containerHasVolumeMounts(resultingPod.container, Map(\n+      HADOOP_FILE_VOLUME -> HADOOP_CONF_DIR_PATH)\n+    ))\n+    assert(containerHasEnvVars(resultingPod.container, Map(\n+      ENV_HADOOP_CONF_DIR -> HADOOP_CONF_DIR_PATH)\n+    ))\n+  }\n+\n+  test(\"Testing bootstrapHadoopConfDir with configMap HADOOP_CONF\") {\n+    val existingHadoopConfigMapName = Some(\"hconfMapName\")\n+    val resultingPod = hadoopBootstrapUtil.bootstrapHadoopConfDir(\n+      None,\n+      None,\n+      existingHadoopConfigMapName,\n+      sparkPod\n+    )\n+    assert(containerHasVolumeMounts(resultingPod.container, Map(\n+      HADOOP_FILE_VOLUME -> HADOOP_CONF_DIR_PATH)\n+    ))\n+    val expectedVolumes = Seq(\n+      new VolumeBuilder()\n+        .withName(HADOOP_FILE_VOLUME)\n+          .withNewConfigMap()\n+            .withName(existingHadoopConfigMapName.get)\n+            .endConfigMap()\n+        .build())\n+    assert(podHasVolumes(resultingPod.pod, expectedVolumes))\n+  }\n+\n+  test(\"Testing buildKrb5ConfigMap\") {\n+    val configMapName = \"hconfMapName\"\n+    val resultingCMap = hadoopBootstrapUtil.buildkrb5ConfigMap(\n+      configMapName,\n+      tmpFile.getAbsolutePath\n+    )\n+    assert(resultingCMap === new ConfigMapBuilder()\n+      .withNewMetadata()\n+        .withName(configMapName)\n+        .endMetadata()\n+      .addToData(Map(tmpFile.toPath.getFileName.toString -> \"contents\").asJava)\n+    .build())\n+  }\n+\n+  test(\"Testing buildHadoopConfigMap\") {\n+    val configMapName = \"hconfMapName\"\n+    val resultingCMap = hadoopBootstrapUtil.buildHadoopConfigMap(\n+      configMapName,\n+      Seq(tmpFile)\n+    )\n+    assert(resultingCMap === new ConfigMapBuilder()\n+      .withNewMetadata()\n+      .withName(configMapName)\n+      .endMetadata()\n+      .addToData(Map(tmpFile.toPath.getFileName.toString -> \"contents\").asJava)\n+      .build())\n+  }\n+\n+\n+  def createTempFile(dir: File, contents: String): File = {\n+    val file = new File(dir, s\"${UUID.randomUUID().toString}\")"
  }],
  "prId": 22760
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "If this assert (and others like it) fails, you'll get a pretty generic message (\"false was not true\" or some such).\r\n\r\nMight be better to make `podHasVolumes` (and friends) assert with a more helpful message that shows at least the computed set.",
    "commit": "f2948a0622f6f6d34834ba4525e573cc81b4ed7b",
    "createdAt": "2018-11-01T22:37:49Z",
    "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features.hadooputils\n+\n+import java.io.File\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model._\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkPod\n+import org.apache.spark.deploy.k8s.features.KubernetesFeaturesTestUtils._\n+import org.apache.spark.util.Utils\n+\n+class HadoopBootstrapUtilSuite extends SparkFunSuite with BeforeAndAfter{\n+  private val sparkPod = SparkPod.initialPod()\n+  private val hadoopBootstrapUtil = new HadoopBootstrapUtil\n+  private var tmpDir: File = _\n+  private var tmpFile: File = _\n+\n+  before {\n+    tmpDir = Utils.createTempDir()\n+    tmpFile = createTempFile(tmpDir, \"contents\")\n+  }\n+\n+  after {\n+    tmpFile.delete()\n+    tmpDir.delete()\n+  }\n+\n+  test(\"Testing bootstrapKerberosPod with file location of krb5\") {\n+    val dtSecretName = \"EXAMPLE_SECRET_NAME\"\n+    val dtSecretItemKey = \"EXAMPLE_ITEM_KEY\"\n+    val userName = \"SPARK_USER_NAME\"\n+    val fileLocation = Some(tmpFile.getAbsolutePath)\n+    val stringPath = tmpFile.toPath.getFileName.toString\n+    val newKrb5ConfName = Some(\"/etc/krb5.conf\")\n+    val resultingPod = hadoopBootstrapUtil.bootstrapKerberosPod(\n+      dtSecretName,\n+      dtSecretItemKey,\n+      userName,\n+      fileLocation,\n+      newKrb5ConfName,\n+      None,\n+      sparkPod)\n+    val expectedVolumes = Seq(\n+      new VolumeBuilder()\n+        .withName(KRB_FILE_VOLUME)\n+        .withNewConfigMap()\n+          .withName(newKrb5ConfName.get)\n+          .withItems(new KeyToPathBuilder()\n+            .withKey(stringPath)\n+            .withPath(stringPath)\n+            .build())\n+        .endConfigMap()\n+        .build(),\n+      new VolumeBuilder()\n+        .withName(SPARK_APP_HADOOP_SECRET_VOLUME_NAME)\n+        .withNewSecret()\n+          .withSecretName(dtSecretName)\n+          .endSecret()\n+        .build()\n+    )\n+    assert(podHasVolumes(resultingPod.pod, expectedVolumes))"
  }],
  "prId": 22760
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Could you create better names for these tests? All of them start with \"Testing\", which is  redundant and also makes it harder to differentiate them.",
    "commit": "f2948a0622f6f6d34834ba4525e573cc81b4ed7b",
    "createdAt": "2018-11-01T22:39:30Z",
    "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features.hadooputils\n+\n+import java.io.File\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model._\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkPod\n+import org.apache.spark.deploy.k8s.features.KubernetesFeaturesTestUtils._\n+import org.apache.spark.util.Utils\n+\n+class HadoopBootstrapUtilSuite extends SparkFunSuite with BeforeAndAfter{\n+  private val sparkPod = SparkPod.initialPod()\n+  private val hadoopBootstrapUtil = new HadoopBootstrapUtil\n+  private var tmpDir: File = _\n+  private var tmpFile: File = _\n+\n+  before {\n+    tmpDir = Utils.createTempDir()\n+    tmpFile = createTempFile(tmpDir, \"contents\")\n+  }\n+\n+  after {\n+    tmpFile.delete()\n+    tmpDir.delete()\n+  }\n+\n+  test(\"Testing bootstrapKerberosPod with file location of krb5\") {"
  }],
  "prId": 22760
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This method is called in a single place. Just inline it.",
    "commit": "f2948a0622f6f6d34834ba4525e573cc81b4ed7b",
    "createdAt": "2018-11-01T22:42:25Z",
    "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features.hadooputils\n+\n+import java.io.File\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model._\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkPod\n+import org.apache.spark.deploy.k8s.features.KubernetesFeaturesTestUtils._\n+import org.apache.spark.util.Utils\n+\n+class HadoopBootstrapUtilSuite extends SparkFunSuite with BeforeAndAfter{\n+  private val sparkPod = SparkPod.initialPod()\n+  private val hadoopBootstrapUtil = new HadoopBootstrapUtil\n+  private var tmpDir: File = _\n+  private var tmpFile: File = _\n+\n+  before {\n+    tmpDir = Utils.createTempDir()\n+    tmpFile = createTempFile(tmpDir, \"contents\")\n+  }\n+\n+  after {\n+    tmpFile.delete()\n+    tmpDir.delete()\n+  }\n+\n+  test(\"Testing bootstrapKerberosPod with file location of krb5\") {\n+    val dtSecretName = \"EXAMPLE_SECRET_NAME\"\n+    val dtSecretItemKey = \"EXAMPLE_ITEM_KEY\"\n+    val userName = \"SPARK_USER_NAME\"\n+    val fileLocation = Some(tmpFile.getAbsolutePath)\n+    val stringPath = tmpFile.toPath.getFileName.toString\n+    val newKrb5ConfName = Some(\"/etc/krb5.conf\")\n+    val resultingPod = hadoopBootstrapUtil.bootstrapKerberosPod(\n+      dtSecretName,\n+      dtSecretItemKey,\n+      userName,\n+      fileLocation,\n+      newKrb5ConfName,\n+      None,\n+      sparkPod)\n+    val expectedVolumes = Seq(\n+      new VolumeBuilder()\n+        .withName(KRB_FILE_VOLUME)\n+        .withNewConfigMap()\n+          .withName(newKrb5ConfName.get)\n+          .withItems(new KeyToPathBuilder()\n+            .withKey(stringPath)\n+            .withPath(stringPath)\n+            .build())\n+        .endConfigMap()\n+        .build(),\n+      new VolumeBuilder()\n+        .withName(SPARK_APP_HADOOP_SECRET_VOLUME_NAME)\n+        .withNewSecret()\n+          .withSecretName(dtSecretName)\n+          .endSecret()\n+        .build()\n+    )\n+    assert(podHasVolumes(resultingPod.pod, expectedVolumes))\n+    assert(containerHasEnvVars(resultingPod.container, Map(\n+      ENV_HADOOP_TOKEN_FILE_LOCATION -> s\"$SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR/$dtSecretItemKey\",\n+      ENV_SPARK_USER -> userName)\n+    ))\n+    assert(containerHasVolumeMounts(resultingPod.container, Map(\n+      SPARK_APP_HADOOP_SECRET_VOLUME_NAME -> SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR,\n+      KRB_FILE_VOLUME -> (KRB_FILE_DIR_PATH + \"/krb5.conf\"))\n+    ))\n+  }\n+\n+  test(\"Testing bootstrapKerberosPod with configMap of krb5\") {\n+    val dtSecretName = \"EXAMPLE_SECRET_NAME\"\n+    val dtSecretItemKey = \"EXAMPLE_ITEM_KEY\"\n+    val userName = \"SPARK_USER_NAME\"\n+    val existingKrb5ConfName = Some(\"krb5CMap\")\n+    val resultingPod = hadoopBootstrapUtil.bootstrapKerberosPod(\n+      dtSecretName,\n+      dtSecretItemKey,\n+      userName,\n+      None,\n+      None,\n+      existingKrb5ConfName,\n+      sparkPod)\n+    val expectedVolumes = Seq(\n+      new VolumeBuilder()\n+        .withName(KRB_FILE_VOLUME)\n+        .withNewConfigMap()\n+          .withName(existingKrb5ConfName.get)\n+          .endConfigMap()\n+        .build(),\n+      new VolumeBuilder()\n+        .withName(SPARK_APP_HADOOP_SECRET_VOLUME_NAME)\n+        .withNewSecret()\n+          .withSecretName(dtSecretName)\n+          .endSecret()\n+        .build()\n+    )\n+    assert(podHasVolumes(resultingPod.pod, expectedVolumes))\n+    assert(containerHasEnvVars(resultingPod.container, Map(\n+      ENV_HADOOP_TOKEN_FILE_LOCATION -> s\"$SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR/$dtSecretItemKey\",\n+      ENV_SPARK_USER -> userName)\n+    ))\n+    assert(containerHasVolumeMounts(resultingPod.container, Map(\n+      SPARK_APP_HADOOP_SECRET_VOLUME_NAME -> SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR)\n+    ))\n+  }\n+\n+  test(\"Testing bootstrapSparkUserPod\") {\n+    val userName = \"SPARK_USER_NAME\"\n+    val resultingPod = hadoopBootstrapUtil.bootstrapSparkUserPod(userName, sparkPod)\n+    assert(containerHasEnvVars(resultingPod.container, Map(ENV_SPARK_USER -> userName)))\n+  }\n+\n+  test(\"Testing bootstrapHadoopConfDir with fileLocation HADOOP_CONF\") {\n+    val hadoopConfDir = Some(tmpDir.getAbsolutePath)\n+    val stringPath = tmpFile.toPath.getFileName.toString\n+    val newHadoopConfigMapName = Some(\"hconfMapName\")\n+    val resultingPod = hadoopBootstrapUtil.bootstrapHadoopConfDir(\n+      hadoopConfDir,\n+      newHadoopConfigMapName,\n+      None,\n+      sparkPod\n+    )\n+    assert(containerHasVolumeMounts(resultingPod.container, Map(\n+      HADOOP_FILE_VOLUME -> HADOOP_CONF_DIR_PATH)\n+    ))\n+    val expectedVolumes = Seq(\n+      new VolumeBuilder()\n+        .withName(HADOOP_FILE_VOLUME)\n+        .withNewConfigMap()\n+          .withName(newHadoopConfigMapName.get)\n+          .withItems(new KeyToPathBuilder()\n+            .withKey(stringPath)\n+            .withPath(stringPath)\n+            .build())\n+          .endConfigMap()\n+        .build()\n+    )\n+\n+    assert(podHasVolumes(resultingPod.pod, expectedVolumes))\n+    assert(containerHasVolumeMounts(resultingPod.container, Map(\n+      HADOOP_FILE_VOLUME -> HADOOP_CONF_DIR_PATH)\n+    ))\n+    assert(containerHasEnvVars(resultingPod.container, Map(\n+      ENV_HADOOP_CONF_DIR -> HADOOP_CONF_DIR_PATH)\n+    ))\n+  }\n+\n+  test(\"Testing bootstrapHadoopConfDir with configMap HADOOP_CONF\") {\n+    val existingHadoopConfigMapName = Some(\"hconfMapName\")\n+    val resultingPod = hadoopBootstrapUtil.bootstrapHadoopConfDir(\n+      None,\n+      None,\n+      existingHadoopConfigMapName,\n+      sparkPod\n+    )\n+    assert(containerHasVolumeMounts(resultingPod.container, Map(\n+      HADOOP_FILE_VOLUME -> HADOOP_CONF_DIR_PATH)\n+    ))\n+    val expectedVolumes = Seq(\n+      new VolumeBuilder()\n+        .withName(HADOOP_FILE_VOLUME)\n+          .withNewConfigMap()\n+            .withName(existingHadoopConfigMapName.get)\n+            .endConfigMap()\n+        .build())\n+    assert(podHasVolumes(resultingPod.pod, expectedVolumes))\n+  }\n+\n+  test(\"Testing buildKrb5ConfigMap\") {\n+    val configMapName = \"hconfMapName\"\n+    val resultingCMap = hadoopBootstrapUtil.buildkrb5ConfigMap(\n+      configMapName,\n+      tmpFile.getAbsolutePath\n+    )\n+    assert(resultingCMap === new ConfigMapBuilder()\n+      .withNewMetadata()\n+        .withName(configMapName)\n+        .endMetadata()\n+      .addToData(Map(tmpFile.toPath.getFileName.toString -> \"contents\").asJava)\n+    .build())\n+  }\n+\n+  test(\"Testing buildHadoopConfigMap\") {\n+    val configMapName = \"hconfMapName\"\n+    val resultingCMap = hadoopBootstrapUtil.buildHadoopConfigMap(\n+      configMapName,\n+      Seq(tmpFile)\n+    )\n+    assert(resultingCMap === new ConfigMapBuilder()\n+      .withNewMetadata()\n+      .withName(configMapName)\n+      .endMetadata()\n+      .addToData(Map(tmpFile.toPath.getFileName.toString -> \"contents\").asJava)\n+      .build())\n+  }\n+\n+\n+  def createTempFile(dir: File, contents: String): File = {"
  }],
  "prId": 22760
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "all that long call is just `tmpFile.getName()`.",
    "commit": "f2948a0622f6f6d34834ba4525e573cc81b4ed7b",
    "createdAt": "2018-11-01T22:43:15Z",
    "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features.hadooputils\n+\n+import java.io.File\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model._\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkPod\n+import org.apache.spark.deploy.k8s.features.KubernetesFeaturesTestUtils._\n+import org.apache.spark.util.Utils\n+\n+class HadoopBootstrapUtilSuite extends SparkFunSuite with BeforeAndAfter{\n+  private val sparkPod = SparkPod.initialPod()\n+  private val hadoopBootstrapUtil = new HadoopBootstrapUtil\n+  private var tmpDir: File = _\n+  private var tmpFile: File = _\n+\n+  before {\n+    tmpDir = Utils.createTempDir()\n+    tmpFile = createTempFile(tmpDir, \"contents\")\n+  }\n+\n+  after {\n+    tmpFile.delete()\n+    tmpDir.delete()\n+  }\n+\n+  test(\"Testing bootstrapKerberosPod with file location of krb5\") {\n+    val dtSecretName = \"EXAMPLE_SECRET_NAME\"\n+    val dtSecretItemKey = \"EXAMPLE_ITEM_KEY\"\n+    val userName = \"SPARK_USER_NAME\"\n+    val fileLocation = Some(tmpFile.getAbsolutePath)\n+    val stringPath = tmpFile.toPath.getFileName.toString\n+    val newKrb5ConfName = Some(\"/etc/krb5.conf\")\n+    val resultingPod = hadoopBootstrapUtil.bootstrapKerberosPod(\n+      dtSecretName,\n+      dtSecretItemKey,\n+      userName,\n+      fileLocation,\n+      newKrb5ConfName,\n+      None,\n+      sparkPod)\n+    val expectedVolumes = Seq(\n+      new VolumeBuilder()\n+        .withName(KRB_FILE_VOLUME)\n+        .withNewConfigMap()\n+          .withName(newKrb5ConfName.get)\n+          .withItems(new KeyToPathBuilder()\n+            .withKey(stringPath)\n+            .withPath(stringPath)\n+            .build())\n+        .endConfigMap()\n+        .build(),\n+      new VolumeBuilder()\n+        .withName(SPARK_APP_HADOOP_SECRET_VOLUME_NAME)\n+        .withNewSecret()\n+          .withSecretName(dtSecretName)\n+          .endSecret()\n+        .build()\n+    )\n+    assert(podHasVolumes(resultingPod.pod, expectedVolumes))\n+    assert(containerHasEnvVars(resultingPod.container, Map(\n+      ENV_HADOOP_TOKEN_FILE_LOCATION -> s\"$SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR/$dtSecretItemKey\",\n+      ENV_SPARK_USER -> userName)\n+    ))\n+    assert(containerHasVolumeMounts(resultingPod.container, Map(\n+      SPARK_APP_HADOOP_SECRET_VOLUME_NAME -> SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR,\n+      KRB_FILE_VOLUME -> (KRB_FILE_DIR_PATH + \"/krb5.conf\"))\n+    ))\n+  }\n+\n+  test(\"Testing bootstrapKerberosPod with configMap of krb5\") {\n+    val dtSecretName = \"EXAMPLE_SECRET_NAME\"\n+    val dtSecretItemKey = \"EXAMPLE_ITEM_KEY\"\n+    val userName = \"SPARK_USER_NAME\"\n+    val existingKrb5ConfName = Some(\"krb5CMap\")\n+    val resultingPod = hadoopBootstrapUtil.bootstrapKerberosPod(\n+      dtSecretName,\n+      dtSecretItemKey,\n+      userName,\n+      None,\n+      None,\n+      existingKrb5ConfName,\n+      sparkPod)\n+    val expectedVolumes = Seq(\n+      new VolumeBuilder()\n+        .withName(KRB_FILE_VOLUME)\n+        .withNewConfigMap()\n+          .withName(existingKrb5ConfName.get)\n+          .endConfigMap()\n+        .build(),\n+      new VolumeBuilder()\n+        .withName(SPARK_APP_HADOOP_SECRET_VOLUME_NAME)\n+        .withNewSecret()\n+          .withSecretName(dtSecretName)\n+          .endSecret()\n+        .build()\n+    )\n+    assert(podHasVolumes(resultingPod.pod, expectedVolumes))\n+    assert(containerHasEnvVars(resultingPod.container, Map(\n+      ENV_HADOOP_TOKEN_FILE_LOCATION -> s\"$SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR/$dtSecretItemKey\",\n+      ENV_SPARK_USER -> userName)\n+    ))\n+    assert(containerHasVolumeMounts(resultingPod.container, Map(\n+      SPARK_APP_HADOOP_SECRET_VOLUME_NAME -> SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR)\n+    ))\n+  }\n+\n+  test(\"Testing bootstrapSparkUserPod\") {\n+    val userName = \"SPARK_USER_NAME\"\n+    val resultingPod = hadoopBootstrapUtil.bootstrapSparkUserPod(userName, sparkPod)\n+    assert(containerHasEnvVars(resultingPod.container, Map(ENV_SPARK_USER -> userName)))\n+  }\n+\n+  test(\"Testing bootstrapHadoopConfDir with fileLocation HADOOP_CONF\") {\n+    val hadoopConfDir = Some(tmpDir.getAbsolutePath)\n+    val stringPath = tmpFile.toPath.getFileName.toString\n+    val newHadoopConfigMapName = Some(\"hconfMapName\")\n+    val resultingPod = hadoopBootstrapUtil.bootstrapHadoopConfDir(\n+      hadoopConfDir,\n+      newHadoopConfigMapName,\n+      None,\n+      sparkPod\n+    )\n+    assert(containerHasVolumeMounts(resultingPod.container, Map(\n+      HADOOP_FILE_VOLUME -> HADOOP_CONF_DIR_PATH)\n+    ))\n+    val expectedVolumes = Seq(\n+      new VolumeBuilder()\n+        .withName(HADOOP_FILE_VOLUME)\n+        .withNewConfigMap()\n+          .withName(newHadoopConfigMapName.get)\n+          .withItems(new KeyToPathBuilder()\n+            .withKey(stringPath)\n+            .withPath(stringPath)\n+            .build())\n+          .endConfigMap()\n+        .build()\n+    )\n+\n+    assert(podHasVolumes(resultingPod.pod, expectedVolumes))\n+    assert(containerHasVolumeMounts(resultingPod.container, Map(\n+      HADOOP_FILE_VOLUME -> HADOOP_CONF_DIR_PATH)\n+    ))\n+    assert(containerHasEnvVars(resultingPod.container, Map(\n+      ENV_HADOOP_CONF_DIR -> HADOOP_CONF_DIR_PATH)\n+    ))\n+  }\n+\n+  test(\"Testing bootstrapHadoopConfDir with configMap HADOOP_CONF\") {\n+    val existingHadoopConfigMapName = Some(\"hconfMapName\")\n+    val resultingPod = hadoopBootstrapUtil.bootstrapHadoopConfDir(\n+      None,\n+      None,\n+      existingHadoopConfigMapName,\n+      sparkPod\n+    )\n+    assert(containerHasVolumeMounts(resultingPod.container, Map(\n+      HADOOP_FILE_VOLUME -> HADOOP_CONF_DIR_PATH)\n+    ))\n+    val expectedVolumes = Seq(\n+      new VolumeBuilder()\n+        .withName(HADOOP_FILE_VOLUME)\n+          .withNewConfigMap()\n+            .withName(existingHadoopConfigMapName.get)\n+            .endConfigMap()\n+        .build())\n+    assert(podHasVolumes(resultingPod.pod, expectedVolumes))\n+  }\n+\n+  test(\"Testing buildKrb5ConfigMap\") {\n+    val configMapName = \"hconfMapName\"\n+    val resultingCMap = hadoopBootstrapUtil.buildkrb5ConfigMap(\n+      configMapName,\n+      tmpFile.getAbsolutePath\n+    )\n+    assert(resultingCMap === new ConfigMapBuilder()\n+      .withNewMetadata()\n+        .withName(configMapName)\n+        .endMetadata()\n+      .addToData(Map(tmpFile.toPath.getFileName.toString -> \"contents\").asJava)"
  }],
  "prId": 22760
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Same.",
    "commit": "f2948a0622f6f6d34834ba4525e573cc81b4ed7b",
    "createdAt": "2018-11-01T22:43:34Z",
    "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features.hadooputils\n+\n+import java.io.File\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model._\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkPod\n+import org.apache.spark.deploy.k8s.features.KubernetesFeaturesTestUtils._\n+import org.apache.spark.util.Utils\n+\n+class HadoopBootstrapUtilSuite extends SparkFunSuite with BeforeAndAfter{\n+  private val sparkPod = SparkPod.initialPod()\n+  private val hadoopBootstrapUtil = new HadoopBootstrapUtil\n+  private var tmpDir: File = _\n+  private var tmpFile: File = _\n+\n+  before {\n+    tmpDir = Utils.createTempDir()\n+    tmpFile = createTempFile(tmpDir, \"contents\")\n+  }\n+\n+  after {\n+    tmpFile.delete()\n+    tmpDir.delete()\n+  }\n+\n+  test(\"Testing bootstrapKerberosPod with file location of krb5\") {\n+    val dtSecretName = \"EXAMPLE_SECRET_NAME\"\n+    val dtSecretItemKey = \"EXAMPLE_ITEM_KEY\"\n+    val userName = \"SPARK_USER_NAME\"\n+    val fileLocation = Some(tmpFile.getAbsolutePath)\n+    val stringPath = tmpFile.toPath.getFileName.toString\n+    val newKrb5ConfName = Some(\"/etc/krb5.conf\")\n+    val resultingPod = hadoopBootstrapUtil.bootstrapKerberosPod(\n+      dtSecretName,\n+      dtSecretItemKey,\n+      userName,\n+      fileLocation,\n+      newKrb5ConfName,\n+      None,\n+      sparkPod)\n+    val expectedVolumes = Seq(\n+      new VolumeBuilder()\n+        .withName(KRB_FILE_VOLUME)\n+        .withNewConfigMap()\n+          .withName(newKrb5ConfName.get)\n+          .withItems(new KeyToPathBuilder()\n+            .withKey(stringPath)\n+            .withPath(stringPath)\n+            .build())\n+        .endConfigMap()\n+        .build(),\n+      new VolumeBuilder()\n+        .withName(SPARK_APP_HADOOP_SECRET_VOLUME_NAME)\n+        .withNewSecret()\n+          .withSecretName(dtSecretName)\n+          .endSecret()\n+        .build()\n+    )\n+    assert(podHasVolumes(resultingPod.pod, expectedVolumes))\n+    assert(containerHasEnvVars(resultingPod.container, Map(\n+      ENV_HADOOP_TOKEN_FILE_LOCATION -> s\"$SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR/$dtSecretItemKey\",\n+      ENV_SPARK_USER -> userName)\n+    ))\n+    assert(containerHasVolumeMounts(resultingPod.container, Map(\n+      SPARK_APP_HADOOP_SECRET_VOLUME_NAME -> SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR,\n+      KRB_FILE_VOLUME -> (KRB_FILE_DIR_PATH + \"/krb5.conf\"))\n+    ))\n+  }\n+\n+  test(\"Testing bootstrapKerberosPod with configMap of krb5\") {\n+    val dtSecretName = \"EXAMPLE_SECRET_NAME\"\n+    val dtSecretItemKey = \"EXAMPLE_ITEM_KEY\"\n+    val userName = \"SPARK_USER_NAME\"\n+    val existingKrb5ConfName = Some(\"krb5CMap\")\n+    val resultingPod = hadoopBootstrapUtil.bootstrapKerberosPod(\n+      dtSecretName,\n+      dtSecretItemKey,\n+      userName,\n+      None,\n+      None,\n+      existingKrb5ConfName,\n+      sparkPod)\n+    val expectedVolumes = Seq(\n+      new VolumeBuilder()\n+        .withName(KRB_FILE_VOLUME)\n+        .withNewConfigMap()\n+          .withName(existingKrb5ConfName.get)\n+          .endConfigMap()\n+        .build(),\n+      new VolumeBuilder()\n+        .withName(SPARK_APP_HADOOP_SECRET_VOLUME_NAME)\n+        .withNewSecret()\n+          .withSecretName(dtSecretName)\n+          .endSecret()\n+        .build()\n+    )\n+    assert(podHasVolumes(resultingPod.pod, expectedVolumes))\n+    assert(containerHasEnvVars(resultingPod.container, Map(\n+      ENV_HADOOP_TOKEN_FILE_LOCATION -> s\"$SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR/$dtSecretItemKey\",\n+      ENV_SPARK_USER -> userName)\n+    ))\n+    assert(containerHasVolumeMounts(resultingPod.container, Map(\n+      SPARK_APP_HADOOP_SECRET_VOLUME_NAME -> SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR)\n+    ))\n+  }\n+\n+  test(\"Testing bootstrapSparkUserPod\") {\n+    val userName = \"SPARK_USER_NAME\"\n+    val resultingPod = hadoopBootstrapUtil.bootstrapSparkUserPod(userName, sparkPod)\n+    assert(containerHasEnvVars(resultingPod.container, Map(ENV_SPARK_USER -> userName)))\n+  }\n+\n+  test(\"Testing bootstrapHadoopConfDir with fileLocation HADOOP_CONF\") {\n+    val hadoopConfDir = Some(tmpDir.getAbsolutePath)\n+    val stringPath = tmpFile.toPath.getFileName.toString\n+    val newHadoopConfigMapName = Some(\"hconfMapName\")\n+    val resultingPod = hadoopBootstrapUtil.bootstrapHadoopConfDir(\n+      hadoopConfDir,\n+      newHadoopConfigMapName,\n+      None,\n+      sparkPod\n+    )\n+    assert(containerHasVolumeMounts(resultingPod.container, Map(\n+      HADOOP_FILE_VOLUME -> HADOOP_CONF_DIR_PATH)\n+    ))\n+    val expectedVolumes = Seq(\n+      new VolumeBuilder()\n+        .withName(HADOOP_FILE_VOLUME)\n+        .withNewConfigMap()\n+          .withName(newHadoopConfigMapName.get)\n+          .withItems(new KeyToPathBuilder()\n+            .withKey(stringPath)\n+            .withPath(stringPath)\n+            .build())\n+          .endConfigMap()\n+        .build()\n+    )\n+\n+    assert(podHasVolumes(resultingPod.pod, expectedVolumes))\n+    assert(containerHasVolumeMounts(resultingPod.container, Map(\n+      HADOOP_FILE_VOLUME -> HADOOP_CONF_DIR_PATH)\n+    ))\n+    assert(containerHasEnvVars(resultingPod.container, Map(\n+      ENV_HADOOP_CONF_DIR -> HADOOP_CONF_DIR_PATH)\n+    ))\n+  }\n+\n+  test(\"Testing bootstrapHadoopConfDir with configMap HADOOP_CONF\") {\n+    val existingHadoopConfigMapName = Some(\"hconfMapName\")\n+    val resultingPod = hadoopBootstrapUtil.bootstrapHadoopConfDir(\n+      None,\n+      None,\n+      existingHadoopConfigMapName,\n+      sparkPod\n+    )\n+    assert(containerHasVolumeMounts(resultingPod.container, Map(\n+      HADOOP_FILE_VOLUME -> HADOOP_CONF_DIR_PATH)\n+    ))\n+    val expectedVolumes = Seq(\n+      new VolumeBuilder()\n+        .withName(HADOOP_FILE_VOLUME)\n+          .withNewConfigMap()\n+            .withName(existingHadoopConfigMapName.get)\n+            .endConfigMap()\n+        .build())\n+    assert(podHasVolumes(resultingPod.pod, expectedVolumes))\n+  }\n+\n+  test(\"Testing buildKrb5ConfigMap\") {\n+    val configMapName = \"hconfMapName\"\n+    val resultingCMap = hadoopBootstrapUtil.buildkrb5ConfigMap(\n+      configMapName,\n+      tmpFile.getAbsolutePath\n+    )\n+    assert(resultingCMap === new ConfigMapBuilder()\n+      .withNewMetadata()\n+        .withName(configMapName)\n+        .endMetadata()\n+      .addToData(Map(tmpFile.toPath.getFileName.toString -> \"contents\").asJava)\n+    .build())\n+  }\n+\n+  test(\"Testing buildHadoopConfigMap\") {\n+    val configMapName = \"hconfMapName\"\n+    val resultingCMap = hadoopBootstrapUtil.buildHadoopConfigMap(\n+      configMapName,\n+      Seq(tmpFile)\n+    )\n+    assert(resultingCMap === new ConfigMapBuilder()\n+      .withNewMetadata()\n+      .withName(configMapName)\n+      .endMetadata()\n+      .addToData(Map(tmpFile.toPath.getFileName.toString -> \"contents\").asJava)"
  }],
  "prId": 22760
}, {
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "These can be constants at the top of the file or in a companion object.",
    "commit": "f2948a0622f6f6d34834ba4525e573cc81b4ed7b",
    "createdAt": "2018-11-05T22:40:11Z",
    "diffHunk": "@@ -0,0 +1,221 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features.hadooputils\n+\n+import java.io.File\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model._\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkPod\n+import org.apache.spark.deploy.k8s.features.KubernetesFeaturesTestUtils._\n+import org.apache.spark.util.Utils\n+\n+class HadoopBootstrapUtilSuite extends SparkFunSuite with BeforeAndAfter{\n+  private val sparkPod = SparkPod.initialPod()\n+  private val hadoopBootstrapUtil = new HadoopBootstrapUtil\n+  private var tmpDir: File = _\n+  private var tmpFile: File = _\n+\n+  before {\n+    tmpDir = Utils.createTempDir()\n+    tmpFile = File.createTempFile(s\"${UUID.randomUUID().toString}\", \".txt\", tmpDir)\n+    Files.write(\"contents\".getBytes, tmpFile)\n+  }\n+\n+  after {\n+    tmpFile.delete()\n+    tmpDir.delete()\n+  }\n+\n+  test(\"bootstrapKerberosPod with file location specified for krb5.conf file\") {\n+    val dtSecretName = \"EXAMPLE_SECRET_NAME\"\n+    val dtSecretItemKey = \"EXAMPLE_ITEM_KEY\"\n+    val userName = \"SPARK_USER_NAME\"\n+    val fileLocation = Some(tmpFile.getAbsolutePath)\n+    val stringPath = tmpFile.getName\n+    val newKrb5ConfName = Some(\"/etc/krb5.conf\")\n+    val resultingPod = hadoopBootstrapUtil.bootstrapKerberosPod(\n+      dtSecretName,\n+      dtSecretItemKey,\n+      userName,\n+      fileLocation,\n+      newKrb5ConfName,\n+      None,\n+      sparkPod)\n+    val expectedVolumes = Seq(\n+      new VolumeBuilder()\n+        .withName(KRB_FILE_VOLUME)\n+        .withNewConfigMap()\n+          .withName(newKrb5ConfName.get)\n+          .withItems(new KeyToPathBuilder()\n+            .withKey(stringPath)\n+            .withPath(stringPath)\n+            .build())\n+        .endConfigMap()\n+        .build(),\n+      new VolumeBuilder()\n+        .withName(SPARK_APP_HADOOP_SECRET_VOLUME_NAME)\n+        .withNewSecret()\n+          .withSecretName(dtSecretName)\n+          .endSecret()\n+        .build()\n+    )\n+    podHasVolumes(resultingPod.pod, expectedVolumes)\n+    containerHasEnvVars(resultingPod.container, Map(\n+      ENV_HADOOP_TOKEN_FILE_LOCATION -> s\"$SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR/$dtSecretItemKey\",\n+      ENV_SPARK_USER -> userName)\n+    )\n+    containerHasVolumeMounts(resultingPod.container, Map(\n+      SPARK_APP_HADOOP_SECRET_VOLUME_NAME -> SPARK_APP_HADOOP_CREDENTIALS_BASE_DIR,\n+      KRB_FILE_VOLUME -> (KRB_FILE_DIR_PATH + \"/krb5.conf\"))\n+    )\n+  }\n+\n+  test(\"bootstrapKerberosPod with pre-existing configMap specified for krb5.conf file\") {\n+    val dtSecretName = \"EXAMPLE_SECRET_NAME\""
  }],
  "prId": 22760
}, {
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "These can be constants in a companion object or at the top of the class.",
    "commit": "f2948a0622f6f6d34834ba4525e573cc81b4ed7b",
    "createdAt": "2018-11-05T22:42:26Z",
    "diffHunk": "@@ -0,0 +1,221 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.features.hadooputils\n+\n+import java.io.File\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters._\n+\n+import com.google.common.io.Files\n+import io.fabric8.kubernetes.api.model._\n+import org.scalatest.BeforeAndAfter\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.deploy.k8s.Constants._\n+import org.apache.spark.deploy.k8s.SparkPod\n+import org.apache.spark.deploy.k8s.features.KubernetesFeaturesTestUtils._\n+import org.apache.spark.util.Utils\n+\n+class HadoopBootstrapUtilSuite extends SparkFunSuite with BeforeAndAfter{\n+  private val sparkPod = SparkPod.initialPod()\n+  private val hadoopBootstrapUtil = new HadoopBootstrapUtil\n+  private var tmpDir: File = _\n+  private var tmpFile: File = _\n+\n+  before {\n+    tmpDir = Utils.createTempDir()\n+    tmpFile = File.createTempFile(s\"${UUID.randomUUID().toString}\", \".txt\", tmpDir)\n+    Files.write(\"contents\".getBytes, tmpFile)\n+  }\n+\n+  after {\n+    tmpFile.delete()\n+    tmpDir.delete()\n+  }\n+\n+  test(\"bootstrapKerberosPod with file location specified for krb5.conf file\") {\n+    val dtSecretName = \"EXAMPLE_SECRET_NAME\""
  }],
  "prId": 22760
}]