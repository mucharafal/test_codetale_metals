[{
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "You can also declare this test with `ignore` rather than `test`. It would make sure it's at least compiled, if that's desirable, but not run. Doesn't matter much. I'd block-comment this much code to disable it though.",
    "commit": "67df340d943d38afd1ea4c12c02b417b5434970f",
    "createdAt": "2018-07-05T23:22:34Z",
    "diffHunk": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.integrationtest\n+\n+import io.fabric8.kubernetes.api.model.Pod\n+\n+private[spark] trait BasicTestsSuite { k8sSuite: KubernetesSuite =>\n+\n+  test(\"Run SparkPi with no resources.\") {\n+    runSparkPiAndVerifyCompletion()\n+  }\n+\n+  test(\"Run SparkPi with a very long application name.\") {\n+    sparkAppConf.set(\"spark.app.name\", \"long\" * 40)\n+    runSparkPiAndVerifyCompletion()\n+  }\n+\n+  test(\"Run SparkPi with a master URL without a scheme.\") {\n+    val url = kubernetesTestComponents.kubernetesClient.getMasterUrl\n+    val k8sMasterUrl = if (url.getPort < 0) {\n+      s\"k8s://${url.getHost}\"\n+    } else {\n+      s\"k8s://${url.getHost}:${url.getPort}\"\n+    }\n+    sparkAppConf.set(\"spark.master\", k8sMasterUrl)\n+    runSparkPiAndVerifyCompletion()\n+  }\n+\n+  test(\"Run SparkPi with an argument.\") {\n+    runSparkPiAndVerifyCompletion(appArgs = Array(\"5\"))\n+  }\n+\n+  test(\"Run SparkPi with custom labels, annotations, and environment variables.\") {\n+    sparkAppConf\n+      .set(\"spark.kubernetes.driver.label.label1\", \"label1-value\")\n+      .set(\"spark.kubernetes.driver.label.label2\", \"label2-value\")\n+      .set(\"spark.kubernetes.driver.annotation.annotation1\", \"annotation1-value\")\n+      .set(\"spark.kubernetes.driver.annotation.annotation2\", \"annotation2-value\")\n+      .set(\"spark.kubernetes.driverEnv.ENV1\", \"VALUE1\")\n+      .set(\"spark.kubernetes.driverEnv.ENV2\", \"VALUE2\")\n+      .set(\"spark.kubernetes.executor.label.label1\", \"label1-value\")\n+      .set(\"spark.kubernetes.executor.label.label2\", \"label2-value\")\n+      .set(\"spark.kubernetes.executor.annotation.annotation1\", \"annotation1-value\")\n+      .set(\"spark.kubernetes.executor.annotation.annotation2\", \"annotation2-value\")\n+      .set(\"spark.executorEnv.ENV1\", \"VALUE1\")\n+      .set(\"spark.executorEnv.ENV2\", \"VALUE2\")\n+\n+    runSparkPiAndVerifyCompletion(\n+      driverPodChecker = (driverPod: Pod) => {\n+        doBasicDriverPodCheck(driverPod)\n+        checkCustomSettings(driverPod)\n+      },\n+      executorPodChecker = (executorPod: Pod) => {\n+        doBasicExecutorPodCheck(executorPod)\n+        checkCustomSettings(executorPod)\n+      })\n+  }\n+\n+  // TODO(ssuchter): Enable the below after debugging\n+  // test(\"Run PageRank using remote data file\") {"
  }, {
    "author": {
      "login": "skonto"
    },
    "body": "yeah good idea as well, can try that.",
    "commit": "67df340d943d38afd1ea4c12c02b417b5434970f",
    "createdAt": "2018-07-10T14:13:52Z",
    "diffHunk": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.integrationtest\n+\n+import io.fabric8.kubernetes.api.model.Pod\n+\n+private[spark] trait BasicTestsSuite { k8sSuite: KubernetesSuite =>\n+\n+  test(\"Run SparkPi with no resources.\") {\n+    runSparkPiAndVerifyCompletion()\n+  }\n+\n+  test(\"Run SparkPi with a very long application name.\") {\n+    sparkAppConf.set(\"spark.app.name\", \"long\" * 40)\n+    runSparkPiAndVerifyCompletion()\n+  }\n+\n+  test(\"Run SparkPi with a master URL without a scheme.\") {\n+    val url = kubernetesTestComponents.kubernetesClient.getMasterUrl\n+    val k8sMasterUrl = if (url.getPort < 0) {\n+      s\"k8s://${url.getHost}\"\n+    } else {\n+      s\"k8s://${url.getHost}:${url.getPort}\"\n+    }\n+    sparkAppConf.set(\"spark.master\", k8sMasterUrl)\n+    runSparkPiAndVerifyCompletion()\n+  }\n+\n+  test(\"Run SparkPi with an argument.\") {\n+    runSparkPiAndVerifyCompletion(appArgs = Array(\"5\"))\n+  }\n+\n+  test(\"Run SparkPi with custom labels, annotations, and environment variables.\") {\n+    sparkAppConf\n+      .set(\"spark.kubernetes.driver.label.label1\", \"label1-value\")\n+      .set(\"spark.kubernetes.driver.label.label2\", \"label2-value\")\n+      .set(\"spark.kubernetes.driver.annotation.annotation1\", \"annotation1-value\")\n+      .set(\"spark.kubernetes.driver.annotation.annotation2\", \"annotation2-value\")\n+      .set(\"spark.kubernetes.driverEnv.ENV1\", \"VALUE1\")\n+      .set(\"spark.kubernetes.driverEnv.ENV2\", \"VALUE2\")\n+      .set(\"spark.kubernetes.executor.label.label1\", \"label1-value\")\n+      .set(\"spark.kubernetes.executor.label.label2\", \"label2-value\")\n+      .set(\"spark.kubernetes.executor.annotation.annotation1\", \"annotation1-value\")\n+      .set(\"spark.kubernetes.executor.annotation.annotation2\", \"annotation2-value\")\n+      .set(\"spark.executorEnv.ENV1\", \"VALUE1\")\n+      .set(\"spark.executorEnv.ENV2\", \"VALUE2\")\n+\n+    runSparkPiAndVerifyCompletion(\n+      driverPodChecker = (driverPod: Pod) => {\n+        doBasicDriverPodCheck(driverPod)\n+        checkCustomSettings(driverPod)\n+      },\n+      executorPodChecker = (executorPod: Pod) => {\n+        doBasicExecutorPodCheck(executorPod)\n+        checkCustomSettings(executorPod)\n+      })\n+  }\n+\n+  // TODO(ssuchter): Enable the below after debugging\n+  // test(\"Run PageRank using remote data file\") {"
  }],
  "prId": 21652
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "This looks like stuff that should just be removed?",
    "commit": "67df340d943d38afd1ea4c12c02b417b5434970f",
    "createdAt": "2018-07-05T23:22:49Z",
    "diffHunk": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.integrationtest\n+\n+import io.fabric8.kubernetes.api.model.Pod\n+\n+private[spark] trait BasicTestsSuite { k8sSuite: KubernetesSuite =>\n+\n+  test(\"Run SparkPi with no resources.\") {\n+    runSparkPiAndVerifyCompletion()\n+  }\n+\n+  test(\"Run SparkPi with a very long application name.\") {\n+    sparkAppConf.set(\"spark.app.name\", \"long\" * 40)\n+    runSparkPiAndVerifyCompletion()\n+  }\n+\n+  test(\"Run SparkPi with a master URL without a scheme.\") {\n+    val url = kubernetesTestComponents.kubernetesClient.getMasterUrl\n+    val k8sMasterUrl = if (url.getPort < 0) {\n+      s\"k8s://${url.getHost}\"\n+    } else {\n+      s\"k8s://${url.getHost}:${url.getPort}\"\n+    }\n+    sparkAppConf.set(\"spark.master\", k8sMasterUrl)\n+    runSparkPiAndVerifyCompletion()\n+  }\n+\n+  test(\"Run SparkPi with an argument.\") {\n+    runSparkPiAndVerifyCompletion(appArgs = Array(\"5\"))\n+  }\n+\n+  test(\"Run SparkPi with custom labels, annotations, and environment variables.\") {\n+    sparkAppConf\n+      .set(\"spark.kubernetes.driver.label.label1\", \"label1-value\")\n+      .set(\"spark.kubernetes.driver.label.label2\", \"label2-value\")\n+      .set(\"spark.kubernetes.driver.annotation.annotation1\", \"annotation1-value\")\n+      .set(\"spark.kubernetes.driver.annotation.annotation2\", \"annotation2-value\")\n+      .set(\"spark.kubernetes.driverEnv.ENV1\", \"VALUE1\")\n+      .set(\"spark.kubernetes.driverEnv.ENV2\", \"VALUE2\")\n+      .set(\"spark.kubernetes.executor.label.label1\", \"label1-value\")\n+      .set(\"spark.kubernetes.executor.label.label2\", \"label2-value\")\n+      .set(\"spark.kubernetes.executor.annotation.annotation1\", \"annotation1-value\")\n+      .set(\"spark.kubernetes.executor.annotation.annotation2\", \"annotation2-value\")\n+      .set(\"spark.executorEnv.ENV1\", \"VALUE1\")\n+      .set(\"spark.executorEnv.ENV2\", \"VALUE2\")\n+\n+    runSparkPiAndVerifyCompletion(\n+      driverPodChecker = (driverPod: Pod) => {\n+        doBasicDriverPodCheck(driverPod)\n+        checkCustomSettings(driverPod)\n+      },\n+      executorPodChecker = (executorPod: Pod) => {\n+        doBasicExecutorPodCheck(executorPod)\n+        checkCustomSettings(executorPod)\n+      })\n+  }\n+\n+  // TODO(ssuchter): Enable the below after debugging\n+  // test(\"Run PageRank using remote data file\") {\n+  //   sparkAppConf\n+  //     .set(\"spark.kubernetes.mountDependencies.filesDownloadDir\",\n+  //       CONTAINER_LOCAL_FILE_DOWNLOAD_PATH)\n+  //     .set(\"spark.files\", REMOTE_PAGE_RANK_DATA_FILE)\n+  //   runSparkPageRankAndVerifyCompletion(\n+  //     appArgs = Array(CONTAINER_LOCAL_DOWNLOADED_PAGE_RANK_DATA_FILE))\n+  // }\n+\n+//  private def runSparkPageRankAndVerifyCompletion(\n+//      appResource: String = containerLocalSparkDistroExamplesJar,\n+//      driverPodChecker: Pod => Unit = doBasicDriverPodCheck,\n+//      executorPodChecker: Pod => Unit = doBasicExecutorPodCheck,\n+//      appArgs: Array[String],\n+//      appLocator: String = appLocator): Unit = {\n+//    runSparkApplicationAndVerifyCompletion(\n+//      appResource,\n+//      SPARK_PAGE_RANK_MAIN_CLASS,\n+//      Seq(\"1 has rank\", \"2 has rank\", \"3 has rank\", \"4 has rank\"),\n+//      appArgs,\n+//      driverPodChecker,\n+//      executorPodChecker,\n+//      appLocator)\n+//  }\n+}\n+\n+private[spark] object BasicTestsSuite {\n+  val SPARK_PAGE_RANK_MAIN_CLASS: String = \"org.apache.spark.examples.SparkPageRank\"\n+  // val CONTAINER_LOCAL_FILE_DOWNLOAD_PATH = \"/var/spark-data/spark-files\"\n+\n+  // val REMOTE_PAGE_RANK_DATA_FILE =\n+  //   \"https://storage.googleapis.com/spark-k8s-integration-tests/files/pagerank_data.txt\"\n+  // val CONTAINER_LOCAL_DOWNLOADED_PAGE_RANK_DATA_FILE =\n+  //   s\"$CONTAINER_LOCAL_FILE_DOWNLOAD_PATH/pagerank_data.txt\"\n+\n+  // case object ShuffleNotReadyException extends Exception"
  }, {
    "author": {
      "login": "skonto"
    },
    "body": "Yes they are left there from previous PR, but I think the plan was re-enable them in the future.",
    "commit": "67df340d943d38afd1ea4c12c02b417b5434970f",
    "createdAt": "2018-07-10T14:13:29Z",
    "diffHunk": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.k8s.integrationtest\n+\n+import io.fabric8.kubernetes.api.model.Pod\n+\n+private[spark] trait BasicTestsSuite { k8sSuite: KubernetesSuite =>\n+\n+  test(\"Run SparkPi with no resources.\") {\n+    runSparkPiAndVerifyCompletion()\n+  }\n+\n+  test(\"Run SparkPi with a very long application name.\") {\n+    sparkAppConf.set(\"spark.app.name\", \"long\" * 40)\n+    runSparkPiAndVerifyCompletion()\n+  }\n+\n+  test(\"Run SparkPi with a master URL without a scheme.\") {\n+    val url = kubernetesTestComponents.kubernetesClient.getMasterUrl\n+    val k8sMasterUrl = if (url.getPort < 0) {\n+      s\"k8s://${url.getHost}\"\n+    } else {\n+      s\"k8s://${url.getHost}:${url.getPort}\"\n+    }\n+    sparkAppConf.set(\"spark.master\", k8sMasterUrl)\n+    runSparkPiAndVerifyCompletion()\n+  }\n+\n+  test(\"Run SparkPi with an argument.\") {\n+    runSparkPiAndVerifyCompletion(appArgs = Array(\"5\"))\n+  }\n+\n+  test(\"Run SparkPi with custom labels, annotations, and environment variables.\") {\n+    sparkAppConf\n+      .set(\"spark.kubernetes.driver.label.label1\", \"label1-value\")\n+      .set(\"spark.kubernetes.driver.label.label2\", \"label2-value\")\n+      .set(\"spark.kubernetes.driver.annotation.annotation1\", \"annotation1-value\")\n+      .set(\"spark.kubernetes.driver.annotation.annotation2\", \"annotation2-value\")\n+      .set(\"spark.kubernetes.driverEnv.ENV1\", \"VALUE1\")\n+      .set(\"spark.kubernetes.driverEnv.ENV2\", \"VALUE2\")\n+      .set(\"spark.kubernetes.executor.label.label1\", \"label1-value\")\n+      .set(\"spark.kubernetes.executor.label.label2\", \"label2-value\")\n+      .set(\"spark.kubernetes.executor.annotation.annotation1\", \"annotation1-value\")\n+      .set(\"spark.kubernetes.executor.annotation.annotation2\", \"annotation2-value\")\n+      .set(\"spark.executorEnv.ENV1\", \"VALUE1\")\n+      .set(\"spark.executorEnv.ENV2\", \"VALUE2\")\n+\n+    runSparkPiAndVerifyCompletion(\n+      driverPodChecker = (driverPod: Pod) => {\n+        doBasicDriverPodCheck(driverPod)\n+        checkCustomSettings(driverPod)\n+      },\n+      executorPodChecker = (executorPod: Pod) => {\n+        doBasicExecutorPodCheck(executorPod)\n+        checkCustomSettings(executorPod)\n+      })\n+  }\n+\n+  // TODO(ssuchter): Enable the below after debugging\n+  // test(\"Run PageRank using remote data file\") {\n+  //   sparkAppConf\n+  //     .set(\"spark.kubernetes.mountDependencies.filesDownloadDir\",\n+  //       CONTAINER_LOCAL_FILE_DOWNLOAD_PATH)\n+  //     .set(\"spark.files\", REMOTE_PAGE_RANK_DATA_FILE)\n+  //   runSparkPageRankAndVerifyCompletion(\n+  //     appArgs = Array(CONTAINER_LOCAL_DOWNLOADED_PAGE_RANK_DATA_FILE))\n+  // }\n+\n+//  private def runSparkPageRankAndVerifyCompletion(\n+//      appResource: String = containerLocalSparkDistroExamplesJar,\n+//      driverPodChecker: Pod => Unit = doBasicDriverPodCheck,\n+//      executorPodChecker: Pod => Unit = doBasicExecutorPodCheck,\n+//      appArgs: Array[String],\n+//      appLocator: String = appLocator): Unit = {\n+//    runSparkApplicationAndVerifyCompletion(\n+//      appResource,\n+//      SPARK_PAGE_RANK_MAIN_CLASS,\n+//      Seq(\"1 has rank\", \"2 has rank\", \"3 has rank\", \"4 has rank\"),\n+//      appArgs,\n+//      driverPodChecker,\n+//      executorPodChecker,\n+//      appLocator)\n+//  }\n+}\n+\n+private[spark] object BasicTestsSuite {\n+  val SPARK_PAGE_RANK_MAIN_CLASS: String = \"org.apache.spark.examples.SparkPageRank\"\n+  // val CONTAINER_LOCAL_FILE_DOWNLOAD_PATH = \"/var/spark-data/spark-files\"\n+\n+  // val REMOTE_PAGE_RANK_DATA_FILE =\n+  //   \"https://storage.googleapis.com/spark-k8s-integration-tests/files/pagerank_data.txt\"\n+  // val CONTAINER_LOCAL_DOWNLOADED_PAGE_RANK_DATA_FILE =\n+  //   s\"$CONTAINER_LOCAL_FILE_DOWNLOAD_PATH/pagerank_data.txt\"\n+\n+  // case object ShuffleNotReadyException extends Exception"
  }],
  "prId": 21652
}]