[{
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "`By default this is set to minikube. The available backends and their prerequisites are described below.`",
    "commit": "cf7ef362352365c7aab4c927db3ea8240fcfaf21",
    "createdAt": "2018-10-25T18:06:40Z",
    "diffHunk": "@@ -13,15 +13,45 @@ The simplest way to run the integration tests is to install and run Minikube, th\n     dev/dev-run-integration-tests.sh\n \n The minimum tested version of Minikube is 0.23.0. The kube-dns addon must be enabled. Minikube should\n-run with a minimum of 3 CPUs and 4G of memory:\n+run with a minimum of 4 CPUs and 6G of memory:\n \n-    minikube start --cpus 3 --memory 4096\n+    minikube start --cpus 4 --memory 6144\n \n You can download Minikube [here](https://github.com/kubernetes/minikube/releases).\n \n # Integration test customization\n \n-Configuration of the integration test runtime is done through passing different arguments to the test script. The main useful options are outlined below.\n+Configuration of the integration test runtime is done through passing different arguments to the test script. \n+The main useful options are outlined below.\n+\n+## Using a different backend\n+\n+The integration test backend i.e. the K8S cluster used for testing is controlled by the `--deploy-mode` option.  By default this\n+is set to `minikube`, the available backends are their perquisites are as follows."
  }, {
    "author": {
      "login": "rvesse"
    },
    "body": "Fixed",
    "commit": "cf7ef362352365c7aab4c927db3ea8240fcfaf21",
    "createdAt": "2018-10-26T09:47:09Z",
    "diffHunk": "@@ -13,15 +13,45 @@ The simplest way to run the integration tests is to install and run Minikube, th\n     dev/dev-run-integration-tests.sh\n \n The minimum tested version of Minikube is 0.23.0. The kube-dns addon must be enabled. Minikube should\n-run with a minimum of 3 CPUs and 4G of memory:\n+run with a minimum of 4 CPUs and 6G of memory:\n \n-    minikube start --cpus 3 --memory 4096\n+    minikube start --cpus 4 --memory 6144\n \n You can download Minikube [here](https://github.com/kubernetes/minikube/releases).\n \n # Integration test customization\n \n-Configuration of the integration test runtime is done through passing different arguments to the test script. The main useful options are outlined below.\n+Configuration of the integration test runtime is done through passing different arguments to the test script. \n+The main useful options are outlined below.\n+\n+## Using a different backend\n+\n+The integration test backend i.e. the K8S cluster used for testing is controlled by the `--deploy-mode` option.  By default this\n+is set to `minikube`, the available backends are their perquisites are as follows."
  }],
  "prId": 22805
}, {
  "comments": [{
    "author": {
      "login": "mccheah"
    },
    "body": "`If you prefer to run just the integration tests directly, then you can customize the behaviour via passing system properties to Maven. For example:`",
    "commit": "cf7ef362352365c7aab4c927db3ea8240fcfaf21",
    "createdAt": "2018-10-25T18:07:49Z",
    "diffHunk": "@@ -41,12 +71,127 @@ The Spark code to test is handed to the integration test system via a tarball. H\n \n * `--spark-tgz <path-to-tgz>` - set `<path-to-tgz>` to point to a tarball containing the Spark distribution to test.\n \n-TODO: Don't require the packaging of the built Spark artifacts into this tarball, just read them out of the current tree.\n+This Tarball should be created by first running `dev/make-distribution.sh` passing the `--tgz` flag and `-Pkubernetes` as one of the\n+options to ensure that Kubernetes support is included in the distribution.  For more details on building a runnable distribution please\n+see the [Building Spark](https://spark.apache.org/docs/latest/building-spark.html#building-a-runnable-distribution) documentation.\n+\n+**TODO:** Don't require the packaging of the built Spark artifacts into this tarball, just read them out of the current tree.\n \n ## Customizing the Namespace and Service Account\n \n-* `--namespace <namespace>` - set `<namespace>` to the namespace in which the tests should be run.\n-* `--service-account <service account name>` - set `<service account name>` to the name of the Kubernetes service account to\n-use in the namespace specified by the `--namespace`. The service account is expected to have permissions to get, list, watch,\n+If no namespace is specified then a temporary namespace will be created and deleted during the test run.  Similarly if no service\n+account is specified then the `default` service account for the namespace will be used.\n+\n+Using the `--namespace <namespace>` flag sets `<namespace>` to the namespace in which the tests should be run.  If this is supplied \n+then the tests assume this namespace exists in the K8S cluster and will not attempt to create it.  Additionally this namespace must \n+have an appropriately authorized service account which can be customised via the `--service-account` flag.\n+\n+The `--service-account <service account name>` flag sets `<service account name>` to the name of the Kubernetes service account to\n+use in the namespace specified by the `--namespace` flag. The service account is expected to have permissions to get, list, watch,\n and create pods. For clusters with RBAC turned on, it's important that the right permissions are granted to the service account\n in the namespace through an appropriate role and role binding. A reference RBAC configuration is provided in `dev/spark-rbac.yaml`.\n+\n+# Running the Test Directly\n+\n+If you prefer to run just the integration tests directly then you can customise the behaviour via properties passed to Maven using the\n+`-Dproperty=value` option e.g."
  }, {
    "author": {
      "login": "rvesse"
    },
    "body": "Fixed",
    "commit": "cf7ef362352365c7aab4c927db3ea8240fcfaf21",
    "createdAt": "2018-10-26T09:47:00Z",
    "diffHunk": "@@ -41,12 +71,127 @@ The Spark code to test is handed to the integration test system via a tarball. H\n \n * `--spark-tgz <path-to-tgz>` - set `<path-to-tgz>` to point to a tarball containing the Spark distribution to test.\n \n-TODO: Don't require the packaging of the built Spark artifacts into this tarball, just read them out of the current tree.\n+This Tarball should be created by first running `dev/make-distribution.sh` passing the `--tgz` flag and `-Pkubernetes` as one of the\n+options to ensure that Kubernetes support is included in the distribution.  For more details on building a runnable distribution please\n+see the [Building Spark](https://spark.apache.org/docs/latest/building-spark.html#building-a-runnable-distribution) documentation.\n+\n+**TODO:** Don't require the packaging of the built Spark artifacts into this tarball, just read them out of the current tree.\n \n ## Customizing the Namespace and Service Account\n \n-* `--namespace <namespace>` - set `<namespace>` to the namespace in which the tests should be run.\n-* `--service-account <service account name>` - set `<service account name>` to the name of the Kubernetes service account to\n-use in the namespace specified by the `--namespace`. The service account is expected to have permissions to get, list, watch,\n+If no namespace is specified then a temporary namespace will be created and deleted during the test run.  Similarly if no service\n+account is specified then the `default` service account for the namespace will be used.\n+\n+Using the `--namespace <namespace>` flag sets `<namespace>` to the namespace in which the tests should be run.  If this is supplied \n+then the tests assume this namespace exists in the K8S cluster and will not attempt to create it.  Additionally this namespace must \n+have an appropriately authorized service account which can be customised via the `--service-account` flag.\n+\n+The `--service-account <service account name>` flag sets `<service account name>` to the name of the Kubernetes service account to\n+use in the namespace specified by the `--namespace` flag. The service account is expected to have permissions to get, list, watch,\n and create pods. For clusters with RBAC turned on, it's important that the right permissions are granted to the service account\n in the namespace through an appropriate role and role binding. A reference RBAC configuration is provided in `dev/spark-rbac.yaml`.\n+\n+# Running the Test Directly\n+\n+If you prefer to run just the integration tests directly then you can customise the behaviour via properties passed to Maven using the\n+`-Dproperty=value` option e.g."
  }],
  "prId": 22805
}]