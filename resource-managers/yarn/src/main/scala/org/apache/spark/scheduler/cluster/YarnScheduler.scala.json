[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Use the shared instance?\r\n\r\nOr, if not using the shared instance, then the `SparkRackResolver` object can go away.",
    "commit": "ad63e154b0aae43e2e83fc2ea7700b0545c2fd99",
    "createdAt": "2019-03-29T23:17:49Z",
    "diffHunk": "@@ -17,23 +17,29 @@\n \n package org.apache.spark.scheduler.cluster\n \n-import org.apache.hadoop.yarn.util.RackResolver\n-import org.apache.log4j.{Level, Logger}\n+import org.apache.hadoop.net.NetworkTopology\n \n import org.apache.spark._\n+import org.apache.spark.deploy.yarn.SparkRackResolver\n import org.apache.spark.scheduler.TaskSchedulerImpl\n import org.apache.spark.util.Utils\n \n private[spark] class YarnScheduler(sc: SparkContext) extends TaskSchedulerImpl(sc) {\n \n-  // RackResolver logs an INFO message whenever it resolves a rack, which is way too often.\n-  if (Logger.getLogger(classOf[RackResolver]).getLevel == null) {\n-    Logger.getLogger(classOf[RackResolver]).setLevel(Level.WARN)\n-  }\n+  override val defaultRackValue: Option[String] = Some(NetworkTopology.DEFAULT_RACK)\n+\n+  private[spark] val resolver = new SparkRackResolver(sc.hadoopConfiguration)"
  }],
  "prId": 24245
}]