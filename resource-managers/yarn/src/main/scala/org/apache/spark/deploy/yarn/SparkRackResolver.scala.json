[{
  "comments": [{
    "author": {
      "login": "LantaoJin"
    },
    "body": "Should we explain how to instantiate a separate resolver with a separate config here?\r\n> Instantiate a separate resolver with a separate config by `new SparkRackResolver(conf)`",
    "commit": "ad63e154b0aae43e2e83fc2ea7700b0545c2fd99",
    "createdAt": "2019-04-02T12:01:40Z",
    "diffHunk": "@@ -17,24 +17,105 @@\n \n package org.apache.spark.deploy.yarn\n \n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.google.common.base.Strings\n import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic\n+import org.apache.hadoop.net._\n+import org.apache.hadoop.util.ReflectionUtils\n import org.apache.hadoop.yarn.util.RackResolver\n import org.apache.log4j.{Level, Logger}\n \n+import org.apache.spark.internal.Logging\n+\n /**\n- * Wrapper around YARN's [[RackResolver]]. This allows Spark tests to easily override the\n+ * Re-implement YARN's [[RackResolver]]. This allows Spark tests to easily override the\n  * default behavior, since YARN's class self-initializes the first time it's called, and\n  * future calls all use the initial configuration.\n  */\n-private[yarn] class SparkRackResolver {\n+private[spark] class SparkRackResolver(conf: Configuration) extends Logging {\n \n   // RackResolver logs an INFO message whenever it resolves a rack, which is way too often.\n   if (Logger.getLogger(classOf[RackResolver]).getLevel == null) {\n     Logger.getLogger(classOf[RackResolver]).setLevel(Level.WARN)\n   }\n \n-  def resolve(conf: Configuration, hostName: String): String = {\n-    RackResolver.resolve(conf, hostName).getNetworkLocation()\n+  private val dnsToSwitchMapping: DNSToSwitchMapping = {\n+    val dnsToSwitchMappingClass =\n+      conf.getClass(CommonConfigurationKeysPublic.NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY,\n+        classOf[ScriptBasedMapping], classOf[DNSToSwitchMapping])\n+    ReflectionUtils.newInstance(dnsToSwitchMappingClass, conf)\n+        .asInstanceOf[DNSToSwitchMapping] match {\n+      case c: CachedDNSToSwitchMapping => c\n+      case o => new CachedDNSToSwitchMapping(o)\n+    }\n+  }\n+\n+  def resolve(hostName: String): String = {\n+    coreResolve(Seq(hostName)).head.getNetworkLocation\n+  }\n+\n+  /**\n+   * Added in SPARK-13704.\n+   * This should be changed to `RackResolver.resolve(conf, hostNames)`\n+   * in hadoop releases with YARN-9332.\n+   */\n+  def resolve(hostNames: Seq[String]): Seq[Node] = {\n+    coreResolve(hostNames)\n+  }\n+\n+  private def coreResolve(hostNames: Seq[String]): Seq[Node] = {\n+    val nodes = new ArrayBuffer[Node]\n+    // dnsToSwitchMapping is thread-safe\n+    val rNameList = dnsToSwitchMapping.resolve(hostNames.toList.asJava).asScala\n+    if (rNameList == null || rNameList.isEmpty) {\n+      hostNames.foreach(nodes += new NodeBase(_, NetworkTopology.DEFAULT_RACK))\n+      logInfo(s\"Got an error when resolving hostNames. \" +\n+        s\"Falling back to ${NetworkTopology.DEFAULT_RACK} for all\")\n+    } else {\n+      for ((hostName, rName) <- hostNames.zip(rNameList)) {\n+        if (Strings.isNullOrEmpty(rName)) {\n+          nodes += new NodeBase(hostName, NetworkTopology.DEFAULT_RACK)\n+          logDebug(s\"Could not resolve $hostName. \" +\n+            s\"Falling back to ${NetworkTopology.DEFAULT_RACK}\")\n+        } else {\n+          nodes += new NodeBase(hostName, rName)\n+        }\n+      }\n+    }\n+    nodes.toList\n+  }\n+}\n+\n+/**\n+ * Utility to resolve the rack for hosts in an efficient manner.\n+ * It will cache the rack for individual hosts to avoid\n+ * repeatedly performing the same expensive lookup.\n+ *\n+ * Its logic refers [[org.apache.hadoop.yarn.util.RackResolver]] and enhanced.\n+ * This will be unnecessary in hadoop releases with YARN-9332.\n+ * With that, we could just directly use [[org.apache.hadoop.yarn.util.RackResolver]].\n+ * In the meantime, this is a re-implementation for spark's use.\n+ */\n+object SparkRackResolver extends Logging {\n+  @volatile private var instance: SparkRackResolver = _\n+\n+  /**\n+   * It will return the static resolver instance.  If there is already an instance, the passed\n+   * conf is entirely ignored.  If there is not a shared instance, it will create one with the\n+   * given conf.",
    "line": 93
  }, {
    "author": {
      "login": "squito"
    },
    "body": "its kinda obvious, no?",
    "commit": "ad63e154b0aae43e2e83fc2ea7700b0545c2fd99",
    "createdAt": "2019-04-02T17:37:59Z",
    "diffHunk": "@@ -17,24 +17,105 @@\n \n package org.apache.spark.deploy.yarn\n \n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.google.common.base.Strings\n import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic\n+import org.apache.hadoop.net._\n+import org.apache.hadoop.util.ReflectionUtils\n import org.apache.hadoop.yarn.util.RackResolver\n import org.apache.log4j.{Level, Logger}\n \n+import org.apache.spark.internal.Logging\n+\n /**\n- * Wrapper around YARN's [[RackResolver]]. This allows Spark tests to easily override the\n+ * Re-implement YARN's [[RackResolver]]. This allows Spark tests to easily override the\n  * default behavior, since YARN's class self-initializes the first time it's called, and\n  * future calls all use the initial configuration.\n  */\n-private[yarn] class SparkRackResolver {\n+private[spark] class SparkRackResolver(conf: Configuration) extends Logging {\n \n   // RackResolver logs an INFO message whenever it resolves a rack, which is way too often.\n   if (Logger.getLogger(classOf[RackResolver]).getLevel == null) {\n     Logger.getLogger(classOf[RackResolver]).setLevel(Level.WARN)\n   }\n \n-  def resolve(conf: Configuration, hostName: String): String = {\n-    RackResolver.resolve(conf, hostName).getNetworkLocation()\n+  private val dnsToSwitchMapping: DNSToSwitchMapping = {\n+    val dnsToSwitchMappingClass =\n+      conf.getClass(CommonConfigurationKeysPublic.NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY,\n+        classOf[ScriptBasedMapping], classOf[DNSToSwitchMapping])\n+    ReflectionUtils.newInstance(dnsToSwitchMappingClass, conf)\n+        .asInstanceOf[DNSToSwitchMapping] match {\n+      case c: CachedDNSToSwitchMapping => c\n+      case o => new CachedDNSToSwitchMapping(o)\n+    }\n+  }\n+\n+  def resolve(hostName: String): String = {\n+    coreResolve(Seq(hostName)).head.getNetworkLocation\n+  }\n+\n+  /**\n+   * Added in SPARK-13704.\n+   * This should be changed to `RackResolver.resolve(conf, hostNames)`\n+   * in hadoop releases with YARN-9332.\n+   */\n+  def resolve(hostNames: Seq[String]): Seq[Node] = {\n+    coreResolve(hostNames)\n+  }\n+\n+  private def coreResolve(hostNames: Seq[String]): Seq[Node] = {\n+    val nodes = new ArrayBuffer[Node]\n+    // dnsToSwitchMapping is thread-safe\n+    val rNameList = dnsToSwitchMapping.resolve(hostNames.toList.asJava).asScala\n+    if (rNameList == null || rNameList.isEmpty) {\n+      hostNames.foreach(nodes += new NodeBase(_, NetworkTopology.DEFAULT_RACK))\n+      logInfo(s\"Got an error when resolving hostNames. \" +\n+        s\"Falling back to ${NetworkTopology.DEFAULT_RACK} for all\")\n+    } else {\n+      for ((hostName, rName) <- hostNames.zip(rNameList)) {\n+        if (Strings.isNullOrEmpty(rName)) {\n+          nodes += new NodeBase(hostName, NetworkTopology.DEFAULT_RACK)\n+          logDebug(s\"Could not resolve $hostName. \" +\n+            s\"Falling back to ${NetworkTopology.DEFAULT_RACK}\")\n+        } else {\n+          nodes += new NodeBase(hostName, rName)\n+        }\n+      }\n+    }\n+    nodes.toList\n+  }\n+}\n+\n+/**\n+ * Utility to resolve the rack for hosts in an efficient manner.\n+ * It will cache the rack for individual hosts to avoid\n+ * repeatedly performing the same expensive lookup.\n+ *\n+ * Its logic refers [[org.apache.hadoop.yarn.util.RackResolver]] and enhanced.\n+ * This will be unnecessary in hadoop releases with YARN-9332.\n+ * With that, we could just directly use [[org.apache.hadoop.yarn.util.RackResolver]].\n+ * In the meantime, this is a re-implementation for spark's use.\n+ */\n+object SparkRackResolver extends Logging {\n+  @volatile private var instance: SparkRackResolver = _\n+\n+  /**\n+   * It will return the static resolver instance.  If there is already an instance, the passed\n+   * conf is entirely ignored.  If there is not a shared instance, it will create one with the\n+   * given conf.",
    "line": 93
  }],
  "prId": 24245
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This paragraph actually refers to the code in the class, now, not the object anymore.",
    "commit": "ad63e154b0aae43e2e83fc2ea7700b0545c2fd99",
    "createdAt": "2019-04-02T18:45:15Z",
    "diffHunk": "@@ -17,24 +17,105 @@\n \n package org.apache.spark.deploy.yarn\n \n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.google.common.base.Strings\n import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic\n+import org.apache.hadoop.net._\n+import org.apache.hadoop.util.ReflectionUtils\n import org.apache.hadoop.yarn.util.RackResolver\n import org.apache.log4j.{Level, Logger}\n \n+import org.apache.spark.internal.Logging\n+\n /**\n- * Wrapper around YARN's [[RackResolver]]. This allows Spark tests to easily override the\n+ * Re-implement YARN's [[RackResolver]]. This allows Spark tests to easily override the\n  * default behavior, since YARN's class self-initializes the first time it's called, and\n  * future calls all use the initial configuration.\n  */\n-private[yarn] class SparkRackResolver {\n+private[spark] class SparkRackResolver(conf: Configuration) extends Logging {\n \n   // RackResolver logs an INFO message whenever it resolves a rack, which is way too often.\n   if (Logger.getLogger(classOf[RackResolver]).getLevel == null) {\n     Logger.getLogger(classOf[RackResolver]).setLevel(Level.WARN)\n   }\n \n-  def resolve(conf: Configuration, hostName: String): String = {\n-    RackResolver.resolve(conf, hostName).getNetworkLocation()\n+  private val dnsToSwitchMapping: DNSToSwitchMapping = {\n+    val dnsToSwitchMappingClass =\n+      conf.getClass(CommonConfigurationKeysPublic.NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY,\n+        classOf[ScriptBasedMapping], classOf[DNSToSwitchMapping])\n+    ReflectionUtils.newInstance(dnsToSwitchMappingClass, conf)\n+        .asInstanceOf[DNSToSwitchMapping] match {\n+      case c: CachedDNSToSwitchMapping => c\n+      case o => new CachedDNSToSwitchMapping(o)\n+    }\n+  }\n+\n+  def resolve(hostName: String): String = {\n+    coreResolve(Seq(hostName)).head.getNetworkLocation\n+  }\n+\n+  /**\n+   * Added in SPARK-13704.\n+   * This should be changed to `RackResolver.resolve(conf, hostNames)`\n+   * in hadoop releases with YARN-9332.\n+   */\n+  def resolve(hostNames: Seq[String]): Seq[Node] = {\n+    coreResolve(hostNames)\n+  }\n+\n+  private def coreResolve(hostNames: Seq[String]): Seq[Node] = {\n+    val nodes = new ArrayBuffer[Node]\n+    // dnsToSwitchMapping is thread-safe\n+    val rNameList = dnsToSwitchMapping.resolve(hostNames.toList.asJava).asScala\n+    if (rNameList == null || rNameList.isEmpty) {\n+      hostNames.foreach(nodes += new NodeBase(_, NetworkTopology.DEFAULT_RACK))\n+      logInfo(s\"Got an error when resolving hostNames. \" +\n+        s\"Falling back to ${NetworkTopology.DEFAULT_RACK} for all\")\n+    } else {\n+      for ((hostName, rName) <- hostNames.zip(rNameList)) {\n+        if (Strings.isNullOrEmpty(rName)) {\n+          nodes += new NodeBase(hostName, NetworkTopology.DEFAULT_RACK)\n+          logDebug(s\"Could not resolve $hostName. \" +\n+            s\"Falling back to ${NetworkTopology.DEFAULT_RACK}\")\n+        } else {\n+          nodes += new NodeBase(hostName, rName)\n+        }\n+      }\n+    }\n+    nodes.toList\n+  }\n+}\n+\n+/**\n+ * Utility to resolve the rack for hosts in an efficient manner.\n+ * It will cache the rack for individual hosts to avoid\n+ * repeatedly performing the same expensive lookup.\n+ *\n+ * Its logic refers [[org.apache.hadoop.yarn.util.RackResolver]] and enhanced."
  }],
  "prId": 24245
}]