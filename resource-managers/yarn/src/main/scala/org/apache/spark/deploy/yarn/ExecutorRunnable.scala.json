[{
  "comments": [{
    "author": {
      "login": "tgravescs"
    },
    "body": "there are other variables that requires the append, like LD_LIBRARY_PATH.   It obviously can do bad things if its specified twice though but generally you shouldn't be doing that.  \r\n\r\nMaybe I missed it in the description exactly how is AM env taking presendence?  Can you please describe the flow as to why moving this or changing it fixes the bug (how is executor getting am env now)?",
    "commit": "2afc91eef55bcbf2e79431d8db64d71fabc21585",
    "createdAt": "2018-03-13T13:15:39Z",
    "diffHunk": "@@ -247,6 +241,18 @@ private[yarn] class ExecutorRunnable(\n \n     System.getenv().asScala.filterKeys(_.startsWith(\"SPARK\"))\n       .foreach { case (k, v) => env(k) = v }\n+\n+    sparkConf.getExecutorEnv.foreach { case (key, value) =>\n+      if (key == Environment.CLASSPATH.name()) {"
  }, {
    "author": {
      "login": "mridulm"
    },
    "body": "@tgravescs In existing code, in `prepareEnvironment`, \"env\" is populated only with `Environment.CLASSPATH`. Hence LD_LIBRARY_PATH does not apply to this specific change.",
    "commit": "2afc91eef55bcbf2e79431d8db64d71fabc21585",
    "createdAt": "2018-03-13T20:27:24Z",
    "diffHunk": "@@ -247,6 +241,18 @@ private[yarn] class ExecutorRunnable(\n \n     System.getenv().asScala.filterKeys(_.startsWith(\"SPARK\"))\n       .foreach { case (k, v) => env(k) = v }\n+\n+    sparkConf.getExecutorEnv.foreach { case (key, value) =>\n+      if (key == Environment.CLASSPATH.name()) {"
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "Ah, I see sorry missed that. So I guess here we are just stomping on whatever is in the system env path now, vs before we were stomping on the executorEnv specified with the system env.  ",
    "commit": "2afc91eef55bcbf2e79431d8db64d71fabc21585",
    "createdAt": "2018-03-13T21:43:27Z",
    "diffHunk": "@@ -247,6 +241,18 @@ private[yarn] class ExecutorRunnable(\n \n     System.getenv().asScala.filterKeys(_.startsWith(\"SPARK\"))\n       .foreach { case (k, v) => env(k) = v }\n+\n+    sparkConf.getExecutorEnv.foreach { case (key, value) =>\n+      if (key == Environment.CLASSPATH.name()) {"
  }],
  "prId": 20799
}, {
  "comments": [{
    "author": {
      "login": "mridulm"
    },
    "body": "@jerryshao I think there is a potential issue with this change - it allows for users to (incorrectly) specify SPARK_LOG_URL_STDERR, SPARK_LOG_URL_STDOUT : which should be generated by driver. The section \"// Add log urls\" above this code snippet.\r\n\r\nNote, this is an existing bug in the code regarding the same - if the same variables had been present in driver env, they would have overridden the generated value's.\r\nWould be good to fix this issue as well as part of this change.\r\n\r\nSolution would be to move the block for '// Add log urls' below this current block",
    "commit": "2afc91eef55bcbf2e79431d8db64d71fabc21585",
    "createdAt": "2018-03-13T20:30:17Z",
    "diffHunk": "@@ -247,6 +241,18 @@ private[yarn] class ExecutorRunnable(\n \n     System.getenv().asScala.filterKeys(_.startsWith(\"SPARK\"))\n       .foreach { case (k, v) => env(k) = v }\n+\n+    sparkConf.getExecutorEnv.foreach { case (key, value) =>\n+      if (key == Environment.CLASSPATH.name()) {\n+        // If the key of env variable is CLASSPATH, we assume it is a path and append it.\n+        // This is kept for backward compatibility and consistency with hadoop\n+        YarnSparkHadoopUtil.addPathToEnvironment(env, key, value)\n+      } else {\n+        // For other env variables, simply overwrite the value.\n+        env(key) = value\n+      }\n+    }",
    "line": 29
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "Sure, I will fix it, thanks for the reminder.",
    "commit": "2afc91eef55bcbf2e79431d8db64d71fabc21585",
    "createdAt": "2018-03-14T01:22:16Z",
    "diffHunk": "@@ -247,6 +241,18 @@ private[yarn] class ExecutorRunnable(\n \n     System.getenv().asScala.filterKeys(_.startsWith(\"SPARK\"))\n       .foreach { case (k, v) => env(k) = v }\n+\n+    sparkConf.getExecutorEnv.foreach { case (key, value) =>\n+      if (key == Environment.CLASSPATH.name()) {\n+        // If the key of env variable is CLASSPATH, we assume it is a path and append it.\n+        // This is kept for backward compatibility and consistency with hadoop\n+        YarnSparkHadoopUtil.addPathToEnvironment(env, key, value)\n+      } else {\n+        // For other env variables, simply overwrite the value.\n+        env(key) = value\n+      }\n+    }",
    "line": 29
  }],
  "prId": 20799
}]