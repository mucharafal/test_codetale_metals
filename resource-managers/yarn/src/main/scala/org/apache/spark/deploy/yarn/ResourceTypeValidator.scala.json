[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`private[spark]` at least.",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-03-19T20:30:21Z",
    "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+\n+object ResourceTypeValidator {"
  }, {
    "author": {
      "login": "szilard-nemeth"
    },
    "body": "fixed",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-05-13T17:49:35Z",
    "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+\n+object ResourceTypeValidator {"
  }],
  "prId": 20761
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`: RequestedResources` is unnecessary.",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-03-19T20:30:45Z",
    "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+\n+object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+\n+  def validateResourceTypes(sparkConf: SparkConf): Unit = {\n+    val requestedResources: RequestedResources = new RequestedResources(sparkConf)"
  }, {
    "author": {
      "login": "szilard-nemeth"
    },
    "body": "fixed",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-05-13T17:49:45Z",
    "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+\n+object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+\n+  def validateResourceTypes(sparkConf: SparkConf): Unit = {\n+    val requestedResources: RequestedResources = new RequestedResources(sparkConf)"
  }],
  "prId": 20761
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Can you write a doc comment explaining what this method is doing? It's not immediately clear from the code what is being validated, what is a valid configuration, etc.\r\n\r\nThe code in this class also looks extremely complicated for what should be a simple validation of config values.",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-03-19T20:37:17Z",
    "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+\n+object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+\n+  def validateResourceTypes(sparkConf: SparkConf): Unit = {"
  }, {
    "author": {
      "login": "szilard-nemeth"
    },
    "body": "Added doc comment for most of the methods and refactored the whole code in this class, please check whether it became more simple to understand like this.",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-05-13T17:50:04Z",
    "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+\n+object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+\n+  def validateResourceTypes(sparkConf: SparkConf): Unit = {"
  }],
  "prId": 20761
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "I think you're a bit confused about what this is controlling. You can't request driver resources in client mode because the client does not run inside YARN. I think what you mean instead of \"driver\" in this case is what the rest of the code refers to as \"AM\".",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-03-19T20:40:30Z",
    "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+\n+object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+\n+  def validateResourceTypes(sparkConf: SparkConf): Unit = {\n+    val requestedResources: RequestedResources = new RequestedResources(sparkConf)\n+\n+    validateDuplicateResourceConfig(requestedResources,\n+      Seq[ResourceTypeConfigProperties](\n+        new ResourceTypeConfigProperties(\"driver\", \"client\", \"memory\"),\n+        new ResourceTypeConfigProperties(\"driver\", \"client\", \"cores\"),\n+        new ResourceTypeConfigProperties(\"driver\", \"cluster\", \"memory\"),\n+        new ResourceTypeConfigProperties(\"driver\", \"cluster\", \"cores\"),\n+        new ResourceTypeConfigProperties(role = \"executor\", resourceType = \"memory\"),\n+        new ResourceTypeConfigProperties(role = \"executor\", resourceType = \"cores\")))\n+  }\n+\n+  private def validateDuplicateResourceConfig(requestedResources: RequestedResources,\n+                                              resourceTypeConfigProperties:\n+                                              Seq[ResourceTypeConfigProperties]): Unit = {\n+    val sb = new mutable.StringBuilder()\n+    resourceTypeConfigProperties\n+      .foreach(rtc => {\n+        val errorMessage = validateDuplicateResourceConfigInternal(requestedResources, rtc)\n+        if (errorMessage.nonEmpty) {\n+          printErrorMessageToBuffer(sb, errorMessage)\n+        }\n+      })\n+\n+    if (sb.nonEmpty) {\n+      throw new SparkException(sb.toString())\n+    }\n+  }\n+\n+  private[spark] def printErrorMessageToBuffer(sb: StringBuilder, str: String) = {\n+    sb.append(s\"$ERROR_PREFIX$str\\n\")\n+  }\n+\n+  private def validateDuplicateResourceConfigInternal(requestedResources: RequestedResources,\n+                                                      rtc: ResourceTypeConfigProperties): String = {\n+    val role = rtc.role\n+    val mode = rtc.mode\n+    val resourceType = rtc.resourceType\n+\n+    if (role != \"driver\" && role != \"executor\") {\n+      throw new IllegalArgumentException(\"Role must be either 'driver' or 'executor'!\")\n+    }\n+    if (mode != \"\" && mode != \"client\" && mode != \"cluster\") {\n+      throw new IllegalArgumentException(\"Mode must be either 'client' or 'cluster'!\")\n+    }\n+    if (resourceType != \"cores\" && resourceType != \"memory\") {\n+      throw new IllegalArgumentException(\"Resource type must be either 'cores' or 'memory'!\")\n+    }\n+\n+    var customResourceTypes: Map[String, String] = null\n+    (role, mode, resourceType) match {\n+      case (\"executor\", _, _) => customResourceTypes = requestedResources\n+        .customResourceTypesForExecutor\n+      case (\"driver\", \"client\", _) => customResourceTypes = requestedResources\n+        .customResourceTypesForDriverClientMode\n+      case (\"driver\", \"cluster\", _) => customResourceTypes = requestedResources\n+        .customResourceTypesForDriverClusterMode\n+    }\n+\n+    var resourceTypeObj: String = null\n+    (role, resourceType) match {\n+      case (\"driver\", \"cores\") => resourceTypeObj = requestedResources.driverCores\n+      case (\"driver\", \"memory\") => resourceTypeObj = requestedResources.driverMemory\n+      case (\"executor\", \"cores\") => resourceTypeObj = requestedResources.executorCores\n+      case (\"executor\", \"memory\") => resourceTypeObj = requestedResources.executorMemory\n+    }\n+\n+    val (standardResourceTypeId: String, customResourceTypeId: String) =\n+      getResourceTypeIdsByRole(role, mode, resourceType)\n+\n+    if (resourceTypeObj != null && customResourceTypes.contains(customResourceTypeId)) {\n+      return formatDuplicateResourceTypeErrorMessage(standardResourceTypeId, customResourceTypeId)\n+    }\n+    \"\"\n+  }\n+\n+  private def formatDuplicateResourceTypeErrorMessage(standardResourceTypeId: String,\n+                                                      customResourceTypeId: String): String = {\n+    s\"$standardResourceTypeId and $customResourceTypeId\" +\n+      \" configs are both present, only one of them is allowed at the same time!\"\n+  }\n+\n+  private def getResourceTypeIdsByRole(role: String, mode: String, resourceType: String) = {\n+    val standardResourceTypeId: String = s\"spark.$role.$resourceType\"\n+\n+    var customResourceTypeId: String = \"\"\n+    (role, mode) match {\n+      case (\"driver\", \"client\") => customResourceTypeId += \"spark.yarn.am.resource.\"\n+      case (\"driver\", \"cluster\") => customResourceTypeId += \"spark.yarn.driver.resource.\"\n+      case (\"executor\", _) => customResourceTypeId += \"spark.yarn.executor.resource.\"\n+    }\n+\n+    customResourceTypeId += resourceType\n+\n+    (standardResourceTypeId, customResourceTypeId)\n+  }\n+\n+  private class ResourceTypeConfigProperties(val role: String,\n+                                             val mode: String = \"\",\n+                                             val resourceType: String)\n+\n+\n+  private class RequestedResources(val sparkConf: SparkConf) {\n+    val driverMemory: String = safelyGetFromSparkConf(sparkConf, \"spark.driver.memory\")\n+    val driverCores: String = safelyGetFromSparkConf(sparkConf, \"spark.driver.cores\")\n+    val executorMemory: String = safelyGetFromSparkConf(sparkConf, \"spark.executor.memory\")\n+    val executorCores: String = safelyGetFromSparkConf(sparkConf, \"spark.executor.cores\")\n+    val customResourceTypesForDriverClientMode: Map[String, String] ="
  }, {
    "author": {
      "login": "szilard-nemeth"
    },
    "body": "Thanks for pointing this. Indeed, after I read your comment and the code / Spark docs again, my understanding was not entirely correct.\r\nI modified the code accordingly, please check again.",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-05-13T17:50:19Z",
    "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+\n+object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+\n+  def validateResourceTypes(sparkConf: SparkConf): Unit = {\n+    val requestedResources: RequestedResources = new RequestedResources(sparkConf)\n+\n+    validateDuplicateResourceConfig(requestedResources,\n+      Seq[ResourceTypeConfigProperties](\n+        new ResourceTypeConfigProperties(\"driver\", \"client\", \"memory\"),\n+        new ResourceTypeConfigProperties(\"driver\", \"client\", \"cores\"),\n+        new ResourceTypeConfigProperties(\"driver\", \"cluster\", \"memory\"),\n+        new ResourceTypeConfigProperties(\"driver\", \"cluster\", \"cores\"),\n+        new ResourceTypeConfigProperties(role = \"executor\", resourceType = \"memory\"),\n+        new ResourceTypeConfigProperties(role = \"executor\", resourceType = \"cores\")))\n+  }\n+\n+  private def validateDuplicateResourceConfig(requestedResources: RequestedResources,\n+                                              resourceTypeConfigProperties:\n+                                              Seq[ResourceTypeConfigProperties]): Unit = {\n+    val sb = new mutable.StringBuilder()\n+    resourceTypeConfigProperties\n+      .foreach(rtc => {\n+        val errorMessage = validateDuplicateResourceConfigInternal(requestedResources, rtc)\n+        if (errorMessage.nonEmpty) {\n+          printErrorMessageToBuffer(sb, errorMessage)\n+        }\n+      })\n+\n+    if (sb.nonEmpty) {\n+      throw new SparkException(sb.toString())\n+    }\n+  }\n+\n+  private[spark] def printErrorMessageToBuffer(sb: StringBuilder, str: String) = {\n+    sb.append(s\"$ERROR_PREFIX$str\\n\")\n+  }\n+\n+  private def validateDuplicateResourceConfigInternal(requestedResources: RequestedResources,\n+                                                      rtc: ResourceTypeConfigProperties): String = {\n+    val role = rtc.role\n+    val mode = rtc.mode\n+    val resourceType = rtc.resourceType\n+\n+    if (role != \"driver\" && role != \"executor\") {\n+      throw new IllegalArgumentException(\"Role must be either 'driver' or 'executor'!\")\n+    }\n+    if (mode != \"\" && mode != \"client\" && mode != \"cluster\") {\n+      throw new IllegalArgumentException(\"Mode must be either 'client' or 'cluster'!\")\n+    }\n+    if (resourceType != \"cores\" && resourceType != \"memory\") {\n+      throw new IllegalArgumentException(\"Resource type must be either 'cores' or 'memory'!\")\n+    }\n+\n+    var customResourceTypes: Map[String, String] = null\n+    (role, mode, resourceType) match {\n+      case (\"executor\", _, _) => customResourceTypes = requestedResources\n+        .customResourceTypesForExecutor\n+      case (\"driver\", \"client\", _) => customResourceTypes = requestedResources\n+        .customResourceTypesForDriverClientMode\n+      case (\"driver\", \"cluster\", _) => customResourceTypes = requestedResources\n+        .customResourceTypesForDriverClusterMode\n+    }\n+\n+    var resourceTypeObj: String = null\n+    (role, resourceType) match {\n+      case (\"driver\", \"cores\") => resourceTypeObj = requestedResources.driverCores\n+      case (\"driver\", \"memory\") => resourceTypeObj = requestedResources.driverMemory\n+      case (\"executor\", \"cores\") => resourceTypeObj = requestedResources.executorCores\n+      case (\"executor\", \"memory\") => resourceTypeObj = requestedResources.executorMemory\n+    }\n+\n+    val (standardResourceTypeId: String, customResourceTypeId: String) =\n+      getResourceTypeIdsByRole(role, mode, resourceType)\n+\n+    if (resourceTypeObj != null && customResourceTypes.contains(customResourceTypeId)) {\n+      return formatDuplicateResourceTypeErrorMessage(standardResourceTypeId, customResourceTypeId)\n+    }\n+    \"\"\n+  }\n+\n+  private def formatDuplicateResourceTypeErrorMessage(standardResourceTypeId: String,\n+                                                      customResourceTypeId: String): String = {\n+    s\"$standardResourceTypeId and $customResourceTypeId\" +\n+      \" configs are both present, only one of them is allowed at the same time!\"\n+  }\n+\n+  private def getResourceTypeIdsByRole(role: String, mode: String, resourceType: String) = {\n+    val standardResourceTypeId: String = s\"spark.$role.$resourceType\"\n+\n+    var customResourceTypeId: String = \"\"\n+    (role, mode) match {\n+      case (\"driver\", \"client\") => customResourceTypeId += \"spark.yarn.am.resource.\"\n+      case (\"driver\", \"cluster\") => customResourceTypeId += \"spark.yarn.driver.resource.\"\n+      case (\"executor\", _) => customResourceTypeId += \"spark.yarn.executor.resource.\"\n+    }\n+\n+    customResourceTypeId += resourceType\n+\n+    (standardResourceTypeId, customResourceTypeId)\n+  }\n+\n+  private class ResourceTypeConfigProperties(val role: String,\n+                                             val mode: String = \"\",\n+                                             val resourceType: String)\n+\n+\n+  private class RequestedResources(val sparkConf: SparkConf) {\n+    val driverMemory: String = safelyGetFromSparkConf(sparkConf, \"spark.driver.memory\")\n+    val driverCores: String = safelyGetFromSparkConf(sparkConf, \"spark.driver.cores\")\n+    val executorMemory: String = safelyGetFromSparkConf(sparkConf, \"spark.executor.memory\")\n+    val executorCores: String = safelyGetFromSparkConf(sparkConf, \"spark.executor.cores\")\n+    val customResourceTypesForDriverClientMode: Map[String, String] ="
  }],
  "prId": 20761
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`.foreach { case (k, v) => `",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-03-19T20:41:00Z",
    "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+\n+object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+\n+  def validateResourceTypes(sparkConf: SparkConf): Unit = {\n+    val requestedResources: RequestedResources = new RequestedResources(sparkConf)\n+\n+    validateDuplicateResourceConfig(requestedResources,\n+      Seq[ResourceTypeConfigProperties](\n+        new ResourceTypeConfigProperties(\"driver\", \"client\", \"memory\"),\n+        new ResourceTypeConfigProperties(\"driver\", \"client\", \"cores\"),\n+        new ResourceTypeConfigProperties(\"driver\", \"cluster\", \"memory\"),\n+        new ResourceTypeConfigProperties(\"driver\", \"cluster\", \"cores\"),\n+        new ResourceTypeConfigProperties(role = \"executor\", resourceType = \"memory\"),\n+        new ResourceTypeConfigProperties(role = \"executor\", resourceType = \"cores\")))\n+  }\n+\n+  private def validateDuplicateResourceConfig(requestedResources: RequestedResources,\n+                                              resourceTypeConfigProperties:\n+                                              Seq[ResourceTypeConfigProperties]): Unit = {\n+    val sb = new mutable.StringBuilder()\n+    resourceTypeConfigProperties\n+      .foreach(rtc => {\n+        val errorMessage = validateDuplicateResourceConfigInternal(requestedResources, rtc)\n+        if (errorMessage.nonEmpty) {\n+          printErrorMessageToBuffer(sb, errorMessage)\n+        }\n+      })\n+\n+    if (sb.nonEmpty) {\n+      throw new SparkException(sb.toString())\n+    }\n+  }\n+\n+  private[spark] def printErrorMessageToBuffer(sb: StringBuilder, str: String) = {\n+    sb.append(s\"$ERROR_PREFIX$str\\n\")\n+  }\n+\n+  private def validateDuplicateResourceConfigInternal(requestedResources: RequestedResources,\n+                                                      rtc: ResourceTypeConfigProperties): String = {\n+    val role = rtc.role\n+    val mode = rtc.mode\n+    val resourceType = rtc.resourceType\n+\n+    if (role != \"driver\" && role != \"executor\") {\n+      throw new IllegalArgumentException(\"Role must be either 'driver' or 'executor'!\")\n+    }\n+    if (mode != \"\" && mode != \"client\" && mode != \"cluster\") {\n+      throw new IllegalArgumentException(\"Mode must be either 'client' or 'cluster'!\")\n+    }\n+    if (resourceType != \"cores\" && resourceType != \"memory\") {\n+      throw new IllegalArgumentException(\"Resource type must be either 'cores' or 'memory'!\")\n+    }\n+\n+    var customResourceTypes: Map[String, String] = null\n+    (role, mode, resourceType) match {\n+      case (\"executor\", _, _) => customResourceTypes = requestedResources\n+        .customResourceTypesForExecutor\n+      case (\"driver\", \"client\", _) => customResourceTypes = requestedResources\n+        .customResourceTypesForDriverClientMode\n+      case (\"driver\", \"cluster\", _) => customResourceTypes = requestedResources\n+        .customResourceTypesForDriverClusterMode\n+    }\n+\n+    var resourceTypeObj: String = null\n+    (role, resourceType) match {\n+      case (\"driver\", \"cores\") => resourceTypeObj = requestedResources.driverCores\n+      case (\"driver\", \"memory\") => resourceTypeObj = requestedResources.driverMemory\n+      case (\"executor\", \"cores\") => resourceTypeObj = requestedResources.executorCores\n+      case (\"executor\", \"memory\") => resourceTypeObj = requestedResources.executorMemory\n+    }\n+\n+    val (standardResourceTypeId: String, customResourceTypeId: String) =\n+      getResourceTypeIdsByRole(role, mode, resourceType)\n+\n+    if (resourceTypeObj != null && customResourceTypes.contains(customResourceTypeId)) {\n+      return formatDuplicateResourceTypeErrorMessage(standardResourceTypeId, customResourceTypeId)\n+    }\n+    \"\"\n+  }\n+\n+  private def formatDuplicateResourceTypeErrorMessage(standardResourceTypeId: String,\n+                                                      customResourceTypeId: String): String = {\n+    s\"$standardResourceTypeId and $customResourceTypeId\" +\n+      \" configs are both present, only one of them is allowed at the same time!\"\n+  }\n+\n+  private def getResourceTypeIdsByRole(role: String, mode: String, resourceType: String) = {\n+    val standardResourceTypeId: String = s\"spark.$role.$resourceType\"\n+\n+    var customResourceTypeId: String = \"\"\n+    (role, mode) match {\n+      case (\"driver\", \"client\") => customResourceTypeId += \"spark.yarn.am.resource.\"\n+      case (\"driver\", \"cluster\") => customResourceTypeId += \"spark.yarn.driver.resource.\"\n+      case (\"executor\", _) => customResourceTypeId += \"spark.yarn.executor.resource.\"\n+    }\n+\n+    customResourceTypeId += resourceType\n+\n+    (standardResourceTypeId, customResourceTypeId)\n+  }\n+\n+  private class ResourceTypeConfigProperties(val role: String,\n+                                             val mode: String = \"\",\n+                                             val resourceType: String)\n+\n+\n+  private class RequestedResources(val sparkConf: SparkConf) {\n+    val driverMemory: String = safelyGetFromSparkConf(sparkConf, \"spark.driver.memory\")\n+    val driverCores: String = safelyGetFromSparkConf(sparkConf, \"spark.driver.cores\")\n+    val executorMemory: String = safelyGetFromSparkConf(sparkConf, \"spark.executor.memory\")\n+    val executorCores: String = safelyGetFromSparkConf(sparkConf, \"spark.executor.cores\")\n+    val customResourceTypesForDriverClientMode: Map[String, String] =\n+      extractCustomResourceTypes(sparkConf, \"spark.yarn.am.resource.\")\n+    val customResourceTypesForDriverClusterMode: Map[String, String] =\n+      extractCustomResourceTypes(sparkConf, \"spark.yarn.driver.resource.\")\n+    val customResourceTypesForExecutor: Map[String, String] =\n+      extractCustomResourceTypes(sparkConf, \"spark.yarn.executor.resource.\")\n+\n+    private def extractCustomResourceTypes(sparkConf: SparkConf,\n+                                           propertyPrefix: String): Map[String, String] = {\n+      val result: collection.mutable.HashMap[String, String] =\n+        new collection.mutable.HashMap[String, String]()\n+\n+      val propertiesWithPrefix: Array[(String, String)] = sparkConf.getAllWithPrefix(propertyPrefix)\n+      propertiesWithPrefix.foreach(e => result.put(propertyPrefix + e._1, e._2))"
  }, {
    "author": {
      "login": "szilard-nemeth"
    },
    "body": "fixed",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-05-13T17:50:27Z",
    "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+\n+object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+\n+  def validateResourceTypes(sparkConf: SparkConf): Unit = {\n+    val requestedResources: RequestedResources = new RequestedResources(sparkConf)\n+\n+    validateDuplicateResourceConfig(requestedResources,\n+      Seq[ResourceTypeConfigProperties](\n+        new ResourceTypeConfigProperties(\"driver\", \"client\", \"memory\"),\n+        new ResourceTypeConfigProperties(\"driver\", \"client\", \"cores\"),\n+        new ResourceTypeConfigProperties(\"driver\", \"cluster\", \"memory\"),\n+        new ResourceTypeConfigProperties(\"driver\", \"cluster\", \"cores\"),\n+        new ResourceTypeConfigProperties(role = \"executor\", resourceType = \"memory\"),\n+        new ResourceTypeConfigProperties(role = \"executor\", resourceType = \"cores\")))\n+  }\n+\n+  private def validateDuplicateResourceConfig(requestedResources: RequestedResources,\n+                                              resourceTypeConfigProperties:\n+                                              Seq[ResourceTypeConfigProperties]): Unit = {\n+    val sb = new mutable.StringBuilder()\n+    resourceTypeConfigProperties\n+      .foreach(rtc => {\n+        val errorMessage = validateDuplicateResourceConfigInternal(requestedResources, rtc)\n+        if (errorMessage.nonEmpty) {\n+          printErrorMessageToBuffer(sb, errorMessage)\n+        }\n+      })\n+\n+    if (sb.nonEmpty) {\n+      throw new SparkException(sb.toString())\n+    }\n+  }\n+\n+  private[spark] def printErrorMessageToBuffer(sb: StringBuilder, str: String) = {\n+    sb.append(s\"$ERROR_PREFIX$str\\n\")\n+  }\n+\n+  private def validateDuplicateResourceConfigInternal(requestedResources: RequestedResources,\n+                                                      rtc: ResourceTypeConfigProperties): String = {\n+    val role = rtc.role\n+    val mode = rtc.mode\n+    val resourceType = rtc.resourceType\n+\n+    if (role != \"driver\" && role != \"executor\") {\n+      throw new IllegalArgumentException(\"Role must be either 'driver' or 'executor'!\")\n+    }\n+    if (mode != \"\" && mode != \"client\" && mode != \"cluster\") {\n+      throw new IllegalArgumentException(\"Mode must be either 'client' or 'cluster'!\")\n+    }\n+    if (resourceType != \"cores\" && resourceType != \"memory\") {\n+      throw new IllegalArgumentException(\"Resource type must be either 'cores' or 'memory'!\")\n+    }\n+\n+    var customResourceTypes: Map[String, String] = null\n+    (role, mode, resourceType) match {\n+      case (\"executor\", _, _) => customResourceTypes = requestedResources\n+        .customResourceTypesForExecutor\n+      case (\"driver\", \"client\", _) => customResourceTypes = requestedResources\n+        .customResourceTypesForDriverClientMode\n+      case (\"driver\", \"cluster\", _) => customResourceTypes = requestedResources\n+        .customResourceTypesForDriverClusterMode\n+    }\n+\n+    var resourceTypeObj: String = null\n+    (role, resourceType) match {\n+      case (\"driver\", \"cores\") => resourceTypeObj = requestedResources.driverCores\n+      case (\"driver\", \"memory\") => resourceTypeObj = requestedResources.driverMemory\n+      case (\"executor\", \"cores\") => resourceTypeObj = requestedResources.executorCores\n+      case (\"executor\", \"memory\") => resourceTypeObj = requestedResources.executorMemory\n+    }\n+\n+    val (standardResourceTypeId: String, customResourceTypeId: String) =\n+      getResourceTypeIdsByRole(role, mode, resourceType)\n+\n+    if (resourceTypeObj != null && customResourceTypes.contains(customResourceTypeId)) {\n+      return formatDuplicateResourceTypeErrorMessage(standardResourceTypeId, customResourceTypeId)\n+    }\n+    \"\"\n+  }\n+\n+  private def formatDuplicateResourceTypeErrorMessage(standardResourceTypeId: String,\n+                                                      customResourceTypeId: String): String = {\n+    s\"$standardResourceTypeId and $customResourceTypeId\" +\n+      \" configs are both present, only one of them is allowed at the same time!\"\n+  }\n+\n+  private def getResourceTypeIdsByRole(role: String, mode: String, resourceType: String) = {\n+    val standardResourceTypeId: String = s\"spark.$role.$resourceType\"\n+\n+    var customResourceTypeId: String = \"\"\n+    (role, mode) match {\n+      case (\"driver\", \"client\") => customResourceTypeId += \"spark.yarn.am.resource.\"\n+      case (\"driver\", \"cluster\") => customResourceTypeId += \"spark.yarn.driver.resource.\"\n+      case (\"executor\", _) => customResourceTypeId += \"spark.yarn.executor.resource.\"\n+    }\n+\n+    customResourceTypeId += resourceType\n+\n+    (standardResourceTypeId, customResourceTypeId)\n+  }\n+\n+  private class ResourceTypeConfigProperties(val role: String,\n+                                             val mode: String = \"\",\n+                                             val resourceType: String)\n+\n+\n+  private class RequestedResources(val sparkConf: SparkConf) {\n+    val driverMemory: String = safelyGetFromSparkConf(sparkConf, \"spark.driver.memory\")\n+    val driverCores: String = safelyGetFromSparkConf(sparkConf, \"spark.driver.cores\")\n+    val executorMemory: String = safelyGetFromSparkConf(sparkConf, \"spark.executor.memory\")\n+    val executorCores: String = safelyGetFromSparkConf(sparkConf, \"spark.executor.cores\")\n+    val customResourceTypesForDriverClientMode: Map[String, String] =\n+      extractCustomResourceTypes(sparkConf, \"spark.yarn.am.resource.\")\n+    val customResourceTypesForDriverClusterMode: Map[String, String] =\n+      extractCustomResourceTypes(sparkConf, \"spark.yarn.driver.resource.\")\n+    val customResourceTypesForExecutor: Map[String, String] =\n+      extractCustomResourceTypes(sparkConf, \"spark.yarn.executor.resource.\")\n+\n+    private def extractCustomResourceTypes(sparkConf: SparkConf,\n+                                           propertyPrefix: String): Map[String, String] = {\n+      val result: collection.mutable.HashMap[String, String] =\n+        new collection.mutable.HashMap[String, String]()\n+\n+      val propertiesWithPrefix: Array[(String, String)] = sparkConf.getAllWithPrefix(propertyPrefix)\n+      propertiesWithPrefix.foreach(e => result.put(propertyPrefix + e._1, e._2))"
  }],
  "prId": 20761
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This is `conf.getOption(key).orNull`. It also does not check for default values. For example, the driver's memory has a config constant (`DRIVER_MEMORY`) which encapsulates its default value.",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-03-19T20:41:25Z",
    "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+\n+object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+\n+  def validateResourceTypes(sparkConf: SparkConf): Unit = {\n+    val requestedResources: RequestedResources = new RequestedResources(sparkConf)\n+\n+    validateDuplicateResourceConfig(requestedResources,\n+      Seq[ResourceTypeConfigProperties](\n+        new ResourceTypeConfigProperties(\"driver\", \"client\", \"memory\"),\n+        new ResourceTypeConfigProperties(\"driver\", \"client\", \"cores\"),\n+        new ResourceTypeConfigProperties(\"driver\", \"cluster\", \"memory\"),\n+        new ResourceTypeConfigProperties(\"driver\", \"cluster\", \"cores\"),\n+        new ResourceTypeConfigProperties(role = \"executor\", resourceType = \"memory\"),\n+        new ResourceTypeConfigProperties(role = \"executor\", resourceType = \"cores\")))\n+  }\n+\n+  private def validateDuplicateResourceConfig(requestedResources: RequestedResources,\n+                                              resourceTypeConfigProperties:\n+                                              Seq[ResourceTypeConfigProperties]): Unit = {\n+    val sb = new mutable.StringBuilder()\n+    resourceTypeConfigProperties\n+      .foreach(rtc => {\n+        val errorMessage = validateDuplicateResourceConfigInternal(requestedResources, rtc)\n+        if (errorMessage.nonEmpty) {\n+          printErrorMessageToBuffer(sb, errorMessage)\n+        }\n+      })\n+\n+    if (sb.nonEmpty) {\n+      throw new SparkException(sb.toString())\n+    }\n+  }\n+\n+  private[spark] def printErrorMessageToBuffer(sb: StringBuilder, str: String) = {\n+    sb.append(s\"$ERROR_PREFIX$str\\n\")\n+  }\n+\n+  private def validateDuplicateResourceConfigInternal(requestedResources: RequestedResources,\n+                                                      rtc: ResourceTypeConfigProperties): String = {\n+    val role = rtc.role\n+    val mode = rtc.mode\n+    val resourceType = rtc.resourceType\n+\n+    if (role != \"driver\" && role != \"executor\") {\n+      throw new IllegalArgumentException(\"Role must be either 'driver' or 'executor'!\")\n+    }\n+    if (mode != \"\" && mode != \"client\" && mode != \"cluster\") {\n+      throw new IllegalArgumentException(\"Mode must be either 'client' or 'cluster'!\")\n+    }\n+    if (resourceType != \"cores\" && resourceType != \"memory\") {\n+      throw new IllegalArgumentException(\"Resource type must be either 'cores' or 'memory'!\")\n+    }\n+\n+    var customResourceTypes: Map[String, String] = null\n+    (role, mode, resourceType) match {\n+      case (\"executor\", _, _) => customResourceTypes = requestedResources\n+        .customResourceTypesForExecutor\n+      case (\"driver\", \"client\", _) => customResourceTypes = requestedResources\n+        .customResourceTypesForDriverClientMode\n+      case (\"driver\", \"cluster\", _) => customResourceTypes = requestedResources\n+        .customResourceTypesForDriverClusterMode\n+    }\n+\n+    var resourceTypeObj: String = null\n+    (role, resourceType) match {\n+      case (\"driver\", \"cores\") => resourceTypeObj = requestedResources.driverCores\n+      case (\"driver\", \"memory\") => resourceTypeObj = requestedResources.driverMemory\n+      case (\"executor\", \"cores\") => resourceTypeObj = requestedResources.executorCores\n+      case (\"executor\", \"memory\") => resourceTypeObj = requestedResources.executorMemory\n+    }\n+\n+    val (standardResourceTypeId: String, customResourceTypeId: String) =\n+      getResourceTypeIdsByRole(role, mode, resourceType)\n+\n+    if (resourceTypeObj != null && customResourceTypes.contains(customResourceTypeId)) {\n+      return formatDuplicateResourceTypeErrorMessage(standardResourceTypeId, customResourceTypeId)\n+    }\n+    \"\"\n+  }\n+\n+  private def formatDuplicateResourceTypeErrorMessage(standardResourceTypeId: String,\n+                                                      customResourceTypeId: String): String = {\n+    s\"$standardResourceTypeId and $customResourceTypeId\" +\n+      \" configs are both present, only one of them is allowed at the same time!\"\n+  }\n+\n+  private def getResourceTypeIdsByRole(role: String, mode: String, resourceType: String) = {\n+    val standardResourceTypeId: String = s\"spark.$role.$resourceType\"\n+\n+    var customResourceTypeId: String = \"\"\n+    (role, mode) match {\n+      case (\"driver\", \"client\") => customResourceTypeId += \"spark.yarn.am.resource.\"\n+      case (\"driver\", \"cluster\") => customResourceTypeId += \"spark.yarn.driver.resource.\"\n+      case (\"executor\", _) => customResourceTypeId += \"spark.yarn.executor.resource.\"\n+    }\n+\n+    customResourceTypeId += resourceType\n+\n+    (standardResourceTypeId, customResourceTypeId)\n+  }\n+\n+  private class ResourceTypeConfigProperties(val role: String,\n+                                             val mode: String = \"\",\n+                                             val resourceType: String)\n+\n+\n+  private class RequestedResources(val sparkConf: SparkConf) {\n+    val driverMemory: String = safelyGetFromSparkConf(sparkConf, \"spark.driver.memory\")\n+    val driverCores: String = safelyGetFromSparkConf(sparkConf, \"spark.driver.cores\")\n+    val executorMemory: String = safelyGetFromSparkConf(sparkConf, \"spark.executor.memory\")\n+    val executorCores: String = safelyGetFromSparkConf(sparkConf, \"spark.executor.cores\")\n+    val customResourceTypesForDriverClientMode: Map[String, String] =\n+      extractCustomResourceTypes(sparkConf, \"spark.yarn.am.resource.\")\n+    val customResourceTypesForDriverClusterMode: Map[String, String] =\n+      extractCustomResourceTypes(sparkConf, \"spark.yarn.driver.resource.\")\n+    val customResourceTypesForExecutor: Map[String, String] =\n+      extractCustomResourceTypes(sparkConf, \"spark.yarn.executor.resource.\")\n+\n+    private def extractCustomResourceTypes(sparkConf: SparkConf,\n+                                           propertyPrefix: String): Map[String, String] = {\n+      val result: collection.mutable.HashMap[String, String] =\n+        new collection.mutable.HashMap[String, String]()\n+\n+      val propertiesWithPrefix: Array[(String, String)] = sparkConf.getAllWithPrefix(propertyPrefix)\n+      propertiesWithPrefix.foreach(e => result.put(propertyPrefix + e._1, e._2))\n+\n+      result.toMap\n+    }\n+\n+    private def safelyGetFromSparkConf(sparkConf: SparkConf, key: String): String = {"
  }, {
    "author": {
      "login": "szilard-nemeth"
    },
    "body": "You mean this one, right? org.apache.spark.internal.config#DRIVER_MEMORY\r\nSince I see no usage of this constant, neither EXECUTOR_MEMORY in code, Iâ€™m a bit confused.\r\nAnyway, I modified the code and started to use these constants for accessing the values, as you suggested.",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-05-13T17:50:48Z",
    "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+\n+object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+\n+  def validateResourceTypes(sparkConf: SparkConf): Unit = {\n+    val requestedResources: RequestedResources = new RequestedResources(sparkConf)\n+\n+    validateDuplicateResourceConfig(requestedResources,\n+      Seq[ResourceTypeConfigProperties](\n+        new ResourceTypeConfigProperties(\"driver\", \"client\", \"memory\"),\n+        new ResourceTypeConfigProperties(\"driver\", \"client\", \"cores\"),\n+        new ResourceTypeConfigProperties(\"driver\", \"cluster\", \"memory\"),\n+        new ResourceTypeConfigProperties(\"driver\", \"cluster\", \"cores\"),\n+        new ResourceTypeConfigProperties(role = \"executor\", resourceType = \"memory\"),\n+        new ResourceTypeConfigProperties(role = \"executor\", resourceType = \"cores\")))\n+  }\n+\n+  private def validateDuplicateResourceConfig(requestedResources: RequestedResources,\n+                                              resourceTypeConfigProperties:\n+                                              Seq[ResourceTypeConfigProperties]): Unit = {\n+    val sb = new mutable.StringBuilder()\n+    resourceTypeConfigProperties\n+      .foreach(rtc => {\n+        val errorMessage = validateDuplicateResourceConfigInternal(requestedResources, rtc)\n+        if (errorMessage.nonEmpty) {\n+          printErrorMessageToBuffer(sb, errorMessage)\n+        }\n+      })\n+\n+    if (sb.nonEmpty) {\n+      throw new SparkException(sb.toString())\n+    }\n+  }\n+\n+  private[spark] def printErrorMessageToBuffer(sb: StringBuilder, str: String) = {\n+    sb.append(s\"$ERROR_PREFIX$str\\n\")\n+  }\n+\n+  private def validateDuplicateResourceConfigInternal(requestedResources: RequestedResources,\n+                                                      rtc: ResourceTypeConfigProperties): String = {\n+    val role = rtc.role\n+    val mode = rtc.mode\n+    val resourceType = rtc.resourceType\n+\n+    if (role != \"driver\" && role != \"executor\") {\n+      throw new IllegalArgumentException(\"Role must be either 'driver' or 'executor'!\")\n+    }\n+    if (mode != \"\" && mode != \"client\" && mode != \"cluster\") {\n+      throw new IllegalArgumentException(\"Mode must be either 'client' or 'cluster'!\")\n+    }\n+    if (resourceType != \"cores\" && resourceType != \"memory\") {\n+      throw new IllegalArgumentException(\"Resource type must be either 'cores' or 'memory'!\")\n+    }\n+\n+    var customResourceTypes: Map[String, String] = null\n+    (role, mode, resourceType) match {\n+      case (\"executor\", _, _) => customResourceTypes = requestedResources\n+        .customResourceTypesForExecutor\n+      case (\"driver\", \"client\", _) => customResourceTypes = requestedResources\n+        .customResourceTypesForDriverClientMode\n+      case (\"driver\", \"cluster\", _) => customResourceTypes = requestedResources\n+        .customResourceTypesForDriverClusterMode\n+    }\n+\n+    var resourceTypeObj: String = null\n+    (role, resourceType) match {\n+      case (\"driver\", \"cores\") => resourceTypeObj = requestedResources.driverCores\n+      case (\"driver\", \"memory\") => resourceTypeObj = requestedResources.driverMemory\n+      case (\"executor\", \"cores\") => resourceTypeObj = requestedResources.executorCores\n+      case (\"executor\", \"memory\") => resourceTypeObj = requestedResources.executorMemory\n+    }\n+\n+    val (standardResourceTypeId: String, customResourceTypeId: String) =\n+      getResourceTypeIdsByRole(role, mode, resourceType)\n+\n+    if (resourceTypeObj != null && customResourceTypes.contains(customResourceTypeId)) {\n+      return formatDuplicateResourceTypeErrorMessage(standardResourceTypeId, customResourceTypeId)\n+    }\n+    \"\"\n+  }\n+\n+  private def formatDuplicateResourceTypeErrorMessage(standardResourceTypeId: String,\n+                                                      customResourceTypeId: String): String = {\n+    s\"$standardResourceTypeId and $customResourceTypeId\" +\n+      \" configs are both present, only one of them is allowed at the same time!\"\n+  }\n+\n+  private def getResourceTypeIdsByRole(role: String, mode: String, resourceType: String) = {\n+    val standardResourceTypeId: String = s\"spark.$role.$resourceType\"\n+\n+    var customResourceTypeId: String = \"\"\n+    (role, mode) match {\n+      case (\"driver\", \"client\") => customResourceTypeId += \"spark.yarn.am.resource.\"\n+      case (\"driver\", \"cluster\") => customResourceTypeId += \"spark.yarn.driver.resource.\"\n+      case (\"executor\", _) => customResourceTypeId += \"spark.yarn.executor.resource.\"\n+    }\n+\n+    customResourceTypeId += resourceType\n+\n+    (standardResourceTypeId, customResourceTypeId)\n+  }\n+\n+  private class ResourceTypeConfigProperties(val role: String,\n+                                             val mode: String = \"\",\n+                                             val resourceType: String)\n+\n+\n+  private class RequestedResources(val sparkConf: SparkConf) {\n+    val driverMemory: String = safelyGetFromSparkConf(sparkConf, \"spark.driver.memory\")\n+    val driverCores: String = safelyGetFromSparkConf(sparkConf, \"spark.driver.cores\")\n+    val executorMemory: String = safelyGetFromSparkConf(sparkConf, \"spark.executor.memory\")\n+    val executorCores: String = safelyGetFromSparkConf(sparkConf, \"spark.executor.cores\")\n+    val customResourceTypesForDriverClientMode: Map[String, String] =\n+      extractCustomResourceTypes(sparkConf, \"spark.yarn.am.resource.\")\n+    val customResourceTypesForDriverClusterMode: Map[String, String] =\n+      extractCustomResourceTypes(sparkConf, \"spark.yarn.driver.resource.\")\n+    val customResourceTypesForExecutor: Map[String, String] =\n+      extractCustomResourceTypes(sparkConf, \"spark.yarn.executor.resource.\")\n+\n+    private def extractCustomResourceTypes(sparkConf: SparkConf,\n+                                           propertyPrefix: String): Map[String, String] = {\n+      val result: collection.mutable.HashMap[String, String] =\n+        new collection.mutable.HashMap[String, String]()\n+\n+      val propertiesWithPrefix: Array[(String, String)] = sparkConf.getAllWithPrefix(propertyPrefix)\n+      propertiesWithPrefix.foreach(e => result.put(propertyPrefix + e._1, e._2))\n+\n+      result.toMap\n+    }\n+\n+    private def safelyGetFromSparkConf(sparkConf: SparkConf, key: String): String = {"
  }],
  "prId": 20761
}, {
  "comments": [{
    "author": {
      "login": "galv"
    },
    "body": "In my opinion, it's better not to use a default argument here. You construct only 6 instances of this anyway. Also consider making the type of runMode an Option[RunMode], so you can use None instead of null. It is my understanding that null is very unidiomatic in scala.\r\n\r\nBy the way,this is a good candidate for a scala case class, but it doesn't really matter.",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-05-23T02:16:57Z",
    "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.ProcessType.{am, driver, executor, ProcessType}\n+import org.apache.spark.deploy.yarn.ResourceType.{cores, memory, ResourceType}\n+import org.apache.spark.deploy.yarn.RunMode.{client, cluster, RunMode}\n+import org.apache.spark.deploy.yarn.config._\n+import org.apache.spark.internal.config._\n+\n+private[spark] object ProcessType extends Enumeration {\n+  type ProcessType = Value\n+  val driver, executor, am = Value\n+}\n+\n+private[spark] object RunMode extends Enumeration {\n+  type RunMode = Value\n+  val client, cluster = Value\n+}\n+\n+private[spark] object ResourceType extends Enumeration {\n+  type ResourceType = Value\n+  val cores, memory = Value\n+}\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[ResourceConfigProperties](\n+    new ResourceConfigProperties(am, client, memory),\n+    new ResourceConfigProperties(am, client, cores),\n+    new ResourceConfigProperties(driver, cluster, memory),\n+    new ResourceConfigProperties(driver, cluster, cores),\n+    new ResourceConfigProperties(processType = executor, resourceType = memory),\n+    new ResourceConfigProperties(processType = executor, resourceType = cores))\n+\n+  /**\n+   * Validates sparkConf and throws a SparkException if a standard resource (memory or cores)\n+   * is defined with the property spark.yarn.x.resource.y<br>\n+   *\n+   * Example of an invalid config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   *\n+   * Please note that if multiple resources are defined like described above,\n+   * the error messages will be concatenated.<br>\n+   * Example of such a config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   * - spark.yarn.executor.resource.cores=2<br>\n+   * Then the following two error messages will be printed:<br>\n+   * - \"memory cannot be requested with config spark.yarn.driver.resource.memory,\n+   * please use config spark.driver.memory instead!<br>\n+   * - \"cores cannot be requested with config spark.yarn.executor.resource.cores,\n+   * please use config spark.executor.cores instead!<br>\n+   *\n+   * @param sparkConf\n+   */\n+  def validateResources(sparkConf: SparkConf): Unit = {\n+    val requestedResources = new RequestedResources(sparkConf)\n+    val sb = new mutable.StringBuilder()\n+    POSSIBLE_RESOURCE_DEFINITIONS.foreach { rcp =>\n+      val customResources: Map[String, String] = getCustomResourceValue(requestedResources, rcp)\n+      val (standardResourceConfigKey: String, customResourceConfigKey: String) =\n+        getResourceConfigKeys(rcp)\n+\n+      val errorMessage =\n+        if (customResources.contains(customResourceConfigKey)) {\n+          s\"${rcp.resourceType} cannot be requested with config $customResourceConfigKey, \" +\n+              s\"please use config $standardResourceConfigKey instead!\"\n+        } else {\n+          \"\"\n+        }\n+      if (errorMessage.nonEmpty) {\n+        printErrorMessageToBuffer(sb, errorMessage)\n+      }\n+    }\n+\n+    if (sb.nonEmpty) {\n+      throw new SparkException(sb.toString())\n+    }\n+  }\n+\n+  /**\n+   * Returns the requested map of custom resources,\n+   * based on the ResourceConfigProperties argument.\n+   * @return\n+   */\n+  private def getCustomResourceValue(\n+      requestedResources: RequestedResources,\n+      rcp: ResourceConfigProperties) = {\n+    var customResources: Map[String, String] = null\n+    (rcp.processType, rcp.runMode, rcp.resourceType) match {\n+      case (ProcessType.executor, _, _) => customResources =\n+          requestedResources.customExecutorResources\n+      case (ProcessType.am, RunMode.client, _) => customResources =\n+          requestedResources.customAMResources\n+      case (ProcessType.driver, RunMode.cluster, _) => customResources =\n+          requestedResources.customDriverResources\n+    }\n+    customResources\n+  }\n+\n+  /**\n+   * Returns a tuple of standard resource config key and custom resource config key, based on the\n+   * processType and runMode fields of the ResourceConfigProperties argument.\n+   * Standard resources are memory and cores.\n+   *\n+   * @param rcp\n+   * @return\n+   */\n+  private def getResourceConfigKeys(rcp: ResourceConfigProperties): (String, String) = {\n+    val standardResourceConfigKey: String = if (rcp.processType == ProcessType.am) {\n+      s\"spark.yarn.${rcp.processType}.${rcp.resourceType}\"\n+    } else {\n+      s\"spark.${rcp.processType}.${rcp.resourceType}\"\n+    }\n+\n+    var customResourceTypeConfigKey: String = \"\"\n+    (rcp.processType, rcp.runMode) match {\n+      case (ProcessType.am, RunMode.client) =>\n+        customResourceTypeConfigKey += YARN_AM_RESOURCE_TYPES_PREFIX\n+      case (ProcessType.driver, RunMode.cluster) =>\n+        customResourceTypeConfigKey += YARN_DRIVER_RESOURCE_TYPES_PREFIX\n+      case (ProcessType.executor, _) =>\n+        customResourceTypeConfigKey += YARN_EXECUTOR_RESOURCE_TYPES_PREFIX\n+    }\n+\n+    customResourceTypeConfigKey += rcp.resourceType\n+\n+    (standardResourceConfigKey, customResourceTypeConfigKey)\n+  }\n+\n+  private[spark] def printErrorMessageToBuffer(sb: StringBuilder, str: String): Unit = {\n+    sb.append(s\"$ERROR_PREFIX$str\\n\")\n+  }\n+\n+  private class ResourceConfigProperties(\n+      val processType: ProcessType,\n+      val runMode: RunMode = null,"
  }, {
    "author": {
      "login": "szilard-nemeth"
    },
    "body": "Modified runMode to be an Option and passed None instead of using default arguments.",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-07-19T16:33:56Z",
    "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.ProcessType.{am, driver, executor, ProcessType}\n+import org.apache.spark.deploy.yarn.ResourceType.{cores, memory, ResourceType}\n+import org.apache.spark.deploy.yarn.RunMode.{client, cluster, RunMode}\n+import org.apache.spark.deploy.yarn.config._\n+import org.apache.spark.internal.config._\n+\n+private[spark] object ProcessType extends Enumeration {\n+  type ProcessType = Value\n+  val driver, executor, am = Value\n+}\n+\n+private[spark] object RunMode extends Enumeration {\n+  type RunMode = Value\n+  val client, cluster = Value\n+}\n+\n+private[spark] object ResourceType extends Enumeration {\n+  type ResourceType = Value\n+  val cores, memory = Value\n+}\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[ResourceConfigProperties](\n+    new ResourceConfigProperties(am, client, memory),\n+    new ResourceConfigProperties(am, client, cores),\n+    new ResourceConfigProperties(driver, cluster, memory),\n+    new ResourceConfigProperties(driver, cluster, cores),\n+    new ResourceConfigProperties(processType = executor, resourceType = memory),\n+    new ResourceConfigProperties(processType = executor, resourceType = cores))\n+\n+  /**\n+   * Validates sparkConf and throws a SparkException if a standard resource (memory or cores)\n+   * is defined with the property spark.yarn.x.resource.y<br>\n+   *\n+   * Example of an invalid config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   *\n+   * Please note that if multiple resources are defined like described above,\n+   * the error messages will be concatenated.<br>\n+   * Example of such a config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   * - spark.yarn.executor.resource.cores=2<br>\n+   * Then the following two error messages will be printed:<br>\n+   * - \"memory cannot be requested with config spark.yarn.driver.resource.memory,\n+   * please use config spark.driver.memory instead!<br>\n+   * - \"cores cannot be requested with config spark.yarn.executor.resource.cores,\n+   * please use config spark.executor.cores instead!<br>\n+   *\n+   * @param sparkConf\n+   */\n+  def validateResources(sparkConf: SparkConf): Unit = {\n+    val requestedResources = new RequestedResources(sparkConf)\n+    val sb = new mutable.StringBuilder()\n+    POSSIBLE_RESOURCE_DEFINITIONS.foreach { rcp =>\n+      val customResources: Map[String, String] = getCustomResourceValue(requestedResources, rcp)\n+      val (standardResourceConfigKey: String, customResourceConfigKey: String) =\n+        getResourceConfigKeys(rcp)\n+\n+      val errorMessage =\n+        if (customResources.contains(customResourceConfigKey)) {\n+          s\"${rcp.resourceType} cannot be requested with config $customResourceConfigKey, \" +\n+              s\"please use config $standardResourceConfigKey instead!\"\n+        } else {\n+          \"\"\n+        }\n+      if (errorMessage.nonEmpty) {\n+        printErrorMessageToBuffer(sb, errorMessage)\n+      }\n+    }\n+\n+    if (sb.nonEmpty) {\n+      throw new SparkException(sb.toString())\n+    }\n+  }\n+\n+  /**\n+   * Returns the requested map of custom resources,\n+   * based on the ResourceConfigProperties argument.\n+   * @return\n+   */\n+  private def getCustomResourceValue(\n+      requestedResources: RequestedResources,\n+      rcp: ResourceConfigProperties) = {\n+    var customResources: Map[String, String] = null\n+    (rcp.processType, rcp.runMode, rcp.resourceType) match {\n+      case (ProcessType.executor, _, _) => customResources =\n+          requestedResources.customExecutorResources\n+      case (ProcessType.am, RunMode.client, _) => customResources =\n+          requestedResources.customAMResources\n+      case (ProcessType.driver, RunMode.cluster, _) => customResources =\n+          requestedResources.customDriverResources\n+    }\n+    customResources\n+  }\n+\n+  /**\n+   * Returns a tuple of standard resource config key and custom resource config key, based on the\n+   * processType and runMode fields of the ResourceConfigProperties argument.\n+   * Standard resources are memory and cores.\n+   *\n+   * @param rcp\n+   * @return\n+   */\n+  private def getResourceConfigKeys(rcp: ResourceConfigProperties): (String, String) = {\n+    val standardResourceConfigKey: String = if (rcp.processType == ProcessType.am) {\n+      s\"spark.yarn.${rcp.processType}.${rcp.resourceType}\"\n+    } else {\n+      s\"spark.${rcp.processType}.${rcp.resourceType}\"\n+    }\n+\n+    var customResourceTypeConfigKey: String = \"\"\n+    (rcp.processType, rcp.runMode) match {\n+      case (ProcessType.am, RunMode.client) =>\n+        customResourceTypeConfigKey += YARN_AM_RESOURCE_TYPES_PREFIX\n+      case (ProcessType.driver, RunMode.cluster) =>\n+        customResourceTypeConfigKey += YARN_DRIVER_RESOURCE_TYPES_PREFIX\n+      case (ProcessType.executor, _) =>\n+        customResourceTypeConfigKey += YARN_EXECUTOR_RESOURCE_TYPES_PREFIX\n+    }\n+\n+    customResourceTypeConfigKey += rcp.resourceType\n+\n+    (standardResourceConfigKey, customResourceTypeConfigKey)\n+  }\n+\n+  private[spark] def printErrorMessageToBuffer(sb: StringBuilder, str: String): Unit = {\n+    sb.append(s\"$ERROR_PREFIX$str\\n\")\n+  }\n+\n+  private class ResourceConfigProperties(\n+      val processType: ProcessType,\n+      val runMode: RunMode = null,"
  }],
  "prId": 20761
}, {
  "comments": [{
    "author": {
      "login": "galv"
    },
    "body": "Why is this `private[spark]` vs `private`?",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-05-23T02:17:56Z",
    "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.ProcessType.{am, driver, executor, ProcessType}\n+import org.apache.spark.deploy.yarn.ResourceType.{cores, memory, ResourceType}\n+import org.apache.spark.deploy.yarn.RunMode.{client, cluster, RunMode}\n+import org.apache.spark.deploy.yarn.config._\n+import org.apache.spark.internal.config._\n+\n+private[spark] object ProcessType extends Enumeration {\n+  type ProcessType = Value\n+  val driver, executor, am = Value\n+}\n+\n+private[spark] object RunMode extends Enumeration {\n+  type RunMode = Value\n+  val client, cluster = Value\n+}\n+\n+private[spark] object ResourceType extends Enumeration {\n+  type ResourceType = Value\n+  val cores, memory = Value\n+}\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[ResourceConfigProperties](\n+    new ResourceConfigProperties(am, client, memory),\n+    new ResourceConfigProperties(am, client, cores),\n+    new ResourceConfigProperties(driver, cluster, memory),\n+    new ResourceConfigProperties(driver, cluster, cores),\n+    new ResourceConfigProperties(processType = executor, resourceType = memory),\n+    new ResourceConfigProperties(processType = executor, resourceType = cores))\n+\n+  /**\n+   * Validates sparkConf and throws a SparkException if a standard resource (memory or cores)\n+   * is defined with the property spark.yarn.x.resource.y<br>\n+   *\n+   * Example of an invalid config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   *\n+   * Please note that if multiple resources are defined like described above,\n+   * the error messages will be concatenated.<br>\n+   * Example of such a config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   * - spark.yarn.executor.resource.cores=2<br>\n+   * Then the following two error messages will be printed:<br>\n+   * - \"memory cannot be requested with config spark.yarn.driver.resource.memory,\n+   * please use config spark.driver.memory instead!<br>\n+   * - \"cores cannot be requested with config spark.yarn.executor.resource.cores,\n+   * please use config spark.executor.cores instead!<br>\n+   *\n+   * @param sparkConf\n+   */\n+  def validateResources(sparkConf: SparkConf): Unit = {\n+    val requestedResources = new RequestedResources(sparkConf)\n+    val sb = new mutable.StringBuilder()\n+    POSSIBLE_RESOURCE_DEFINITIONS.foreach { rcp =>\n+      val customResources: Map[String, String] = getCustomResourceValue(requestedResources, rcp)\n+      val (standardResourceConfigKey: String, customResourceConfigKey: String) =\n+        getResourceConfigKeys(rcp)\n+\n+      val errorMessage =\n+        if (customResources.contains(customResourceConfigKey)) {\n+          s\"${rcp.resourceType} cannot be requested with config $customResourceConfigKey, \" +\n+              s\"please use config $standardResourceConfigKey instead!\"\n+        } else {\n+          \"\"\n+        }\n+      if (errorMessage.nonEmpty) {\n+        printErrorMessageToBuffer(sb, errorMessage)\n+      }\n+    }\n+\n+    if (sb.nonEmpty) {\n+      throw new SparkException(sb.toString())\n+    }\n+  }\n+\n+  /**\n+   * Returns the requested map of custom resources,\n+   * based on the ResourceConfigProperties argument.\n+   * @return\n+   */\n+  private def getCustomResourceValue(\n+      requestedResources: RequestedResources,\n+      rcp: ResourceConfigProperties) = {\n+    var customResources: Map[String, String] = null\n+    (rcp.processType, rcp.runMode, rcp.resourceType) match {\n+      case (ProcessType.executor, _, _) => customResources =\n+          requestedResources.customExecutorResources\n+      case (ProcessType.am, RunMode.client, _) => customResources =\n+          requestedResources.customAMResources\n+      case (ProcessType.driver, RunMode.cluster, _) => customResources =\n+          requestedResources.customDriverResources\n+    }\n+    customResources\n+  }\n+\n+  /**\n+   * Returns a tuple of standard resource config key and custom resource config key, based on the\n+   * processType and runMode fields of the ResourceConfigProperties argument.\n+   * Standard resources are memory and cores.\n+   *\n+   * @param rcp\n+   * @return\n+   */\n+  private def getResourceConfigKeys(rcp: ResourceConfigProperties): (String, String) = {\n+    val standardResourceConfigKey: String = if (rcp.processType == ProcessType.am) {\n+      s\"spark.yarn.${rcp.processType}.${rcp.resourceType}\"\n+    } else {\n+      s\"spark.${rcp.processType}.${rcp.resourceType}\"\n+    }\n+\n+    var customResourceTypeConfigKey: String = \"\"\n+    (rcp.processType, rcp.runMode) match {\n+      case (ProcessType.am, RunMode.client) =>\n+        customResourceTypeConfigKey += YARN_AM_RESOURCE_TYPES_PREFIX\n+      case (ProcessType.driver, RunMode.cluster) =>\n+        customResourceTypeConfigKey += YARN_DRIVER_RESOURCE_TYPES_PREFIX\n+      case (ProcessType.executor, _) =>\n+        customResourceTypeConfigKey += YARN_EXECUTOR_RESOURCE_TYPES_PREFIX\n+    }\n+\n+    customResourceTypeConfigKey += rcp.resourceType\n+\n+    (standardResourceConfigKey, customResourceTypeConfigKey)\n+  }\n+\n+  private[spark] def printErrorMessageToBuffer(sb: StringBuilder, str: String): Unit = {"
  }, {
    "author": {
      "login": "szilard-nemeth"
    },
    "body": "Fixed",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-07-19T16:34:07Z",
    "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.ProcessType.{am, driver, executor, ProcessType}\n+import org.apache.spark.deploy.yarn.ResourceType.{cores, memory, ResourceType}\n+import org.apache.spark.deploy.yarn.RunMode.{client, cluster, RunMode}\n+import org.apache.spark.deploy.yarn.config._\n+import org.apache.spark.internal.config._\n+\n+private[spark] object ProcessType extends Enumeration {\n+  type ProcessType = Value\n+  val driver, executor, am = Value\n+}\n+\n+private[spark] object RunMode extends Enumeration {\n+  type RunMode = Value\n+  val client, cluster = Value\n+}\n+\n+private[spark] object ResourceType extends Enumeration {\n+  type ResourceType = Value\n+  val cores, memory = Value\n+}\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[ResourceConfigProperties](\n+    new ResourceConfigProperties(am, client, memory),\n+    new ResourceConfigProperties(am, client, cores),\n+    new ResourceConfigProperties(driver, cluster, memory),\n+    new ResourceConfigProperties(driver, cluster, cores),\n+    new ResourceConfigProperties(processType = executor, resourceType = memory),\n+    new ResourceConfigProperties(processType = executor, resourceType = cores))\n+\n+  /**\n+   * Validates sparkConf and throws a SparkException if a standard resource (memory or cores)\n+   * is defined with the property spark.yarn.x.resource.y<br>\n+   *\n+   * Example of an invalid config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   *\n+   * Please note that if multiple resources are defined like described above,\n+   * the error messages will be concatenated.<br>\n+   * Example of such a config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   * - spark.yarn.executor.resource.cores=2<br>\n+   * Then the following two error messages will be printed:<br>\n+   * - \"memory cannot be requested with config spark.yarn.driver.resource.memory,\n+   * please use config spark.driver.memory instead!<br>\n+   * - \"cores cannot be requested with config spark.yarn.executor.resource.cores,\n+   * please use config spark.executor.cores instead!<br>\n+   *\n+   * @param sparkConf\n+   */\n+  def validateResources(sparkConf: SparkConf): Unit = {\n+    val requestedResources = new RequestedResources(sparkConf)\n+    val sb = new mutable.StringBuilder()\n+    POSSIBLE_RESOURCE_DEFINITIONS.foreach { rcp =>\n+      val customResources: Map[String, String] = getCustomResourceValue(requestedResources, rcp)\n+      val (standardResourceConfigKey: String, customResourceConfigKey: String) =\n+        getResourceConfigKeys(rcp)\n+\n+      val errorMessage =\n+        if (customResources.contains(customResourceConfigKey)) {\n+          s\"${rcp.resourceType} cannot be requested with config $customResourceConfigKey, \" +\n+              s\"please use config $standardResourceConfigKey instead!\"\n+        } else {\n+          \"\"\n+        }\n+      if (errorMessage.nonEmpty) {\n+        printErrorMessageToBuffer(sb, errorMessage)\n+      }\n+    }\n+\n+    if (sb.nonEmpty) {\n+      throw new SparkException(sb.toString())\n+    }\n+  }\n+\n+  /**\n+   * Returns the requested map of custom resources,\n+   * based on the ResourceConfigProperties argument.\n+   * @return\n+   */\n+  private def getCustomResourceValue(\n+      requestedResources: RequestedResources,\n+      rcp: ResourceConfigProperties) = {\n+    var customResources: Map[String, String] = null\n+    (rcp.processType, rcp.runMode, rcp.resourceType) match {\n+      case (ProcessType.executor, _, _) => customResources =\n+          requestedResources.customExecutorResources\n+      case (ProcessType.am, RunMode.client, _) => customResources =\n+          requestedResources.customAMResources\n+      case (ProcessType.driver, RunMode.cluster, _) => customResources =\n+          requestedResources.customDriverResources\n+    }\n+    customResources\n+  }\n+\n+  /**\n+   * Returns a tuple of standard resource config key and custom resource config key, based on the\n+   * processType and runMode fields of the ResourceConfigProperties argument.\n+   * Standard resources are memory and cores.\n+   *\n+   * @param rcp\n+   * @return\n+   */\n+  private def getResourceConfigKeys(rcp: ResourceConfigProperties): (String, String) = {\n+    val standardResourceConfigKey: String = if (rcp.processType == ProcessType.am) {\n+      s\"spark.yarn.${rcp.processType}.${rcp.resourceType}\"\n+    } else {\n+      s\"spark.${rcp.processType}.${rcp.resourceType}\"\n+    }\n+\n+    var customResourceTypeConfigKey: String = \"\"\n+    (rcp.processType, rcp.runMode) match {\n+      case (ProcessType.am, RunMode.client) =>\n+        customResourceTypeConfigKey += YARN_AM_RESOURCE_TYPES_PREFIX\n+      case (ProcessType.driver, RunMode.cluster) =>\n+        customResourceTypeConfigKey += YARN_DRIVER_RESOURCE_TYPES_PREFIX\n+      case (ProcessType.executor, _) =>\n+        customResourceTypeConfigKey += YARN_EXECUTOR_RESOURCE_TYPES_PREFIX\n+    }\n+\n+    customResourceTypeConfigKey += rcp.resourceType\n+\n+    (standardResourceConfigKey, customResourceTypeConfigKey)\n+  }\n+\n+  private[spark] def printErrorMessageToBuffer(sb: StringBuilder, str: String): Unit = {"
  }],
  "prId": 20761
}, {
  "comments": [{
    "author": {
      "login": "galv"
    },
    "body": "Return type would be helpful here.",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-05-23T02:25:11Z",
    "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.ProcessType.{am, driver, executor, ProcessType}\n+import org.apache.spark.deploy.yarn.ResourceType.{cores, memory, ResourceType}\n+import org.apache.spark.deploy.yarn.RunMode.{client, cluster, RunMode}\n+import org.apache.spark.deploy.yarn.config._\n+import org.apache.spark.internal.config._\n+\n+private[spark] object ProcessType extends Enumeration {\n+  type ProcessType = Value\n+  val driver, executor, am = Value\n+}\n+\n+private[spark] object RunMode extends Enumeration {\n+  type RunMode = Value\n+  val client, cluster = Value\n+}\n+\n+private[spark] object ResourceType extends Enumeration {\n+  type ResourceType = Value\n+  val cores, memory = Value\n+}\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[ResourceConfigProperties](\n+    new ResourceConfigProperties(am, client, memory),\n+    new ResourceConfigProperties(am, client, cores),\n+    new ResourceConfigProperties(driver, cluster, memory),\n+    new ResourceConfigProperties(driver, cluster, cores),\n+    new ResourceConfigProperties(processType = executor, resourceType = memory),\n+    new ResourceConfigProperties(processType = executor, resourceType = cores))\n+\n+  /**\n+   * Validates sparkConf and throws a SparkException if a standard resource (memory or cores)\n+   * is defined with the property spark.yarn.x.resource.y<br>\n+   *\n+   * Example of an invalid config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   *\n+   * Please note that if multiple resources are defined like described above,\n+   * the error messages will be concatenated.<br>\n+   * Example of such a config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   * - spark.yarn.executor.resource.cores=2<br>\n+   * Then the following two error messages will be printed:<br>\n+   * - \"memory cannot be requested with config spark.yarn.driver.resource.memory,\n+   * please use config spark.driver.memory instead!<br>\n+   * - \"cores cannot be requested with config spark.yarn.executor.resource.cores,\n+   * please use config spark.executor.cores instead!<br>\n+   *\n+   * @param sparkConf\n+   */\n+  def validateResources(sparkConf: SparkConf): Unit = {\n+    val requestedResources = new RequestedResources(sparkConf)\n+    val sb = new mutable.StringBuilder()\n+    POSSIBLE_RESOURCE_DEFINITIONS.foreach { rcp =>\n+      val customResources: Map[String, String] = getCustomResourceValue(requestedResources, rcp)\n+      val (standardResourceConfigKey: String, customResourceConfigKey: String) =\n+        getResourceConfigKeys(rcp)\n+\n+      val errorMessage =\n+        if (customResources.contains(customResourceConfigKey)) {\n+          s\"${rcp.resourceType} cannot be requested with config $customResourceConfigKey, \" +\n+              s\"please use config $standardResourceConfigKey instead!\"\n+        } else {\n+          \"\"\n+        }\n+      if (errorMessage.nonEmpty) {\n+        printErrorMessageToBuffer(sb, errorMessage)\n+      }\n+    }\n+\n+    if (sb.nonEmpty) {\n+      throw new SparkException(sb.toString())\n+    }\n+  }\n+\n+  /**\n+   * Returns the requested map of custom resources,\n+   * based on the ResourceConfigProperties argument.\n+   * @return\n+   */\n+  private def getCustomResourceValue("
  }, {
    "author": {
      "login": "szilard-nemeth"
    },
    "body": "Added Map[String, String] as return type",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-07-19T16:27:13Z",
    "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.ProcessType.{am, driver, executor, ProcessType}\n+import org.apache.spark.deploy.yarn.ResourceType.{cores, memory, ResourceType}\n+import org.apache.spark.deploy.yarn.RunMode.{client, cluster, RunMode}\n+import org.apache.spark.deploy.yarn.config._\n+import org.apache.spark.internal.config._\n+\n+private[spark] object ProcessType extends Enumeration {\n+  type ProcessType = Value\n+  val driver, executor, am = Value\n+}\n+\n+private[spark] object RunMode extends Enumeration {\n+  type RunMode = Value\n+  val client, cluster = Value\n+}\n+\n+private[spark] object ResourceType extends Enumeration {\n+  type ResourceType = Value\n+  val cores, memory = Value\n+}\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[ResourceConfigProperties](\n+    new ResourceConfigProperties(am, client, memory),\n+    new ResourceConfigProperties(am, client, cores),\n+    new ResourceConfigProperties(driver, cluster, memory),\n+    new ResourceConfigProperties(driver, cluster, cores),\n+    new ResourceConfigProperties(processType = executor, resourceType = memory),\n+    new ResourceConfigProperties(processType = executor, resourceType = cores))\n+\n+  /**\n+   * Validates sparkConf and throws a SparkException if a standard resource (memory or cores)\n+   * is defined with the property spark.yarn.x.resource.y<br>\n+   *\n+   * Example of an invalid config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   *\n+   * Please note that if multiple resources are defined like described above,\n+   * the error messages will be concatenated.<br>\n+   * Example of such a config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   * - spark.yarn.executor.resource.cores=2<br>\n+   * Then the following two error messages will be printed:<br>\n+   * - \"memory cannot be requested with config spark.yarn.driver.resource.memory,\n+   * please use config spark.driver.memory instead!<br>\n+   * - \"cores cannot be requested with config spark.yarn.executor.resource.cores,\n+   * please use config spark.executor.cores instead!<br>\n+   *\n+   * @param sparkConf\n+   */\n+  def validateResources(sparkConf: SparkConf): Unit = {\n+    val requestedResources = new RequestedResources(sparkConf)\n+    val sb = new mutable.StringBuilder()\n+    POSSIBLE_RESOURCE_DEFINITIONS.foreach { rcp =>\n+      val customResources: Map[String, String] = getCustomResourceValue(requestedResources, rcp)\n+      val (standardResourceConfigKey: String, customResourceConfigKey: String) =\n+        getResourceConfigKeys(rcp)\n+\n+      val errorMessage =\n+        if (customResources.contains(customResourceConfigKey)) {\n+          s\"${rcp.resourceType} cannot be requested with config $customResourceConfigKey, \" +\n+              s\"please use config $standardResourceConfigKey instead!\"\n+        } else {\n+          \"\"\n+        }\n+      if (errorMessage.nonEmpty) {\n+        printErrorMessageToBuffer(sb, errorMessage)\n+      }\n+    }\n+\n+    if (sb.nonEmpty) {\n+      throw new SparkException(sb.toString())\n+    }\n+  }\n+\n+  /**\n+   * Returns the requested map of custom resources,\n+   * based on the ResourceConfigProperties argument.\n+   * @return\n+   */\n+  private def getCustomResourceValue("
  }],
  "prId": 20761
}, {
  "comments": [{
    "author": {
      "login": "galv"
    },
    "body": "I'm having a hard time understanding this method. I will have to come back to it later.",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-05-23T02:25:29Z",
    "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.ProcessType.{am, driver, executor, ProcessType}\n+import org.apache.spark.deploy.yarn.ResourceType.{cores, memory, ResourceType}\n+import org.apache.spark.deploy.yarn.RunMode.{client, cluster, RunMode}\n+import org.apache.spark.deploy.yarn.config._\n+import org.apache.spark.internal.config._\n+\n+private[spark] object ProcessType extends Enumeration {\n+  type ProcessType = Value\n+  val driver, executor, am = Value\n+}\n+\n+private[spark] object RunMode extends Enumeration {\n+  type RunMode = Value\n+  val client, cluster = Value\n+}\n+\n+private[spark] object ResourceType extends Enumeration {\n+  type ResourceType = Value\n+  val cores, memory = Value\n+}\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[ResourceConfigProperties](\n+    new ResourceConfigProperties(am, client, memory),\n+    new ResourceConfigProperties(am, client, cores),\n+    new ResourceConfigProperties(driver, cluster, memory),\n+    new ResourceConfigProperties(driver, cluster, cores),\n+    new ResourceConfigProperties(processType = executor, resourceType = memory),\n+    new ResourceConfigProperties(processType = executor, resourceType = cores))\n+\n+  /**\n+   * Validates sparkConf and throws a SparkException if a standard resource (memory or cores)\n+   * is defined with the property spark.yarn.x.resource.y<br>\n+   *\n+   * Example of an invalid config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   *\n+   * Please note that if multiple resources are defined like described above,\n+   * the error messages will be concatenated.<br>\n+   * Example of such a config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   * - spark.yarn.executor.resource.cores=2<br>\n+   * Then the following two error messages will be printed:<br>\n+   * - \"memory cannot be requested with config spark.yarn.driver.resource.memory,\n+   * please use config spark.driver.memory instead!<br>\n+   * - \"cores cannot be requested with config spark.yarn.executor.resource.cores,\n+   * please use config spark.executor.cores instead!<br>\n+   *\n+   * @param sparkConf\n+   */\n+  def validateResources(sparkConf: SparkConf): Unit = {\n+    val requestedResources = new RequestedResources(sparkConf)\n+    val sb = new mutable.StringBuilder()\n+    POSSIBLE_RESOURCE_DEFINITIONS.foreach { rcp =>"
  }, {
    "author": {
      "login": "szilard-nemeth"
    },
    "body": "Please see my explanation for ResourceTypeValidator in a separate comment.",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-07-19T16:25:11Z",
    "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.ProcessType.{am, driver, executor, ProcessType}\n+import org.apache.spark.deploy.yarn.ResourceType.{cores, memory, ResourceType}\n+import org.apache.spark.deploy.yarn.RunMode.{client, cluster, RunMode}\n+import org.apache.spark.deploy.yarn.config._\n+import org.apache.spark.internal.config._\n+\n+private[spark] object ProcessType extends Enumeration {\n+  type ProcessType = Value\n+  val driver, executor, am = Value\n+}\n+\n+private[spark] object RunMode extends Enumeration {\n+  type RunMode = Value\n+  val client, cluster = Value\n+}\n+\n+private[spark] object ResourceType extends Enumeration {\n+  type ResourceType = Value\n+  val cores, memory = Value\n+}\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[ResourceConfigProperties](\n+    new ResourceConfigProperties(am, client, memory),\n+    new ResourceConfigProperties(am, client, cores),\n+    new ResourceConfigProperties(driver, cluster, memory),\n+    new ResourceConfigProperties(driver, cluster, cores),\n+    new ResourceConfigProperties(processType = executor, resourceType = memory),\n+    new ResourceConfigProperties(processType = executor, resourceType = cores))\n+\n+  /**\n+   * Validates sparkConf and throws a SparkException if a standard resource (memory or cores)\n+   * is defined with the property spark.yarn.x.resource.y<br>\n+   *\n+   * Example of an invalid config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   *\n+   * Please note that if multiple resources are defined like described above,\n+   * the error messages will be concatenated.<br>\n+   * Example of such a config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   * - spark.yarn.executor.resource.cores=2<br>\n+   * Then the following two error messages will be printed:<br>\n+   * - \"memory cannot be requested with config spark.yarn.driver.resource.memory,\n+   * please use config spark.driver.memory instead!<br>\n+   * - \"cores cannot be requested with config spark.yarn.executor.resource.cores,\n+   * please use config spark.executor.cores instead!<br>\n+   *\n+   * @param sparkConf\n+   */\n+  def validateResources(sparkConf: SparkConf): Unit = {\n+    val requestedResources = new RequestedResources(sparkConf)\n+    val sb = new mutable.StringBuilder()\n+    POSSIBLE_RESOURCE_DEFINITIONS.foreach { rcp =>"
  }],
  "prId": 20761
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Just `private` in all these?",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-05-23T23:43:18Z",
    "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.ProcessType.{am, driver, executor, ProcessType}\n+import org.apache.spark.deploy.yarn.ResourceType.{cores, memory, ResourceType}\n+import org.apache.spark.deploy.yarn.RunMode.{client, cluster, RunMode}\n+import org.apache.spark.deploy.yarn.config._\n+import org.apache.spark.internal.config._\n+\n+private[spark] object ProcessType extends Enumeration {"
  }, {
    "author": {
      "login": "szilard-nemeth"
    },
    "body": "Fixed",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-07-19T16:37:23Z",
    "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.ProcessType.{am, driver, executor, ProcessType}\n+import org.apache.spark.deploy.yarn.ResourceType.{cores, memory, ResourceType}\n+import org.apache.spark.deploy.yarn.RunMode.{client, cluster, RunMode}\n+import org.apache.spark.deploy.yarn.config._\n+import org.apache.spark.internal.config._\n+\n+private[spark] object ProcessType extends Enumeration {"
  }],
  "prId": 20761
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "We use all caps for enum constants.",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-05-23T23:44:20Z",
    "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.ProcessType.{am, driver, executor, ProcessType}\n+import org.apache.spark.deploy.yarn.ResourceType.{cores, memory, ResourceType}\n+import org.apache.spark.deploy.yarn.RunMode.{client, cluster, RunMode}\n+import org.apache.spark.deploy.yarn.config._\n+import org.apache.spark.internal.config._\n+\n+private[spark] object ProcessType extends Enumeration {\n+  type ProcessType = Value\n+  val driver, executor, am = Value"
  }, {
    "author": {
      "login": "szilard-nemeth"
    },
    "body": "Fixed",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-07-19T16:37:29Z",
    "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.ProcessType.{am, driver, executor, ProcessType}\n+import org.apache.spark.deploy.yarn.ResourceType.{cores, memory, ResourceType}\n+import org.apache.spark.deploy.yarn.RunMode.{client, cluster, RunMode}\n+import org.apache.spark.deploy.yarn.config._\n+import org.apache.spark.internal.config._\n+\n+private[spark] object ProcessType extends Enumeration {\n+  type ProcessType = Value\n+  val driver, executor, am = Value"
  }],
  "prId": 20761
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Hmmm... how is this whole file different than having these two lines in `Client.scala`:\r\n\r\n```\r\nrequire(!amResources.contains(\"memory\"), \"Memory cannot be specified as a YARN resource directly.\")\r\nrequire(!amResources.contains(\"cores\"), \"Cores cannot be specified as a YARN resource directly.\")\r\n```\r\n\r\nYou can put that in a method and call it twice (for the AM and for executors).\r\n\r\nAlso wasn't the resource name for memory actually `memory-mb`?",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-05-23T23:47:41Z",
    "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.ProcessType.{am, driver, executor, ProcessType}\n+import org.apache.spark.deploy.yarn.ResourceType.{cores, memory, ResourceType}\n+import org.apache.spark.deploy.yarn.RunMode.{client, cluster, RunMode}\n+import org.apache.spark.deploy.yarn.config._\n+import org.apache.spark.internal.config._\n+\n+private[spark] object ProcessType extends Enumeration {\n+  type ProcessType = Value\n+  val driver, executor, am = Value\n+}\n+\n+private[spark] object RunMode extends Enumeration {\n+  type RunMode = Value\n+  val client, cluster = Value\n+}\n+\n+private[spark] object ResourceType extends Enumeration {\n+  type ResourceType = Value\n+  val cores, memory = Value\n+}\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[ResourceConfigProperties](\n+    new ResourceConfigProperties(am, client, memory),\n+    new ResourceConfigProperties(am, client, cores),\n+    new ResourceConfigProperties(driver, cluster, memory),\n+    new ResourceConfigProperties(driver, cluster, cores),\n+    new ResourceConfigProperties(processType = executor, resourceType = memory),\n+    new ResourceConfigProperties(processType = executor, resourceType = cores))\n+\n+  /**\n+   * Validates sparkConf and throws a SparkException if a standard resource (memory or cores)\n+   * is defined with the property spark.yarn.x.resource.y<br>\n+   *\n+   * Example of an invalid config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   *\n+   * Please note that if multiple resources are defined like described above,\n+   * the error messages will be concatenated.<br>\n+   * Example of such a config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   * - spark.yarn.executor.resource.cores=2<br>\n+   * Then the following two error messages will be printed:<br>\n+   * - \"memory cannot be requested with config spark.yarn.driver.resource.memory,\n+   * please use config spark.driver.memory instead!<br>\n+   * - \"cores cannot be requested with config spark.yarn.executor.resource.cores,\n+   * please use config spark.executor.cores instead!<br>\n+   *\n+   * @param sparkConf\n+   */\n+  def validateResources(sparkConf: SparkConf): Unit = {"
  }, {
    "author": {
      "login": "szilard-nemeth"
    },
    "body": "Please see my explanation for ResourceTypeValidator in a separate comment.",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-07-19T16:24:06Z",
    "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.ProcessType.{am, driver, executor, ProcessType}\n+import org.apache.spark.deploy.yarn.ResourceType.{cores, memory, ResourceType}\n+import org.apache.spark.deploy.yarn.RunMode.{client, cluster, RunMode}\n+import org.apache.spark.deploy.yarn.config._\n+import org.apache.spark.internal.config._\n+\n+private[spark] object ProcessType extends Enumeration {\n+  type ProcessType = Value\n+  val driver, executor, am = Value\n+}\n+\n+private[spark] object RunMode extends Enumeration {\n+  type RunMode = Value\n+  val client, cluster = Value\n+}\n+\n+private[spark] object ResourceType extends Enumeration {\n+  type ResourceType = Value\n+  val cores, memory = Value\n+}\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[ResourceConfigProperties](\n+    new ResourceConfigProperties(am, client, memory),\n+    new ResourceConfigProperties(am, client, cores),\n+    new ResourceConfigProperties(driver, cluster, memory),\n+    new ResourceConfigProperties(driver, cluster, cores),\n+    new ResourceConfigProperties(processType = executor, resourceType = memory),\n+    new ResourceConfigProperties(processType = executor, resourceType = cores))\n+\n+  /**\n+   * Validates sparkConf and throws a SparkException if a standard resource (memory or cores)\n+   * is defined with the property spark.yarn.x.resource.y<br>\n+   *\n+   * Example of an invalid config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   *\n+   * Please note that if multiple resources are defined like described above,\n+   * the error messages will be concatenated.<br>\n+   * Example of such a config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   * - spark.yarn.executor.resource.cores=2<br>\n+   * Then the following two error messages will be printed:<br>\n+   * - \"memory cannot be requested with config spark.yarn.driver.resource.memory,\n+   * please use config spark.driver.memory instead!<br>\n+   * - \"cores cannot be requested with config spark.yarn.executor.resource.cores,\n+   * please use config spark.executor.cores instead!<br>\n+   *\n+   * @param sparkConf\n+   */\n+  def validateResources(sparkConf: SparkConf): Unit = {"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "You said: \r\n\r\n>  the error message points you to the right direction, explicitly printing the right config to use instead.\r\n\r\nSo does my code above, it just need to be changed a little to be more parameterized. And it's an order of magnitude smaller and simpler than your validation code.\r\n\r\nSo, same question: what else is your validator checking that those two asserts (properly parameterized to check for driver, am, or executor) do not cover?\r\n",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-08-21T19:40:07Z",
    "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.ProcessType.{am, driver, executor, ProcessType}\n+import org.apache.spark.deploy.yarn.ResourceType.{cores, memory, ResourceType}\n+import org.apache.spark.deploy.yarn.RunMode.{client, cluster, RunMode}\n+import org.apache.spark.deploy.yarn.config._\n+import org.apache.spark.internal.config._\n+\n+private[spark] object ProcessType extends Enumeration {\n+  type ProcessType = Value\n+  val driver, executor, am = Value\n+}\n+\n+private[spark] object RunMode extends Enumeration {\n+  type RunMode = Value\n+  val client, cluster = Value\n+}\n+\n+private[spark] object ResourceType extends Enumeration {\n+  type ResourceType = Value\n+  val cores, memory = Value\n+}\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[ResourceConfigProperties](\n+    new ResourceConfigProperties(am, client, memory),\n+    new ResourceConfigProperties(am, client, cores),\n+    new ResourceConfigProperties(driver, cluster, memory),\n+    new ResourceConfigProperties(driver, cluster, cores),\n+    new ResourceConfigProperties(processType = executor, resourceType = memory),\n+    new ResourceConfigProperties(processType = executor, resourceType = cores))\n+\n+  /**\n+   * Validates sparkConf and throws a SparkException if a standard resource (memory or cores)\n+   * is defined with the property spark.yarn.x.resource.y<br>\n+   *\n+   * Example of an invalid config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   *\n+   * Please note that if multiple resources are defined like described above,\n+   * the error messages will be concatenated.<br>\n+   * Example of such a config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   * - spark.yarn.executor.resource.cores=2<br>\n+   * Then the following two error messages will be printed:<br>\n+   * - \"memory cannot be requested with config spark.yarn.driver.resource.memory,\n+   * please use config spark.driver.memory instead!<br>\n+   * - \"cores cannot be requested with config spark.yarn.executor.resource.cores,\n+   * please use config spark.executor.cores instead!<br>\n+   *\n+   * @param sparkConf\n+   */\n+  def validateResources(sparkConf: SparkConf): Unit = {"
  }],
  "prId": 20761
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Just make the text above the value of the `@return` tag.",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-07-31T18:54:44Z",
    "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.ProcessType.{AM, DRIVER, EXECUTOR, ProcessType}\n+import org.apache.spark.deploy.yarn.ResourceType.{CORES, MEMORY, ResourceType}\n+import org.apache.spark.deploy.yarn.RunMode.{CLIENT, CLUSTER, RunMode}\n+import org.apache.spark.deploy.yarn.config._\n+import org.apache.spark.internal.config._\n+\n+private object ProcessType extends Enumeration {\n+  type ProcessType = Value\n+  val DRIVER = Value(\"driver\")\n+  val EXECUTOR = Value(\"executor\")\n+  val AM = Value(\"am\")\n+}\n+\n+private object RunMode extends Enumeration {\n+  type RunMode = Value\n+  val CLIENT = Value(\"client\")\n+  val CLUSTER = Value(\"cluster\")\n+}\n+\n+private object ResourceType extends Enumeration {\n+  type ResourceType = Value\n+  val CORES = Value(\"cores\")\n+  val MEMORY = Value(\"memory\")\n+}\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[ResourceConfigProperties](\n+    new ResourceConfigProperties(AM, Some(CLIENT), MEMORY),\n+    new ResourceConfigProperties(AM, Some(CLIENT), CORES),\n+    new ResourceConfigProperties(DRIVER, Some(CLUSTER), MEMORY),\n+    new ResourceConfigProperties(DRIVER, Some(CLUSTER), CORES),\n+    new ResourceConfigProperties(EXECUTOR, None, MEMORY),\n+    new ResourceConfigProperties(EXECUTOR, None, CORES))\n+\n+  /**\n+   * Validates sparkConf and throws a SparkException if a standard resource (memory or cores)\n+   * is defined with the property spark.yarn.x.resource.y<br>\n+   *\n+   * Example of an invalid config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   *\n+   * Please note that if multiple resources are defined like described above,\n+   * the error messages will be concatenated.<br>\n+   * Example of such a config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   * - spark.yarn.executor.resource.cores=2<br>\n+   * Then the following two error messages will be printed:<br>\n+   * - \"memory cannot be requested with config spark.yarn.driver.resource.memory,\n+   * please use config spark.driver.memory instead!<br>\n+   * - \"cores cannot be requested with config spark.yarn.executor.resource.cores,\n+   * please use config spark.executor.cores instead!<br>\n+   *\n+   * @param sparkConf\n+   */\n+  def validateResources(sparkConf: SparkConf): Unit = {\n+    val requestedResources = new RequestedResources(sparkConf)\n+    val sb = new mutable.StringBuilder()\n+    POSSIBLE_RESOURCE_DEFINITIONS.foreach { rcp =>\n+      val customResources: Map[String, String] = getCustomResourceValue(requestedResources, rcp)\n+      val (standardResourceConfigKey: String, customResourceConfigKey: String) =\n+        getResourceConfigKeys(rcp)\n+\n+      val errorMessage =\n+        if (customResources.contains(customResourceConfigKey)) {\n+          s\"${rcp.resourceType} cannot be requested with config $customResourceConfigKey, \" +\n+              s\"please use config $standardResourceConfigKey instead!\"\n+        } else {\n+          \"\"\n+        }\n+      if (errorMessage.nonEmpty) {\n+        printErrorMessageToBuffer(sb, errorMessage)\n+      }\n+    }\n+\n+    if (sb.nonEmpty) {\n+      throw new SparkException(sb.toString())\n+    }\n+  }\n+\n+  /**\n+   * Returns the requested map of custom resources,\n+   * based on the ResourceConfigProperties argument.\n+   * @return"
  }, {
    "author": {
      "login": "szilard-nemeth"
    },
    "body": "fixed",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-09-17T13:56:40Z",
    "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.ProcessType.{AM, DRIVER, EXECUTOR, ProcessType}\n+import org.apache.spark.deploy.yarn.ResourceType.{CORES, MEMORY, ResourceType}\n+import org.apache.spark.deploy.yarn.RunMode.{CLIENT, CLUSTER, RunMode}\n+import org.apache.spark.deploy.yarn.config._\n+import org.apache.spark.internal.config._\n+\n+private object ProcessType extends Enumeration {\n+  type ProcessType = Value\n+  val DRIVER = Value(\"driver\")\n+  val EXECUTOR = Value(\"executor\")\n+  val AM = Value(\"am\")\n+}\n+\n+private object RunMode extends Enumeration {\n+  type RunMode = Value\n+  val CLIENT = Value(\"client\")\n+  val CLUSTER = Value(\"cluster\")\n+}\n+\n+private object ResourceType extends Enumeration {\n+  type ResourceType = Value\n+  val CORES = Value(\"cores\")\n+  val MEMORY = Value(\"memory\")\n+}\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[ResourceConfigProperties](\n+    new ResourceConfigProperties(AM, Some(CLIENT), MEMORY),\n+    new ResourceConfigProperties(AM, Some(CLIENT), CORES),\n+    new ResourceConfigProperties(DRIVER, Some(CLUSTER), MEMORY),\n+    new ResourceConfigProperties(DRIVER, Some(CLUSTER), CORES),\n+    new ResourceConfigProperties(EXECUTOR, None, MEMORY),\n+    new ResourceConfigProperties(EXECUTOR, None, CORES))\n+\n+  /**\n+   * Validates sparkConf and throws a SparkException if a standard resource (memory or cores)\n+   * is defined with the property spark.yarn.x.resource.y<br>\n+   *\n+   * Example of an invalid config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   *\n+   * Please note that if multiple resources are defined like described above,\n+   * the error messages will be concatenated.<br>\n+   * Example of such a config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   * - spark.yarn.executor.resource.cores=2<br>\n+   * Then the following two error messages will be printed:<br>\n+   * - \"memory cannot be requested with config spark.yarn.driver.resource.memory,\n+   * please use config spark.driver.memory instead!<br>\n+   * - \"cores cannot be requested with config spark.yarn.executor.resource.cores,\n+   * please use config spark.executor.cores instead!<br>\n+   *\n+   * @param sparkConf\n+   */\n+  def validateResources(sparkConf: SparkConf): Unit = {\n+    val requestedResources = new RequestedResources(sparkConf)\n+    val sb = new mutable.StringBuilder()\n+    POSSIBLE_RESOURCE_DEFINITIONS.foreach { rcp =>\n+      val customResources: Map[String, String] = getCustomResourceValue(requestedResources, rcp)\n+      val (standardResourceConfigKey: String, customResourceConfigKey: String) =\n+        getResourceConfigKeys(rcp)\n+\n+      val errorMessage =\n+        if (customResources.contains(customResourceConfigKey)) {\n+          s\"${rcp.resourceType} cannot be requested with config $customResourceConfigKey, \" +\n+              s\"please use config $standardResourceConfigKey instead!\"\n+        } else {\n+          \"\"\n+        }\n+      if (errorMessage.nonEmpty) {\n+        printErrorMessageToBuffer(sb, errorMessage)\n+      }\n+    }\n+\n+    if (sb.nonEmpty) {\n+      throw new SparkException(sb.toString())\n+    }\n+  }\n+\n+  /**\n+   * Returns the requested map of custom resources,\n+   * based on the ResourceConfigProperties argument.\n+   * @return"
  }],
  "prId": 20761
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Splitting the assignment across lines looks funny and might fail style checks. Why assign a var though, why not just end the method with the match clause ...\r\n\r\n```\r\n(rcp.processType, rcp.runMode, rcp.resourceType) match {\r\n   ...\r\n}\r\n```\r\n\r\nYou can still throw an exception from the last case.",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-07-31T18:55:53Z",
    "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.ProcessType.{AM, DRIVER, EXECUTOR, ProcessType}\n+import org.apache.spark.deploy.yarn.ResourceType.{CORES, MEMORY, ResourceType}\n+import org.apache.spark.deploy.yarn.RunMode.{CLIENT, CLUSTER, RunMode}\n+import org.apache.spark.deploy.yarn.config._\n+import org.apache.spark.internal.config._\n+\n+private object ProcessType extends Enumeration {\n+  type ProcessType = Value\n+  val DRIVER = Value(\"driver\")\n+  val EXECUTOR = Value(\"executor\")\n+  val AM = Value(\"am\")\n+}\n+\n+private object RunMode extends Enumeration {\n+  type RunMode = Value\n+  val CLIENT = Value(\"client\")\n+  val CLUSTER = Value(\"cluster\")\n+}\n+\n+private object ResourceType extends Enumeration {\n+  type ResourceType = Value\n+  val CORES = Value(\"cores\")\n+  val MEMORY = Value(\"memory\")\n+}\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[ResourceConfigProperties](\n+    new ResourceConfigProperties(AM, Some(CLIENT), MEMORY),\n+    new ResourceConfigProperties(AM, Some(CLIENT), CORES),\n+    new ResourceConfigProperties(DRIVER, Some(CLUSTER), MEMORY),\n+    new ResourceConfigProperties(DRIVER, Some(CLUSTER), CORES),\n+    new ResourceConfigProperties(EXECUTOR, None, MEMORY),\n+    new ResourceConfigProperties(EXECUTOR, None, CORES))\n+\n+  /**\n+   * Validates sparkConf and throws a SparkException if a standard resource (memory or cores)\n+   * is defined with the property spark.yarn.x.resource.y<br>\n+   *\n+   * Example of an invalid config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   *\n+   * Please note that if multiple resources are defined like described above,\n+   * the error messages will be concatenated.<br>\n+   * Example of such a config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   * - spark.yarn.executor.resource.cores=2<br>\n+   * Then the following two error messages will be printed:<br>\n+   * - \"memory cannot be requested with config spark.yarn.driver.resource.memory,\n+   * please use config spark.driver.memory instead!<br>\n+   * - \"cores cannot be requested with config spark.yarn.executor.resource.cores,\n+   * please use config spark.executor.cores instead!<br>\n+   *\n+   * @param sparkConf\n+   */\n+  def validateResources(sparkConf: SparkConf): Unit = {\n+    val requestedResources = new RequestedResources(sparkConf)\n+    val sb = new mutable.StringBuilder()\n+    POSSIBLE_RESOURCE_DEFINITIONS.foreach { rcp =>\n+      val customResources: Map[String, String] = getCustomResourceValue(requestedResources, rcp)\n+      val (standardResourceConfigKey: String, customResourceConfigKey: String) =\n+        getResourceConfigKeys(rcp)\n+\n+      val errorMessage =\n+        if (customResources.contains(customResourceConfigKey)) {\n+          s\"${rcp.resourceType} cannot be requested with config $customResourceConfigKey, \" +\n+              s\"please use config $standardResourceConfigKey instead!\"\n+        } else {\n+          \"\"\n+        }\n+      if (errorMessage.nonEmpty) {\n+        printErrorMessageToBuffer(sb, errorMessage)\n+      }\n+    }\n+\n+    if (sb.nonEmpty) {\n+      throw new SparkException(sb.toString())\n+    }\n+  }\n+\n+  /**\n+   * Returns the requested map of custom resources,\n+   * based on the ResourceConfigProperties argument.\n+   * @return\n+   */\n+  private def getCustomResourceValue(\n+      requestedResources: RequestedResources,\n+      rcp: ResourceConfigProperties): Map[String, String] = {\n+    var customResources: Map[String, String] = null\n+    (rcp.processType, rcp.runMode, rcp.resourceType) match {\n+      case (ProcessType.EXECUTOR, _, _) => customResources ="
  }, {
    "author": {
      "login": "szilard-nemeth"
    },
    "body": "Fixed",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-09-17T14:07:50Z",
    "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.ProcessType.{AM, DRIVER, EXECUTOR, ProcessType}\n+import org.apache.spark.deploy.yarn.ResourceType.{CORES, MEMORY, ResourceType}\n+import org.apache.spark.deploy.yarn.RunMode.{CLIENT, CLUSTER, RunMode}\n+import org.apache.spark.deploy.yarn.config._\n+import org.apache.spark.internal.config._\n+\n+private object ProcessType extends Enumeration {\n+  type ProcessType = Value\n+  val DRIVER = Value(\"driver\")\n+  val EXECUTOR = Value(\"executor\")\n+  val AM = Value(\"am\")\n+}\n+\n+private object RunMode extends Enumeration {\n+  type RunMode = Value\n+  val CLIENT = Value(\"client\")\n+  val CLUSTER = Value(\"cluster\")\n+}\n+\n+private object ResourceType extends Enumeration {\n+  type ResourceType = Value\n+  val CORES = Value(\"cores\")\n+  val MEMORY = Value(\"memory\")\n+}\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[ResourceConfigProperties](\n+    new ResourceConfigProperties(AM, Some(CLIENT), MEMORY),\n+    new ResourceConfigProperties(AM, Some(CLIENT), CORES),\n+    new ResourceConfigProperties(DRIVER, Some(CLUSTER), MEMORY),\n+    new ResourceConfigProperties(DRIVER, Some(CLUSTER), CORES),\n+    new ResourceConfigProperties(EXECUTOR, None, MEMORY),\n+    new ResourceConfigProperties(EXECUTOR, None, CORES))\n+\n+  /**\n+   * Validates sparkConf and throws a SparkException if a standard resource (memory or cores)\n+   * is defined with the property spark.yarn.x.resource.y<br>\n+   *\n+   * Example of an invalid config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   *\n+   * Please note that if multiple resources are defined like described above,\n+   * the error messages will be concatenated.<br>\n+   * Example of such a config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   * - spark.yarn.executor.resource.cores=2<br>\n+   * Then the following two error messages will be printed:<br>\n+   * - \"memory cannot be requested with config spark.yarn.driver.resource.memory,\n+   * please use config spark.driver.memory instead!<br>\n+   * - \"cores cannot be requested with config spark.yarn.executor.resource.cores,\n+   * please use config spark.executor.cores instead!<br>\n+   *\n+   * @param sparkConf\n+   */\n+  def validateResources(sparkConf: SparkConf): Unit = {\n+    val requestedResources = new RequestedResources(sparkConf)\n+    val sb = new mutable.StringBuilder()\n+    POSSIBLE_RESOURCE_DEFINITIONS.foreach { rcp =>\n+      val customResources: Map[String, String] = getCustomResourceValue(requestedResources, rcp)\n+      val (standardResourceConfigKey: String, customResourceConfigKey: String) =\n+        getResourceConfigKeys(rcp)\n+\n+      val errorMessage =\n+        if (customResources.contains(customResourceConfigKey)) {\n+          s\"${rcp.resourceType} cannot be requested with config $customResourceConfigKey, \" +\n+              s\"please use config $standardResourceConfigKey instead!\"\n+        } else {\n+          \"\"\n+        }\n+      if (errorMessage.nonEmpty) {\n+        printErrorMessageToBuffer(sb, errorMessage)\n+      }\n+    }\n+\n+    if (sb.nonEmpty) {\n+      throw new SparkException(sb.toString())\n+    }\n+  }\n+\n+  /**\n+   * Returns the requested map of custom resources,\n+   * based on the ResourceConfigProperties argument.\n+   * @return\n+   */\n+  private def getCustomResourceValue(\n+      requestedResources: RequestedResources,\n+      rcp: ResourceConfigProperties): Map[String, String] = {\n+    var customResources: Map[String, String] = null\n+    (rcp.processType, rcp.runMode, rcp.resourceType) match {\n+      case (ProcessType.EXECUTOR, _, _) => customResources ="
  }],
  "prId": 20761
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Same, no point in having blank tags here, when the text that goes in them is already above.",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-07-31T18:56:19Z",
    "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.ProcessType.{AM, DRIVER, EXECUTOR, ProcessType}\n+import org.apache.spark.deploy.yarn.ResourceType.{CORES, MEMORY, ResourceType}\n+import org.apache.spark.deploy.yarn.RunMode.{CLIENT, CLUSTER, RunMode}\n+import org.apache.spark.deploy.yarn.config._\n+import org.apache.spark.internal.config._\n+\n+private object ProcessType extends Enumeration {\n+  type ProcessType = Value\n+  val DRIVER = Value(\"driver\")\n+  val EXECUTOR = Value(\"executor\")\n+  val AM = Value(\"am\")\n+}\n+\n+private object RunMode extends Enumeration {\n+  type RunMode = Value\n+  val CLIENT = Value(\"client\")\n+  val CLUSTER = Value(\"cluster\")\n+}\n+\n+private object ResourceType extends Enumeration {\n+  type ResourceType = Value\n+  val CORES = Value(\"cores\")\n+  val MEMORY = Value(\"memory\")\n+}\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[ResourceConfigProperties](\n+    new ResourceConfigProperties(AM, Some(CLIENT), MEMORY),\n+    new ResourceConfigProperties(AM, Some(CLIENT), CORES),\n+    new ResourceConfigProperties(DRIVER, Some(CLUSTER), MEMORY),\n+    new ResourceConfigProperties(DRIVER, Some(CLUSTER), CORES),\n+    new ResourceConfigProperties(EXECUTOR, None, MEMORY),\n+    new ResourceConfigProperties(EXECUTOR, None, CORES))\n+\n+  /**\n+   * Validates sparkConf and throws a SparkException if a standard resource (memory or cores)\n+   * is defined with the property spark.yarn.x.resource.y<br>\n+   *\n+   * Example of an invalid config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   *\n+   * Please note that if multiple resources are defined like described above,\n+   * the error messages will be concatenated.<br>\n+   * Example of such a config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   * - spark.yarn.executor.resource.cores=2<br>\n+   * Then the following two error messages will be printed:<br>\n+   * - \"memory cannot be requested with config spark.yarn.driver.resource.memory,\n+   * please use config spark.driver.memory instead!<br>\n+   * - \"cores cannot be requested with config spark.yarn.executor.resource.cores,\n+   * please use config spark.executor.cores instead!<br>\n+   *\n+   * @param sparkConf\n+   */\n+  def validateResources(sparkConf: SparkConf): Unit = {\n+    val requestedResources = new RequestedResources(sparkConf)\n+    val sb = new mutable.StringBuilder()\n+    POSSIBLE_RESOURCE_DEFINITIONS.foreach { rcp =>\n+      val customResources: Map[String, String] = getCustomResourceValue(requestedResources, rcp)\n+      val (standardResourceConfigKey: String, customResourceConfigKey: String) =\n+        getResourceConfigKeys(rcp)\n+\n+      val errorMessage =\n+        if (customResources.contains(customResourceConfigKey)) {\n+          s\"${rcp.resourceType} cannot be requested with config $customResourceConfigKey, \" +\n+              s\"please use config $standardResourceConfigKey instead!\"\n+        } else {\n+          \"\"\n+        }\n+      if (errorMessage.nonEmpty) {\n+        printErrorMessageToBuffer(sb, errorMessage)\n+      }\n+    }\n+\n+    if (sb.nonEmpty) {\n+      throw new SparkException(sb.toString())\n+    }\n+  }\n+\n+  /**\n+   * Returns the requested map of custom resources,\n+   * based on the ResourceConfigProperties argument.\n+   * @return\n+   */\n+  private def getCustomResourceValue(\n+      requestedResources: RequestedResources,\n+      rcp: ResourceConfigProperties): Map[String, String] = {\n+    var customResources: Map[String, String] = null\n+    (rcp.processType, rcp.runMode, rcp.resourceType) match {\n+      case (ProcessType.EXECUTOR, _, _) => customResources =\n+          requestedResources.customExecutorResources\n+      case (ProcessType.AM, Some(RunMode.CLIENT), _) => customResources =\n+          requestedResources.customAMResources\n+      case (ProcessType.DRIVER, Some(RunMode.CLUSTER), _) => customResources =\n+          requestedResources.customDriverResources\n+      case _ => throw new SparkException(s\"ResourceConfigProperties contains an \" +\n+          s\"invalid combination of fields: $rcp\")\n+    }\n+    customResources\n+  }\n+\n+  /**\n+   * Returns a tuple of standard resource config key and custom resource config key, based on the\n+   * processType and runMode fields of the ResourceConfigProperties argument.\n+   * Standard resources are memory and cores.\n+   *\n+   * @param rcp\n+   * @return"
  }, {
    "author": {
      "login": "szilard-nemeth"
    },
    "body": "fixed",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-09-17T13:57:44Z",
    "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.ProcessType.{AM, DRIVER, EXECUTOR, ProcessType}\n+import org.apache.spark.deploy.yarn.ResourceType.{CORES, MEMORY, ResourceType}\n+import org.apache.spark.deploy.yarn.RunMode.{CLIENT, CLUSTER, RunMode}\n+import org.apache.spark.deploy.yarn.config._\n+import org.apache.spark.internal.config._\n+\n+private object ProcessType extends Enumeration {\n+  type ProcessType = Value\n+  val DRIVER = Value(\"driver\")\n+  val EXECUTOR = Value(\"executor\")\n+  val AM = Value(\"am\")\n+}\n+\n+private object RunMode extends Enumeration {\n+  type RunMode = Value\n+  val CLIENT = Value(\"client\")\n+  val CLUSTER = Value(\"cluster\")\n+}\n+\n+private object ResourceType extends Enumeration {\n+  type ResourceType = Value\n+  val CORES = Value(\"cores\")\n+  val MEMORY = Value(\"memory\")\n+}\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[ResourceConfigProperties](\n+    new ResourceConfigProperties(AM, Some(CLIENT), MEMORY),\n+    new ResourceConfigProperties(AM, Some(CLIENT), CORES),\n+    new ResourceConfigProperties(DRIVER, Some(CLUSTER), MEMORY),\n+    new ResourceConfigProperties(DRIVER, Some(CLUSTER), CORES),\n+    new ResourceConfigProperties(EXECUTOR, None, MEMORY),\n+    new ResourceConfigProperties(EXECUTOR, None, CORES))\n+\n+  /**\n+   * Validates sparkConf and throws a SparkException if a standard resource (memory or cores)\n+   * is defined with the property spark.yarn.x.resource.y<br>\n+   *\n+   * Example of an invalid config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   *\n+   * Please note that if multiple resources are defined like described above,\n+   * the error messages will be concatenated.<br>\n+   * Example of such a config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   * - spark.yarn.executor.resource.cores=2<br>\n+   * Then the following two error messages will be printed:<br>\n+   * - \"memory cannot be requested with config spark.yarn.driver.resource.memory,\n+   * please use config spark.driver.memory instead!<br>\n+   * - \"cores cannot be requested with config spark.yarn.executor.resource.cores,\n+   * please use config spark.executor.cores instead!<br>\n+   *\n+   * @param sparkConf\n+   */\n+  def validateResources(sparkConf: SparkConf): Unit = {\n+    val requestedResources = new RequestedResources(sparkConf)\n+    val sb = new mutable.StringBuilder()\n+    POSSIBLE_RESOURCE_DEFINITIONS.foreach { rcp =>\n+      val customResources: Map[String, String] = getCustomResourceValue(requestedResources, rcp)\n+      val (standardResourceConfigKey: String, customResourceConfigKey: String) =\n+        getResourceConfigKeys(rcp)\n+\n+      val errorMessage =\n+        if (customResources.contains(customResourceConfigKey)) {\n+          s\"${rcp.resourceType} cannot be requested with config $customResourceConfigKey, \" +\n+              s\"please use config $standardResourceConfigKey instead!\"\n+        } else {\n+          \"\"\n+        }\n+      if (errorMessage.nonEmpty) {\n+        printErrorMessageToBuffer(sb, errorMessage)\n+      }\n+    }\n+\n+    if (sb.nonEmpty) {\n+      throw new SparkException(sb.toString())\n+    }\n+  }\n+\n+  /**\n+   * Returns the requested map of custom resources,\n+   * based on the ResourceConfigProperties argument.\n+   * @return\n+   */\n+  private def getCustomResourceValue(\n+      requestedResources: RequestedResources,\n+      rcp: ResourceConfigProperties): Map[String, String] = {\n+    var customResources: Map[String, String] = null\n+    (rcp.processType, rcp.runMode, rcp.resourceType) match {\n+      case (ProcessType.EXECUTOR, _, _) => customResources =\n+          requestedResources.customExecutorResources\n+      case (ProcessType.AM, Some(RunMode.CLIENT), _) => customResources =\n+          requestedResources.customAMResources\n+      case (ProcessType.DRIVER, Some(RunMode.CLUSTER), _) => customResources =\n+          requestedResources.customDriverResources\n+      case _ => throw new SparkException(s\"ResourceConfigProperties contains an \" +\n+          s\"invalid combination of fields: $rcp\")\n+    }\n+    customResources\n+  }\n+\n+  /**\n+   * Returns a tuple of standard resource config key and custom resource config key, based on the\n+   * processType and runMode fields of the ResourceConfigProperties argument.\n+   * Standard resources are memory and cores.\n+   *\n+   * @param rcp\n+   * @return"
  }],
  "prId": 20761
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Same comment about the var plus match that assigns it; just assign the value of the match to a val.",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-07-31T18:56:44Z",
    "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.ProcessType.{AM, DRIVER, EXECUTOR, ProcessType}\n+import org.apache.spark.deploy.yarn.ResourceType.{CORES, MEMORY, ResourceType}\n+import org.apache.spark.deploy.yarn.RunMode.{CLIENT, CLUSTER, RunMode}\n+import org.apache.spark.deploy.yarn.config._\n+import org.apache.spark.internal.config._\n+\n+private object ProcessType extends Enumeration {\n+  type ProcessType = Value\n+  val DRIVER = Value(\"driver\")\n+  val EXECUTOR = Value(\"executor\")\n+  val AM = Value(\"am\")\n+}\n+\n+private object RunMode extends Enumeration {\n+  type RunMode = Value\n+  val CLIENT = Value(\"client\")\n+  val CLUSTER = Value(\"cluster\")\n+}\n+\n+private object ResourceType extends Enumeration {\n+  type ResourceType = Value\n+  val CORES = Value(\"cores\")\n+  val MEMORY = Value(\"memory\")\n+}\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[ResourceConfigProperties](\n+    new ResourceConfigProperties(AM, Some(CLIENT), MEMORY),\n+    new ResourceConfigProperties(AM, Some(CLIENT), CORES),\n+    new ResourceConfigProperties(DRIVER, Some(CLUSTER), MEMORY),\n+    new ResourceConfigProperties(DRIVER, Some(CLUSTER), CORES),\n+    new ResourceConfigProperties(EXECUTOR, None, MEMORY),\n+    new ResourceConfigProperties(EXECUTOR, None, CORES))\n+\n+  /**\n+   * Validates sparkConf and throws a SparkException if a standard resource (memory or cores)\n+   * is defined with the property spark.yarn.x.resource.y<br>\n+   *\n+   * Example of an invalid config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   *\n+   * Please note that if multiple resources are defined like described above,\n+   * the error messages will be concatenated.<br>\n+   * Example of such a config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   * - spark.yarn.executor.resource.cores=2<br>\n+   * Then the following two error messages will be printed:<br>\n+   * - \"memory cannot be requested with config spark.yarn.driver.resource.memory,\n+   * please use config spark.driver.memory instead!<br>\n+   * - \"cores cannot be requested with config spark.yarn.executor.resource.cores,\n+   * please use config spark.executor.cores instead!<br>\n+   *\n+   * @param sparkConf\n+   */\n+  def validateResources(sparkConf: SparkConf): Unit = {\n+    val requestedResources = new RequestedResources(sparkConf)\n+    val sb = new mutable.StringBuilder()\n+    POSSIBLE_RESOURCE_DEFINITIONS.foreach { rcp =>\n+      val customResources: Map[String, String] = getCustomResourceValue(requestedResources, rcp)\n+      val (standardResourceConfigKey: String, customResourceConfigKey: String) =\n+        getResourceConfigKeys(rcp)\n+\n+      val errorMessage =\n+        if (customResources.contains(customResourceConfigKey)) {\n+          s\"${rcp.resourceType} cannot be requested with config $customResourceConfigKey, \" +\n+              s\"please use config $standardResourceConfigKey instead!\"\n+        } else {\n+          \"\"\n+        }\n+      if (errorMessage.nonEmpty) {\n+        printErrorMessageToBuffer(sb, errorMessage)\n+      }\n+    }\n+\n+    if (sb.nonEmpty) {\n+      throw new SparkException(sb.toString())\n+    }\n+  }\n+\n+  /**\n+   * Returns the requested map of custom resources,\n+   * based on the ResourceConfigProperties argument.\n+   * @return\n+   */\n+  private def getCustomResourceValue(\n+      requestedResources: RequestedResources,\n+      rcp: ResourceConfigProperties): Map[String, String] = {\n+    var customResources: Map[String, String] = null\n+    (rcp.processType, rcp.runMode, rcp.resourceType) match {\n+      case (ProcessType.EXECUTOR, _, _) => customResources =\n+          requestedResources.customExecutorResources\n+      case (ProcessType.AM, Some(RunMode.CLIENT), _) => customResources =\n+          requestedResources.customAMResources\n+      case (ProcessType.DRIVER, Some(RunMode.CLUSTER), _) => customResources =\n+          requestedResources.customDriverResources\n+      case _ => throw new SparkException(s\"ResourceConfigProperties contains an \" +\n+          s\"invalid combination of fields: $rcp\")\n+    }\n+    customResources\n+  }\n+\n+  /**\n+   * Returns a tuple of standard resource config key and custom resource config key, based on the\n+   * processType and runMode fields of the ResourceConfigProperties argument.\n+   * Standard resources are memory and cores.\n+   *\n+   * @param rcp\n+   * @return\n+   */\n+  private def getResourceConfigKeys(rcp: ResourceConfigProperties): (String, String) = {\n+    val standardResourceConfigKey: String = if (rcp.processType == ProcessType.AM) {\n+      s\"spark.yarn.${rcp.processType}.${rcp.resourceType}\"\n+    } else {\n+      s\"spark.${rcp.processType}.${rcp.resourceType}\"\n+    }\n+\n+    var customResourceTypeConfigKey: String = \"\"\n+    (rcp.processType, rcp.runMode) match {"
  }, {
    "author": {
      "login": "szilard-nemeth"
    },
    "body": "Good point, fixed!",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-09-17T14:06:22Z",
    "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.ProcessType.{AM, DRIVER, EXECUTOR, ProcessType}\n+import org.apache.spark.deploy.yarn.ResourceType.{CORES, MEMORY, ResourceType}\n+import org.apache.spark.deploy.yarn.RunMode.{CLIENT, CLUSTER, RunMode}\n+import org.apache.spark.deploy.yarn.config._\n+import org.apache.spark.internal.config._\n+\n+private object ProcessType extends Enumeration {\n+  type ProcessType = Value\n+  val DRIVER = Value(\"driver\")\n+  val EXECUTOR = Value(\"executor\")\n+  val AM = Value(\"am\")\n+}\n+\n+private object RunMode extends Enumeration {\n+  type RunMode = Value\n+  val CLIENT = Value(\"client\")\n+  val CLUSTER = Value(\"cluster\")\n+}\n+\n+private object ResourceType extends Enumeration {\n+  type ResourceType = Value\n+  val CORES = Value(\"cores\")\n+  val MEMORY = Value(\"memory\")\n+}\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[ResourceConfigProperties](\n+    new ResourceConfigProperties(AM, Some(CLIENT), MEMORY),\n+    new ResourceConfigProperties(AM, Some(CLIENT), CORES),\n+    new ResourceConfigProperties(DRIVER, Some(CLUSTER), MEMORY),\n+    new ResourceConfigProperties(DRIVER, Some(CLUSTER), CORES),\n+    new ResourceConfigProperties(EXECUTOR, None, MEMORY),\n+    new ResourceConfigProperties(EXECUTOR, None, CORES))\n+\n+  /**\n+   * Validates sparkConf and throws a SparkException if a standard resource (memory or cores)\n+   * is defined with the property spark.yarn.x.resource.y<br>\n+   *\n+   * Example of an invalid config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   *\n+   * Please note that if multiple resources are defined like described above,\n+   * the error messages will be concatenated.<br>\n+   * Example of such a config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   * - spark.yarn.executor.resource.cores=2<br>\n+   * Then the following two error messages will be printed:<br>\n+   * - \"memory cannot be requested with config spark.yarn.driver.resource.memory,\n+   * please use config spark.driver.memory instead!<br>\n+   * - \"cores cannot be requested with config spark.yarn.executor.resource.cores,\n+   * please use config spark.executor.cores instead!<br>\n+   *\n+   * @param sparkConf\n+   */\n+  def validateResources(sparkConf: SparkConf): Unit = {\n+    val requestedResources = new RequestedResources(sparkConf)\n+    val sb = new mutable.StringBuilder()\n+    POSSIBLE_RESOURCE_DEFINITIONS.foreach { rcp =>\n+      val customResources: Map[String, String] = getCustomResourceValue(requestedResources, rcp)\n+      val (standardResourceConfigKey: String, customResourceConfigKey: String) =\n+        getResourceConfigKeys(rcp)\n+\n+      val errorMessage =\n+        if (customResources.contains(customResourceConfigKey)) {\n+          s\"${rcp.resourceType} cannot be requested with config $customResourceConfigKey, \" +\n+              s\"please use config $standardResourceConfigKey instead!\"\n+        } else {\n+          \"\"\n+        }\n+      if (errorMessage.nonEmpty) {\n+        printErrorMessageToBuffer(sb, errorMessage)\n+      }\n+    }\n+\n+    if (sb.nonEmpty) {\n+      throw new SparkException(sb.toString())\n+    }\n+  }\n+\n+  /**\n+   * Returns the requested map of custom resources,\n+   * based on the ResourceConfigProperties argument.\n+   * @return\n+   */\n+  private def getCustomResourceValue(\n+      requestedResources: RequestedResources,\n+      rcp: ResourceConfigProperties): Map[String, String] = {\n+    var customResources: Map[String, String] = null\n+    (rcp.processType, rcp.runMode, rcp.resourceType) match {\n+      case (ProcessType.EXECUTOR, _, _) => customResources =\n+          requestedResources.customExecutorResources\n+      case (ProcessType.AM, Some(RunMode.CLIENT), _) => customResources =\n+          requestedResources.customAMResources\n+      case (ProcessType.DRIVER, Some(RunMode.CLUSTER), _) => customResources =\n+          requestedResources.customDriverResources\n+      case _ => throw new SparkException(s\"ResourceConfigProperties contains an \" +\n+          s\"invalid combination of fields: $rcp\")\n+    }\n+    customResources\n+  }\n+\n+  /**\n+   * Returns a tuple of standard resource config key and custom resource config key, based on the\n+   * processType and runMode fields of the ResourceConfigProperties argument.\n+   * Standard resources are memory and cores.\n+   *\n+   * @param rcp\n+   * @return\n+   */\n+  private def getResourceConfigKeys(rcp: ResourceConfigProperties): (String, String) = {\n+    val standardResourceConfigKey: String = if (rcp.processType == ProcessType.AM) {\n+      s\"spark.yarn.${rcp.processType}.${rcp.resourceType}\"\n+    } else {\n+      s\"spark.${rcp.processType}.${rcp.resourceType}\"\n+    }\n+\n+    var customResourceTypeConfigKey: String = \"\"\n+    (rcp.processType, rcp.runMode) match {"
  }],
  "prId": 20761
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "You can omit types on local vars/vals ; only in cases where it's unclear or ambiguous is it really needed.",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-07-31T18:57:18Z",
    "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.ProcessType.{AM, DRIVER, EXECUTOR, ProcessType}\n+import org.apache.spark.deploy.yarn.ResourceType.{CORES, MEMORY, ResourceType}\n+import org.apache.spark.deploy.yarn.RunMode.{CLIENT, CLUSTER, RunMode}\n+import org.apache.spark.deploy.yarn.config._\n+import org.apache.spark.internal.config._\n+\n+private object ProcessType extends Enumeration {\n+  type ProcessType = Value\n+  val DRIVER = Value(\"driver\")\n+  val EXECUTOR = Value(\"executor\")\n+  val AM = Value(\"am\")\n+}\n+\n+private object RunMode extends Enumeration {\n+  type RunMode = Value\n+  val CLIENT = Value(\"client\")\n+  val CLUSTER = Value(\"cluster\")\n+}\n+\n+private object ResourceType extends Enumeration {\n+  type ResourceType = Value\n+  val CORES = Value(\"cores\")\n+  val MEMORY = Value(\"memory\")\n+}\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[ResourceConfigProperties](\n+    new ResourceConfigProperties(AM, Some(CLIENT), MEMORY),\n+    new ResourceConfigProperties(AM, Some(CLIENT), CORES),\n+    new ResourceConfigProperties(DRIVER, Some(CLUSTER), MEMORY),\n+    new ResourceConfigProperties(DRIVER, Some(CLUSTER), CORES),\n+    new ResourceConfigProperties(EXECUTOR, None, MEMORY),\n+    new ResourceConfigProperties(EXECUTOR, None, CORES))\n+\n+  /**\n+   * Validates sparkConf and throws a SparkException if a standard resource (memory or cores)\n+   * is defined with the property spark.yarn.x.resource.y<br>\n+   *\n+   * Example of an invalid config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   *\n+   * Please note that if multiple resources are defined like described above,\n+   * the error messages will be concatenated.<br>\n+   * Example of such a config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   * - spark.yarn.executor.resource.cores=2<br>\n+   * Then the following two error messages will be printed:<br>\n+   * - \"memory cannot be requested with config spark.yarn.driver.resource.memory,\n+   * please use config spark.driver.memory instead!<br>\n+   * - \"cores cannot be requested with config spark.yarn.executor.resource.cores,\n+   * please use config spark.executor.cores instead!<br>\n+   *\n+   * @param sparkConf\n+   */\n+  def validateResources(sparkConf: SparkConf): Unit = {\n+    val requestedResources = new RequestedResources(sparkConf)\n+    val sb = new mutable.StringBuilder()\n+    POSSIBLE_RESOURCE_DEFINITIONS.foreach { rcp =>\n+      val customResources: Map[String, String] = getCustomResourceValue(requestedResources, rcp)\n+      val (standardResourceConfigKey: String, customResourceConfigKey: String) =\n+        getResourceConfigKeys(rcp)\n+\n+      val errorMessage =\n+        if (customResources.contains(customResourceConfigKey)) {\n+          s\"${rcp.resourceType} cannot be requested with config $customResourceConfigKey, \" +\n+              s\"please use config $standardResourceConfigKey instead!\"\n+        } else {\n+          \"\"\n+        }\n+      if (errorMessage.nonEmpty) {\n+        printErrorMessageToBuffer(sb, errorMessage)\n+      }\n+    }\n+\n+    if (sb.nonEmpty) {\n+      throw new SparkException(sb.toString())\n+    }\n+  }\n+\n+  /**\n+   * Returns the requested map of custom resources,\n+   * based on the ResourceConfigProperties argument.\n+   * @return\n+   */\n+  private def getCustomResourceValue(\n+      requestedResources: RequestedResources,\n+      rcp: ResourceConfigProperties): Map[String, String] = {\n+    var customResources: Map[String, String] = null\n+    (rcp.processType, rcp.runMode, rcp.resourceType) match {\n+      case (ProcessType.EXECUTOR, _, _) => customResources =\n+          requestedResources.customExecutorResources\n+      case (ProcessType.AM, Some(RunMode.CLIENT), _) => customResources =\n+          requestedResources.customAMResources\n+      case (ProcessType.DRIVER, Some(RunMode.CLUSTER), _) => customResources =\n+          requestedResources.customDriverResources\n+      case _ => throw new SparkException(s\"ResourceConfigProperties contains an \" +\n+          s\"invalid combination of fields: $rcp\")\n+    }\n+    customResources\n+  }\n+\n+  /**\n+   * Returns a tuple of standard resource config key and custom resource config key, based on the\n+   * processType and runMode fields of the ResourceConfigProperties argument.\n+   * Standard resources are memory and cores.\n+   *\n+   * @param rcp\n+   * @return\n+   */\n+  private def getResourceConfigKeys(rcp: ResourceConfigProperties): (String, String) = {\n+    val standardResourceConfigKey: String = if (rcp.processType == ProcessType.AM) {"
  }, {
    "author": {
      "login": "szilard-nemeth"
    },
    "body": "Good to know, removed the type from here.",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-09-17T13:59:59Z",
    "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.ProcessType.{AM, DRIVER, EXECUTOR, ProcessType}\n+import org.apache.spark.deploy.yarn.ResourceType.{CORES, MEMORY, ResourceType}\n+import org.apache.spark.deploy.yarn.RunMode.{CLIENT, CLUSTER, RunMode}\n+import org.apache.spark.deploy.yarn.config._\n+import org.apache.spark.internal.config._\n+\n+private object ProcessType extends Enumeration {\n+  type ProcessType = Value\n+  val DRIVER = Value(\"driver\")\n+  val EXECUTOR = Value(\"executor\")\n+  val AM = Value(\"am\")\n+}\n+\n+private object RunMode extends Enumeration {\n+  type RunMode = Value\n+  val CLIENT = Value(\"client\")\n+  val CLUSTER = Value(\"cluster\")\n+}\n+\n+private object ResourceType extends Enumeration {\n+  type ResourceType = Value\n+  val CORES = Value(\"cores\")\n+  val MEMORY = Value(\"memory\")\n+}\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[ResourceConfigProperties](\n+    new ResourceConfigProperties(AM, Some(CLIENT), MEMORY),\n+    new ResourceConfigProperties(AM, Some(CLIENT), CORES),\n+    new ResourceConfigProperties(DRIVER, Some(CLUSTER), MEMORY),\n+    new ResourceConfigProperties(DRIVER, Some(CLUSTER), CORES),\n+    new ResourceConfigProperties(EXECUTOR, None, MEMORY),\n+    new ResourceConfigProperties(EXECUTOR, None, CORES))\n+\n+  /**\n+   * Validates sparkConf and throws a SparkException if a standard resource (memory or cores)\n+   * is defined with the property spark.yarn.x.resource.y<br>\n+   *\n+   * Example of an invalid config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   *\n+   * Please note that if multiple resources are defined like described above,\n+   * the error messages will be concatenated.<br>\n+   * Example of such a config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   * - spark.yarn.executor.resource.cores=2<br>\n+   * Then the following two error messages will be printed:<br>\n+   * - \"memory cannot be requested with config spark.yarn.driver.resource.memory,\n+   * please use config spark.driver.memory instead!<br>\n+   * - \"cores cannot be requested with config spark.yarn.executor.resource.cores,\n+   * please use config spark.executor.cores instead!<br>\n+   *\n+   * @param sparkConf\n+   */\n+  def validateResources(sparkConf: SparkConf): Unit = {\n+    val requestedResources = new RequestedResources(sparkConf)\n+    val sb = new mutable.StringBuilder()\n+    POSSIBLE_RESOURCE_DEFINITIONS.foreach { rcp =>\n+      val customResources: Map[String, String] = getCustomResourceValue(requestedResources, rcp)\n+      val (standardResourceConfigKey: String, customResourceConfigKey: String) =\n+        getResourceConfigKeys(rcp)\n+\n+      val errorMessage =\n+        if (customResources.contains(customResourceConfigKey)) {\n+          s\"${rcp.resourceType} cannot be requested with config $customResourceConfigKey, \" +\n+              s\"please use config $standardResourceConfigKey instead!\"\n+        } else {\n+          \"\"\n+        }\n+      if (errorMessage.nonEmpty) {\n+        printErrorMessageToBuffer(sb, errorMessage)\n+      }\n+    }\n+\n+    if (sb.nonEmpty) {\n+      throw new SparkException(sb.toString())\n+    }\n+  }\n+\n+  /**\n+   * Returns the requested map of custom resources,\n+   * based on the ResourceConfigProperties argument.\n+   * @return\n+   */\n+  private def getCustomResourceValue(\n+      requestedResources: RequestedResources,\n+      rcp: ResourceConfigProperties): Map[String, String] = {\n+    var customResources: Map[String, String] = null\n+    (rcp.processType, rcp.runMode, rcp.resourceType) match {\n+      case (ProcessType.EXECUTOR, _, _) => customResources =\n+          requestedResources.customExecutorResources\n+      case (ProcessType.AM, Some(RunMode.CLIENT), _) => customResources =\n+          requestedResources.customAMResources\n+      case (ProcessType.DRIVER, Some(RunMode.CLUSTER), _) => customResources =\n+          requestedResources.customDriverResources\n+      case _ => throw new SparkException(s\"ResourceConfigProperties contains an \" +\n+          s\"invalid combination of fields: $rcp\")\n+    }\n+    customResources\n+  }\n+\n+  /**\n+   * Returns a tuple of standard resource config key and custom resource config key, based on the\n+   * processType and runMode fields of the ResourceConfigProperties argument.\n+   * Standard resources are memory and cores.\n+   *\n+   * @param rcp\n+   * @return\n+   */\n+  private def getResourceConfigKeys(rcp: ResourceConfigProperties): (String, String) = {\n+    val standardResourceConfigKey: String = if (rcp.processType == ProcessType.AM) {"
  }],
  "prId": 20761
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Nit: I think we have avoided this syntax in the project except in a few limited cases. I'd just write `key.contains(\"am\")` etc and avoid the compiler warning.",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-09-18T15:18:48Z",
    "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.config._\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[(String, String)](\n+    (\"spark.yarn.am.memory\", YARN_AM_RESOURCE_TYPES_PREFIX + \"memory\"),\n+    (\"spark.yarn.am.cores\", YARN_AM_RESOURCE_TYPES_PREFIX + \"cores\"),\n+    (\"spark.driver.memory\", YARN_DRIVER_RESOURCE_TYPES_PREFIX + \"memory\"),\n+    (\"spark.driver.cores\", YARN_DRIVER_RESOURCE_TYPES_PREFIX + \"cores\"),\n+    (\"spark.executor.memory\", YARN_EXECUTOR_RESOURCE_TYPES_PREFIX + \"memory\"),\n+    (\"spark.executor.cores\", YARN_EXECUTOR_RESOURCE_TYPES_PREFIX + \"cores\"))\n+\n+  /**\n+   * Validates sparkConf and throws a SparkException if a standard resource (memory or cores)\n+   * is defined with the property spark.yarn.x.resource.y<br>\n+   *\n+   * Example of an invalid config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   *\n+   * Please note that if multiple resources are defined like described above,\n+   * the error messages will be concatenated.<br>\n+   * Example of such a config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   * - spark.yarn.executor.resource.cores=2<br>\n+   * Then the following two error messages will be printed:<br>\n+   * - \"memory cannot be requested with config spark.yarn.driver.resource.memory,\n+   * please use config spark.driver.memory instead!<br>\n+   * - \"cores cannot be requested with config spark.yarn.executor.resource.cores,\n+   * please use config spark.executor.cores instead!<br>\n+   *\n+   * @param sparkConf\n+   */\n+  def validateResources(sparkConf: SparkConf): Unit = {\n+    val overallErrorMessage = new mutable.StringBuilder()\n+    POSSIBLE_RESOURCE_DEFINITIONS.foreach { resdef =>\n+      val customResources: Map[String, String] =\n+        getCustomResourceValuesForKey(sparkConf, resdef._1)\n+\n+      val errorMessage = getErrorMessage(customResources, resdef._1, resdef._2)\n+      if (errorMessage.nonEmpty) {\n+        overallErrorMessage.append(s\"$ERROR_PREFIX$errorMessage\\n\")\n+      }\n+    }\n+\n+    // throw exception after loop\n+    if (overallErrorMessage.nonEmpty) {\n+      throw new SparkException(overallErrorMessage.toString())\n+    }\n+  }\n+\n+  def getCustomResourceValuesForKey(sparkConf: SparkConf, key: String): Map[String, String] = {\n+    if (key contains \"am\") {"
  }, {
    "author": {
      "login": "szilard-nemeth"
    },
    "body": "Thanks for pointing this out.\r\nActually, I searched in the code before using this syntax and I realized it is used, but you are right, it's getting used in fewer places than the syntax you suggested.",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-09-19T11:07:15Z",
    "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.config._\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[(String, String)](\n+    (\"spark.yarn.am.memory\", YARN_AM_RESOURCE_TYPES_PREFIX + \"memory\"),\n+    (\"spark.yarn.am.cores\", YARN_AM_RESOURCE_TYPES_PREFIX + \"cores\"),\n+    (\"spark.driver.memory\", YARN_DRIVER_RESOURCE_TYPES_PREFIX + \"memory\"),\n+    (\"spark.driver.cores\", YARN_DRIVER_RESOURCE_TYPES_PREFIX + \"cores\"),\n+    (\"spark.executor.memory\", YARN_EXECUTOR_RESOURCE_TYPES_PREFIX + \"memory\"),\n+    (\"spark.executor.cores\", YARN_EXECUTOR_RESOURCE_TYPES_PREFIX + \"cores\"))\n+\n+  /**\n+   * Validates sparkConf and throws a SparkException if a standard resource (memory or cores)\n+   * is defined with the property spark.yarn.x.resource.y<br>\n+   *\n+   * Example of an invalid config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   *\n+   * Please note that if multiple resources are defined like described above,\n+   * the error messages will be concatenated.<br>\n+   * Example of such a config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   * - spark.yarn.executor.resource.cores=2<br>\n+   * Then the following two error messages will be printed:<br>\n+   * - \"memory cannot be requested with config spark.yarn.driver.resource.memory,\n+   * please use config spark.driver.memory instead!<br>\n+   * - \"cores cannot be requested with config spark.yarn.executor.resource.cores,\n+   * please use config spark.executor.cores instead!<br>\n+   *\n+   * @param sparkConf\n+   */\n+  def validateResources(sparkConf: SparkConf): Unit = {\n+    val overallErrorMessage = new mutable.StringBuilder()\n+    POSSIBLE_RESOURCE_DEFINITIONS.foreach { resdef =>\n+      val customResources: Map[String, String] =\n+        getCustomResourceValuesForKey(sparkConf, resdef._1)\n+\n+      val errorMessage = getErrorMessage(customResources, resdef._1, resdef._2)\n+      if (errorMessage.nonEmpty) {\n+        overallErrorMessage.append(s\"$ERROR_PREFIX$errorMessage\\n\")\n+      }\n+    }\n+\n+    // throw exception after loop\n+    if (overallErrorMessage.nonEmpty) {\n+      throw new SparkException(overallErrorMessage.toString())\n+    }\n+  }\n+\n+  def getCustomResourceValuesForKey(sparkConf: SparkConf, key: String): Map[String, String] = {\n+    if (key contains \"am\") {"
  }],
  "prId": 20761
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Do you need to duplicate the type here?",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-09-18T15:19:12Z",
    "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.config._\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[(String, String)](\n+    (\"spark.yarn.am.memory\", YARN_AM_RESOURCE_TYPES_PREFIX + \"memory\"),\n+    (\"spark.yarn.am.cores\", YARN_AM_RESOURCE_TYPES_PREFIX + \"cores\"),\n+    (\"spark.driver.memory\", YARN_DRIVER_RESOURCE_TYPES_PREFIX + \"memory\"),\n+    (\"spark.driver.cores\", YARN_DRIVER_RESOURCE_TYPES_PREFIX + \"cores\"),\n+    (\"spark.executor.memory\", YARN_EXECUTOR_RESOURCE_TYPES_PREFIX + \"memory\"),\n+    (\"spark.executor.cores\", YARN_EXECUTOR_RESOURCE_TYPES_PREFIX + \"cores\"))\n+\n+  /**\n+   * Validates sparkConf and throws a SparkException if a standard resource (memory or cores)\n+   * is defined with the property spark.yarn.x.resource.y<br>\n+   *\n+   * Example of an invalid config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   *\n+   * Please note that if multiple resources are defined like described above,\n+   * the error messages will be concatenated.<br>\n+   * Example of such a config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   * - spark.yarn.executor.resource.cores=2<br>\n+   * Then the following two error messages will be printed:<br>\n+   * - \"memory cannot be requested with config spark.yarn.driver.resource.memory,\n+   * please use config spark.driver.memory instead!<br>\n+   * - \"cores cannot be requested with config spark.yarn.executor.resource.cores,\n+   * please use config spark.executor.cores instead!<br>\n+   *\n+   * @param sparkConf\n+   */\n+  def validateResources(sparkConf: SparkConf): Unit = {\n+    val overallErrorMessage = new mutable.StringBuilder()\n+    POSSIBLE_RESOURCE_DEFINITIONS.foreach { resdef =>\n+      val customResources: Map[String, String] =\n+        getCustomResourceValuesForKey(sparkConf, resdef._1)\n+\n+      val errorMessage = getErrorMessage(customResources, resdef._1, resdef._2)\n+      if (errorMessage.nonEmpty) {\n+        overallErrorMessage.append(s\"$ERROR_PREFIX$errorMessage\\n\")\n+      }\n+    }\n+\n+    // throw exception after loop\n+    if (overallErrorMessage.nonEmpty) {\n+      throw new SparkException(overallErrorMessage.toString())\n+    }\n+  }\n+\n+  def getCustomResourceValuesForKey(sparkConf: SparkConf, key: String): Map[String, String] = {\n+    if (key contains \"am\") {\n+      extractCustomResources(sparkConf, YARN_AM_RESOURCE_TYPES_PREFIX)\n+    } else if (key contains \"driver\") {\n+      extractCustomResources(sparkConf, YARN_DRIVER_RESOURCE_TYPES_PREFIX)\n+    } else if (key contains \"executor\") {\n+      extractCustomResources(sparkConf, YARN_EXECUTOR_RESOURCE_TYPES_PREFIX)\n+    } else Map.empty[String, String]\n+  }\n+\n+  def getErrorMessage(\n+      customResources: Map[String, String],\n+      standardResourceConfigKey: String,\n+      customResourceConfigKey: String): String = {\n+      if (customResources.contains(customResourceConfigKey)) {\n+        val idx = standardResourceConfigKey.lastIndexOf(\".\")\n+        val resourceType = standardResourceConfigKey.substring(idx + 1)\n+        s\"$resourceType cannot be requested with config $customResourceConfigKey, \" +\n+            s\"please use config $standardResourceConfigKey instead!\"\n+      } else {\n+        \"\"\n+      }\n+  }\n+\n+  private def extractCustomResources(\n+      sparkConf: SparkConf,\n+      propertyPrefix: String): Map[String, String] = {\n+    val result: collection.mutable.HashMap[String, String] ="
  }, {
    "author": {
      "login": "szilard-nemeth"
    },
    "body": "I guess you meant to remove the type declaration from 'result', so I removed it.",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-09-19T11:13:06Z",
    "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.config._\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[(String, String)](\n+    (\"spark.yarn.am.memory\", YARN_AM_RESOURCE_TYPES_PREFIX + \"memory\"),\n+    (\"spark.yarn.am.cores\", YARN_AM_RESOURCE_TYPES_PREFIX + \"cores\"),\n+    (\"spark.driver.memory\", YARN_DRIVER_RESOURCE_TYPES_PREFIX + \"memory\"),\n+    (\"spark.driver.cores\", YARN_DRIVER_RESOURCE_TYPES_PREFIX + \"cores\"),\n+    (\"spark.executor.memory\", YARN_EXECUTOR_RESOURCE_TYPES_PREFIX + \"memory\"),\n+    (\"spark.executor.cores\", YARN_EXECUTOR_RESOURCE_TYPES_PREFIX + \"cores\"))\n+\n+  /**\n+   * Validates sparkConf and throws a SparkException if a standard resource (memory or cores)\n+   * is defined with the property spark.yarn.x.resource.y<br>\n+   *\n+   * Example of an invalid config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   *\n+   * Please note that if multiple resources are defined like described above,\n+   * the error messages will be concatenated.<br>\n+   * Example of such a config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   * - spark.yarn.executor.resource.cores=2<br>\n+   * Then the following two error messages will be printed:<br>\n+   * - \"memory cannot be requested with config spark.yarn.driver.resource.memory,\n+   * please use config spark.driver.memory instead!<br>\n+   * - \"cores cannot be requested with config spark.yarn.executor.resource.cores,\n+   * please use config spark.executor.cores instead!<br>\n+   *\n+   * @param sparkConf\n+   */\n+  def validateResources(sparkConf: SparkConf): Unit = {\n+    val overallErrorMessage = new mutable.StringBuilder()\n+    POSSIBLE_RESOURCE_DEFINITIONS.foreach { resdef =>\n+      val customResources: Map[String, String] =\n+        getCustomResourceValuesForKey(sparkConf, resdef._1)\n+\n+      val errorMessage = getErrorMessage(customResources, resdef._1, resdef._2)\n+      if (errorMessage.nonEmpty) {\n+        overallErrorMessage.append(s\"$ERROR_PREFIX$errorMessage\\n\")\n+      }\n+    }\n+\n+    // throw exception after loop\n+    if (overallErrorMessage.nonEmpty) {\n+      throw new SparkException(overallErrorMessage.toString())\n+    }\n+  }\n+\n+  def getCustomResourceValuesForKey(sparkConf: SparkConf, key: String): Map[String, String] = {\n+    if (key contains \"am\") {\n+      extractCustomResources(sparkConf, YARN_AM_RESOURCE_TYPES_PREFIX)\n+    } else if (key contains \"driver\") {\n+      extractCustomResources(sparkConf, YARN_DRIVER_RESOURCE_TYPES_PREFIX)\n+    } else if (key contains \"executor\") {\n+      extractCustomResources(sparkConf, YARN_EXECUTOR_RESOURCE_TYPES_PREFIX)\n+    } else Map.empty[String, String]\n+  }\n+\n+  def getErrorMessage(\n+      customResources: Map[String, String],\n+      standardResourceConfigKey: String,\n+      customResourceConfigKey: String): String = {\n+      if (customResources.contains(customResourceConfigKey)) {\n+        val idx = standardResourceConfigKey.lastIndexOf(\".\")\n+        val resourceType = standardResourceConfigKey.substring(idx + 1)\n+        s\"$resourceType cannot be requested with config $customResourceConfigKey, \" +\n+            s\"please use config $standardResourceConfigKey instead!\"\n+      } else {\n+        \"\"\n+      }\n+  }\n+\n+  private def extractCustomResources(\n+      sparkConf: SparkConf,\n+      propertyPrefix: String): Map[String, String] = {\n+    val result: collection.mutable.HashMap[String, String] ="
  }],
  "prId": 20761
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Use config constants (e.g. `AM_MEMORY.key` for this one).",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-09-24T23:45:06Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.config._\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[(String, String)](\n+    (\"spark.yarn.am.memory\", YARN_AM_RESOURCE_TYPES_PREFIX + \"memory\"),"
  }, {
    "author": {
      "login": "szilard-nemeth"
    },
    "body": "fixed",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-10-02T02:44:17Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.config._\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[(String, String)](\n+    (\"spark.yarn.am.memory\", YARN_AM_RESOURCE_TYPES_PREFIX + \"memory\"),"
  }],
  "prId": 20761
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`.foreach { case (sparkName, resourceRequest) =>`",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-09-24T23:45:35Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.config._\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[(String, String)](\n+    (\"spark.yarn.am.memory\", YARN_AM_RESOURCE_TYPES_PREFIX + \"memory\"),\n+    (\"spark.yarn.am.cores\", YARN_AM_RESOURCE_TYPES_PREFIX + \"cores\"),\n+    (\"spark.driver.memory\", YARN_DRIVER_RESOURCE_TYPES_PREFIX + \"memory\"),\n+    (\"spark.driver.cores\", YARN_DRIVER_RESOURCE_TYPES_PREFIX + \"cores\"),\n+    (\"spark.executor.memory\", YARN_EXECUTOR_RESOURCE_TYPES_PREFIX + \"memory\"),\n+    (\"spark.executor.cores\", YARN_EXECUTOR_RESOURCE_TYPES_PREFIX + \"cores\"))\n+\n+  /**\n+   * Validates sparkConf and throws a SparkException if a standard resource (memory or cores)\n+   * is defined with the property spark.yarn.x.resource.y<br>\n+   *\n+   * Example of an invalid config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   *\n+   * Please note that if multiple resources are defined like described above,\n+   * the error messages will be concatenated.<br>\n+   * Example of such a config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   * - spark.yarn.executor.resource.cores=2<br>\n+   * Then the following two error messages will be printed:<br>\n+   * - \"memory cannot be requested with config spark.yarn.driver.resource.memory,\n+   * please use config spark.driver.memory instead!<br>\n+   * - \"cores cannot be requested with config spark.yarn.executor.resource.cores,\n+   * please use config spark.executor.cores instead!<br>\n+   *\n+   * @param sparkConf\n+   */\n+  def validateResources(sparkConf: SparkConf): Unit = {\n+    val overallErrorMessage = new mutable.StringBuilder()\n+    POSSIBLE_RESOURCE_DEFINITIONS.foreach { resdef =>"
  }, {
    "author": {
      "login": "szilard-nemeth"
    },
    "body": "Yes, it's really more readable with this. fixed.",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-10-02T02:56:48Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.config._\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[(String, String)](\n+    (\"spark.yarn.am.memory\", YARN_AM_RESOURCE_TYPES_PREFIX + \"memory\"),\n+    (\"spark.yarn.am.cores\", YARN_AM_RESOURCE_TYPES_PREFIX + \"cores\"),\n+    (\"spark.driver.memory\", YARN_DRIVER_RESOURCE_TYPES_PREFIX + \"memory\"),\n+    (\"spark.driver.cores\", YARN_DRIVER_RESOURCE_TYPES_PREFIX + \"cores\"),\n+    (\"spark.executor.memory\", YARN_EXECUTOR_RESOURCE_TYPES_PREFIX + \"memory\"),\n+    (\"spark.executor.cores\", YARN_EXECUTOR_RESOURCE_TYPES_PREFIX + \"cores\"))\n+\n+  /**\n+   * Validates sparkConf and throws a SparkException if a standard resource (memory or cores)\n+   * is defined with the property spark.yarn.x.resource.y<br>\n+   *\n+   * Example of an invalid config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   *\n+   * Please note that if multiple resources are defined like described above,\n+   * the error messages will be concatenated.<br>\n+   * Example of such a config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   * - spark.yarn.executor.resource.cores=2<br>\n+   * Then the following two error messages will be printed:<br>\n+   * - \"memory cannot be requested with config spark.yarn.driver.resource.memory,\n+   * please use config spark.driver.memory instead!<br>\n+   * - \"cores cannot be requested with config spark.yarn.executor.resource.cores,\n+   * please use config spark.executor.cores instead!<br>\n+   *\n+   * @param sparkConf\n+   */\n+  def validateResources(sparkConf: SparkConf): Unit = {\n+    val overallErrorMessage = new mutable.StringBuilder()\n+    POSSIBLE_RESOURCE_DEFINITIONS.foreach { resdef =>"
  }],
  "prId": 20761
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "I'm going to ask the same question I've asked before: why is this so complicated?\r\n\r\nWhy do you have to parse the config into a map for every key you're checking, instead of just checking if the illegal key exists in the configuration?\r\n\r\ne.g. with the names I suggested above:\r\n\r\n```\r\nif (sparkConf.contains(resourceRequest)) {\r\n  errorMessage.append(s\"Don't use $resourceRequest; use $sparkName instead.\")\r\n}\r\n```\r\n\r\nThat's 3 lines of code, has all the info you need, and doesn't parse the config over an over again on each iteration. And is also what I've been suggesting since my first review of this class.",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-09-24T23:48:20Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.config._\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[(String, String)](\n+    (\"spark.yarn.am.memory\", YARN_AM_RESOURCE_TYPES_PREFIX + \"memory\"),\n+    (\"spark.yarn.am.cores\", YARN_AM_RESOURCE_TYPES_PREFIX + \"cores\"),\n+    (\"spark.driver.memory\", YARN_DRIVER_RESOURCE_TYPES_PREFIX + \"memory\"),\n+    (\"spark.driver.cores\", YARN_DRIVER_RESOURCE_TYPES_PREFIX + \"cores\"),\n+    (\"spark.executor.memory\", YARN_EXECUTOR_RESOURCE_TYPES_PREFIX + \"memory\"),\n+    (\"spark.executor.cores\", YARN_EXECUTOR_RESOURCE_TYPES_PREFIX + \"cores\"))\n+\n+  /**\n+   * Validates sparkConf and throws a SparkException if a standard resource (memory or cores)\n+   * is defined with the property spark.yarn.x.resource.y<br>\n+   *\n+   * Example of an invalid config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   *\n+   * Please note that if multiple resources are defined like described above,\n+   * the error messages will be concatenated.<br>\n+   * Example of such a config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   * - spark.yarn.executor.resource.cores=2<br>\n+   * Then the following two error messages will be printed:<br>\n+   * - \"memory cannot be requested with config spark.yarn.driver.resource.memory,\n+   * please use config spark.driver.memory instead!<br>\n+   * - \"cores cannot be requested with config spark.yarn.executor.resource.cores,\n+   * please use config spark.executor.cores instead!<br>\n+   *\n+   * @param sparkConf\n+   */\n+  def validateResources(sparkConf: SparkConf): Unit = {\n+    val overallErrorMessage = new mutable.StringBuilder()\n+    POSSIBLE_RESOURCE_DEFINITIONS.foreach { resdef =>\n+      val customResources: Map[String, String] ="
  }, {
    "author": {
      "login": "szilard-nemeth"
    },
    "body": "Yeah, you are right, actually. \r\nSince we had many iterations and changes of this class this became clear for me finally that I can simplify this even more as you suggested. \r\n",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-10-02T04:22:19Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.config._\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[(String, String)](\n+    (\"spark.yarn.am.memory\", YARN_AM_RESOURCE_TYPES_PREFIX + \"memory\"),\n+    (\"spark.yarn.am.cores\", YARN_AM_RESOURCE_TYPES_PREFIX + \"cores\"),\n+    (\"spark.driver.memory\", YARN_DRIVER_RESOURCE_TYPES_PREFIX + \"memory\"),\n+    (\"spark.driver.cores\", YARN_DRIVER_RESOURCE_TYPES_PREFIX + \"cores\"),\n+    (\"spark.executor.memory\", YARN_EXECUTOR_RESOURCE_TYPES_PREFIX + \"memory\"),\n+    (\"spark.executor.cores\", YARN_EXECUTOR_RESOURCE_TYPES_PREFIX + \"cores\"))\n+\n+  /**\n+   * Validates sparkConf and throws a SparkException if a standard resource (memory or cores)\n+   * is defined with the property spark.yarn.x.resource.y<br>\n+   *\n+   * Example of an invalid config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   *\n+   * Please note that if multiple resources are defined like described above,\n+   * the error messages will be concatenated.<br>\n+   * Example of such a config:<br>\n+   * - spark.yarn.driver.resource.memory=2g<br>\n+   * - spark.yarn.executor.resource.cores=2<br>\n+   * Then the following two error messages will be printed:<br>\n+   * - \"memory cannot be requested with config spark.yarn.driver.resource.memory,\n+   * please use config spark.driver.memory instead!<br>\n+   * - \"cores cannot be requested with config spark.yarn.executor.resource.cores,\n+   * please use config spark.executor.cores instead!<br>\n+   *\n+   * @param sparkConf\n+   */\n+  def validateResources(sparkConf: SparkConf): Unit = {\n+    val overallErrorMessage = new mutable.StringBuilder()\n+    POSSIBLE_RESOURCE_DEFINITIONS.foreach { resdef =>\n+      val customResources: Map[String, String] ="
  }],
  "prId": 20761
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This really should be inside the validate method, since it's not needed anywhere else.",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-09-24T23:50:34Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.config._\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[(String, String)]("
  }, {
    "author": {
      "login": "szilard-nemeth"
    },
    "body": "Fixed.",
    "commit": "dc2e382ff1e468f7e54e14a12fdfcf983b70ea0f",
    "createdAt": "2018-10-02T02:57:59Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.deploy.yarn.config._\n+\n+private object ResourceTypeValidator {\n+  private val ERROR_PREFIX: String = \"Error: \"\n+  private val POSSIBLE_RESOURCE_DEFINITIONS = Seq[(String, String)]("
  }],
  "prId": 20761
}]