[{
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "why is this called \"sum\"?  I think the old name `failedExecutorTimestamps` is more appropriate, same for the other places you added \"sum\"",
    "commit": "f71c7c547f173c902eec101d510c87d50c7abb86",
    "createdAt": "2018-04-13T21:22:27Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.{Clock, SystemClock}\n+\n+private[spark] class FailureWithinTimeIntervalTracker(sparkConf: SparkConf) extends Logging {\n+\n+  private var clock: Clock = new SystemClock\n+\n+  private val executorFailuresValidityInterval =\n+    sparkConf.get(config.EXECUTOR_ATTEMPT_FAILURE_VALIDITY_INTERVAL_MS).getOrElse(-1L)\n+\n+  // Queue to store the timestamp of failed executors for each host\n+  private val failedExecutorsTimeStampsPerHost = mutable.Map[String, mutable.Queue[Long]]()\n+\n+  private val sumFailedExecutorsTimeStamps = new mutable.Queue[Long]()"
  }],
  "prId": 21068
}, {
  "comments": [{
    "author": {
      "login": "henryr"
    },
    "body": "It's counter-intuitive that this `get*` method mutates state. If I called \r\n\r\n    getNumFailuresWithinValidityInterval(foo, 0)\r\n    getNumFailuresWithinValidityInterval(foo, 10)\r\n    getNumFailuresWithinValidityInterval(foo, 0)\r\n\r\nThe last call can return something different from the first because all the failures that weren't within `10 - executorFailuresValidityInterval` will have been dropped. ",
    "commit": "f71c7c547f173c902eec101d510c87d50c7abb86",
    "createdAt": "2018-04-16T23:06:35Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.{Clock, SystemClock}\n+\n+private[spark] class FailureWithinTimeIntervalTracker(sparkConf: SparkConf) extends Logging {\n+\n+  private var clock: Clock = new SystemClock\n+\n+  private val executorFailuresValidityInterval =\n+    sparkConf.get(config.EXECUTOR_ATTEMPT_FAILURE_VALIDITY_INTERVAL_MS).getOrElse(-1L)\n+\n+  // Queue to store the timestamp of failed executors for each host\n+  private val failedExecutorsTimeStampsPerHost = mutable.Map[String, mutable.Queue[Long]]()\n+\n+  private val failedExecutorsTimeStamps = new mutable.Queue[Long]()\n+\n+  private def getNumFailuresWithinValidityInterval(\n+      failedExecutorsTimeStampsForHost: mutable.Queue[Long],\n+      endTime: Long): Int = {\n+    while (executorFailuresValidityInterval > 0\n+      && failedExecutorsTimeStampsForHost.nonEmpty\n+      && failedExecutorsTimeStampsForHost.head < endTime - executorFailuresValidityInterval) {\n+      failedExecutorsTimeStampsForHost.dequeue()"
  }, {
    "author": {
      "login": "attilapiros"
    },
    "body": "Ok, I will take your recommendation and drop endTime with renaming the method to  getRecentFailureCount.",
    "commit": "f71c7c547f173c902eec101d510c87d50c7abb86",
    "createdAt": "2018-04-18T13:05:35Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.{Clock, SystemClock}\n+\n+private[spark] class FailureWithinTimeIntervalTracker(sparkConf: SparkConf) extends Logging {\n+\n+  private var clock: Clock = new SystemClock\n+\n+  private val executorFailuresValidityInterval =\n+    sparkConf.get(config.EXECUTOR_ATTEMPT_FAILURE_VALIDITY_INTERVAL_MS).getOrElse(-1L)\n+\n+  // Queue to store the timestamp of failed executors for each host\n+  private val failedExecutorsTimeStampsPerHost = mutable.Map[String, mutable.Queue[Long]]()\n+\n+  private val failedExecutorsTimeStamps = new mutable.Queue[Long]()\n+\n+  private def getNumFailuresWithinValidityInterval(\n+      failedExecutorsTimeStampsForHost: mutable.Queue[Long],\n+      endTime: Long): Int = {\n+    while (executorFailuresValidityInterval > 0\n+      && failedExecutorsTimeStampsForHost.nonEmpty\n+      && failedExecutorsTimeStampsForHost.head < endTime - executorFailuresValidityInterval) {\n+      failedExecutorsTimeStampsForHost.dequeue()"
  }],
  "prId": 21068
}, {
  "comments": [{
    "author": {
      "login": "henryr"
    },
    "body": "This relies on the fact the `clock` is monotonic, but if it's a `SystemClock` it's based on `System.currentTimeMillis()` which is not monotonic and can time-travel. ",
    "commit": "f71c7c547f173c902eec101d510c87d50c7abb86",
    "createdAt": "2018-04-16T23:11:29Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.{Clock, SystemClock}\n+\n+private[spark] class FailureWithinTimeIntervalTracker(sparkConf: SparkConf) extends Logging {\n+\n+  private var clock: Clock = new SystemClock\n+\n+  private val executorFailuresValidityInterval =\n+    sparkConf.get(config.EXECUTOR_ATTEMPT_FAILURE_VALIDITY_INTERVAL_MS).getOrElse(-1L)\n+\n+  // Queue to store the timestamp of failed executors for each host\n+  private val failedExecutorsTimeStampsPerHost = mutable.Map[String, mutable.Queue[Long]]()\n+\n+  private val failedExecutorsTimeStamps = new mutable.Queue[Long]()\n+\n+  private def getNumFailuresWithinValidityInterval(\n+      failedExecutorsTimeStampsForHost: mutable.Queue[Long],\n+      endTime: Long): Int = {\n+    while (executorFailuresValidityInterval > 0\n+      && failedExecutorsTimeStampsForHost.nonEmpty\n+      && failedExecutorsTimeStampsForHost.head < endTime - executorFailuresValidityInterval) {"
  }, {
    "author": {
      "login": "attilapiros"
    },
    "body": "This code is coming from [YarnAllocator.scala#L175](https://github.com/apache/spark/blob/master/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocator.scala#L175).\r\n\r\nAs I see it the common solution to use Clock, SystemClock and ManualClock in Spark. And here the validity time is much higher then the diff NTP can apply.",
    "commit": "f71c7c547f173c902eec101d510c87d50c7abb86",
    "createdAt": "2018-04-18T12:44:28Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.{Clock, SystemClock}\n+\n+private[spark] class FailureWithinTimeIntervalTracker(sparkConf: SparkConf) extends Logging {\n+\n+  private var clock: Clock = new SystemClock\n+\n+  private val executorFailuresValidityInterval =\n+    sparkConf.get(config.EXECUTOR_ATTEMPT_FAILURE_VALIDITY_INTERVAL_MS).getOrElse(-1L)\n+\n+  // Queue to store the timestamp of failed executors for each host\n+  private val failedExecutorsTimeStampsPerHost = mutable.Map[String, mutable.Queue[Long]]()\n+\n+  private val failedExecutorsTimeStamps = new mutable.Queue[Long]()\n+\n+  private def getNumFailuresWithinValidityInterval(\n+      failedExecutorsTimeStampsForHost: mutable.Queue[Long],\n+      endTime: Long): Int = {\n+    while (executorFailuresValidityInterval > 0\n+      && failedExecutorsTimeStampsForHost.nonEmpty\n+      && failedExecutorsTimeStampsForHost.head < endTime - executorFailuresValidityInterval) {"
  }],
  "prId": 21068
}, {
  "comments": [{
    "author": {
      "login": "henryr"
    },
    "body": "It's not really clear what a 'validity interval' is. I think it means that only failures that have happened recently are considered valid? I think it would be clearer to call this `getNumFailuresSince()`, or `getRecentFailureCount()` or similar, and explicitly pass in the timestamp the caller wants to consider failures since. \r\n\r\nIf you do the latter, and drop the `endTime` argument, then you partly address the issue I raise below about how this mutates state, because `getRecentFailureCount()` suggests more clearly that it's expecting to take into account the current time. ",
    "commit": "f71c7c547f173c902eec101d510c87d50c7abb86",
    "createdAt": "2018-04-16T23:15:06Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.{Clock, SystemClock}\n+\n+private[spark] class FailureWithinTimeIntervalTracker(sparkConf: SparkConf) extends Logging {\n+\n+  private var clock: Clock = new SystemClock\n+\n+  private val executorFailuresValidityInterval =\n+    sparkConf.get(config.EXECUTOR_ATTEMPT_FAILURE_VALIDITY_INTERVAL_MS).getOrElse(-1L)\n+\n+  // Queue to store the timestamp of failed executors for each host\n+  private val failedExecutorsTimeStampsPerHost = mutable.Map[String, mutable.Queue[Long]]()\n+\n+  private val failedExecutorsTimeStamps = new mutable.Queue[Long]()\n+\n+  private def getNumFailuresWithinValidityInterval("
  }, {
    "author": {
      "login": "attilapiros"
    },
    "body": "Thanks then getRecentFailureCount will be the method name without the endTime argument.",
    "commit": "f71c7c547f173c902eec101d510c87d50c7abb86",
    "createdAt": "2018-04-18T13:08:19Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.{Clock, SystemClock}\n+\n+private[spark] class FailureWithinTimeIntervalTracker(sparkConf: SparkConf) extends Logging {\n+\n+  private var clock: Clock = new SystemClock\n+\n+  private val executorFailuresValidityInterval =\n+    sparkConf.get(config.EXECUTOR_ATTEMPT_FAILURE_VALIDITY_INTERVAL_MS).getOrElse(-1L)\n+\n+  // Queue to store the timestamp of failed executors for each host\n+  private val failedExecutorsTimeStampsPerHost = mutable.Map[String, mutable.Queue[Long]]()\n+\n+  private val failedExecutorsTimeStamps = new mutable.Queue[Long]()\n+\n+  private def getNumFailuresWithinValidityInterval("
  }],
  "prId": 21068
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Add scaladoc explaining what this does?",
    "commit": "f71c7c547f173c902eec101d510c87d50c7abb86",
    "createdAt": "2018-04-25T21:32:33Z",
    "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.{Clock, SystemClock}\n+\n+private[spark] class FailureWithinTimeIntervalTracker(sparkConf: SparkConf) extends Logging {"
  }],
  "prId": 21068
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This should be a constructor argument.",
    "commit": "f71c7c547f173c902eec101d510c87d50c7abb86",
    "createdAt": "2018-04-25T21:32:50Z",
    "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.{Clock, SystemClock}\n+\n+private[spark] class FailureWithinTimeIntervalTracker(sparkConf: SparkConf) extends Logging {\n+\n+  private var clock: Clock = new SystemClock"
  }],
  "prId": 21068
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: `&&` goes in previous line, double indent following lines to separate them from body below.",
    "commit": "f71c7c547f173c902eec101d510c87d50c7abb86",
    "createdAt": "2018-04-25T21:43:19Z",
    "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.{Clock, SystemClock}\n+\n+private[spark] class FailureWithinTimeIntervalTracker(sparkConf: SparkConf) extends Logging {\n+\n+  private var clock: Clock = new SystemClock\n+\n+  private val executorFailuresValidityInterval =\n+    sparkConf.get(config.EXECUTOR_ATTEMPT_FAILURE_VALIDITY_INTERVAL_MS).getOrElse(-1L)\n+\n+  // Queue to store the timestamp of failed executors for each host\n+  private val failedExecutorsTimeStampsPerHost = mutable.Map[String, mutable.Queue[Long]]()\n+\n+  private val failedExecutorsTimeStamps = new mutable.Queue[Long]()\n+\n+  private def getRecentFailureCount(failedExecutorsTimeStampsForHost: mutable.Queue[Long]): Int = {\n+    val endTime = clock.getTimeMillis()\n+    while (executorFailuresValidityInterval > 0\n+      && failedExecutorsTimeStampsForHost.nonEmpty"
  }],
  "prId": 21068
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: add braces in multi-line methods.\r\n\r\nAlso, I know the YARN module is generally this way, but this seems like a good chance to start to use the more usual Spark convention of not naming methods with \"get\" (and also take the chance to make the name shorter). e.g., `numFailuresOnHost` or some variation of that.",
    "commit": "f71c7c547f173c902eec101d510c87d50c7abb86",
    "createdAt": "2018-04-25T21:44:50Z",
    "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.{Clock, SystemClock}\n+\n+private[spark] class FailureWithinTimeIntervalTracker(sparkConf: SparkConf) extends Logging {\n+\n+  private var clock: Clock = new SystemClock\n+\n+  private val executorFailuresValidityInterval =\n+    sparkConf.get(config.EXECUTOR_ATTEMPT_FAILURE_VALIDITY_INTERVAL_MS).getOrElse(-1L)\n+\n+  // Queue to store the timestamp of failed executors for each host\n+  private val failedExecutorsTimeStampsPerHost = mutable.Map[String, mutable.Queue[Long]]()\n+\n+  private val failedExecutorsTimeStamps = new mutable.Queue[Long]()\n+\n+  private def getRecentFailureCount(failedExecutorsTimeStampsForHost: mutable.Queue[Long]): Int = {\n+    val endTime = clock.getTimeMillis()\n+    while (executorFailuresValidityInterval > 0\n+      && failedExecutorsTimeStampsForHost.nonEmpty\n+      && failedExecutorsTimeStampsForHost.head < endTime - executorFailuresValidityInterval) {\n+      failedExecutorsTimeStampsForHost.dequeue()\n+    }\n+    failedExecutorsTimeStampsForHost.size\n+  }\n+\n+  /**\n+   * Use a different clock. This is mainly used for testing.\n+   */\n+  def setClock(newClock: Clock): Unit = {\n+    clock = newClock\n+  }\n+\n+  def getNumExecutorsFailed: Int = synchronized {\n+    getRecentFailureCount(failedExecutorsTimeStamps)\n+  }\n+\n+  def registerFailureOnHost(hostname: String): Unit = synchronized {\n+    val timeMillis = clock.getTimeMillis()\n+    failedExecutorsTimeStamps.enqueue(timeMillis)\n+    val failedExecutorsOnHost =\n+      failedExecutorsTimeStampsPerHost.getOrElse(hostname, {\n+        val failureOnHost = mutable.Queue[Long]()\n+        failedExecutorsTimeStampsPerHost.put(hostname, failureOnHost)\n+        failureOnHost\n+      })\n+    failedExecutorsOnHost.enqueue(timeMillis)  }\n+\n+  def registerExecutorFailure(): Unit = synchronized {\n+    val timeMillis = clock.getTimeMillis()\n+    failedExecutorsTimeStamps.enqueue(timeMillis)\n+  }\n+\n+  def getNumExecutorFailuresOnHost(hostname: String): Int ="
  }],
  "prId": 21068
}]