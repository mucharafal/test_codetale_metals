[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: hosts",
    "commit": "f71c7c547f173c902eec101d510c87d50c7abb86",
    "createdAt": "2018-04-30T20:50:24Z",
    "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.{Clock, SystemClock}\n+\n+/**\n+ * FailureTracker is responsible for tracking executor failures both for each host separately\n+ * and for all host altogether."
  }],
  "prId": 21068
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "The goal of having this as a constructor parameter was to make it implicitly a `val` (like the `sparkConf` parameter) and avoid the `setClock` method.",
    "commit": "f71c7c547f173c902eec101d510c87d50c7abb86",
    "createdAt": "2018-04-30T20:50:53Z",
    "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.{Clock, SystemClock}\n+\n+/**\n+ * FailureTracker is responsible for tracking executor failures both for each host separately\n+ * and for all host altogether.\n+*/\n+private[spark] class FailureTracker(\n+    sparkConf: SparkConf,\n+    var clock: Clock = new SystemClock) extends Logging {"
  }, {
    "author": {
      "login": "attilapiros"
    },
    "body": "But in that case we have to extend YarnAllocator constructor with a Clock too as in the YarnAllocatorSuite at the \"window based failure executor counting\" test method a manual clock is used. Of course that can be an optional parameter initialised to SystemClock by default. \r\n",
    "commit": "f71c7c547f173c902eec101d510c87d50c7abb86",
    "createdAt": "2018-05-02T09:08:31Z",
    "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.{Clock, SystemClock}\n+\n+/**\n+ * FailureTracker is responsible for tracking executor failures both for each host separately\n+ * and for all host altogether.\n+*/\n+private[spark] class FailureTracker(\n+    sparkConf: SparkConf,\n+    var clock: Clock = new SystemClock) extends Logging {"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Yes, that would be nice. That's what `FsHistoryProvider` does for example (although it uses two constructors instead of a default value - either is fine really).",
    "commit": "f71c7c547f173c902eec101d510c87d50c7abb86",
    "createdAt": "2018-05-02T19:53:40Z",
    "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.{Clock, SystemClock}\n+\n+/**\n+ * FailureTracker is responsible for tracking executor failures both for each host separately\n+ * and for all host altogether.\n+*/\n+private[spark] class FailureTracker(\n+    sparkConf: SparkConf,\n+    var clock: Clock = new SystemClock) extends Logging {"
  }],
  "prId": 21068
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`numFailedExecutors`",
    "commit": "f71c7c547f173c902eec101d510c87d50c7abb86",
    "createdAt": "2018-04-30T21:01:04Z",
    "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.{Clock, SystemClock}\n+\n+/**\n+ * FailureTracker is responsible for tracking executor failures both for each host separately\n+ * and for all host altogether.\n+*/\n+private[spark] class FailureTracker(\n+    sparkConf: SparkConf,\n+    var clock: Clock = new SystemClock) extends Logging {\n+\n+  private val executorFailuresValidityInterval =\n+    sparkConf.get(config.EXECUTOR_ATTEMPT_FAILURE_VALIDITY_INTERVAL_MS).getOrElse(-1L)\n+\n+  // Queue to store the timestamp of failed executors for each host\n+  private val failedExecutorsTimeStampsPerHost = mutable.Map[String, mutable.Queue[Long]]()\n+\n+  private val failedExecutorsTimeStamps = new mutable.Queue[Long]()\n+\n+  private def recentFailureCount(failedExecutorsTimeStampsForHost: mutable.Queue[Long]): Int = {\n+    val endTime = clock.getTimeMillis()\n+    while (executorFailuresValidityInterval > 0 &&\n+        failedExecutorsTimeStampsForHost.nonEmpty &&\n+        failedExecutorsTimeStampsForHost.head < endTime - executorFailuresValidityInterval) {\n+      failedExecutorsTimeStampsForHost.dequeue()\n+    }\n+    failedExecutorsTimeStampsForHost.size\n+  }\n+\n+  /**\n+   * Use a different clock. This is mainly used for testing.\n+   */\n+  def setClock(newClock: Clock): Unit = {\n+    clock = newClock\n+  }\n+\n+  def numExecutorsFailed: Int = synchronized {"
  }, {
    "author": {
      "login": "attilapiros"
    },
    "body": "This name is coming from YarnAllocator. \r\n```\r\n  def getNumExecutorsFailed: Int = failureTracker.numExecutorsFailed\r\n```\r\n\r\nShould I correct that one too?",
    "commit": "f71c7c547f173c902eec101d510c87d50c7abb86",
    "createdAt": "2018-05-02T09:10:47Z",
    "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.{Clock, SystemClock}\n+\n+/**\n+ * FailureTracker is responsible for tracking executor failures both for each host separately\n+ * and for all host altogether.\n+*/\n+private[spark] class FailureTracker(\n+    sparkConf: SparkConf,\n+    var clock: Clock = new SystemClock) extends Logging {\n+\n+  private val executorFailuresValidityInterval =\n+    sparkConf.get(config.EXECUTOR_ATTEMPT_FAILURE_VALIDITY_INTERVAL_MS).getOrElse(-1L)\n+\n+  // Queue to store the timestamp of failed executors for each host\n+  private val failedExecutorsTimeStampsPerHost = mutable.Map[String, mutable.Queue[Long]]()\n+\n+  private val failedExecutorsTimeStamps = new mutable.Queue[Long]()\n+\n+  private def recentFailureCount(failedExecutorsTimeStampsForHost: mutable.Queue[Long]): Int = {\n+    val endTime = clock.getTimeMillis()\n+    while (executorFailuresValidityInterval > 0 &&\n+        failedExecutorsTimeStampsForHost.nonEmpty &&\n+        failedExecutorsTimeStampsForHost.head < endTime - executorFailuresValidityInterval) {\n+      failedExecutorsTimeStampsForHost.dequeue()\n+    }\n+    failedExecutorsTimeStampsForHost.size\n+  }\n+\n+  /**\n+   * Use a different clock. This is mainly used for testing.\n+   */\n+  def setClock(newClock: Clock): Unit = {\n+    clock = newClock\n+  }\n+\n+  def numExecutorsFailed: Int = synchronized {"
  }],
  "prId": 21068
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "The parameter name is misleading; this method is used both for per-host failures and for the global failures.\r\n\r\nI'd also call it something that reflects the fact that it's updating its parameter (maybe `updateAndCountFailures`).",
    "commit": "f71c7c547f173c902eec101d510c87d50c7abb86",
    "createdAt": "2018-04-30T21:03:30Z",
    "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.{Clock, SystemClock}\n+\n+/**\n+ * FailureTracker is responsible for tracking executor failures both for each host separately\n+ * and for all host altogether.\n+*/\n+private[spark] class FailureTracker(\n+    sparkConf: SparkConf,\n+    var clock: Clock = new SystemClock) extends Logging {\n+\n+  private val executorFailuresValidityInterval =\n+    sparkConf.get(config.EXECUTOR_ATTEMPT_FAILURE_VALIDITY_INTERVAL_MS).getOrElse(-1L)\n+\n+  // Queue to store the timestamp of failed executors for each host\n+  private val failedExecutorsTimeStampsPerHost = mutable.Map[String, mutable.Queue[Long]]()\n+\n+  private val failedExecutorsTimeStamps = new mutable.Queue[Long]()\n+\n+  private def recentFailureCount(failedExecutorsTimeStampsForHost: mutable.Queue[Long]): Int = {"
  }],
  "prId": 21068
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "super nit: extra space here to align `*`s",
    "commit": "f71c7c547f173c902eec101d510c87d50c7abb86",
    "createdAt": "2018-05-02T15:04:21Z",
    "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.deploy.yarn\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.{Clock, SystemClock}\n+\n+/**\n+ * FailureTracker is responsible for tracking executor failures both for each host separately\n+ * and for all hosts altogether.\n+*/"
  }],
  "prId": 21068
}]