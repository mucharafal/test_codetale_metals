[{
  "comments": [{
    "author": {
      "login": "steveloughran"
    },
    "body": "I'd suggested adding a handler for UnknownHostException too, but now I think that could hide problems with client config. Best to leave as is.",
    "commit": "790cd02502e7f0ffd98f75af4382e4398929bd3c",
    "createdAt": "2017-05-05T12:31:26Z",
    "diffHunk": "@@ -81,8 +90,15 @@ private[security] class HadoopFSCredentialProvider\n     sparkConf.get(PRINCIPAL).flatMap { renewer =>\n       val creds = new Credentials()\n       hadoopFSsToAccess(hadoopConf, sparkConf).foreach { dst =>\n-        val dstFs = dst.getFileSystem(hadoopConf)\n-        dstFs.addDelegationTokens(renewer, creds)\n+        try {\n+          val dstFs = dst.getFileSystem(hadoopConf)\n+          dstFs.addDelegationTokens(renewer, creds)\n+        } catch {\n+          case e: StandbyException =>\n+            logWarning(s\"Namenode ${dst} is in state standby\", e)\n+          case e: RemoteException =>"
  }],
  "prId": 17872
}, {
  "comments": [{
    "author": {
      "login": "jerryshao"
    },
    "body": "It's not accurate to say \"Namenode\" here, because we may configure to other non-HDFS. ",
    "commit": "790cd02502e7f0ffd98f75af4382e4398929bd3c",
    "createdAt": "2017-05-08T06:50:04Z",
    "diffHunk": "@@ -48,9 +50,16 @@ private[security] class HadoopFSCredentialProvider\n     val tmpCreds = new Credentials()\n     val tokenRenewer = getTokenRenewer(hadoopConf)\n     hadoopFSsToAccess(hadoopConf, sparkConf).foreach { dst =>\n-      val dstFs = dst.getFileSystem(hadoopConf)\n-      logInfo(\"getting token for: \" + dst)\n-      dstFs.addDelegationTokens(tokenRenewer, tmpCreds)\n+      try {\n+        val dstFs = dst.getFileSystem(hadoopConf)\n+        logInfo(\"getting token for: \" + dst)\n+        dstFs.addDelegationTokens(tokenRenewer, tmpCreds)\n+      } catch {\n+        case e: StandbyException =>\n+          logWarning(s\"Namenode ${dst} is in state standby\", e)"
  }, {
    "author": {
      "login": "morenn520"
    },
    "body": "hum..Here is actually fetching tokens from hadoopFS, including in hadoopFSCredentialProvider, which means it's exactly HDFS?",
    "commit": "790cd02502e7f0ffd98f75af4382e4398929bd3c",
    "createdAt": "2017-05-08T06:56:26Z",
    "diffHunk": "@@ -48,9 +50,16 @@ private[security] class HadoopFSCredentialProvider\n     val tmpCreds = new Credentials()\n     val tokenRenewer = getTokenRenewer(hadoopConf)\n     hadoopFSsToAccess(hadoopConf, sparkConf).foreach { dst =>\n-      val dstFs = dst.getFileSystem(hadoopConf)\n-      logInfo(\"getting token for: \" + dst)\n-      dstFs.addDelegationTokens(tokenRenewer, tmpCreds)\n+      try {\n+        val dstFs = dst.getFileSystem(hadoopConf)\n+        logInfo(\"getting token for: \" + dst)\n+        dstFs.addDelegationTokens(tokenRenewer, tmpCreds)\n+      } catch {\n+        case e: StandbyException =>\n+          logWarning(s\"Namenode ${dst} is in state standby\", e)"
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "Hadoop compatible FS doesn't equal to HDFS, we can configure to wasb, adls and others. Also wasb and adls support fetching delegation tokens from common FS API, so we should avoid mentioning Namenode which is only existed in HDFS.",
    "commit": "790cd02502e7f0ffd98f75af4382e4398929bd3c",
    "createdAt": "2017-05-08T07:00:35Z",
    "diffHunk": "@@ -48,9 +50,16 @@ private[security] class HadoopFSCredentialProvider\n     val tmpCreds = new Credentials()\n     val tokenRenewer = getTokenRenewer(hadoopConf)\n     hadoopFSsToAccess(hadoopConf, sparkConf).foreach { dst =>\n-      val dstFs = dst.getFileSystem(hadoopConf)\n-      logInfo(\"getting token for: \" + dst)\n-      dstFs.addDelegationTokens(tokenRenewer, tmpCreds)\n+      try {\n+        val dstFs = dst.getFileSystem(hadoopConf)\n+        logInfo(\"getting token for: \" + dst)\n+        dstFs.addDelegationTokens(tokenRenewer, tmpCreds)\n+      } catch {\n+        case e: StandbyException =>\n+          logWarning(s\"Namenode ${dst} is in state standby\", e)"
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "Also for the below \"RemoteException\", how do you know \"RemoteException\" is exactly a standby exception?",
    "commit": "790cd02502e7f0ffd98f75af4382e4398929bd3c",
    "createdAt": "2017-05-08T07:02:41Z",
    "diffHunk": "@@ -48,9 +50,16 @@ private[security] class HadoopFSCredentialProvider\n     val tmpCreds = new Credentials()\n     val tokenRenewer = getTokenRenewer(hadoopConf)\n     hadoopFSsToAccess(hadoopConf, sparkConf).foreach { dst =>\n-      val dstFs = dst.getFileSystem(hadoopConf)\n-      logInfo(\"getting token for: \" + dst)\n-      dstFs.addDelegationTokens(tokenRenewer, tmpCreds)\n+      try {\n+        val dstFs = dst.getFileSystem(hadoopConf)\n+        logInfo(\"getting token for: \" + dst)\n+        dstFs.addDelegationTokens(tokenRenewer, tmpCreds)\n+      } catch {\n+        case e: StandbyException =>\n+          logWarning(s\"Namenode ${dst} is in state standby\", e)"
  }, {
    "author": {
      "login": "morenn520"
    },
    "body": "In our tests, there are two possible exceptions when yarn.spark.access.namenodes=hdfs://activeNamenode,hdfs://standbyNamenode\r\n1) Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category WRITE is not supported in state standby\r\n2) Caused by: org.apache.hadoop.ipc.StandbyException: Operation category WRITE is not supported in state standby\r\nMaybe RemoteException should be caught by better way.",
    "commit": "790cd02502e7f0ffd98f75af4382e4398929bd3c",
    "createdAt": "2017-05-08T07:06:26Z",
    "diffHunk": "@@ -48,9 +50,16 @@ private[security] class HadoopFSCredentialProvider\n     val tmpCreds = new Credentials()\n     val tokenRenewer = getTokenRenewer(hadoopConf)\n     hadoopFSsToAccess(hadoopConf, sparkConf).foreach { dst =>\n-      val dstFs = dst.getFileSystem(hadoopConf)\n-      logInfo(\"getting token for: \" + dst)\n-      dstFs.addDelegationTokens(tokenRenewer, tmpCreds)\n+      try {\n+        val dstFs = dst.getFileSystem(hadoopConf)\n+        logInfo(\"getting token for: \" + dst)\n+        dstFs.addDelegationTokens(tokenRenewer, tmpCreds)\n+      } catch {\n+        case e: StandbyException =>\n+          logWarning(s\"Namenode ${dst} is in state standby\", e)"
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "What I mean is that if \"RemoteException\" is caused by others, it is not correct to log as \"Namenode ${dst} is in state standby\".",
    "commit": "790cd02502e7f0ffd98f75af4382e4398929bd3c",
    "createdAt": "2017-05-08T07:19:24Z",
    "diffHunk": "@@ -48,9 +50,16 @@ private[security] class HadoopFSCredentialProvider\n     val tmpCreds = new Credentials()\n     val tokenRenewer = getTokenRenewer(hadoopConf)\n     hadoopFSsToAccess(hadoopConf, sparkConf).foreach { dst =>\n-      val dstFs = dst.getFileSystem(hadoopConf)\n-      logInfo(\"getting token for: \" + dst)\n-      dstFs.addDelegationTokens(tokenRenewer, tmpCreds)\n+      try {\n+        val dstFs = dst.getFileSystem(hadoopConf)\n+        logInfo(\"getting token for: \" + dst)\n+        dstFs.addDelegationTokens(tokenRenewer, tmpCreds)\n+      } catch {\n+        case e: StandbyException =>\n+          logWarning(s\"Namenode ${dst} is in state standby\", e)"
  }, {
    "author": {
      "login": "morenn520"
    },
    "body": "You are right. I will refactor the exception log.",
    "commit": "790cd02502e7f0ffd98f75af4382e4398929bd3c",
    "createdAt": "2017-05-08T07:33:30Z",
    "diffHunk": "@@ -48,9 +50,16 @@ private[security] class HadoopFSCredentialProvider\n     val tmpCreds = new Credentials()\n     val tokenRenewer = getTokenRenewer(hadoopConf)\n     hadoopFSsToAccess(hadoopConf, sparkConf).foreach { dst =>\n-      val dstFs = dst.getFileSystem(hadoopConf)\n-      logInfo(\"getting token for: \" + dst)\n-      dstFs.addDelegationTokens(tokenRenewer, tmpCreds)\n+      try {\n+        val dstFs = dst.getFileSystem(hadoopConf)\n+        logInfo(\"getting token for: \" + dst)\n+        dstFs.addDelegationTokens(tokenRenewer, tmpCreds)\n+      } catch {\n+        case e: StandbyException =>\n+          logWarning(s\"Namenode ${dst} is in state standby\", e)"
  }, {
    "author": {
      "login": "morenn520"
    },
    "body": "I refactored it. Please review and give some more advices :)",
    "commit": "790cd02502e7f0ffd98f75af4382e4398929bd3c",
    "createdAt": "2017-05-08T07:55:02Z",
    "diffHunk": "@@ -48,9 +50,16 @@ private[security] class HadoopFSCredentialProvider\n     val tmpCreds = new Credentials()\n     val tokenRenewer = getTokenRenewer(hadoopConf)\n     hadoopFSsToAccess(hadoopConf, sparkConf).foreach { dst =>\n-      val dstFs = dst.getFileSystem(hadoopConf)\n-      logInfo(\"getting token for: \" + dst)\n-      dstFs.addDelegationTokens(tokenRenewer, tmpCreds)\n+      try {\n+        val dstFs = dst.getFileSystem(hadoopConf)\n+        logInfo(\"getting token for: \" + dst)\n+        dstFs.addDelegationTokens(tokenRenewer, tmpCreds)\n+      } catch {\n+        case e: StandbyException =>\n+          logWarning(s\"Namenode ${dst} is in state standby\", e)"
  }],
  "prId": 17872
}, {
  "comments": [{
    "author": {
      "login": "jerryshao"
    },
    "body": "This two line of imports can be merged into one line.",
    "commit": "790cd02502e7f0ffd98f75af4382e4398929bd3c",
    "createdAt": "2017-05-08T08:47:04Z",
    "diffHunk": "@@ -22,6 +22,8 @@ import scala.util.Try\n \n import org.apache.hadoop.conf.Configuration\n import org.apache.hadoop.fs.{FileSystem, Path}\n+import org.apache.hadoop.ipc.RemoteException\n+import org.apache.hadoop.ipc.StandbyException"
  }],
  "prId": 17872
}]