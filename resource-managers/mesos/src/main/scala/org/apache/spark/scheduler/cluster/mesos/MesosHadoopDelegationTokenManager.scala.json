[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "One arg per line.",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-11-09T18:42:53Z",
    "diffHunk": "@@ -0,0 +1,180 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{ScheduledExecutorService, TimeUnit}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+/**\n+ * The MesosHadoopDelegationTokenManager fetches and updates Hadoop delegation tokens on the behalf\n+ * of the MesosCoarseGrainedSchedulerBackend. It is modeled after the YARN AMCredentialRenewer,\n+ * and similarly will renew the Credentials when 75% of the renewal interval has passed.\n+ * The principal difference is that instead of writing the new credentials to HDFS and\n+ * incrementing the timestamp of the file, the new credentials (called Tokens when they are\n+ * serialized) are broadcast to all running executors. On the executor side, when new Tokens are\n+ * received they overwrite the current credentials.\n+ */\n+private[spark] class MesosHadoopDelegationTokenManager(\n+    conf: SparkConf, hadoopConfig: Configuration,"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "You should probably assert that mode is \"tgt\" in this case.",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-11-09T18:44:50Z",
    "diffHunk": "@@ -0,0 +1,180 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{ScheduledExecutorService, TimeUnit}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+/**\n+ * The MesosHadoopDelegationTokenManager fetches and updates Hadoop delegation tokens on the behalf\n+ * of the MesosCoarseGrainedSchedulerBackend. It is modeled after the YARN AMCredentialRenewer,\n+ * and similarly will renew the Credentials when 75% of the renewal interval has passed.\n+ * The principal difference is that instead of writing the new credentials to HDFS and\n+ * incrementing the timestamp of the file, the new credentials (called Tokens when they are\n+ * serialized) are broadcast to all running executors. On the executor side, when new Tokens are\n+ * received they overwrite the current credentials.\n+ */\n+private[spark] class MesosHadoopDelegationTokenManager(\n+    conf: SparkConf, hadoopConfig: Configuration,\n+    driverEndpoint: RpcEndpointRef)\n+  extends Logging {\n+\n+  private val credentialRenewerThread: ScheduledExecutorService =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"Credential Renewal Thread\")\n+\n+  private val tokenManager: HadoopDelegationTokenManager =\n+    new HadoopDelegationTokenManager(conf, hadoopConfig)\n+\n+  private val principal: String = conf.get(config.PRINCIPAL).orNull\n+\n+  private val (secretFile: String, mode: String) = getSecretFile(conf)\n+\n+  private var (tokens: Array[Byte], timeOfNextRenewal: Long) = {\n+    try {\n+      val creds = UserGroupInformation.getCurrentUser.getCredentials\n+      val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+      val rt = tokenManager.obtainDelegationTokens(hadoopConf, creds)\n+      logInfo(s\"Initialized tokens: ${SparkHadoopUtil.get.dumpTokens(creds)}\")\n+      (SparkHadoopUtil.get.serialize(creds), rt)\n+    } catch {\n+      case e: Exception =>\n+        throw new IllegalStateException(\"Failed to initialize Hadoop delegation tokens\\n\" +\n+          s\"\\tPricipal: $principal\\n\\tmode: $mode\\n\\tsecret file $secretFile\\n\\tException: $e\")\n+    }\n+  }\n+\n+  scheduleTokenRenewal()\n+\n+  private def getSecretFile(conf: SparkConf): (String, String) = {\n+    val keytab = conf.get(config.KEYTAB).orNull\n+    val tgt = conf.getenv(\"KRB5CCNAME\")\n+    require(keytab != null || tgt != null, \"A keytab or TGT required.\")\n+    // if both Keytab and TGT are detected we use the Keytab.\n+    val (secretFile, mode) = if (keytab != null && tgt != null) {\n+      logWarning(s\"Keytab and TGT were detected, using keytab, \" +\n+        s\"unset ${config.KEYTAB.key} to use TGT\")\n+      (keytab, \"keytab\")\n+    } else {\n+      val m = if (keytab != null) \"keytab\" else \"tgt\"\n+      val sf = if (keytab != null) keytab else tgt\n+      (sf, m)\n+    }\n+\n+    if (principal == null) {\n+      logInfo(s\"Using mode: $mode to retrieve Hadoop delegation tokens\")"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Prefer to use `()` in methods that do non-trivial things.",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-11-09T18:46:12Z",
    "diffHunk": "@@ -0,0 +1,180 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{ScheduledExecutorService, TimeUnit}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+/**\n+ * The MesosHadoopDelegationTokenManager fetches and updates Hadoop delegation tokens on the behalf\n+ * of the MesosCoarseGrainedSchedulerBackend. It is modeled after the YARN AMCredentialRenewer,\n+ * and similarly will renew the Credentials when 75% of the renewal interval has passed.\n+ * The principal difference is that instead of writing the new credentials to HDFS and\n+ * incrementing the timestamp of the file, the new credentials (called Tokens when they are\n+ * serialized) are broadcast to all running executors. On the executor side, when new Tokens are\n+ * received they overwrite the current credentials.\n+ */\n+private[spark] class MesosHadoopDelegationTokenManager(\n+    conf: SparkConf, hadoopConfig: Configuration,\n+    driverEndpoint: RpcEndpointRef)\n+  extends Logging {\n+\n+  private val credentialRenewerThread: ScheduledExecutorService =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"Credential Renewal Thread\")\n+\n+  private val tokenManager: HadoopDelegationTokenManager =\n+    new HadoopDelegationTokenManager(conf, hadoopConfig)\n+\n+  private val principal: String = conf.get(config.PRINCIPAL).orNull\n+\n+  private val (secretFile: String, mode: String) = getSecretFile(conf)\n+\n+  private var (tokens: Array[Byte], timeOfNextRenewal: Long) = {\n+    try {\n+      val creds = UserGroupInformation.getCurrentUser.getCredentials\n+      val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+      val rt = tokenManager.obtainDelegationTokens(hadoopConf, creds)\n+      logInfo(s\"Initialized tokens: ${SparkHadoopUtil.get.dumpTokens(creds)}\")\n+      (SparkHadoopUtil.get.serialize(creds), rt)\n+    } catch {\n+      case e: Exception =>\n+        throw new IllegalStateException(\"Failed to initialize Hadoop delegation tokens\\n\" +\n+          s\"\\tPricipal: $principal\\n\\tmode: $mode\\n\\tsecret file $secretFile\\n\\tException: $e\")\n+    }\n+  }\n+\n+  scheduleTokenRenewal()\n+\n+  private def getSecretFile(conf: SparkConf): (String, String) = {\n+    val keytab = conf.get(config.KEYTAB).orNull\n+    val tgt = conf.getenv(\"KRB5CCNAME\")\n+    require(keytab != null || tgt != null, \"A keytab or TGT required.\")\n+    // if both Keytab and TGT are detected we use the Keytab.\n+    val (secretFile, mode) = if (keytab != null && tgt != null) {\n+      logWarning(s\"Keytab and TGT were detected, using keytab, \" +\n+        s\"unset ${config.KEYTAB.key} to use TGT\")\n+      (keytab, \"keytab\")\n+    } else {\n+      val m = if (keytab != null) \"keytab\" else \"tgt\"\n+      val sf = if (keytab != null) keytab else tgt\n+      (sf, m)\n+    }\n+\n+    if (principal == null) {\n+      logInfo(s\"Using mode: $mode to retrieve Hadoop delegation tokens\")\n+    } else {\n+      logInfo(s\"Using principal: $principal with mode: $mode to retrieve Hadoop delegation tokens\")\n+    }\n+\n+    logDebug(s\"secretFile is $secretFile\")\n+    (secretFile, mode)\n+  }\n+\n+  private def scheduleTokenRenewal(): Unit = {\n+    def scheduleRenewal(runnable: Runnable): Unit = {\n+      val remainingTime = timeOfNextRenewal - System.currentTimeMillis()\n+      if (remainingTime <= 0) {\n+        logInfo(\"Credentials have expired, creating new ones now.\")\n+        runnable.run()\n+      } else {\n+        logInfo(s\"Scheduling login from keytab in $remainingTime millis.\")\n+        credentialRenewerThread.schedule(runnable, remainingTime, TimeUnit.MILLISECONDS)\n+      }\n+    }\n+\n+    val credentialRenewerRunnable =\n+      new Runnable {\n+        override def run(): Unit = {\n+          try {\n+            val tokensBytes = getNewDelegationTokens\n+            broadcastDelegationTokens(tokensBytes)\n+          } catch {\n+            case e: Exception =>\n+              // Log the error and try to write new tokens back in an hour\n+              logWarning(\"Couldn't broadcast tokens, trying again in an hour\", e)\n+              credentialRenewerThread.schedule(this, 1, TimeUnit.HOURS)\n+              return\n+          }\n+          scheduleRenewal(this)\n+        }\n+      }\n+    scheduleRenewal(credentialRenewerRunnable)\n+  }\n+\n+  private def getNewDelegationTokens: Array[Byte] = {"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Use `e` as the cause of the exception you're throwing.",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-11-09T18:48:55Z",
    "diffHunk": "@@ -0,0 +1,180 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{ScheduledExecutorService, TimeUnit}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+/**\n+ * The MesosHadoopDelegationTokenManager fetches and updates Hadoop delegation tokens on the behalf\n+ * of the MesosCoarseGrainedSchedulerBackend. It is modeled after the YARN AMCredentialRenewer,\n+ * and similarly will renew the Credentials when 75% of the renewal interval has passed.\n+ * The principal difference is that instead of writing the new credentials to HDFS and\n+ * incrementing the timestamp of the file, the new credentials (called Tokens when they are\n+ * serialized) are broadcast to all running executors. On the executor side, when new Tokens are\n+ * received they overwrite the current credentials.\n+ */\n+private[spark] class MesosHadoopDelegationTokenManager(\n+    conf: SparkConf, hadoopConfig: Configuration,\n+    driverEndpoint: RpcEndpointRef)\n+  extends Logging {\n+\n+  private val credentialRenewerThread: ScheduledExecutorService =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"Credential Renewal Thread\")\n+\n+  private val tokenManager: HadoopDelegationTokenManager =\n+    new HadoopDelegationTokenManager(conf, hadoopConfig)\n+\n+  private val principal: String = conf.get(config.PRINCIPAL).orNull\n+\n+  private val (secretFile: String, mode: String) = getSecretFile(conf)\n+\n+  private var (tokens: Array[Byte], timeOfNextRenewal: Long) = {\n+    try {\n+      val creds = UserGroupInformation.getCurrentUser.getCredentials\n+      val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+      val rt = tokenManager.obtainDelegationTokens(hadoopConf, creds)\n+      logInfo(s\"Initialized tokens: ${SparkHadoopUtil.get.dumpTokens(creds)}\")\n+      (SparkHadoopUtil.get.serialize(creds), rt)\n+    } catch {\n+      case e: Exception =>\n+        throw new IllegalStateException(\"Failed to initialize Hadoop delegation tokens\\n\" +\n+          s\"\\tPricipal: $principal\\n\\tmode: $mode\\n\\tsecret file $secretFile\\n\\tException: $e\")"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Is that really the case? `KRB5CCNAME` is not a required env variable. It has a default value, and the `UGI` class will use the credentials from the default location if they're available (and reloading the cache periodically).\r\n\r\nSo I think you don't really need this, but just to track whether there's a principal and keytab. And you don't need to call `getUGIFromTicketCache` later on since I'm pretty sure UGI takes care of that for you.",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-11-09T18:53:06Z",
    "diffHunk": "@@ -0,0 +1,180 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{ScheduledExecutorService, TimeUnit}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+/**\n+ * The MesosHadoopDelegationTokenManager fetches and updates Hadoop delegation tokens on the behalf\n+ * of the MesosCoarseGrainedSchedulerBackend. It is modeled after the YARN AMCredentialRenewer,\n+ * and similarly will renew the Credentials when 75% of the renewal interval has passed.\n+ * The principal difference is that instead of writing the new credentials to HDFS and\n+ * incrementing the timestamp of the file, the new credentials (called Tokens when they are\n+ * serialized) are broadcast to all running executors. On the executor side, when new Tokens are\n+ * received they overwrite the current credentials.\n+ */\n+private[spark] class MesosHadoopDelegationTokenManager(\n+    conf: SparkConf, hadoopConfig: Configuration,\n+    driverEndpoint: RpcEndpointRef)\n+  extends Logging {\n+\n+  private val credentialRenewerThread: ScheduledExecutorService =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"Credential Renewal Thread\")\n+\n+  private val tokenManager: HadoopDelegationTokenManager =\n+    new HadoopDelegationTokenManager(conf, hadoopConfig)\n+\n+  private val principal: String = conf.get(config.PRINCIPAL).orNull\n+\n+  private val (secretFile: String, mode: String) = getSecretFile(conf)\n+\n+  private var (tokens: Array[Byte], timeOfNextRenewal: Long) = {\n+    try {\n+      val creds = UserGroupInformation.getCurrentUser.getCredentials\n+      val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+      val rt = tokenManager.obtainDelegationTokens(hadoopConf, creds)\n+      logInfo(s\"Initialized tokens: ${SparkHadoopUtil.get.dumpTokens(creds)}\")\n+      (SparkHadoopUtil.get.serialize(creds), rt)\n+    } catch {\n+      case e: Exception =>\n+        throw new IllegalStateException(\"Failed to initialize Hadoop delegation tokens\\n\" +\n+          s\"\\tPricipal: $principal\\n\\tmode: $mode\\n\\tsecret file $secretFile\\n\\tException: $e\")\n+    }\n+  }\n+\n+  scheduleTokenRenewal()\n+\n+  private def getSecretFile(conf: SparkConf): (String, String) = {\n+    val keytab = conf.get(config.KEYTAB).orNull\n+    val tgt = conf.getenv(\"KRB5CCNAME\")\n+    require(keytab != null || tgt != null, \"A keytab or TGT required.\")"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "```\r\nval nextRenewalTime = ugi.doAs(new PrivilegedExceptionAction[Long] { ... }\r\n```",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-11-09T18:54:37Z",
    "diffHunk": "@@ -0,0 +1,180 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{ScheduledExecutorService, TimeUnit}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+/**\n+ * The MesosHadoopDelegationTokenManager fetches and updates Hadoop delegation tokens on the behalf\n+ * of the MesosCoarseGrainedSchedulerBackend. It is modeled after the YARN AMCredentialRenewer,\n+ * and similarly will renew the Credentials when 75% of the renewal interval has passed.\n+ * The principal difference is that instead of writing the new credentials to HDFS and\n+ * incrementing the timestamp of the file, the new credentials (called Tokens when they are\n+ * serialized) are broadcast to all running executors. On the executor side, when new Tokens are\n+ * received they overwrite the current credentials.\n+ */\n+private[spark] class MesosHadoopDelegationTokenManager(\n+    conf: SparkConf, hadoopConfig: Configuration,\n+    driverEndpoint: RpcEndpointRef)\n+  extends Logging {\n+\n+  private val credentialRenewerThread: ScheduledExecutorService =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"Credential Renewal Thread\")\n+\n+  private val tokenManager: HadoopDelegationTokenManager =\n+    new HadoopDelegationTokenManager(conf, hadoopConfig)\n+\n+  private val principal: String = conf.get(config.PRINCIPAL).orNull\n+\n+  private val (secretFile: String, mode: String) = getSecretFile(conf)\n+\n+  private var (tokens: Array[Byte], timeOfNextRenewal: Long) = {\n+    try {\n+      val creds = UserGroupInformation.getCurrentUser.getCredentials\n+      val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+      val rt = tokenManager.obtainDelegationTokens(hadoopConf, creds)\n+      logInfo(s\"Initialized tokens: ${SparkHadoopUtil.get.dumpTokens(creds)}\")\n+      (SparkHadoopUtil.get.serialize(creds), rt)\n+    } catch {\n+      case e: Exception =>\n+        throw new IllegalStateException(\"Failed to initialize Hadoop delegation tokens\\n\" +\n+          s\"\\tPricipal: $principal\\n\\tmode: $mode\\n\\tsecret file $secretFile\\n\\tException: $e\")\n+    }\n+  }\n+\n+  scheduleTokenRenewal()\n+\n+  private def getSecretFile(conf: SparkConf): (String, String) = {\n+    val keytab = conf.get(config.KEYTAB).orNull\n+    val tgt = conf.getenv(\"KRB5CCNAME\")\n+    require(keytab != null || tgt != null, \"A keytab or TGT required.\")\n+    // if both Keytab and TGT are detected we use the Keytab.\n+    val (secretFile, mode) = if (keytab != null && tgt != null) {\n+      logWarning(s\"Keytab and TGT were detected, using keytab, \" +\n+        s\"unset ${config.KEYTAB.key} to use TGT\")\n+      (keytab, \"keytab\")\n+    } else {\n+      val m = if (keytab != null) \"keytab\" else \"tgt\"\n+      val sf = if (keytab != null) keytab else tgt\n+      (sf, m)\n+    }\n+\n+    if (principal == null) {\n+      logInfo(s\"Using mode: $mode to retrieve Hadoop delegation tokens\")\n+    } else {\n+      logInfo(s\"Using principal: $principal with mode: $mode to retrieve Hadoop delegation tokens\")\n+    }\n+\n+    logDebug(s\"secretFile is $secretFile\")\n+    (secretFile, mode)\n+  }\n+\n+  private def scheduleTokenRenewal(): Unit = {\n+    def scheduleRenewal(runnable: Runnable): Unit = {\n+      val remainingTime = timeOfNextRenewal - System.currentTimeMillis()\n+      if (remainingTime <= 0) {\n+        logInfo(\"Credentials have expired, creating new ones now.\")\n+        runnable.run()\n+      } else {\n+        logInfo(s\"Scheduling login from keytab in $remainingTime millis.\")\n+        credentialRenewerThread.schedule(runnable, remainingTime, TimeUnit.MILLISECONDS)\n+      }\n+    }\n+\n+    val credentialRenewerRunnable =\n+      new Runnable {\n+        override def run(): Unit = {\n+          try {\n+            val tokensBytes = getNewDelegationTokens\n+            broadcastDelegationTokens(tokensBytes)\n+          } catch {\n+            case e: Exception =>\n+              // Log the error and try to write new tokens back in an hour\n+              logWarning(\"Couldn't broadcast tokens, trying again in an hour\", e)\n+              credentialRenewerThread.schedule(this, 1, TimeUnit.HOURS)\n+              return\n+          }\n+          scheduleRenewal(this)\n+        }\n+      }\n+    scheduleRenewal(credentialRenewerRunnable)\n+  }\n+\n+  private def getNewDelegationTokens: Array[Byte] = {\n+    logInfo(s\"Attempting to login to KDC with ${conf.get(config.PRINCIPAL).orNull}\")\n+    // Get new delegation tokens by logging in with a new UGI\n+    // inspired by AMCredentialRenewer.scala:L174\n+    val ugi = if (mode == \"keytab\") {\n+      UserGroupInformation.loginUserFromKeytabAndReturnUGI(principal, secretFile)\n+    } else {\n+      UserGroupInformation.getUGIFromTicketCache(secretFile, principal)\n+    }\n+    logInfo(\"Successfully logged into KDC\")\n+\n+    val tempCreds = ugi.getCredentials\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    var nextRenewalTime = Long.MaxValue"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "I'd make this `logInfo` (similar message in YARN code has helped me a lot).",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-11-09T18:55:42Z",
    "diffHunk": "@@ -0,0 +1,180 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{ScheduledExecutorService, TimeUnit}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+/**\n+ * The MesosHadoopDelegationTokenManager fetches and updates Hadoop delegation tokens on the behalf\n+ * of the MesosCoarseGrainedSchedulerBackend. It is modeled after the YARN AMCredentialRenewer,\n+ * and similarly will renew the Credentials when 75% of the renewal interval has passed.\n+ * The principal difference is that instead of writing the new credentials to HDFS and\n+ * incrementing the timestamp of the file, the new credentials (called Tokens when they are\n+ * serialized) are broadcast to all running executors. On the executor side, when new Tokens are\n+ * received they overwrite the current credentials.\n+ */\n+private[spark] class MesosHadoopDelegationTokenManager(\n+    conf: SparkConf, hadoopConfig: Configuration,\n+    driverEndpoint: RpcEndpointRef)\n+  extends Logging {\n+\n+  private val credentialRenewerThread: ScheduledExecutorService =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"Credential Renewal Thread\")\n+\n+  private val tokenManager: HadoopDelegationTokenManager =\n+    new HadoopDelegationTokenManager(conf, hadoopConfig)\n+\n+  private val principal: String = conf.get(config.PRINCIPAL).orNull\n+\n+  private val (secretFile: String, mode: String) = getSecretFile(conf)\n+\n+  private var (tokens: Array[Byte], timeOfNextRenewal: Long) = {\n+    try {\n+      val creds = UserGroupInformation.getCurrentUser.getCredentials\n+      val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+      val rt = tokenManager.obtainDelegationTokens(hadoopConf, creds)\n+      logInfo(s\"Initialized tokens: ${SparkHadoopUtil.get.dumpTokens(creds)}\")\n+      (SparkHadoopUtil.get.serialize(creds), rt)\n+    } catch {\n+      case e: Exception =>\n+        throw new IllegalStateException(\"Failed to initialize Hadoop delegation tokens\\n\" +\n+          s\"\\tPricipal: $principal\\n\\tmode: $mode\\n\\tsecret file $secretFile\\n\\tException: $e\")\n+    }\n+  }\n+\n+  scheduleTokenRenewal()\n+\n+  private def getSecretFile(conf: SparkConf): (String, String) = {\n+    val keytab = conf.get(config.KEYTAB).orNull\n+    val tgt = conf.getenv(\"KRB5CCNAME\")\n+    require(keytab != null || tgt != null, \"A keytab or TGT required.\")\n+    // if both Keytab and TGT are detected we use the Keytab.\n+    val (secretFile, mode) = if (keytab != null && tgt != null) {\n+      logWarning(s\"Keytab and TGT were detected, using keytab, \" +\n+        s\"unset ${config.KEYTAB.key} to use TGT\")\n+      (keytab, \"keytab\")\n+    } else {\n+      val m = if (keytab != null) \"keytab\" else \"tgt\"\n+      val sf = if (keytab != null) keytab else tgt\n+      (sf, m)\n+    }\n+\n+    if (principal == null) {\n+      logInfo(s\"Using mode: $mode to retrieve Hadoop delegation tokens\")\n+    } else {\n+      logInfo(s\"Using principal: $principal with mode: $mode to retrieve Hadoop delegation tokens\")\n+    }\n+\n+    logDebug(s\"secretFile is $secretFile\")\n+    (secretFile, mode)\n+  }\n+\n+  private def scheduleTokenRenewal(): Unit = {\n+    def scheduleRenewal(runnable: Runnable): Unit = {\n+      val remainingTime = timeOfNextRenewal - System.currentTimeMillis()\n+      if (remainingTime <= 0) {\n+        logInfo(\"Credentials have expired, creating new ones now.\")\n+        runnable.run()\n+      } else {\n+        logInfo(s\"Scheduling login from keytab in $remainingTime millis.\")\n+        credentialRenewerThread.schedule(runnable, remainingTime, TimeUnit.MILLISECONDS)\n+      }\n+    }\n+\n+    val credentialRenewerRunnable =\n+      new Runnable {\n+        override def run(): Unit = {\n+          try {\n+            val tokensBytes = getNewDelegationTokens\n+            broadcastDelegationTokens(tokensBytes)\n+          } catch {\n+            case e: Exception =>\n+              // Log the error and try to write new tokens back in an hour\n+              logWarning(\"Couldn't broadcast tokens, trying again in an hour\", e)\n+              credentialRenewerThread.schedule(this, 1, TimeUnit.HOURS)\n+              return\n+          }\n+          scheduleRenewal(this)\n+        }\n+      }\n+    scheduleRenewal(credentialRenewerRunnable)\n+  }\n+\n+  private def getNewDelegationTokens: Array[Byte] = {\n+    logInfo(s\"Attempting to login to KDC with ${conf.get(config.PRINCIPAL).orNull}\")\n+    // Get new delegation tokens by logging in with a new UGI\n+    // inspired by AMCredentialRenewer.scala:L174\n+    val ugi = if (mode == \"keytab\") {\n+      UserGroupInformation.loginUserFromKeytabAndReturnUGI(principal, secretFile)\n+    } else {\n+      UserGroupInformation.getUGIFromTicketCache(secretFile, principal)\n+    }\n+    logInfo(\"Successfully logged into KDC\")\n+\n+    val tempCreds = ugi.getCredentials\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    var nextRenewalTime = Long.MaxValue\n+    ugi.doAs(new PrivilegedExceptionAction[Void] {\n+      override def run(): Void = {\n+        nextRenewalTime = tokenManager.obtainDelegationTokens(hadoopConf, tempCreds)\n+        null\n+      }\n+    })\n+\n+    val currTime = System.currentTimeMillis()\n+    timeOfNextRenewal = if (nextRenewalTime <= currTime) {\n+      logWarning(s\"Next credential renewal time ($nextRenewalTime) is earlier than \" +\n+        s\"current time ($currTime), which is unexpected, please check your credential renewal \" +\n+        \"related configurations in the target services.\")\n+      currTime\n+    } else {\n+      SparkHadoopUtil.getDateOfNextUpdate(nextRenewalTime, 0.75)\n+    }\n+    logInfo(s\"Time of next renewal is in ${timeOfNextRenewal - System.currentTimeMillis()} ms\")\n+\n+    // Add the temp credentials back to the original ones.\n+    UserGroupInformation.getCurrentUser.addCredentials(tempCreds)\n+    SparkHadoopUtil.get.serialize(tempCreds)\n+  }\n+\n+  private def broadcastDelegationTokens(tokens: Array[Byte]) = {\n+    logDebug(\"Sending new tokens to all executors\")"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Make this a `require` in the constructor?",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-11-09T18:56:30Z",
    "diffHunk": "@@ -0,0 +1,180 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{ScheduledExecutorService, TimeUnit}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+/**\n+ * The MesosHadoopDelegationTokenManager fetches and updates Hadoop delegation tokens on the behalf\n+ * of the MesosCoarseGrainedSchedulerBackend. It is modeled after the YARN AMCredentialRenewer,\n+ * and similarly will renew the Credentials when 75% of the renewal interval has passed.\n+ * The principal difference is that instead of writing the new credentials to HDFS and\n+ * incrementing the timestamp of the file, the new credentials (called Tokens when they are\n+ * serialized) are broadcast to all running executors. On the executor side, when new Tokens are\n+ * received they overwrite the current credentials.\n+ */\n+private[spark] class MesosHadoopDelegationTokenManager(\n+    conf: SparkConf, hadoopConfig: Configuration,\n+    driverEndpoint: RpcEndpointRef)\n+  extends Logging {\n+\n+  private val credentialRenewerThread: ScheduledExecutorService =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"Credential Renewal Thread\")\n+\n+  private val tokenManager: HadoopDelegationTokenManager =\n+    new HadoopDelegationTokenManager(conf, hadoopConfig)\n+\n+  private val principal: String = conf.get(config.PRINCIPAL).orNull\n+\n+  private val (secretFile: String, mode: String) = getSecretFile(conf)\n+\n+  private var (tokens: Array[Byte], timeOfNextRenewal: Long) = {\n+    try {\n+      val creds = UserGroupInformation.getCurrentUser.getCredentials\n+      val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+      val rt = tokenManager.obtainDelegationTokens(hadoopConf, creds)\n+      logInfo(s\"Initialized tokens: ${SparkHadoopUtil.get.dumpTokens(creds)}\")\n+      (SparkHadoopUtil.get.serialize(creds), rt)\n+    } catch {\n+      case e: Exception =>\n+        throw new IllegalStateException(\"Failed to initialize Hadoop delegation tokens\\n\" +\n+          s\"\\tPricipal: $principal\\n\\tmode: $mode\\n\\tsecret file $secretFile\\n\\tException: $e\")\n+    }\n+  }\n+\n+  scheduleTokenRenewal()\n+\n+  private def getSecretFile(conf: SparkConf): (String, String) = {\n+    val keytab = conf.get(config.KEYTAB).orNull\n+    val tgt = conf.getenv(\"KRB5CCNAME\")\n+    require(keytab != null || tgt != null, \"A keytab or TGT required.\")\n+    // if both Keytab and TGT are detected we use the Keytab.\n+    val (secretFile, mode) = if (keytab != null && tgt != null) {\n+      logWarning(s\"Keytab and TGT were detected, using keytab, \" +\n+        s\"unset ${config.KEYTAB.key} to use TGT\")\n+      (keytab, \"keytab\")\n+    } else {\n+      val m = if (keytab != null) \"keytab\" else \"tgt\"\n+      val sf = if (keytab != null) keytab else tgt\n+      (sf, m)\n+    }\n+\n+    if (principal == null) {\n+      logInfo(s\"Using mode: $mode to retrieve Hadoop delegation tokens\")\n+    } else {\n+      logInfo(s\"Using principal: $principal with mode: $mode to retrieve Hadoop delegation tokens\")\n+    }\n+\n+    logDebug(s\"secretFile is $secretFile\")\n+    (secretFile, mode)\n+  }\n+\n+  private def scheduleTokenRenewal(): Unit = {\n+    def scheduleRenewal(runnable: Runnable): Unit = {\n+      val remainingTime = timeOfNextRenewal - System.currentTimeMillis()\n+      if (remainingTime <= 0) {\n+        logInfo(\"Credentials have expired, creating new ones now.\")\n+        runnable.run()\n+      } else {\n+        logInfo(s\"Scheduling login from keytab in $remainingTime millis.\")\n+        credentialRenewerThread.schedule(runnable, remainingTime, TimeUnit.MILLISECONDS)\n+      }\n+    }\n+\n+    val credentialRenewerRunnable =\n+      new Runnable {\n+        override def run(): Unit = {\n+          try {\n+            val tokensBytes = getNewDelegationTokens\n+            broadcastDelegationTokens(tokensBytes)\n+          } catch {\n+            case e: Exception =>\n+              // Log the error and try to write new tokens back in an hour\n+              logWarning(\"Couldn't broadcast tokens, trying again in an hour\", e)\n+              credentialRenewerThread.schedule(this, 1, TimeUnit.HOURS)\n+              return\n+          }\n+          scheduleRenewal(this)\n+        }\n+      }\n+    scheduleRenewal(credentialRenewerRunnable)\n+  }\n+\n+  private def getNewDelegationTokens: Array[Byte] = {\n+    logInfo(s\"Attempting to login to KDC with ${conf.get(config.PRINCIPAL).orNull}\")\n+    // Get new delegation tokens by logging in with a new UGI\n+    // inspired by AMCredentialRenewer.scala:L174\n+    val ugi = if (mode == \"keytab\") {\n+      UserGroupInformation.loginUserFromKeytabAndReturnUGI(principal, secretFile)\n+    } else {\n+      UserGroupInformation.getUGIFromTicketCache(secretFile, principal)\n+    }\n+    logInfo(\"Successfully logged into KDC\")\n+\n+    val tempCreds = ugi.getCredentials\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    var nextRenewalTime = Long.MaxValue\n+    ugi.doAs(new PrivilegedExceptionAction[Void] {\n+      override def run(): Void = {\n+        nextRenewalTime = tokenManager.obtainDelegationTokens(hadoopConf, tempCreds)\n+        null\n+      }\n+    })\n+\n+    val currTime = System.currentTimeMillis()\n+    timeOfNextRenewal = if (nextRenewalTime <= currTime) {\n+      logWarning(s\"Next credential renewal time ($nextRenewalTime) is earlier than \" +\n+        s\"current time ($currTime), which is unexpected, please check your credential renewal \" +\n+        \"related configurations in the target services.\")\n+      currTime\n+    } else {\n+      SparkHadoopUtil.getDateOfNextUpdate(nextRenewalTime, 0.75)\n+    }\n+    logInfo(s\"Time of next renewal is in ${timeOfNextRenewal - System.currentTimeMillis()} ms\")\n+\n+    // Add the temp credentials back to the original ones.\n+    UserGroupInformation.getCurrentUser.addCredentials(tempCreds)\n+    SparkHadoopUtil.get.serialize(tempCreds)\n+  }\n+\n+  private def broadcastDelegationTokens(tokens: Array[Byte]) = {\n+    logDebug(\"Sending new tokens to all executors\")\n+    if (driverEndpoint == null) {"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "The point I was trying to make is that you do not need *any* special handling for TGT. The UGI class already does everything you need, you just need to get the current user. It will keep the TGT updated with any changes that happen on disk. You don't need to handle `KRB5CCNAME` anywhere, because UGI should be doing that for you. If it's not, you need to explain why you need this special handling, because the expected behavior is for this to work without you needing to do anything.\r\n\r\nSo you can simplify this class by *only* handling the principal / keytab case, and just using `UserGroupInformation.getCurrentUser` in the other case. You don't need to keep track of the \"mode\" or anything else, just whether you're using a principal / keytab pair.",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-11-13T22:43:42Z",
    "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{ScheduledExecutorService, TimeUnit}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+/**\n+ * The MesosHadoopDelegationTokenManager fetches and updates Hadoop delegation tokens on the behalf\n+ * of the MesosCoarseGrainedSchedulerBackend. It is modeled after the YARN AMCredentialRenewer,\n+ * and similarly will renew the Credentials when 75% of the renewal interval has passed.\n+ * The principal difference is that instead of writing the new credentials to HDFS and\n+ * incrementing the timestamp of the file, the new credentials (called Tokens when they are\n+ * serialized) are broadcast to all running executors. On the executor side, when new Tokens are\n+ * received they overwrite the current credentials.\n+ */\n+private[spark] class MesosHadoopDelegationTokenManager(\n+    conf: SparkConf,\n+    hadoopConfig: Configuration,\n+    driverEndpoint: RpcEndpointRef)\n+  extends Logging {\n+\n+  require(driverEndpoint != null, \"DriverEndpoint is not initialized\")\n+\n+  private val credentialRenewerThread: ScheduledExecutorService =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"Credential Renewal Thread\")\n+\n+  private val tokenManager: HadoopDelegationTokenManager =\n+    new HadoopDelegationTokenManager(conf, hadoopConfig)\n+\n+  private val principal: String = conf.get(config.PRINCIPAL).orNull\n+\n+  private val (secretFile: Option[String], mode: String) = getSecretFile(conf)\n+\n+  private var (tokens: Array[Byte], timeOfNextRenewal: Long) = {\n+    try {\n+      val creds = UserGroupInformation.getCurrentUser.getCredentials\n+      val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+      val rt = tokenManager.obtainDelegationTokens(hadoopConf, creds)\n+      logInfo(s\"Initialized tokens: ${SparkHadoopUtil.get.dumpTokens(creds)}\")\n+      (SparkHadoopUtil.get.serialize(creds), rt)\n+    } catch {\n+      case e: Exception =>\n+        logError(\"Failed to initialize Hadoop delegation tokens\\n\" +\n+          s\"\\tPricipal: $principal\\n\\tmode: $mode\\n\\tsecret file $secretFile\\n\\tException: $e\")\n+        throw e\n+    }\n+  }\n+\n+  scheduleTokenRenewal()\n+\n+  private def getSecretFile(conf: SparkConf): (Option[String], String) = {\n+    val keytab = conf.get(config.KEYTAB)\n+    val tgt = Option(conf.getenv(SparkHadoopUtil.TICKET_CACHE_ENVVAR))\n+    val (secretFile, mode) = if (keytab.isDefined && tgt.isDefined) {\n+      // if a keytab and a specific ticket cache is specified use the keytab and log the behavior\n+      logWarning(s\"Keytab and TGT were detected, using keytab, ${keytab.get}, \" +\n+        s\"unset ${config.KEYTAB.key} to use TGT (${tgt.get})\")\n+      (keytab, \"keytab\")\n+    } else {\n+      val m = if (keytab.isDefined) \"keytab\" else \"tgt\"\n+      val sf = if (keytab.isDefined) keytab else tgt\n+      (sf, m)\n+    }\n+\n+    if (principal == null) {\n+      require(mode == \"tgt\", s\"Must specify a principal when using a Keytab, was $principal\")\n+      logInfo(s\"Using ticket cache to fetch Hadoop delegation tokens\")\n+    } else {\n+      logInfo(s\"Using principal: $principal with mode and keytab $keytab \" +\n+        s\"to fetch Hadoop delegation tokens\")\n+    }\n+\n+    logDebug(s\"secretFile is $secretFile\")\n+    (secretFile, mode)\n+  }\n+\n+  private def scheduleTokenRenewal(): Unit = {\n+    def scheduleRenewal(runnable: Runnable): Unit = {\n+      val remainingTime = timeOfNextRenewal - System.currentTimeMillis()\n+      if (remainingTime <= 0) {\n+        logInfo(\"Credentials have expired, creating new ones now.\")\n+        runnable.run()\n+      } else {\n+        logInfo(s\"Scheduling login from keytab in $remainingTime millis.\")\n+        credentialRenewerThread.schedule(runnable, remainingTime, TimeUnit.MILLISECONDS)\n+      }\n+    }\n+\n+    val credentialRenewerRunnable =\n+      new Runnable {\n+        override def run(): Unit = {\n+          try {\n+            val tokensBytes = getNewDelegationTokens\n+            broadcastDelegationTokens(tokensBytes)\n+          } catch {\n+            case e: Exception =>\n+              // Log the error and try to write new tokens back in an hour\n+              logWarning(\"Couldn't broadcast tokens, trying again in an hour\", e)\n+              credentialRenewerThread.schedule(this, 1, TimeUnit.HOURS)\n+              return\n+          }\n+          scheduleRenewal(this)\n+        }\n+      }\n+    scheduleRenewal(credentialRenewerRunnable)\n+  }\n+\n+  private def getNewDelegationTokens(): Array[Byte] = {\n+    logInfo(s\"Attempting to login to KDC with ${conf.get(config.PRINCIPAL).orNull}\")\n+    // Get new delegation tokens by logging in with a new UGI\n+    // inspired by AMCredentialRenewer.scala:L174\n+    val ugi = if (mode == \"keytab\") {\n+      UserGroupInformation.loginUserFromKeytabAndReturnUGI(principal, secretFile.get)\n+    } else {\n+      // if the ticket cache is not explicitly defined, use the default\n+      if (secretFile.isEmpty) {"
  }, {
    "author": {
      "login": "ArtRand"
    },
    "body": "How do the executors get the TGT though?  `KRB5CCNAME` would need to be set in the executor containers as well, right? If it is, I suppose you don't need to broadcast the delegation tokens at all because `runAsSparkUser` takes care of it for you? \r\n\r\nI guess now that https://github.com/apache/spark/pull/19437 has been merged we can disseminate the TGT though Mesos. ",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-11-14T00:58:23Z",
    "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{ScheduledExecutorService, TimeUnit}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+/**\n+ * The MesosHadoopDelegationTokenManager fetches and updates Hadoop delegation tokens on the behalf\n+ * of the MesosCoarseGrainedSchedulerBackend. It is modeled after the YARN AMCredentialRenewer,\n+ * and similarly will renew the Credentials when 75% of the renewal interval has passed.\n+ * The principal difference is that instead of writing the new credentials to HDFS and\n+ * incrementing the timestamp of the file, the new credentials (called Tokens when they are\n+ * serialized) are broadcast to all running executors. On the executor side, when new Tokens are\n+ * received they overwrite the current credentials.\n+ */\n+private[spark] class MesosHadoopDelegationTokenManager(\n+    conf: SparkConf,\n+    hadoopConfig: Configuration,\n+    driverEndpoint: RpcEndpointRef)\n+  extends Logging {\n+\n+  require(driverEndpoint != null, \"DriverEndpoint is not initialized\")\n+\n+  private val credentialRenewerThread: ScheduledExecutorService =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"Credential Renewal Thread\")\n+\n+  private val tokenManager: HadoopDelegationTokenManager =\n+    new HadoopDelegationTokenManager(conf, hadoopConfig)\n+\n+  private val principal: String = conf.get(config.PRINCIPAL).orNull\n+\n+  private val (secretFile: Option[String], mode: String) = getSecretFile(conf)\n+\n+  private var (tokens: Array[Byte], timeOfNextRenewal: Long) = {\n+    try {\n+      val creds = UserGroupInformation.getCurrentUser.getCredentials\n+      val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+      val rt = tokenManager.obtainDelegationTokens(hadoopConf, creds)\n+      logInfo(s\"Initialized tokens: ${SparkHadoopUtil.get.dumpTokens(creds)}\")\n+      (SparkHadoopUtil.get.serialize(creds), rt)\n+    } catch {\n+      case e: Exception =>\n+        logError(\"Failed to initialize Hadoop delegation tokens\\n\" +\n+          s\"\\tPricipal: $principal\\n\\tmode: $mode\\n\\tsecret file $secretFile\\n\\tException: $e\")\n+        throw e\n+    }\n+  }\n+\n+  scheduleTokenRenewal()\n+\n+  private def getSecretFile(conf: SparkConf): (Option[String], String) = {\n+    val keytab = conf.get(config.KEYTAB)\n+    val tgt = Option(conf.getenv(SparkHadoopUtil.TICKET_CACHE_ENVVAR))\n+    val (secretFile, mode) = if (keytab.isDefined && tgt.isDefined) {\n+      // if a keytab and a specific ticket cache is specified use the keytab and log the behavior\n+      logWarning(s\"Keytab and TGT were detected, using keytab, ${keytab.get}, \" +\n+        s\"unset ${config.KEYTAB.key} to use TGT (${tgt.get})\")\n+      (keytab, \"keytab\")\n+    } else {\n+      val m = if (keytab.isDefined) \"keytab\" else \"tgt\"\n+      val sf = if (keytab.isDefined) keytab else tgt\n+      (sf, m)\n+    }\n+\n+    if (principal == null) {\n+      require(mode == \"tgt\", s\"Must specify a principal when using a Keytab, was $principal\")\n+      logInfo(s\"Using ticket cache to fetch Hadoop delegation tokens\")\n+    } else {\n+      logInfo(s\"Using principal: $principal with mode and keytab $keytab \" +\n+        s\"to fetch Hadoop delegation tokens\")\n+    }\n+\n+    logDebug(s\"secretFile is $secretFile\")\n+    (secretFile, mode)\n+  }\n+\n+  private def scheduleTokenRenewal(): Unit = {\n+    def scheduleRenewal(runnable: Runnable): Unit = {\n+      val remainingTime = timeOfNextRenewal - System.currentTimeMillis()\n+      if (remainingTime <= 0) {\n+        logInfo(\"Credentials have expired, creating new ones now.\")\n+        runnable.run()\n+      } else {\n+        logInfo(s\"Scheduling login from keytab in $remainingTime millis.\")\n+        credentialRenewerThread.schedule(runnable, remainingTime, TimeUnit.MILLISECONDS)\n+      }\n+    }\n+\n+    val credentialRenewerRunnable =\n+      new Runnable {\n+        override def run(): Unit = {\n+          try {\n+            val tokensBytes = getNewDelegationTokens\n+            broadcastDelegationTokens(tokensBytes)\n+          } catch {\n+            case e: Exception =>\n+              // Log the error and try to write new tokens back in an hour\n+              logWarning(\"Couldn't broadcast tokens, trying again in an hour\", e)\n+              credentialRenewerThread.schedule(this, 1, TimeUnit.HOURS)\n+              return\n+          }\n+          scheduleRenewal(this)\n+        }\n+      }\n+    scheduleRenewal(credentialRenewerRunnable)\n+  }\n+\n+  private def getNewDelegationTokens(): Array[Byte] = {\n+    logInfo(s\"Attempting to login to KDC with ${conf.get(config.PRINCIPAL).orNull}\")\n+    // Get new delegation tokens by logging in with a new UGI\n+    // inspired by AMCredentialRenewer.scala:L174\n+    val ugi = if (mode == \"keytab\") {\n+      UserGroupInformation.loginUserFromKeytabAndReturnUGI(principal, secretFile.get)\n+    } else {\n+      // if the ticket cache is not explicitly defined, use the default\n+      if (secretFile.isEmpty) {"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "> How do the executors get the TGT though? \r\n\r\nThey don't. That's why you're creating delegation tokens and sending *them* to the executor.",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-11-14T01:00:46Z",
    "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{ScheduledExecutorService, TimeUnit}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+/**\n+ * The MesosHadoopDelegationTokenManager fetches and updates Hadoop delegation tokens on the behalf\n+ * of the MesosCoarseGrainedSchedulerBackend. It is modeled after the YARN AMCredentialRenewer,\n+ * and similarly will renew the Credentials when 75% of the renewal interval has passed.\n+ * The principal difference is that instead of writing the new credentials to HDFS and\n+ * incrementing the timestamp of the file, the new credentials (called Tokens when they are\n+ * serialized) are broadcast to all running executors. On the executor side, when new Tokens are\n+ * received they overwrite the current credentials.\n+ */\n+private[spark] class MesosHadoopDelegationTokenManager(\n+    conf: SparkConf,\n+    hadoopConfig: Configuration,\n+    driverEndpoint: RpcEndpointRef)\n+  extends Logging {\n+\n+  require(driverEndpoint != null, \"DriverEndpoint is not initialized\")\n+\n+  private val credentialRenewerThread: ScheduledExecutorService =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"Credential Renewal Thread\")\n+\n+  private val tokenManager: HadoopDelegationTokenManager =\n+    new HadoopDelegationTokenManager(conf, hadoopConfig)\n+\n+  private val principal: String = conf.get(config.PRINCIPAL).orNull\n+\n+  private val (secretFile: Option[String], mode: String) = getSecretFile(conf)\n+\n+  private var (tokens: Array[Byte], timeOfNextRenewal: Long) = {\n+    try {\n+      val creds = UserGroupInformation.getCurrentUser.getCredentials\n+      val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+      val rt = tokenManager.obtainDelegationTokens(hadoopConf, creds)\n+      logInfo(s\"Initialized tokens: ${SparkHadoopUtil.get.dumpTokens(creds)}\")\n+      (SparkHadoopUtil.get.serialize(creds), rt)\n+    } catch {\n+      case e: Exception =>\n+        logError(\"Failed to initialize Hadoop delegation tokens\\n\" +\n+          s\"\\tPricipal: $principal\\n\\tmode: $mode\\n\\tsecret file $secretFile\\n\\tException: $e\")\n+        throw e\n+    }\n+  }\n+\n+  scheduleTokenRenewal()\n+\n+  private def getSecretFile(conf: SparkConf): (Option[String], String) = {\n+    val keytab = conf.get(config.KEYTAB)\n+    val tgt = Option(conf.getenv(SparkHadoopUtil.TICKET_CACHE_ENVVAR))\n+    val (secretFile, mode) = if (keytab.isDefined && tgt.isDefined) {\n+      // if a keytab and a specific ticket cache is specified use the keytab and log the behavior\n+      logWarning(s\"Keytab and TGT were detected, using keytab, ${keytab.get}, \" +\n+        s\"unset ${config.KEYTAB.key} to use TGT (${tgt.get})\")\n+      (keytab, \"keytab\")\n+    } else {\n+      val m = if (keytab.isDefined) \"keytab\" else \"tgt\"\n+      val sf = if (keytab.isDefined) keytab else tgt\n+      (sf, m)\n+    }\n+\n+    if (principal == null) {\n+      require(mode == \"tgt\", s\"Must specify a principal when using a Keytab, was $principal\")\n+      logInfo(s\"Using ticket cache to fetch Hadoop delegation tokens\")\n+    } else {\n+      logInfo(s\"Using principal: $principal with mode and keytab $keytab \" +\n+        s\"to fetch Hadoop delegation tokens\")\n+    }\n+\n+    logDebug(s\"secretFile is $secretFile\")\n+    (secretFile, mode)\n+  }\n+\n+  private def scheduleTokenRenewal(): Unit = {\n+    def scheduleRenewal(runnable: Runnable): Unit = {\n+      val remainingTime = timeOfNextRenewal - System.currentTimeMillis()\n+      if (remainingTime <= 0) {\n+        logInfo(\"Credentials have expired, creating new ones now.\")\n+        runnable.run()\n+      } else {\n+        logInfo(s\"Scheduling login from keytab in $remainingTime millis.\")\n+        credentialRenewerThread.schedule(runnable, remainingTime, TimeUnit.MILLISECONDS)\n+      }\n+    }\n+\n+    val credentialRenewerRunnable =\n+      new Runnable {\n+        override def run(): Unit = {\n+          try {\n+            val tokensBytes = getNewDelegationTokens\n+            broadcastDelegationTokens(tokensBytes)\n+          } catch {\n+            case e: Exception =>\n+              // Log the error and try to write new tokens back in an hour\n+              logWarning(\"Couldn't broadcast tokens, trying again in an hour\", e)\n+              credentialRenewerThread.schedule(this, 1, TimeUnit.HOURS)\n+              return\n+          }\n+          scheduleRenewal(this)\n+        }\n+      }\n+    scheduleRenewal(credentialRenewerRunnable)\n+  }\n+\n+  private def getNewDelegationTokens(): Array[Byte] = {\n+    logInfo(s\"Attempting to login to KDC with ${conf.get(config.PRINCIPAL).orNull}\")\n+    // Get new delegation tokens by logging in with a new UGI\n+    // inspired by AMCredentialRenewer.scala:L174\n+    val ugi = if (mode == \"keytab\") {\n+      UserGroupInformation.loginUserFromKeytabAndReturnUGI(principal, secretFile.get)\n+    } else {\n+      // if the ticket cache is not explicitly defined, use the default\n+      if (secretFile.isEmpty) {"
  }, {
    "author": {
      "login": "ArtRand"
    },
    "body": "Ah gotcha. I misunderstood your previous comment to mean that you wouldn’t need to renew the tokens when using the ticket cache. I’ll simplify the logic. Thanks. ",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-11-14T01:44:02Z",
    "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{ScheduledExecutorService, TimeUnit}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+/**\n+ * The MesosHadoopDelegationTokenManager fetches and updates Hadoop delegation tokens on the behalf\n+ * of the MesosCoarseGrainedSchedulerBackend. It is modeled after the YARN AMCredentialRenewer,\n+ * and similarly will renew the Credentials when 75% of the renewal interval has passed.\n+ * The principal difference is that instead of writing the new credentials to HDFS and\n+ * incrementing the timestamp of the file, the new credentials (called Tokens when they are\n+ * serialized) are broadcast to all running executors. On the executor side, when new Tokens are\n+ * received they overwrite the current credentials.\n+ */\n+private[spark] class MesosHadoopDelegationTokenManager(\n+    conf: SparkConf,\n+    hadoopConfig: Configuration,\n+    driverEndpoint: RpcEndpointRef)\n+  extends Logging {\n+\n+  require(driverEndpoint != null, \"DriverEndpoint is not initialized\")\n+\n+  private val credentialRenewerThread: ScheduledExecutorService =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"Credential Renewal Thread\")\n+\n+  private val tokenManager: HadoopDelegationTokenManager =\n+    new HadoopDelegationTokenManager(conf, hadoopConfig)\n+\n+  private val principal: String = conf.get(config.PRINCIPAL).orNull\n+\n+  private val (secretFile: Option[String], mode: String) = getSecretFile(conf)\n+\n+  private var (tokens: Array[Byte], timeOfNextRenewal: Long) = {\n+    try {\n+      val creds = UserGroupInformation.getCurrentUser.getCredentials\n+      val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+      val rt = tokenManager.obtainDelegationTokens(hadoopConf, creds)\n+      logInfo(s\"Initialized tokens: ${SparkHadoopUtil.get.dumpTokens(creds)}\")\n+      (SparkHadoopUtil.get.serialize(creds), rt)\n+    } catch {\n+      case e: Exception =>\n+        logError(\"Failed to initialize Hadoop delegation tokens\\n\" +\n+          s\"\\tPricipal: $principal\\n\\tmode: $mode\\n\\tsecret file $secretFile\\n\\tException: $e\")\n+        throw e\n+    }\n+  }\n+\n+  scheduleTokenRenewal()\n+\n+  private def getSecretFile(conf: SparkConf): (Option[String], String) = {\n+    val keytab = conf.get(config.KEYTAB)\n+    val tgt = Option(conf.getenv(SparkHadoopUtil.TICKET_CACHE_ENVVAR))\n+    val (secretFile, mode) = if (keytab.isDefined && tgt.isDefined) {\n+      // if a keytab and a specific ticket cache is specified use the keytab and log the behavior\n+      logWarning(s\"Keytab and TGT were detected, using keytab, ${keytab.get}, \" +\n+        s\"unset ${config.KEYTAB.key} to use TGT (${tgt.get})\")\n+      (keytab, \"keytab\")\n+    } else {\n+      val m = if (keytab.isDefined) \"keytab\" else \"tgt\"\n+      val sf = if (keytab.isDefined) keytab else tgt\n+      (sf, m)\n+    }\n+\n+    if (principal == null) {\n+      require(mode == \"tgt\", s\"Must specify a principal when using a Keytab, was $principal\")\n+      logInfo(s\"Using ticket cache to fetch Hadoop delegation tokens\")\n+    } else {\n+      logInfo(s\"Using principal: $principal with mode and keytab $keytab \" +\n+        s\"to fetch Hadoop delegation tokens\")\n+    }\n+\n+    logDebug(s\"secretFile is $secretFile\")\n+    (secretFile, mode)\n+  }\n+\n+  private def scheduleTokenRenewal(): Unit = {\n+    def scheduleRenewal(runnable: Runnable): Unit = {\n+      val remainingTime = timeOfNextRenewal - System.currentTimeMillis()\n+      if (remainingTime <= 0) {\n+        logInfo(\"Credentials have expired, creating new ones now.\")\n+        runnable.run()\n+      } else {\n+        logInfo(s\"Scheduling login from keytab in $remainingTime millis.\")\n+        credentialRenewerThread.schedule(runnable, remainingTime, TimeUnit.MILLISECONDS)\n+      }\n+    }\n+\n+    val credentialRenewerRunnable =\n+      new Runnable {\n+        override def run(): Unit = {\n+          try {\n+            val tokensBytes = getNewDelegationTokens\n+            broadcastDelegationTokens(tokensBytes)\n+          } catch {\n+            case e: Exception =>\n+              // Log the error and try to write new tokens back in an hour\n+              logWarning(\"Couldn't broadcast tokens, trying again in an hour\", e)\n+              credentialRenewerThread.schedule(this, 1, TimeUnit.HOURS)\n+              return\n+          }\n+          scheduleRenewal(this)\n+        }\n+      }\n+    scheduleRenewal(credentialRenewerRunnable)\n+  }\n+\n+  private def getNewDelegationTokens(): Array[Byte] = {\n+    logInfo(s\"Attempting to login to KDC with ${conf.get(config.PRINCIPAL).orNull}\")\n+    // Get new delegation tokens by logging in with a new UGI\n+    // inspired by AMCredentialRenewer.scala:L174\n+    val ugi = if (mode == \"keytab\") {\n+      UserGroupInformation.loginUserFromKeytabAndReturnUGI(principal, secretFile.get)\n+    } else {\n+      // if the ticket cache is not explicitly defined, use the default\n+      if (secretFile.isEmpty) {"
  }, {
    "author": {
      "login": "ArtRand"
    },
    "body": "So `getCurrentUser` actually doesn't work. I believe for the reason mentioned [here](https://github.com/apache/spark/blob/a18d637112b97d2caaca0a8324bdd99086664b24/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/security/AMCredentialRenewer.scala#L152). The same expired credentials are returned (causing the renewer to loop). \r\n\r\nI understand that optionally using a TGT instead of a keytab is different than the YARN reference implementation, and it's unusual to use it in this case since it expires anyways. Do you think it would be better to avoid the ticket renewal logic all together when using a TGT or to keep the older `UserGroupInformation.getUGIFromTicketCache`-based method? ",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-11-14T19:12:55Z",
    "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{ScheduledExecutorService, TimeUnit}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+/**\n+ * The MesosHadoopDelegationTokenManager fetches and updates Hadoop delegation tokens on the behalf\n+ * of the MesosCoarseGrainedSchedulerBackend. It is modeled after the YARN AMCredentialRenewer,\n+ * and similarly will renew the Credentials when 75% of the renewal interval has passed.\n+ * The principal difference is that instead of writing the new credentials to HDFS and\n+ * incrementing the timestamp of the file, the new credentials (called Tokens when they are\n+ * serialized) are broadcast to all running executors. On the executor side, when new Tokens are\n+ * received they overwrite the current credentials.\n+ */\n+private[spark] class MesosHadoopDelegationTokenManager(\n+    conf: SparkConf,\n+    hadoopConfig: Configuration,\n+    driverEndpoint: RpcEndpointRef)\n+  extends Logging {\n+\n+  require(driverEndpoint != null, \"DriverEndpoint is not initialized\")\n+\n+  private val credentialRenewerThread: ScheduledExecutorService =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"Credential Renewal Thread\")\n+\n+  private val tokenManager: HadoopDelegationTokenManager =\n+    new HadoopDelegationTokenManager(conf, hadoopConfig)\n+\n+  private val principal: String = conf.get(config.PRINCIPAL).orNull\n+\n+  private val (secretFile: Option[String], mode: String) = getSecretFile(conf)\n+\n+  private var (tokens: Array[Byte], timeOfNextRenewal: Long) = {\n+    try {\n+      val creds = UserGroupInformation.getCurrentUser.getCredentials\n+      val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+      val rt = tokenManager.obtainDelegationTokens(hadoopConf, creds)\n+      logInfo(s\"Initialized tokens: ${SparkHadoopUtil.get.dumpTokens(creds)}\")\n+      (SparkHadoopUtil.get.serialize(creds), rt)\n+    } catch {\n+      case e: Exception =>\n+        logError(\"Failed to initialize Hadoop delegation tokens\\n\" +\n+          s\"\\tPricipal: $principal\\n\\tmode: $mode\\n\\tsecret file $secretFile\\n\\tException: $e\")\n+        throw e\n+    }\n+  }\n+\n+  scheduleTokenRenewal()\n+\n+  private def getSecretFile(conf: SparkConf): (Option[String], String) = {\n+    val keytab = conf.get(config.KEYTAB)\n+    val tgt = Option(conf.getenv(SparkHadoopUtil.TICKET_CACHE_ENVVAR))\n+    val (secretFile, mode) = if (keytab.isDefined && tgt.isDefined) {\n+      // if a keytab and a specific ticket cache is specified use the keytab and log the behavior\n+      logWarning(s\"Keytab and TGT were detected, using keytab, ${keytab.get}, \" +\n+        s\"unset ${config.KEYTAB.key} to use TGT (${tgt.get})\")\n+      (keytab, \"keytab\")\n+    } else {\n+      val m = if (keytab.isDefined) \"keytab\" else \"tgt\"\n+      val sf = if (keytab.isDefined) keytab else tgt\n+      (sf, m)\n+    }\n+\n+    if (principal == null) {\n+      require(mode == \"tgt\", s\"Must specify a principal when using a Keytab, was $principal\")\n+      logInfo(s\"Using ticket cache to fetch Hadoop delegation tokens\")\n+    } else {\n+      logInfo(s\"Using principal: $principal with mode and keytab $keytab \" +\n+        s\"to fetch Hadoop delegation tokens\")\n+    }\n+\n+    logDebug(s\"secretFile is $secretFile\")\n+    (secretFile, mode)\n+  }\n+\n+  private def scheduleTokenRenewal(): Unit = {\n+    def scheduleRenewal(runnable: Runnable): Unit = {\n+      val remainingTime = timeOfNextRenewal - System.currentTimeMillis()\n+      if (remainingTime <= 0) {\n+        logInfo(\"Credentials have expired, creating new ones now.\")\n+        runnable.run()\n+      } else {\n+        logInfo(s\"Scheduling login from keytab in $remainingTime millis.\")\n+        credentialRenewerThread.schedule(runnable, remainingTime, TimeUnit.MILLISECONDS)\n+      }\n+    }\n+\n+    val credentialRenewerRunnable =\n+      new Runnable {\n+        override def run(): Unit = {\n+          try {\n+            val tokensBytes = getNewDelegationTokens\n+            broadcastDelegationTokens(tokensBytes)\n+          } catch {\n+            case e: Exception =>\n+              // Log the error and try to write new tokens back in an hour\n+              logWarning(\"Couldn't broadcast tokens, trying again in an hour\", e)\n+              credentialRenewerThread.schedule(this, 1, TimeUnit.HOURS)\n+              return\n+          }\n+          scheduleRenewal(this)\n+        }\n+      }\n+    scheduleRenewal(credentialRenewerRunnable)\n+  }\n+\n+  private def getNewDelegationTokens(): Array[Byte] = {\n+    logInfo(s\"Attempting to login to KDC with ${conf.get(config.PRINCIPAL).orNull}\")\n+    // Get new delegation tokens by logging in with a new UGI\n+    // inspired by AMCredentialRenewer.scala:L174\n+    val ugi = if (mode == \"keytab\") {\n+      UserGroupInformation.loginUserFromKeytabAndReturnUGI(principal, secretFile.get)\n+    } else {\n+      // if the ticket cache is not explicitly defined, use the default\n+      if (secretFile.isEmpty) {"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "> Do you think it would be better to avoid the ticket renewal logic all together when using a TGT\r\n\r\nIf `getCurrentUser` does not work, then yes, that's probably the best way forward. Requiring the user to set `KRB5CCNAME` is not really a good way to go about this. Also because in that case the user still has to make sure the TGT is updated through other means.",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-11-14T19:17:24Z",
    "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{ScheduledExecutorService, TimeUnit}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+/**\n+ * The MesosHadoopDelegationTokenManager fetches and updates Hadoop delegation tokens on the behalf\n+ * of the MesosCoarseGrainedSchedulerBackend. It is modeled after the YARN AMCredentialRenewer,\n+ * and similarly will renew the Credentials when 75% of the renewal interval has passed.\n+ * The principal difference is that instead of writing the new credentials to HDFS and\n+ * incrementing the timestamp of the file, the new credentials (called Tokens when they are\n+ * serialized) are broadcast to all running executors. On the executor side, when new Tokens are\n+ * received they overwrite the current credentials.\n+ */\n+private[spark] class MesosHadoopDelegationTokenManager(\n+    conf: SparkConf,\n+    hadoopConfig: Configuration,\n+    driverEndpoint: RpcEndpointRef)\n+  extends Logging {\n+\n+  require(driverEndpoint != null, \"DriverEndpoint is not initialized\")\n+\n+  private val credentialRenewerThread: ScheduledExecutorService =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"Credential Renewal Thread\")\n+\n+  private val tokenManager: HadoopDelegationTokenManager =\n+    new HadoopDelegationTokenManager(conf, hadoopConfig)\n+\n+  private val principal: String = conf.get(config.PRINCIPAL).orNull\n+\n+  private val (secretFile: Option[String], mode: String) = getSecretFile(conf)\n+\n+  private var (tokens: Array[Byte], timeOfNextRenewal: Long) = {\n+    try {\n+      val creds = UserGroupInformation.getCurrentUser.getCredentials\n+      val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+      val rt = tokenManager.obtainDelegationTokens(hadoopConf, creds)\n+      logInfo(s\"Initialized tokens: ${SparkHadoopUtil.get.dumpTokens(creds)}\")\n+      (SparkHadoopUtil.get.serialize(creds), rt)\n+    } catch {\n+      case e: Exception =>\n+        logError(\"Failed to initialize Hadoop delegation tokens\\n\" +\n+          s\"\\tPricipal: $principal\\n\\tmode: $mode\\n\\tsecret file $secretFile\\n\\tException: $e\")\n+        throw e\n+    }\n+  }\n+\n+  scheduleTokenRenewal()\n+\n+  private def getSecretFile(conf: SparkConf): (Option[String], String) = {\n+    val keytab = conf.get(config.KEYTAB)\n+    val tgt = Option(conf.getenv(SparkHadoopUtil.TICKET_CACHE_ENVVAR))\n+    val (secretFile, mode) = if (keytab.isDefined && tgt.isDefined) {\n+      // if a keytab and a specific ticket cache is specified use the keytab and log the behavior\n+      logWarning(s\"Keytab and TGT were detected, using keytab, ${keytab.get}, \" +\n+        s\"unset ${config.KEYTAB.key} to use TGT (${tgt.get})\")\n+      (keytab, \"keytab\")\n+    } else {\n+      val m = if (keytab.isDefined) \"keytab\" else \"tgt\"\n+      val sf = if (keytab.isDefined) keytab else tgt\n+      (sf, m)\n+    }\n+\n+    if (principal == null) {\n+      require(mode == \"tgt\", s\"Must specify a principal when using a Keytab, was $principal\")\n+      logInfo(s\"Using ticket cache to fetch Hadoop delegation tokens\")\n+    } else {\n+      logInfo(s\"Using principal: $principal with mode and keytab $keytab \" +\n+        s\"to fetch Hadoop delegation tokens\")\n+    }\n+\n+    logDebug(s\"secretFile is $secretFile\")\n+    (secretFile, mode)\n+  }\n+\n+  private def scheduleTokenRenewal(): Unit = {\n+    def scheduleRenewal(runnable: Runnable): Unit = {\n+      val remainingTime = timeOfNextRenewal - System.currentTimeMillis()\n+      if (remainingTime <= 0) {\n+        logInfo(\"Credentials have expired, creating new ones now.\")\n+        runnable.run()\n+      } else {\n+        logInfo(s\"Scheduling login from keytab in $remainingTime millis.\")\n+        credentialRenewerThread.schedule(runnable, remainingTime, TimeUnit.MILLISECONDS)\n+      }\n+    }\n+\n+    val credentialRenewerRunnable =\n+      new Runnable {\n+        override def run(): Unit = {\n+          try {\n+            val tokensBytes = getNewDelegationTokens\n+            broadcastDelegationTokens(tokensBytes)\n+          } catch {\n+            case e: Exception =>\n+              // Log the error and try to write new tokens back in an hour\n+              logWarning(\"Couldn't broadcast tokens, trying again in an hour\", e)\n+              credentialRenewerThread.schedule(this, 1, TimeUnit.HOURS)\n+              return\n+          }\n+          scheduleRenewal(this)\n+        }\n+      }\n+    scheduleRenewal(credentialRenewerRunnable)\n+  }\n+\n+  private def getNewDelegationTokens(): Array[Byte] = {\n+    logInfo(s\"Attempting to login to KDC with ${conf.get(config.PRINCIPAL).orNull}\")\n+    // Get new delegation tokens by logging in with a new UGI\n+    // inspired by AMCredentialRenewer.scala:L174\n+    val ugi = if (mode == \"keytab\") {\n+      UserGroupInformation.loginUserFromKeytabAndReturnUGI(principal, secretFile.get)\n+    } else {\n+      // if the ticket cache is not explicitly defined, use the default\n+      if (secretFile.isEmpty) {"
  }, {
    "author": {
      "login": "ArtRand"
    },
    "body": "Agreed, thanks. ",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-11-14T19:45:55Z",
    "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{ScheduledExecutorService, TimeUnit}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+/**\n+ * The MesosHadoopDelegationTokenManager fetches and updates Hadoop delegation tokens on the behalf\n+ * of the MesosCoarseGrainedSchedulerBackend. It is modeled after the YARN AMCredentialRenewer,\n+ * and similarly will renew the Credentials when 75% of the renewal interval has passed.\n+ * The principal difference is that instead of writing the new credentials to HDFS and\n+ * incrementing the timestamp of the file, the new credentials (called Tokens when they are\n+ * serialized) are broadcast to all running executors. On the executor side, when new Tokens are\n+ * received they overwrite the current credentials.\n+ */\n+private[spark] class MesosHadoopDelegationTokenManager(\n+    conf: SparkConf,\n+    hadoopConfig: Configuration,\n+    driverEndpoint: RpcEndpointRef)\n+  extends Logging {\n+\n+  require(driverEndpoint != null, \"DriverEndpoint is not initialized\")\n+\n+  private val credentialRenewerThread: ScheduledExecutorService =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"Credential Renewal Thread\")\n+\n+  private val tokenManager: HadoopDelegationTokenManager =\n+    new HadoopDelegationTokenManager(conf, hadoopConfig)\n+\n+  private val principal: String = conf.get(config.PRINCIPAL).orNull\n+\n+  private val (secretFile: Option[String], mode: String) = getSecretFile(conf)\n+\n+  private var (tokens: Array[Byte], timeOfNextRenewal: Long) = {\n+    try {\n+      val creds = UserGroupInformation.getCurrentUser.getCredentials\n+      val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+      val rt = tokenManager.obtainDelegationTokens(hadoopConf, creds)\n+      logInfo(s\"Initialized tokens: ${SparkHadoopUtil.get.dumpTokens(creds)}\")\n+      (SparkHadoopUtil.get.serialize(creds), rt)\n+    } catch {\n+      case e: Exception =>\n+        logError(\"Failed to initialize Hadoop delegation tokens\\n\" +\n+          s\"\\tPricipal: $principal\\n\\tmode: $mode\\n\\tsecret file $secretFile\\n\\tException: $e\")\n+        throw e\n+    }\n+  }\n+\n+  scheduleTokenRenewal()\n+\n+  private def getSecretFile(conf: SparkConf): (Option[String], String) = {\n+    val keytab = conf.get(config.KEYTAB)\n+    val tgt = Option(conf.getenv(SparkHadoopUtil.TICKET_CACHE_ENVVAR))\n+    val (secretFile, mode) = if (keytab.isDefined && tgt.isDefined) {\n+      // if a keytab and a specific ticket cache is specified use the keytab and log the behavior\n+      logWarning(s\"Keytab and TGT were detected, using keytab, ${keytab.get}, \" +\n+        s\"unset ${config.KEYTAB.key} to use TGT (${tgt.get})\")\n+      (keytab, \"keytab\")\n+    } else {\n+      val m = if (keytab.isDefined) \"keytab\" else \"tgt\"\n+      val sf = if (keytab.isDefined) keytab else tgt\n+      (sf, m)\n+    }\n+\n+    if (principal == null) {\n+      require(mode == \"tgt\", s\"Must specify a principal when using a Keytab, was $principal\")\n+      logInfo(s\"Using ticket cache to fetch Hadoop delegation tokens\")\n+    } else {\n+      logInfo(s\"Using principal: $principal with mode and keytab $keytab \" +\n+        s\"to fetch Hadoop delegation tokens\")\n+    }\n+\n+    logDebug(s\"secretFile is $secretFile\")\n+    (secretFile, mode)\n+  }\n+\n+  private def scheduleTokenRenewal(): Unit = {\n+    def scheduleRenewal(runnable: Runnable): Unit = {\n+      val remainingTime = timeOfNextRenewal - System.currentTimeMillis()\n+      if (remainingTime <= 0) {\n+        logInfo(\"Credentials have expired, creating new ones now.\")\n+        runnable.run()\n+      } else {\n+        logInfo(s\"Scheduling login from keytab in $remainingTime millis.\")\n+        credentialRenewerThread.schedule(runnable, remainingTime, TimeUnit.MILLISECONDS)\n+      }\n+    }\n+\n+    val credentialRenewerRunnable =\n+      new Runnable {\n+        override def run(): Unit = {\n+          try {\n+            val tokensBytes = getNewDelegationTokens\n+            broadcastDelegationTokens(tokensBytes)\n+          } catch {\n+            case e: Exception =>\n+              // Log the error and try to write new tokens back in an hour\n+              logWarning(\"Couldn't broadcast tokens, trying again in an hour\", e)\n+              credentialRenewerThread.schedule(this, 1, TimeUnit.HOURS)\n+              return\n+          }\n+          scheduleRenewal(this)\n+        }\n+      }\n+    scheduleRenewal(credentialRenewerRunnable)\n+  }\n+\n+  private def getNewDelegationTokens(): Array[Byte] = {\n+    logInfo(s\"Attempting to login to KDC with ${conf.get(config.PRINCIPAL).orNull}\")\n+    // Get new delegation tokens by logging in with a new UGI\n+    // inspired by AMCredentialRenewer.scala:L174\n+    val ugi = if (mode == \"keytab\") {\n+      UserGroupInformation.loginUserFromKeytabAndReturnUGI(principal, secretFile.get)\n+    } else {\n+      // if the ticket cache is not explicitly defined, use the default\n+      if (secretFile.isEmpty) {"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`tokens` is never updated, so `fetchHadoopDelegationTokens()` will always return the initial set even after it's expired.",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-11-15T18:49:55Z",
    "diffHunk": "@@ -0,0 +1,157 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{ScheduledExecutorService, TimeUnit}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+/**\n+ * The MesosHadoopDelegationTokenManager fetches and updates Hadoop delegation tokens on the behalf\n+ * of the MesosCoarseGrainedSchedulerBackend. It is modeled after the YARN AMCredentialRenewer,\n+ * and similarly will renew the Credentials when 75% of the renewal interval has passed.\n+ * The principal difference is that instead of writing the new credentials to HDFS and\n+ * incrementing the timestamp of the file, the new credentials (called Tokens when they are\n+ * serialized) are broadcast to all running executors. On the executor side, when new Tokens are\n+ * received they overwrite the current credentials.\n+ */\n+private[spark] class MesosHadoopDelegationTokenManager(\n+    conf: SparkConf,\n+    hadoopConfig: Configuration,\n+    driverEndpoint: RpcEndpointRef)\n+  extends Logging {\n+\n+  require(driverEndpoint != null, \"DriverEndpoint is not initialized\")\n+\n+  private val credentialRenewerThread: ScheduledExecutorService =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"Credential Renewal Thread\")\n+\n+  private val tokenManager: HadoopDelegationTokenManager =\n+    new HadoopDelegationTokenManager(conf, hadoopConfig)\n+\n+  private val principal: String = conf.get(config.PRINCIPAL).orNull\n+\n+  private var (tokens: Array[Byte], timeOfNextRenewal: Long) = {\n+    try {\n+      val creds = UserGroupInformation.getCurrentUser.getCredentials\n+      val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+      val rt = tokenManager.obtainDelegationTokens(hadoopConf, creds)\n+      logInfo(s\"Initialized tokens: ${SparkHadoopUtil.get.dumpTokens(creds)}\")\n+      (SparkHadoopUtil.get.serialize(creds), rt)\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Failed to fetch Hadoop delegation tokens $e\")\n+        throw e\n+    }\n+  }\n+\n+  private val keytabFile: Option[String] = conf.get(config.KEYTAB)\n+\n+  scheduleTokenRenewal()\n+\n+  private def scheduleTokenRenewal(): Unit = {\n+    if (keytabFile.isDefined) {\n+      require(principal != null, \"Principal is required for Keytab-based authentication\")\n+      logInfo(s\"Using keytab: ${keytabFile.get} and principal $principal\")\n+    } else {\n+      logInfo(\"Using ticket cache for Kerberos authentication, no token renewal.\")\n+      return\n+    }\n+\n+    def scheduleRenewal(runnable: Runnable): Unit = {\n+      val remainingTime = timeOfNextRenewal - System.currentTimeMillis()\n+      if (remainingTime <= 0) {\n+        logInfo(\"Credentials have expired, creating new ones now.\")\n+        runnable.run()\n+      } else {\n+        logInfo(s\"Scheduling login from keytab in $remainingTime millis.\")\n+        credentialRenewerThread.schedule(runnable, remainingTime, TimeUnit.MILLISECONDS)\n+      }\n+    }\n+\n+    val credentialRenewerRunnable =\n+      new Runnable {\n+        override def run(): Unit = {\n+          try {\n+            val tokensBytes = getNewDelegationTokens()\n+            broadcastDelegationTokens(tokensBytes)\n+          } catch {\n+            case e: Exception =>\n+              // Log the error and try to write new tokens back in an hour\n+              logWarning(\"Couldn't broadcast tokens, trying again in an hour\", e)\n+              credentialRenewerThread.schedule(this, 1, TimeUnit.HOURS)\n+              return\n+          }\n+          scheduleRenewal(this)\n+        }\n+      }\n+    scheduleRenewal(credentialRenewerRunnable)\n+  }\n+\n+  private def getNewDelegationTokens(): Array[Byte] = {\n+    logInfo(s\"Attempting to login to KDC with principal ${principal}\")\n+    // Get new delegation tokens by logging in with a new UGI\n+    // inspired by AMCredentialRenewer.scala:L174.\n+    // Don't protect against keytabFile being empty because it's guarded above.\n+    val ugi = UserGroupInformation.loginUserFromKeytabAndReturnUGI(principal, keytabFile.get)\n+    logInfo(\"Successfully logged into KDC\")\n+    val tempCreds = ugi.getCredentials\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    val nextRenewalTime = ugi.doAs(new PrivilegedExceptionAction[Long] {\n+      override def run(): Long = {\n+        tokenManager.obtainDelegationTokens(hadoopConf, tempCreds)\n+      }\n+    })\n+\n+    val currTime = System.currentTimeMillis()\n+    timeOfNextRenewal = if (nextRenewalTime <= currTime) {\n+      logWarning(s\"Next credential renewal time ($nextRenewalTime) is earlier than \" +\n+        s\"current time ($currTime), which is unexpected, please check your credential renewal \" +\n+        \"related configurations in the target services.\")\n+      currTime\n+    } else {\n+      SparkHadoopUtil.getDateOfNextUpdate(nextRenewalTime, 0.75)\n+    }\n+    logInfo(s\"Time of next renewal is in ${timeOfNextRenewal - System.currentTimeMillis()} ms\")\n+\n+    // Add the temp credentials back to the original ones.\n+    UserGroupInformation.getCurrentUser.addCredentials(tempCreds)\n+    SparkHadoopUtil.get.serialize(tempCreds)\n+  }\n+\n+  private def broadcastDelegationTokens(tokens: Array[Byte]) = {\n+    logInfo(\"Sending new tokens to all executors\")\n+    driverEndpoint.send(UpdateDelegationTokens(tokens))\n+  }\n+\n+  def getTokens(): Array[Byte] = {\n+    tokens",
    "line": 154
  }, {
    "author": {
      "login": "ArtRand"
    },
    "body": "Thanks for catching this, `tokens` are now updated for late-joining executors.  https://github.com/apache/spark/pull/19272/files#diff-765ac3c4db227cd2c5d796f00794016fR145",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-11-15T20:13:44Z",
    "diffHunk": "@@ -0,0 +1,157 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{ScheduledExecutorService, TimeUnit}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+/**\n+ * The MesosHadoopDelegationTokenManager fetches and updates Hadoop delegation tokens on the behalf\n+ * of the MesosCoarseGrainedSchedulerBackend. It is modeled after the YARN AMCredentialRenewer,\n+ * and similarly will renew the Credentials when 75% of the renewal interval has passed.\n+ * The principal difference is that instead of writing the new credentials to HDFS and\n+ * incrementing the timestamp of the file, the new credentials (called Tokens when they are\n+ * serialized) are broadcast to all running executors. On the executor side, when new Tokens are\n+ * received they overwrite the current credentials.\n+ */\n+private[spark] class MesosHadoopDelegationTokenManager(\n+    conf: SparkConf,\n+    hadoopConfig: Configuration,\n+    driverEndpoint: RpcEndpointRef)\n+  extends Logging {\n+\n+  require(driverEndpoint != null, \"DriverEndpoint is not initialized\")\n+\n+  private val credentialRenewerThread: ScheduledExecutorService =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"Credential Renewal Thread\")\n+\n+  private val tokenManager: HadoopDelegationTokenManager =\n+    new HadoopDelegationTokenManager(conf, hadoopConfig)\n+\n+  private val principal: String = conf.get(config.PRINCIPAL).orNull\n+\n+  private var (tokens: Array[Byte], timeOfNextRenewal: Long) = {\n+    try {\n+      val creds = UserGroupInformation.getCurrentUser.getCredentials\n+      val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+      val rt = tokenManager.obtainDelegationTokens(hadoopConf, creds)\n+      logInfo(s\"Initialized tokens: ${SparkHadoopUtil.get.dumpTokens(creds)}\")\n+      (SparkHadoopUtil.get.serialize(creds), rt)\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Failed to fetch Hadoop delegation tokens $e\")\n+        throw e\n+    }\n+  }\n+\n+  private val keytabFile: Option[String] = conf.get(config.KEYTAB)\n+\n+  scheduleTokenRenewal()\n+\n+  private def scheduleTokenRenewal(): Unit = {\n+    if (keytabFile.isDefined) {\n+      require(principal != null, \"Principal is required for Keytab-based authentication\")\n+      logInfo(s\"Using keytab: ${keytabFile.get} and principal $principal\")\n+    } else {\n+      logInfo(\"Using ticket cache for Kerberos authentication, no token renewal.\")\n+      return\n+    }\n+\n+    def scheduleRenewal(runnable: Runnable): Unit = {\n+      val remainingTime = timeOfNextRenewal - System.currentTimeMillis()\n+      if (remainingTime <= 0) {\n+        logInfo(\"Credentials have expired, creating new ones now.\")\n+        runnable.run()\n+      } else {\n+        logInfo(s\"Scheduling login from keytab in $remainingTime millis.\")\n+        credentialRenewerThread.schedule(runnable, remainingTime, TimeUnit.MILLISECONDS)\n+      }\n+    }\n+\n+    val credentialRenewerRunnable =\n+      new Runnable {\n+        override def run(): Unit = {\n+          try {\n+            val tokensBytes = getNewDelegationTokens()\n+            broadcastDelegationTokens(tokensBytes)\n+          } catch {\n+            case e: Exception =>\n+              // Log the error and try to write new tokens back in an hour\n+              logWarning(\"Couldn't broadcast tokens, trying again in an hour\", e)\n+              credentialRenewerThread.schedule(this, 1, TimeUnit.HOURS)\n+              return\n+          }\n+          scheduleRenewal(this)\n+        }\n+      }\n+    scheduleRenewal(credentialRenewerRunnable)\n+  }\n+\n+  private def getNewDelegationTokens(): Array[Byte] = {\n+    logInfo(s\"Attempting to login to KDC with principal ${principal}\")\n+    // Get new delegation tokens by logging in with a new UGI\n+    // inspired by AMCredentialRenewer.scala:L174.\n+    // Don't protect against keytabFile being empty because it's guarded above.\n+    val ugi = UserGroupInformation.loginUserFromKeytabAndReturnUGI(principal, keytabFile.get)\n+    logInfo(\"Successfully logged into KDC\")\n+    val tempCreds = ugi.getCredentials\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    val nextRenewalTime = ugi.doAs(new PrivilegedExceptionAction[Long] {\n+      override def run(): Long = {\n+        tokenManager.obtainDelegationTokens(hadoopConf, tempCreds)\n+      }\n+    })\n+\n+    val currTime = System.currentTimeMillis()\n+    timeOfNextRenewal = if (nextRenewalTime <= currTime) {\n+      logWarning(s\"Next credential renewal time ($nextRenewalTime) is earlier than \" +\n+        s\"current time ($currTime), which is unexpected, please check your credential renewal \" +\n+        \"related configurations in the target services.\")\n+      currTime\n+    } else {\n+      SparkHadoopUtil.getDateOfNextUpdate(nextRenewalTime, 0.75)\n+    }\n+    logInfo(s\"Time of next renewal is in ${timeOfNextRenewal - System.currentTimeMillis()} ms\")\n+\n+    // Add the temp credentials back to the original ones.\n+    UserGroupInformation.getCurrentUser.addCredentials(tempCreds)\n+    SparkHadoopUtil.get.serialize(tempCreds)\n+  }\n+\n+  private def broadcastDelegationTokens(tokens: Array[Byte]) = {\n+    logInfo(\"Sending new tokens to all executors\")\n+    driverEndpoint.send(UpdateDelegationTokens(tokens))\n+  }\n+\n+  def getTokens(): Array[Byte] = {\n+    tokens",
    "line": 154
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: mentioning line numbers will make this stale very quickly.",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-11-15T18:54:10Z",
    "diffHunk": "@@ -0,0 +1,157 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{ScheduledExecutorService, TimeUnit}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+/**\n+ * The MesosHadoopDelegationTokenManager fetches and updates Hadoop delegation tokens on the behalf\n+ * of the MesosCoarseGrainedSchedulerBackend. It is modeled after the YARN AMCredentialRenewer,\n+ * and similarly will renew the Credentials when 75% of the renewal interval has passed.\n+ * The principal difference is that instead of writing the new credentials to HDFS and\n+ * incrementing the timestamp of the file, the new credentials (called Tokens when they are\n+ * serialized) are broadcast to all running executors. On the executor side, when new Tokens are\n+ * received they overwrite the current credentials.\n+ */\n+private[spark] class MesosHadoopDelegationTokenManager(\n+    conf: SparkConf,\n+    hadoopConfig: Configuration,\n+    driverEndpoint: RpcEndpointRef)\n+  extends Logging {\n+\n+  require(driverEndpoint != null, \"DriverEndpoint is not initialized\")\n+\n+  private val credentialRenewerThread: ScheduledExecutorService =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"Credential Renewal Thread\")\n+\n+  private val tokenManager: HadoopDelegationTokenManager =\n+    new HadoopDelegationTokenManager(conf, hadoopConfig)\n+\n+  private val principal: String = conf.get(config.PRINCIPAL).orNull\n+\n+  private var (tokens: Array[Byte], timeOfNextRenewal: Long) = {\n+    try {\n+      val creds = UserGroupInformation.getCurrentUser.getCredentials\n+      val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+      val rt = tokenManager.obtainDelegationTokens(hadoopConf, creds)\n+      logInfo(s\"Initialized tokens: ${SparkHadoopUtil.get.dumpTokens(creds)}\")\n+      (SparkHadoopUtil.get.serialize(creds), rt)\n+    } catch {\n+      case e: Exception =>\n+        logError(s\"Failed to fetch Hadoop delegation tokens $e\")\n+        throw e\n+    }\n+  }\n+\n+  private val keytabFile: Option[String] = conf.get(config.KEYTAB)\n+\n+  scheduleTokenRenewal()\n+\n+  private def scheduleTokenRenewal(): Unit = {\n+    if (keytabFile.isDefined) {\n+      require(principal != null, \"Principal is required for Keytab-based authentication\")\n+      logInfo(s\"Using keytab: ${keytabFile.get} and principal $principal\")\n+    } else {\n+      logInfo(\"Using ticket cache for Kerberos authentication, no token renewal.\")\n+      return\n+    }\n+\n+    def scheduleRenewal(runnable: Runnable): Unit = {\n+      val remainingTime = timeOfNextRenewal - System.currentTimeMillis()\n+      if (remainingTime <= 0) {\n+        logInfo(\"Credentials have expired, creating new ones now.\")\n+        runnable.run()\n+      } else {\n+        logInfo(s\"Scheduling login from keytab in $remainingTime millis.\")\n+        credentialRenewerThread.schedule(runnable, remainingTime, TimeUnit.MILLISECONDS)\n+      }\n+    }\n+\n+    val credentialRenewerRunnable =\n+      new Runnable {\n+        override def run(): Unit = {\n+          try {\n+            val tokensBytes = getNewDelegationTokens()\n+            broadcastDelegationTokens(tokensBytes)\n+          } catch {\n+            case e: Exception =>\n+              // Log the error and try to write new tokens back in an hour\n+              logWarning(\"Couldn't broadcast tokens, trying again in an hour\", e)\n+              credentialRenewerThread.schedule(this, 1, TimeUnit.HOURS)\n+              return\n+          }\n+          scheduleRenewal(this)\n+        }\n+      }\n+    scheduleRenewal(credentialRenewerRunnable)\n+  }\n+\n+  private def getNewDelegationTokens(): Array[Byte] = {\n+    logInfo(s\"Attempting to login to KDC with principal ${principal}\")\n+    // Get new delegation tokens by logging in with a new UGI\n+    // inspired by AMCredentialRenewer.scala:L174."
  }],
  "prId": 19272
}]