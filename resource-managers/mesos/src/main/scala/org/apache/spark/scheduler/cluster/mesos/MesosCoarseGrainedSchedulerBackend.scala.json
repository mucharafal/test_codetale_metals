[{
  "comments": [{
    "author": {
      "login": "skonto"
    },
    "body": "hadoopDelegationCreds.get call. Should we check against none? Creds are loaded in CoarseGrainedSchedulerBackend but if they are missing we should fail here?",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-09-19T09:34:56Z",
    "diffHunk": "@@ -194,6 +198,24 @@ private[spark] class MesosCoarseGrainedSchedulerBackend(\n       sc.conf.getOption(\"spark.mesos.driver.frameworkId\").map(_ + suffix)\n     )\n \n+    if (principal != null) {\n+      logDebug(s\"Principal found ($principal) starting token renewer\")\n+      val credentialRenewerThread = new Thread {\n+        setName(\"MesosCredentialRenewer\")\n+        override def run(): Unit = {\n+          val credentialRenewer =\n+            new MesosCredentialRenewer(\n+              conf,\n+              hadoopDelegationTokenManager.get,\n+              MesosCredentialRenewer.getTokenRenewalInterval(hadoopDelegationCreds.get, conf),"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "susanxhuynh"
    },
    "body": "What is this for?",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-09-20T23:23:16Z",
    "diffHunk": "@@ -198,16 +198,19 @@ private[spark] class MesosCoarseGrainedSchedulerBackend(\n       sc.conf.getOption(\"spark.mesos.driver.frameworkId\").map(_ + suffix)\n     )\n \n-    if (principal != null) {\n+    // check that the credentials are defined, even though it's likely that auth would have failed\n+    // already if you've made it this far\n+    if (principal != null && hadoopDelegationCreds.isDefined) {\n       logDebug(s\"Principal found ($principal) starting token renewer\")\n       val credentialRenewerThread = new Thread {\n         setName(\"MesosCredentialRenewer\")\n         override def run(): Unit = {\n+          val dummy: Option[Array[Byte]] = None"
  }, {
    "author": {
      "login": "ArtRand"
    },
    "body": "whoops! ",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-09-20T23:29:30Z",
    "diffHunk": "@@ -198,16 +198,19 @@ private[spark] class MesosCoarseGrainedSchedulerBackend(\n       sc.conf.getOption(\"spark.mesos.driver.frameworkId\").map(_ + suffix)\n     )\n \n-    if (principal != null) {\n+    // check that the credentials are defined, even though it's likely that auth would have failed\n+    // already if you've made it this far\n+    if (principal != null && hadoopDelegationCreds.isDefined) {\n       logDebug(s\"Principal found ($principal) starting token renewer\")\n       val credentialRenewerThread = new Thread {\n         setName(\"MesosCredentialRenewer\")\n         override def run(): Unit = {\n+          val dummy: Option[Array[Byte]] = None"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "kalvinnchau"
    },
    "body": "This sets the first renewal time to be the expiration time of the token.\r\n\r\nIt should be similar to the way next renewal time in the MesosCredentialRenewer class is calculated so that it renews the first token after 75% of expiration time has passed:\r\n\r\n```scala\r\nval currTime = System.currentTimeMillis()\r\nval renewTime = MesosCredentialRenewer.getTokenRenewalTime(hadoopDelegationCreds.get, conf)\r\nval rt = 0.75 * (renewTime - currTime)\r\n\r\nval credentialRenewer =\r\n   new MesosCredentialRenewer(\r\n     conf,\r\n     hadoopDelegationTokenManager.get,\r\n     (currTime + rt).toLong,\r\n     driverEndpoint)\r\ncredentialRenewer.scheduleTokenRenewal()\r\n```\r\n",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-09-25T16:41:08Z",
    "diffHunk": "@@ -194,6 +198,26 @@ private[spark] class MesosCoarseGrainedSchedulerBackend(\n       sc.conf.getOption(\"spark.mesos.driver.frameworkId\").map(_ + suffix)\n     )\n \n+    // check that the credentials are defined, even though it's likely that auth would have failed\n+    // already if you've made it this far\n+    if (principal != null && hadoopDelegationCreds.isDefined) {\n+      logDebug(s\"Principal found ($principal) starting token renewer\")\n+      val credentialRenewerThread = new Thread {\n+        setName(\"MesosCredentialRenewer\")\n+        override def run(): Unit = {\n+          val credentialRenewer =\n+            new MesosCredentialRenewer(\n+              conf,\n+              hadoopDelegationTokenManager.get,\n+              MesosCredentialRenewer.getTokenRenewalTime(hadoopDelegationCreds.get, conf),"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This config is defined in core already (`PRINCIPAL`).",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-10-10T19:59:46Z",
    "diffHunk": "@@ -60,6 +62,8 @@ private[spark] class MesosCoarseGrainedSchedulerBackend(\n   override def hadoopDelegationTokenManager: Option[HadoopDelegationTokenManager] =\n     Some(new HadoopDelegationTokenManager(sc.conf, sc.hadoopConfiguration))\n \n+  private val principal = conf.get(\"spark.yarn.principal\", null)"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Why do you need this thread if it's going to be short-lived?",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-10-10T20:03:25Z",
    "diffHunk": "@@ -194,6 +198,27 @@ private[spark] class MesosCoarseGrainedSchedulerBackend(\n       sc.conf.getOption(\"spark.mesos.driver.frameworkId\").map(_ + suffix)\n     )\n \n+    // check that the credentials are defined, even though it's likely that auth would have failed\n+    // already if you've made it this far\n+    if (principal != null && hadoopDelegationCreds.isDefined) {\n+      logDebug(s\"Principal found ($principal) starting token renewer\")\n+      val credentialRenewerThread = new Thread {\n+        setName(\"MesosCredentialRenewer\")\n+        override def run(): Unit = {\n+          val rt = MesosCredentialRenewer.getTokenRenewalTime(hadoopDelegationCreds.get, conf)\n+          val credentialRenewer =\n+            new MesosCredentialRenewer(\n+              conf,\n+              hadoopDelegationTokenManager.get,\n+              MesosCredentialRenewer.getNextRenewalTime(rt),\n+              driverEndpoint)\n+          credentialRenewer.scheduleTokenRenewal()\n+        }\n+      }\n+\n+      credentialRenewerThread.start()\n+      credentialRenewerThread.join()"
  }, {
    "author": {
      "login": "ArtRand"
    },
    "body": "Yes, I believe so. If you look here (https://github.com/apache/spark/blob/6735433cde44b3430fb44edfff58ef8149c66c13/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala#L283) it's the same pattern. The thread needs to run as long as the application driver, correct? ",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-10-18T02:02:25Z",
    "diffHunk": "@@ -194,6 +198,27 @@ private[spark] class MesosCoarseGrainedSchedulerBackend(\n       sc.conf.getOption(\"spark.mesos.driver.frameworkId\").map(_ + suffix)\n     )\n \n+    // check that the credentials are defined, even though it's likely that auth would have failed\n+    // already if you've made it this far\n+    if (principal != null && hadoopDelegationCreds.isDefined) {\n+      logDebug(s\"Principal found ($principal) starting token renewer\")\n+      val credentialRenewerThread = new Thread {\n+        setName(\"MesosCredentialRenewer\")\n+        override def run(): Unit = {\n+          val rt = MesosCredentialRenewer.getTokenRenewalTime(hadoopDelegationCreds.get, conf)\n+          val credentialRenewer =\n+            new MesosCredentialRenewer(\n+              conf,\n+              hadoopDelegationTokenManager.get,\n+              MesosCredentialRenewer.getNextRenewalTime(rt),\n+              driverEndpoint)\n+          credentialRenewer.scheduleTokenRenewal()\n+        }\n+      }\n+\n+      credentialRenewerThread.start()\n+      credentialRenewerThread.join()"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "There's a comment explaining why that thread exists right above the code you linked. Did you look at it?\r\n\r\nAlso, you're calling `join()` on the thread, so it's obviously going away.",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-10-18T17:37:19Z",
    "diffHunk": "@@ -194,6 +198,27 @@ private[spark] class MesosCoarseGrainedSchedulerBackend(\n       sc.conf.getOption(\"spark.mesos.driver.frameworkId\").map(_ + suffix)\n     )\n \n+    // check that the credentials are defined, even though it's likely that auth would have failed\n+    // already if you've made it this far\n+    if (principal != null && hadoopDelegationCreds.isDefined) {\n+      logDebug(s\"Principal found ($principal) starting token renewer\")\n+      val credentialRenewerThread = new Thread {\n+        setName(\"MesosCredentialRenewer\")\n+        override def run(): Unit = {\n+          val rt = MesosCredentialRenewer.getTokenRenewalTime(hadoopDelegationCreds.get, conf)\n+          val credentialRenewer =\n+            new MesosCredentialRenewer(\n+              conf,\n+              hadoopDelegationTokenManager.get,\n+              MesosCredentialRenewer.getNextRenewalTime(rt),\n+              driverEndpoint)\n+          credentialRenewer.scheduleTokenRenewal()\n+        }\n+      }\n+\n+      credentialRenewerThread.start()\n+      credentialRenewerThread.join()"
  }, {
    "author": {
      "login": "ArtRand"
    },
    "body": "Yes, sorry, for some reason I understood you you to mean the credential renewer itself. I added a comment to the same effect as the YARN analogue. ",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-10-20T01:23:23Z",
    "diffHunk": "@@ -194,6 +198,27 @@ private[spark] class MesosCoarseGrainedSchedulerBackend(\n       sc.conf.getOption(\"spark.mesos.driver.frameworkId\").map(_ + suffix)\n     )\n \n+    // check that the credentials are defined, even though it's likely that auth would have failed\n+    // already if you've made it this far\n+    if (principal != null && hadoopDelegationCreds.isDefined) {\n+      logDebug(s\"Principal found ($principal) starting token renewer\")\n+      val credentialRenewerThread = new Thread {\n+        setName(\"MesosCredentialRenewer\")\n+        override def run(): Unit = {\n+          val rt = MesosCredentialRenewer.getTokenRenewalTime(hadoopDelegationCreds.get, conf)\n+          val credentialRenewer =\n+            new MesosCredentialRenewer(\n+              conf,\n+              hadoopDelegationTokenManager.get,\n+              MesosCredentialRenewer.getNextRenewalTime(rt),\n+              driverEndpoint)\n+          credentialRenewer.scheduleTokenRenewal()\n+        }\n+      }\n+\n+      credentialRenewerThread.start()\n+      credentialRenewerThread.join()"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "I don't think you really understood why the YARN code needs a thread and why I'm telling you this code does not. Read the comment you added here again; what makes you think the current thread does not have access to those classes?",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-10-20T19:50:08Z",
    "diffHunk": "@@ -194,6 +198,27 @@ private[spark] class MesosCoarseGrainedSchedulerBackend(\n       sc.conf.getOption(\"spark.mesos.driver.frameworkId\").map(_ + suffix)\n     )\n \n+    // check that the credentials are defined, even though it's likely that auth would have failed\n+    // already if you've made it this far\n+    if (principal != null && hadoopDelegationCreds.isDefined) {\n+      logDebug(s\"Principal found ($principal) starting token renewer\")\n+      val credentialRenewerThread = new Thread {\n+        setName(\"MesosCredentialRenewer\")\n+        override def run(): Unit = {\n+          val rt = MesosCredentialRenewer.getTokenRenewalTime(hadoopDelegationCreds.get, conf)\n+          val credentialRenewer =\n+            new MesosCredentialRenewer(\n+              conf,\n+              hadoopDelegationTokenManager.get,\n+              MesosCredentialRenewer.getNextRenewalTime(rt),\n+              driverEndpoint)\n+          credentialRenewer.scheduleTokenRenewal()\n+        }\n+      }\n+\n+      credentialRenewerThread.start()\n+      credentialRenewerThread.join()"
  }, {
    "author": {
      "login": "ArtRand"
    },
    "body": "Ok, you're probably right. It appears that the YARN code uses `setContextClassLoader(userClassLoader)` whereas in Mesos doesn't have `userClassLoader` or anything like it. Therefore we don't need the separate thread in the Mesos code. Do I have this correct? Thanks for showing me this!",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-10-21T02:15:08Z",
    "diffHunk": "@@ -194,6 +198,27 @@ private[spark] class MesosCoarseGrainedSchedulerBackend(\n       sc.conf.getOption(\"spark.mesos.driver.frameworkId\").map(_ + suffix)\n     )\n \n+    // check that the credentials are defined, even though it's likely that auth would have failed\n+    // already if you've made it this far\n+    if (principal != null && hadoopDelegationCreds.isDefined) {\n+      logDebug(s\"Principal found ($principal) starting token renewer\")\n+      val credentialRenewerThread = new Thread {\n+        setName(\"MesosCredentialRenewer\")\n+        override def run(): Unit = {\n+          val rt = MesosCredentialRenewer.getTokenRenewalTime(hadoopDelegationCreds.get, conf)\n+          val credentialRenewer =\n+            new MesosCredentialRenewer(\n+              conf,\n+              hadoopDelegationTokenManager.get,\n+              MesosCredentialRenewer.getNextRenewalTime(rt),\n+              driverEndpoint)\n+          credentialRenewer.scheduleTokenRenewal()\n+        }\n+      }\n+\n+      credentialRenewerThread.start()\n+      credentialRenewerThread.join()"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "That's the gist of it.",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-10-24T01:29:30Z",
    "diffHunk": "@@ -194,6 +198,27 @@ private[spark] class MesosCoarseGrainedSchedulerBackend(\n       sc.conf.getOption(\"spark.mesos.driver.frameworkId\").map(_ + suffix)\n     )\n \n+    // check that the credentials are defined, even though it's likely that auth would have failed\n+    // already if you've made it this far\n+    if (principal != null && hadoopDelegationCreds.isDefined) {\n+      logDebug(s\"Principal found ($principal) starting token renewer\")\n+      val credentialRenewerThread = new Thread {\n+        setName(\"MesosCredentialRenewer\")\n+        override def run(): Unit = {\n+          val rt = MesosCredentialRenewer.getTokenRenewalTime(hadoopDelegationCreds.get, conf)\n+          val credentialRenewer =\n+            new MesosCredentialRenewer(\n+              conf,\n+              hadoopDelegationTokenManager.get,\n+              MesosCredentialRenewer.getNextRenewalTime(rt),\n+              driverEndpoint)\n+          credentialRenewer.scheduleTokenRenewal()\n+        }\n+      }\n+\n+      credentialRenewerThread.start()\n+      credentialRenewerThread.join()"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "So, you need this because `hadoopDelegationCreds` doesn't keep the information about when the tokens should be renewed (a.k.a. the return value of `obtainDelegationTokens`). Perhaps some minor refactoring would help clean this up?\r\n\r\nIn fact, `hadoopDelegationCreds` is a `val`, so any executors that start after the initial token set expires will fail, no? Because they'll fetch `hadoopDelegationCreds` from the driver, and won't get the `UpdateDelegationTokens` until it's way too late.",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-10-10T21:25:43Z",
    "diffHunk": "@@ -194,6 +198,27 @@ private[spark] class MesosCoarseGrainedSchedulerBackend(\n       sc.conf.getOption(\"spark.mesos.driver.frameworkId\").map(_ + suffix)\n     )\n \n+    // check that the credentials are defined, even though it's likely that auth would have failed\n+    // already if you've made it this far\n+    if (principal != null && hadoopDelegationCreds.isDefined) {\n+      logDebug(s\"Principal found ($principal) starting token renewer\")\n+      val credentialRenewerThread = new Thread {\n+        setName(\"MesosCredentialRenewer\")\n+        override def run(): Unit = {\n+          val rt = MesosCredentialRenewer.getTokenRenewalTime(hadoopDelegationCreds.get, conf)"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "I still don't like this. You should not need to implement this separate method of getting the renewal time just because the current code is throwing out that information. Instead you should fix the code so that the information is preserved.\r\n\r\n`getHadoopDelegationCreds` is called in only one place, so my suggestion would be to encapsulate initializing the token manager and getting the initial set of tokens into a single method (instead of the current two).\r\n\r\nThen in that method's implementation you can get the initial set of tokens, initialize the renewer thread with the correct renewal period, and return the data needed by the scheduler.",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-10-24T01:47:36Z",
    "diffHunk": "@@ -213,6 +216,24 @@ private[spark] class MesosCoarseGrainedSchedulerBackend(\n       sc.conf.getOption(\"spark.mesos.driver.frameworkId\").map(_ + suffix)\n     )\n \n+    // check that the credentials are defined, even though it's likely that auth would have failed\n+    // already if you've made it this far\n+    if (principal != null && currentHadoopDelegationTokens.isDefined) {\n+      logDebug(s\"Principal found ($principal) starting token renewer\")\n+      // The renewal time is ignored when getting the initial delegation tokens\n+      // (CoarseGrainedSchedulerBackend.scala:getHadoopDelegationCreds), so we get the renewal\n+      // time here and schedule a thread to renew them.\n+      val renewalTime ="
  }, {
    "author": {
      "login": "ArtRand"
    },
    "body": "Ok, I agree. I refrained from touching the current code only because it was added recently. I assumed the omission of renewal time was intentional (or at least semi-intentional). I'll submit a refactor. Are you suggesting that we make the renewal thread part of `CoarseGrainedSchedulerBackend` (with the appropriate override for YARN/Mesos)?  Or make a resource-manager specific method?",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-10-24T07:26:01Z",
    "diffHunk": "@@ -213,6 +216,24 @@ private[spark] class MesosCoarseGrainedSchedulerBackend(\n       sc.conf.getOption(\"spark.mesos.driver.frameworkId\").map(_ + suffix)\n     )\n \n+    // check that the credentials are defined, even though it's likely that auth would have failed\n+    // already if you've made it this far\n+    if (principal != null && currentHadoopDelegationTokens.isDefined) {\n+      logDebug(s\"Principal found ($principal) starting token renewer\")\n+      // The renewal time is ignored when getting the initial delegation tokens\n+      // (CoarseGrainedSchedulerBackend.scala:getHadoopDelegationCreds), so we get the renewal\n+      // time here and schedule a thread to renew them.\n+      val renewalTime ="
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "It makes more sense to have it in `CoarseGrainedSchedulerBackend` (then YARN and Mesos can easily share it); my worry is exposing the functionality to standalone (it would work, but not be secure, which is a problem).\r\n\r\nSo maybe add a second method `def supportsTokenRenewal` that subclasses can override to return `true` or something.",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-10-24T16:46:08Z",
    "diffHunk": "@@ -213,6 +216,24 @@ private[spark] class MesosCoarseGrainedSchedulerBackend(\n       sc.conf.getOption(\"spark.mesos.driver.frameworkId\").map(_ + suffix)\n     )\n \n+    // check that the credentials are defined, even though it's likely that auth would have failed\n+    // already if you've made it this far\n+    if (principal != null && currentHadoopDelegationTokens.isDefined) {\n+      logDebug(s\"Principal found ($principal) starting token renewer\")\n+      // The renewal time is ignored when getting the initial delegation tokens\n+      // (CoarseGrainedSchedulerBackend.scala:getHadoopDelegationCreds), so we get the renewal\n+      // time here and schedule a thread to renew them.\n+      val renewalTime ="
  }, {
    "author": {
      "login": "ArtRand"
    },
    "body": "Maybe an overly simplistic solution, but what about putting this functionality into `HadoopDelegationTokenManager`?",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-10-25T13:26:57Z",
    "diffHunk": "@@ -213,6 +216,24 @@ private[spark] class MesosCoarseGrainedSchedulerBackend(\n       sc.conf.getOption(\"spark.mesos.driver.frameworkId\").map(_ + suffix)\n     )\n \n+    // check that the credentials are defined, even though it's likely that auth would have failed\n+    // already if you've made it this far\n+    if (principal != null && currentHadoopDelegationTokens.isDefined) {\n+      logDebug(s\"Principal found ($principal) starting token renewer\")\n+      // The renewal time is ignored when getting the initial delegation tokens\n+      // (CoarseGrainedSchedulerBackend.scala:getHadoopDelegationCreds), so we get the renewal\n+      // time here and schedule a thread to renew them.\n+      val renewalTime ="
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "I guess that would be ok. I was a little worried about mixing the two but they are kinda closely related...",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-10-25T18:20:34Z",
    "diffHunk": "@@ -213,6 +216,24 @@ private[spark] class MesosCoarseGrainedSchedulerBackend(\n       sc.conf.getOption(\"spark.mesos.driver.frameworkId\").map(_ + suffix)\n     )\n \n+    // check that the credentials are defined, even though it's likely that auth would have failed\n+    // already if you've made it this far\n+    if (principal != null && currentHadoopDelegationTokens.isDefined) {\n+      logDebug(s\"Principal found ($principal) starting token renewer\")\n+      // The renewal time is ignored when getting the initial delegation tokens\n+      // (CoarseGrainedSchedulerBackend.scala:getHadoopDelegationCreds), so we get the renewal\n+      // time here and schedule a thread to renew them.\n+      val renewalTime ="
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "But do note that you still can't start the token renewer automatically, since YARN still has its own renewer and things would get mixed up.",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-10-25T18:28:25Z",
    "diffHunk": "@@ -213,6 +216,24 @@ private[spark] class MesosCoarseGrainedSchedulerBackend(\n       sc.conf.getOption(\"spark.mesos.driver.frameworkId\").map(_ + suffix)\n     )\n \n+    // check that the credentials are defined, even though it's likely that auth would have failed\n+    // already if you've made it this far\n+    if (principal != null && currentHadoopDelegationTokens.isDefined) {\n+      logDebug(s\"Principal found ($principal) starting token renewer\")\n+      // The renewal time is ignored when getting the initial delegation tokens\n+      // (CoarseGrainedSchedulerBackend.scala:getHadoopDelegationCreds), so we get the renewal\n+      // time here and schedule a thread to renew them.\n+      val renewalTime ="
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "ArtRand"
    },
    "body": "For Mesos, the credential renewer contains the tokens, the renewal time, and all logic to renew the tokens. Should never be evaluated (and tokens never initialized) if `UserGroupInformation.isSecurityEnabled` evaluates to `false`",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-11-06T02:16:54Z",
    "diffHunk": "@@ -58,8 +62,9 @@ private[spark] class MesosCoarseGrainedSchedulerBackend(\n   extends CoarseGrainedSchedulerBackend(scheduler, sc.env.rpcEnv)\n     with org.apache.mesos.Scheduler with MesosSchedulerUtils {\n \n-  override def hadoopDelegationTokenManager: Option[HadoopDelegationTokenManager] =\n-    Some(new HadoopDelegationTokenManager(sc.conf, sc.hadoopConfiguration))\n+  private lazy val hadoopCredentialRenewer: MesosCredentialRenewer ="
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "You shouldn't do this here, otherwise you need to keep that field `protected` in the parent class and that adds unnecessary coupling. Instead, do this in `initializeHadoopDelegationTokens`.",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-11-06T23:32:45Z",
    "diffHunk": "@@ -213,6 +216,14 @@ private[spark] class MesosCoarseGrainedSchedulerBackend(\n       sc.conf.getOption(\"spark.mesos.driver.frameworkId\").map(_ + suffix)\n     )\n \n+    // check that the credentials are defined, even though it's likely that auth would have failed\n+    // already if you've made it this far, then start the token renewer\n+    if (hadoopDelegationTokens.isDefined) {"
  }, {
    "author": {
      "login": "ArtRand"
    },
    "body": "I agree that I shouldn't need to use the conditional `hadoopDelegationTokens.isDefined`, however there will need to be some check (`UserGroupInformation.isSecurityEnabled` or similar) to pass the `driverEndpoint` to the renewer/manager here. When the initial tokens are generated `driverEndpoint` is still `None` because `start()` hasn't been called yet. So I could _schedule_ the renewal, but I'll still have to at least update the `driverEndpoint` here. \r\n\r\nI could initialize the `driverEndpoint` in `initializeHadoopDelegationTokens` for Mesos and change around the logic in `start()` (for the `MesosCoarseGrainedSchedulerBackend`) but then you're just switching one conditional for another...",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-11-08T00:56:19Z",
    "diffHunk": "@@ -213,6 +216,14 @@ private[spark] class MesosCoarseGrainedSchedulerBackend(\n       sc.conf.getOption(\"spark.mesos.driver.frameworkId\").map(_ + suffix)\n     )\n \n+    // check that the credentials are defined, even though it's likely that auth would have failed\n+    // already if you've made it this far, then start the token renewer\n+    if (hadoopDelegationTokens.isDefined) {"
  }, {
    "author": {
      "login": "ArtRand"
    },
    "body": "I may have spoke too soon, there might be a way..",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-11-08T02:58:28Z",
    "diffHunk": "@@ -213,6 +216,14 @@ private[spark] class MesosCoarseGrainedSchedulerBackend(\n       sc.conf.getOption(\"spark.mesos.driver.frameworkId\").map(_ + suffix)\n     )\n \n+    // check that the credentials are defined, even though it's likely that auth would have failed\n+    // already if you've made it this far, then start the token renewer\n+    if (hadoopDelegationTokens.isDefined) {"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "You could call `initializeHadoopDelegationTokens` in `start` after everything that's needed is initialized. It would also better follow the scheduler's lifecycle.",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-11-08T19:38:28Z",
    "diffHunk": "@@ -213,6 +216,14 @@ private[spark] class MesosCoarseGrainedSchedulerBackend(\n       sc.conf.getOption(\"spark.mesos.driver.frameworkId\").map(_ + suffix)\n     )\n \n+    // check that the credentials are defined, even though it's likely that auth would have failed\n+    // already if you've made it this far, then start the token renewer\n+    if (hadoopDelegationTokens.isDefined) {"
  }, {
    "author": {
      "login": "ArtRand"
    },
    "body": "Check out the patch now. `hadoopDelegationTokens` now calls `initializeHadoopDelegationTokens` (renamed `fetchHadoopDelegationTokens`) by name:\r\n```scala\r\n  private val hadoopDelegationTokens: () => Option[Array[Byte]] = fetchHadoopDelegationTokens\r\n```\r\nwhere\r\n```scala\r\n  override def fetchHadoopDelegationTokens(): Option[Array[Byte]] = {\r\n    if (UserGroupInformation.isSecurityEnabled) {\r\n      Some(hadoopDelegationTokenManager.getTokens())\r\n    } else {\r\n      None\r\n    }\r\n  }\r\n```\r\n\r\n This has the effect of only generating the first set of delegation tokens once the first `RetrieveSparkAppConfig` message is received. At this point, everything has been initialized because renewer (renamed `MesosHadoopDelegationTokenManager`) is evaluated lazily with the correct `driverEndpoint`. \r\n\r\n```scala\r\n  private lazy val hadoopDelegationTokenManager: MesosHadoopDelegationTokenManager =\r\n    new MesosHadoopDelegationTokenManager(conf, sc.hadoopConfiguration, driverEndpoint)\r\n```\r\n\r\nIt's maybe a bit confusing to just avoid an extra conditional. WDYT? ",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-11-08T21:09:25Z",
    "diffHunk": "@@ -213,6 +216,14 @@ private[spark] class MesosCoarseGrainedSchedulerBackend(\n       sc.conf.getOption(\"spark.mesos.driver.frameworkId\").map(_ + suffix)\n     )\n \n+    // check that the credentials are defined, even though it's likely that auth would have failed\n+    // already if you've made it this far, then start the token renewer\n+    if (hadoopDelegationTokens.isDefined) {"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Why pass in a `HadoopDelegationTokenManager` if it's not used by this class? The renewer can create one itself.",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-11-06T23:39:06Z",
    "diffHunk": "@@ -58,8 +60,9 @@ private[spark] class MesosCoarseGrainedSchedulerBackend(\n   extends CoarseGrainedSchedulerBackend(scheduler, sc.env.rpcEnv)\n     with org.apache.mesos.Scheduler with MesosSchedulerUtils {\n \n-  override def hadoopDelegationTokenManager: Option[HadoopDelegationTokenManager] =\n-    Some(new HadoopDelegationTokenManager(sc.conf, sc.hadoopConfiguration))\n+  private lazy val hadoopCredentialRenewer: MesosCredentialRenewer =\n+    new MesosCredentialRenewer(\n+      conf, new HadoopDelegationTokenManager(sc.conf, sc.hadoopConfiguration))"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "So, seems to me that your \"renewer\" is doing more than just renewing tokens; it's also being used to generate the initial set. So aside from my comments about initializing the renewer here, you should also probably make this API a little cleaner. Right now there's too much coupling.\r\n\r\nThe renewer should do renewals only, otherwise it should be called something different.",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-11-06T23:42:07Z",
    "diffHunk": "@@ -772,6 +783,14 @@ private[spark] class MesosCoarseGrainedSchedulerBackend(\n       offer.getHostname\n     }\n   }\n+\n+  override def initializeHadoopDelegationTokens(): Option[Array[Byte]] = {\n+    if (UserGroupInformation.isSecurityEnabled) {\n+      Some(hadoopCredentialRenewer.tokens)"
  }],
  "prId": 19272
}]