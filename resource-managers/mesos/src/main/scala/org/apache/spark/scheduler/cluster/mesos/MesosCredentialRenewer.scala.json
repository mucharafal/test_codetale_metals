[{
  "comments": [{
    "author": {
      "login": "skonto"
    },
    "body": "Same as spark.yarn.credentials.renewalTime, should not we have a common value somewhere?",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-09-19T08:57:27Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{Executors, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+class MesosCredentialRenewer(\n+    conf: SparkConf,\n+    tokenManager: HadoopDelegationTokenManager,\n+    nextRenewal: Long,\n+    de: RpcEndpointRef) extends Logging {\n+  private val credentialRenewerThread =\n+    Executors.newSingleThreadScheduledExecutor(\n+      ThreadUtils.namedThreadFactory(\"Credential Refresh Thread\"))\n+\n+  @volatile private var timeOfNextRenewal = nextRenewal\n+\n+  private val principal = conf.get(\"spark.yarn.principal\")\n+\n+  private val (secretFile, mode) = getSecretFile(conf)\n+\n+  private def getSecretFile(conf: SparkConf): (String, String) = {\n+    val keytab64 = conf.get(\"spark.yarn.keytab\", null)\n+    val tgt64 = System.getenv(\"KRB5CCNAME\")\n+    require(keytab64 != null || tgt64 != null, \"keytab or tgt required\")\n+    require(keytab64 == null || tgt64 == null, \"keytab and tgt cannot be used at the same time\")\n+    val mode = if (keytab64 != null) \"keytab\" else \"tgt\"\n+    val secretFile = if (keytab64 != null) keytab64 else tgt64\n+    logInfo(s\"Logging in as $principal with mode $mode to retrieve HDFS delegation tokens\")\n+    logDebug(s\"secretFile is $secretFile\")\n+    (secretFile, mode)\n+  }\n+\n+  def scheduleTokenRenewal(): Unit = {\n+    def scheduleRenewal(runnable: Runnable): Unit = {\n+      val remainingTime = timeOfNextRenewal - System.currentTimeMillis()\n+      if (remainingTime <= 0) {\n+        logInfo(\"Credentials have expired, creating new ones now.\")\n+        runnable.run()\n+      } else {\n+        logInfo(s\"Scheduling login from keytab in $remainingTime millis.\")\n+        credentialRenewerThread.schedule(runnable, remainingTime, TimeUnit.MILLISECONDS)\n+      }\n+    }\n+\n+    val credentialRenewerRunnable =\n+      new Runnable {\n+        override def run(): Unit = {\n+          try {\n+            val creds = getRenewedDelegationTokens(conf)\n+            broadcastDelegationTokens(creds)\n+          } catch {\n+            case e: Exception =>\n+              // Log the error and try to write new tokens back in an hour\n+              logWarning(\"Couldn't broadcast tokens, trying agin in 20 seconds\", e)\n+              credentialRenewerThread.schedule(this, 20, TimeUnit.SECONDS)\n+              return\n+          }\n+          scheduleRenewal(this)\n+        }\n+      }\n+    scheduleRenewal(credentialRenewerRunnable)\n+  }\n+\n+  private def getRenewedDelegationTokens(conf: SparkConf): Array[Byte] = {\n+    logInfo(s\"Attempting to login with ${conf.get(\"spark.yarn.principal\", null)}\")\n+    // Get new delegation tokens by logging in with a new UGI\n+    // inspired by AMCredentialRenewer.scala:L174\n+    val ugi = if (mode == \"keytab\") {\n+      UserGroupInformation.loginUserFromKeytabAndReturnUGI(principal, secretFile)\n+    } else {\n+      UserGroupInformation.getUGIFromTicketCache(secretFile, principal)\n+    }\n+    val tempCreds = ugi.getCredentials\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    var nextRenewalTime = Long.MaxValue"
  }, {
    "author": {
      "login": "skonto"
    },
    "body": "When driver is restarted in case of yarn the old renewalTime is restored: https://github.com/apache/spark/blob/e92ffe6f1771e3fe9ea2e62ba552c1b5cf255368/streaming/src/main/scala/org/apache/spark/streaming/Checkpoint.scala#L59\r\nDoes the code here cover this?",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-09-19T08:59:16Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{Executors, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+class MesosCredentialRenewer(\n+    conf: SparkConf,\n+    tokenManager: HadoopDelegationTokenManager,\n+    nextRenewal: Long,\n+    de: RpcEndpointRef) extends Logging {\n+  private val credentialRenewerThread =\n+    Executors.newSingleThreadScheduledExecutor(\n+      ThreadUtils.namedThreadFactory(\"Credential Refresh Thread\"))\n+\n+  @volatile private var timeOfNextRenewal = nextRenewal\n+\n+  private val principal = conf.get(\"spark.yarn.principal\")\n+\n+  private val (secretFile, mode) = getSecretFile(conf)\n+\n+  private def getSecretFile(conf: SparkConf): (String, String) = {\n+    val keytab64 = conf.get(\"spark.yarn.keytab\", null)\n+    val tgt64 = System.getenv(\"KRB5CCNAME\")\n+    require(keytab64 != null || tgt64 != null, \"keytab or tgt required\")\n+    require(keytab64 == null || tgt64 == null, \"keytab and tgt cannot be used at the same time\")\n+    val mode = if (keytab64 != null) \"keytab\" else \"tgt\"\n+    val secretFile = if (keytab64 != null) keytab64 else tgt64\n+    logInfo(s\"Logging in as $principal with mode $mode to retrieve HDFS delegation tokens\")\n+    logDebug(s\"secretFile is $secretFile\")\n+    (secretFile, mode)\n+  }\n+\n+  def scheduleTokenRenewal(): Unit = {\n+    def scheduleRenewal(runnable: Runnable): Unit = {\n+      val remainingTime = timeOfNextRenewal - System.currentTimeMillis()\n+      if (remainingTime <= 0) {\n+        logInfo(\"Credentials have expired, creating new ones now.\")\n+        runnable.run()\n+      } else {\n+        logInfo(s\"Scheduling login from keytab in $remainingTime millis.\")\n+        credentialRenewerThread.schedule(runnable, remainingTime, TimeUnit.MILLISECONDS)\n+      }\n+    }\n+\n+    val credentialRenewerRunnable =\n+      new Runnable {\n+        override def run(): Unit = {\n+          try {\n+            val creds = getRenewedDelegationTokens(conf)\n+            broadcastDelegationTokens(creds)\n+          } catch {\n+            case e: Exception =>\n+              // Log the error and try to write new tokens back in an hour\n+              logWarning(\"Couldn't broadcast tokens, trying agin in 20 seconds\", e)\n+              credentialRenewerThread.schedule(this, 20, TimeUnit.SECONDS)\n+              return\n+          }\n+          scheduleRenewal(this)\n+        }\n+      }\n+    scheduleRenewal(credentialRenewerRunnable)\n+  }\n+\n+  private def getRenewedDelegationTokens(conf: SparkConf): Array[Byte] = {\n+    logInfo(s\"Attempting to login with ${conf.get(\"spark.yarn.principal\", null)}\")\n+    // Get new delegation tokens by logging in with a new UGI\n+    // inspired by AMCredentialRenewer.scala:L174\n+    val ugi = if (mode == \"keytab\") {\n+      UserGroupInformation.loginUserFromKeytabAndReturnUGI(principal, secretFile)\n+    } else {\n+      UserGroupInformation.getUGIFromTicketCache(secretFile, principal)\n+    }\n+    val tempCreds = ugi.getCredentials\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    var nextRenewalTime = Long.MaxValue"
  }, {
    "author": {
      "login": "ArtRand"
    },
    "body": "Right now when the `MesosCredentialRenewer` is initialized, it renews the current tokens and sets the renewal time to whatever the expiration time of those tokens is. On a driver restart, the same thing would happen. We could add `spark.yarn.credentials.renewalTime` as an override, but if the driver restarts, say 2 days later, `spark.yarn.credentials.renewalTime` is no longer relevant and it'll just immediately renew anyways. \r\n\r\nRelavent code: \r\nhttps://github.com/mesosphere/spark/blob/spark-21842-450-kerberos-ticket-renewal/resource-managers/mesos/src/main/scala/org/apache/spark/scheduler/cluster/mesos/MesosCoarseGrainedSchedulerBackend.scala#L210 \r\n^^ Where the initial renewal time is set\r\nhttps://github.com/mesosphere/spark/blob/spark-21842-450-kerberos-ticket-renewal/resource-managers/mesos/src/main/scala/org/apache/spark/scheduler/cluster/mesos/MesosCredentialRenewer.scala#L66\r\n^^ where we initialize the renewal time if the renewal time has passed",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-09-19T23:59:34Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{Executors, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+class MesosCredentialRenewer(\n+    conf: SparkConf,\n+    tokenManager: HadoopDelegationTokenManager,\n+    nextRenewal: Long,\n+    de: RpcEndpointRef) extends Logging {\n+  private val credentialRenewerThread =\n+    Executors.newSingleThreadScheduledExecutor(\n+      ThreadUtils.namedThreadFactory(\"Credential Refresh Thread\"))\n+\n+  @volatile private var timeOfNextRenewal = nextRenewal\n+\n+  private val principal = conf.get(\"spark.yarn.principal\")\n+\n+  private val (secretFile, mode) = getSecretFile(conf)\n+\n+  private def getSecretFile(conf: SparkConf): (String, String) = {\n+    val keytab64 = conf.get(\"spark.yarn.keytab\", null)\n+    val tgt64 = System.getenv(\"KRB5CCNAME\")\n+    require(keytab64 != null || tgt64 != null, \"keytab or tgt required\")\n+    require(keytab64 == null || tgt64 == null, \"keytab and tgt cannot be used at the same time\")\n+    val mode = if (keytab64 != null) \"keytab\" else \"tgt\"\n+    val secretFile = if (keytab64 != null) keytab64 else tgt64\n+    logInfo(s\"Logging in as $principal with mode $mode to retrieve HDFS delegation tokens\")\n+    logDebug(s\"secretFile is $secretFile\")\n+    (secretFile, mode)\n+  }\n+\n+  def scheduleTokenRenewal(): Unit = {\n+    def scheduleRenewal(runnable: Runnable): Unit = {\n+      val remainingTime = timeOfNextRenewal - System.currentTimeMillis()\n+      if (remainingTime <= 0) {\n+        logInfo(\"Credentials have expired, creating new ones now.\")\n+        runnable.run()\n+      } else {\n+        logInfo(s\"Scheduling login from keytab in $remainingTime millis.\")\n+        credentialRenewerThread.schedule(runnable, remainingTime, TimeUnit.MILLISECONDS)\n+      }\n+    }\n+\n+    val credentialRenewerRunnable =\n+      new Runnable {\n+        override def run(): Unit = {\n+          try {\n+            val creds = getRenewedDelegationTokens(conf)\n+            broadcastDelegationTokens(creds)\n+          } catch {\n+            case e: Exception =>\n+              // Log the error and try to write new tokens back in an hour\n+              logWarning(\"Couldn't broadcast tokens, trying agin in 20 seconds\", e)\n+              credentialRenewerThread.schedule(this, 20, TimeUnit.SECONDS)\n+              return\n+          }\n+          scheduleRenewal(this)\n+        }\n+      }\n+    scheduleRenewal(credentialRenewerRunnable)\n+  }\n+\n+  private def getRenewedDelegationTokens(conf: SparkConf): Array[Byte] = {\n+    logInfo(s\"Attempting to login with ${conf.get(\"spark.yarn.principal\", null)}\")\n+    // Get new delegation tokens by logging in with a new UGI\n+    // inspired by AMCredentialRenewer.scala:L174\n+    val ugi = if (mode == \"keytab\") {\n+      UserGroupInformation.loginUserFromKeytabAndReturnUGI(principal, secretFile)\n+    } else {\n+      UserGroupInformation.getUGIFromTicketCache(secretFile, principal)\n+    }\n+    val tempCreds = ugi.getCredentials\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    var nextRenewalTime = Long.MaxValue"
  }, {
    "author": {
      "login": "skonto"
    },
    "body": "Ok so we always re-new when we start by fetching the tokens, got it.",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-09-20T20:47:58Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{Executors, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+class MesosCredentialRenewer(\n+    conf: SparkConf,\n+    tokenManager: HadoopDelegationTokenManager,\n+    nextRenewal: Long,\n+    de: RpcEndpointRef) extends Logging {\n+  private val credentialRenewerThread =\n+    Executors.newSingleThreadScheduledExecutor(\n+      ThreadUtils.namedThreadFactory(\"Credential Refresh Thread\"))\n+\n+  @volatile private var timeOfNextRenewal = nextRenewal\n+\n+  private val principal = conf.get(\"spark.yarn.principal\")\n+\n+  private val (secretFile, mode) = getSecretFile(conf)\n+\n+  private def getSecretFile(conf: SparkConf): (String, String) = {\n+    val keytab64 = conf.get(\"spark.yarn.keytab\", null)\n+    val tgt64 = System.getenv(\"KRB5CCNAME\")\n+    require(keytab64 != null || tgt64 != null, \"keytab or tgt required\")\n+    require(keytab64 == null || tgt64 == null, \"keytab and tgt cannot be used at the same time\")\n+    val mode = if (keytab64 != null) \"keytab\" else \"tgt\"\n+    val secretFile = if (keytab64 != null) keytab64 else tgt64\n+    logInfo(s\"Logging in as $principal with mode $mode to retrieve HDFS delegation tokens\")\n+    logDebug(s\"secretFile is $secretFile\")\n+    (secretFile, mode)\n+  }\n+\n+  def scheduleTokenRenewal(): Unit = {\n+    def scheduleRenewal(runnable: Runnable): Unit = {\n+      val remainingTime = timeOfNextRenewal - System.currentTimeMillis()\n+      if (remainingTime <= 0) {\n+        logInfo(\"Credentials have expired, creating new ones now.\")\n+        runnable.run()\n+      } else {\n+        logInfo(s\"Scheduling login from keytab in $remainingTime millis.\")\n+        credentialRenewerThread.schedule(runnable, remainingTime, TimeUnit.MILLISECONDS)\n+      }\n+    }\n+\n+    val credentialRenewerRunnable =\n+      new Runnable {\n+        override def run(): Unit = {\n+          try {\n+            val creds = getRenewedDelegationTokens(conf)\n+            broadcastDelegationTokens(creds)\n+          } catch {\n+            case e: Exception =>\n+              // Log the error and try to write new tokens back in an hour\n+              logWarning(\"Couldn't broadcast tokens, trying agin in 20 seconds\", e)\n+              credentialRenewerThread.schedule(this, 20, TimeUnit.SECONDS)\n+              return\n+          }\n+          scheduleRenewal(this)\n+        }\n+      }\n+    scheduleRenewal(credentialRenewerRunnable)\n+  }\n+\n+  private def getRenewedDelegationTokens(conf: SparkConf): Array[Byte] = {\n+    logInfo(s\"Attempting to login with ${conf.get(\"spark.yarn.principal\", null)}\")\n+    // Get new delegation tokens by logging in with a new UGI\n+    // inspired by AMCredentialRenewer.scala:L174\n+    val ugi = if (mode == \"keytab\") {\n+      UserGroupInformation.loginUserFromKeytabAndReturnUGI(principal, secretFile)\n+    } else {\n+      UserGroupInformation.getUGIFromTicketCache(secretFile, principal)\n+    }\n+    val tempCreds = ugi.getCredentials\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    var nextRenewalTime = Long.MaxValue"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "skonto"
    },
    "body": "t -> token\r\nThis method does not return an interval, it just returns the new expiration time. \r\nCompare with: https://github.com/apache/spark/blob/b9ab791a9efb0dc165ba283c91acf831fa6be5d8/core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala#L102",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-09-19T09:26:37Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{Executors, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+class MesosCredentialRenewer(\n+    conf: SparkConf,\n+    tokenManager: HadoopDelegationTokenManager,\n+    nextRenewal: Long,\n+    de: RpcEndpointRef) extends Logging {\n+  private val credentialRenewerThread =\n+    Executors.newSingleThreadScheduledExecutor(\n+      ThreadUtils.namedThreadFactory(\"Credential Refresh Thread\"))\n+\n+  @volatile private var timeOfNextRenewal = nextRenewal\n+\n+  private val principal = conf.get(\"spark.yarn.principal\")\n+\n+  private val (secretFile, mode) = getSecretFile(conf)\n+\n+  private def getSecretFile(conf: SparkConf): (String, String) = {\n+    val keytab64 = conf.get(\"spark.yarn.keytab\", null)\n+    val tgt64 = System.getenv(\"KRB5CCNAME\")\n+    require(keytab64 != null || tgt64 != null, \"keytab or tgt required\")\n+    require(keytab64 == null || tgt64 == null, \"keytab and tgt cannot be used at the same time\")\n+    val mode = if (keytab64 != null) \"keytab\" else \"tgt\"\n+    val secretFile = if (keytab64 != null) keytab64 else tgt64\n+    logInfo(s\"Logging in as $principal with mode $mode to retrieve HDFS delegation tokens\")\n+    logDebug(s\"secretFile is $secretFile\")\n+    (secretFile, mode)\n+  }\n+\n+  def scheduleTokenRenewal(): Unit = {\n+    def scheduleRenewal(runnable: Runnable): Unit = {\n+      val remainingTime = timeOfNextRenewal - System.currentTimeMillis()\n+      if (remainingTime <= 0) {\n+        logInfo(\"Credentials have expired, creating new ones now.\")\n+        runnable.run()\n+      } else {\n+        logInfo(s\"Scheduling login from keytab in $remainingTime millis.\")\n+        credentialRenewerThread.schedule(runnable, remainingTime, TimeUnit.MILLISECONDS)\n+      }\n+    }\n+\n+    val credentialRenewerRunnable =\n+      new Runnable {\n+        override def run(): Unit = {\n+          try {\n+            val creds = getRenewedDelegationTokens(conf)\n+            broadcastDelegationTokens(creds)\n+          } catch {\n+            case e: Exception =>\n+              // Log the error and try to write new tokens back in an hour\n+              logWarning(\"Couldn't broadcast tokens, trying agin in 20 seconds\", e)\n+              credentialRenewerThread.schedule(this, 20, TimeUnit.SECONDS)\n+              return\n+          }\n+          scheduleRenewal(this)\n+        }\n+      }\n+    scheduleRenewal(credentialRenewerRunnable)\n+  }\n+\n+  private def getRenewedDelegationTokens(conf: SparkConf): Array[Byte] = {\n+    logInfo(s\"Attempting to login with ${conf.get(\"spark.yarn.principal\", null)}\")\n+    // Get new delegation tokens by logging in with a new UGI\n+    // inspired by AMCredentialRenewer.scala:L174\n+    val ugi = if (mode == \"keytab\") {\n+      UserGroupInformation.loginUserFromKeytabAndReturnUGI(principal, secretFile)\n+    } else {\n+      UserGroupInformation.getUGIFromTicketCache(secretFile, principal)\n+    }\n+    val tempCreds = ugi.getCredentials\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    var nextRenewalTime = Long.MaxValue\n+    ugi.doAs(new PrivilegedExceptionAction[Void] {\n+      override def run(): Void = {\n+        nextRenewalTime = tokenManager.obtainDelegationTokens(hadoopConf, tempCreds)\n+        null\n+      }\n+    })\n+\n+    val currTime = System.currentTimeMillis()\n+    timeOfNextRenewal = if (nextRenewalTime <= currTime) {\n+      logWarning(s\"Next credential renewal time ($nextRenewalTime) is earlier than \" +\n+        s\"current time ($currTime), which is unexpected, please check your credential renewal \" +\n+        \"related configurations in the target services.\")\n+      currTime\n+    } else {\n+      val rt = 0.75 * (nextRenewalTime - currTime)\n+      (currTime + rt).toLong\n+    }\n+    logInfo(s\"Time of next renewal is $timeOfNextRenewal\")\n+\n+    // Add the temp credentials back to the original ones.\n+    UserGroupInformation.getCurrentUser.addCredentials(tempCreds)\n+    SparkHadoopUtil.get.serialize(tempCreds)\n+  }\n+\n+  private def broadcastDelegationTokens(tokens: Array[Byte]): Unit = {\n+    // send token to existing executors\n+    logInfo(\"Sending new tokens to all executors\")\n+    de.send(UpdateDelegationTokens(tokens))\n+  }\n+}\n+\n+object MesosCredentialRenewer extends Logging {\n+  def getTokenRenewalInterval(bytes: Array[Byte], conf: SparkConf): Long = {\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    val creds = SparkHadoopUtil.get.deserialize(bytes)\n+    val intervals = creds.getAllTokens.asScala.flatMap { t =>\n+      Try {"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "susanxhuynh"
    },
    "body": "(sp) \"ag**a**in\"",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-09-19T15:22:56Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{Executors, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+class MesosCredentialRenewer(\n+    conf: SparkConf,\n+    tokenManager: HadoopDelegationTokenManager,\n+    nextRenewal: Long,\n+    de: RpcEndpointRef) extends Logging {\n+  private val credentialRenewerThread =\n+    Executors.newSingleThreadScheduledExecutor(\n+      ThreadUtils.namedThreadFactory(\"Credential Refresh Thread\"))\n+\n+  @volatile private var timeOfNextRenewal = nextRenewal\n+\n+  private val principal = conf.get(\"spark.yarn.principal\")\n+\n+  private val (secretFile, mode) = getSecretFile(conf)\n+\n+  private def getSecretFile(conf: SparkConf): (String, String) = {\n+    val keytab64 = conf.get(\"spark.yarn.keytab\", null)\n+    val tgt64 = System.getenv(\"KRB5CCNAME\")\n+    require(keytab64 != null || tgt64 != null, \"keytab or tgt required\")\n+    require(keytab64 == null || tgt64 == null, \"keytab and tgt cannot be used at the same time\")\n+    val mode = if (keytab64 != null) \"keytab\" else \"tgt\"\n+    val secretFile = if (keytab64 != null) keytab64 else tgt64\n+    logInfo(s\"Logging in as $principal with mode $mode to retrieve HDFS delegation tokens\")\n+    logDebug(s\"secretFile is $secretFile\")\n+    (secretFile, mode)\n+  }\n+\n+  def scheduleTokenRenewal(): Unit = {\n+    def scheduleRenewal(runnable: Runnable): Unit = {\n+      val remainingTime = timeOfNextRenewal - System.currentTimeMillis()\n+      if (remainingTime <= 0) {\n+        logInfo(\"Credentials have expired, creating new ones now.\")\n+        runnable.run()\n+      } else {\n+        logInfo(s\"Scheduling login from keytab in $remainingTime millis.\")\n+        credentialRenewerThread.schedule(runnable, remainingTime, TimeUnit.MILLISECONDS)\n+      }\n+    }\n+\n+    val credentialRenewerRunnable =\n+      new Runnable {\n+        override def run(): Unit = {\n+          try {\n+            val creds = getRenewedDelegationTokens(conf)\n+            broadcastDelegationTokens(creds)\n+          } catch {\n+            case e: Exception =>\n+              // Log the error and try to write new tokens back in an hour\n+              logWarning(\"Couldn't broadcast tokens, trying agin in 20 seconds\", e)"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "susanxhuynh"
    },
    "body": "Comment says \"an hour\" but code has 20 seconds.",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-09-19T18:08:29Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{Executors, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+class MesosCredentialRenewer(\n+    conf: SparkConf,\n+    tokenManager: HadoopDelegationTokenManager,\n+    nextRenewal: Long,\n+    de: RpcEndpointRef) extends Logging {\n+  private val credentialRenewerThread =\n+    Executors.newSingleThreadScheduledExecutor(\n+      ThreadUtils.namedThreadFactory(\"Credential Refresh Thread\"))\n+\n+  @volatile private var timeOfNextRenewal = nextRenewal\n+\n+  private val principal = conf.get(\"spark.yarn.principal\")\n+\n+  private val (secretFile, mode) = getSecretFile(conf)\n+\n+  private def getSecretFile(conf: SparkConf): (String, String) = {\n+    val keytab64 = conf.get(\"spark.yarn.keytab\", null)\n+    val tgt64 = System.getenv(\"KRB5CCNAME\")\n+    require(keytab64 != null || tgt64 != null, \"keytab or tgt required\")\n+    require(keytab64 == null || tgt64 == null, \"keytab and tgt cannot be used at the same time\")\n+    val mode = if (keytab64 != null) \"keytab\" else \"tgt\"\n+    val secretFile = if (keytab64 != null) keytab64 else tgt64\n+    logInfo(s\"Logging in as $principal with mode $mode to retrieve HDFS delegation tokens\")\n+    logDebug(s\"secretFile is $secretFile\")\n+    (secretFile, mode)\n+  }\n+\n+  def scheduleTokenRenewal(): Unit = {\n+    def scheduleRenewal(runnable: Runnable): Unit = {\n+      val remainingTime = timeOfNextRenewal - System.currentTimeMillis()\n+      if (remainingTime <= 0) {\n+        logInfo(\"Credentials have expired, creating new ones now.\")\n+        runnable.run()\n+      } else {\n+        logInfo(s\"Scheduling login from keytab in $remainingTime millis.\")\n+        credentialRenewerThread.schedule(runnable, remainingTime, TimeUnit.MILLISECONDS)\n+      }\n+    }\n+\n+    val credentialRenewerRunnable =\n+      new Runnable {\n+        override def run(): Unit = {\n+          try {\n+            val creds = getRenewedDelegationTokens(conf)\n+            broadcastDelegationTokens(creds)\n+          } catch {\n+            case e: Exception =>\n+              // Log the error and try to write new tokens back in an hour"
  }, {
    "author": {
      "login": "ArtRand"
    },
    "body": "good catch, I changed the code to match the YARN equivalent. ",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-09-20T20:07:57Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{Executors, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+class MesosCredentialRenewer(\n+    conf: SparkConf,\n+    tokenManager: HadoopDelegationTokenManager,\n+    nextRenewal: Long,\n+    de: RpcEndpointRef) extends Logging {\n+  private val credentialRenewerThread =\n+    Executors.newSingleThreadScheduledExecutor(\n+      ThreadUtils.namedThreadFactory(\"Credential Refresh Thread\"))\n+\n+  @volatile private var timeOfNextRenewal = nextRenewal\n+\n+  private val principal = conf.get(\"spark.yarn.principal\")\n+\n+  private val (secretFile, mode) = getSecretFile(conf)\n+\n+  private def getSecretFile(conf: SparkConf): (String, String) = {\n+    val keytab64 = conf.get(\"spark.yarn.keytab\", null)\n+    val tgt64 = System.getenv(\"KRB5CCNAME\")\n+    require(keytab64 != null || tgt64 != null, \"keytab or tgt required\")\n+    require(keytab64 == null || tgt64 == null, \"keytab and tgt cannot be used at the same time\")\n+    val mode = if (keytab64 != null) \"keytab\" else \"tgt\"\n+    val secretFile = if (keytab64 != null) keytab64 else tgt64\n+    logInfo(s\"Logging in as $principal with mode $mode to retrieve HDFS delegation tokens\")\n+    logDebug(s\"secretFile is $secretFile\")\n+    (secretFile, mode)\n+  }\n+\n+  def scheduleTokenRenewal(): Unit = {\n+    def scheduleRenewal(runnable: Runnable): Unit = {\n+      val remainingTime = timeOfNextRenewal - System.currentTimeMillis()\n+      if (remainingTime <= 0) {\n+        logInfo(\"Credentials have expired, creating new ones now.\")\n+        runnable.run()\n+      } else {\n+        logInfo(s\"Scheduling login from keytab in $remainingTime millis.\")\n+        credentialRenewerThread.schedule(runnable, remainingTime, TimeUnit.MILLISECONDS)\n+      }\n+    }\n+\n+    val credentialRenewerRunnable =\n+      new Runnable {\n+        override def run(): Unit = {\n+          try {\n+            val creds = getRenewedDelegationTokens(conf)\n+            broadcastDelegationTokens(creds)\n+          } catch {\n+            case e: Exception =>\n+              // Log the error and try to write new tokens back in an hour"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "susanxhuynh"
    },
    "body": "Why 5000?",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-09-20T23:24:55Z",
    "diffHunk": "@@ -63,7 +63,8 @@ class MesosCredentialRenewer(\n \n   def scheduleTokenRenewal(): Unit = {\n     def scheduleRenewal(runnable: Runnable): Unit = {\n-      val remainingTime = timeOfNextRenewal - System.currentTimeMillis()\n+      // val remainingTime = timeOfNextRenewal - System.currentTimeMillis()\n+      val remainingTime = 5000"
  }, {
    "author": {
      "login": "ArtRand"
    },
    "body": "well that's embarrassing, just a debugging tool that I forgot to remove.\r\n",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-09-20T23:31:50Z",
    "diffHunk": "@@ -63,7 +63,8 @@ class MesosCredentialRenewer(\n \n   def scheduleTokenRenewal(): Unit = {\n     def scheduleRenewal(runnable: Runnable): Unit = {\n-      val remainingTime = timeOfNextRenewal - System.currentTimeMillis()\n+      // val remainingTime = timeOfNextRenewal - System.currentTimeMillis()\n+      val remainingTime = 5000"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "kalvinnchau"
    },
    "body": "I don't see where it refreshes the delegation tokens until the max-lifetime, then re-login with the keytab to get a new delegation tokens that'll last until the max-lifetime.\r\n\r\nDoes this skip over the potential issues with expiring delegation tokens (after the max-lifetime, 7 days default) by just re-logging in with the keytab every time the delegation tokens need to refresh, and then grabbing a new set of delegation tokens?\r\n",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-09-21T18:38:38Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{Executors, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+class MesosCredentialRenewer(\n+    conf: SparkConf,\n+    tokenManager: HadoopDelegationTokenManager,\n+    nextRenewal: Long,\n+    de: RpcEndpointRef) extends Logging {\n+  private val credentialRenewerThread =\n+    Executors.newSingleThreadScheduledExecutor(\n+      ThreadUtils.namedThreadFactory(\"Credential Refresh Thread\"))\n+\n+  @volatile private var timeOfNextRenewal = nextRenewal\n+\n+  private val principal = conf.get(\"spark.yarn.principal\")\n+\n+  private val (secretFile, mode) = getSecretFile(conf)\n+\n+  private def getSecretFile(conf: SparkConf): (String, String) = {\n+    val keytab64 = conf.get(\"spark.yarn.keytab\", null)\n+    val tgt64 = System.getenv(\"KRB5CCNAME\")\n+    require(keytab64 != null || tgt64 != null, \"keytab or tgt required\")\n+    require(keytab64 == null || tgt64 == null, \"keytab and tgt cannot be used at the same time\")\n+    val mode = if (keytab64 != null) \"keytab\" else \"tgt\"\n+    val secretFile = if (keytab64 != null) keytab64 else tgt64\n+    logInfo(s\"Logging in as $principal with mode $mode to retrieve HDFS delegation tokens\")\n+    logDebug(s\"secretFile is $secretFile\")\n+    (secretFile, mode)\n+  }\n+\n+  def scheduleTokenRenewal(): Unit = {\n+    def scheduleRenewal(runnable: Runnable): Unit = {\n+      val remainingTime = timeOfNextRenewal - System.currentTimeMillis()\n+      if (remainingTime <= 0) {\n+        logInfo(\"Credentials have expired, creating new ones now.\")\n+        runnable.run()\n+      } else {\n+        logInfo(s\"Scheduling login from keytab in $remainingTime millis.\")\n+        credentialRenewerThread.schedule(runnable, remainingTime, TimeUnit.MILLISECONDS)\n+      }\n+    }\n+\n+    val credentialRenewerRunnable =\n+      new Runnable {\n+        override def run(): Unit = {\n+          try {\n+            val creds = getRenewedDelegationTokens(conf)\n+            broadcastDelegationTokens(creds)\n+          } catch {\n+            case e: Exception =>\n+              // Log the error and try to write new tokens back in an hour\n+              logWarning(\"Couldn't broadcast tokens, trying again in an hour\", e)\n+              credentialRenewerThread.schedule(this, 1, TimeUnit.HOURS)\n+              return\n+          }\n+          scheduleRenewal(this)\n+        }\n+      }\n+    scheduleRenewal(credentialRenewerRunnable)\n+  }\n+\n+  private def getRenewedDelegationTokens(conf: SparkConf): Array[Byte] = {\n+    logInfo(s\"Attempting to login with ${conf.get(\"spark.yarn.principal\", null)}\")\n+    // Get new delegation tokens by logging in with a new UGI\n+    // inspired by AMCredentialRenewer.scala:L174\n+    val ugi = if (mode == \"keytab\") {"
  }, {
    "author": {
      "login": "ArtRand"
    },
    "body": "Hello @kalvinnchau You are correct, all this does is keep track of when the tokens will expire and renew them at that time. Part of my motivation for doing this is to avoid writing any files to disk (like new TGTs, if that's what you're suggesting). We can simply mount the keytab via the Mesos secrets primitive, then renew the tokens every so often. In order to be consistent I tried to keep this solution as close to YARN as possible. ",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-09-23T03:50:47Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{Executors, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+class MesosCredentialRenewer(\n+    conf: SparkConf,\n+    tokenManager: HadoopDelegationTokenManager,\n+    nextRenewal: Long,\n+    de: RpcEndpointRef) extends Logging {\n+  private val credentialRenewerThread =\n+    Executors.newSingleThreadScheduledExecutor(\n+      ThreadUtils.namedThreadFactory(\"Credential Refresh Thread\"))\n+\n+  @volatile private var timeOfNextRenewal = nextRenewal\n+\n+  private val principal = conf.get(\"spark.yarn.principal\")\n+\n+  private val (secretFile, mode) = getSecretFile(conf)\n+\n+  private def getSecretFile(conf: SparkConf): (String, String) = {\n+    val keytab64 = conf.get(\"spark.yarn.keytab\", null)\n+    val tgt64 = System.getenv(\"KRB5CCNAME\")\n+    require(keytab64 != null || tgt64 != null, \"keytab or tgt required\")\n+    require(keytab64 == null || tgt64 == null, \"keytab and tgt cannot be used at the same time\")\n+    val mode = if (keytab64 != null) \"keytab\" else \"tgt\"\n+    val secretFile = if (keytab64 != null) keytab64 else tgt64\n+    logInfo(s\"Logging in as $principal with mode $mode to retrieve HDFS delegation tokens\")\n+    logDebug(s\"secretFile is $secretFile\")\n+    (secretFile, mode)\n+  }\n+\n+  def scheduleTokenRenewal(): Unit = {\n+    def scheduleRenewal(runnable: Runnable): Unit = {\n+      val remainingTime = timeOfNextRenewal - System.currentTimeMillis()\n+      if (remainingTime <= 0) {\n+        logInfo(\"Credentials have expired, creating new ones now.\")\n+        runnable.run()\n+      } else {\n+        logInfo(s\"Scheduling login from keytab in $remainingTime millis.\")\n+        credentialRenewerThread.schedule(runnable, remainingTime, TimeUnit.MILLISECONDS)\n+      }\n+    }\n+\n+    val credentialRenewerRunnable =\n+      new Runnable {\n+        override def run(): Unit = {\n+          try {\n+            val creds = getRenewedDelegationTokens(conf)\n+            broadcastDelegationTokens(creds)\n+          } catch {\n+            case e: Exception =>\n+              // Log the error and try to write new tokens back in an hour\n+              logWarning(\"Couldn't broadcast tokens, trying again in an hour\", e)\n+              credentialRenewerThread.schedule(this, 1, TimeUnit.HOURS)\n+              return\n+          }\n+          scheduleRenewal(this)\n+        }\n+      }\n+    scheduleRenewal(credentialRenewerRunnable)\n+  }\n+\n+  private def getRenewedDelegationTokens(conf: SparkConf): Array[Byte] = {\n+    logInfo(s\"Attempting to login with ${conf.get(\"spark.yarn.principal\", null)}\")\n+    // Get new delegation tokens by logging in with a new UGI\n+    // inspired by AMCredentialRenewer.scala:L174\n+    val ugi = if (mode == \"keytab\") {"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "The correct way would be for the credential management code to differentiate between token creation and token renewal; that way it would renew tokens at the renewal internal and create new ones after the max lifetime.\r\n\r\nBut it seems the original implementation took a shortcut and just creates new one instead of renewing existing ones; changing that would require changes in the credential provider interfaces, so this is enough for now.",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-10-10T21:18:21Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{Executors, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+class MesosCredentialRenewer(\n+    conf: SparkConf,\n+    tokenManager: HadoopDelegationTokenManager,\n+    nextRenewal: Long,\n+    de: RpcEndpointRef) extends Logging {\n+  private val credentialRenewerThread =\n+    Executors.newSingleThreadScheduledExecutor(\n+      ThreadUtils.namedThreadFactory(\"Credential Refresh Thread\"))\n+\n+  @volatile private var timeOfNextRenewal = nextRenewal\n+\n+  private val principal = conf.get(\"spark.yarn.principal\")\n+\n+  private val (secretFile, mode) = getSecretFile(conf)\n+\n+  private def getSecretFile(conf: SparkConf): (String, String) = {\n+    val keytab64 = conf.get(\"spark.yarn.keytab\", null)\n+    val tgt64 = System.getenv(\"KRB5CCNAME\")\n+    require(keytab64 != null || tgt64 != null, \"keytab or tgt required\")\n+    require(keytab64 == null || tgt64 == null, \"keytab and tgt cannot be used at the same time\")\n+    val mode = if (keytab64 != null) \"keytab\" else \"tgt\"\n+    val secretFile = if (keytab64 != null) keytab64 else tgt64\n+    logInfo(s\"Logging in as $principal with mode $mode to retrieve HDFS delegation tokens\")\n+    logDebug(s\"secretFile is $secretFile\")\n+    (secretFile, mode)\n+  }\n+\n+  def scheduleTokenRenewal(): Unit = {\n+    def scheduleRenewal(runnable: Runnable): Unit = {\n+      val remainingTime = timeOfNextRenewal - System.currentTimeMillis()\n+      if (remainingTime <= 0) {\n+        logInfo(\"Credentials have expired, creating new ones now.\")\n+        runnable.run()\n+      } else {\n+        logInfo(s\"Scheduling login from keytab in $remainingTime millis.\")\n+        credentialRenewerThread.schedule(runnable, remainingTime, TimeUnit.MILLISECONDS)\n+      }\n+    }\n+\n+    val credentialRenewerRunnable =\n+      new Runnable {\n+        override def run(): Unit = {\n+          try {\n+            val creds = getRenewedDelegationTokens(conf)\n+            broadcastDelegationTokens(creds)\n+          } catch {\n+            case e: Exception =>\n+              // Log the error and try to write new tokens back in an hour\n+              logWarning(\"Couldn't broadcast tokens, trying again in an hour\", e)\n+              credentialRenewerThread.schedule(this, 1, TimeUnit.HOURS)\n+              return\n+          }\n+          scheduleRenewal(this)\n+        }\n+      }\n+    scheduleRenewal(credentialRenewerRunnable)\n+  }\n+\n+  private def getRenewedDelegationTokens(conf: SparkConf): Array[Byte] = {\n+    logInfo(s\"Attempting to login with ${conf.get(\"spark.yarn.principal\", null)}\")\n+    // Get new delegation tokens by logging in with a new UGI\n+    // inspired by AMCredentialRenewer.scala:L174\n+    val ugi = if (mode == \"keytab\") {"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`ThreadUtils.newDaemonSingleThreadScheduledExecutor`?",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-10-10T20:05:09Z",
    "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{Executors, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+class MesosCredentialRenewer(\n+    conf: SparkConf,\n+    tokenManager: HadoopDelegationTokenManager,\n+    nextRenewal: Long,\n+    de: RpcEndpointRef) extends Logging {\n+  private val credentialRenewerThread =\n+    Executors.newSingleThreadScheduledExecutor("
  }, {
    "author": {
      "login": "ArtRand"
    },
    "body": "I also changed `AMCredentialRenewer` to the same. ",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-10-18T01:58:08Z",
    "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{Executors, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+class MesosCredentialRenewer(\n+    conf: SparkConf,\n+    tokenManager: HadoopDelegationTokenManager,\n+    nextRenewal: Long,\n+    de: RpcEndpointRef) extends Logging {\n+  private val credentialRenewerThread =\n+    Executors.newSingleThreadScheduledExecutor("
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Use the `PRINCIPAL` constant.",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-10-10T20:11:24Z",
    "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{Executors, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+class MesosCredentialRenewer(\n+    conf: SparkConf,\n+    tokenManager: HadoopDelegationTokenManager,\n+    nextRenewal: Long,\n+    de: RpcEndpointRef) extends Logging {\n+  private val credentialRenewerThread =\n+    Executors.newSingleThreadScheduledExecutor(\n+      ThreadUtils.namedThreadFactory(\"Credential Refresh Thread\"))\n+\n+  @volatile private var timeOfNextRenewal = nextRenewal\n+\n+  private val principal = conf.get(\"spark.yarn.principal\")"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Similarly there's a `KEYTAB` constant. Also why `64`?",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-10-10T20:11:39Z",
    "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{Executors, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+class MesosCredentialRenewer(\n+    conf: SparkConf,\n+    tokenManager: HadoopDelegationTokenManager,\n+    nextRenewal: Long,\n+    de: RpcEndpointRef) extends Logging {\n+  private val credentialRenewerThread =\n+    Executors.newSingleThreadScheduledExecutor(\n+      ThreadUtils.namedThreadFactory(\"Credential Refresh Thread\"))\n+\n+  @volatile private var timeOfNextRenewal = nextRenewal\n+\n+  private val principal = conf.get(\"spark.yarn.principal\")\n+\n+  private val (secretFile, mode) = getSecretFile(conf)\n+\n+  private def getSecretFile(conf: SparkConf): (String, String) = {\n+    val keytab64 = conf.get(\"spark.yarn.keytab\", null)"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`64`?\r\n\r\nAlso, using `conf.getenv` would allow tests to be written.",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-10-10T20:14:47Z",
    "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{Executors, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+class MesosCredentialRenewer(\n+    conf: SparkConf,\n+    tokenManager: HadoopDelegationTokenManager,\n+    nextRenewal: Long,\n+    de: RpcEndpointRef) extends Logging {\n+  private val credentialRenewerThread =\n+    Executors.newSingleThreadScheduledExecutor(\n+      ThreadUtils.namedThreadFactory(\"Credential Refresh Thread\"))\n+\n+  @volatile private var timeOfNextRenewal = nextRenewal\n+\n+  private val principal = conf.get(\"spark.yarn.principal\")\n+\n+  private val (secretFile, mode) = getSecretFile(conf)\n+\n+  private def getSecretFile(conf: SparkConf): (String, String) = {\n+    val keytab64 = conf.get(\"spark.yarn.keytab\", null)\n+    val tgt64 = System.getenv(\"KRB5CCNAME\")"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`KRB5CCNAME` is something that people might have in their environment for various reasons, so I'd avoid this requirement.",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-10-10T21:14:09Z",
    "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{Executors, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+class MesosCredentialRenewer(\n+    conf: SparkConf,\n+    tokenManager: HadoopDelegationTokenManager,\n+    nextRenewal: Long,\n+    de: RpcEndpointRef) extends Logging {\n+  private val credentialRenewerThread =\n+    Executors.newSingleThreadScheduledExecutor(\n+      ThreadUtils.namedThreadFactory(\"Credential Refresh Thread\"))\n+\n+  @volatile private var timeOfNextRenewal = nextRenewal\n+\n+  private val principal = conf.get(\"spark.yarn.principal\")\n+\n+  private val (secretFile, mode) = getSecretFile(conf)\n+\n+  private def getSecretFile(conf: SparkConf): (String, String) = {\n+    val keytab64 = conf.get(\"spark.yarn.keytab\", null)\n+    val tgt64 = System.getenv(\"KRB5CCNAME\")\n+    require(keytab64 != null || tgt64 != null, \"keytab or tgt required\")\n+    require(keytab64 == null || tgt64 == null, \"keytab and tgt cannot be used at the same time\")"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "How about a more descriptive variable name?",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-10-10T21:28:21Z",
    "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{Executors, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+class MesosCredentialRenewer(\n+    conf: SparkConf,\n+    tokenManager: HadoopDelegationTokenManager,\n+    nextRenewal: Long,\n+    de: RpcEndpointRef) extends Logging {"
  }, {
    "author": {
      "login": "ArtRand"
    },
    "body": "fixed.",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-10-18T01:32:38Z",
    "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{Executors, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+class MesosCredentialRenewer(\n+    conf: SparkConf,\n+    tokenManager: HadoopDelegationTokenManager,\n+    nextRenewal: Long,\n+    de: RpcEndpointRef) extends Logging {"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This looks similar but not exactly the same as the logic in YARN's `writeNewCredentialsToHDFS`. Both should be using the same logic to calculate these things, so probably time for some refactoring.",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-10-24T01:50:32Z",
    "diffHunk": "@@ -0,0 +1,169 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{Executors, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+/**\n+ * The MesosCredentialRenewer will update the Hadoop credentials for Spark drivers accessing\n+ * secured services using Kerberos authentication. It is modeled after the YARN AMCredential\n+ * renewer, and similarly will renew the Credentials when 75% of the renewal interval has passed.\n+ * The principal difference is that instead of writing the new credentials to HDFS and\n+ * incrementing the timestamp of the file, the new credentials (called Tokens when they are\n+ * serialized) are broadcast to all running executors. On the executor side, when new Tokens are\n+ * recieved they overwrite the current credentials.\n+ */\n+class MesosCredentialRenewer(\n+    conf: SparkConf,\n+    tokenManager: HadoopDelegationTokenManager,\n+    nextRenewal: Long,\n+    driverEndpoint: RpcEndpointRef) extends Logging {\n+  private val credentialRenewerThread =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"Credential Renewal Thread\")\n+\n+  @volatile private var timeOfNextRenewal = nextRenewal\n+\n+  private val principal = conf.get(config.PRINCIPAL).orNull\n+\n+  private val (secretFile, mode) = getSecretFile(conf)\n+\n+  private def getSecretFile(conf: SparkConf): (String, String) = {\n+    val keytab = conf.get(config.KEYTAB).orNull\n+    val tgt = conf.getenv(\"KRB5CCNAME\")\n+    require(keytab != null || tgt != null, \"A keytab or TGT required.\")\n+    // if both Keytab and TGT are detected we use the Keytab.\n+    val (secretFile, mode) = if (keytab != null && tgt != null) {\n+      logWarning(s\"Keytab and TGT were detected, using keytab, unset $keytab to use TGT\")\n+      (keytab, \"keytab\")\n+    } else {\n+      val m = if (keytab != null) \"keytab\" else \"tgt\"\n+      val sf = if (keytab != null) keytab else tgt\n+      (sf, m)\n+    }\n+    logInfo(s\"Logging in as $principal with mode $mode to retrieve Hadoop delegation tokens\")\n+    logDebug(s\"secretFile is $secretFile\")\n+    (secretFile, mode)\n+  }\n+\n+  def scheduleTokenRenewal(): Unit = {\n+    def scheduleRenewal(runnable: Runnable): Unit = {\n+      val remainingTime = timeOfNextRenewal - System.currentTimeMillis()\n+      if (remainingTime <= 0) {\n+        logInfo(\"Credentials have expired, creating new ones now.\")\n+        runnable.run()\n+      } else {\n+        logInfo(s\"Scheduling login from keytab in $remainingTime millis.\")\n+        credentialRenewerThread.schedule(runnable, remainingTime, TimeUnit.MILLISECONDS)\n+      }\n+    }\n+\n+    val credentialRenewerRunnable =\n+      new Runnable {\n+        override def run(): Unit = {\n+          try {\n+            val creds = getRenewedDelegationTokens(conf)\n+            broadcastDelegationTokens(creds)\n+          } catch {\n+            case e: Exception =>\n+              // Log the error and try to write new tokens back in an hour\n+              logWarning(\"Couldn't broadcast tokens, trying again in an hour\", e)\n+              credentialRenewerThread.schedule(this, 1, TimeUnit.HOURS)\n+              return\n+          }\n+          scheduleRenewal(this)\n+        }\n+      }\n+    scheduleRenewal(credentialRenewerRunnable)\n+  }\n+\n+  private def getRenewedDelegationTokens(conf: SparkConf): Array[Byte] = {\n+    logInfo(s\"Attempting to login with ${conf.get(config.PRINCIPAL).orNull}\")\n+    // Get new delegation tokens by logging in with a new UGI\n+    // inspired by AMCredentialRenewer.scala:L174\n+    val ugi = if (mode == \"keytab\") {\n+      UserGroupInformation.loginUserFromKeytabAndReturnUGI(principal, secretFile)\n+    } else {\n+      UserGroupInformation.getUGIFromTicketCache(secretFile, principal)\n+    }\n+    val tempCreds = ugi.getCredentials\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    var nextRenewalTime = Long.MaxValue\n+    ugi.doAs(new PrivilegedExceptionAction[Void] {\n+      override def run(): Void = {\n+        nextRenewalTime = tokenManager.obtainDelegationTokens(hadoopConf, tempCreds)\n+        null\n+      }\n+    })\n+\n+    val currTime = System.currentTimeMillis()\n+    timeOfNextRenewal = if (nextRenewalTime <= currTime) {\n+      logWarning(s\"Next credential renewal time ($nextRenewalTime) is earlier than \" +\n+        s\"current time ($currTime), which is unexpected, please check your credential renewal \" +\n+        \"related configurations in the target services.\")\n+      currTime\n+    } else {\n+      MesosCredentialRenewer.getNextRenewalTime(nextRenewalTime)\n+    }\n+    logInfo(s\"Time of next renewal is $timeOfNextRenewal\")\n+\n+    // Add the temp credentials back to the original ones.\n+    UserGroupInformation.getCurrentUser.addCredentials(tempCreds)\n+    SparkHadoopUtil.get.serialize(tempCreds)\n+  }\n+\n+  private def broadcastDelegationTokens(tokens: Array[Byte]): Unit = {\n+    // send token to existing executors\n+    logInfo(\"Sending new tokens to all executors\")\n+    driverEndpoint.send(UpdateDelegationTokens(tokens))\n+  }\n+}\n+\n+object MesosCredentialRenewer extends Logging {\n+  def getTokenRenewalTime(bytes: Array[Byte], conf: SparkConf): Long = {\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    val creds = SparkHadoopUtil.get.deserialize(bytes)\n+    val renewalTimes = creds.getAllTokens.asScala.flatMap { t =>\n+      Try {\n+        t.renew(hadoopConf)\n+      }.toOption\n+    }\n+    if (renewalTimes.isEmpty) Long.MaxValue else renewalTimes.min\n+  }\n+\n+  def getNextRenewalTime(t: Long): Long = {"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`${KEYTAB.key}`?",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-10-24T01:53:27Z",
    "diffHunk": "@@ -0,0 +1,169 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{Executors, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+/**\n+ * The MesosCredentialRenewer will update the Hadoop credentials for Spark drivers accessing\n+ * secured services using Kerberos authentication. It is modeled after the YARN AMCredential\n+ * renewer, and similarly will renew the Credentials when 75% of the renewal interval has passed.\n+ * The principal difference is that instead of writing the new credentials to HDFS and\n+ * incrementing the timestamp of the file, the new credentials (called Tokens when they are\n+ * serialized) are broadcast to all running executors. On the executor side, when new Tokens are\n+ * recieved they overwrite the current credentials.\n+ */\n+class MesosCredentialRenewer(\n+    conf: SparkConf,\n+    tokenManager: HadoopDelegationTokenManager,\n+    nextRenewal: Long,\n+    driverEndpoint: RpcEndpointRef) extends Logging {\n+  private val credentialRenewerThread =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"Credential Renewal Thread\")\n+\n+  @volatile private var timeOfNextRenewal = nextRenewal\n+\n+  private val principal = conf.get(config.PRINCIPAL).orNull\n+\n+  private val (secretFile, mode) = getSecretFile(conf)\n+\n+  private def getSecretFile(conf: SparkConf): (String, String) = {\n+    val keytab = conf.get(config.KEYTAB).orNull\n+    val tgt = conf.getenv(\"KRB5CCNAME\")\n+    require(keytab != null || tgt != null, \"A keytab or TGT required.\")\n+    // if both Keytab and TGT are detected we use the Keytab.\n+    val (secretFile, mode) = if (keytab != null && tgt != null) {\n+      logWarning(s\"Keytab and TGT were detected, using keytab, unset $keytab to use TGT\")"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Comment is redundant.",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-10-24T01:54:57Z",
    "diffHunk": "@@ -0,0 +1,169 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{Executors, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.internal.config\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+/**\n+ * The MesosCredentialRenewer will update the Hadoop credentials for Spark drivers accessing\n+ * secured services using Kerberos authentication. It is modeled after the YARN AMCredential\n+ * renewer, and similarly will renew the Credentials when 75% of the renewal interval has passed.\n+ * The principal difference is that instead of writing the new credentials to HDFS and\n+ * incrementing the timestamp of the file, the new credentials (called Tokens when they are\n+ * serialized) are broadcast to all running executors. On the executor side, when new Tokens are\n+ * recieved they overwrite the current credentials.\n+ */\n+class MesosCredentialRenewer(\n+    conf: SparkConf,\n+    tokenManager: HadoopDelegationTokenManager,\n+    nextRenewal: Long,\n+    driverEndpoint: RpcEndpointRef) extends Logging {\n+  private val credentialRenewerThread =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"Credential Renewal Thread\")\n+\n+  @volatile private var timeOfNextRenewal = nextRenewal\n+\n+  private val principal = conf.get(config.PRINCIPAL).orNull\n+\n+  private val (secretFile, mode) = getSecretFile(conf)\n+\n+  private def getSecretFile(conf: SparkConf): (String, String) = {\n+    val keytab = conf.get(config.KEYTAB).orNull\n+    val tgt = conf.getenv(\"KRB5CCNAME\")\n+    require(keytab != null || tgt != null, \"A keytab or TGT required.\")\n+    // if both Keytab and TGT are detected we use the Keytab.\n+    val (secretFile, mode) = if (keytab != null && tgt != null) {\n+      logWarning(s\"Keytab and TGT were detected, using keytab, unset $keytab to use TGT\")\n+      (keytab, \"keytab\")\n+    } else {\n+      val m = if (keytab != null) \"keytab\" else \"tgt\"\n+      val sf = if (keytab != null) keytab else tgt\n+      (sf, m)\n+    }\n+    logInfo(s\"Logging in as $principal with mode $mode to retrieve Hadoop delegation tokens\")\n+    logDebug(s\"secretFile is $secretFile\")\n+    (secretFile, mode)\n+  }\n+\n+  def scheduleTokenRenewal(): Unit = {\n+    def scheduleRenewal(runnable: Runnable): Unit = {\n+      val remainingTime = timeOfNextRenewal - System.currentTimeMillis()\n+      if (remainingTime <= 0) {\n+        logInfo(\"Credentials have expired, creating new ones now.\")\n+        runnable.run()\n+      } else {\n+        logInfo(s\"Scheduling login from keytab in $remainingTime millis.\")\n+        credentialRenewerThread.schedule(runnable, remainingTime, TimeUnit.MILLISECONDS)\n+      }\n+    }\n+\n+    val credentialRenewerRunnable =\n+      new Runnable {\n+        override def run(): Unit = {\n+          try {\n+            val creds = getRenewedDelegationTokens(conf)\n+            broadcastDelegationTokens(creds)\n+          } catch {\n+            case e: Exception =>\n+              // Log the error and try to write new tokens back in an hour\n+              logWarning(\"Couldn't broadcast tokens, trying again in an hour\", e)\n+              credentialRenewerThread.schedule(this, 1, TimeUnit.HOURS)\n+              return\n+          }\n+          scheduleRenewal(this)\n+        }\n+      }\n+    scheduleRenewal(credentialRenewerRunnable)\n+  }\n+\n+  private def getRenewedDelegationTokens(conf: SparkConf): Array[Byte] = {\n+    logInfo(s\"Attempting to login with ${conf.get(config.PRINCIPAL).orNull}\")\n+    // Get new delegation tokens by logging in with a new UGI\n+    // inspired by AMCredentialRenewer.scala:L174\n+    val ugi = if (mode == \"keytab\") {\n+      UserGroupInformation.loginUserFromKeytabAndReturnUGI(principal, secretFile)\n+    } else {\n+      UserGroupInformation.getUGIFromTicketCache(secretFile, principal)\n+    }\n+    val tempCreds = ugi.getCredentials\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    var nextRenewalTime = Long.MaxValue\n+    ugi.doAs(new PrivilegedExceptionAction[Void] {\n+      override def run(): Void = {\n+        nextRenewalTime = tokenManager.obtainDelegationTokens(hadoopConf, tempCreds)\n+        null\n+      }\n+    })\n+\n+    val currTime = System.currentTimeMillis()\n+    timeOfNextRenewal = if (nextRenewalTime <= currTime) {\n+      logWarning(s\"Next credential renewal time ($nextRenewalTime) is earlier than \" +\n+        s\"current time ($currTime), which is unexpected, please check your credential renewal \" +\n+        \"related configurations in the target services.\")\n+      currTime\n+    } else {\n+      MesosCredentialRenewer.getNextRenewalTime(nextRenewalTime)\n+    }\n+    logInfo(s\"Time of next renewal is $timeOfNextRenewal\")\n+\n+    // Add the temp credentials back to the original ones.\n+    UserGroupInformation.getCurrentUser.addCredentials(tempCreds)\n+    SparkHadoopUtil.get.serialize(tempCreds)\n+  }\n+\n+  private def broadcastDelegationTokens(tokens: Array[Byte]): Unit = {\n+    // send token to existing executors"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`private[spark]`",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-11-06T23:33:53Z",
    "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{ScheduledExecutorService, TimeUnit}\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+/**\n+ * The MesosCredentialRenewer will update the Hadoop credentials for Spark drivers accessing\n+ * secured services using Kerberos authentication. It is modeled after the YARN AMCredential\n+ * renewer, and similarly will renew the Credentials when 75% of the renewal interval has passed.\n+ * The principal difference is that instead of writing the new credentials to HDFS and\n+ * incrementing the timestamp of the file, the new credentials (called Tokens when they are\n+ * serialized) are broadcast to all running executors. On the executor side, when new Tokens are\n+ * recieved they overwrite the current credentials.\n+ */\n+class MesosCredentialRenewer("
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`private`?",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-11-06T23:34:24Z",
    "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{ScheduledExecutorService, TimeUnit}\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+/**\n+ * The MesosCredentialRenewer will update the Hadoop credentials for Spark drivers accessing\n+ * secured services using Kerberos authentication. It is modeled after the YARN AMCredential\n+ * renewer, and similarly will renew the Credentials when 75% of the renewal interval has passed.\n+ * The principal difference is that instead of writing the new credentials to HDFS and\n+ * incrementing the timestamp of the file, the new credentials (called Tokens when they are\n+ * serialized) are broadcast to all running executors. On the executor side, when new Tokens are\n+ * recieved they overwrite the current credentials.\n+ */\n+class MesosCredentialRenewer(\n+    conf: SparkConf,\n+    tokenManager: HadoopDelegationTokenManager) extends Logging {\n+\n+  private val credentialRenewerThread: ScheduledExecutorService =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"Credential Renewal Thread\")\n+\n+  private val principal = conf.get(config.PRINCIPAL).orNull\n+\n+  private val (secretFile, mode) = getSecretFile(conf)\n+\n+  var (tokens: Array[Byte], timeOfNextRenewal: Long) = {"
  }],
  "prId": 19272
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Why isn't this done in the constructor? There's a single call to this method, and the renewal interval could very easily be turned into a constructor arg.",
    "commit": "049e4b554b38f12bd1a2bd6855fcbb6a937fbf99",
    "createdAt": "2017-11-06T23:37:18Z",
    "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import java.security.PrivilegedExceptionAction\n+import java.util.concurrent.{ScheduledExecutorService, TimeUnit}\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.security.HadoopDelegationTokenManager\n+import org.apache.spark.internal.{config, Logging}\n+import org.apache.spark.rpc.RpcEndpointRef\n+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens\n+import org.apache.spark.util.ThreadUtils\n+\n+\n+/**\n+ * The MesosCredentialRenewer will update the Hadoop credentials for Spark drivers accessing\n+ * secured services using Kerberos authentication. It is modeled after the YARN AMCredential\n+ * renewer, and similarly will renew the Credentials when 75% of the renewal interval has passed.\n+ * The principal difference is that instead of writing the new credentials to HDFS and\n+ * incrementing the timestamp of the file, the new credentials (called Tokens when they are\n+ * serialized) are broadcast to all running executors. On the executor side, when new Tokens are\n+ * recieved they overwrite the current credentials.\n+ */\n+class MesosCredentialRenewer(\n+    conf: SparkConf,\n+    tokenManager: HadoopDelegationTokenManager) extends Logging {\n+\n+  private val credentialRenewerThread: ScheduledExecutorService =\n+    ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"Credential Renewal Thread\")\n+\n+  private val principal = conf.get(config.PRINCIPAL).orNull\n+\n+  private val (secretFile, mode) = getSecretFile(conf)\n+\n+  var (tokens: Array[Byte], timeOfNextRenewal: Long) = {\n+    try {\n+      val creds = UserGroupInformation.getCurrentUser.getCredentials\n+      val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+      val rt = tokenManager.obtainDelegationTokens(hadoopConf, creds)\n+      (SparkHadoopUtil.get.serialize(creds), rt)\n+    } catch {\n+      case e: Exception =>\n+        throw new IllegalStateException(\"Failed to initialize Hadoop delegation tokens\\n\" +\n+          s\"\\tPricipal: $principal\\n\\tmode: $mode\\n\\tsecret file $secretFile\\n\\tException: $e\")\n+    }\n+\n+  }\n+\n+  private def getSecretFile(conf: SparkConf): (String, String) = {\n+    val keytab = conf.get(config.KEYTAB).orNull\n+    val tgt = conf.getenv(\"KRB5CCNAME\")\n+    require(keytab != null || tgt != null, \"A keytab or TGT required.\")\n+    // if both Keytab and TGT are detected we use the Keytab.\n+    val (secretFile, mode) = if (keytab != null && tgt != null) {\n+      logWarning(s\"Keytab and TGT were detected, using keytab, \" +\n+        s\"unset ${config.KEYTAB.key} to use TGT\")\n+      (keytab, \"keytab\")\n+    } else {\n+      val m = if (keytab != null) \"keytab\" else \"tgt\"\n+      val sf = if (keytab != null) keytab else tgt\n+      (sf, m)\n+    }\n+\n+    if (principal == null) {\n+      logInfo(s\"Using mode: $mode to retrieve Hadoop delegation tokens\")\n+    } else {\n+      logInfo(s\"Using principal: $principal with mode: $mode to retrieve Hadoop delegation tokens\")\n+    }\n+\n+    logDebug(s\"secretFile is $secretFile\")\n+    (secretFile, mode)\n+  }\n+\n+  def scheduleTokenRenewal(driverEndpoint: RpcEndpointRef): Unit = {"
  }],
  "prId": 19272
}]