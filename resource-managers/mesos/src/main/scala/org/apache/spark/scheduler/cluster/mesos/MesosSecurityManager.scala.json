[{
  "comments": [{
    "author": {
      "login": "mridulm"
    },
    "body": "This is potentially buggy - and will work only the first time for some credential providers - hbase (for example).\r\nToken acquisition will fail for hbase when (prev) TOKEN is already present in the credentials - irrespective of whether kerberos credentials are present or not.\r\nAcquiring credentials in context of ugi returned via `UserGroupInformation.loginUserFromKeytabAndReturnUGI` avoids this issue.\r\n\r\n@jerryshao might have a better reference for the hbase behavior - since IIRC he worked a bit on it.",
    "commit": "e32afeeac95883138751c060a3ebfaf309e3d22f",
    "createdAt": "2017-04-18T08:07:47Z",
    "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import javax.xml.bind.DatatypeConverter\n+\n+import org.apache.hadoop.security.{Credentials, UserGroupInformation}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.mesos.config\n+import org.apache.spark.deploy.security.ConfigurableCredentialManager\n+import org.apache.spark.internal.Logging\n+\n+\n+private[mesos] class MesosSecurityManager extends Logging {\n+  def isSecurityEnabled(): Boolean = {\n+    UserGroupInformation.isSecurityEnabled\n+  }\n+\n+  /** Writes delegation tokens to spark.mesos.kerberos.ugiTokens */\n+  def setUGITokens(conf: SparkConf): Unit = {\n+    val userCreds = getDelegationTokens(conf)\n+\n+    val byteStream = new java.io.ByteArrayOutputStream(1024 * 1024)\n+    val dataStream = new java.io.DataOutputStream(byteStream)\n+    userCreds.writeTokenStorageToStream(dataStream)\n+    val credsBytes = byteStream.toByteArray\n+\n+    logInfo(s\"Writing ${credsBytes.length} bytes to ${config.USER_CREDENTIALS.key}.\")\n+\n+    val creds64 = DatatypeConverter.printBase64Binary(credsBytes)\n+    conf.set(config.USER_CREDENTIALS, creds64)\n+  }\n+\n+  /**\n+   * Returns the user's credentials, with new delegation tokens added for all configured\n+   * services.\n+   */\n+  private def getDelegationTokens(conf: SparkConf): Credentials = {\n+    val userCreds = UserGroupInformation.getCurrentUser.getCredentials\n+    val numTokensBefore = userCreds.numberOfTokens\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    val credentialManager = new ConfigurableCredentialManager(conf, hadoopConf)\n+    credentialManager.obtainCredentials(hadoopConf, userCreds)"
  }, {
    "author": {
      "login": "mgummelt"
    },
    "body": "Fetching tokens multiple times isn't supported yet.  That will come in the renewal PR.\r\n\r\nWhen you say it will fail, do you mean it will throw an exception, or just fail to fetch a token?  Looking at the code, I don't see how either would happen: https://github.com/mesosphere/spark/blob/SPARK-16742-kerberos/core/src/main/scala/org/apache/spark/deploy/security/HBaseCredentialProvider.scala#L46 It looks like it fetches a token indepedent of the current credentials, then overwrites any current HBase token via `creds.addToken`",
    "commit": "e32afeeac95883138751c060a3ebfaf309e3d22f",
    "createdAt": "2017-04-18T17:29:36Z",
    "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import javax.xml.bind.DatatypeConverter\n+\n+import org.apache.hadoop.security.{Credentials, UserGroupInformation}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.mesos.config\n+import org.apache.spark.deploy.security.ConfigurableCredentialManager\n+import org.apache.spark.internal.Logging\n+\n+\n+private[mesos] class MesosSecurityManager extends Logging {\n+  def isSecurityEnabled(): Boolean = {\n+    UserGroupInformation.isSecurityEnabled\n+  }\n+\n+  /** Writes delegation tokens to spark.mesos.kerberos.ugiTokens */\n+  def setUGITokens(conf: SparkConf): Unit = {\n+    val userCreds = getDelegationTokens(conf)\n+\n+    val byteStream = new java.io.ByteArrayOutputStream(1024 * 1024)\n+    val dataStream = new java.io.DataOutputStream(byteStream)\n+    userCreds.writeTokenStorageToStream(dataStream)\n+    val credsBytes = byteStream.toByteArray\n+\n+    logInfo(s\"Writing ${credsBytes.length} bytes to ${config.USER_CREDENTIALS.key}.\")\n+\n+    val creds64 = DatatypeConverter.printBase64Binary(credsBytes)\n+    conf.set(config.USER_CREDENTIALS, creds64)\n+  }\n+\n+  /**\n+   * Returns the user's credentials, with new delegation tokens added for all configured\n+   * services.\n+   */\n+  private def getDelegationTokens(conf: SparkConf): Credentials = {\n+    val userCreds = UserGroupInformation.getCurrentUser.getCredentials\n+    val numTokensBefore = userCreds.numberOfTokens\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    val credentialManager = new ConfigurableCredentialManager(conf, hadoopConf)\n+    credentialManager.obtainCredentials(hadoopConf, userCreds)"
  }, {
    "author": {
      "login": "mridulm"
    },
    "body": "@mgummelt It might be a bit involved, and has to do with how hbase issues tokens at its side (not in client library, but at the server) - if it sees a token in the incoming request, it will not issue new tokens.\r\n@jerryshao is the right person to detail it if required, but bottomline is, if we add acquired tokens to ugi.currentUser, then subsequently we cannot use ugi.currentUser to acquire tokens for hbase. This actually comes from semantics of token, and might be applicable to other token providers as well (hbase is one case I know where it does this for sure).",
    "commit": "e32afeeac95883138751c060a3ebfaf309e3d22f",
    "createdAt": "2017-04-18T19:33:57Z",
    "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import javax.xml.bind.DatatypeConverter\n+\n+import org.apache.hadoop.security.{Credentials, UserGroupInformation}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.mesos.config\n+import org.apache.spark.deploy.security.ConfigurableCredentialManager\n+import org.apache.spark.internal.Logging\n+\n+\n+private[mesos] class MesosSecurityManager extends Logging {\n+  def isSecurityEnabled(): Boolean = {\n+    UserGroupInformation.isSecurityEnabled\n+  }\n+\n+  /** Writes delegation tokens to spark.mesos.kerberos.ugiTokens */\n+  def setUGITokens(conf: SparkConf): Unit = {\n+    val userCreds = getDelegationTokens(conf)\n+\n+    val byteStream = new java.io.ByteArrayOutputStream(1024 * 1024)\n+    val dataStream = new java.io.DataOutputStream(byteStream)\n+    userCreds.writeTokenStorageToStream(dataStream)\n+    val credsBytes = byteStream.toByteArray\n+\n+    logInfo(s\"Writing ${credsBytes.length} bytes to ${config.USER_CREDENTIALS.key}.\")\n+\n+    val creds64 = DatatypeConverter.printBase64Binary(credsBytes)\n+    conf.set(config.USER_CREDENTIALS, creds64)\n+  }\n+\n+  /**\n+   * Returns the user's credentials, with new delegation tokens added for all configured\n+   * services.\n+   */\n+  private def getDelegationTokens(conf: SparkConf): Credentials = {\n+    val userCreds = UserGroupInformation.getCurrentUser.getCredentials\n+    val numTokensBefore = userCreds.numberOfTokens\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    val credentialManager = new ConfigurableCredentialManager(conf, hadoopConf)\n+    credentialManager.obtainCredentials(hadoopConf, userCreds)"
  }, {
    "author": {
      "login": "mgummelt"
    },
    "body": "Regardless, it shouldn't be an issue until we add renewal, right?  Without renewal, we only ever fetch a single token.",
    "commit": "e32afeeac95883138751c060a3ebfaf309e3d22f",
    "createdAt": "2017-04-18T20:30:36Z",
    "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import javax.xml.bind.DatatypeConverter\n+\n+import org.apache.hadoop.security.{Credentials, UserGroupInformation}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.mesos.config\n+import org.apache.spark.deploy.security.ConfigurableCredentialManager\n+import org.apache.spark.internal.Logging\n+\n+\n+private[mesos] class MesosSecurityManager extends Logging {\n+  def isSecurityEnabled(): Boolean = {\n+    UserGroupInformation.isSecurityEnabled\n+  }\n+\n+  /** Writes delegation tokens to spark.mesos.kerberos.ugiTokens */\n+  def setUGITokens(conf: SparkConf): Unit = {\n+    val userCreds = getDelegationTokens(conf)\n+\n+    val byteStream = new java.io.ByteArrayOutputStream(1024 * 1024)\n+    val dataStream = new java.io.DataOutputStream(byteStream)\n+    userCreds.writeTokenStorageToStream(dataStream)\n+    val credsBytes = byteStream.toByteArray\n+\n+    logInfo(s\"Writing ${credsBytes.length} bytes to ${config.USER_CREDENTIALS.key}.\")\n+\n+    val creds64 = DatatypeConverter.printBase64Binary(credsBytes)\n+    conf.set(config.USER_CREDENTIALS, creds64)\n+  }\n+\n+  /**\n+   * Returns the user's credentials, with new delegation tokens added for all configured\n+   * services.\n+   */\n+  private def getDelegationTokens(conf: SparkConf): Credentials = {\n+    val userCreds = UserGroupInformation.getCurrentUser.getCredentials\n+    val numTokensBefore = userCreds.numberOfTokens\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    val credentialManager = new ConfigurableCredentialManager(conf, hadoopConf)\n+    credentialManager.obtainCredentials(hadoopConf, userCreds)"
  }, {
    "author": {
      "login": "mridulm"
    },
    "body": "Yes, that is why I said this is potentially buggy :-)\r\nAs it currently stands in the current happy path - since there is only a single invocation of the method, it will work.\r\nBut as soon as another invocation of the method happens in the same jvm (through any means); it will fail to acquire tokens.",
    "commit": "e32afeeac95883138751c060a3ebfaf309e3d22f",
    "createdAt": "2017-04-18T20:49:37Z",
    "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import javax.xml.bind.DatatypeConverter\n+\n+import org.apache.hadoop.security.{Credentials, UserGroupInformation}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.mesos.config\n+import org.apache.spark.deploy.security.ConfigurableCredentialManager\n+import org.apache.spark.internal.Logging\n+\n+\n+private[mesos] class MesosSecurityManager extends Logging {\n+  def isSecurityEnabled(): Boolean = {\n+    UserGroupInformation.isSecurityEnabled\n+  }\n+\n+  /** Writes delegation tokens to spark.mesos.kerberos.ugiTokens */\n+  def setUGITokens(conf: SparkConf): Unit = {\n+    val userCreds = getDelegationTokens(conf)\n+\n+    val byteStream = new java.io.ByteArrayOutputStream(1024 * 1024)\n+    val dataStream = new java.io.DataOutputStream(byteStream)\n+    userCreds.writeTokenStorageToStream(dataStream)\n+    val credsBytes = byteStream.toByteArray\n+\n+    logInfo(s\"Writing ${credsBytes.length} bytes to ${config.USER_CREDENTIALS.key}.\")\n+\n+    val creds64 = DatatypeConverter.printBase64Binary(credsBytes)\n+    conf.set(config.USER_CREDENTIALS, creds64)\n+  }\n+\n+  /**\n+   * Returns the user's credentials, with new delegation tokens added for all configured\n+   * services.\n+   */\n+  private def getDelegationTokens(conf: SparkConf): Credentials = {\n+    val userCreds = UserGroupInformation.getCurrentUser.getCredentials\n+    val numTokensBefore = userCreds.numberOfTokens\n+    val hadoopConf = SparkHadoopUtil.get.newConfiguration(conf)\n+    val credentialManager = new ConfigurableCredentialManager(conf, hadoopConf)\n+    credentialManager.obtainCredentials(hadoopConf, userCreds)"
  }],
  "prId": 17665
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Is it necessary to create a large buffer up front? Credentials will be much smaller than that in most situations.\r\n\r\nAlso, why not just import these types?",
    "commit": "e32afeeac95883138751c060a3ebfaf309e3d22f",
    "createdAt": "2017-04-19T19:48:32Z",
    "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import javax.xml.bind.DatatypeConverter\n+\n+import org.apache.hadoop.security.{Credentials, UserGroupInformation}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.mesos.config\n+import org.apache.spark.deploy.security.ConfigurableCredentialManager\n+import org.apache.spark.internal.Logging\n+\n+\n+private[mesos] class MesosSecurityManager extends Logging {\n+  def isSecurityEnabled(): Boolean = {\n+    UserGroupInformation.isSecurityEnabled\n+  }\n+\n+  /** Writes delegation tokens to spark.mesos.kerberos.ugiTokens */\n+  def setUGITokens(conf: SparkConf): Unit = {\n+    val userCreds = getDelegationTokens(conf)\n+\n+    val byteStream = new java.io.ByteArrayOutputStream(1024 * 1024)"
  }, {
    "author": {
      "login": "mgummelt"
    },
    "body": "No good reason.  Fixed both.",
    "commit": "e32afeeac95883138751c060a3ebfaf309e3d22f",
    "createdAt": "2017-04-20T22:18:21Z",
    "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.scheduler.cluster.mesos\n+\n+import javax.xml.bind.DatatypeConverter\n+\n+import org.apache.hadoop.security.{Credentials, UserGroupInformation}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.deploy.mesos.config\n+import org.apache.spark.deploy.security.ConfigurableCredentialManager\n+import org.apache.spark.internal.Logging\n+\n+\n+private[mesos] class MesosSecurityManager extends Logging {\n+  def isSecurityEnabled(): Boolean = {\n+    UserGroupInformation.isSecurityEnabled\n+  }\n+\n+  /** Writes delegation tokens to spark.mesos.kerberos.ugiTokens */\n+  def setUGITokens(conf: SparkConf): Unit = {\n+    val userCreds = getDelegationTokens(conf)\n+\n+    val byteStream = new java.io.ByteArrayOutputStream(1024 * 1024)"
  }],
  "prId": 17665
}]