[{
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "Is it possible that `sc` is null?\n",
    "commit": "543d208dfaa819aabdf62d0d3c958a110518d966",
    "createdAt": "2015-07-23T16:10:30Z",
    "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kinesis\n+\n+import org.scalatest.{BeforeAndAfter, BeforeAndAfterAll}\n+\n+import org.apache.spark.storage.{BlockId, BlockManager, StorageLevel, StreamBlockId}\n+import org.apache.spark.{SparkConf, SparkContext, SparkException, SparkFunSuite}\n+\n+class KinesisBackedBlockRDDSuite extends KinesisFunSuite with BeforeAndAfterAll {\n+\n+  private val regionId = \"us-east-1\"\n+  private val endpointUrl = \"https://kinesis.us-east-1.amazonaws.com\"\n+  private val testData = 1 to 8\n+\n+  private var testUtils: KinesisTestUtils = null\n+  private var shardIds: Seq[String] = null\n+  private var shardIdToData: Map[String, Seq[Int]] = null\n+  private var shardIdToSeqNumbers: Map[String, Seq[String]] = null\n+  private var shardIdToDataAndSeqNumbers: Map[String, Seq[(Int, String)]] = null\n+  private var shardIdToRange: Map[String, SequenceNumberRange] = null\n+  private var allRanges: Seq[SequenceNumberRange] = null\n+\n+  private var sc: SparkContext = null\n+  private var blockManager: BlockManager = null\n+\n+\n+  override def beforeAll(): Unit = {\n+    runIfTestsEnabled(\"Prepare KinesisTestUtils\") {\n+      testUtils = new KinesisTestUtils(endpointUrl)\n+      testUtils.createStream()\n+\n+      shardIdToDataAndSeqNumbers = testUtils.pushData(testData)\n+      require(shardIdToDataAndSeqNumbers.size > 1, \"Need data to be sent to multiple shards\")\n+\n+      shardIds = shardIdToDataAndSeqNumbers.keySet.toSeq\n+      shardIdToData = shardIdToDataAndSeqNumbers.mapValues { _.map { _._1 }}\n+      shardIdToSeqNumbers = shardIdToDataAndSeqNumbers.mapValues { _.map { _._2 }}\n+      shardIdToRange = shardIdToSeqNumbers.map { case (shardId, seqNumbers) =>\n+        val seqNumRange = SequenceNumberRange(\n+          testUtils.streamName, shardId, seqNumbers.head, seqNumbers.last)\n+        (shardId, seqNumRange)\n+      }\n+      allRanges = shardIdToRange.values.toSeq\n+\n+      val conf = new SparkConf().setMaster(\"local[4]\").setAppName(\"KinesisBackedBlockRDDSuite\")\n+      sc = new SparkContext(conf)\n+      blockManager = sc.env.blockManager\n+    }\n+  }\n+\n+  override def afterAll(): Unit = {\n+    sc.stop()"
  }],
  "prId": 7578
}]