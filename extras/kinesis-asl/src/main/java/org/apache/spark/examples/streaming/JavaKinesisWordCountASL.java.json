[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: word counts by key doesnt make sense.\n",
    "commit": "47745816b21d7d2255a98283e3055a5a2a397a27",
    "createdAt": "2014-08-02T02:31:57Z",
    "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.regex.Pattern;\n+\n+import org.apache.log4j.Logger;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.Function2;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+import org.apache.spark.streaming.Duration;\n+import org.apache.spark.streaming.api.java.JavaDStream;\n+import org.apache.spark.streaming.api.java.JavaPairDStream;\n+import org.apache.spark.streaming.api.java.JavaStreamingContext;\n+import org.apache.spark.streaming.kinesis.KinesisUtils;\n+\n+import scala.Tuple2;\n+\n+import com.amazonaws.auth.DefaultAWSCredentialsProviderChain;\n+import com.amazonaws.services.kinesis.AmazonKinesisClient;\n+import com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream;\n+import com.google.common.collect.Lists;\n+\n+/**\n+ * Java-friendly Kinesis Spark Streaming WordCount example\n+ *\n+ * See http://spark.apache.org/docs/latest/streaming-kinesis.html for more details\n+ * on the Kinesis Spark Streaming integration.\n+ *\n+ * This example spins up 1 Kinesis Worker (Spark Streaming Receiver) per shard\n+ *   for the given stream.\n+ * It then starts pulling from the last checkpointed sequence number of the given\n+ *   <stream-name> and <endpoint-url>. \n+ *\n+ * Valid endpoint urls:  http://docs.aws.amazon.com/general/latest/gr/rande.html#ak_region\n+ *\n+ * This code uses the DefaultAWSCredentialsProviderChain and searches for credentials \n+ *  in the following order of precedence: \n+ *         Environment Variables - AWS_ACCESS_KEY_ID and AWS_SECRET_KEY\n+ *         Java System Properties - aws.accessKeyId and aws.secretKey\n+ *         Credential profiles file - default location (~/.aws/credentials) shared by all AWS SDKs\n+ *         Instance profile credentials - delivered through the Amazon EC2 metadata service\n+ *\n+ * Usage: JavaKinesisWordCountASL <stream-name> <endpoint-url>\n+ *         <stream-name> is the name of the Kinesis stream (ie. mySparkStream)\n+ *         <endpoint-url> is the endpoint of the Kinesis service \n+ *           (ie. https://kinesis.us-east-1.amazonaws.com)\n+ *\n+ * Example:\n+ *      $ export AWS_ACCESS_KEY_ID=<your-access-key>\n+ *      $ export AWS_SECRET_KEY=<your-secret-key>\n+ *      $ $SPARK_HOME/bin/run-example \\\n+ *            org.apache.spark.examples.streaming.JavaKinesisWordCountASL mySparkStream \\\n+ *            https://kinesis.us-east-1.amazonaws.com\n+ *\n+ * There is a companion helper class called KinesisWordCountProducerASL which puts dummy data \n+ *   onto the Kinesis stream. \n+ * Usage instructions for KinesisWordCountProducerASL are provided in the class definition.\n+ */\n+public final class JavaKinesisWordCountASL {\n+    private static final Pattern WORD_SEPARATOR = Pattern.compile(\" \");\n+    private static final Logger logger = Logger.getLogger(JavaKinesisWordCountASL.class);\n+\n+    /*\n+     * Make the constructor private to enforce singleton\n+     */\n+    private JavaKinesisWordCountASL() {\n+    }\n+\n+    public static void main(String[] args) {\n+        /*\n+         * Check that all required args were passed in.\n+         */\n+        if (args.length < 2) {\n+          System.err.println(\n+              \"|Usage: KinesisWordCount <stream-name> <endpoint-url>\\n\" +\n+              \"|    <stream-name> is the name of the Kinesis stream\\n\" +\n+              \"|    <endpoint-url> is the endpoint of the Kinesis service\\n\" +\n+              \"|                   (e.g. https://kinesis.us-east-1.amazonaws.com)\\n\");\n+          System.exit(1);\n+        }\n+\n+        StreamingExamples.setStreamingLogLevels();\n+\n+        /* Populate the appropriate variables from the given args */\n+        String streamName = args[0];\n+        String endpointUrl = args[1];\n+        /* Set the batch interval to a fixed 2000 millis (2 seconds) */\n+        Duration batchInterval = new Duration(2000);\n+\n+        /* Create a Kinesis client in order to determine the number of shards for the given stream */\n+        AmazonKinesisClient kinesisClient = new AmazonKinesisClient(\n+                new DefaultAWSCredentialsProviderChain());\n+        kinesisClient.setEndpoint(endpointUrl);\n+\n+        /* Determine the number of shards from the stream */\n+        int numShards = kinesisClient.describeStream(streamName)\n+                .getStreamDescription().getShards().size();\n+\n+        /* In this example, we're going to create 1 Kinesis Worker/Receiver/DStream for each shard */ \n+        int numStreams = numShards;\n+\n+        /* Must add 1 more thread than the number of receivers or the output won't show properly from the driver */\n+        int numSparkThreads = numStreams + 1;\n+\n+        /* Setup the Spark config. */\n+        SparkConf sparkConfig = new SparkConf().setAppName(\"KinesisWordCount\").setMaster(\n+                \"local[\" + numSparkThreads + \"]\");\n+\n+        /* Kinesis checkpoint interval.  Same as batchInterval for this example. */\n+        Duration checkpointInterval = batchInterval;\n+\n+        /* Setup the StreamingContext */\n+        JavaStreamingContext jssc = new JavaStreamingContext(sparkConfig, batchInterval);\n+\n+        /* Setup the checkpoint directory used by Spark Streaming */\n+        jssc.checkpoint(\"/tmp/checkpoint\");\n+\n+        /* Create the same number of Kinesis DStreams/Receivers as Kinesis stream's shards */\n+        List<JavaDStream<byte[]>> streamsList = new ArrayList<JavaDStream<byte[]>>(numStreams);\n+        for (int i = 0; i < numStreams; i++) {\n+        \tstreamsList.add(\n+                KinesisUtils.createStream(jssc, streamName, endpointUrl, checkpointInterval, \n+                InitialPositionInStream.LATEST, StorageLevel.MEMORY_AND_DISK_2())\n+            );\n+        }\n+\n+        /* Union all the streams if there is more than 1 stream */\n+        JavaDStream<byte[]> unionStreams;\n+        if (streamsList.size() > 1) {\n+            unionStreams = jssc.union(streamsList.get(0), streamsList.subList(1, streamsList.size()));\n+        } else {\n+            /* Otherwise, just use the 1 stream */\n+            unionStreams = streamsList.get(0);\n+        }\n+\n+        /*\n+         * Split each line of the union'd DStreams into multiple words using flatMap to produce the collection.\n+         * Convert lines of byte[] to multiple Strings by first converting to String, then splitting on WORD_SEPARATOR.\n+         */\n+        JavaDStream<String> words = unionStreams.flatMap(new FlatMapFunction<byte[], String>() {\n+                @Override\n+                public Iterable<String> call(byte[] line) {\n+                    return Lists.newArrayList(WORD_SEPARATOR.split(new String(line)));\n+                }\n+            });\n+\n+        /* Map each word to a (word, 1) tuple, then reduce/aggregate by key. */\n+        JavaPairDStream<String, Integer> wordCounts = words.mapToPair(\n+            new PairFunction<String, String, Integer>() {\n+                @Override\n+                public Tuple2<String, Integer> call(String s) {\n+                    return new Tuple2<String, Integer>(s, 1);\n+                }\n+            }).reduceByKey(new Function2<Integer, Integer, Integer>() {\n+                @Override\n+                public Integer call(Integer i1, Integer i2) {\n+                  return i1 + i2;\n+                }\n+            });\n+\n+        /* Print the first 10 wordCounts by key */"
  }],
  "prId": 1434
}]