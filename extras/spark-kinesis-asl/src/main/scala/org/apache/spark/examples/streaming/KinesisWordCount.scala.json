[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Just like the comment in JavaKinesisWordCount, please make the batch duration constant, 2 seconds, and other intervals constant as well. \n",
    "commit": "47745816b21d7d2255a98283e3055a5a2a397a27",
    "createdAt": "2014-07-30T06:37:52Z",
    "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming\n+\n+import java.nio.ByteBuffer\n+import org.apache.log4j.Level\n+import org.apache.log4j.Logger\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.streaming.Milliseconds\n+import org.apache.spark.streaming.StreamingContext\n+import org.apache.spark.streaming.StreamingContext.toPairDStreamFunctions\n+import org.apache.spark.streaming.kinesis.KinesisStringRecordSerializer\n+import org.apache.spark.streaming.kinesis.KinesisUtils\n+import com.amazonaws.auth.DefaultAWSCredentialsProviderChain\n+import com.amazonaws.services.kinesis.AmazonKinesisClient\n+import com.amazonaws.services.kinesis.model.PutRecordRequest\n+import scala.util.Random\n+import com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.dstream.ReceiverInputDStream\n+import org.apache.spark.streaming.dstream.DStream\n+\n+/**\n+ * Kinesis Spark Streaming WordCount example.\n+ *\n+ * See http://spark.apache.org/docs/latest/streaming-programming-guide.html for more details on the Kinesis Spark Streaming integration.\n+ *\n+ * This example spins up 1 Kinesis Worker (Spark Streaming Receivers) per shard of the given stream.\n+ * It then starts pulling from the tip of the given <stream-name> and <endpoint-url> at the given <batch-interval>.\n+ * Because we're pulling from the tip (InitialPositionInStream.LATEST), only new stream data will be picked up after the KinesisReceiver starts.\n+ * This could lead to missed records if data is added to the stream while no KinesisReceivers are running.\n+ * In production, you'll want to switch to InitialPositionInStream.TRIM_HORIZON which will read up to 24 hours (Kinesis limit) of previous stream data \n+ *  depending on the checkpoint frequency.\n+ *\n+ * InitialPositionInStream.TRIM_HORIZON may lead to duplicate processing of records depending on the checkpoint frequency.\n+ * Record processing should be idempotent when possible.\n+ *\n+ * This code uses the DefaultAWSCredentialsProviderChain and searches for credentials in the following order of precedence:\n+ * Environment Variables - AWS_ACCESS_KEY_ID and AWS_SECRET_KEY\n+ * Java System Properties - aws.accessKeyId and aws.secretKey\n+ * Credential profiles file - default location (~/.aws/credentials) shared by all AWS SDKs\n+ * Instance profile credentials - delivered through the Amazon EC2 metadata service\n+ *\n+ * Usage: KinesisWordCount <stream-name> <endpoint-url> <batch-interval>\n+ *   <stream-name> is the name of the Kinesis stream (ie. mySparkStream)\n+ *   <endpoint-url> is the endpoint of the Kinesis service (ie. https://kinesis.us-east-1.amazonaws.com)\n+ *   <batch-interval> is the batch interval in millis (ie. 1000ms)\n+ *\n+ * Example:\n+ *      $ export AWS_ACCESS_KEY_ID=<your-access-key>\n+ *      $ export AWS_SECRET_KEY=<your-secret-key>\n+ *    $ bin/run-kinesis-example \\\n+ *        org.apache.spark.examples.streaming.KinesisWordCount mySparkStream https://kinesis.us-east-1.amazonaws.com 100\n+ *\n+ * There is a companion helper class below called KinesisWordCountProducer which puts dummy data onto the Kinesis stream.\n+ * Usage instructions for KinesisWordCountProducer are provided in that class definition.\n+ */\n+object KinesisWordCount extends Logging {\n+  val WordSeparator = \" \"\n+\n+  def main(args: Array[String]) {\n+/**\n+ * Check that all required args were passed in.\n+ */\n+    if (args.length < 3) {\n+      System.err.println(\"Usage: KinesisWordCount <stream-name> <endpoint-url> <batch-interval>\")\n+      System.exit(1)\n+    }\n+\n+    /**\n+     * (This was lifted from the StreamingExamples.scala in order to avoid the dependency on the spark-examples artifact.)\n+     * Set reasonable logging levels for streaming if the user has not configured log4j.\n+     */\n+    val log4jInitialized = Logger.getRootLogger.getAllAppenders.hasMoreElements\n+    if (!log4jInitialized) {\n+      /** We first log something to initialize Spark's default logging, then we override the logging level. */\n+      logInfo(\"Setting log level to [INFO] for streaming example.\" +\n+        \" To override add a custom log4j.properties to the classpath.\")\n+\n+      Logger.getRootLogger().setLevel(Level.INFO)\n+      Logger.getLogger(\"org.apache.spark.examples.streaming\").setLevel(Level.DEBUG);\n+    }\n+\n+    /** Populate the appropriate variables from the given args */\n+    val Array(stream, endpoint, batchIntervalMillisStr) = args\n+    val batchIntervalMillis = batchIntervalMillisStr.toInt"
  }],
  "prId": 1434
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "This is great example with a lot of functionality but for a newbie trying to learn Spark Streaming + Kinesis, this example looks a little scary because of its size. All we want to demonstrate in this example is how to use KinesisUtils, not that you can use window operations and stuff. There are other examples for that. So I would prefer this example being very short and to the point, like KafkaWordCount. Can you please change it to something like that?\n",
    "commit": "47745816b21d7d2255a98283e3055a5a2a397a27",
    "createdAt": "2014-07-30T07:00:08Z",
    "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming\n+\n+import java.nio.ByteBuffer\n+import org.apache.log4j.Level\n+import org.apache.log4j.Logger\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.streaming.Milliseconds\n+import org.apache.spark.streaming.StreamingContext\n+import org.apache.spark.streaming.StreamingContext.toPairDStreamFunctions\n+import org.apache.spark.streaming.kinesis.KinesisStringRecordSerializer\n+import org.apache.spark.streaming.kinesis.KinesisUtils\n+import com.amazonaws.auth.DefaultAWSCredentialsProviderChain\n+import com.amazonaws.services.kinesis.AmazonKinesisClient\n+import com.amazonaws.services.kinesis.model.PutRecordRequest\n+import scala.util.Random\n+import com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.dstream.ReceiverInputDStream\n+import org.apache.spark.streaming.dstream.DStream\n+\n+/**\n+ * Kinesis Spark Streaming WordCount example.\n+ *\n+ * See http://spark.apache.org/docs/latest/streaming-programming-guide.html for more details on the Kinesis Spark Streaming integration.\n+ *\n+ * This example spins up 1 Kinesis Worker (Spark Streaming Receivers) per shard of the given stream.\n+ * It then starts pulling from the tip of the given <stream-name> and <endpoint-url> at the given <batch-interval>.\n+ * Because we're pulling from the tip (InitialPositionInStream.LATEST), only new stream data will be picked up after the KinesisReceiver starts.\n+ * This could lead to missed records if data is added to the stream while no KinesisReceivers are running.\n+ * In production, you'll want to switch to InitialPositionInStream.TRIM_HORIZON which will read up to 24 hours (Kinesis limit) of previous stream data \n+ *  depending on the checkpoint frequency.\n+ *\n+ * InitialPositionInStream.TRIM_HORIZON may lead to duplicate processing of records depending on the checkpoint frequency.\n+ * Record processing should be idempotent when possible.\n+ *\n+ * This code uses the DefaultAWSCredentialsProviderChain and searches for credentials in the following order of precedence:\n+ * Environment Variables - AWS_ACCESS_KEY_ID and AWS_SECRET_KEY\n+ * Java System Properties - aws.accessKeyId and aws.secretKey\n+ * Credential profiles file - default location (~/.aws/credentials) shared by all AWS SDKs\n+ * Instance profile credentials - delivered through the Amazon EC2 metadata service\n+ *\n+ * Usage: KinesisWordCount <stream-name> <endpoint-url> <batch-interval>\n+ *   <stream-name> is the name of the Kinesis stream (ie. mySparkStream)\n+ *   <endpoint-url> is the endpoint of the Kinesis service (ie. https://kinesis.us-east-1.amazonaws.com)\n+ *   <batch-interval> is the batch interval in millis (ie. 1000ms)\n+ *\n+ * Example:\n+ *      $ export AWS_ACCESS_KEY_ID=<your-access-key>\n+ *      $ export AWS_SECRET_KEY=<your-secret-key>\n+ *    $ bin/run-kinesis-example \\\n+ *        org.apache.spark.examples.streaming.KinesisWordCount mySparkStream https://kinesis.us-east-1.amazonaws.com 100\n+ *\n+ * There is a companion helper class below called KinesisWordCountProducer which puts dummy data onto the Kinesis stream.\n+ * Usage instructions for KinesisWordCountProducer are provided in that class definition.\n+ */\n+object KinesisWordCount extends Logging {\n+  val WordSeparator = \" \"\n+\n+  def main(args: Array[String]) {\n+/**\n+ * Check that all required args were passed in.\n+ */\n+    if (args.length < 3) {\n+      System.err.println(\"Usage: KinesisWordCount <stream-name> <endpoint-url> <batch-interval>\")\n+      System.exit(1)\n+    }\n+\n+    /**\n+     * (This was lifted from the StreamingExamples.scala in order to avoid the dependency on the spark-examples artifact.)\n+     * Set reasonable logging levels for streaming if the user has not configured log4j.\n+     */\n+    val log4jInitialized = Logger.getRootLogger.getAllAppenders.hasMoreElements\n+    if (!log4jInitialized) {\n+      /** We first log something to initialize Spark's default logging, then we override the logging level. */\n+      logInfo(\"Setting log level to [INFO] for streaming example.\" +\n+        \" To override add a custom log4j.properties to the classpath.\")\n+\n+      Logger.getRootLogger().setLevel(Level.INFO)\n+      Logger.getLogger(\"org.apache.spark.examples.streaming\").setLevel(Level.DEBUG);\n+    }\n+\n+    /** Populate the appropriate variables from the given args */\n+    val Array(stream, endpoint, batchIntervalMillisStr) = args\n+    val batchIntervalMillis = batchIntervalMillisStr.toInt\n+\n+    /** Create a Kinesis client in order to determine the number of shards for the given stream */\n+    val KinesisClient = new AmazonKinesisClient(new DefaultAWSCredentialsProviderChain());\n+\n+    /** Determine the number of shards from the stream */\n+    val numShards = KinesisClient.describeStream(stream).getStreamDescription().getShards().size()\n+\n+    /** In this example, we're going to create 1 Kinesis Worker/Receiver/DStreams for each stream shard */\n+    val numStreams = numShards\n+\n+    /** Must add 1 more thread than the number of receivers or the output won't show properly from the driver */\n+    val numSparkThreads = numStreams + 1\n+\n+    /** Set the app name */\n+    val app = \"KinesisWordCount\"\n+\n+    /** Setup the Spark config. */\n+    val sparkConfig = new SparkConf().setAppName(app).setMaster(s\"local[$numSparkThreads]\")\n+\n+    /**\n+     * Set the batch interval.\n+     * Records will be pulled from the Kinesis stream and stored as a single DStream within Spark every batch interval.\n+     */\n+    val batchInterval = Milliseconds(batchIntervalMillis)\n+\n+    /**\n+     * It's recommended that you perform a Spark checkpoint between 5 and 10 times the batch interval.\n+     * While this is the Spark checkpoint interval, we're going to use it for the Kinesis checkpoint interval, as well.\n+     */\n+    val checkpointInterval = batchInterval * 5\n+\n+    /** Setup the StreamingContext */\n+    val ssc = new StreamingContext(sparkConfig, batchInterval)\n+\n+    /** Setup the checkpoint directory used by Spark Streaming */\n+    ssc.checkpoint(\"/tmp/checkpoint\");\n+\n+    /** Create the same number of Kinesis Receivers/DStreams as stream shards, then union them all */\n+    var allStreams: DStream[Array[Byte]] = KinesisUtils.createStream(ssc, app, stream, endpoint, checkpointInterval.milliseconds,\n+      InitialPositionInStream.LATEST, StorageLevel.MEMORY_AND_DISK_2)\n+      /** Set the checkpoint interval */\n+    allStreams.checkpoint(checkpointInterval)\n+    for (i <- 1 until numStreams) {\n+      /** Create a new Receiver/DStream for each stream shard */\n+      val dStream = KinesisUtils.createStream(ssc, app, stream, endpoint, checkpointInterval.milliseconds,\n+        InitialPositionInStream.LATEST, StorageLevel.MEMORY_AND_DISK_2)\n+      /** Set the Spark checkpoint interval */\n+      dStream.checkpoint(checkpointInterval)\n+\n+      /** Union with the existing streams */\n+      allStreams = allStreams.union(dStream)\n+    }\n+\n+    /** This implementation uses the String-based KinesisRecordSerializer impl */\n+    val recordSerializer = new KinesisStringRecordSerializer()\n+\n+    /**\n+     * Sort and print the given dstream.\n+     * This is an Output Operation that will materialize the underlying DStream.\n+     * Everything up to this point is a lazy Transformation Operation.\n+     * \n+     * @param description of the dstream for logging purposes\n+     * @param dstream to sort and print\n+     */\n+    def sortAndPrint(description: String, dstream: DStream[(String,Int)]) = {\n+        dstream.foreachRDD((batch, endOfWindowTime) => {"
  }, {
    "author": {
      "login": "cfregly"
    },
    "body": "removed the window operation complexity\n",
    "commit": "47745816b21d7d2255a98283e3055a5a2a397a27",
    "createdAt": "2014-07-30T23:10:25Z",
    "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming\n+\n+import java.nio.ByteBuffer\n+import org.apache.log4j.Level\n+import org.apache.log4j.Logger\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.streaming.Milliseconds\n+import org.apache.spark.streaming.StreamingContext\n+import org.apache.spark.streaming.StreamingContext.toPairDStreamFunctions\n+import org.apache.spark.streaming.kinesis.KinesisStringRecordSerializer\n+import org.apache.spark.streaming.kinesis.KinesisUtils\n+import com.amazonaws.auth.DefaultAWSCredentialsProviderChain\n+import com.amazonaws.services.kinesis.AmazonKinesisClient\n+import com.amazonaws.services.kinesis.model.PutRecordRequest\n+import scala.util.Random\n+import com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.dstream.ReceiverInputDStream\n+import org.apache.spark.streaming.dstream.DStream\n+\n+/**\n+ * Kinesis Spark Streaming WordCount example.\n+ *\n+ * See http://spark.apache.org/docs/latest/streaming-programming-guide.html for more details on the Kinesis Spark Streaming integration.\n+ *\n+ * This example spins up 1 Kinesis Worker (Spark Streaming Receivers) per shard of the given stream.\n+ * It then starts pulling from the tip of the given <stream-name> and <endpoint-url> at the given <batch-interval>.\n+ * Because we're pulling from the tip (InitialPositionInStream.LATEST), only new stream data will be picked up after the KinesisReceiver starts.\n+ * This could lead to missed records if data is added to the stream while no KinesisReceivers are running.\n+ * In production, you'll want to switch to InitialPositionInStream.TRIM_HORIZON which will read up to 24 hours (Kinesis limit) of previous stream data \n+ *  depending on the checkpoint frequency.\n+ *\n+ * InitialPositionInStream.TRIM_HORIZON may lead to duplicate processing of records depending on the checkpoint frequency.\n+ * Record processing should be idempotent when possible.\n+ *\n+ * This code uses the DefaultAWSCredentialsProviderChain and searches for credentials in the following order of precedence:\n+ * Environment Variables - AWS_ACCESS_KEY_ID and AWS_SECRET_KEY\n+ * Java System Properties - aws.accessKeyId and aws.secretKey\n+ * Credential profiles file - default location (~/.aws/credentials) shared by all AWS SDKs\n+ * Instance profile credentials - delivered through the Amazon EC2 metadata service\n+ *\n+ * Usage: KinesisWordCount <stream-name> <endpoint-url> <batch-interval>\n+ *   <stream-name> is the name of the Kinesis stream (ie. mySparkStream)\n+ *   <endpoint-url> is the endpoint of the Kinesis service (ie. https://kinesis.us-east-1.amazonaws.com)\n+ *   <batch-interval> is the batch interval in millis (ie. 1000ms)\n+ *\n+ * Example:\n+ *      $ export AWS_ACCESS_KEY_ID=<your-access-key>\n+ *      $ export AWS_SECRET_KEY=<your-secret-key>\n+ *    $ bin/run-kinesis-example \\\n+ *        org.apache.spark.examples.streaming.KinesisWordCount mySparkStream https://kinesis.us-east-1.amazonaws.com 100\n+ *\n+ * There is a companion helper class below called KinesisWordCountProducer which puts dummy data onto the Kinesis stream.\n+ * Usage instructions for KinesisWordCountProducer are provided in that class definition.\n+ */\n+object KinesisWordCount extends Logging {\n+  val WordSeparator = \" \"\n+\n+  def main(args: Array[String]) {\n+/**\n+ * Check that all required args were passed in.\n+ */\n+    if (args.length < 3) {\n+      System.err.println(\"Usage: KinesisWordCount <stream-name> <endpoint-url> <batch-interval>\")\n+      System.exit(1)\n+    }\n+\n+    /**\n+     * (This was lifted from the StreamingExamples.scala in order to avoid the dependency on the spark-examples artifact.)\n+     * Set reasonable logging levels for streaming if the user has not configured log4j.\n+     */\n+    val log4jInitialized = Logger.getRootLogger.getAllAppenders.hasMoreElements\n+    if (!log4jInitialized) {\n+      /** We first log something to initialize Spark's default logging, then we override the logging level. */\n+      logInfo(\"Setting log level to [INFO] for streaming example.\" +\n+        \" To override add a custom log4j.properties to the classpath.\")\n+\n+      Logger.getRootLogger().setLevel(Level.INFO)\n+      Logger.getLogger(\"org.apache.spark.examples.streaming\").setLevel(Level.DEBUG);\n+    }\n+\n+    /** Populate the appropriate variables from the given args */\n+    val Array(stream, endpoint, batchIntervalMillisStr) = args\n+    val batchIntervalMillis = batchIntervalMillisStr.toInt\n+\n+    /** Create a Kinesis client in order to determine the number of shards for the given stream */\n+    val KinesisClient = new AmazonKinesisClient(new DefaultAWSCredentialsProviderChain());\n+\n+    /** Determine the number of shards from the stream */\n+    val numShards = KinesisClient.describeStream(stream).getStreamDescription().getShards().size()\n+\n+    /** In this example, we're going to create 1 Kinesis Worker/Receiver/DStreams for each stream shard */\n+    val numStreams = numShards\n+\n+    /** Must add 1 more thread than the number of receivers or the output won't show properly from the driver */\n+    val numSparkThreads = numStreams + 1\n+\n+    /** Set the app name */\n+    val app = \"KinesisWordCount\"\n+\n+    /** Setup the Spark config. */\n+    val sparkConfig = new SparkConf().setAppName(app).setMaster(s\"local[$numSparkThreads]\")\n+\n+    /**\n+     * Set the batch interval.\n+     * Records will be pulled from the Kinesis stream and stored as a single DStream within Spark every batch interval.\n+     */\n+    val batchInterval = Milliseconds(batchIntervalMillis)\n+\n+    /**\n+     * It's recommended that you perform a Spark checkpoint between 5 and 10 times the batch interval.\n+     * While this is the Spark checkpoint interval, we're going to use it for the Kinesis checkpoint interval, as well.\n+     */\n+    val checkpointInterval = batchInterval * 5\n+\n+    /** Setup the StreamingContext */\n+    val ssc = new StreamingContext(sparkConfig, batchInterval)\n+\n+    /** Setup the checkpoint directory used by Spark Streaming */\n+    ssc.checkpoint(\"/tmp/checkpoint\");\n+\n+    /** Create the same number of Kinesis Receivers/DStreams as stream shards, then union them all */\n+    var allStreams: DStream[Array[Byte]] = KinesisUtils.createStream(ssc, app, stream, endpoint, checkpointInterval.milliseconds,\n+      InitialPositionInStream.LATEST, StorageLevel.MEMORY_AND_DISK_2)\n+      /** Set the checkpoint interval */\n+    allStreams.checkpoint(checkpointInterval)\n+    for (i <- 1 until numStreams) {\n+      /** Create a new Receiver/DStream for each stream shard */\n+      val dStream = KinesisUtils.createStream(ssc, app, stream, endpoint, checkpointInterval.milliseconds,\n+        InitialPositionInStream.LATEST, StorageLevel.MEMORY_AND_DISK_2)\n+      /** Set the Spark checkpoint interval */\n+      dStream.checkpoint(checkpointInterval)\n+\n+      /** Union with the existing streams */\n+      allStreams = allStreams.union(dStream)\n+    }\n+\n+    /** This implementation uses the String-based KinesisRecordSerializer impl */\n+    val recordSerializer = new KinesisStringRecordSerializer()\n+\n+    /**\n+     * Sort and print the given dstream.\n+     * This is an Output Operation that will materialize the underlying DStream.\n+     * Everything up to this point is a lazy Transformation Operation.\n+     * \n+     * @param description of the dstream for logging purposes\n+     * @param dstream to sort and print\n+     */\n+    def sortAndPrint(description: String, dstream: DStream[(String,Int)]) = {\n+        dstream.foreachRDD((batch, endOfWindowTime) => {"
  }],
  "prId": 1434
}]