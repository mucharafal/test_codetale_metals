[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Better line break is ...\n\n... allStreams.flatMap(   // new line here\n",
    "commit": "47745816b21d7d2255a98283e3055a5a2a397a27",
    "createdAt": "2014-07-29T03:33:35Z",
    "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.List;\n+import java.util.regex.Pattern;\n+\n+import org.apache.log4j.Level;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.Function;\n+import org.apache.spark.api.java.function.Function2;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+import org.apache.spark.streaming.Duration;\n+import org.apache.spark.streaming.Milliseconds;\n+import org.apache.spark.streaming.api.java.JavaDStream;\n+import org.apache.spark.streaming.api.java.JavaPairDStream;\n+import org.apache.spark.streaming.api.java.JavaStreamingContext;\n+import org.apache.spark.streaming.dstream.DStream;\n+import org.apache.spark.streaming.kinesis.KinesisRecordSerializer;\n+import org.apache.spark.streaming.kinesis.KinesisStringRecordSerializer;\n+import org.apache.spark.streaming.kinesis.KinesisUtils;\n+\n+import scala.Tuple2;\n+\n+import com.amazonaws.auth.DefaultAWSCredentialsProviderChain;\n+import com.amazonaws.services.kinesis.AmazonKinesisClient;\n+import com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream;\n+import com.google.common.base.Optional;\n+import com.google.common.collect.Lists;\n+\n+/**\n+ * Java-friendly Kinesis Spark Streaming WordCount example\n+ *\n+ * See http://spark.apache.org/docs/latest/streaming-programming-guide.html for more details on the Kinesis Spark Streaming integration.\n+ *\n+ * This example spins up 1 Kinesis Worker (Spark Streaming Receivers) per shard of the given stream.\n+ * It then starts pulling from the tip of the given <stream-name> and <endpoint-url> at the given <batch-interval>.\n+ * Because we're pulling from the tip (InitialPositionInStream.LATEST), only new stream data will be picked up after the KinesisReceiver starts.\n+ * This could lead to missed records if data is added to the stream while no KinesisReceivers are running.\n+ * In production, you'll want to switch to InitialPositionInStream.TRIM_HORIZON which will read up to 24 hours (Kinesis limit) of previous stream data \n+ *  depending on the checkpoint frequency.\n+ * InitialPositionInStream.TRIM_HORIZON may lead to duplicate processing of records depending on the checkpoint frequency.\n+ * Record processing should be idempotent when possible.\n+ *\n+ * This code uses the DefaultAWSCredentialsProviderChain and searches for credentials in the following order of precedence: \n+ *         Environment Variables - AWS_ACCESS_KEY_ID and AWS_SECRET_KEY\n+ *         Java System Properties - aws.accessKeyId and aws.secretKey\n+ *         Credential profiles file - default location (~/.aws/credentials) shared by all AWS SDKs\n+ *         Instance profile credentials - delivered through the Amazon EC2 metadata service\n+ *\n+ * Usage: JavaKinesisWordCount <stream-name> <endpoint-url> <batch-interval>\n+ *         <stream-name> is the name of the Kinesis stream (ie. mySparkStream)\n+ *         <endpoint-url> is the endpoint of the Kinesis service (ie. https://kinesis.us-east-1.amazonaws.com)\n+ *         <batch-interval> is the batch interval in milliseconds (ie. 1000ms)\n+ *\n+ * Example:\n+ *      $ export AWS_ACCESS_KEY_ID=<your-access-key>\n+ *      $ export AWS_SECRET_KEY=<your-secret-key>\n+ *        $ bin/run-kinesis-example  \\\n+ *            org.apache.spark.examples.streaming.JavaKinesisWordCount mySparkStream https://kinesis.us-east-1.amazonaws.com 1000\n+ *\n+ * There is a companion helper class called KinesisWordCountProducer which puts dummy data onto the Kinesis stream. \n+ * Usage instructions for KinesisWordCountProducer are provided in the class definition.\n+ */\n+public final class JavaKinesisWordCount {\n+    private static final Pattern WORD_SEPARATOR = Pattern.compile(\" \");\n+    private static final Logger logger = Logger.getLogger(JavaKinesisWordCount.class);\n+\n+    /**\n+     * Make the constructor private to enforce singleton\n+     */\n+    private JavaKinesisWordCount() {\n+    }\n+\n+    public static void main(String[] args) {\n+        /**\n+         * Check that all required args were passed in.\n+         */\n+        if (args.length < 3) {\n+            System.err.println(\"Usage: JavaKinesisWordCount <stream-name> <kinesis-endpoint-url> <batch-interval>\");\n+            System.exit(1);\n+        }\n+\n+        /**\n+         * (This was lifted from the StreamingExamples.scala in order to avoid the dependency on the spark-examples artifact.)\n+         * Set reasonable logging levels for streaming if the user has not configured log4j.\n+         */\n+        boolean log4jInitialized = Logger.getRootLogger().getAllAppenders()\n+                .hasMoreElements();\n+        if (!log4jInitialized) {\n+            /** We first log something to initialize Spark's default logging, then we override the logging level. */\n+            Logger.getRootLogger()\n+                    .info(\"Setting log level to [ERROR] for streaming example.\"\n+                            + \" To override add a custom log4j.properties to the classpath.\");\n+            Logger.getRootLogger().setLevel(Level.ERROR);\n+            Logger.getLogger(\"org.apache.spark.examples.streaming\").setLevel(Level.DEBUG);\n+        }\n+\n+        /** Populate the appropriate variables from the given args */\n+        String stream = args[0];\n+        String endpoint = args[1];\n+        Integer batchIntervalMillis = Integer.valueOf(args[2]);\n+\n+        /** Create a Kinesis client in order to determine the number of shards for the given stream */\n+        AmazonKinesisClient KinesisClient = new AmazonKinesisClient(\n+                new DefaultAWSCredentialsProviderChain());\n+\n+        /** Determine the number of shards from the stream */\n+        int numShards = KinesisClient.describeStream(stream)\n+                .getStreamDescription().getShards().size();\n+\n+        /** In this example, we're going to create 1 Kinesis Worker/Receiver/DStreams for each stream shard */ \n+        int numStreams = numShards;\n+\n+        /** Must add 1 more thread than the number of receivers or the output won't show properly from the driver */\n+        int numSparkThreads = numStreams + 1;\n+\n+        /** Set the app name */\n+        String app = \"KinesisWordCount\";\n+\n+        /** Setup the Spark config. */\n+        SparkConf sparkConfig = new SparkConf().setAppName(app).setMaster(\n+                \"local[\" + numSparkThreads + \"]\");\n+\n+        /**\n+         * Set the batch interval.\n+         * Records will be pulled from the Kinesis stream and stored as a single DStream within Spark every batch interval.\n+         */\n+        Duration batchInterval = Milliseconds.apply(batchIntervalMillis);\n+\n+        /**\n+         * It's recommended that you perform a Spark checkpoint between 5 and 10 times the batch interval. \n+         * While this is the Spark checkpoint interval, we're going to use it for the Kinesis checkpoint interval, as well.\n+         */\n+        Duration checkpointInterval = batchInterval.$times(5);\n+\n+        /** Setup the StreamingContext */\n+        JavaStreamingContext jssc = new JavaStreamingContext(sparkConfig, batchInterval);\n+\n+        /** Setup the checkpoint directory used by Spark Streaming */\n+        jssc.checkpoint(\"/tmp/checkpoint\");\n+\n+        /** Create the same number of Kinesis Receivers/DStreams as stream shards, then union them all */\n+        JavaDStream<byte[]> allStreams = KinesisUtils\n+                .createJavaStream(jssc, app, stream, endpoint, checkpointInterval.milliseconds(), \n+                                    InitialPositionInStream.LATEST, StorageLevel.MEMORY_AND_DISK_2());\n+        /** Set the checkpoint interval */\n+        allStreams.checkpoint(checkpointInterval);\n+        for (int i = 1; i < numStreams; i++) {\n+            /** Create a new Receiver/DStream for each stream shard */\n+            JavaDStream<byte[]> dStream = KinesisUtils\n+                    .createJavaStream(jssc, app, stream, endpoint, checkpointInterval.milliseconds(), \n+                                        InitialPositionInStream.LATEST, StorageLevel.MEMORY_AND_DISK_2());            \n+            /** Set the Spark checkpoint interval */\n+            dStream.checkpoint(checkpointInterval);\n+\n+            /** Union with the existing streams */\n+            allStreams = allStreams.union(dStream);\n+        }\n+\n+        /** This implementation uses the String-based KinesisRecordSerializer impl */\n+        final KinesisRecordSerializer<String> recordSerializer = new KinesisStringRecordSerializer();\n+\n+        /**\n+          * Split each line of the union'd DStreams into multiple words using flatMap to produce the collection.\n+          * Convert lines of byte[] to multiple Strings by first converting to String, then splitting on WORD_SEPARATOR\n+          * We're caching the result here so that we can use it later without having to re-materialize the underlying RDDs.\n+          */\n+        JavaDStream<String> words = allStreams\n+                .flatMap(new FlatMapFunction<byte[], String>() {\n+                    /**"
  }],
  "prId": 1434
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Can you make the batch interval a fixed duration? This stays consistent with most other streaming examples. 2 seconds should be good enough. \n",
    "commit": "47745816b21d7d2255a98283e3055a5a2a397a27",
    "createdAt": "2014-07-30T06:07:51Z",
    "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.List;\n+import java.util.regex.Pattern;\n+\n+import org.apache.log4j.Level;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.Function;\n+import org.apache.spark.api.java.function.Function2;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+import org.apache.spark.streaming.Duration;\n+import org.apache.spark.streaming.Milliseconds;\n+import org.apache.spark.streaming.api.java.JavaDStream;\n+import org.apache.spark.streaming.api.java.JavaPairDStream;\n+import org.apache.spark.streaming.api.java.JavaStreamingContext;\n+import org.apache.spark.streaming.dstream.DStream;\n+import org.apache.spark.streaming.kinesis.KinesisRecordSerializer;\n+import org.apache.spark.streaming.kinesis.KinesisStringRecordSerializer;\n+import org.apache.spark.streaming.kinesis.KinesisUtils;\n+\n+import scala.Tuple2;\n+\n+import com.amazonaws.auth.DefaultAWSCredentialsProviderChain;\n+import com.amazonaws.services.kinesis.AmazonKinesisClient;\n+import com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream;\n+import com.google.common.base.Optional;\n+import com.google.common.collect.Lists;\n+\n+/**\n+ * Java-friendly Kinesis Spark Streaming WordCount example\n+ *\n+ * See http://spark.apache.org/docs/latest/streaming-programming-guide.html for more details on the Kinesis Spark Streaming integration.\n+ *\n+ * This example spins up 1 Kinesis Worker (Spark Streaming Receivers) per shard of the given stream.\n+ * It then starts pulling from the tip of the given <stream-name> and <endpoint-url> at the given <batch-interval>.\n+ * Because we're pulling from the tip (InitialPositionInStream.LATEST), only new stream data will be picked up after the KinesisReceiver starts.\n+ * This could lead to missed records if data is added to the stream while no KinesisReceivers are running.\n+ * In production, you'll want to switch to InitialPositionInStream.TRIM_HORIZON which will read up to 24 hours (Kinesis limit) of previous stream data \n+ *  depending on the checkpoint frequency.\n+ * InitialPositionInStream.TRIM_HORIZON may lead to duplicate processing of records depending on the checkpoint frequency.\n+ * Record processing should be idempotent when possible.\n+ *\n+ * This code uses the DefaultAWSCredentialsProviderChain and searches for credentials in the following order of precedence: \n+ *         Environment Variables - AWS_ACCESS_KEY_ID and AWS_SECRET_KEY\n+ *         Java System Properties - aws.accessKeyId and aws.secretKey\n+ *         Credential profiles file - default location (~/.aws/credentials) shared by all AWS SDKs\n+ *         Instance profile credentials - delivered through the Amazon EC2 metadata service\n+ *\n+ * Usage: JavaKinesisWordCount <stream-name> <endpoint-url> <batch-interval>\n+ *         <stream-name> is the name of the Kinesis stream (ie. mySparkStream)\n+ *         <endpoint-url> is the endpoint of the Kinesis service (ie. https://kinesis.us-east-1.amazonaws.com)\n+ *         <batch-interval> is the batch interval in milliseconds (ie. 1000ms)\n+ *\n+ * Example:\n+ *      $ export AWS_ACCESS_KEY_ID=<your-access-key>\n+ *      $ export AWS_SECRET_KEY=<your-secret-key>\n+ *        $ bin/run-kinesis-example  \\\n+ *            org.apache.spark.examples.streaming.JavaKinesisWordCount mySparkStream https://kinesis.us-east-1.amazonaws.com 1000\n+ *\n+ * There is a companion helper class called KinesisWordCountProducer which puts dummy data onto the Kinesis stream. \n+ * Usage instructions for KinesisWordCountProducer are provided in the class definition.\n+ */\n+public final class JavaKinesisWordCount {\n+    private static final Pattern WORD_SEPARATOR = Pattern.compile(\" \");\n+    private static final Logger logger = Logger.getLogger(JavaKinesisWordCount.class);\n+\n+    /**\n+     * Make the constructor private to enforce singleton\n+     */\n+    private JavaKinesisWordCount() {\n+    }\n+\n+    public static void main(String[] args) {\n+        /**\n+         * Check that all required args were passed in.\n+         */\n+        if (args.length < 3) {\n+            System.err.println(\"Usage: JavaKinesisWordCount <stream-name> <kinesis-endpoint-url> <batch-interval>\");\n+            System.exit(1);\n+        }\n+\n+        /**\n+         * (This was lifted from the StreamingExamples.scala in order to avoid the dependency on the spark-examples artifact.)\n+         * Set reasonable logging levels for streaming if the user has not configured log4j.\n+         */\n+        boolean log4jInitialized = Logger.getRootLogger().getAllAppenders()\n+                .hasMoreElements();\n+        if (!log4jInitialized) {\n+            /** We first log something to initialize Spark's default logging, then we override the logging level. */\n+            Logger.getRootLogger()\n+                    .info(\"Setting log level to [ERROR] for streaming example.\"\n+                            + \" To override add a custom log4j.properties to the classpath.\");\n+            Logger.getRootLogger().setLevel(Level.ERROR);\n+            Logger.getLogger(\"org.apache.spark.examples.streaming\").setLevel(Level.DEBUG);\n+        }\n+\n+        /** Populate the appropriate variables from the given args */\n+        String stream = args[0];\n+        String endpoint = args[1];\n+        Integer batchIntervalMillis = Integer.valueOf(args[2]);"
  }, {
    "author": {
      "login": "cfregly"
    },
    "body": "fixed at 2 seconds.  i kept the dependent intervals as multiples of this batch interval because i think this is important to enforce.  i'm no longer relying on ther $times() method.\n",
    "commit": "47745816b21d7d2255a98283e3055a5a2a397a27",
    "createdAt": "2014-07-30T23:10:04Z",
    "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.List;\n+import java.util.regex.Pattern;\n+\n+import org.apache.log4j.Level;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.Function;\n+import org.apache.spark.api.java.function.Function2;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+import org.apache.spark.streaming.Duration;\n+import org.apache.spark.streaming.Milliseconds;\n+import org.apache.spark.streaming.api.java.JavaDStream;\n+import org.apache.spark.streaming.api.java.JavaPairDStream;\n+import org.apache.spark.streaming.api.java.JavaStreamingContext;\n+import org.apache.spark.streaming.dstream.DStream;\n+import org.apache.spark.streaming.kinesis.KinesisRecordSerializer;\n+import org.apache.spark.streaming.kinesis.KinesisStringRecordSerializer;\n+import org.apache.spark.streaming.kinesis.KinesisUtils;\n+\n+import scala.Tuple2;\n+\n+import com.amazonaws.auth.DefaultAWSCredentialsProviderChain;\n+import com.amazonaws.services.kinesis.AmazonKinesisClient;\n+import com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream;\n+import com.google.common.base.Optional;\n+import com.google.common.collect.Lists;\n+\n+/**\n+ * Java-friendly Kinesis Spark Streaming WordCount example\n+ *\n+ * See http://spark.apache.org/docs/latest/streaming-programming-guide.html for more details on the Kinesis Spark Streaming integration.\n+ *\n+ * This example spins up 1 Kinesis Worker (Spark Streaming Receivers) per shard of the given stream.\n+ * It then starts pulling from the tip of the given <stream-name> and <endpoint-url> at the given <batch-interval>.\n+ * Because we're pulling from the tip (InitialPositionInStream.LATEST), only new stream data will be picked up after the KinesisReceiver starts.\n+ * This could lead to missed records if data is added to the stream while no KinesisReceivers are running.\n+ * In production, you'll want to switch to InitialPositionInStream.TRIM_HORIZON which will read up to 24 hours (Kinesis limit) of previous stream data \n+ *  depending on the checkpoint frequency.\n+ * InitialPositionInStream.TRIM_HORIZON may lead to duplicate processing of records depending on the checkpoint frequency.\n+ * Record processing should be idempotent when possible.\n+ *\n+ * This code uses the DefaultAWSCredentialsProviderChain and searches for credentials in the following order of precedence: \n+ *         Environment Variables - AWS_ACCESS_KEY_ID and AWS_SECRET_KEY\n+ *         Java System Properties - aws.accessKeyId and aws.secretKey\n+ *         Credential profiles file - default location (~/.aws/credentials) shared by all AWS SDKs\n+ *         Instance profile credentials - delivered through the Amazon EC2 metadata service\n+ *\n+ * Usage: JavaKinesisWordCount <stream-name> <endpoint-url> <batch-interval>\n+ *         <stream-name> is the name of the Kinesis stream (ie. mySparkStream)\n+ *         <endpoint-url> is the endpoint of the Kinesis service (ie. https://kinesis.us-east-1.amazonaws.com)\n+ *         <batch-interval> is the batch interval in milliseconds (ie. 1000ms)\n+ *\n+ * Example:\n+ *      $ export AWS_ACCESS_KEY_ID=<your-access-key>\n+ *      $ export AWS_SECRET_KEY=<your-secret-key>\n+ *        $ bin/run-kinesis-example  \\\n+ *            org.apache.spark.examples.streaming.JavaKinesisWordCount mySparkStream https://kinesis.us-east-1.amazonaws.com 1000\n+ *\n+ * There is a companion helper class called KinesisWordCountProducer which puts dummy data onto the Kinesis stream. \n+ * Usage instructions for KinesisWordCountProducer are provided in the class definition.\n+ */\n+public final class JavaKinesisWordCount {\n+    private static final Pattern WORD_SEPARATOR = Pattern.compile(\" \");\n+    private static final Logger logger = Logger.getLogger(JavaKinesisWordCount.class);\n+\n+    /**\n+     * Make the constructor private to enforce singleton\n+     */\n+    private JavaKinesisWordCount() {\n+    }\n+\n+    public static void main(String[] args) {\n+        /**\n+         * Check that all required args were passed in.\n+         */\n+        if (args.length < 3) {\n+            System.err.println(\"Usage: JavaKinesisWordCount <stream-name> <kinesis-endpoint-url> <batch-interval>\");\n+            System.exit(1);\n+        }\n+\n+        /**\n+         * (This was lifted from the StreamingExamples.scala in order to avoid the dependency on the spark-examples artifact.)\n+         * Set reasonable logging levels for streaming if the user has not configured log4j.\n+         */\n+        boolean log4jInitialized = Logger.getRootLogger().getAllAppenders()\n+                .hasMoreElements();\n+        if (!log4jInitialized) {\n+            /** We first log something to initialize Spark's default logging, then we override the logging level. */\n+            Logger.getRootLogger()\n+                    .info(\"Setting log level to [ERROR] for streaming example.\"\n+                            + \" To override add a custom log4j.properties to the classpath.\");\n+            Logger.getRootLogger().setLevel(Level.ERROR);\n+            Logger.getLogger(\"org.apache.spark.examples.streaming\").setLevel(Level.DEBUG);\n+        }\n+\n+        /** Populate the appropriate variables from the given args */\n+        String stream = args[0];\n+        String endpoint = args[1];\n+        Integer batchIntervalMillis = Integer.valueOf(args[2]);"
  }],
  "prId": 1434
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Please do not use $times(), its not a proper exposed function in  Duration class, that should be used in Java. If the batch duration is constant 2 seconds, then the other examples can be set as constants.\n",
    "commit": "47745816b21d7d2255a98283e3055a5a2a397a27",
    "createdAt": "2014-07-30T06:13:23Z",
    "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.List;\n+import java.util.regex.Pattern;\n+\n+import org.apache.log4j.Level;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.Function;\n+import org.apache.spark.api.java.function.Function2;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+import org.apache.spark.streaming.Duration;\n+import org.apache.spark.streaming.Milliseconds;\n+import org.apache.spark.streaming.api.java.JavaDStream;\n+import org.apache.spark.streaming.api.java.JavaPairDStream;\n+import org.apache.spark.streaming.api.java.JavaStreamingContext;\n+import org.apache.spark.streaming.dstream.DStream;\n+import org.apache.spark.streaming.kinesis.KinesisRecordSerializer;\n+import org.apache.spark.streaming.kinesis.KinesisStringRecordSerializer;\n+import org.apache.spark.streaming.kinesis.KinesisUtils;\n+\n+import scala.Tuple2;\n+\n+import com.amazonaws.auth.DefaultAWSCredentialsProviderChain;\n+import com.amazonaws.services.kinesis.AmazonKinesisClient;\n+import com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream;\n+import com.google.common.base.Optional;\n+import com.google.common.collect.Lists;\n+\n+/**\n+ * Java-friendly Kinesis Spark Streaming WordCount example\n+ *\n+ * See http://spark.apache.org/docs/latest/streaming-programming-guide.html for more details on the Kinesis Spark Streaming integration.\n+ *\n+ * This example spins up 1 Kinesis Worker (Spark Streaming Receivers) per shard of the given stream.\n+ * It then starts pulling from the tip of the given <stream-name> and <endpoint-url> at the given <batch-interval>.\n+ * Because we're pulling from the tip (InitialPositionInStream.LATEST), only new stream data will be picked up after the KinesisReceiver starts.\n+ * This could lead to missed records if data is added to the stream while no KinesisReceivers are running.\n+ * In production, you'll want to switch to InitialPositionInStream.TRIM_HORIZON which will read up to 24 hours (Kinesis limit) of previous stream data \n+ *  depending on the checkpoint frequency.\n+ * InitialPositionInStream.TRIM_HORIZON may lead to duplicate processing of records depending on the checkpoint frequency.\n+ * Record processing should be idempotent when possible.\n+ *\n+ * This code uses the DefaultAWSCredentialsProviderChain and searches for credentials in the following order of precedence: \n+ *         Environment Variables - AWS_ACCESS_KEY_ID and AWS_SECRET_KEY\n+ *         Java System Properties - aws.accessKeyId and aws.secretKey\n+ *         Credential profiles file - default location (~/.aws/credentials) shared by all AWS SDKs\n+ *         Instance profile credentials - delivered through the Amazon EC2 metadata service\n+ *\n+ * Usage: JavaKinesisWordCount <stream-name> <endpoint-url> <batch-interval>\n+ *         <stream-name> is the name of the Kinesis stream (ie. mySparkStream)\n+ *         <endpoint-url> is the endpoint of the Kinesis service (ie. https://kinesis.us-east-1.amazonaws.com)\n+ *         <batch-interval> is the batch interval in milliseconds (ie. 1000ms)\n+ *\n+ * Example:\n+ *      $ export AWS_ACCESS_KEY_ID=<your-access-key>\n+ *      $ export AWS_SECRET_KEY=<your-secret-key>\n+ *        $ bin/run-kinesis-example  \\\n+ *            org.apache.spark.examples.streaming.JavaKinesisWordCount mySparkStream https://kinesis.us-east-1.amazonaws.com 1000\n+ *\n+ * There is a companion helper class called KinesisWordCountProducer which puts dummy data onto the Kinesis stream. \n+ * Usage instructions for KinesisWordCountProducer are provided in the class definition.\n+ */\n+public final class JavaKinesisWordCount {\n+    private static final Pattern WORD_SEPARATOR = Pattern.compile(\" \");\n+    private static final Logger logger = Logger.getLogger(JavaKinesisWordCount.class);\n+\n+    /**\n+     * Make the constructor private to enforce singleton\n+     */\n+    private JavaKinesisWordCount() {\n+    }\n+\n+    public static void main(String[] args) {\n+        /**\n+         * Check that all required args were passed in.\n+         */\n+        if (args.length < 3) {\n+            System.err.println(\"Usage: JavaKinesisWordCount <stream-name> <kinesis-endpoint-url> <batch-interval>\");\n+            System.exit(1);\n+        }\n+\n+        /**\n+         * (This was lifted from the StreamingExamples.scala in order to avoid the dependency on the spark-examples artifact.)\n+         * Set reasonable logging levels for streaming if the user has not configured log4j.\n+         */\n+        boolean log4jInitialized = Logger.getRootLogger().getAllAppenders()\n+                .hasMoreElements();\n+        if (!log4jInitialized) {\n+            /** We first log something to initialize Spark's default logging, then we override the logging level. */\n+            Logger.getRootLogger()\n+                    .info(\"Setting log level to [ERROR] for streaming example.\"\n+                            + \" To override add a custom log4j.properties to the classpath.\");\n+            Logger.getRootLogger().setLevel(Level.ERROR);\n+            Logger.getLogger(\"org.apache.spark.examples.streaming\").setLevel(Level.DEBUG);\n+        }\n+\n+        /** Populate the appropriate variables from the given args */\n+        String stream = args[0];\n+        String endpoint = args[1];\n+        Integer batchIntervalMillis = Integer.valueOf(args[2]);\n+\n+        /** Create a Kinesis client in order to determine the number of shards for the given stream */\n+        AmazonKinesisClient KinesisClient = new AmazonKinesisClient(\n+                new DefaultAWSCredentialsProviderChain());\n+\n+        /** Determine the number of shards from the stream */\n+        int numShards = KinesisClient.describeStream(stream)\n+                .getStreamDescription().getShards().size();\n+\n+        /** In this example, we're going to create 1 Kinesis Worker/Receiver/DStreams for each stream shard */ \n+        int numStreams = numShards;\n+\n+        /** Must add 1 more thread than the number of receivers or the output won't show properly from the driver */\n+        int numSparkThreads = numStreams + 1;\n+\n+        /** Set the app name */\n+        String app = \"KinesisWordCount\";\n+\n+        /** Setup the Spark config. */\n+        SparkConf sparkConfig = new SparkConf().setAppName(app).setMaster(\n+                \"local[\" + numSparkThreads + \"]\");\n+\n+        /**\n+         * Set the batch interval.\n+         * Records will be pulled from the Kinesis stream and stored as a single DStream within Spark every batch interval.\n+         */\n+        Duration batchInterval = Milliseconds.apply(batchIntervalMillis);\n+\n+        /**\n+         * It's recommended that you perform a Spark checkpoint between 5 and 10 times the batch interval. \n+         * While this is the Spark checkpoint interval, we're going to use it for the Kinesis checkpoint interval, as well.\n+         */\n+        Duration checkpointInterval = batchInterval.$times(5);\n+"
  }],
  "prId": 1434
}]