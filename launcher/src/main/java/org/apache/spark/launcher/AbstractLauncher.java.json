[{
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "could you make `spark-defaults.conf` a static variable `DEFAULT_PROPERTIES_FILE` or something\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T02:46:03Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");"
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "unindent\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T02:46:25Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");"
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "this is missing docs. It should say where it's getting its properties file from\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T02:47:24Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {"
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "why `..`?\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T02:56:38Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "`java.home` generally points to the `jre` subdirectory. Probably works fine that way too.\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T19:34:31Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));"
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "should this consider `SPARK_CONF_DIR`? maybe it makes sense to abstract it to a `getConfDir` or something since it's used in multiple places\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T02:58:15Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Makes sense, but the original code did not look at `SPARK_CONF_DIR`, so that's why I kept it this way.\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T19:35:56Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "I see. I think that's just an omission in the original code. Let's just fix it here\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T19:51:20Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));"
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "`buildJavaCommand` to be more consistent?\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T03:05:10Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {"
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "not used, I don't think, and `java.util.Enumeration` up there\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T03:05:38Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;"
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "would be good to move this comment block down to L257 where datanucleus jars are actually handled\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T03:07:58Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    return cmd;\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  protected List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    String confDir = getenv(\"SPARK_CONF_DIR\");\n+    if (!isEmpty(confDir)) {\n+      addToClassPath(cp, confDir);\n+    } else {\n+      addToClassPath(cp, join(File.separator, getSparkHome(), \"conf\"));\n+    }\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars."
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "The comment explains why the code right underneath it is looking into the assembly jar at all. So I think it's better where it is.\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T19:42:21Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    return cmd;\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  protected List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    String confDir = getenv(\"SPARK_CONF_DIR\");\n+    if (!isEmpty(confDir)) {\n+      addToClassPath(cp, confDir);\n+    } else {\n+      addToClassPath(cp, join(File.separator, getSparkHome(), \"conf\"));\n+    }\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars."
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "ok, sounds fine\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T23:53:26Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    return cmd;\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  protected List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    String confDir = getenv(\"SPARK_CONF_DIR\");\n+    if (!isEmpty(confDir)) {\n+      addToClassPath(cp, confDir);\n+    } else {\n+      addToClassPath(cp, join(File.separator, getSparkHome(), \"conf\"));\n+    }\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars."
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "```\nprepareForOs(\n    list<String> cmd,\n    String libPath,\n    Map...) {\n```\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T03:12:05Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    return cmd;\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  protected List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    String confDir = getenv(\"SPARK_CONF_DIR\");\n+    if (!isEmpty(confDir)) {\n+      addToClassPath(cp, confDir);\n+    } else {\n+      addToClassPath(cp, join(File.separator, getSparkHome(), \"conf\"));\n+    }\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars.\n+    //\n+    // This block also serves as a check for SPARK-1703, when the assembly jar is built with\n+    // Java 7 and ends up with too many files, causing issues with other JDK versions.\n+    boolean needsDataNucleus = false;\n+    JarFile assemblyJar = null;\n+    try {\n+      assemblyJar = new JarFile(assembly);\n+      needsDataNucleus = assemblyJar.getEntry(\"org/apache/hadoop/hive/ql/exec/\") != null;\n+    } catch (IOException ioe) {\n+      if (ioe.getMessage().indexOf(\"invalid CEN header\") > 0) {\n+        System.err.println(\n+          \"Loading Spark jar with '$JAR_CMD' failed.\\n\" +\n+          \"This is likely because Spark was compiled with Java 7 and run\\n\" +\n+          \"with Java 6 (see SPARK-1703). Please use Java 7 to run Spark\\n\" +\n+          \"or build Spark with Java 6.\");\n+        System.exit(1);\n+      } else {\n+        throw ioe;\n+      }\n+    } finally {\n+      if (assemblyJar != null) {\n+        try {\n+          assemblyJar.close();\n+        } catch (IOException e) {\n+          // Ignore.\n+        }\n+      }\n+    }\n+\n+    if (needsDataNucleus) {\n+      System.err.println(\"Spark assembly has been built with Hive, including Datanucleus jars \" +\n+        \"in classpath.\");\n+      File libdir;\n+      if (new File(sparkHome, \"RELEASE\").isFile()) {\n+        libdir = new File(sparkHome, \"lib\");\n+      } else {\n+        libdir = new File(sparkHome, \"lib_managed/jars\");\n+      }\n+\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+        libdir.getAbsolutePath());\n+      for (File jar : libdir.listFiles()) {\n+        if (jar.getName().startsWith(\"datanucleus-\")) {\n+          addToClassPath(cp, jar.getAbsolutePath());\n+        }\n+      }\n+    }\n+\n+    addToClassPath(cp, getenv(\"HADOOP_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"YARN_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"SPARK_DIST_CLASSPATH\"));\n+    return cp;\n+  }\n+\n+  private void addToClassPath(List<String> cp, String entries) {\n+    if (isEmpty(entries)) {\n+      return;\n+    }\n+    String[] split = entries.split(Pattern.quote(File.pathSeparator));\n+    for (String entry : split) {\n+      if (!isEmpty(entry)) {\n+        if (new File(entry).isDirectory() && !entry.endsWith(File.separator)) {\n+          entry += File.separator;\n+        }\n+        cp.add(entry);\n+      }\n+    }\n+  }\n+\n+  protected String getScalaVersion() {\n+    String scala = getenv(\"SPARK_SCALA_VERSION\");\n+    if (scala != null) {\n+      return scala;\n+    }\n+\n+    String sparkHome = getSparkHome();\n+    File scala210 = new File(sparkHome, \"assembly/target/scala-2.10\");\n+    File scala211 = new File(sparkHome, \"assembly/target/scala-2.11\");\n+    if (scala210.isDirectory() && scala211.isDirectory()) {\n+      checkState(false,\n+        \"Presence of build for both scala versions (2.10 and 2.11) detected.\\n\" +\n+        \"Either clean one of them or set SPARK_SCALA_VERSION in your environment.\");\n+    } else if (scala210.isDirectory()) {\n+      return \"2.10\";\n+    } else {\n+      checkState(scala211.isDirectory(), \"Cannot find any assembly build directories.\");\n+      return \"2.11\";\n+    }\n+\n+    throw new IllegalStateException(\"Should not reach here.\");\n+  }\n+\n+  /**\n+   * Which OS is running defines two things:\n+   * - the name of the environment variable used to define the lookup path for native libs\n+   * - how to execute the command in general.\n+   *\n+   * The name is easy: PATH on Win32, DYLD_LIBRARY_PATH on MacOS, LD_LIBRARY_PATH elsewhere.\n+   *\n+   * On Unix-like, we're assuming bash is available. So we print one argument per line to\n+   * the output, and use bash's array handling to execute the right thing.\n+   *\n+    * For Win32, see {@link #prepareForWindows(List<String>,String)}.\n+   */\n+  protected List<String> prepareForOs(List<String> cmd,\n+      String libPath,\n+      Map<String, String> env) {"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "here and `prepareForWindows`\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T03:17:43Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    return cmd;\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  protected List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    String confDir = getenv(\"SPARK_CONF_DIR\");\n+    if (!isEmpty(confDir)) {\n+      addToClassPath(cp, confDir);\n+    } else {\n+      addToClassPath(cp, join(File.separator, getSparkHome(), \"conf\"));\n+    }\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars.\n+    //\n+    // This block also serves as a check for SPARK-1703, when the assembly jar is built with\n+    // Java 7 and ends up with too many files, causing issues with other JDK versions.\n+    boolean needsDataNucleus = false;\n+    JarFile assemblyJar = null;\n+    try {\n+      assemblyJar = new JarFile(assembly);\n+      needsDataNucleus = assemblyJar.getEntry(\"org/apache/hadoop/hive/ql/exec/\") != null;\n+    } catch (IOException ioe) {\n+      if (ioe.getMessage().indexOf(\"invalid CEN header\") > 0) {\n+        System.err.println(\n+          \"Loading Spark jar with '$JAR_CMD' failed.\\n\" +\n+          \"This is likely because Spark was compiled with Java 7 and run\\n\" +\n+          \"with Java 6 (see SPARK-1703). Please use Java 7 to run Spark\\n\" +\n+          \"or build Spark with Java 6.\");\n+        System.exit(1);\n+      } else {\n+        throw ioe;\n+      }\n+    } finally {\n+      if (assemblyJar != null) {\n+        try {\n+          assemblyJar.close();\n+        } catch (IOException e) {\n+          // Ignore.\n+        }\n+      }\n+    }\n+\n+    if (needsDataNucleus) {\n+      System.err.println(\"Spark assembly has been built with Hive, including Datanucleus jars \" +\n+        \"in classpath.\");\n+      File libdir;\n+      if (new File(sparkHome, \"RELEASE\").isFile()) {\n+        libdir = new File(sparkHome, \"lib\");\n+      } else {\n+        libdir = new File(sparkHome, \"lib_managed/jars\");\n+      }\n+\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+        libdir.getAbsolutePath());\n+      for (File jar : libdir.listFiles()) {\n+        if (jar.getName().startsWith(\"datanucleus-\")) {\n+          addToClassPath(cp, jar.getAbsolutePath());\n+        }\n+      }\n+    }\n+\n+    addToClassPath(cp, getenv(\"HADOOP_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"YARN_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"SPARK_DIST_CLASSPATH\"));\n+    return cp;\n+  }\n+\n+  private void addToClassPath(List<String> cp, String entries) {\n+    if (isEmpty(entries)) {\n+      return;\n+    }\n+    String[] split = entries.split(Pattern.quote(File.pathSeparator));\n+    for (String entry : split) {\n+      if (!isEmpty(entry)) {\n+        if (new File(entry).isDirectory() && !entry.endsWith(File.separator)) {\n+          entry += File.separator;\n+        }\n+        cp.add(entry);\n+      }\n+    }\n+  }\n+\n+  protected String getScalaVersion() {\n+    String scala = getenv(\"SPARK_SCALA_VERSION\");\n+    if (scala != null) {\n+      return scala;\n+    }\n+\n+    String sparkHome = getSparkHome();\n+    File scala210 = new File(sparkHome, \"assembly/target/scala-2.10\");\n+    File scala211 = new File(sparkHome, \"assembly/target/scala-2.11\");\n+    if (scala210.isDirectory() && scala211.isDirectory()) {\n+      checkState(false,\n+        \"Presence of build for both scala versions (2.10 and 2.11) detected.\\n\" +\n+        \"Either clean one of them or set SPARK_SCALA_VERSION in your environment.\");\n+    } else if (scala210.isDirectory()) {\n+      return \"2.10\";\n+    } else {\n+      checkState(scala211.isDirectory(), \"Cannot find any assembly build directories.\");\n+      return \"2.11\";\n+    }\n+\n+    throw new IllegalStateException(\"Should not reach here.\");\n+  }\n+\n+  /**\n+   * Which OS is running defines two things:\n+   * - the name of the environment variable used to define the lookup path for native libs\n+   * - how to execute the command in general.\n+   *\n+   * The name is easy: PATH on Win32, DYLD_LIBRARY_PATH on MacOS, LD_LIBRARY_PATH elsewhere.\n+   *\n+   * On Unix-like, we're assuming bash is available. So we print one argument per line to\n+   * the output, and use bash's array handling to execute the right thing.\n+   *\n+    * For Win32, see {@link #prepareForWindows(List<String>,String)}.\n+   */\n+  protected List<String> prepareForOs(List<String> cmd,\n+      String libPath,\n+      Map<String, String> env) {"
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "can you add java doc here to explain what `entries` look like, whether it mutates `cp` etc.\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T03:14:58Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    return cmd;\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  protected List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    String confDir = getenv(\"SPARK_CONF_DIR\");\n+    if (!isEmpty(confDir)) {\n+      addToClassPath(cp, confDir);\n+    } else {\n+      addToClassPath(cp, join(File.separator, getSparkHome(), \"conf\"));\n+    }\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars.\n+    //\n+    // This block also serves as a check for SPARK-1703, when the assembly jar is built with\n+    // Java 7 and ends up with too many files, causing issues with other JDK versions.\n+    boolean needsDataNucleus = false;\n+    JarFile assemblyJar = null;\n+    try {\n+      assemblyJar = new JarFile(assembly);\n+      needsDataNucleus = assemblyJar.getEntry(\"org/apache/hadoop/hive/ql/exec/\") != null;\n+    } catch (IOException ioe) {\n+      if (ioe.getMessage().indexOf(\"invalid CEN header\") > 0) {\n+        System.err.println(\n+          \"Loading Spark jar with '$JAR_CMD' failed.\\n\" +\n+          \"This is likely because Spark was compiled with Java 7 and run\\n\" +\n+          \"with Java 6 (see SPARK-1703). Please use Java 7 to run Spark\\n\" +\n+          \"or build Spark with Java 6.\");\n+        System.exit(1);\n+      } else {\n+        throw ioe;\n+      }\n+    } finally {\n+      if (assemblyJar != null) {\n+        try {\n+          assemblyJar.close();\n+        } catch (IOException e) {\n+          // Ignore.\n+        }\n+      }\n+    }\n+\n+    if (needsDataNucleus) {\n+      System.err.println(\"Spark assembly has been built with Hive, including Datanucleus jars \" +\n+        \"in classpath.\");\n+      File libdir;\n+      if (new File(sparkHome, \"RELEASE\").isFile()) {\n+        libdir = new File(sparkHome, \"lib\");\n+      } else {\n+        libdir = new File(sparkHome, \"lib_managed/jars\");\n+      }\n+\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+        libdir.getAbsolutePath());\n+      for (File jar : libdir.listFiles()) {\n+        if (jar.getName().startsWith(\"datanucleus-\")) {\n+          addToClassPath(cp, jar.getAbsolutePath());\n+        }\n+      }\n+    }\n+\n+    addToClassPath(cp, getenv(\"HADOOP_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"YARN_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"SPARK_DIST_CLASSPATH\"));\n+    return cp;\n+  }\n+\n+  private void addToClassPath(List<String> cp, String entries) {"
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "could all of these go into `LauncherCommon`? Seems like a more appropriate place there\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T03:17:16Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    return cmd;\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  protected List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    String confDir = getenv(\"SPARK_CONF_DIR\");\n+    if (!isEmpty(confDir)) {\n+      addToClassPath(cp, confDir);\n+    } else {\n+      addToClassPath(cp, join(File.separator, getSparkHome(), \"conf\"));\n+    }\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars.\n+    //\n+    // This block also serves as a check for SPARK-1703, when the assembly jar is built with\n+    // Java 7 and ends up with too many files, causing issues with other JDK versions.\n+    boolean needsDataNucleus = false;\n+    JarFile assemblyJar = null;\n+    try {\n+      assemblyJar = new JarFile(assembly);\n+      needsDataNucleus = assemblyJar.getEntry(\"org/apache/hadoop/hive/ql/exec/\") != null;\n+    } catch (IOException ioe) {\n+      if (ioe.getMessage().indexOf(\"invalid CEN header\") > 0) {\n+        System.err.println(\n+          \"Loading Spark jar with '$JAR_CMD' failed.\\n\" +\n+          \"This is likely because Spark was compiled with Java 7 and run\\n\" +\n+          \"with Java 6 (see SPARK-1703). Please use Java 7 to run Spark\\n\" +\n+          \"or build Spark with Java 6.\");\n+        System.exit(1);\n+      } else {\n+        throw ioe;\n+      }\n+    } finally {\n+      if (assemblyJar != null) {\n+        try {\n+          assemblyJar.close();\n+        } catch (IOException e) {\n+          // Ignore.\n+        }\n+      }\n+    }\n+\n+    if (needsDataNucleus) {\n+      System.err.println(\"Spark assembly has been built with Hive, including Datanucleus jars \" +\n+        \"in classpath.\");\n+      File libdir;\n+      if (new File(sparkHome, \"RELEASE\").isFile()) {\n+        libdir = new File(sparkHome, \"lib\");\n+      } else {\n+        libdir = new File(sparkHome, \"lib_managed/jars\");\n+      }\n+\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+        libdir.getAbsolutePath());\n+      for (File jar : libdir.listFiles()) {\n+        if (jar.getName().startsWith(\"datanucleus-\")) {\n+          addToClassPath(cp, jar.getAbsolutePath());\n+        }\n+      }\n+    }\n+\n+    addToClassPath(cp, getenv(\"HADOOP_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"YARN_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"SPARK_DIST_CLASSPATH\"));\n+    return cp;\n+  }\n+\n+  private void addToClassPath(List<String> cp, String entries) {\n+    if (isEmpty(entries)) {\n+      return;\n+    }\n+    String[] split = entries.split(Pattern.quote(File.pathSeparator));\n+    for (String entry : split) {\n+      if (!isEmpty(entry)) {\n+        if (new File(entry).isDirectory() && !entry.endsWith(File.separator)) {\n+          entry += File.separator;\n+        }\n+        cp.add(entry);\n+      }\n+    }\n+  }\n+\n+  protected String getScalaVersion() {\n+    String scala = getenv(\"SPARK_SCALA_VERSION\");\n+    if (scala != null) {\n+      return scala;\n+    }\n+\n+    String sparkHome = getSparkHome();\n+    File scala210 = new File(sparkHome, \"assembly/target/scala-2.10\");\n+    File scala211 = new File(sparkHome, \"assembly/target/scala-2.11\");\n+    if (scala210.isDirectory() && scala211.isDirectory()) {\n+      checkState(false,\n+        \"Presence of build for both scala versions (2.10 and 2.11) detected.\\n\" +\n+        \"Either clean one of them or set SPARK_SCALA_VERSION in your environment.\");\n+    } else if (scala210.isDirectory()) {\n+      return \"2.10\";\n+    } else {\n+      checkState(scala211.isDirectory(), \"Cannot find any assembly build directories.\");\n+      return \"2.11\";\n+    }\n+\n+    throw new IllegalStateException(\"Should not reach here.\");\n+  }\n+\n+  /**\n+   * Which OS is running defines two things:\n+   * - the name of the environment variable used to define the lookup path for native libs\n+   * - how to execute the command in general.\n+   *\n+   * The name is easy: PATH on Win32, DYLD_LIBRARY_PATH on MacOS, LD_LIBRARY_PATH elsewhere.\n+   *\n+   * On Unix-like, we're assuming bash is available. So we print one argument per line to\n+   * the output, and use bash's array handling to execute the right thing.\n+   *\n+    * For Win32, see {@link #prepareForWindows(List<String>,String)}.\n+   */\n+  protected List<String> prepareForOs(List<String> cmd,\n+      String libPath,\n+      Map<String, String> env) {\n+\n+    // If SPARK_HOME does not come from the environment, explicitly set it\n+    // in the child's environment.\n+    Map<String, String> childEnv = env;\n+    if (System.getenv(\"SPARK_HOME\") == null && !env.containsKey(\"SPARK_HOME\")) {\n+      childEnv = new HashMap<String, String>(env);\n+      childEnv.put(\"SPARK_HOME\", sparkHome);\n+    }\n+\n+    if (isWindows()) {\n+      return prepareForWindows(cmd, libPath, childEnv);\n+    }\n+\n+    if (isEmpty(libPath) && childEnv.isEmpty()) {\n+      return cmd;\n+    }\n+\n+    List<String> newCmd = new ArrayList<String>();\n+    newCmd.add(\"env\");\n+\n+    if (!isEmpty(libPath)) {\n+      String envName = getLibPathEnvName();\n+      String currEnvValue = getenv(envName);\n+      String newEnvValue = join(File.pathSeparator, currEnvValue, libPath);\n+      newCmd.add(String.format(\"%s=%s\", envName, newEnvValue));\n+    }\n+    for (Map.Entry<String, String> e : childEnv.entrySet()) {\n+      newCmd.add(String.format(\"%s=%s\", e.getKey(), e.getValue()));\n+    }\n+    newCmd.addAll(cmd);\n+    return newCmd;\n+  }\n+\n+  protected String shQuote(String s) {\n+    StringBuilder quoted = new StringBuilder();\n+    boolean hasWhitespace = false;\n+    for (int i = 0; i < s.length(); i++) {\n+      if (Character.isWhitespace(s.codePointAt(i))) {\n+        quoted.append('\"');\n+        hasWhitespace = true;\n+        break;\n+      }\n+    }\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int cp = s.codePointAt(i);\n+      switch (cp) {\n+        case '\\'':\n+          if (hasWhitespace) {\n+            quoted.appendCodePoint(cp);\n+            break;\n+          }\n+        case '\"':\n+        case '\\\\':\n+          quoted.append('\\\\');\n+          // Fall through.\n+        default:\n+          if (Character.isWhitespace(cp)) {\n+            hasWhitespace=true;\n+          }\n+          quoted.appendCodePoint(cp);\n+      }\n+    }\n+    if (hasWhitespace) {\n+      quoted.append('\"');\n+    }\n+    return quoted.toString();\n+  }\n+\n+  // Visible for testing.\n+  List<String> parseOptionString(String s) {\n+    List<String> opts = new ArrayList<String>();\n+    StringBuilder opt = new StringBuilder();\n+    boolean inOpt = false;\n+    boolean inSingleQuote = false;\n+    boolean inDoubleQuote = false;\n+    boolean escapeNext = false;\n+    boolean hasData = false;\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int c = s.codePointAt(i);\n+      if (escapeNext) {\n+        if (!inOpt) {\n+          inOpt = true;\n+        }\n+        opt.appendCodePoint(c);\n+        escapeNext = false;\n+      } else if (inOpt) {\n+        switch (c) {\n+        case '\\\\':\n+          if (inSingleQuote) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            escapeNext = true;\n+          }\n+          break;\n+        case '\\'':\n+          if (inDoubleQuote) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            inSingleQuote = !inSingleQuote;\n+          }\n+          break;\n+        case '\"':\n+          if (inSingleQuote) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            inDoubleQuote = !inDoubleQuote;\n+          }\n+          break;\n+        default:\n+          if (inSingleQuote || inDoubleQuote || !Character.isWhitespace(c)) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            finishOpt(opts, opt);\n+            inOpt = false;\n+            hasData = false;\n+          }\n+        }\n+      } else {\n+        switch (c) {\n+        case '\\'':\n+          inSingleQuote = true;\n+          inOpt = true;\n+          hasData = true;\n+          break;\n+        case '\"':\n+          inDoubleQuote = true;\n+          inOpt = true;\n+          hasData = true;\n+          break;\n+        case '\\\\':\n+          escapeNext = true;\n+          break;\n+        default:\n+          if (!Character.isWhitespace(c)) {\n+            inOpt = true;\n+            opt.appendCodePoint(c);\n+          }\n+        }\n+      }\n+    }\n+\n+    checkArgument(!inSingleQuote && !inDoubleQuote && !escapeNext, \"Invalid option string: %s\", s);\n+    if (opt.length() > 0 || hasData) {\n+      opts.add(opt.toString());\n+    }\n+    return opts;\n+  }\n+\n+  private void finishOpt(List<String> opts, StringBuilder opt) {\n+    opts.add(opt.toString());\n+    opt.setLength(0);\n+  }\n+\n+  private String findAssembly(String scalaVersion) {\n+    String sparkHome = getSparkHome();\n+    File libdir;\n+    if (new File(sparkHome, \"RELEASE\").isFile()) {\n+      libdir = new File(sparkHome, \"lib\");\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+          libdir.getAbsolutePath());\n+    } else {\n+      libdir = new File(sparkHome, String.format(\"assembly/target/scala-%s\", scalaVersion));\n+    }\n+\n+    final Pattern re = Pattern.compile(\"spark-assembly.*hadoop.*\\\\.jar\");\n+    FileFilter filter = new FileFilter() {\n+      @Override\n+      public boolean accept(File file) {\n+        return file.isFile() && re.matcher(file.getName()).matches();\n+      }\n+    };\n+    File[] assemblies = libdir.listFiles(filter);\n+    checkState(assemblies != null && assemblies.length > 0, \"No assemblies found in '%s'.\", libdir);\n+    checkState(assemblies.length == 1, \"Multiple assemblies found in '%s'.\", libdir);\n+    return assemblies[0].getAbsolutePath();\n+  }\n+\n+  private String getenv(String key) {\n+    return first(env != null ? env.get(key) : null, System.getenv(key));\n+  }\n+\n+  /**\n+   * Prepare a command line for execution on Windows.\n+   *\n+   * Two things need to be done:\n+   *\n+   * - If a custom library path is needed, extend PATH to add it. Based on:\n+   *   http://superuser.com/questions/223104/setting-environment-variable-for-just-one-command-in-windows-cmd-exe\n+   *\n+   * - Quote all arguments so that spaces are handled as expected. Quotes within arguments are\n+   *   \"double quoted\" (which is batch for escaping a quote). This page has more details about\n+   *   quoting and other batch script fun stuff: http://ss64.com/nt/syntax-esc.html\n+   */\n+  private List<String> prepareForWindows(List<String> cmd,\n+      String libPath,\n+      Map<String, String> env) {\n+    StringBuilder cmdline = new StringBuilder(\"cmd /c \\\"\");\n+    if (libPath != null) {\n+      cmdline.append(\"set PATH=%PATH%;\").append(libPath).append(\" &&\");\n+    }\n+    for (Map.Entry<String, String> e : env.entrySet()) {\n+      cmdline.append(String.format(\"set %s=%s\", e.getKey(), e.getValue()));\n+      cmdline.append(\" &&\");\n+    }\n+    for (String arg : cmd) {\n+      if (cmdline.length() > 0) {\n+        cmdline.append(\" \");\n+      }\n+      cmdline.append(quote(arg));\n+    }\n+    cmdline.append(\"\\\"\");\n+    return Arrays.asList(cmdline.toString());\n+  }\n+\n+  /**\n+   * Quoting arguments that don't need quoting in Windows seems to cause weird issues. So only\n+   * quote arguments when there is whitespace in them.\n+   */\n+  private boolean needsQuoting(String arg) {\n+    for (int i = 0; i < arg.length(); i++) {\n+      if (Character.isWhitespace(arg.codePointAt(i))) {\n+        return true;\n+      }\n+    }\n+    return false;\n+  }\n+\n+  private String quote(String arg) {"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "I'd rather keep methods only used in a particular class together with that class's code.\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T19:45:34Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    return cmd;\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  protected List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    String confDir = getenv(\"SPARK_CONF_DIR\");\n+    if (!isEmpty(confDir)) {\n+      addToClassPath(cp, confDir);\n+    } else {\n+      addToClassPath(cp, join(File.separator, getSparkHome(), \"conf\"));\n+    }\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars.\n+    //\n+    // This block also serves as a check for SPARK-1703, when the assembly jar is built with\n+    // Java 7 and ends up with too many files, causing issues with other JDK versions.\n+    boolean needsDataNucleus = false;\n+    JarFile assemblyJar = null;\n+    try {\n+      assemblyJar = new JarFile(assembly);\n+      needsDataNucleus = assemblyJar.getEntry(\"org/apache/hadoop/hive/ql/exec/\") != null;\n+    } catch (IOException ioe) {\n+      if (ioe.getMessage().indexOf(\"invalid CEN header\") > 0) {\n+        System.err.println(\n+          \"Loading Spark jar with '$JAR_CMD' failed.\\n\" +\n+          \"This is likely because Spark was compiled with Java 7 and run\\n\" +\n+          \"with Java 6 (see SPARK-1703). Please use Java 7 to run Spark\\n\" +\n+          \"or build Spark with Java 6.\");\n+        System.exit(1);\n+      } else {\n+        throw ioe;\n+      }\n+    } finally {\n+      if (assemblyJar != null) {\n+        try {\n+          assemblyJar.close();\n+        } catch (IOException e) {\n+          // Ignore.\n+        }\n+      }\n+    }\n+\n+    if (needsDataNucleus) {\n+      System.err.println(\"Spark assembly has been built with Hive, including Datanucleus jars \" +\n+        \"in classpath.\");\n+      File libdir;\n+      if (new File(sparkHome, \"RELEASE\").isFile()) {\n+        libdir = new File(sparkHome, \"lib\");\n+      } else {\n+        libdir = new File(sparkHome, \"lib_managed/jars\");\n+      }\n+\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+        libdir.getAbsolutePath());\n+      for (File jar : libdir.listFiles()) {\n+        if (jar.getName().startsWith(\"datanucleus-\")) {\n+          addToClassPath(cp, jar.getAbsolutePath());\n+        }\n+      }\n+    }\n+\n+    addToClassPath(cp, getenv(\"HADOOP_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"YARN_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"SPARK_DIST_CLASSPATH\"));\n+    return cp;\n+  }\n+\n+  private void addToClassPath(List<String> cp, String entries) {\n+    if (isEmpty(entries)) {\n+      return;\n+    }\n+    String[] split = entries.split(Pattern.quote(File.pathSeparator));\n+    for (String entry : split) {\n+      if (!isEmpty(entry)) {\n+        if (new File(entry).isDirectory() && !entry.endsWith(File.separator)) {\n+          entry += File.separator;\n+        }\n+        cp.add(entry);\n+      }\n+    }\n+  }\n+\n+  protected String getScalaVersion() {\n+    String scala = getenv(\"SPARK_SCALA_VERSION\");\n+    if (scala != null) {\n+      return scala;\n+    }\n+\n+    String sparkHome = getSparkHome();\n+    File scala210 = new File(sparkHome, \"assembly/target/scala-2.10\");\n+    File scala211 = new File(sparkHome, \"assembly/target/scala-2.11\");\n+    if (scala210.isDirectory() && scala211.isDirectory()) {\n+      checkState(false,\n+        \"Presence of build for both scala versions (2.10 and 2.11) detected.\\n\" +\n+        \"Either clean one of them or set SPARK_SCALA_VERSION in your environment.\");\n+    } else if (scala210.isDirectory()) {\n+      return \"2.10\";\n+    } else {\n+      checkState(scala211.isDirectory(), \"Cannot find any assembly build directories.\");\n+      return \"2.11\";\n+    }\n+\n+    throw new IllegalStateException(\"Should not reach here.\");\n+  }\n+\n+  /**\n+   * Which OS is running defines two things:\n+   * - the name of the environment variable used to define the lookup path for native libs\n+   * - how to execute the command in general.\n+   *\n+   * The name is easy: PATH on Win32, DYLD_LIBRARY_PATH on MacOS, LD_LIBRARY_PATH elsewhere.\n+   *\n+   * On Unix-like, we're assuming bash is available. So we print one argument per line to\n+   * the output, and use bash's array handling to execute the right thing.\n+   *\n+    * For Win32, see {@link #prepareForWindows(List<String>,String)}.\n+   */\n+  protected List<String> prepareForOs(List<String> cmd,\n+      String libPath,\n+      Map<String, String> env) {\n+\n+    // If SPARK_HOME does not come from the environment, explicitly set it\n+    // in the child's environment.\n+    Map<String, String> childEnv = env;\n+    if (System.getenv(\"SPARK_HOME\") == null && !env.containsKey(\"SPARK_HOME\")) {\n+      childEnv = new HashMap<String, String>(env);\n+      childEnv.put(\"SPARK_HOME\", sparkHome);\n+    }\n+\n+    if (isWindows()) {\n+      return prepareForWindows(cmd, libPath, childEnv);\n+    }\n+\n+    if (isEmpty(libPath) && childEnv.isEmpty()) {\n+      return cmd;\n+    }\n+\n+    List<String> newCmd = new ArrayList<String>();\n+    newCmd.add(\"env\");\n+\n+    if (!isEmpty(libPath)) {\n+      String envName = getLibPathEnvName();\n+      String currEnvValue = getenv(envName);\n+      String newEnvValue = join(File.pathSeparator, currEnvValue, libPath);\n+      newCmd.add(String.format(\"%s=%s\", envName, newEnvValue));\n+    }\n+    for (Map.Entry<String, String> e : childEnv.entrySet()) {\n+      newCmd.add(String.format(\"%s=%s\", e.getKey(), e.getValue()));\n+    }\n+    newCmd.addAll(cmd);\n+    return newCmd;\n+  }\n+\n+  protected String shQuote(String s) {\n+    StringBuilder quoted = new StringBuilder();\n+    boolean hasWhitespace = false;\n+    for (int i = 0; i < s.length(); i++) {\n+      if (Character.isWhitespace(s.codePointAt(i))) {\n+        quoted.append('\"');\n+        hasWhitespace = true;\n+        break;\n+      }\n+    }\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int cp = s.codePointAt(i);\n+      switch (cp) {\n+        case '\\'':\n+          if (hasWhitespace) {\n+            quoted.appendCodePoint(cp);\n+            break;\n+          }\n+        case '\"':\n+        case '\\\\':\n+          quoted.append('\\\\');\n+          // Fall through.\n+        default:\n+          if (Character.isWhitespace(cp)) {\n+            hasWhitespace=true;\n+          }\n+          quoted.appendCodePoint(cp);\n+      }\n+    }\n+    if (hasWhitespace) {\n+      quoted.append('\"');\n+    }\n+    return quoted.toString();\n+  }\n+\n+  // Visible for testing.\n+  List<String> parseOptionString(String s) {\n+    List<String> opts = new ArrayList<String>();\n+    StringBuilder opt = new StringBuilder();\n+    boolean inOpt = false;\n+    boolean inSingleQuote = false;\n+    boolean inDoubleQuote = false;\n+    boolean escapeNext = false;\n+    boolean hasData = false;\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int c = s.codePointAt(i);\n+      if (escapeNext) {\n+        if (!inOpt) {\n+          inOpt = true;\n+        }\n+        opt.appendCodePoint(c);\n+        escapeNext = false;\n+      } else if (inOpt) {\n+        switch (c) {\n+        case '\\\\':\n+          if (inSingleQuote) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            escapeNext = true;\n+          }\n+          break;\n+        case '\\'':\n+          if (inDoubleQuote) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            inSingleQuote = !inSingleQuote;\n+          }\n+          break;\n+        case '\"':\n+          if (inSingleQuote) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            inDoubleQuote = !inDoubleQuote;\n+          }\n+          break;\n+        default:\n+          if (inSingleQuote || inDoubleQuote || !Character.isWhitespace(c)) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            finishOpt(opts, opt);\n+            inOpt = false;\n+            hasData = false;\n+          }\n+        }\n+      } else {\n+        switch (c) {\n+        case '\\'':\n+          inSingleQuote = true;\n+          inOpt = true;\n+          hasData = true;\n+          break;\n+        case '\"':\n+          inDoubleQuote = true;\n+          inOpt = true;\n+          hasData = true;\n+          break;\n+        case '\\\\':\n+          escapeNext = true;\n+          break;\n+        default:\n+          if (!Character.isWhitespace(c)) {\n+            inOpt = true;\n+            opt.appendCodePoint(c);\n+          }\n+        }\n+      }\n+    }\n+\n+    checkArgument(!inSingleQuote && !inDoubleQuote && !escapeNext, \"Invalid option string: %s\", s);\n+    if (opt.length() > 0 || hasData) {\n+      opts.add(opt.toString());\n+    }\n+    return opts;\n+  }\n+\n+  private void finishOpt(List<String> opts, StringBuilder opt) {\n+    opts.add(opt.toString());\n+    opt.setLength(0);\n+  }\n+\n+  private String findAssembly(String scalaVersion) {\n+    String sparkHome = getSparkHome();\n+    File libdir;\n+    if (new File(sparkHome, \"RELEASE\").isFile()) {\n+      libdir = new File(sparkHome, \"lib\");\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+          libdir.getAbsolutePath());\n+    } else {\n+      libdir = new File(sparkHome, String.format(\"assembly/target/scala-%s\", scalaVersion));\n+    }\n+\n+    final Pattern re = Pattern.compile(\"spark-assembly.*hadoop.*\\\\.jar\");\n+    FileFilter filter = new FileFilter() {\n+      @Override\n+      public boolean accept(File file) {\n+        return file.isFile() && re.matcher(file.getName()).matches();\n+      }\n+    };\n+    File[] assemblies = libdir.listFiles(filter);\n+    checkState(assemblies != null && assemblies.length > 0, \"No assemblies found in '%s'.\", libdir);\n+    checkState(assemblies.length == 1, \"Multiple assemblies found in '%s'.\", libdir);\n+    return assemblies[0].getAbsolutePath();\n+  }\n+\n+  private String getenv(String key) {\n+    return first(env != null ? env.get(key) : null, System.getenv(key));\n+  }\n+\n+  /**\n+   * Prepare a command line for execution on Windows.\n+   *\n+   * Two things need to be done:\n+   *\n+   * - If a custom library path is needed, extend PATH to add it. Based on:\n+   *   http://superuser.com/questions/223104/setting-environment-variable-for-just-one-command-in-windows-cmd-exe\n+   *\n+   * - Quote all arguments so that spaces are handled as expected. Quotes within arguments are\n+   *   \"double quoted\" (which is batch for escaping a quote). This page has more details about\n+   *   quoting and other batch script fun stuff: http://ss64.com/nt/syntax-esc.html\n+   */\n+  private List<String> prepareForWindows(List<String> cmd,\n+      String libPath,\n+      Map<String, String> env) {\n+    StringBuilder cmdline = new StringBuilder(\"cmd /c \\\"\");\n+    if (libPath != null) {\n+      cmdline.append(\"set PATH=%PATH%;\").append(libPath).append(\" &&\");\n+    }\n+    for (Map.Entry<String, String> e : env.entrySet()) {\n+      cmdline.append(String.format(\"set %s=%s\", e.getKey(), e.getValue()));\n+      cmdline.append(\" &&\");\n+    }\n+    for (String arg : cmd) {\n+      if (cmdline.length() > 0) {\n+        cmdline.append(\" \");\n+      }\n+      cmdline.append(quote(arg));\n+    }\n+    cmdline.append(\"\\\"\");\n+    return Arrays.asList(cmdline.toString());\n+  }\n+\n+  /**\n+   * Quoting arguments that don't need quoting in Windows seems to cause weird issues. So only\n+   * quote arguments when there is whitespace in them.\n+   */\n+  private boolean needsQuoting(String arg) {\n+    for (int i = 0; i < arg.length(); i++) {\n+      if (Character.isWhitespace(arg.codePointAt(i))) {\n+        return true;\n+      }\n+    }\n+    return false;\n+  }\n+\n+  private String quote(String arg) {"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "I see. My concern is just that these classes have become quite large (500+ lines in this case). I think it would be good to define a `LauncherUtils` or something that contains all of the helper methods that don't need to be class methods. The abstraction here is not so much for code reuse but more for minimizing the size of the class.\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T22:06:47Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    return cmd;\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  protected List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    String confDir = getenv(\"SPARK_CONF_DIR\");\n+    if (!isEmpty(confDir)) {\n+      addToClassPath(cp, confDir);\n+    } else {\n+      addToClassPath(cp, join(File.separator, getSparkHome(), \"conf\"));\n+    }\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars.\n+    //\n+    // This block also serves as a check for SPARK-1703, when the assembly jar is built with\n+    // Java 7 and ends up with too many files, causing issues with other JDK versions.\n+    boolean needsDataNucleus = false;\n+    JarFile assemblyJar = null;\n+    try {\n+      assemblyJar = new JarFile(assembly);\n+      needsDataNucleus = assemblyJar.getEntry(\"org/apache/hadoop/hive/ql/exec/\") != null;\n+    } catch (IOException ioe) {\n+      if (ioe.getMessage().indexOf(\"invalid CEN header\") > 0) {\n+        System.err.println(\n+          \"Loading Spark jar with '$JAR_CMD' failed.\\n\" +\n+          \"This is likely because Spark was compiled with Java 7 and run\\n\" +\n+          \"with Java 6 (see SPARK-1703). Please use Java 7 to run Spark\\n\" +\n+          \"or build Spark with Java 6.\");\n+        System.exit(1);\n+      } else {\n+        throw ioe;\n+      }\n+    } finally {\n+      if (assemblyJar != null) {\n+        try {\n+          assemblyJar.close();\n+        } catch (IOException e) {\n+          // Ignore.\n+        }\n+      }\n+    }\n+\n+    if (needsDataNucleus) {\n+      System.err.println(\"Spark assembly has been built with Hive, including Datanucleus jars \" +\n+        \"in classpath.\");\n+      File libdir;\n+      if (new File(sparkHome, \"RELEASE\").isFile()) {\n+        libdir = new File(sparkHome, \"lib\");\n+      } else {\n+        libdir = new File(sparkHome, \"lib_managed/jars\");\n+      }\n+\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+        libdir.getAbsolutePath());\n+      for (File jar : libdir.listFiles()) {\n+        if (jar.getName().startsWith(\"datanucleus-\")) {\n+          addToClassPath(cp, jar.getAbsolutePath());\n+        }\n+      }\n+    }\n+\n+    addToClassPath(cp, getenv(\"HADOOP_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"YARN_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"SPARK_DIST_CLASSPATH\"));\n+    return cp;\n+  }\n+\n+  private void addToClassPath(List<String> cp, String entries) {\n+    if (isEmpty(entries)) {\n+      return;\n+    }\n+    String[] split = entries.split(Pattern.quote(File.pathSeparator));\n+    for (String entry : split) {\n+      if (!isEmpty(entry)) {\n+        if (new File(entry).isDirectory() && !entry.endsWith(File.separator)) {\n+          entry += File.separator;\n+        }\n+        cp.add(entry);\n+      }\n+    }\n+  }\n+\n+  protected String getScalaVersion() {\n+    String scala = getenv(\"SPARK_SCALA_VERSION\");\n+    if (scala != null) {\n+      return scala;\n+    }\n+\n+    String sparkHome = getSparkHome();\n+    File scala210 = new File(sparkHome, \"assembly/target/scala-2.10\");\n+    File scala211 = new File(sparkHome, \"assembly/target/scala-2.11\");\n+    if (scala210.isDirectory() && scala211.isDirectory()) {\n+      checkState(false,\n+        \"Presence of build for both scala versions (2.10 and 2.11) detected.\\n\" +\n+        \"Either clean one of them or set SPARK_SCALA_VERSION in your environment.\");\n+    } else if (scala210.isDirectory()) {\n+      return \"2.10\";\n+    } else {\n+      checkState(scala211.isDirectory(), \"Cannot find any assembly build directories.\");\n+      return \"2.11\";\n+    }\n+\n+    throw new IllegalStateException(\"Should not reach here.\");\n+  }\n+\n+  /**\n+   * Which OS is running defines two things:\n+   * - the name of the environment variable used to define the lookup path for native libs\n+   * - how to execute the command in general.\n+   *\n+   * The name is easy: PATH on Win32, DYLD_LIBRARY_PATH on MacOS, LD_LIBRARY_PATH elsewhere.\n+   *\n+   * On Unix-like, we're assuming bash is available. So we print one argument per line to\n+   * the output, and use bash's array handling to execute the right thing.\n+   *\n+    * For Win32, see {@link #prepareForWindows(List<String>,String)}.\n+   */\n+  protected List<String> prepareForOs(List<String> cmd,\n+      String libPath,\n+      Map<String, String> env) {\n+\n+    // If SPARK_HOME does not come from the environment, explicitly set it\n+    // in the child's environment.\n+    Map<String, String> childEnv = env;\n+    if (System.getenv(\"SPARK_HOME\") == null && !env.containsKey(\"SPARK_HOME\")) {\n+      childEnv = new HashMap<String, String>(env);\n+      childEnv.put(\"SPARK_HOME\", sparkHome);\n+    }\n+\n+    if (isWindows()) {\n+      return prepareForWindows(cmd, libPath, childEnv);\n+    }\n+\n+    if (isEmpty(libPath) && childEnv.isEmpty()) {\n+      return cmd;\n+    }\n+\n+    List<String> newCmd = new ArrayList<String>();\n+    newCmd.add(\"env\");\n+\n+    if (!isEmpty(libPath)) {\n+      String envName = getLibPathEnvName();\n+      String currEnvValue = getenv(envName);\n+      String newEnvValue = join(File.pathSeparator, currEnvValue, libPath);\n+      newCmd.add(String.format(\"%s=%s\", envName, newEnvValue));\n+    }\n+    for (Map.Entry<String, String> e : childEnv.entrySet()) {\n+      newCmd.add(String.format(\"%s=%s\", e.getKey(), e.getValue()));\n+    }\n+    newCmd.addAll(cmd);\n+    return newCmd;\n+  }\n+\n+  protected String shQuote(String s) {\n+    StringBuilder quoted = new StringBuilder();\n+    boolean hasWhitespace = false;\n+    for (int i = 0; i < s.length(); i++) {\n+      if (Character.isWhitespace(s.codePointAt(i))) {\n+        quoted.append('\"');\n+        hasWhitespace = true;\n+        break;\n+      }\n+    }\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int cp = s.codePointAt(i);\n+      switch (cp) {\n+        case '\\'':\n+          if (hasWhitespace) {\n+            quoted.appendCodePoint(cp);\n+            break;\n+          }\n+        case '\"':\n+        case '\\\\':\n+          quoted.append('\\\\');\n+          // Fall through.\n+        default:\n+          if (Character.isWhitespace(cp)) {\n+            hasWhitespace=true;\n+          }\n+          quoted.appendCodePoint(cp);\n+      }\n+    }\n+    if (hasWhitespace) {\n+      quoted.append('\"');\n+    }\n+    return quoted.toString();\n+  }\n+\n+  // Visible for testing.\n+  List<String> parseOptionString(String s) {\n+    List<String> opts = new ArrayList<String>();\n+    StringBuilder opt = new StringBuilder();\n+    boolean inOpt = false;\n+    boolean inSingleQuote = false;\n+    boolean inDoubleQuote = false;\n+    boolean escapeNext = false;\n+    boolean hasData = false;\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int c = s.codePointAt(i);\n+      if (escapeNext) {\n+        if (!inOpt) {\n+          inOpt = true;\n+        }\n+        opt.appendCodePoint(c);\n+        escapeNext = false;\n+      } else if (inOpt) {\n+        switch (c) {\n+        case '\\\\':\n+          if (inSingleQuote) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            escapeNext = true;\n+          }\n+          break;\n+        case '\\'':\n+          if (inDoubleQuote) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            inSingleQuote = !inSingleQuote;\n+          }\n+          break;\n+        case '\"':\n+          if (inSingleQuote) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            inDoubleQuote = !inDoubleQuote;\n+          }\n+          break;\n+        default:\n+          if (inSingleQuote || inDoubleQuote || !Character.isWhitespace(c)) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            finishOpt(opts, opt);\n+            inOpt = false;\n+            hasData = false;\n+          }\n+        }\n+      } else {\n+        switch (c) {\n+        case '\\'':\n+          inSingleQuote = true;\n+          inOpt = true;\n+          hasData = true;\n+          break;\n+        case '\"':\n+          inDoubleQuote = true;\n+          inOpt = true;\n+          hasData = true;\n+          break;\n+        case '\\\\':\n+          escapeNext = true;\n+          break;\n+        default:\n+          if (!Character.isWhitespace(c)) {\n+            inOpt = true;\n+            opt.appendCodePoint(c);\n+          }\n+        }\n+      }\n+    }\n+\n+    checkArgument(!inSingleQuote && !inDoubleQuote && !escapeNext, \"Invalid option string: %s\", s);\n+    if (opt.length() > 0 || hasData) {\n+      opts.add(opt.toString());\n+    }\n+    return opts;\n+  }\n+\n+  private void finishOpt(List<String> opts, StringBuilder opt) {\n+    opts.add(opt.toString());\n+    opt.setLength(0);\n+  }\n+\n+  private String findAssembly(String scalaVersion) {\n+    String sparkHome = getSparkHome();\n+    File libdir;\n+    if (new File(sparkHome, \"RELEASE\").isFile()) {\n+      libdir = new File(sparkHome, \"lib\");\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+          libdir.getAbsolutePath());\n+    } else {\n+      libdir = new File(sparkHome, String.format(\"assembly/target/scala-%s\", scalaVersion));\n+    }\n+\n+    final Pattern re = Pattern.compile(\"spark-assembly.*hadoop.*\\\\.jar\");\n+    FileFilter filter = new FileFilter() {\n+      @Override\n+      public boolean accept(File file) {\n+        return file.isFile() && re.matcher(file.getName()).matches();\n+      }\n+    };\n+    File[] assemblies = libdir.listFiles(filter);\n+    checkState(assemblies != null && assemblies.length > 0, \"No assemblies found in '%s'.\", libdir);\n+    checkState(assemblies.length == 1, \"Multiple assemblies found in '%s'.\", libdir);\n+    return assemblies[0].getAbsolutePath();\n+  }\n+\n+  private String getenv(String key) {\n+    return first(env != null ? env.get(key) : null, System.getenv(key));\n+  }\n+\n+  /**\n+   * Prepare a command line for execution on Windows.\n+   *\n+   * Two things need to be done:\n+   *\n+   * - If a custom library path is needed, extend PATH to add it. Based on:\n+   *   http://superuser.com/questions/223104/setting-environment-variable-for-just-one-command-in-windows-cmd-exe\n+   *\n+   * - Quote all arguments so that spaces are handled as expected. Quotes within arguments are\n+   *   \"double quoted\" (which is batch for escaping a quote). This page has more details about\n+   *   quoting and other batch script fun stuff: http://ss64.com/nt/syntax-esc.html\n+   */\n+  private List<String> prepareForWindows(List<String> cmd,\n+      String libPath,\n+      Map<String, String> env) {\n+    StringBuilder cmdline = new StringBuilder(\"cmd /c \\\"\");\n+    if (libPath != null) {\n+      cmdline.append(\"set PATH=%PATH%;\").append(libPath).append(\" &&\");\n+    }\n+    for (Map.Entry<String, String> e : env.entrySet()) {\n+      cmdline.append(String.format(\"set %s=%s\", e.getKey(), e.getValue()));\n+      cmdline.append(\" &&\");\n+    }\n+    for (String arg : cmd) {\n+      if (cmdline.length() > 0) {\n+        cmdline.append(\" \");\n+      }\n+      cmdline.append(quote(arg));\n+    }\n+    cmdline.append(\"\\\"\");\n+    return Arrays.asList(cmdline.toString());\n+  }\n+\n+  /**\n+   * Quoting arguments that don't need quoting in Windows seems to cause weird issues. So only\n+   * quote arguments when there is whitespace in them.\n+   */\n+  private boolean needsQuoting(String arg) {\n+    for (int i = 0; i < arg.length(); i++) {\n+      if (Character.isWhitespace(arg.codePointAt(i))) {\n+        return true;\n+      }\n+    }\n+    return false;\n+  }\n+\n+  private String quote(String arg) {"
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "I think this should be private\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T03:20:01Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "That would look a little weird, since a public class in this package extends it and it provides public APIs. Is there any good reason for making it private?\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T19:47:30Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "Well, we don't expect people to extend this on their own. In general I think we should use the tightest visibility where possible, but in this case it seems that this thing also comes with public methods that the user calls directly. Never mind then.\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T20:51:55Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {"
  }, {
    "author": {
      "login": "sryza"
    },
    "body": "It looks like we're not actually using any attributes of LauncherCommon (slash it doesn't have any).  Can we just do a static import instead of extending it?\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-14T22:17:32Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {"
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "same here, having java docs for these helper methods will significantly speed up a new person trying to understand how the launcher works\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T19:55:47Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    return cmd;\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  protected List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    String confDir = getenv(\"SPARK_CONF_DIR\");\n+    if (!isEmpty(confDir)) {\n+      addToClassPath(cp, confDir);\n+    } else {\n+      addToClassPath(cp, join(File.separator, getSparkHome(), \"conf\"));\n+    }\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars.\n+    //\n+    // This block also serves as a check for SPARK-1703, when the assembly jar is built with\n+    // Java 7 and ends up with too many files, causing issues with other JDK versions.\n+    boolean needsDataNucleus = false;\n+    JarFile assemblyJar = null;\n+    try {\n+      assemblyJar = new JarFile(assembly);\n+      needsDataNucleus = assemblyJar.getEntry(\"org/apache/hadoop/hive/ql/exec/\") != null;\n+    } catch (IOException ioe) {\n+      if (ioe.getMessage().indexOf(\"invalid CEN header\") > 0) {\n+        System.err.println(\n+          \"Loading Spark jar with '$JAR_CMD' failed.\\n\" +\n+          \"This is likely because Spark was compiled with Java 7 and run\\n\" +\n+          \"with Java 6 (see SPARK-1703). Please use Java 7 to run Spark\\n\" +\n+          \"or build Spark with Java 6.\");\n+        System.exit(1);\n+      } else {\n+        throw ioe;\n+      }\n+    } finally {\n+      if (assemblyJar != null) {\n+        try {\n+          assemblyJar.close();\n+        } catch (IOException e) {\n+          // Ignore.\n+        }\n+      }\n+    }\n+\n+    if (needsDataNucleus) {\n+      System.err.println(\"Spark assembly has been built with Hive, including Datanucleus jars \" +\n+        \"in classpath.\");\n+      File libdir;\n+      if (new File(sparkHome, \"RELEASE\").isFile()) {\n+        libdir = new File(sparkHome, \"lib\");\n+      } else {\n+        libdir = new File(sparkHome, \"lib_managed/jars\");\n+      }\n+\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+        libdir.getAbsolutePath());\n+      for (File jar : libdir.listFiles()) {\n+        if (jar.getName().startsWith(\"datanucleus-\")) {\n+          addToClassPath(cp, jar.getAbsolutePath());\n+        }\n+      }\n+    }\n+\n+    addToClassPath(cp, getenv(\"HADOOP_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"YARN_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"SPARK_DIST_CLASSPATH\"));\n+    return cp;\n+  }\n+\n+  private void addToClassPath(List<String> cp, String entries) {\n+    if (isEmpty(entries)) {\n+      return;\n+    }\n+    String[] split = entries.split(Pattern.quote(File.pathSeparator));\n+    for (String entry : split) {\n+      if (!isEmpty(entry)) {\n+        if (new File(entry).isDirectory() && !entry.endsWith(File.separator)) {\n+          entry += File.separator;\n+        }\n+        cp.add(entry);\n+      }\n+    }\n+  }\n+\n+  protected String getScalaVersion() {\n+    String scala = getenv(\"SPARK_SCALA_VERSION\");\n+    if (scala != null) {\n+      return scala;\n+    }\n+\n+    String sparkHome = getSparkHome();\n+    File scala210 = new File(sparkHome, \"assembly/target/scala-2.10\");\n+    File scala211 = new File(sparkHome, \"assembly/target/scala-2.11\");\n+    if (scala210.isDirectory() && scala211.isDirectory()) {\n+      checkState(false,\n+        \"Presence of build for both scala versions (2.10 and 2.11) detected.\\n\" +\n+        \"Either clean one of them or set SPARK_SCALA_VERSION in your environment.\");\n+    } else if (scala210.isDirectory()) {\n+      return \"2.10\";\n+    } else {\n+      checkState(scala211.isDirectory(), \"Cannot find any assembly build directories.\");\n+      return \"2.11\";\n+    }\n+\n+    throw new IllegalStateException(\"Should not reach here.\");\n+  }\n+\n+  /**\n+   * Which OS is running defines two things:\n+   * - the name of the environment variable used to define the lookup path for native libs\n+   * - how to execute the command in general.\n+   *\n+   * The name is easy: PATH on Win32, DYLD_LIBRARY_PATH on MacOS, LD_LIBRARY_PATH elsewhere.\n+   *\n+   * On Unix-like, we're assuming bash is available. So we print one argument per line to\n+   * the output, and use bash's array handling to execute the right thing.\n+   *\n+    * For Win32, see {@link #prepareForWindows(List<String>,String)}.\n+   */\n+  protected List<String> prepareForOs(List<String> cmd,\n+      String libPath,\n+      Map<String, String> env) {\n+\n+    // If SPARK_HOME does not come from the environment, explicitly set it\n+    // in the child's environment.\n+    Map<String, String> childEnv = env;\n+    if (System.getenv(\"SPARK_HOME\") == null && !env.containsKey(\"SPARK_HOME\")) {\n+      childEnv = new HashMap<String, String>(env);\n+      childEnv.put(\"SPARK_HOME\", sparkHome);\n+    }\n+\n+    if (isWindows()) {\n+      return prepareForWindows(cmd, libPath, childEnv);\n+    }\n+\n+    if (isEmpty(libPath) && childEnv.isEmpty()) {\n+      return cmd;\n+    }\n+\n+    List<String> newCmd = new ArrayList<String>();\n+    newCmd.add(\"env\");\n+\n+    if (!isEmpty(libPath)) {\n+      String envName = getLibPathEnvName();\n+      String currEnvValue = getenv(envName);\n+      String newEnvValue = join(File.pathSeparator, currEnvValue, libPath);\n+      newCmd.add(String.format(\"%s=%s\", envName, newEnvValue));\n+    }\n+    for (Map.Entry<String, String> e : childEnv.entrySet()) {\n+      newCmd.add(String.format(\"%s=%s\", e.getKey(), e.getValue()));\n+    }\n+    newCmd.addAll(cmd);\n+    return newCmd;\n+  }\n+\n+  protected String shQuote(String s) {\n+    StringBuilder quoted = new StringBuilder();\n+    boolean hasWhitespace = false;\n+    for (int i = 0; i < s.length(); i++) {\n+      if (Character.isWhitespace(s.codePointAt(i))) {\n+        quoted.append('\"');\n+        hasWhitespace = true;\n+        break;\n+      }\n+    }\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int cp = s.codePointAt(i);\n+      switch (cp) {\n+        case '\\'':\n+          if (hasWhitespace) {\n+            quoted.appendCodePoint(cp);\n+            break;\n+          }\n+        case '\"':\n+        case '\\\\':\n+          quoted.append('\\\\');\n+          // Fall through.\n+        default:\n+          if (Character.isWhitespace(cp)) {\n+            hasWhitespace=true;\n+          }\n+          quoted.appendCodePoint(cp);\n+      }\n+    }\n+    if (hasWhitespace) {\n+      quoted.append('\"');\n+    }\n+    return quoted.toString();\n+  }\n+\n+  // Visible for testing.\n+  List<String> parseOptionString(String s) {"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "it would be good if you can give an example of what the input and the corresponding output look like\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T21:06:58Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    return cmd;\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  protected List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    String confDir = getenv(\"SPARK_CONF_DIR\");\n+    if (!isEmpty(confDir)) {\n+      addToClassPath(cp, confDir);\n+    } else {\n+      addToClassPath(cp, join(File.separator, getSparkHome(), \"conf\"));\n+    }\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars.\n+    //\n+    // This block also serves as a check for SPARK-1703, when the assembly jar is built with\n+    // Java 7 and ends up with too many files, causing issues with other JDK versions.\n+    boolean needsDataNucleus = false;\n+    JarFile assemblyJar = null;\n+    try {\n+      assemblyJar = new JarFile(assembly);\n+      needsDataNucleus = assemblyJar.getEntry(\"org/apache/hadoop/hive/ql/exec/\") != null;\n+    } catch (IOException ioe) {\n+      if (ioe.getMessage().indexOf(\"invalid CEN header\") > 0) {\n+        System.err.println(\n+          \"Loading Spark jar with '$JAR_CMD' failed.\\n\" +\n+          \"This is likely because Spark was compiled with Java 7 and run\\n\" +\n+          \"with Java 6 (see SPARK-1703). Please use Java 7 to run Spark\\n\" +\n+          \"or build Spark with Java 6.\");\n+        System.exit(1);\n+      } else {\n+        throw ioe;\n+      }\n+    } finally {\n+      if (assemblyJar != null) {\n+        try {\n+          assemblyJar.close();\n+        } catch (IOException e) {\n+          // Ignore.\n+        }\n+      }\n+    }\n+\n+    if (needsDataNucleus) {\n+      System.err.println(\"Spark assembly has been built with Hive, including Datanucleus jars \" +\n+        \"in classpath.\");\n+      File libdir;\n+      if (new File(sparkHome, \"RELEASE\").isFile()) {\n+        libdir = new File(sparkHome, \"lib\");\n+      } else {\n+        libdir = new File(sparkHome, \"lib_managed/jars\");\n+      }\n+\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+        libdir.getAbsolutePath());\n+      for (File jar : libdir.listFiles()) {\n+        if (jar.getName().startsWith(\"datanucleus-\")) {\n+          addToClassPath(cp, jar.getAbsolutePath());\n+        }\n+      }\n+    }\n+\n+    addToClassPath(cp, getenv(\"HADOOP_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"YARN_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"SPARK_DIST_CLASSPATH\"));\n+    return cp;\n+  }\n+\n+  private void addToClassPath(List<String> cp, String entries) {\n+    if (isEmpty(entries)) {\n+      return;\n+    }\n+    String[] split = entries.split(Pattern.quote(File.pathSeparator));\n+    for (String entry : split) {\n+      if (!isEmpty(entry)) {\n+        if (new File(entry).isDirectory() && !entry.endsWith(File.separator)) {\n+          entry += File.separator;\n+        }\n+        cp.add(entry);\n+      }\n+    }\n+  }\n+\n+  protected String getScalaVersion() {\n+    String scala = getenv(\"SPARK_SCALA_VERSION\");\n+    if (scala != null) {\n+      return scala;\n+    }\n+\n+    String sparkHome = getSparkHome();\n+    File scala210 = new File(sparkHome, \"assembly/target/scala-2.10\");\n+    File scala211 = new File(sparkHome, \"assembly/target/scala-2.11\");\n+    if (scala210.isDirectory() && scala211.isDirectory()) {\n+      checkState(false,\n+        \"Presence of build for both scala versions (2.10 and 2.11) detected.\\n\" +\n+        \"Either clean one of them or set SPARK_SCALA_VERSION in your environment.\");\n+    } else if (scala210.isDirectory()) {\n+      return \"2.10\";\n+    } else {\n+      checkState(scala211.isDirectory(), \"Cannot find any assembly build directories.\");\n+      return \"2.11\";\n+    }\n+\n+    throw new IllegalStateException(\"Should not reach here.\");\n+  }\n+\n+  /**\n+   * Which OS is running defines two things:\n+   * - the name of the environment variable used to define the lookup path for native libs\n+   * - how to execute the command in general.\n+   *\n+   * The name is easy: PATH on Win32, DYLD_LIBRARY_PATH on MacOS, LD_LIBRARY_PATH elsewhere.\n+   *\n+   * On Unix-like, we're assuming bash is available. So we print one argument per line to\n+   * the output, and use bash's array handling to execute the right thing.\n+   *\n+    * For Win32, see {@link #prepareForWindows(List<String>,String)}.\n+   */\n+  protected List<String> prepareForOs(List<String> cmd,\n+      String libPath,\n+      Map<String, String> env) {\n+\n+    // If SPARK_HOME does not come from the environment, explicitly set it\n+    // in the child's environment.\n+    Map<String, String> childEnv = env;\n+    if (System.getenv(\"SPARK_HOME\") == null && !env.containsKey(\"SPARK_HOME\")) {\n+      childEnv = new HashMap<String, String>(env);\n+      childEnv.put(\"SPARK_HOME\", sparkHome);\n+    }\n+\n+    if (isWindows()) {\n+      return prepareForWindows(cmd, libPath, childEnv);\n+    }\n+\n+    if (isEmpty(libPath) && childEnv.isEmpty()) {\n+      return cmd;\n+    }\n+\n+    List<String> newCmd = new ArrayList<String>();\n+    newCmd.add(\"env\");\n+\n+    if (!isEmpty(libPath)) {\n+      String envName = getLibPathEnvName();\n+      String currEnvValue = getenv(envName);\n+      String newEnvValue = join(File.pathSeparator, currEnvValue, libPath);\n+      newCmd.add(String.format(\"%s=%s\", envName, newEnvValue));\n+    }\n+    for (Map.Entry<String, String> e : childEnv.entrySet()) {\n+      newCmd.add(String.format(\"%s=%s\", e.getKey(), e.getValue()));\n+    }\n+    newCmd.addAll(cmd);\n+    return newCmd;\n+  }\n+\n+  protected String shQuote(String s) {\n+    StringBuilder quoted = new StringBuilder();\n+    boolean hasWhitespace = false;\n+    for (int i = 0; i < s.length(); i++) {\n+      if (Character.isWhitespace(s.codePointAt(i))) {\n+        quoted.append('\"');\n+        hasWhitespace = true;\n+        break;\n+      }\n+    }\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int cp = s.codePointAt(i);\n+      switch (cp) {\n+        case '\\'':\n+          if (hasWhitespace) {\n+            quoted.appendCodePoint(cp);\n+            break;\n+          }\n+        case '\"':\n+        case '\\\\':\n+          quoted.append('\\\\');\n+          // Fall through.\n+        default:\n+          if (Character.isWhitespace(cp)) {\n+            hasWhitespace=true;\n+          }\n+          quoted.appendCodePoint(cp);\n+      }\n+    }\n+    if (hasWhitespace) {\n+      quoted.append('\"');\n+    }\n+    return quoted.toString();\n+  }\n+\n+  // Visible for testing.\n+  List<String> parseOptionString(String s) {"
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "you can just do `inOpt = true` here\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T21:00:20Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    return cmd;\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  protected List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    String confDir = getenv(\"SPARK_CONF_DIR\");\n+    if (!isEmpty(confDir)) {\n+      addToClassPath(cp, confDir);\n+    } else {\n+      addToClassPath(cp, join(File.separator, getSparkHome(), \"conf\"));\n+    }\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars.\n+    //\n+    // This block also serves as a check for SPARK-1703, when the assembly jar is built with\n+    // Java 7 and ends up with too many files, causing issues with other JDK versions.\n+    boolean needsDataNucleus = false;\n+    JarFile assemblyJar = null;\n+    try {\n+      assemblyJar = new JarFile(assembly);\n+      needsDataNucleus = assemblyJar.getEntry(\"org/apache/hadoop/hive/ql/exec/\") != null;\n+    } catch (IOException ioe) {\n+      if (ioe.getMessage().indexOf(\"invalid CEN header\") > 0) {\n+        System.err.println(\n+          \"Loading Spark jar with '$JAR_CMD' failed.\\n\" +\n+          \"This is likely because Spark was compiled with Java 7 and run\\n\" +\n+          \"with Java 6 (see SPARK-1703). Please use Java 7 to run Spark\\n\" +\n+          \"or build Spark with Java 6.\");\n+        System.exit(1);\n+      } else {\n+        throw ioe;\n+      }\n+    } finally {\n+      if (assemblyJar != null) {\n+        try {\n+          assemblyJar.close();\n+        } catch (IOException e) {\n+          // Ignore.\n+        }\n+      }\n+    }\n+\n+    if (needsDataNucleus) {\n+      System.err.println(\"Spark assembly has been built with Hive, including Datanucleus jars \" +\n+        \"in classpath.\");\n+      File libdir;\n+      if (new File(sparkHome, \"RELEASE\").isFile()) {\n+        libdir = new File(sparkHome, \"lib\");\n+      } else {\n+        libdir = new File(sparkHome, \"lib_managed/jars\");\n+      }\n+\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+        libdir.getAbsolutePath());\n+      for (File jar : libdir.listFiles()) {\n+        if (jar.getName().startsWith(\"datanucleus-\")) {\n+          addToClassPath(cp, jar.getAbsolutePath());\n+        }\n+      }\n+    }\n+\n+    addToClassPath(cp, getenv(\"HADOOP_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"YARN_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"SPARK_DIST_CLASSPATH\"));\n+    return cp;\n+  }\n+\n+  private void addToClassPath(List<String> cp, String entries) {\n+    if (isEmpty(entries)) {\n+      return;\n+    }\n+    String[] split = entries.split(Pattern.quote(File.pathSeparator));\n+    for (String entry : split) {\n+      if (!isEmpty(entry)) {\n+        if (new File(entry).isDirectory() && !entry.endsWith(File.separator)) {\n+          entry += File.separator;\n+        }\n+        cp.add(entry);\n+      }\n+    }\n+  }\n+\n+  protected String getScalaVersion() {\n+    String scala = getenv(\"SPARK_SCALA_VERSION\");\n+    if (scala != null) {\n+      return scala;\n+    }\n+\n+    String sparkHome = getSparkHome();\n+    File scala210 = new File(sparkHome, \"assembly/target/scala-2.10\");\n+    File scala211 = new File(sparkHome, \"assembly/target/scala-2.11\");\n+    if (scala210.isDirectory() && scala211.isDirectory()) {\n+      checkState(false,\n+        \"Presence of build for both scala versions (2.10 and 2.11) detected.\\n\" +\n+        \"Either clean one of them or set SPARK_SCALA_VERSION in your environment.\");\n+    } else if (scala210.isDirectory()) {\n+      return \"2.10\";\n+    } else {\n+      checkState(scala211.isDirectory(), \"Cannot find any assembly build directories.\");\n+      return \"2.11\";\n+    }\n+\n+    throw new IllegalStateException(\"Should not reach here.\");\n+  }\n+\n+  /**\n+   * Which OS is running defines two things:\n+   * - the name of the environment variable used to define the lookup path for native libs\n+   * - how to execute the command in general.\n+   *\n+   * The name is easy: PATH on Win32, DYLD_LIBRARY_PATH on MacOS, LD_LIBRARY_PATH elsewhere.\n+   *\n+   * On Unix-like, we're assuming bash is available. So we print one argument per line to\n+   * the output, and use bash's array handling to execute the right thing.\n+   *\n+    * For Win32, see {@link #prepareForWindows(List<String>,String)}.\n+   */\n+  protected List<String> prepareForOs(List<String> cmd,\n+      String libPath,\n+      Map<String, String> env) {\n+\n+    // If SPARK_HOME does not come from the environment, explicitly set it\n+    // in the child's environment.\n+    Map<String, String> childEnv = env;\n+    if (System.getenv(\"SPARK_HOME\") == null && !env.containsKey(\"SPARK_HOME\")) {\n+      childEnv = new HashMap<String, String>(env);\n+      childEnv.put(\"SPARK_HOME\", sparkHome);\n+    }\n+\n+    if (isWindows()) {\n+      return prepareForWindows(cmd, libPath, childEnv);\n+    }\n+\n+    if (isEmpty(libPath) && childEnv.isEmpty()) {\n+      return cmd;\n+    }\n+\n+    List<String> newCmd = new ArrayList<String>();\n+    newCmd.add(\"env\");\n+\n+    if (!isEmpty(libPath)) {\n+      String envName = getLibPathEnvName();\n+      String currEnvValue = getenv(envName);\n+      String newEnvValue = join(File.pathSeparator, currEnvValue, libPath);\n+      newCmd.add(String.format(\"%s=%s\", envName, newEnvValue));\n+    }\n+    for (Map.Entry<String, String> e : childEnv.entrySet()) {\n+      newCmd.add(String.format(\"%s=%s\", e.getKey(), e.getValue()));\n+    }\n+    newCmd.addAll(cmd);\n+    return newCmd;\n+  }\n+\n+  protected String shQuote(String s) {\n+    StringBuilder quoted = new StringBuilder();\n+    boolean hasWhitespace = false;\n+    for (int i = 0; i < s.length(); i++) {\n+      if (Character.isWhitespace(s.codePointAt(i))) {\n+        quoted.append('\"');\n+        hasWhitespace = true;\n+        break;\n+      }\n+    }\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int cp = s.codePointAt(i);\n+      switch (cp) {\n+        case '\\'':\n+          if (hasWhitespace) {\n+            quoted.appendCodePoint(cp);\n+            break;\n+          }\n+        case '\"':\n+        case '\\\\':\n+          quoted.append('\\\\');\n+          // Fall through.\n+        default:\n+          if (Character.isWhitespace(cp)) {\n+            hasWhitespace=true;\n+          }\n+          quoted.appendCodePoint(cp);\n+      }\n+    }\n+    if (hasWhitespace) {\n+      quoted.append('\"');\n+    }\n+    return quoted.toString();\n+  }\n+\n+  // Visible for testing.\n+  List<String> parseOptionString(String s) {\n+    List<String> opts = new ArrayList<String>();\n+    StringBuilder opt = new StringBuilder();\n+    boolean inOpt = false;\n+    boolean inSingleQuote = false;\n+    boolean inDoubleQuote = false;\n+    boolean escapeNext = false;\n+    boolean hasData = false;\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int c = s.codePointAt(i);\n+      if (escapeNext) {\n+        if (!inOpt) {\n+          inOpt = true;\n+        }"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "actually I'm not sure why this is set here. Since this method seems general enough to be used beyond java opts, you might have an option that ends with a backslash (i.e. you might have `\\\\_` two backslashes followed by a space, in which case we're not `inOpt` anymore right?). Though this is probably a corner case we'll never hit.\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T21:17:49Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    return cmd;\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  protected List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    String confDir = getenv(\"SPARK_CONF_DIR\");\n+    if (!isEmpty(confDir)) {\n+      addToClassPath(cp, confDir);\n+    } else {\n+      addToClassPath(cp, join(File.separator, getSparkHome(), \"conf\"));\n+    }\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars.\n+    //\n+    // This block also serves as a check for SPARK-1703, when the assembly jar is built with\n+    // Java 7 and ends up with too many files, causing issues with other JDK versions.\n+    boolean needsDataNucleus = false;\n+    JarFile assemblyJar = null;\n+    try {\n+      assemblyJar = new JarFile(assembly);\n+      needsDataNucleus = assemblyJar.getEntry(\"org/apache/hadoop/hive/ql/exec/\") != null;\n+    } catch (IOException ioe) {\n+      if (ioe.getMessage().indexOf(\"invalid CEN header\") > 0) {\n+        System.err.println(\n+          \"Loading Spark jar with '$JAR_CMD' failed.\\n\" +\n+          \"This is likely because Spark was compiled with Java 7 and run\\n\" +\n+          \"with Java 6 (see SPARK-1703). Please use Java 7 to run Spark\\n\" +\n+          \"or build Spark with Java 6.\");\n+        System.exit(1);\n+      } else {\n+        throw ioe;\n+      }\n+    } finally {\n+      if (assemblyJar != null) {\n+        try {\n+          assemblyJar.close();\n+        } catch (IOException e) {\n+          // Ignore.\n+        }\n+      }\n+    }\n+\n+    if (needsDataNucleus) {\n+      System.err.println(\"Spark assembly has been built with Hive, including Datanucleus jars \" +\n+        \"in classpath.\");\n+      File libdir;\n+      if (new File(sparkHome, \"RELEASE\").isFile()) {\n+        libdir = new File(sparkHome, \"lib\");\n+      } else {\n+        libdir = new File(sparkHome, \"lib_managed/jars\");\n+      }\n+\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+        libdir.getAbsolutePath());\n+      for (File jar : libdir.listFiles()) {\n+        if (jar.getName().startsWith(\"datanucleus-\")) {\n+          addToClassPath(cp, jar.getAbsolutePath());\n+        }\n+      }\n+    }\n+\n+    addToClassPath(cp, getenv(\"HADOOP_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"YARN_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"SPARK_DIST_CLASSPATH\"));\n+    return cp;\n+  }\n+\n+  private void addToClassPath(List<String> cp, String entries) {\n+    if (isEmpty(entries)) {\n+      return;\n+    }\n+    String[] split = entries.split(Pattern.quote(File.pathSeparator));\n+    for (String entry : split) {\n+      if (!isEmpty(entry)) {\n+        if (new File(entry).isDirectory() && !entry.endsWith(File.separator)) {\n+          entry += File.separator;\n+        }\n+        cp.add(entry);\n+      }\n+    }\n+  }\n+\n+  protected String getScalaVersion() {\n+    String scala = getenv(\"SPARK_SCALA_VERSION\");\n+    if (scala != null) {\n+      return scala;\n+    }\n+\n+    String sparkHome = getSparkHome();\n+    File scala210 = new File(sparkHome, \"assembly/target/scala-2.10\");\n+    File scala211 = new File(sparkHome, \"assembly/target/scala-2.11\");\n+    if (scala210.isDirectory() && scala211.isDirectory()) {\n+      checkState(false,\n+        \"Presence of build for both scala versions (2.10 and 2.11) detected.\\n\" +\n+        \"Either clean one of them or set SPARK_SCALA_VERSION in your environment.\");\n+    } else if (scala210.isDirectory()) {\n+      return \"2.10\";\n+    } else {\n+      checkState(scala211.isDirectory(), \"Cannot find any assembly build directories.\");\n+      return \"2.11\";\n+    }\n+\n+    throw new IllegalStateException(\"Should not reach here.\");\n+  }\n+\n+  /**\n+   * Which OS is running defines two things:\n+   * - the name of the environment variable used to define the lookup path for native libs\n+   * - how to execute the command in general.\n+   *\n+   * The name is easy: PATH on Win32, DYLD_LIBRARY_PATH on MacOS, LD_LIBRARY_PATH elsewhere.\n+   *\n+   * On Unix-like, we're assuming bash is available. So we print one argument per line to\n+   * the output, and use bash's array handling to execute the right thing.\n+   *\n+    * For Win32, see {@link #prepareForWindows(List<String>,String)}.\n+   */\n+  protected List<String> prepareForOs(List<String> cmd,\n+      String libPath,\n+      Map<String, String> env) {\n+\n+    // If SPARK_HOME does not come from the environment, explicitly set it\n+    // in the child's environment.\n+    Map<String, String> childEnv = env;\n+    if (System.getenv(\"SPARK_HOME\") == null && !env.containsKey(\"SPARK_HOME\")) {\n+      childEnv = new HashMap<String, String>(env);\n+      childEnv.put(\"SPARK_HOME\", sparkHome);\n+    }\n+\n+    if (isWindows()) {\n+      return prepareForWindows(cmd, libPath, childEnv);\n+    }\n+\n+    if (isEmpty(libPath) && childEnv.isEmpty()) {\n+      return cmd;\n+    }\n+\n+    List<String> newCmd = new ArrayList<String>();\n+    newCmd.add(\"env\");\n+\n+    if (!isEmpty(libPath)) {\n+      String envName = getLibPathEnvName();\n+      String currEnvValue = getenv(envName);\n+      String newEnvValue = join(File.pathSeparator, currEnvValue, libPath);\n+      newCmd.add(String.format(\"%s=%s\", envName, newEnvValue));\n+    }\n+    for (Map.Entry<String, String> e : childEnv.entrySet()) {\n+      newCmd.add(String.format(\"%s=%s\", e.getKey(), e.getValue()));\n+    }\n+    newCmd.addAll(cmd);\n+    return newCmd;\n+  }\n+\n+  protected String shQuote(String s) {\n+    StringBuilder quoted = new StringBuilder();\n+    boolean hasWhitespace = false;\n+    for (int i = 0; i < s.length(); i++) {\n+      if (Character.isWhitespace(s.codePointAt(i))) {\n+        quoted.append('\"');\n+        hasWhitespace = true;\n+        break;\n+      }\n+    }\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int cp = s.codePointAt(i);\n+      switch (cp) {\n+        case '\\'':\n+          if (hasWhitespace) {\n+            quoted.appendCodePoint(cp);\n+            break;\n+          }\n+        case '\"':\n+        case '\\\\':\n+          quoted.append('\\\\');\n+          // Fall through.\n+        default:\n+          if (Character.isWhitespace(cp)) {\n+            hasWhitespace=true;\n+          }\n+          quoted.appendCodePoint(cp);\n+      }\n+    }\n+    if (hasWhitespace) {\n+      quoted.append('\"');\n+    }\n+    return quoted.toString();\n+  }\n+\n+  // Visible for testing.\n+  List<String> parseOptionString(String s) {\n+    List<String> opts = new ArrayList<String>();\n+    StringBuilder opt = new StringBuilder();\n+    boolean inOpt = false;\n+    boolean inSingleQuote = false;\n+    boolean inDoubleQuote = false;\n+    boolean escapeNext = false;\n+    boolean hasData = false;\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int c = s.codePointAt(i);\n+      if (escapeNext) {\n+        if (!inOpt) {\n+          inOpt = true;\n+        }"
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "this probably doesn't need to be its own method\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T21:11:38Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    return cmd;\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  protected List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    String confDir = getenv(\"SPARK_CONF_DIR\");\n+    if (!isEmpty(confDir)) {\n+      addToClassPath(cp, confDir);\n+    } else {\n+      addToClassPath(cp, join(File.separator, getSparkHome(), \"conf\"));\n+    }\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars.\n+    //\n+    // This block also serves as a check for SPARK-1703, when the assembly jar is built with\n+    // Java 7 and ends up with too many files, causing issues with other JDK versions.\n+    boolean needsDataNucleus = false;\n+    JarFile assemblyJar = null;\n+    try {\n+      assemblyJar = new JarFile(assembly);\n+      needsDataNucleus = assemblyJar.getEntry(\"org/apache/hadoop/hive/ql/exec/\") != null;\n+    } catch (IOException ioe) {\n+      if (ioe.getMessage().indexOf(\"invalid CEN header\") > 0) {\n+        System.err.println(\n+          \"Loading Spark jar with '$JAR_CMD' failed.\\n\" +\n+          \"This is likely because Spark was compiled with Java 7 and run\\n\" +\n+          \"with Java 6 (see SPARK-1703). Please use Java 7 to run Spark\\n\" +\n+          \"or build Spark with Java 6.\");\n+        System.exit(1);\n+      } else {\n+        throw ioe;\n+      }\n+    } finally {\n+      if (assemblyJar != null) {\n+        try {\n+          assemblyJar.close();\n+        } catch (IOException e) {\n+          // Ignore.\n+        }\n+      }\n+    }\n+\n+    if (needsDataNucleus) {\n+      System.err.println(\"Spark assembly has been built with Hive, including Datanucleus jars \" +\n+        \"in classpath.\");\n+      File libdir;\n+      if (new File(sparkHome, \"RELEASE\").isFile()) {\n+        libdir = new File(sparkHome, \"lib\");\n+      } else {\n+        libdir = new File(sparkHome, \"lib_managed/jars\");\n+      }\n+\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+        libdir.getAbsolutePath());\n+      for (File jar : libdir.listFiles()) {\n+        if (jar.getName().startsWith(\"datanucleus-\")) {\n+          addToClassPath(cp, jar.getAbsolutePath());\n+        }\n+      }\n+    }\n+\n+    addToClassPath(cp, getenv(\"HADOOP_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"YARN_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"SPARK_DIST_CLASSPATH\"));\n+    return cp;\n+  }\n+\n+  private void addToClassPath(List<String> cp, String entries) {\n+    if (isEmpty(entries)) {\n+      return;\n+    }\n+    String[] split = entries.split(Pattern.quote(File.pathSeparator));\n+    for (String entry : split) {\n+      if (!isEmpty(entry)) {\n+        if (new File(entry).isDirectory() && !entry.endsWith(File.separator)) {\n+          entry += File.separator;\n+        }\n+        cp.add(entry);\n+      }\n+    }\n+  }\n+\n+  protected String getScalaVersion() {\n+    String scala = getenv(\"SPARK_SCALA_VERSION\");\n+    if (scala != null) {\n+      return scala;\n+    }\n+\n+    String sparkHome = getSparkHome();\n+    File scala210 = new File(sparkHome, \"assembly/target/scala-2.10\");\n+    File scala211 = new File(sparkHome, \"assembly/target/scala-2.11\");\n+    if (scala210.isDirectory() && scala211.isDirectory()) {\n+      checkState(false,\n+        \"Presence of build for both scala versions (2.10 and 2.11) detected.\\n\" +\n+        \"Either clean one of them or set SPARK_SCALA_VERSION in your environment.\");\n+    } else if (scala210.isDirectory()) {\n+      return \"2.10\";\n+    } else {\n+      checkState(scala211.isDirectory(), \"Cannot find any assembly build directories.\");\n+      return \"2.11\";\n+    }\n+\n+    throw new IllegalStateException(\"Should not reach here.\");\n+  }\n+\n+  /**\n+   * Which OS is running defines two things:\n+   * - the name of the environment variable used to define the lookup path for native libs\n+   * - how to execute the command in general.\n+   *\n+   * The name is easy: PATH on Win32, DYLD_LIBRARY_PATH on MacOS, LD_LIBRARY_PATH elsewhere.\n+   *\n+   * On Unix-like, we're assuming bash is available. So we print one argument per line to\n+   * the output, and use bash's array handling to execute the right thing.\n+   *\n+    * For Win32, see {@link #prepareForWindows(List<String>,String)}.\n+   */\n+  protected List<String> prepareForOs(List<String> cmd,\n+      String libPath,\n+      Map<String, String> env) {\n+\n+    // If SPARK_HOME does not come from the environment, explicitly set it\n+    // in the child's environment.\n+    Map<String, String> childEnv = env;\n+    if (System.getenv(\"SPARK_HOME\") == null && !env.containsKey(\"SPARK_HOME\")) {\n+      childEnv = new HashMap<String, String>(env);\n+      childEnv.put(\"SPARK_HOME\", sparkHome);\n+    }\n+\n+    if (isWindows()) {\n+      return prepareForWindows(cmd, libPath, childEnv);\n+    }\n+\n+    if (isEmpty(libPath) && childEnv.isEmpty()) {\n+      return cmd;\n+    }\n+\n+    List<String> newCmd = new ArrayList<String>();\n+    newCmd.add(\"env\");\n+\n+    if (!isEmpty(libPath)) {\n+      String envName = getLibPathEnvName();\n+      String currEnvValue = getenv(envName);\n+      String newEnvValue = join(File.pathSeparator, currEnvValue, libPath);\n+      newCmd.add(String.format(\"%s=%s\", envName, newEnvValue));\n+    }\n+    for (Map.Entry<String, String> e : childEnv.entrySet()) {\n+      newCmd.add(String.format(\"%s=%s\", e.getKey(), e.getValue()));\n+    }\n+    newCmd.addAll(cmd);\n+    return newCmd;\n+  }\n+\n+  protected String shQuote(String s) {\n+    StringBuilder quoted = new StringBuilder();\n+    boolean hasWhitespace = false;\n+    for (int i = 0; i < s.length(); i++) {\n+      if (Character.isWhitespace(s.codePointAt(i))) {\n+        quoted.append('\"');\n+        hasWhitespace = true;\n+        break;\n+      }\n+    }\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int cp = s.codePointAt(i);\n+      switch (cp) {\n+        case '\\'':\n+          if (hasWhitespace) {\n+            quoted.appendCodePoint(cp);\n+            break;\n+          }\n+        case '\"':\n+        case '\\\\':\n+          quoted.append('\\\\');\n+          // Fall through.\n+        default:\n+          if (Character.isWhitespace(cp)) {\n+            hasWhitespace=true;\n+          }\n+          quoted.appendCodePoint(cp);\n+      }\n+    }\n+    if (hasWhitespace) {\n+      quoted.append('\"');\n+    }\n+    return quoted.toString();\n+  }\n+\n+  // Visible for testing.\n+  List<String> parseOptionString(String s) {\n+    List<String> opts = new ArrayList<String>();\n+    StringBuilder opt = new StringBuilder();\n+    boolean inOpt = false;\n+    boolean inSingleQuote = false;\n+    boolean inDoubleQuote = false;\n+    boolean escapeNext = false;\n+    boolean hasData = false;\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int c = s.codePointAt(i);\n+      if (escapeNext) {\n+        if (!inOpt) {\n+          inOpt = true;\n+        }\n+        opt.appendCodePoint(c);\n+        escapeNext = false;\n+      } else if (inOpt) {\n+        switch (c) {\n+        case '\\\\':\n+          if (inSingleQuote) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            escapeNext = true;\n+          }\n+          break;\n+        case '\\'':\n+          if (inDoubleQuote) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            inSingleQuote = !inSingleQuote;\n+          }\n+          break;\n+        case '\"':\n+          if (inSingleQuote) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            inDoubleQuote = !inDoubleQuote;\n+          }\n+          break;\n+        default:\n+          if (inSingleQuote || inDoubleQuote || !Character.isWhitespace(c)) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            finishOpt(opts, opt);\n+            inOpt = false;\n+            hasData = false;\n+          }\n+        }\n+      } else {\n+        switch (c) {\n+        case '\\'':\n+          inSingleQuote = true;\n+          inOpt = true;\n+          hasData = true;\n+          break;\n+        case '\"':\n+          inDoubleQuote = true;\n+          inOpt = true;\n+          hasData = true;\n+          break;\n+        case '\\\\':\n+          escapeNext = true;\n+          break;\n+        default:\n+          if (!Character.isWhitespace(c)) {\n+            inOpt = true;\n+            opt.appendCodePoint(c);\n+          }\n+        }\n+      }\n+    }\n+\n+    checkArgument(!inSingleQuote && !inDoubleQuote && !escapeNext, \"Invalid option string: %s\", s);\n+    if (opt.length() > 0 || hasData) {\n+      opts.add(opt.toString());\n+    }\n+    return opts;\n+  }\n+\n+  private void finishOpt(List<String> opts, StringBuilder opt) {"
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "do we need to check `hasData` here? Would the behavior be the same if we just checked for `opt.length() > 0` here?\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T21:27:33Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    return cmd;\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  protected List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    String confDir = getenv(\"SPARK_CONF_DIR\");\n+    if (!isEmpty(confDir)) {\n+      addToClassPath(cp, confDir);\n+    } else {\n+      addToClassPath(cp, join(File.separator, getSparkHome(), \"conf\"));\n+    }\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars.\n+    //\n+    // This block also serves as a check for SPARK-1703, when the assembly jar is built with\n+    // Java 7 and ends up with too many files, causing issues with other JDK versions.\n+    boolean needsDataNucleus = false;\n+    JarFile assemblyJar = null;\n+    try {\n+      assemblyJar = new JarFile(assembly);\n+      needsDataNucleus = assemblyJar.getEntry(\"org/apache/hadoop/hive/ql/exec/\") != null;\n+    } catch (IOException ioe) {\n+      if (ioe.getMessage().indexOf(\"invalid CEN header\") > 0) {\n+        System.err.println(\n+          \"Loading Spark jar with '$JAR_CMD' failed.\\n\" +\n+          \"This is likely because Spark was compiled with Java 7 and run\\n\" +\n+          \"with Java 6 (see SPARK-1703). Please use Java 7 to run Spark\\n\" +\n+          \"or build Spark with Java 6.\");\n+        System.exit(1);\n+      } else {\n+        throw ioe;\n+      }\n+    } finally {\n+      if (assemblyJar != null) {\n+        try {\n+          assemblyJar.close();\n+        } catch (IOException e) {\n+          // Ignore.\n+        }\n+      }\n+    }\n+\n+    if (needsDataNucleus) {\n+      System.err.println(\"Spark assembly has been built with Hive, including Datanucleus jars \" +\n+        \"in classpath.\");\n+      File libdir;\n+      if (new File(sparkHome, \"RELEASE\").isFile()) {\n+        libdir = new File(sparkHome, \"lib\");\n+      } else {\n+        libdir = new File(sparkHome, \"lib_managed/jars\");\n+      }\n+\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+        libdir.getAbsolutePath());\n+      for (File jar : libdir.listFiles()) {\n+        if (jar.getName().startsWith(\"datanucleus-\")) {\n+          addToClassPath(cp, jar.getAbsolutePath());\n+        }\n+      }\n+    }\n+\n+    addToClassPath(cp, getenv(\"HADOOP_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"YARN_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"SPARK_DIST_CLASSPATH\"));\n+    return cp;\n+  }\n+\n+  private void addToClassPath(List<String> cp, String entries) {\n+    if (isEmpty(entries)) {\n+      return;\n+    }\n+    String[] split = entries.split(Pattern.quote(File.pathSeparator));\n+    for (String entry : split) {\n+      if (!isEmpty(entry)) {\n+        if (new File(entry).isDirectory() && !entry.endsWith(File.separator)) {\n+          entry += File.separator;\n+        }\n+        cp.add(entry);\n+      }\n+    }\n+  }\n+\n+  protected String getScalaVersion() {\n+    String scala = getenv(\"SPARK_SCALA_VERSION\");\n+    if (scala != null) {\n+      return scala;\n+    }\n+\n+    String sparkHome = getSparkHome();\n+    File scala210 = new File(sparkHome, \"assembly/target/scala-2.10\");\n+    File scala211 = new File(sparkHome, \"assembly/target/scala-2.11\");\n+    if (scala210.isDirectory() && scala211.isDirectory()) {\n+      checkState(false,\n+        \"Presence of build for both scala versions (2.10 and 2.11) detected.\\n\" +\n+        \"Either clean one of them or set SPARK_SCALA_VERSION in your environment.\");\n+    } else if (scala210.isDirectory()) {\n+      return \"2.10\";\n+    } else {\n+      checkState(scala211.isDirectory(), \"Cannot find any assembly build directories.\");\n+      return \"2.11\";\n+    }\n+\n+    throw new IllegalStateException(\"Should not reach here.\");\n+  }\n+\n+  /**\n+   * Which OS is running defines two things:\n+   * - the name of the environment variable used to define the lookup path for native libs\n+   * - how to execute the command in general.\n+   *\n+   * The name is easy: PATH on Win32, DYLD_LIBRARY_PATH on MacOS, LD_LIBRARY_PATH elsewhere.\n+   *\n+   * On Unix-like, we're assuming bash is available. So we print one argument per line to\n+   * the output, and use bash's array handling to execute the right thing.\n+   *\n+    * For Win32, see {@link #prepareForWindows(List<String>,String)}.\n+   */\n+  protected List<String> prepareForOs(List<String> cmd,\n+      String libPath,\n+      Map<String, String> env) {\n+\n+    // If SPARK_HOME does not come from the environment, explicitly set it\n+    // in the child's environment.\n+    Map<String, String> childEnv = env;\n+    if (System.getenv(\"SPARK_HOME\") == null && !env.containsKey(\"SPARK_HOME\")) {\n+      childEnv = new HashMap<String, String>(env);\n+      childEnv.put(\"SPARK_HOME\", sparkHome);\n+    }\n+\n+    if (isWindows()) {\n+      return prepareForWindows(cmd, libPath, childEnv);\n+    }\n+\n+    if (isEmpty(libPath) && childEnv.isEmpty()) {\n+      return cmd;\n+    }\n+\n+    List<String> newCmd = new ArrayList<String>();\n+    newCmd.add(\"env\");\n+\n+    if (!isEmpty(libPath)) {\n+      String envName = getLibPathEnvName();\n+      String currEnvValue = getenv(envName);\n+      String newEnvValue = join(File.pathSeparator, currEnvValue, libPath);\n+      newCmd.add(String.format(\"%s=%s\", envName, newEnvValue));\n+    }\n+    for (Map.Entry<String, String> e : childEnv.entrySet()) {\n+      newCmd.add(String.format(\"%s=%s\", e.getKey(), e.getValue()));\n+    }\n+    newCmd.addAll(cmd);\n+    return newCmd;\n+  }\n+\n+  protected String shQuote(String s) {\n+    StringBuilder quoted = new StringBuilder();\n+    boolean hasWhitespace = false;\n+    for (int i = 0; i < s.length(); i++) {\n+      if (Character.isWhitespace(s.codePointAt(i))) {\n+        quoted.append('\"');\n+        hasWhitespace = true;\n+        break;\n+      }\n+    }\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int cp = s.codePointAt(i);\n+      switch (cp) {\n+        case '\\'':\n+          if (hasWhitespace) {\n+            quoted.appendCodePoint(cp);\n+            break;\n+          }\n+        case '\"':\n+        case '\\\\':\n+          quoted.append('\\\\');\n+          // Fall through.\n+        default:\n+          if (Character.isWhitespace(cp)) {\n+            hasWhitespace=true;\n+          }\n+          quoted.appendCodePoint(cp);\n+      }\n+    }\n+    if (hasWhitespace) {\n+      quoted.append('\"');\n+    }\n+    return quoted.toString();\n+  }\n+\n+  // Visible for testing.\n+  List<String> parseOptionString(String s) {\n+    List<String> opts = new ArrayList<String>();\n+    StringBuilder opt = new StringBuilder();\n+    boolean inOpt = false;\n+    boolean inSingleQuote = false;\n+    boolean inDoubleQuote = false;\n+    boolean escapeNext = false;\n+    boolean hasData = false;\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int c = s.codePointAt(i);\n+      if (escapeNext) {\n+        if (!inOpt) {\n+          inOpt = true;\n+        }\n+        opt.appendCodePoint(c);\n+        escapeNext = false;\n+      } else if (inOpt) {\n+        switch (c) {\n+        case '\\\\':\n+          if (inSingleQuote) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            escapeNext = true;\n+          }\n+          break;\n+        case '\\'':\n+          if (inDoubleQuote) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            inSingleQuote = !inSingleQuote;\n+          }\n+          break;\n+        case '\"':\n+          if (inSingleQuote) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            inDoubleQuote = !inDoubleQuote;\n+          }\n+          break;\n+        default:\n+          if (inSingleQuote || inDoubleQuote || !Character.isWhitespace(c)) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            finishOpt(opts, opt);\n+            inOpt = false;\n+            hasData = false;\n+          }\n+        }\n+      } else {\n+        switch (c) {\n+        case '\\'':\n+          inSingleQuote = true;\n+          inOpt = true;\n+          hasData = true;\n+          break;\n+        case '\"':\n+          inDoubleQuote = true;\n+          inOpt = true;\n+          hasData = true;\n+          break;\n+        case '\\\\':\n+          escapeNext = true;\n+          break;\n+        default:\n+          if (!Character.isWhitespace(c)) {\n+            inOpt = true;\n+            opt.appendCodePoint(c);\n+          }\n+        }\n+      }\n+    }\n+\n+    checkArgument(!inSingleQuote && !inDoubleQuote && !escapeNext, \"Invalid option string: %s\", s);\n+    if (opt.length() > 0 || hasData) {"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "If the input is `''` or `\"\"`, `opt.length()` is zero, but there's an actual argument there (the empty string).\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T21:34:03Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    return cmd;\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  protected List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    String confDir = getenv(\"SPARK_CONF_DIR\");\n+    if (!isEmpty(confDir)) {\n+      addToClassPath(cp, confDir);\n+    } else {\n+      addToClassPath(cp, join(File.separator, getSparkHome(), \"conf\"));\n+    }\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars.\n+    //\n+    // This block also serves as a check for SPARK-1703, when the assembly jar is built with\n+    // Java 7 and ends up with too many files, causing issues with other JDK versions.\n+    boolean needsDataNucleus = false;\n+    JarFile assemblyJar = null;\n+    try {\n+      assemblyJar = new JarFile(assembly);\n+      needsDataNucleus = assemblyJar.getEntry(\"org/apache/hadoop/hive/ql/exec/\") != null;\n+    } catch (IOException ioe) {\n+      if (ioe.getMessage().indexOf(\"invalid CEN header\") > 0) {\n+        System.err.println(\n+          \"Loading Spark jar with '$JAR_CMD' failed.\\n\" +\n+          \"This is likely because Spark was compiled with Java 7 and run\\n\" +\n+          \"with Java 6 (see SPARK-1703). Please use Java 7 to run Spark\\n\" +\n+          \"or build Spark with Java 6.\");\n+        System.exit(1);\n+      } else {\n+        throw ioe;\n+      }\n+    } finally {\n+      if (assemblyJar != null) {\n+        try {\n+          assemblyJar.close();\n+        } catch (IOException e) {\n+          // Ignore.\n+        }\n+      }\n+    }\n+\n+    if (needsDataNucleus) {\n+      System.err.println(\"Spark assembly has been built with Hive, including Datanucleus jars \" +\n+        \"in classpath.\");\n+      File libdir;\n+      if (new File(sparkHome, \"RELEASE\").isFile()) {\n+        libdir = new File(sparkHome, \"lib\");\n+      } else {\n+        libdir = new File(sparkHome, \"lib_managed/jars\");\n+      }\n+\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+        libdir.getAbsolutePath());\n+      for (File jar : libdir.listFiles()) {\n+        if (jar.getName().startsWith(\"datanucleus-\")) {\n+          addToClassPath(cp, jar.getAbsolutePath());\n+        }\n+      }\n+    }\n+\n+    addToClassPath(cp, getenv(\"HADOOP_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"YARN_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"SPARK_DIST_CLASSPATH\"));\n+    return cp;\n+  }\n+\n+  private void addToClassPath(List<String> cp, String entries) {\n+    if (isEmpty(entries)) {\n+      return;\n+    }\n+    String[] split = entries.split(Pattern.quote(File.pathSeparator));\n+    for (String entry : split) {\n+      if (!isEmpty(entry)) {\n+        if (new File(entry).isDirectory() && !entry.endsWith(File.separator)) {\n+          entry += File.separator;\n+        }\n+        cp.add(entry);\n+      }\n+    }\n+  }\n+\n+  protected String getScalaVersion() {\n+    String scala = getenv(\"SPARK_SCALA_VERSION\");\n+    if (scala != null) {\n+      return scala;\n+    }\n+\n+    String sparkHome = getSparkHome();\n+    File scala210 = new File(sparkHome, \"assembly/target/scala-2.10\");\n+    File scala211 = new File(sparkHome, \"assembly/target/scala-2.11\");\n+    if (scala210.isDirectory() && scala211.isDirectory()) {\n+      checkState(false,\n+        \"Presence of build for both scala versions (2.10 and 2.11) detected.\\n\" +\n+        \"Either clean one of them or set SPARK_SCALA_VERSION in your environment.\");\n+    } else if (scala210.isDirectory()) {\n+      return \"2.10\";\n+    } else {\n+      checkState(scala211.isDirectory(), \"Cannot find any assembly build directories.\");\n+      return \"2.11\";\n+    }\n+\n+    throw new IllegalStateException(\"Should not reach here.\");\n+  }\n+\n+  /**\n+   * Which OS is running defines two things:\n+   * - the name of the environment variable used to define the lookup path for native libs\n+   * - how to execute the command in general.\n+   *\n+   * The name is easy: PATH on Win32, DYLD_LIBRARY_PATH on MacOS, LD_LIBRARY_PATH elsewhere.\n+   *\n+   * On Unix-like, we're assuming bash is available. So we print one argument per line to\n+   * the output, and use bash's array handling to execute the right thing.\n+   *\n+    * For Win32, see {@link #prepareForWindows(List<String>,String)}.\n+   */\n+  protected List<String> prepareForOs(List<String> cmd,\n+      String libPath,\n+      Map<String, String> env) {\n+\n+    // If SPARK_HOME does not come from the environment, explicitly set it\n+    // in the child's environment.\n+    Map<String, String> childEnv = env;\n+    if (System.getenv(\"SPARK_HOME\") == null && !env.containsKey(\"SPARK_HOME\")) {\n+      childEnv = new HashMap<String, String>(env);\n+      childEnv.put(\"SPARK_HOME\", sparkHome);\n+    }\n+\n+    if (isWindows()) {\n+      return prepareForWindows(cmd, libPath, childEnv);\n+    }\n+\n+    if (isEmpty(libPath) && childEnv.isEmpty()) {\n+      return cmd;\n+    }\n+\n+    List<String> newCmd = new ArrayList<String>();\n+    newCmd.add(\"env\");\n+\n+    if (!isEmpty(libPath)) {\n+      String envName = getLibPathEnvName();\n+      String currEnvValue = getenv(envName);\n+      String newEnvValue = join(File.pathSeparator, currEnvValue, libPath);\n+      newCmd.add(String.format(\"%s=%s\", envName, newEnvValue));\n+    }\n+    for (Map.Entry<String, String> e : childEnv.entrySet()) {\n+      newCmd.add(String.format(\"%s=%s\", e.getKey(), e.getValue()));\n+    }\n+    newCmd.addAll(cmd);\n+    return newCmd;\n+  }\n+\n+  protected String shQuote(String s) {\n+    StringBuilder quoted = new StringBuilder();\n+    boolean hasWhitespace = false;\n+    for (int i = 0; i < s.length(); i++) {\n+      if (Character.isWhitespace(s.codePointAt(i))) {\n+        quoted.append('\"');\n+        hasWhitespace = true;\n+        break;\n+      }\n+    }\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int cp = s.codePointAt(i);\n+      switch (cp) {\n+        case '\\'':\n+          if (hasWhitespace) {\n+            quoted.appendCodePoint(cp);\n+            break;\n+          }\n+        case '\"':\n+        case '\\\\':\n+          quoted.append('\\\\');\n+          // Fall through.\n+        default:\n+          if (Character.isWhitespace(cp)) {\n+            hasWhitespace=true;\n+          }\n+          quoted.appendCodePoint(cp);\n+      }\n+    }\n+    if (hasWhitespace) {\n+      quoted.append('\"');\n+    }\n+    return quoted.toString();\n+  }\n+\n+  // Visible for testing.\n+  List<String> parseOptionString(String s) {\n+    List<String> opts = new ArrayList<String>();\n+    StringBuilder opt = new StringBuilder();\n+    boolean inOpt = false;\n+    boolean inSingleQuote = false;\n+    boolean inDoubleQuote = false;\n+    boolean escapeNext = false;\n+    boolean hasData = false;\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int c = s.codePointAt(i);\n+      if (escapeNext) {\n+        if (!inOpt) {\n+          inOpt = true;\n+        }\n+        opt.appendCodePoint(c);\n+        escapeNext = false;\n+      } else if (inOpt) {\n+        switch (c) {\n+        case '\\\\':\n+          if (inSingleQuote) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            escapeNext = true;\n+          }\n+          break;\n+        case '\\'':\n+          if (inDoubleQuote) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            inSingleQuote = !inSingleQuote;\n+          }\n+          break;\n+        case '\"':\n+          if (inSingleQuote) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            inDoubleQuote = !inDoubleQuote;\n+          }\n+          break;\n+        default:\n+          if (inSingleQuote || inDoubleQuote || !Character.isWhitespace(c)) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            finishOpt(opts, opt);\n+            inOpt = false;\n+            hasData = false;\n+          }\n+        }\n+      } else {\n+        switch (c) {\n+        case '\\'':\n+          inSingleQuote = true;\n+          inOpt = true;\n+          hasData = true;\n+          break;\n+        case '\"':\n+          inDoubleQuote = true;\n+          inOpt = true;\n+          hasData = true;\n+          break;\n+        case '\\\\':\n+          escapeNext = true;\n+          break;\n+        default:\n+          if (!Character.isWhitespace(c)) {\n+            inOpt = true;\n+            opt.appendCodePoint(c);\n+          }\n+        }\n+      }\n+    }\n+\n+    checkArgument(!inSingleQuote && !inDoubleQuote && !escapeNext, \"Invalid option string: %s\", s);\n+    if (opt.length() > 0 || hasData) {"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "I see\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T22:49:23Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    return cmd;\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  protected List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    String confDir = getenv(\"SPARK_CONF_DIR\");\n+    if (!isEmpty(confDir)) {\n+      addToClassPath(cp, confDir);\n+    } else {\n+      addToClassPath(cp, join(File.separator, getSparkHome(), \"conf\"));\n+    }\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars.\n+    //\n+    // This block also serves as a check for SPARK-1703, when the assembly jar is built with\n+    // Java 7 and ends up with too many files, causing issues with other JDK versions.\n+    boolean needsDataNucleus = false;\n+    JarFile assemblyJar = null;\n+    try {\n+      assemblyJar = new JarFile(assembly);\n+      needsDataNucleus = assemblyJar.getEntry(\"org/apache/hadoop/hive/ql/exec/\") != null;\n+    } catch (IOException ioe) {\n+      if (ioe.getMessage().indexOf(\"invalid CEN header\") > 0) {\n+        System.err.println(\n+          \"Loading Spark jar with '$JAR_CMD' failed.\\n\" +\n+          \"This is likely because Spark was compiled with Java 7 and run\\n\" +\n+          \"with Java 6 (see SPARK-1703). Please use Java 7 to run Spark\\n\" +\n+          \"or build Spark with Java 6.\");\n+        System.exit(1);\n+      } else {\n+        throw ioe;\n+      }\n+    } finally {\n+      if (assemblyJar != null) {\n+        try {\n+          assemblyJar.close();\n+        } catch (IOException e) {\n+          // Ignore.\n+        }\n+      }\n+    }\n+\n+    if (needsDataNucleus) {\n+      System.err.println(\"Spark assembly has been built with Hive, including Datanucleus jars \" +\n+        \"in classpath.\");\n+      File libdir;\n+      if (new File(sparkHome, \"RELEASE\").isFile()) {\n+        libdir = new File(sparkHome, \"lib\");\n+      } else {\n+        libdir = new File(sparkHome, \"lib_managed/jars\");\n+      }\n+\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+        libdir.getAbsolutePath());\n+      for (File jar : libdir.listFiles()) {\n+        if (jar.getName().startsWith(\"datanucleus-\")) {\n+          addToClassPath(cp, jar.getAbsolutePath());\n+        }\n+      }\n+    }\n+\n+    addToClassPath(cp, getenv(\"HADOOP_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"YARN_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"SPARK_DIST_CLASSPATH\"));\n+    return cp;\n+  }\n+\n+  private void addToClassPath(List<String> cp, String entries) {\n+    if (isEmpty(entries)) {\n+      return;\n+    }\n+    String[] split = entries.split(Pattern.quote(File.pathSeparator));\n+    for (String entry : split) {\n+      if (!isEmpty(entry)) {\n+        if (new File(entry).isDirectory() && !entry.endsWith(File.separator)) {\n+          entry += File.separator;\n+        }\n+        cp.add(entry);\n+      }\n+    }\n+  }\n+\n+  protected String getScalaVersion() {\n+    String scala = getenv(\"SPARK_SCALA_VERSION\");\n+    if (scala != null) {\n+      return scala;\n+    }\n+\n+    String sparkHome = getSparkHome();\n+    File scala210 = new File(sparkHome, \"assembly/target/scala-2.10\");\n+    File scala211 = new File(sparkHome, \"assembly/target/scala-2.11\");\n+    if (scala210.isDirectory() && scala211.isDirectory()) {\n+      checkState(false,\n+        \"Presence of build for both scala versions (2.10 and 2.11) detected.\\n\" +\n+        \"Either clean one of them or set SPARK_SCALA_VERSION in your environment.\");\n+    } else if (scala210.isDirectory()) {\n+      return \"2.10\";\n+    } else {\n+      checkState(scala211.isDirectory(), \"Cannot find any assembly build directories.\");\n+      return \"2.11\";\n+    }\n+\n+    throw new IllegalStateException(\"Should not reach here.\");\n+  }\n+\n+  /**\n+   * Which OS is running defines two things:\n+   * - the name of the environment variable used to define the lookup path for native libs\n+   * - how to execute the command in general.\n+   *\n+   * The name is easy: PATH on Win32, DYLD_LIBRARY_PATH on MacOS, LD_LIBRARY_PATH elsewhere.\n+   *\n+   * On Unix-like, we're assuming bash is available. So we print one argument per line to\n+   * the output, and use bash's array handling to execute the right thing.\n+   *\n+    * For Win32, see {@link #prepareForWindows(List<String>,String)}.\n+   */\n+  protected List<String> prepareForOs(List<String> cmd,\n+      String libPath,\n+      Map<String, String> env) {\n+\n+    // If SPARK_HOME does not come from the environment, explicitly set it\n+    // in the child's environment.\n+    Map<String, String> childEnv = env;\n+    if (System.getenv(\"SPARK_HOME\") == null && !env.containsKey(\"SPARK_HOME\")) {\n+      childEnv = new HashMap<String, String>(env);\n+      childEnv.put(\"SPARK_HOME\", sparkHome);\n+    }\n+\n+    if (isWindows()) {\n+      return prepareForWindows(cmd, libPath, childEnv);\n+    }\n+\n+    if (isEmpty(libPath) && childEnv.isEmpty()) {\n+      return cmd;\n+    }\n+\n+    List<String> newCmd = new ArrayList<String>();\n+    newCmd.add(\"env\");\n+\n+    if (!isEmpty(libPath)) {\n+      String envName = getLibPathEnvName();\n+      String currEnvValue = getenv(envName);\n+      String newEnvValue = join(File.pathSeparator, currEnvValue, libPath);\n+      newCmd.add(String.format(\"%s=%s\", envName, newEnvValue));\n+    }\n+    for (Map.Entry<String, String> e : childEnv.entrySet()) {\n+      newCmd.add(String.format(\"%s=%s\", e.getKey(), e.getValue()));\n+    }\n+    newCmd.addAll(cmd);\n+    return newCmd;\n+  }\n+\n+  protected String shQuote(String s) {\n+    StringBuilder quoted = new StringBuilder();\n+    boolean hasWhitespace = false;\n+    for (int i = 0; i < s.length(); i++) {\n+      if (Character.isWhitespace(s.codePointAt(i))) {\n+        quoted.append('\"');\n+        hasWhitespace = true;\n+        break;\n+      }\n+    }\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int cp = s.codePointAt(i);\n+      switch (cp) {\n+        case '\\'':\n+          if (hasWhitespace) {\n+            quoted.appendCodePoint(cp);\n+            break;\n+          }\n+        case '\"':\n+        case '\\\\':\n+          quoted.append('\\\\');\n+          // Fall through.\n+        default:\n+          if (Character.isWhitespace(cp)) {\n+            hasWhitespace=true;\n+          }\n+          quoted.appendCodePoint(cp);\n+      }\n+    }\n+    if (hasWhitespace) {\n+      quoted.append('\"');\n+    }\n+    return quoted.toString();\n+  }\n+\n+  // Visible for testing.\n+  List<String> parseOptionString(String s) {\n+    List<String> opts = new ArrayList<String>();\n+    StringBuilder opt = new StringBuilder();\n+    boolean inOpt = false;\n+    boolean inSingleQuote = false;\n+    boolean inDoubleQuote = false;\n+    boolean escapeNext = false;\n+    boolean hasData = false;\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int c = s.codePointAt(i);\n+      if (escapeNext) {\n+        if (!inOpt) {\n+          inOpt = true;\n+        }\n+        opt.appendCodePoint(c);\n+        escapeNext = false;\n+      } else if (inOpt) {\n+        switch (c) {\n+        case '\\\\':\n+          if (inSingleQuote) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            escapeNext = true;\n+          }\n+          break;\n+        case '\\'':\n+          if (inDoubleQuote) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            inSingleQuote = !inSingleQuote;\n+          }\n+          break;\n+        case '\"':\n+          if (inSingleQuote) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            inDoubleQuote = !inDoubleQuote;\n+          }\n+          break;\n+        default:\n+          if (inSingleQuote || inDoubleQuote || !Character.isWhitespace(c)) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            finishOpt(opts, opt);\n+            inOpt = false;\n+            hasData = false;\n+          }\n+        }\n+      } else {\n+        switch (c) {\n+        case '\\'':\n+          inSingleQuote = true;\n+          inOpt = true;\n+          hasData = true;\n+          break;\n+        case '\"':\n+          inDoubleQuote = true;\n+          inOpt = true;\n+          hasData = true;\n+          break;\n+        case '\\\\':\n+          escapeNext = true;\n+          break;\n+        default:\n+          if (!Character.isWhitespace(c)) {\n+            inOpt = true;\n+            opt.appendCodePoint(c);\n+          }\n+        }\n+      }\n+    }\n+\n+    checkArgument(!inSingleQuote && !inDoubleQuote && !escapeNext, \"Invalid option string: %s\", s);\n+    if (opt.length() > 0 || hasData) {"
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "space around = \n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T21:30:02Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    return cmd;\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  protected List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    String confDir = getenv(\"SPARK_CONF_DIR\");\n+    if (!isEmpty(confDir)) {\n+      addToClassPath(cp, confDir);\n+    } else {\n+      addToClassPath(cp, join(File.separator, getSparkHome(), \"conf\"));\n+    }\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars.\n+    //\n+    // This block also serves as a check for SPARK-1703, when the assembly jar is built with\n+    // Java 7 and ends up with too many files, causing issues with other JDK versions.\n+    boolean needsDataNucleus = false;\n+    JarFile assemblyJar = null;\n+    try {\n+      assemblyJar = new JarFile(assembly);\n+      needsDataNucleus = assemblyJar.getEntry(\"org/apache/hadoop/hive/ql/exec/\") != null;\n+    } catch (IOException ioe) {\n+      if (ioe.getMessage().indexOf(\"invalid CEN header\") > 0) {\n+        System.err.println(\n+          \"Loading Spark jar with '$JAR_CMD' failed.\\n\" +\n+          \"This is likely because Spark was compiled with Java 7 and run\\n\" +\n+          \"with Java 6 (see SPARK-1703). Please use Java 7 to run Spark\\n\" +\n+          \"or build Spark with Java 6.\");\n+        System.exit(1);\n+      } else {\n+        throw ioe;\n+      }\n+    } finally {\n+      if (assemblyJar != null) {\n+        try {\n+          assemblyJar.close();\n+        } catch (IOException e) {\n+          // Ignore.\n+        }\n+      }\n+    }\n+\n+    if (needsDataNucleus) {\n+      System.err.println(\"Spark assembly has been built with Hive, including Datanucleus jars \" +\n+        \"in classpath.\");\n+      File libdir;\n+      if (new File(sparkHome, \"RELEASE\").isFile()) {\n+        libdir = new File(sparkHome, \"lib\");\n+      } else {\n+        libdir = new File(sparkHome, \"lib_managed/jars\");\n+      }\n+\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+        libdir.getAbsolutePath());\n+      for (File jar : libdir.listFiles()) {\n+        if (jar.getName().startsWith(\"datanucleus-\")) {\n+          addToClassPath(cp, jar.getAbsolutePath());\n+        }\n+      }\n+    }\n+\n+    addToClassPath(cp, getenv(\"HADOOP_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"YARN_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"SPARK_DIST_CLASSPATH\"));\n+    return cp;\n+  }\n+\n+  private void addToClassPath(List<String> cp, String entries) {\n+    if (isEmpty(entries)) {\n+      return;\n+    }\n+    String[] split = entries.split(Pattern.quote(File.pathSeparator));\n+    for (String entry : split) {\n+      if (!isEmpty(entry)) {\n+        if (new File(entry).isDirectory() && !entry.endsWith(File.separator)) {\n+          entry += File.separator;\n+        }\n+        cp.add(entry);\n+      }\n+    }\n+  }\n+\n+  protected String getScalaVersion() {\n+    String scala = getenv(\"SPARK_SCALA_VERSION\");\n+    if (scala != null) {\n+      return scala;\n+    }\n+\n+    String sparkHome = getSparkHome();\n+    File scala210 = new File(sparkHome, \"assembly/target/scala-2.10\");\n+    File scala211 = new File(sparkHome, \"assembly/target/scala-2.11\");\n+    if (scala210.isDirectory() && scala211.isDirectory()) {\n+      checkState(false,\n+        \"Presence of build for both scala versions (2.10 and 2.11) detected.\\n\" +\n+        \"Either clean one of them or set SPARK_SCALA_VERSION in your environment.\");\n+    } else if (scala210.isDirectory()) {\n+      return \"2.10\";\n+    } else {\n+      checkState(scala211.isDirectory(), \"Cannot find any assembly build directories.\");\n+      return \"2.11\";\n+    }\n+\n+    throw new IllegalStateException(\"Should not reach here.\");\n+  }\n+\n+  /**\n+   * Which OS is running defines two things:\n+   * - the name of the environment variable used to define the lookup path for native libs\n+   * - how to execute the command in general.\n+   *\n+   * The name is easy: PATH on Win32, DYLD_LIBRARY_PATH on MacOS, LD_LIBRARY_PATH elsewhere.\n+   *\n+   * On Unix-like, we're assuming bash is available. So we print one argument per line to\n+   * the output, and use bash's array handling to execute the right thing.\n+   *\n+    * For Win32, see {@link #prepareForWindows(List<String>,String)}.\n+   */\n+  protected List<String> prepareForOs(List<String> cmd,\n+      String libPath,\n+      Map<String, String> env) {\n+\n+    // If SPARK_HOME does not come from the environment, explicitly set it\n+    // in the child's environment.\n+    Map<String, String> childEnv = env;\n+    if (System.getenv(\"SPARK_HOME\") == null && !env.containsKey(\"SPARK_HOME\")) {\n+      childEnv = new HashMap<String, String>(env);\n+      childEnv.put(\"SPARK_HOME\", sparkHome);\n+    }\n+\n+    if (isWindows()) {\n+      return prepareForWindows(cmd, libPath, childEnv);\n+    }\n+\n+    if (isEmpty(libPath) && childEnv.isEmpty()) {\n+      return cmd;\n+    }\n+\n+    List<String> newCmd = new ArrayList<String>();\n+    newCmd.add(\"env\");\n+\n+    if (!isEmpty(libPath)) {\n+      String envName = getLibPathEnvName();\n+      String currEnvValue = getenv(envName);\n+      String newEnvValue = join(File.pathSeparator, currEnvValue, libPath);\n+      newCmd.add(String.format(\"%s=%s\", envName, newEnvValue));\n+    }\n+    for (Map.Entry<String, String> e : childEnv.entrySet()) {\n+      newCmd.add(String.format(\"%s=%s\", e.getKey(), e.getValue()));\n+    }\n+    newCmd.addAll(cmd);\n+    return newCmd;\n+  }\n+\n+  protected String shQuote(String s) {\n+    StringBuilder quoted = new StringBuilder();\n+    boolean hasWhitespace = false;\n+    for (int i = 0; i < s.length(); i++) {\n+      if (Character.isWhitespace(s.codePointAt(i))) {\n+        quoted.append('\"');\n+        hasWhitespace = true;\n+        break;\n+      }\n+    }\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int cp = s.codePointAt(i);\n+      switch (cp) {\n+        case '\\'':\n+          if (hasWhitespace) {\n+            quoted.appendCodePoint(cp);\n+            break;\n+          }\n+        case '\"':\n+        case '\\\\':\n+          quoted.append('\\\\');\n+          // Fall through.\n+        default:\n+          if (Character.isWhitespace(cp)) {\n+            hasWhitespace=true;"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "actually why do you need to check again here? You already checked in L372\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T21:31:47Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    return cmd;\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  protected List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    String confDir = getenv(\"SPARK_CONF_DIR\");\n+    if (!isEmpty(confDir)) {\n+      addToClassPath(cp, confDir);\n+    } else {\n+      addToClassPath(cp, join(File.separator, getSparkHome(), \"conf\"));\n+    }\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars.\n+    //\n+    // This block also serves as a check for SPARK-1703, when the assembly jar is built with\n+    // Java 7 and ends up with too many files, causing issues with other JDK versions.\n+    boolean needsDataNucleus = false;\n+    JarFile assemblyJar = null;\n+    try {\n+      assemblyJar = new JarFile(assembly);\n+      needsDataNucleus = assemblyJar.getEntry(\"org/apache/hadoop/hive/ql/exec/\") != null;\n+    } catch (IOException ioe) {\n+      if (ioe.getMessage().indexOf(\"invalid CEN header\") > 0) {\n+        System.err.println(\n+          \"Loading Spark jar with '$JAR_CMD' failed.\\n\" +\n+          \"This is likely because Spark was compiled with Java 7 and run\\n\" +\n+          \"with Java 6 (see SPARK-1703). Please use Java 7 to run Spark\\n\" +\n+          \"or build Spark with Java 6.\");\n+        System.exit(1);\n+      } else {\n+        throw ioe;\n+      }\n+    } finally {\n+      if (assemblyJar != null) {\n+        try {\n+          assemblyJar.close();\n+        } catch (IOException e) {\n+          // Ignore.\n+        }\n+      }\n+    }\n+\n+    if (needsDataNucleus) {\n+      System.err.println(\"Spark assembly has been built with Hive, including Datanucleus jars \" +\n+        \"in classpath.\");\n+      File libdir;\n+      if (new File(sparkHome, \"RELEASE\").isFile()) {\n+        libdir = new File(sparkHome, \"lib\");\n+      } else {\n+        libdir = new File(sparkHome, \"lib_managed/jars\");\n+      }\n+\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+        libdir.getAbsolutePath());\n+      for (File jar : libdir.listFiles()) {\n+        if (jar.getName().startsWith(\"datanucleus-\")) {\n+          addToClassPath(cp, jar.getAbsolutePath());\n+        }\n+      }\n+    }\n+\n+    addToClassPath(cp, getenv(\"HADOOP_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"YARN_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"SPARK_DIST_CLASSPATH\"));\n+    return cp;\n+  }\n+\n+  private void addToClassPath(List<String> cp, String entries) {\n+    if (isEmpty(entries)) {\n+      return;\n+    }\n+    String[] split = entries.split(Pattern.quote(File.pathSeparator));\n+    for (String entry : split) {\n+      if (!isEmpty(entry)) {\n+        if (new File(entry).isDirectory() && !entry.endsWith(File.separator)) {\n+          entry += File.separator;\n+        }\n+        cp.add(entry);\n+      }\n+    }\n+  }\n+\n+  protected String getScalaVersion() {\n+    String scala = getenv(\"SPARK_SCALA_VERSION\");\n+    if (scala != null) {\n+      return scala;\n+    }\n+\n+    String sparkHome = getSparkHome();\n+    File scala210 = new File(sparkHome, \"assembly/target/scala-2.10\");\n+    File scala211 = new File(sparkHome, \"assembly/target/scala-2.11\");\n+    if (scala210.isDirectory() && scala211.isDirectory()) {\n+      checkState(false,\n+        \"Presence of build for both scala versions (2.10 and 2.11) detected.\\n\" +\n+        \"Either clean one of them or set SPARK_SCALA_VERSION in your environment.\");\n+    } else if (scala210.isDirectory()) {\n+      return \"2.10\";\n+    } else {\n+      checkState(scala211.isDirectory(), \"Cannot find any assembly build directories.\");\n+      return \"2.11\";\n+    }\n+\n+    throw new IllegalStateException(\"Should not reach here.\");\n+  }\n+\n+  /**\n+   * Which OS is running defines two things:\n+   * - the name of the environment variable used to define the lookup path for native libs\n+   * - how to execute the command in general.\n+   *\n+   * The name is easy: PATH on Win32, DYLD_LIBRARY_PATH on MacOS, LD_LIBRARY_PATH elsewhere.\n+   *\n+   * On Unix-like, we're assuming bash is available. So we print one argument per line to\n+   * the output, and use bash's array handling to execute the right thing.\n+   *\n+    * For Win32, see {@link #prepareForWindows(List<String>,String)}.\n+   */\n+  protected List<String> prepareForOs(List<String> cmd,\n+      String libPath,\n+      Map<String, String> env) {\n+\n+    // If SPARK_HOME does not come from the environment, explicitly set it\n+    // in the child's environment.\n+    Map<String, String> childEnv = env;\n+    if (System.getenv(\"SPARK_HOME\") == null && !env.containsKey(\"SPARK_HOME\")) {\n+      childEnv = new HashMap<String, String>(env);\n+      childEnv.put(\"SPARK_HOME\", sparkHome);\n+    }\n+\n+    if (isWindows()) {\n+      return prepareForWindows(cmd, libPath, childEnv);\n+    }\n+\n+    if (isEmpty(libPath) && childEnv.isEmpty()) {\n+      return cmd;\n+    }\n+\n+    List<String> newCmd = new ArrayList<String>();\n+    newCmd.add(\"env\");\n+\n+    if (!isEmpty(libPath)) {\n+      String envName = getLibPathEnvName();\n+      String currEnvValue = getenv(envName);\n+      String newEnvValue = join(File.pathSeparator, currEnvValue, libPath);\n+      newCmd.add(String.format(\"%s=%s\", envName, newEnvValue));\n+    }\n+    for (Map.Entry<String, String> e : childEnv.entrySet()) {\n+      newCmd.add(String.format(\"%s=%s\", e.getKey(), e.getValue()));\n+    }\n+    newCmd.addAll(cmd);\n+    return newCmd;\n+  }\n+\n+  protected String shQuote(String s) {\n+    StringBuilder quoted = new StringBuilder();\n+    boolean hasWhitespace = false;\n+    for (int i = 0; i < s.length(); i++) {\n+      if (Character.isWhitespace(s.codePointAt(i))) {\n+        quoted.append('\"');\n+        hasWhitespace = true;\n+        break;\n+      }\n+    }\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int cp = s.codePointAt(i);\n+      switch (cp) {\n+        case '\\'':\n+          if (hasWhitespace) {\n+            quoted.appendCodePoint(cp);\n+            break;\n+          }\n+        case '\"':\n+        case '\\\\':\n+          quoted.append('\\\\');\n+          // Fall through.\n+        default:\n+          if (Character.isWhitespace(cp)) {\n+            hasWhitespace=true;"
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "Are you falling through so that you add two backslashes instead of 1? If so I think it'll be clearer if we just add the two backslashes here instead of falling through later...\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T21:52:46Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    return cmd;\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  protected List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    String confDir = getenv(\"SPARK_CONF_DIR\");\n+    if (!isEmpty(confDir)) {\n+      addToClassPath(cp, confDir);\n+    } else {\n+      addToClassPath(cp, join(File.separator, getSparkHome(), \"conf\"));\n+    }\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars.\n+    //\n+    // This block also serves as a check for SPARK-1703, when the assembly jar is built with\n+    // Java 7 and ends up with too many files, causing issues with other JDK versions.\n+    boolean needsDataNucleus = false;\n+    JarFile assemblyJar = null;\n+    try {\n+      assemblyJar = new JarFile(assembly);\n+      needsDataNucleus = assemblyJar.getEntry(\"org/apache/hadoop/hive/ql/exec/\") != null;\n+    } catch (IOException ioe) {\n+      if (ioe.getMessage().indexOf(\"invalid CEN header\") > 0) {\n+        System.err.println(\n+          \"Loading Spark jar with '$JAR_CMD' failed.\\n\" +\n+          \"This is likely because Spark was compiled with Java 7 and run\\n\" +\n+          \"with Java 6 (see SPARK-1703). Please use Java 7 to run Spark\\n\" +\n+          \"or build Spark with Java 6.\");\n+        System.exit(1);\n+      } else {\n+        throw ioe;\n+      }\n+    } finally {\n+      if (assemblyJar != null) {\n+        try {\n+          assemblyJar.close();\n+        } catch (IOException e) {\n+          // Ignore.\n+        }\n+      }\n+    }\n+\n+    if (needsDataNucleus) {\n+      System.err.println(\"Spark assembly has been built with Hive, including Datanucleus jars \" +\n+        \"in classpath.\");\n+      File libdir;\n+      if (new File(sparkHome, \"RELEASE\").isFile()) {\n+        libdir = new File(sparkHome, \"lib\");\n+      } else {\n+        libdir = new File(sparkHome, \"lib_managed/jars\");\n+      }\n+\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+        libdir.getAbsolutePath());\n+      for (File jar : libdir.listFiles()) {\n+        if (jar.getName().startsWith(\"datanucleus-\")) {\n+          addToClassPath(cp, jar.getAbsolutePath());\n+        }\n+      }\n+    }\n+\n+    addToClassPath(cp, getenv(\"HADOOP_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"YARN_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"SPARK_DIST_CLASSPATH\"));\n+    return cp;\n+  }\n+\n+  private void addToClassPath(List<String> cp, String entries) {\n+    if (isEmpty(entries)) {\n+      return;\n+    }\n+    String[] split = entries.split(Pattern.quote(File.pathSeparator));\n+    for (String entry : split) {\n+      if (!isEmpty(entry)) {\n+        if (new File(entry).isDirectory() && !entry.endsWith(File.separator)) {\n+          entry += File.separator;\n+        }\n+        cp.add(entry);\n+      }\n+    }\n+  }\n+\n+  protected String getScalaVersion() {\n+    String scala = getenv(\"SPARK_SCALA_VERSION\");\n+    if (scala != null) {\n+      return scala;\n+    }\n+\n+    String sparkHome = getSparkHome();\n+    File scala210 = new File(sparkHome, \"assembly/target/scala-2.10\");\n+    File scala211 = new File(sparkHome, \"assembly/target/scala-2.11\");\n+    if (scala210.isDirectory() && scala211.isDirectory()) {\n+      checkState(false,\n+        \"Presence of build for both scala versions (2.10 and 2.11) detected.\\n\" +\n+        \"Either clean one of them or set SPARK_SCALA_VERSION in your environment.\");\n+    } else if (scala210.isDirectory()) {\n+      return \"2.10\";\n+    } else {\n+      checkState(scala211.isDirectory(), \"Cannot find any assembly build directories.\");\n+      return \"2.11\";\n+    }\n+\n+    throw new IllegalStateException(\"Should not reach here.\");\n+  }\n+\n+  /**\n+   * Which OS is running defines two things:\n+   * - the name of the environment variable used to define the lookup path for native libs\n+   * - how to execute the command in general.\n+   *\n+   * The name is easy: PATH on Win32, DYLD_LIBRARY_PATH on MacOS, LD_LIBRARY_PATH elsewhere.\n+   *\n+   * On Unix-like, we're assuming bash is available. So we print one argument per line to\n+   * the output, and use bash's array handling to execute the right thing.\n+   *\n+    * For Win32, see {@link #prepareForWindows(List<String>,String)}.\n+   */\n+  protected List<String> prepareForOs(List<String> cmd,\n+      String libPath,\n+      Map<String, String> env) {\n+\n+    // If SPARK_HOME does not come from the environment, explicitly set it\n+    // in the child's environment.\n+    Map<String, String> childEnv = env;\n+    if (System.getenv(\"SPARK_HOME\") == null && !env.containsKey(\"SPARK_HOME\")) {\n+      childEnv = new HashMap<String, String>(env);\n+      childEnv.put(\"SPARK_HOME\", sparkHome);\n+    }\n+\n+    if (isWindows()) {\n+      return prepareForWindows(cmd, libPath, childEnv);\n+    }\n+\n+    if (isEmpty(libPath) && childEnv.isEmpty()) {\n+      return cmd;\n+    }\n+\n+    List<String> newCmd = new ArrayList<String>();\n+    newCmd.add(\"env\");\n+\n+    if (!isEmpty(libPath)) {\n+      String envName = getLibPathEnvName();\n+      String currEnvValue = getenv(envName);\n+      String newEnvValue = join(File.pathSeparator, currEnvValue, libPath);\n+      newCmd.add(String.format(\"%s=%s\", envName, newEnvValue));\n+    }\n+    for (Map.Entry<String, String> e : childEnv.entrySet()) {\n+      newCmd.add(String.format(\"%s=%s\", e.getKey(), e.getValue()));\n+    }\n+    newCmd.addAll(cmd);\n+    return newCmd;\n+  }\n+\n+  protected String shQuote(String s) {\n+    StringBuilder quoted = new StringBuilder();\n+    boolean hasWhitespace = false;\n+    for (int i = 0; i < s.length(); i++) {\n+      if (Character.isWhitespace(s.codePointAt(i))) {\n+        quoted.append('\"');\n+        hasWhitespace = true;\n+        break;\n+      }\n+    }\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int cp = s.codePointAt(i);\n+      switch (cp) {\n+        case '\\'':\n+          if (hasWhitespace) {\n+            quoted.appendCodePoint(cp);\n+            break;\n+          }\n+        case '\"':\n+        case '\\\\':\n+          quoted.append('\\\\');\n+          // Fall through."
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Yes.\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T21:54:35Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    return cmd;\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  protected List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    String confDir = getenv(\"SPARK_CONF_DIR\");\n+    if (!isEmpty(confDir)) {\n+      addToClassPath(cp, confDir);\n+    } else {\n+      addToClassPath(cp, join(File.separator, getSparkHome(), \"conf\"));\n+    }\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars.\n+    //\n+    // This block also serves as a check for SPARK-1703, when the assembly jar is built with\n+    // Java 7 and ends up with too many files, causing issues with other JDK versions.\n+    boolean needsDataNucleus = false;\n+    JarFile assemblyJar = null;\n+    try {\n+      assemblyJar = new JarFile(assembly);\n+      needsDataNucleus = assemblyJar.getEntry(\"org/apache/hadoop/hive/ql/exec/\") != null;\n+    } catch (IOException ioe) {\n+      if (ioe.getMessage().indexOf(\"invalid CEN header\") > 0) {\n+        System.err.println(\n+          \"Loading Spark jar with '$JAR_CMD' failed.\\n\" +\n+          \"This is likely because Spark was compiled with Java 7 and run\\n\" +\n+          \"with Java 6 (see SPARK-1703). Please use Java 7 to run Spark\\n\" +\n+          \"or build Spark with Java 6.\");\n+        System.exit(1);\n+      } else {\n+        throw ioe;\n+      }\n+    } finally {\n+      if (assemblyJar != null) {\n+        try {\n+          assemblyJar.close();\n+        } catch (IOException e) {\n+          // Ignore.\n+        }\n+      }\n+    }\n+\n+    if (needsDataNucleus) {\n+      System.err.println(\"Spark assembly has been built with Hive, including Datanucleus jars \" +\n+        \"in classpath.\");\n+      File libdir;\n+      if (new File(sparkHome, \"RELEASE\").isFile()) {\n+        libdir = new File(sparkHome, \"lib\");\n+      } else {\n+        libdir = new File(sparkHome, \"lib_managed/jars\");\n+      }\n+\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+        libdir.getAbsolutePath());\n+      for (File jar : libdir.listFiles()) {\n+        if (jar.getName().startsWith(\"datanucleus-\")) {\n+          addToClassPath(cp, jar.getAbsolutePath());\n+        }\n+      }\n+    }\n+\n+    addToClassPath(cp, getenv(\"HADOOP_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"YARN_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"SPARK_DIST_CLASSPATH\"));\n+    return cp;\n+  }\n+\n+  private void addToClassPath(List<String> cp, String entries) {\n+    if (isEmpty(entries)) {\n+      return;\n+    }\n+    String[] split = entries.split(Pattern.quote(File.pathSeparator));\n+    for (String entry : split) {\n+      if (!isEmpty(entry)) {\n+        if (new File(entry).isDirectory() && !entry.endsWith(File.separator)) {\n+          entry += File.separator;\n+        }\n+        cp.add(entry);\n+      }\n+    }\n+  }\n+\n+  protected String getScalaVersion() {\n+    String scala = getenv(\"SPARK_SCALA_VERSION\");\n+    if (scala != null) {\n+      return scala;\n+    }\n+\n+    String sparkHome = getSparkHome();\n+    File scala210 = new File(sparkHome, \"assembly/target/scala-2.10\");\n+    File scala211 = new File(sparkHome, \"assembly/target/scala-2.11\");\n+    if (scala210.isDirectory() && scala211.isDirectory()) {\n+      checkState(false,\n+        \"Presence of build for both scala versions (2.10 and 2.11) detected.\\n\" +\n+        \"Either clean one of them or set SPARK_SCALA_VERSION in your environment.\");\n+    } else if (scala210.isDirectory()) {\n+      return \"2.10\";\n+    } else {\n+      checkState(scala211.isDirectory(), \"Cannot find any assembly build directories.\");\n+      return \"2.11\";\n+    }\n+\n+    throw new IllegalStateException(\"Should not reach here.\");\n+  }\n+\n+  /**\n+   * Which OS is running defines two things:\n+   * - the name of the environment variable used to define the lookup path for native libs\n+   * - how to execute the command in general.\n+   *\n+   * The name is easy: PATH on Win32, DYLD_LIBRARY_PATH on MacOS, LD_LIBRARY_PATH elsewhere.\n+   *\n+   * On Unix-like, we're assuming bash is available. So we print one argument per line to\n+   * the output, and use bash's array handling to execute the right thing.\n+   *\n+    * For Win32, see {@link #prepareForWindows(List<String>,String)}.\n+   */\n+  protected List<String> prepareForOs(List<String> cmd,\n+      String libPath,\n+      Map<String, String> env) {\n+\n+    // If SPARK_HOME does not come from the environment, explicitly set it\n+    // in the child's environment.\n+    Map<String, String> childEnv = env;\n+    if (System.getenv(\"SPARK_HOME\") == null && !env.containsKey(\"SPARK_HOME\")) {\n+      childEnv = new HashMap<String, String>(env);\n+      childEnv.put(\"SPARK_HOME\", sparkHome);\n+    }\n+\n+    if (isWindows()) {\n+      return prepareForWindows(cmd, libPath, childEnv);\n+    }\n+\n+    if (isEmpty(libPath) && childEnv.isEmpty()) {\n+      return cmd;\n+    }\n+\n+    List<String> newCmd = new ArrayList<String>();\n+    newCmd.add(\"env\");\n+\n+    if (!isEmpty(libPath)) {\n+      String envName = getLibPathEnvName();\n+      String currEnvValue = getenv(envName);\n+      String newEnvValue = join(File.pathSeparator, currEnvValue, libPath);\n+      newCmd.add(String.format(\"%s=%s\", envName, newEnvValue));\n+    }\n+    for (Map.Entry<String, String> e : childEnv.entrySet()) {\n+      newCmd.add(String.format(\"%s=%s\", e.getKey(), e.getValue()));\n+    }\n+    newCmd.addAll(cmd);\n+    return newCmd;\n+  }\n+\n+  protected String shQuote(String s) {\n+    StringBuilder quoted = new StringBuilder();\n+    boolean hasWhitespace = false;\n+    for (int i = 0; i < s.length(); i++) {\n+      if (Character.isWhitespace(s.codePointAt(i))) {\n+        quoted.append('\"');\n+        hasWhitespace = true;\n+        break;\n+      }\n+    }\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int cp = s.codePointAt(i);\n+      switch (cp) {\n+        case '\\'':\n+          if (hasWhitespace) {\n+            quoted.appendCodePoint(cp);\n+            break;\n+          }\n+        case '\"':\n+        case '\\\\':\n+          quoted.append('\\\\');\n+          // Fall through."
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "(BTW I simplified this method a bit in the version I'll upload later.)\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T22:03:13Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    return cmd;\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  protected List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    String confDir = getenv(\"SPARK_CONF_DIR\");\n+    if (!isEmpty(confDir)) {\n+      addToClassPath(cp, confDir);\n+    } else {\n+      addToClassPath(cp, join(File.separator, getSparkHome(), \"conf\"));\n+    }\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars.\n+    //\n+    // This block also serves as a check for SPARK-1703, when the assembly jar is built with\n+    // Java 7 and ends up with too many files, causing issues with other JDK versions.\n+    boolean needsDataNucleus = false;\n+    JarFile assemblyJar = null;\n+    try {\n+      assemblyJar = new JarFile(assembly);\n+      needsDataNucleus = assemblyJar.getEntry(\"org/apache/hadoop/hive/ql/exec/\") != null;\n+    } catch (IOException ioe) {\n+      if (ioe.getMessage().indexOf(\"invalid CEN header\") > 0) {\n+        System.err.println(\n+          \"Loading Spark jar with '$JAR_CMD' failed.\\n\" +\n+          \"This is likely because Spark was compiled with Java 7 and run\\n\" +\n+          \"with Java 6 (see SPARK-1703). Please use Java 7 to run Spark\\n\" +\n+          \"or build Spark with Java 6.\");\n+        System.exit(1);\n+      } else {\n+        throw ioe;\n+      }\n+    } finally {\n+      if (assemblyJar != null) {\n+        try {\n+          assemblyJar.close();\n+        } catch (IOException e) {\n+          // Ignore.\n+        }\n+      }\n+    }\n+\n+    if (needsDataNucleus) {\n+      System.err.println(\"Spark assembly has been built with Hive, including Datanucleus jars \" +\n+        \"in classpath.\");\n+      File libdir;\n+      if (new File(sparkHome, \"RELEASE\").isFile()) {\n+        libdir = new File(sparkHome, \"lib\");\n+      } else {\n+        libdir = new File(sparkHome, \"lib_managed/jars\");\n+      }\n+\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+        libdir.getAbsolutePath());\n+      for (File jar : libdir.listFiles()) {\n+        if (jar.getName().startsWith(\"datanucleus-\")) {\n+          addToClassPath(cp, jar.getAbsolutePath());\n+        }\n+      }\n+    }\n+\n+    addToClassPath(cp, getenv(\"HADOOP_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"YARN_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"SPARK_DIST_CLASSPATH\"));\n+    return cp;\n+  }\n+\n+  private void addToClassPath(List<String> cp, String entries) {\n+    if (isEmpty(entries)) {\n+      return;\n+    }\n+    String[] split = entries.split(Pattern.quote(File.pathSeparator));\n+    for (String entry : split) {\n+      if (!isEmpty(entry)) {\n+        if (new File(entry).isDirectory() && !entry.endsWith(File.separator)) {\n+          entry += File.separator;\n+        }\n+        cp.add(entry);\n+      }\n+    }\n+  }\n+\n+  protected String getScalaVersion() {\n+    String scala = getenv(\"SPARK_SCALA_VERSION\");\n+    if (scala != null) {\n+      return scala;\n+    }\n+\n+    String sparkHome = getSparkHome();\n+    File scala210 = new File(sparkHome, \"assembly/target/scala-2.10\");\n+    File scala211 = new File(sparkHome, \"assembly/target/scala-2.11\");\n+    if (scala210.isDirectory() && scala211.isDirectory()) {\n+      checkState(false,\n+        \"Presence of build for both scala versions (2.10 and 2.11) detected.\\n\" +\n+        \"Either clean one of them or set SPARK_SCALA_VERSION in your environment.\");\n+    } else if (scala210.isDirectory()) {\n+      return \"2.10\";\n+    } else {\n+      checkState(scala211.isDirectory(), \"Cannot find any assembly build directories.\");\n+      return \"2.11\";\n+    }\n+\n+    throw new IllegalStateException(\"Should not reach here.\");\n+  }\n+\n+  /**\n+   * Which OS is running defines two things:\n+   * - the name of the environment variable used to define the lookup path for native libs\n+   * - how to execute the command in general.\n+   *\n+   * The name is easy: PATH on Win32, DYLD_LIBRARY_PATH on MacOS, LD_LIBRARY_PATH elsewhere.\n+   *\n+   * On Unix-like, we're assuming bash is available. So we print one argument per line to\n+   * the output, and use bash's array handling to execute the right thing.\n+   *\n+    * For Win32, see {@link #prepareForWindows(List<String>,String)}.\n+   */\n+  protected List<String> prepareForOs(List<String> cmd,\n+      String libPath,\n+      Map<String, String> env) {\n+\n+    // If SPARK_HOME does not come from the environment, explicitly set it\n+    // in the child's environment.\n+    Map<String, String> childEnv = env;\n+    if (System.getenv(\"SPARK_HOME\") == null && !env.containsKey(\"SPARK_HOME\")) {\n+      childEnv = new HashMap<String, String>(env);\n+      childEnv.put(\"SPARK_HOME\", sparkHome);\n+    }\n+\n+    if (isWindows()) {\n+      return prepareForWindows(cmd, libPath, childEnv);\n+    }\n+\n+    if (isEmpty(libPath) && childEnv.isEmpty()) {\n+      return cmd;\n+    }\n+\n+    List<String> newCmd = new ArrayList<String>();\n+    newCmd.add(\"env\");\n+\n+    if (!isEmpty(libPath)) {\n+      String envName = getLibPathEnvName();\n+      String currEnvValue = getenv(envName);\n+      String newEnvValue = join(File.pathSeparator, currEnvValue, libPath);\n+      newCmd.add(String.format(\"%s=%s\", envName, newEnvValue));\n+    }\n+    for (Map.Entry<String, String> e : childEnv.entrySet()) {\n+      newCmd.add(String.format(\"%s=%s\", e.getKey(), e.getValue()));\n+    }\n+    newCmd.addAll(cmd);\n+    return newCmd;\n+  }\n+\n+  protected String shQuote(String s) {\n+    StringBuilder quoted = new StringBuilder();\n+    boolean hasWhitespace = false;\n+    for (int i = 0; i < s.length(); i++) {\n+      if (Character.isWhitespace(s.codePointAt(i))) {\n+        quoted.append('\"');\n+        hasWhitespace = true;\n+        break;\n+      }\n+    }\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int cp = s.codePointAt(i);\n+      switch (cp) {\n+        case '\\'':\n+          if (hasWhitespace) {\n+            quoted.appendCodePoint(cp);\n+            break;\n+          }\n+        case '\"':\n+        case '\\\\':\n+          quoted.append('\\\\');\n+          // Fall through."
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "if you have a double quote here that falls through, don't you end up having something like\n`\"this option contains a \" double quote\"`? Do we need to escape the double quote?\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T21:53:24Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    return cmd;\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  protected List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    String confDir = getenv(\"SPARK_CONF_DIR\");\n+    if (!isEmpty(confDir)) {\n+      addToClassPath(cp, confDir);\n+    } else {\n+      addToClassPath(cp, join(File.separator, getSparkHome(), \"conf\"));\n+    }\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars.\n+    //\n+    // This block also serves as a check for SPARK-1703, when the assembly jar is built with\n+    // Java 7 and ends up with too many files, causing issues with other JDK versions.\n+    boolean needsDataNucleus = false;\n+    JarFile assemblyJar = null;\n+    try {\n+      assemblyJar = new JarFile(assembly);\n+      needsDataNucleus = assemblyJar.getEntry(\"org/apache/hadoop/hive/ql/exec/\") != null;\n+    } catch (IOException ioe) {\n+      if (ioe.getMessage().indexOf(\"invalid CEN header\") > 0) {\n+        System.err.println(\n+          \"Loading Spark jar with '$JAR_CMD' failed.\\n\" +\n+          \"This is likely because Spark was compiled with Java 7 and run\\n\" +\n+          \"with Java 6 (see SPARK-1703). Please use Java 7 to run Spark\\n\" +\n+          \"or build Spark with Java 6.\");\n+        System.exit(1);\n+      } else {\n+        throw ioe;\n+      }\n+    } finally {\n+      if (assemblyJar != null) {\n+        try {\n+          assemblyJar.close();\n+        } catch (IOException e) {\n+          // Ignore.\n+        }\n+      }\n+    }\n+\n+    if (needsDataNucleus) {\n+      System.err.println(\"Spark assembly has been built with Hive, including Datanucleus jars \" +\n+        \"in classpath.\");\n+      File libdir;\n+      if (new File(sparkHome, \"RELEASE\").isFile()) {\n+        libdir = new File(sparkHome, \"lib\");\n+      } else {\n+        libdir = new File(sparkHome, \"lib_managed/jars\");\n+      }\n+\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+        libdir.getAbsolutePath());\n+      for (File jar : libdir.listFiles()) {\n+        if (jar.getName().startsWith(\"datanucleus-\")) {\n+          addToClassPath(cp, jar.getAbsolutePath());\n+        }\n+      }\n+    }\n+\n+    addToClassPath(cp, getenv(\"HADOOP_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"YARN_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"SPARK_DIST_CLASSPATH\"));\n+    return cp;\n+  }\n+\n+  private void addToClassPath(List<String> cp, String entries) {\n+    if (isEmpty(entries)) {\n+      return;\n+    }\n+    String[] split = entries.split(Pattern.quote(File.pathSeparator));\n+    for (String entry : split) {\n+      if (!isEmpty(entry)) {\n+        if (new File(entry).isDirectory() && !entry.endsWith(File.separator)) {\n+          entry += File.separator;\n+        }\n+        cp.add(entry);\n+      }\n+    }\n+  }\n+\n+  protected String getScalaVersion() {\n+    String scala = getenv(\"SPARK_SCALA_VERSION\");\n+    if (scala != null) {\n+      return scala;\n+    }\n+\n+    String sparkHome = getSparkHome();\n+    File scala210 = new File(sparkHome, \"assembly/target/scala-2.10\");\n+    File scala211 = new File(sparkHome, \"assembly/target/scala-2.11\");\n+    if (scala210.isDirectory() && scala211.isDirectory()) {\n+      checkState(false,\n+        \"Presence of build for both scala versions (2.10 and 2.11) detected.\\n\" +\n+        \"Either clean one of them or set SPARK_SCALA_VERSION in your environment.\");\n+    } else if (scala210.isDirectory()) {\n+      return \"2.10\";\n+    } else {\n+      checkState(scala211.isDirectory(), \"Cannot find any assembly build directories.\");\n+      return \"2.11\";\n+    }\n+\n+    throw new IllegalStateException(\"Should not reach here.\");\n+  }\n+\n+  /**\n+   * Which OS is running defines two things:\n+   * - the name of the environment variable used to define the lookup path for native libs\n+   * - how to execute the command in general.\n+   *\n+   * The name is easy: PATH on Win32, DYLD_LIBRARY_PATH on MacOS, LD_LIBRARY_PATH elsewhere.\n+   *\n+   * On Unix-like, we're assuming bash is available. So we print one argument per line to\n+   * the output, and use bash's array handling to execute the right thing.\n+   *\n+    * For Win32, see {@link #prepareForWindows(List<String>,String)}.\n+   */\n+  protected List<String> prepareForOs(List<String> cmd,\n+      String libPath,\n+      Map<String, String> env) {\n+\n+    // If SPARK_HOME does not come from the environment, explicitly set it\n+    // in the child's environment.\n+    Map<String, String> childEnv = env;\n+    if (System.getenv(\"SPARK_HOME\") == null && !env.containsKey(\"SPARK_HOME\")) {\n+      childEnv = new HashMap<String, String>(env);\n+      childEnv.put(\"SPARK_HOME\", sparkHome);\n+    }\n+\n+    if (isWindows()) {\n+      return prepareForWindows(cmd, libPath, childEnv);\n+    }\n+\n+    if (isEmpty(libPath) && childEnv.isEmpty()) {\n+      return cmd;\n+    }\n+\n+    List<String> newCmd = new ArrayList<String>();\n+    newCmd.add(\"env\");\n+\n+    if (!isEmpty(libPath)) {\n+      String envName = getLibPathEnvName();\n+      String currEnvValue = getenv(envName);\n+      String newEnvValue = join(File.pathSeparator, currEnvValue, libPath);\n+      newCmd.add(String.format(\"%s=%s\", envName, newEnvValue));\n+    }\n+    for (Map.Entry<String, String> e : childEnv.entrySet()) {\n+      newCmd.add(String.format(\"%s=%s\", e.getKey(), e.getValue()));\n+    }\n+    newCmd.addAll(cmd);\n+    return newCmd;\n+  }\n+\n+  protected String shQuote(String s) {\n+    StringBuilder quoted = new StringBuilder();\n+    boolean hasWhitespace = false;\n+    for (int i = 0; i < s.length(); i++) {\n+      if (Character.isWhitespace(s.codePointAt(i))) {\n+        quoted.append('\"');\n+        hasWhitespace = true;\n+        break;\n+      }\n+    }\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int cp = s.codePointAt(i);\n+      switch (cp) {\n+        case '\\'':\n+          if (hasWhitespace) {\n+            quoted.appendCodePoint(cp);\n+            break;\n+          }\n+        case '\"':"
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "From what I understand this quotes the given string and escapes any characters that need to be escaped. Can you name this something better and add a java doc with input/output examples?\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T21:55:41Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    return cmd;\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  protected List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    String confDir = getenv(\"SPARK_CONF_DIR\");\n+    if (!isEmpty(confDir)) {\n+      addToClassPath(cp, confDir);\n+    } else {\n+      addToClassPath(cp, join(File.separator, getSparkHome(), \"conf\"));\n+    }\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars.\n+    //\n+    // This block also serves as a check for SPARK-1703, when the assembly jar is built with\n+    // Java 7 and ends up with too many files, causing issues with other JDK versions.\n+    boolean needsDataNucleus = false;\n+    JarFile assemblyJar = null;\n+    try {\n+      assemblyJar = new JarFile(assembly);\n+      needsDataNucleus = assemblyJar.getEntry(\"org/apache/hadoop/hive/ql/exec/\") != null;\n+    } catch (IOException ioe) {\n+      if (ioe.getMessage().indexOf(\"invalid CEN header\") > 0) {\n+        System.err.println(\n+          \"Loading Spark jar with '$JAR_CMD' failed.\\n\" +\n+          \"This is likely because Spark was compiled with Java 7 and run\\n\" +\n+          \"with Java 6 (see SPARK-1703). Please use Java 7 to run Spark\\n\" +\n+          \"or build Spark with Java 6.\");\n+        System.exit(1);\n+      } else {\n+        throw ioe;\n+      }\n+    } finally {\n+      if (assemblyJar != null) {\n+        try {\n+          assemblyJar.close();\n+        } catch (IOException e) {\n+          // Ignore.\n+        }\n+      }\n+    }\n+\n+    if (needsDataNucleus) {\n+      System.err.println(\"Spark assembly has been built with Hive, including Datanucleus jars \" +\n+        \"in classpath.\");\n+      File libdir;\n+      if (new File(sparkHome, \"RELEASE\").isFile()) {\n+        libdir = new File(sparkHome, \"lib\");\n+      } else {\n+        libdir = new File(sparkHome, \"lib_managed/jars\");\n+      }\n+\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+        libdir.getAbsolutePath());\n+      for (File jar : libdir.listFiles()) {\n+        if (jar.getName().startsWith(\"datanucleus-\")) {\n+          addToClassPath(cp, jar.getAbsolutePath());\n+        }\n+      }\n+    }\n+\n+    addToClassPath(cp, getenv(\"HADOOP_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"YARN_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"SPARK_DIST_CLASSPATH\"));\n+    return cp;\n+  }\n+\n+  private void addToClassPath(List<String> cp, String entries) {\n+    if (isEmpty(entries)) {\n+      return;\n+    }\n+    String[] split = entries.split(Pattern.quote(File.pathSeparator));\n+    for (String entry : split) {\n+      if (!isEmpty(entry)) {\n+        if (new File(entry).isDirectory() && !entry.endsWith(File.separator)) {\n+          entry += File.separator;\n+        }\n+        cp.add(entry);\n+      }\n+    }\n+  }\n+\n+  protected String getScalaVersion() {\n+    String scala = getenv(\"SPARK_SCALA_VERSION\");\n+    if (scala != null) {\n+      return scala;\n+    }\n+\n+    String sparkHome = getSparkHome();\n+    File scala210 = new File(sparkHome, \"assembly/target/scala-2.10\");\n+    File scala211 = new File(sparkHome, \"assembly/target/scala-2.11\");\n+    if (scala210.isDirectory() && scala211.isDirectory()) {\n+      checkState(false,\n+        \"Presence of build for both scala versions (2.10 and 2.11) detected.\\n\" +\n+        \"Either clean one of them or set SPARK_SCALA_VERSION in your environment.\");\n+    } else if (scala210.isDirectory()) {\n+      return \"2.10\";\n+    } else {\n+      checkState(scala211.isDirectory(), \"Cannot find any assembly build directories.\");\n+      return \"2.11\";\n+    }\n+\n+    throw new IllegalStateException(\"Should not reach here.\");\n+  }\n+\n+  /**\n+   * Which OS is running defines two things:\n+   * - the name of the environment variable used to define the lookup path for native libs\n+   * - how to execute the command in general.\n+   *\n+   * The name is easy: PATH on Win32, DYLD_LIBRARY_PATH on MacOS, LD_LIBRARY_PATH elsewhere.\n+   *\n+   * On Unix-like, we're assuming bash is available. So we print one argument per line to\n+   * the output, and use bash's array handling to execute the right thing.\n+   *\n+    * For Win32, see {@link #prepareForWindows(List<String>,String)}.\n+   */\n+  protected List<String> prepareForOs(List<String> cmd,\n+      String libPath,\n+      Map<String, String> env) {\n+\n+    // If SPARK_HOME does not come from the environment, explicitly set it\n+    // in the child's environment.\n+    Map<String, String> childEnv = env;\n+    if (System.getenv(\"SPARK_HOME\") == null && !env.containsKey(\"SPARK_HOME\")) {\n+      childEnv = new HashMap<String, String>(env);\n+      childEnv.put(\"SPARK_HOME\", sparkHome);\n+    }\n+\n+    if (isWindows()) {\n+      return prepareForWindows(cmd, libPath, childEnv);\n+    }\n+\n+    if (isEmpty(libPath) && childEnv.isEmpty()) {\n+      return cmd;\n+    }\n+\n+    List<String> newCmd = new ArrayList<String>();\n+    newCmd.add(\"env\");\n+\n+    if (!isEmpty(libPath)) {\n+      String envName = getLibPathEnvName();\n+      String currEnvValue = getenv(envName);\n+      String newEnvValue = join(File.pathSeparator, currEnvValue, libPath);\n+      newCmd.add(String.format(\"%s=%s\", envName, newEnvValue));\n+    }\n+    for (Map.Entry<String, String> e : childEnv.entrySet()) {\n+      newCmd.add(String.format(\"%s=%s\", e.getKey(), e.getValue()));\n+    }\n+    newCmd.addAll(cmd);\n+    return newCmd;\n+  }\n+\n+  protected String shQuote(String s) {"
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "can we have something like\n\n```\ndef prepareForOs(cmd, libPath) = prepareForOs(cmd, libPath, new Map)\n```\n\nin java syntax? It's a little weird that SparkLauncher and others need to create and pass in an empty env map.\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T22:52:54Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    return cmd;\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  protected List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    String confDir = getenv(\"SPARK_CONF_DIR\");\n+    if (!isEmpty(confDir)) {\n+      addToClassPath(cp, confDir);\n+    } else {\n+      addToClassPath(cp, join(File.separator, getSparkHome(), \"conf\"));\n+    }\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars.\n+    //\n+    // This block also serves as a check for SPARK-1703, when the assembly jar is built with\n+    // Java 7 and ends up with too many files, causing issues with other JDK versions.\n+    boolean needsDataNucleus = false;\n+    JarFile assemblyJar = null;\n+    try {\n+      assemblyJar = new JarFile(assembly);\n+      needsDataNucleus = assemblyJar.getEntry(\"org/apache/hadoop/hive/ql/exec/\") != null;\n+    } catch (IOException ioe) {\n+      if (ioe.getMessage().indexOf(\"invalid CEN header\") > 0) {\n+        System.err.println(\n+          \"Loading Spark jar with '$JAR_CMD' failed.\\n\" +\n+          \"This is likely because Spark was compiled with Java 7 and run\\n\" +\n+          \"with Java 6 (see SPARK-1703). Please use Java 7 to run Spark\\n\" +\n+          \"or build Spark with Java 6.\");\n+        System.exit(1);\n+      } else {\n+        throw ioe;\n+      }\n+    } finally {\n+      if (assemblyJar != null) {\n+        try {\n+          assemblyJar.close();\n+        } catch (IOException e) {\n+          // Ignore.\n+        }\n+      }\n+    }\n+\n+    if (needsDataNucleus) {\n+      System.err.println(\"Spark assembly has been built with Hive, including Datanucleus jars \" +\n+        \"in classpath.\");\n+      File libdir;\n+      if (new File(sparkHome, \"RELEASE\").isFile()) {\n+        libdir = new File(sparkHome, \"lib\");\n+      } else {\n+        libdir = new File(sparkHome, \"lib_managed/jars\");\n+      }\n+\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+        libdir.getAbsolutePath());\n+      for (File jar : libdir.listFiles()) {\n+        if (jar.getName().startsWith(\"datanucleus-\")) {\n+          addToClassPath(cp, jar.getAbsolutePath());\n+        }\n+      }\n+    }\n+\n+    addToClassPath(cp, getenv(\"HADOOP_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"YARN_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"SPARK_DIST_CLASSPATH\"));\n+    return cp;\n+  }\n+\n+  private void addToClassPath(List<String> cp, String entries) {\n+    if (isEmpty(entries)) {\n+      return;\n+    }\n+    String[] split = entries.split(Pattern.quote(File.pathSeparator));\n+    for (String entry : split) {\n+      if (!isEmpty(entry)) {\n+        if (new File(entry).isDirectory() && !entry.endsWith(File.separator)) {\n+          entry += File.separator;\n+        }\n+        cp.add(entry);\n+      }\n+    }\n+  }\n+\n+  protected String getScalaVersion() {\n+    String scala = getenv(\"SPARK_SCALA_VERSION\");\n+    if (scala != null) {\n+      return scala;\n+    }\n+\n+    String sparkHome = getSparkHome();\n+    File scala210 = new File(sparkHome, \"assembly/target/scala-2.10\");\n+    File scala211 = new File(sparkHome, \"assembly/target/scala-2.11\");\n+    if (scala210.isDirectory() && scala211.isDirectory()) {\n+      checkState(false,\n+        \"Presence of build for both scala versions (2.10 and 2.11) detected.\\n\" +\n+        \"Either clean one of them or set SPARK_SCALA_VERSION in your environment.\");\n+    } else if (scala210.isDirectory()) {\n+      return \"2.10\";\n+    } else {\n+      checkState(scala211.isDirectory(), \"Cannot find any assembly build directories.\");\n+      return \"2.11\";\n+    }\n+\n+    throw new IllegalStateException(\"Should not reach here.\");\n+  }\n+\n+  /**\n+   * Which OS is running defines two things:\n+   * - the name of the environment variable used to define the lookup path for native libs\n+   * - how to execute the command in general.\n+   *\n+   * The name is easy: PATH on Win32, DYLD_LIBRARY_PATH on MacOS, LD_LIBRARY_PATH elsewhere.\n+   *\n+   * On Unix-like, we're assuming bash is available. So we print one argument per line to\n+   * the output, and use bash's array handling to execute the right thing.\n+   *\n+    * For Win32, see {@link #prepareForWindows(List<String>,String)}.\n+   */\n+  protected List<String> prepareForOs(List<String> cmd,"
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "this java doc should start with something like \"prepend the command with all environment variables to be read by the application\" or something\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T22:55:19Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    return cmd;\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  protected List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    String confDir = getenv(\"SPARK_CONF_DIR\");\n+    if (!isEmpty(confDir)) {\n+      addToClassPath(cp, confDir);\n+    } else {\n+      addToClassPath(cp, join(File.separator, getSparkHome(), \"conf\"));\n+    }\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars.\n+    //\n+    // This block also serves as a check for SPARK-1703, when the assembly jar is built with\n+    // Java 7 and ends up with too many files, causing issues with other JDK versions.\n+    boolean needsDataNucleus = false;\n+    JarFile assemblyJar = null;\n+    try {\n+      assemblyJar = new JarFile(assembly);\n+      needsDataNucleus = assemblyJar.getEntry(\"org/apache/hadoop/hive/ql/exec/\") != null;\n+    } catch (IOException ioe) {\n+      if (ioe.getMessage().indexOf(\"invalid CEN header\") > 0) {\n+        System.err.println(\n+          \"Loading Spark jar with '$JAR_CMD' failed.\\n\" +\n+          \"This is likely because Spark was compiled with Java 7 and run\\n\" +\n+          \"with Java 6 (see SPARK-1703). Please use Java 7 to run Spark\\n\" +\n+          \"or build Spark with Java 6.\");\n+        System.exit(1);\n+      } else {\n+        throw ioe;\n+      }\n+    } finally {\n+      if (assemblyJar != null) {\n+        try {\n+          assemblyJar.close();\n+        } catch (IOException e) {\n+          // Ignore.\n+        }\n+      }\n+    }\n+\n+    if (needsDataNucleus) {\n+      System.err.println(\"Spark assembly has been built with Hive, including Datanucleus jars \" +\n+        \"in classpath.\");\n+      File libdir;\n+      if (new File(sparkHome, \"RELEASE\").isFile()) {\n+        libdir = new File(sparkHome, \"lib\");\n+      } else {\n+        libdir = new File(sparkHome, \"lib_managed/jars\");\n+      }\n+\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+        libdir.getAbsolutePath());\n+      for (File jar : libdir.listFiles()) {\n+        if (jar.getName().startsWith(\"datanucleus-\")) {\n+          addToClassPath(cp, jar.getAbsolutePath());\n+        }\n+      }\n+    }\n+\n+    addToClassPath(cp, getenv(\"HADOOP_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"YARN_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"SPARK_DIST_CLASSPATH\"));\n+    return cp;\n+  }\n+\n+  private void addToClassPath(List<String> cp, String entries) {\n+    if (isEmpty(entries)) {\n+      return;\n+    }\n+    String[] split = entries.split(Pattern.quote(File.pathSeparator));\n+    for (String entry : split) {\n+      if (!isEmpty(entry)) {\n+        if (new File(entry).isDirectory() && !entry.endsWith(File.separator)) {\n+          entry += File.separator;\n+        }\n+        cp.add(entry);\n+      }\n+    }\n+  }\n+\n+  protected String getScalaVersion() {\n+    String scala = getenv(\"SPARK_SCALA_VERSION\");\n+    if (scala != null) {\n+      return scala;\n+    }\n+\n+    String sparkHome = getSparkHome();\n+    File scala210 = new File(sparkHome, \"assembly/target/scala-2.10\");\n+    File scala211 = new File(sparkHome, \"assembly/target/scala-2.11\");\n+    if (scala210.isDirectory() && scala211.isDirectory()) {\n+      checkState(false,\n+        \"Presence of build for both scala versions (2.10 and 2.11) detected.\\n\" +\n+        \"Either clean one of them or set SPARK_SCALA_VERSION in your environment.\");\n+    } else if (scala210.isDirectory()) {\n+      return \"2.10\";\n+    } else {\n+      checkState(scala211.isDirectory(), \"Cannot find any assembly build directories.\");\n+      return \"2.11\";\n+    }\n+\n+    throw new IllegalStateException(\"Should not reach here.\");\n+  }\n+\n+  /**\n+   * Which OS is running defines two things:\n+   * - the name of the environment variable used to define the lookup path for native libs\n+   * - how to execute the command in general.\n+   *\n+   * The name is easy: PATH on Win32, DYLD_LIBRARY_PATH on MacOS, LD_LIBRARY_PATH elsewhere.\n+   *\n+   * On Unix-like, we're assuming bash is available. So we print one argument per line to\n+   * the output, and use bash's array handling to execute the right thing.\n+   *\n+    * For Win32, see {@link #prepareForWindows(List<String>,String)}.\n+   */"
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "it seems that this always returns a 1-element list that is the windows command. If this is intended we should document this in the javadocs and explain why.\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T22:57:53Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    return cmd;\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  protected List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    String confDir = getenv(\"SPARK_CONF_DIR\");\n+    if (!isEmpty(confDir)) {\n+      addToClassPath(cp, confDir);\n+    } else {\n+      addToClassPath(cp, join(File.separator, getSparkHome(), \"conf\"));\n+    }\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars.\n+    //\n+    // This block also serves as a check for SPARK-1703, when the assembly jar is built with\n+    // Java 7 and ends up with too many files, causing issues with other JDK versions.\n+    boolean needsDataNucleus = false;\n+    JarFile assemblyJar = null;\n+    try {\n+      assemblyJar = new JarFile(assembly);\n+      needsDataNucleus = assemblyJar.getEntry(\"org/apache/hadoop/hive/ql/exec/\") != null;\n+    } catch (IOException ioe) {\n+      if (ioe.getMessage().indexOf(\"invalid CEN header\") > 0) {\n+        System.err.println(\n+          \"Loading Spark jar with '$JAR_CMD' failed.\\n\" +\n+          \"This is likely because Spark was compiled with Java 7 and run\\n\" +\n+          \"with Java 6 (see SPARK-1703). Please use Java 7 to run Spark\\n\" +\n+          \"or build Spark with Java 6.\");\n+        System.exit(1);\n+      } else {\n+        throw ioe;\n+      }\n+    } finally {\n+      if (assemblyJar != null) {\n+        try {\n+          assemblyJar.close();\n+        } catch (IOException e) {\n+          // Ignore.\n+        }\n+      }\n+    }\n+\n+    if (needsDataNucleus) {\n+      System.err.println(\"Spark assembly has been built with Hive, including Datanucleus jars \" +\n+        \"in classpath.\");\n+      File libdir;\n+      if (new File(sparkHome, \"RELEASE\").isFile()) {\n+        libdir = new File(sparkHome, \"lib\");\n+      } else {\n+        libdir = new File(sparkHome, \"lib_managed/jars\");\n+      }\n+\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+        libdir.getAbsolutePath());\n+      for (File jar : libdir.listFiles()) {\n+        if (jar.getName().startsWith(\"datanucleus-\")) {\n+          addToClassPath(cp, jar.getAbsolutePath());\n+        }\n+      }\n+    }\n+\n+    addToClassPath(cp, getenv(\"HADOOP_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"YARN_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"SPARK_DIST_CLASSPATH\"));\n+    return cp;\n+  }\n+\n+  private void addToClassPath(List<String> cp, String entries) {\n+    if (isEmpty(entries)) {\n+      return;\n+    }\n+    String[] split = entries.split(Pattern.quote(File.pathSeparator));\n+    for (String entry : split) {\n+      if (!isEmpty(entry)) {\n+        if (new File(entry).isDirectory() && !entry.endsWith(File.separator)) {\n+          entry += File.separator;\n+        }\n+        cp.add(entry);\n+      }\n+    }\n+  }\n+\n+  protected String getScalaVersion() {\n+    String scala = getenv(\"SPARK_SCALA_VERSION\");\n+    if (scala != null) {\n+      return scala;\n+    }\n+\n+    String sparkHome = getSparkHome();\n+    File scala210 = new File(sparkHome, \"assembly/target/scala-2.10\");\n+    File scala211 = new File(sparkHome, \"assembly/target/scala-2.11\");\n+    if (scala210.isDirectory() && scala211.isDirectory()) {\n+      checkState(false,\n+        \"Presence of build for both scala versions (2.10 and 2.11) detected.\\n\" +\n+        \"Either clean one of them or set SPARK_SCALA_VERSION in your environment.\");\n+    } else if (scala210.isDirectory()) {\n+      return \"2.10\";\n+    } else {\n+      checkState(scala211.isDirectory(), \"Cannot find any assembly build directories.\");\n+      return \"2.11\";\n+    }\n+\n+    throw new IllegalStateException(\"Should not reach here.\");\n+  }\n+\n+  /**\n+   * Which OS is running defines two things:\n+   * - the name of the environment variable used to define the lookup path for native libs\n+   * - how to execute the command in general.\n+   *\n+   * The name is easy: PATH on Win32, DYLD_LIBRARY_PATH on MacOS, LD_LIBRARY_PATH elsewhere.\n+   *\n+   * On Unix-like, we're assuming bash is available. So we print one argument per line to\n+   * the output, and use bash's array handling to execute the right thing.\n+   *\n+    * For Win32, see {@link #prepareForWindows(List<String>,String)}.\n+   */\n+  protected List<String> prepareForOs(List<String> cmd,\n+      String libPath,\n+      Map<String, String> env) {\n+\n+    // If SPARK_HOME does not come from the environment, explicitly set it\n+    // in the child's environment.\n+    Map<String, String> childEnv = env;\n+    if (System.getenv(\"SPARK_HOME\") == null && !env.containsKey(\"SPARK_HOME\")) {\n+      childEnv = new HashMap<String, String>(env);\n+      childEnv.put(\"SPARK_HOME\", sparkHome);\n+    }\n+\n+    if (isWindows()) {\n+      return prepareForWindows(cmd, libPath, childEnv);\n+    }\n+\n+    if (isEmpty(libPath) && childEnv.isEmpty()) {\n+      return cmd;\n+    }\n+\n+    List<String> newCmd = new ArrayList<String>();\n+    newCmd.add(\"env\");\n+\n+    if (!isEmpty(libPath)) {\n+      String envName = getLibPathEnvName();\n+      String currEnvValue = getenv(envName);\n+      String newEnvValue = join(File.pathSeparator, currEnvValue, libPath);\n+      newCmd.add(String.format(\"%s=%s\", envName, newEnvValue));\n+    }\n+    for (Map.Entry<String, String> e : childEnv.entrySet()) {\n+      newCmd.add(String.format(\"%s=%s\", e.getKey(), e.getValue()));\n+    }\n+    newCmd.addAll(cmd);\n+    return newCmd;\n+  }\n+\n+  protected String shQuote(String s) {\n+    StringBuilder quoted = new StringBuilder();\n+    boolean hasWhitespace = false;\n+    for (int i = 0; i < s.length(); i++) {\n+      if (Character.isWhitespace(s.codePointAt(i))) {\n+        quoted.append('\"');\n+        hasWhitespace = true;\n+        break;\n+      }\n+    }\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int cp = s.codePointAt(i);\n+      switch (cp) {\n+        case '\\'':\n+          if (hasWhitespace) {\n+            quoted.appendCodePoint(cp);\n+            break;\n+          }\n+        case '\"':\n+        case '\\\\':\n+          quoted.append('\\\\');\n+          // Fall through.\n+        default:\n+          if (Character.isWhitespace(cp)) {\n+            hasWhitespace=true;\n+          }\n+          quoted.appendCodePoint(cp);\n+      }\n+    }\n+    if (hasWhitespace) {\n+      quoted.append('\"');\n+    }\n+    return quoted.toString();\n+  }\n+\n+  // Visible for testing.\n+  List<String> parseOptionString(String s) {\n+    List<String> opts = new ArrayList<String>();\n+    StringBuilder opt = new StringBuilder();\n+    boolean inOpt = false;\n+    boolean inSingleQuote = false;\n+    boolean inDoubleQuote = false;\n+    boolean escapeNext = false;\n+    boolean hasData = false;\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int c = s.codePointAt(i);\n+      if (escapeNext) {\n+        if (!inOpt) {\n+          inOpt = true;\n+        }\n+        opt.appendCodePoint(c);\n+        escapeNext = false;\n+      } else if (inOpt) {\n+        switch (c) {\n+        case '\\\\':\n+          if (inSingleQuote) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            escapeNext = true;\n+          }\n+          break;\n+        case '\\'':\n+          if (inDoubleQuote) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            inSingleQuote = !inSingleQuote;\n+          }\n+          break;\n+        case '\"':\n+          if (inSingleQuote) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            inDoubleQuote = !inDoubleQuote;\n+          }\n+          break;\n+        default:\n+          if (inSingleQuote || inDoubleQuote || !Character.isWhitespace(c)) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            finishOpt(opts, opt);\n+            inOpt = false;\n+            hasData = false;\n+          }\n+        }\n+      } else {\n+        switch (c) {\n+        case '\\'':\n+          inSingleQuote = true;\n+          inOpt = true;\n+          hasData = true;\n+          break;\n+        case '\"':\n+          inDoubleQuote = true;\n+          inOpt = true;\n+          hasData = true;\n+          break;\n+        case '\\\\':\n+          escapeNext = true;\n+          break;\n+        default:\n+          if (!Character.isWhitespace(c)) {\n+            inOpt = true;\n+            opt.appendCodePoint(c);\n+          }\n+        }\n+      }\n+    }\n+\n+    checkArgument(!inSingleQuote && !inDoubleQuote && !escapeNext, \"Invalid option string: %s\", s);\n+    if (opt.length() > 0 || hasData) {\n+      opts.add(opt.toString());\n+    }\n+    return opts;\n+  }\n+\n+  private void finishOpt(List<String> opts, StringBuilder opt) {\n+    opts.add(opt.toString());\n+    opt.setLength(0);\n+  }\n+\n+  private String findAssembly(String scalaVersion) {\n+    String sparkHome = getSparkHome();\n+    File libdir;\n+    if (new File(sparkHome, \"RELEASE\").isFile()) {\n+      libdir = new File(sparkHome, \"lib\");\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+          libdir.getAbsolutePath());\n+    } else {\n+      libdir = new File(sparkHome, String.format(\"assembly/target/scala-%s\", scalaVersion));\n+    }\n+\n+    final Pattern re = Pattern.compile(\"spark-assembly.*hadoop.*\\\\.jar\");\n+    FileFilter filter = new FileFilter() {\n+      @Override\n+      public boolean accept(File file) {\n+        return file.isFile() && re.matcher(file.getName()).matches();\n+      }\n+    };\n+    File[] assemblies = libdir.listFiles(filter);\n+    checkState(assemblies != null && assemblies.length > 0, \"No assemblies found in '%s'.\", libdir);\n+    checkState(assemblies.length == 1, \"Multiple assemblies found in '%s'.\", libdir);\n+    return assemblies[0].getAbsolutePath();\n+  }\n+\n+  private String getenv(String key) {\n+    return first(env != null ? env.get(key) : null, System.getenv(key));\n+  }\n+\n+  /**\n+   * Prepare a command line for execution on Windows.\n+   *\n+   * Two things need to be done:\n+   *\n+   * - If a custom library path is needed, extend PATH to add it. Based on:\n+   *   http://superuser.com/questions/223104/setting-environment-variable-for-just-one-command-in-windows-cmd-exe\n+   *\n+   * - Quote all arguments so that spaces are handled as expected. Quotes within arguments are\n+   *   \"double quoted\" (which is batch for escaping a quote). This page has more details about\n+   *   quoting and other batch script fun stuff: http://ss64.com/nt/syntax-esc.html\n+   */\n+  private List<String> prepareForWindows(List<String> cmd,\n+      String libPath,\n+      Map<String, String> env) {\n+    StringBuilder cmdline = new StringBuilder(\"cmd /c \\\"\");\n+    if (libPath != null) {\n+      cmdline.append(\"set PATH=%PATH%;\").append(libPath).append(\" &&\");\n+    }\n+    for (Map.Entry<String, String> e : env.entrySet()) {\n+      cmdline.append(String.format(\"set %s=%s\", e.getKey(), e.getValue()));\n+      cmdline.append(\" &&\");\n+    }\n+    for (String arg : cmd) {\n+      if (cmdline.length() > 0) {\n+        cmdline.append(\" \");\n+      }\n+      cmdline.append(quote(arg));\n+    }\n+    cmdline.append(\"\\\"\");\n+    return Arrays.asList(cmdline.toString());"
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "andrewor14"
    },
    "body": "this will look something like\n\n```\n... &&set a=b &&set c=d &&set e=f\n```\n\nis this intended? Do we need spaces after the `&&`?\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T22:58:43Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    return cmd;\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  protected List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    String confDir = getenv(\"SPARK_CONF_DIR\");\n+    if (!isEmpty(confDir)) {\n+      addToClassPath(cp, confDir);\n+    } else {\n+      addToClassPath(cp, join(File.separator, getSparkHome(), \"conf\"));\n+    }\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars.\n+    //\n+    // This block also serves as a check for SPARK-1703, when the assembly jar is built with\n+    // Java 7 and ends up with too many files, causing issues with other JDK versions.\n+    boolean needsDataNucleus = false;\n+    JarFile assemblyJar = null;\n+    try {\n+      assemblyJar = new JarFile(assembly);\n+      needsDataNucleus = assemblyJar.getEntry(\"org/apache/hadoop/hive/ql/exec/\") != null;\n+    } catch (IOException ioe) {\n+      if (ioe.getMessage().indexOf(\"invalid CEN header\") > 0) {\n+        System.err.println(\n+          \"Loading Spark jar with '$JAR_CMD' failed.\\n\" +\n+          \"This is likely because Spark was compiled with Java 7 and run\\n\" +\n+          \"with Java 6 (see SPARK-1703). Please use Java 7 to run Spark\\n\" +\n+          \"or build Spark with Java 6.\");\n+        System.exit(1);\n+      } else {\n+        throw ioe;\n+      }\n+    } finally {\n+      if (assemblyJar != null) {\n+        try {\n+          assemblyJar.close();\n+        } catch (IOException e) {\n+          // Ignore.\n+        }\n+      }\n+    }\n+\n+    if (needsDataNucleus) {\n+      System.err.println(\"Spark assembly has been built with Hive, including Datanucleus jars \" +\n+        \"in classpath.\");\n+      File libdir;\n+      if (new File(sparkHome, \"RELEASE\").isFile()) {\n+        libdir = new File(sparkHome, \"lib\");\n+      } else {\n+        libdir = new File(sparkHome, \"lib_managed/jars\");\n+      }\n+\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+        libdir.getAbsolutePath());\n+      for (File jar : libdir.listFiles()) {\n+        if (jar.getName().startsWith(\"datanucleus-\")) {\n+          addToClassPath(cp, jar.getAbsolutePath());\n+        }\n+      }\n+    }\n+\n+    addToClassPath(cp, getenv(\"HADOOP_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"YARN_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"SPARK_DIST_CLASSPATH\"));\n+    return cp;\n+  }\n+\n+  private void addToClassPath(List<String> cp, String entries) {\n+    if (isEmpty(entries)) {\n+      return;\n+    }\n+    String[] split = entries.split(Pattern.quote(File.pathSeparator));\n+    for (String entry : split) {\n+      if (!isEmpty(entry)) {\n+        if (new File(entry).isDirectory() && !entry.endsWith(File.separator)) {\n+          entry += File.separator;\n+        }\n+        cp.add(entry);\n+      }\n+    }\n+  }\n+\n+  protected String getScalaVersion() {\n+    String scala = getenv(\"SPARK_SCALA_VERSION\");\n+    if (scala != null) {\n+      return scala;\n+    }\n+\n+    String sparkHome = getSparkHome();\n+    File scala210 = new File(sparkHome, \"assembly/target/scala-2.10\");\n+    File scala211 = new File(sparkHome, \"assembly/target/scala-2.11\");\n+    if (scala210.isDirectory() && scala211.isDirectory()) {\n+      checkState(false,\n+        \"Presence of build for both scala versions (2.10 and 2.11) detected.\\n\" +\n+        \"Either clean one of them or set SPARK_SCALA_VERSION in your environment.\");\n+    } else if (scala210.isDirectory()) {\n+      return \"2.10\";\n+    } else {\n+      checkState(scala211.isDirectory(), \"Cannot find any assembly build directories.\");\n+      return \"2.11\";\n+    }\n+\n+    throw new IllegalStateException(\"Should not reach here.\");\n+  }\n+\n+  /**\n+   * Which OS is running defines two things:\n+   * - the name of the environment variable used to define the lookup path for native libs\n+   * - how to execute the command in general.\n+   *\n+   * The name is easy: PATH on Win32, DYLD_LIBRARY_PATH on MacOS, LD_LIBRARY_PATH elsewhere.\n+   *\n+   * On Unix-like, we're assuming bash is available. So we print one argument per line to\n+   * the output, and use bash's array handling to execute the right thing.\n+   *\n+    * For Win32, see {@link #prepareForWindows(List<String>,String)}.\n+   */\n+  protected List<String> prepareForOs(List<String> cmd,\n+      String libPath,\n+      Map<String, String> env) {\n+\n+    // If SPARK_HOME does not come from the environment, explicitly set it\n+    // in the child's environment.\n+    Map<String, String> childEnv = env;\n+    if (System.getenv(\"SPARK_HOME\") == null && !env.containsKey(\"SPARK_HOME\")) {\n+      childEnv = new HashMap<String, String>(env);\n+      childEnv.put(\"SPARK_HOME\", sparkHome);\n+    }\n+\n+    if (isWindows()) {\n+      return prepareForWindows(cmd, libPath, childEnv);\n+    }\n+\n+    if (isEmpty(libPath) && childEnv.isEmpty()) {\n+      return cmd;\n+    }\n+\n+    List<String> newCmd = new ArrayList<String>();\n+    newCmd.add(\"env\");\n+\n+    if (!isEmpty(libPath)) {\n+      String envName = getLibPathEnvName();\n+      String currEnvValue = getenv(envName);\n+      String newEnvValue = join(File.pathSeparator, currEnvValue, libPath);\n+      newCmd.add(String.format(\"%s=%s\", envName, newEnvValue));\n+    }\n+    for (Map.Entry<String, String> e : childEnv.entrySet()) {\n+      newCmd.add(String.format(\"%s=%s\", e.getKey(), e.getValue()));\n+    }\n+    newCmd.addAll(cmd);\n+    return newCmd;\n+  }\n+\n+  protected String shQuote(String s) {\n+    StringBuilder quoted = new StringBuilder();\n+    boolean hasWhitespace = false;\n+    for (int i = 0; i < s.length(); i++) {\n+      if (Character.isWhitespace(s.codePointAt(i))) {\n+        quoted.append('\"');\n+        hasWhitespace = true;\n+        break;\n+      }\n+    }\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int cp = s.codePointAt(i);\n+      switch (cp) {\n+        case '\\'':\n+          if (hasWhitespace) {\n+            quoted.appendCodePoint(cp);\n+            break;\n+          }\n+        case '\"':\n+        case '\\\\':\n+          quoted.append('\\\\');\n+          // Fall through.\n+        default:\n+          if (Character.isWhitespace(cp)) {\n+            hasWhitespace=true;\n+          }\n+          quoted.appendCodePoint(cp);\n+      }\n+    }\n+    if (hasWhitespace) {\n+      quoted.append('\"');\n+    }\n+    return quoted.toString();\n+  }\n+\n+  // Visible for testing.\n+  List<String> parseOptionString(String s) {\n+    List<String> opts = new ArrayList<String>();\n+    StringBuilder opt = new StringBuilder();\n+    boolean inOpt = false;\n+    boolean inSingleQuote = false;\n+    boolean inDoubleQuote = false;\n+    boolean escapeNext = false;\n+    boolean hasData = false;\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int c = s.codePointAt(i);\n+      if (escapeNext) {\n+        if (!inOpt) {\n+          inOpt = true;\n+        }\n+        opt.appendCodePoint(c);\n+        escapeNext = false;\n+      } else if (inOpt) {\n+        switch (c) {\n+        case '\\\\':\n+          if (inSingleQuote) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            escapeNext = true;\n+          }\n+          break;\n+        case '\\'':\n+          if (inDoubleQuote) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            inSingleQuote = !inSingleQuote;\n+          }\n+          break;\n+        case '\"':\n+          if (inSingleQuote) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            inDoubleQuote = !inDoubleQuote;\n+          }\n+          break;\n+        default:\n+          if (inSingleQuote || inDoubleQuote || !Character.isWhitespace(c)) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            finishOpt(opts, opt);\n+            inOpt = false;\n+            hasData = false;\n+          }\n+        }\n+      } else {\n+        switch (c) {\n+        case '\\'':\n+          inSingleQuote = true;\n+          inOpt = true;\n+          hasData = true;\n+          break;\n+        case '\"':\n+          inDoubleQuote = true;\n+          inOpt = true;\n+          hasData = true;\n+          break;\n+        case '\\\\':\n+          escapeNext = true;\n+          break;\n+        default:\n+          if (!Character.isWhitespace(c)) {\n+            inOpt = true;\n+            opt.appendCodePoint(c);\n+          }\n+        }\n+      }\n+    }\n+\n+    checkArgument(!inSingleQuote && !inDoubleQuote && !escapeNext, \"Invalid option string: %s\", s);\n+    if (opt.length() > 0 || hasData) {\n+      opts.add(opt.toString());\n+    }\n+    return opts;\n+  }\n+\n+  private void finishOpt(List<String> opts, StringBuilder opt) {\n+    opts.add(opt.toString());\n+    opt.setLength(0);\n+  }\n+\n+  private String findAssembly(String scalaVersion) {\n+    String sparkHome = getSparkHome();\n+    File libdir;\n+    if (new File(sparkHome, \"RELEASE\").isFile()) {\n+      libdir = new File(sparkHome, \"lib\");\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+          libdir.getAbsolutePath());\n+    } else {\n+      libdir = new File(sparkHome, String.format(\"assembly/target/scala-%s\", scalaVersion));\n+    }\n+\n+    final Pattern re = Pattern.compile(\"spark-assembly.*hadoop.*\\\\.jar\");\n+    FileFilter filter = new FileFilter() {\n+      @Override\n+      public boolean accept(File file) {\n+        return file.isFile() && re.matcher(file.getName()).matches();\n+      }\n+    };\n+    File[] assemblies = libdir.listFiles(filter);\n+    checkState(assemblies != null && assemblies.length > 0, \"No assemblies found in '%s'.\", libdir);\n+    checkState(assemblies.length == 1, \"Multiple assemblies found in '%s'.\", libdir);\n+    return assemblies[0].getAbsolutePath();\n+  }\n+\n+  private String getenv(String key) {\n+    return first(env != null ? env.get(key) : null, System.getenv(key));\n+  }\n+\n+  /**\n+   * Prepare a command line for execution on Windows.\n+   *\n+   * Two things need to be done:\n+   *\n+   * - If a custom library path is needed, extend PATH to add it. Based on:\n+   *   http://superuser.com/questions/223104/setting-environment-variable-for-just-one-command-in-windows-cmd-exe\n+   *\n+   * - Quote all arguments so that spaces are handled as expected. Quotes within arguments are\n+   *   \"double quoted\" (which is batch for escaping a quote). This page has more details about\n+   *   quoting and other batch script fun stuff: http://ss64.com/nt/syntax-esc.html\n+   */\n+  private List<String> prepareForWindows(List<String> cmd,\n+      String libPath,\n+      Map<String, String> env) {\n+    StringBuilder cmdline = new StringBuilder(\"cmd /c \\\"\");\n+    if (libPath != null) {\n+      cmdline.append(\"set PATH=%PATH%;\").append(libPath).append(\" &&\");\n+    }\n+    for (Map.Entry<String, String> e : env.entrySet()) {\n+      cmdline.append(String.format(\"set %s=%s\", e.getKey(), e.getValue()));\n+      cmdline.append(\" &&\");"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "It doesn't really make any functional difference whether there's a space there or not...\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-13T23:06:49Z",
    "diffHunk": "@@ -0,0 +1,600 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  protected static final String DEFAULT_MEM = \"512m\";\n+\n+  protected String javaHome;\n+  protected String sparkHome;\n+  protected String propertiesFile;\n+  protected final Map<String, String> conf = new HashMap<String, String>();\n+  private final Map<String, String> env;\n+\n+  protected AbstractLauncher() {\n+    this(null);\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.env = env;\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.javaHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String path) {\n+    checkNotNull(path, \"path\");\n+    this.sparkHome = path;\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed.\n+   */\n+  protected abstract List<String> buildLauncherCommand() throws IOException;\n+\n+  protected Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      String confDir = getenv(\"SPARK_CONF_DIR\");\n+      if (confDir == null) {\n+        confDir = join(File.separator, getSparkHome(), \"conf\");\n+      }\n+      propsFile = new File(confDir, \"spark-defaults.conf\");\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  protected String getSparkHome() {\n+    String path = first(sparkHome, getenv(\"SPARK_HOME\"));\n+    checkState(path != null,\n+        \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> createJavaCommand() throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"..\", \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) == 1 && Integer.parseInt(version[1]) < 8) {\n+      cmd.add(\"-XX:MaxPermSize=128m\");\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getSparkHome(), \"conf\", \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    return cmd;\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  protected List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    String confDir = getenv(\"SPARK_CONF_DIR\");\n+    if (!isEmpty(confDir)) {\n+      addToClassPath(cp, confDir);\n+    } else {\n+      addToClassPath(cp, join(File.separator, getSparkHome(), \"conf\"));\n+    }\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars.\n+    //\n+    // This block also serves as a check for SPARK-1703, when the assembly jar is built with\n+    // Java 7 and ends up with too many files, causing issues with other JDK versions.\n+    boolean needsDataNucleus = false;\n+    JarFile assemblyJar = null;\n+    try {\n+      assemblyJar = new JarFile(assembly);\n+      needsDataNucleus = assemblyJar.getEntry(\"org/apache/hadoop/hive/ql/exec/\") != null;\n+    } catch (IOException ioe) {\n+      if (ioe.getMessage().indexOf(\"invalid CEN header\") > 0) {\n+        System.err.println(\n+          \"Loading Spark jar with '$JAR_CMD' failed.\\n\" +\n+          \"This is likely because Spark was compiled with Java 7 and run\\n\" +\n+          \"with Java 6 (see SPARK-1703). Please use Java 7 to run Spark\\n\" +\n+          \"or build Spark with Java 6.\");\n+        System.exit(1);\n+      } else {\n+        throw ioe;\n+      }\n+    } finally {\n+      if (assemblyJar != null) {\n+        try {\n+          assemblyJar.close();\n+        } catch (IOException e) {\n+          // Ignore.\n+        }\n+      }\n+    }\n+\n+    if (needsDataNucleus) {\n+      System.err.println(\"Spark assembly has been built with Hive, including Datanucleus jars \" +\n+        \"in classpath.\");\n+      File libdir;\n+      if (new File(sparkHome, \"RELEASE\").isFile()) {\n+        libdir = new File(sparkHome, \"lib\");\n+      } else {\n+        libdir = new File(sparkHome, \"lib_managed/jars\");\n+      }\n+\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+        libdir.getAbsolutePath());\n+      for (File jar : libdir.listFiles()) {\n+        if (jar.getName().startsWith(\"datanucleus-\")) {\n+          addToClassPath(cp, jar.getAbsolutePath());\n+        }\n+      }\n+    }\n+\n+    addToClassPath(cp, getenv(\"HADOOP_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"YARN_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"SPARK_DIST_CLASSPATH\"));\n+    return cp;\n+  }\n+\n+  private void addToClassPath(List<String> cp, String entries) {\n+    if (isEmpty(entries)) {\n+      return;\n+    }\n+    String[] split = entries.split(Pattern.quote(File.pathSeparator));\n+    for (String entry : split) {\n+      if (!isEmpty(entry)) {\n+        if (new File(entry).isDirectory() && !entry.endsWith(File.separator)) {\n+          entry += File.separator;\n+        }\n+        cp.add(entry);\n+      }\n+    }\n+  }\n+\n+  protected String getScalaVersion() {\n+    String scala = getenv(\"SPARK_SCALA_VERSION\");\n+    if (scala != null) {\n+      return scala;\n+    }\n+\n+    String sparkHome = getSparkHome();\n+    File scala210 = new File(sparkHome, \"assembly/target/scala-2.10\");\n+    File scala211 = new File(sparkHome, \"assembly/target/scala-2.11\");\n+    if (scala210.isDirectory() && scala211.isDirectory()) {\n+      checkState(false,\n+        \"Presence of build for both scala versions (2.10 and 2.11) detected.\\n\" +\n+        \"Either clean one of them or set SPARK_SCALA_VERSION in your environment.\");\n+    } else if (scala210.isDirectory()) {\n+      return \"2.10\";\n+    } else {\n+      checkState(scala211.isDirectory(), \"Cannot find any assembly build directories.\");\n+      return \"2.11\";\n+    }\n+\n+    throw new IllegalStateException(\"Should not reach here.\");\n+  }\n+\n+  /**\n+   * Which OS is running defines two things:\n+   * - the name of the environment variable used to define the lookup path for native libs\n+   * - how to execute the command in general.\n+   *\n+   * The name is easy: PATH on Win32, DYLD_LIBRARY_PATH on MacOS, LD_LIBRARY_PATH elsewhere.\n+   *\n+   * On Unix-like, we're assuming bash is available. So we print one argument per line to\n+   * the output, and use bash's array handling to execute the right thing.\n+   *\n+    * For Win32, see {@link #prepareForWindows(List<String>,String)}.\n+   */\n+  protected List<String> prepareForOs(List<String> cmd,\n+      String libPath,\n+      Map<String, String> env) {\n+\n+    // If SPARK_HOME does not come from the environment, explicitly set it\n+    // in the child's environment.\n+    Map<String, String> childEnv = env;\n+    if (System.getenv(\"SPARK_HOME\") == null && !env.containsKey(\"SPARK_HOME\")) {\n+      childEnv = new HashMap<String, String>(env);\n+      childEnv.put(\"SPARK_HOME\", sparkHome);\n+    }\n+\n+    if (isWindows()) {\n+      return prepareForWindows(cmd, libPath, childEnv);\n+    }\n+\n+    if (isEmpty(libPath) && childEnv.isEmpty()) {\n+      return cmd;\n+    }\n+\n+    List<String> newCmd = new ArrayList<String>();\n+    newCmd.add(\"env\");\n+\n+    if (!isEmpty(libPath)) {\n+      String envName = getLibPathEnvName();\n+      String currEnvValue = getenv(envName);\n+      String newEnvValue = join(File.pathSeparator, currEnvValue, libPath);\n+      newCmd.add(String.format(\"%s=%s\", envName, newEnvValue));\n+    }\n+    for (Map.Entry<String, String> e : childEnv.entrySet()) {\n+      newCmd.add(String.format(\"%s=%s\", e.getKey(), e.getValue()));\n+    }\n+    newCmd.addAll(cmd);\n+    return newCmd;\n+  }\n+\n+  protected String shQuote(String s) {\n+    StringBuilder quoted = new StringBuilder();\n+    boolean hasWhitespace = false;\n+    for (int i = 0; i < s.length(); i++) {\n+      if (Character.isWhitespace(s.codePointAt(i))) {\n+        quoted.append('\"');\n+        hasWhitespace = true;\n+        break;\n+      }\n+    }\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int cp = s.codePointAt(i);\n+      switch (cp) {\n+        case '\\'':\n+          if (hasWhitespace) {\n+            quoted.appendCodePoint(cp);\n+            break;\n+          }\n+        case '\"':\n+        case '\\\\':\n+          quoted.append('\\\\');\n+          // Fall through.\n+        default:\n+          if (Character.isWhitespace(cp)) {\n+            hasWhitespace=true;\n+          }\n+          quoted.appendCodePoint(cp);\n+      }\n+    }\n+    if (hasWhitespace) {\n+      quoted.append('\"');\n+    }\n+    return quoted.toString();\n+  }\n+\n+  // Visible for testing.\n+  List<String> parseOptionString(String s) {\n+    List<String> opts = new ArrayList<String>();\n+    StringBuilder opt = new StringBuilder();\n+    boolean inOpt = false;\n+    boolean inSingleQuote = false;\n+    boolean inDoubleQuote = false;\n+    boolean escapeNext = false;\n+    boolean hasData = false;\n+\n+    for (int i = 0; i < s.length(); i++) {\n+      int c = s.codePointAt(i);\n+      if (escapeNext) {\n+        if (!inOpt) {\n+          inOpt = true;\n+        }\n+        opt.appendCodePoint(c);\n+        escapeNext = false;\n+      } else if (inOpt) {\n+        switch (c) {\n+        case '\\\\':\n+          if (inSingleQuote) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            escapeNext = true;\n+          }\n+          break;\n+        case '\\'':\n+          if (inDoubleQuote) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            inSingleQuote = !inSingleQuote;\n+          }\n+          break;\n+        case '\"':\n+          if (inSingleQuote) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            inDoubleQuote = !inDoubleQuote;\n+          }\n+          break;\n+        default:\n+          if (inSingleQuote || inDoubleQuote || !Character.isWhitespace(c)) {\n+            opt.appendCodePoint(c);\n+          } else {\n+            finishOpt(opts, opt);\n+            inOpt = false;\n+            hasData = false;\n+          }\n+        }\n+      } else {\n+        switch (c) {\n+        case '\\'':\n+          inSingleQuote = true;\n+          inOpt = true;\n+          hasData = true;\n+          break;\n+        case '\"':\n+          inDoubleQuote = true;\n+          inOpt = true;\n+          hasData = true;\n+          break;\n+        case '\\\\':\n+          escapeNext = true;\n+          break;\n+        default:\n+          if (!Character.isWhitespace(c)) {\n+            inOpt = true;\n+            opt.appendCodePoint(c);\n+          }\n+        }\n+      }\n+    }\n+\n+    checkArgument(!inSingleQuote && !inDoubleQuote && !escapeNext, \"Invalid option string: %s\", s);\n+    if (opt.length() > 0 || hasData) {\n+      opts.add(opt.toString());\n+    }\n+    return opts;\n+  }\n+\n+  private void finishOpt(List<String> opts, StringBuilder opt) {\n+    opts.add(opt.toString());\n+    opt.setLength(0);\n+  }\n+\n+  private String findAssembly(String scalaVersion) {\n+    String sparkHome = getSparkHome();\n+    File libdir;\n+    if (new File(sparkHome, \"RELEASE\").isFile()) {\n+      libdir = new File(sparkHome, \"lib\");\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+          libdir.getAbsolutePath());\n+    } else {\n+      libdir = new File(sparkHome, String.format(\"assembly/target/scala-%s\", scalaVersion));\n+    }\n+\n+    final Pattern re = Pattern.compile(\"spark-assembly.*hadoop.*\\\\.jar\");\n+    FileFilter filter = new FileFilter() {\n+      @Override\n+      public boolean accept(File file) {\n+        return file.isFile() && re.matcher(file.getName()).matches();\n+      }\n+    };\n+    File[] assemblies = libdir.listFiles(filter);\n+    checkState(assemblies != null && assemblies.length > 0, \"No assemblies found in '%s'.\", libdir);\n+    checkState(assemblies.length == 1, \"Multiple assemblies found in '%s'.\", libdir);\n+    return assemblies[0].getAbsolutePath();\n+  }\n+\n+  private String getenv(String key) {\n+    return first(env != null ? env.get(key) : null, System.getenv(key));\n+  }\n+\n+  /**\n+   * Prepare a command line for execution on Windows.\n+   *\n+   * Two things need to be done:\n+   *\n+   * - If a custom library path is needed, extend PATH to add it. Based on:\n+   *   http://superuser.com/questions/223104/setting-environment-variable-for-just-one-command-in-windows-cmd-exe\n+   *\n+   * - Quote all arguments so that spaces are handled as expected. Quotes within arguments are\n+   *   \"double quoted\" (which is batch for escaping a quote). This page has more details about\n+   *   quoting and other batch script fun stuff: http://ss64.com/nt/syntax-esc.html\n+   */\n+  private List<String> prepareForWindows(List<String> cmd,\n+      String libPath,\n+      Map<String, String> env) {\n+    StringBuilder cmdline = new StringBuilder(\"cmd /c \\\"\");\n+    if (libPath != null) {\n+      cmdline.append(\"set PATH=%PATH%;\").append(libPath).append(\" &&\");\n+    }\n+    for (Map.Entry<String, String> e : env.entrySet()) {\n+      cmdline.append(String.format(\"set %s=%s\", e.getKey(), e.getValue()));\n+      cmdline.append(\" &&\");"
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "This could use a little explanation.  What is a launcher?  When should someone consider extending this class?\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-14T22:19:13Z",
    "diffHunk": "@@ -0,0 +1,451 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers."
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "This class is not meant to be extended outside of Spark; it's public because it actually contains some public APIs. I'll add a comment explaining that.\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-14T22:29:48Z",
    "diffHunk": "@@ -0,0 +1,451 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers."
  }, {
    "author": {
      "login": "sryza"
    },
    "body": "I still think a little more documentation would be useful for internal developers.\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-14T22:35:51Z",
    "diffHunk": "@@ -0,0 +1,451 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers."
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "tgravescs"
    },
    "body": "extra space after is\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-01-21T15:25:28Z",
    "diffHunk": "@@ -0,0 +1,461 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers - objects that encapsulate knowledge about how to build the\n+ * commands to run a Spark application or service. This class is not meant to be extended by user\n+ * code.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  private static final String ENV_SPARK_HOME = \"SPARK_HOME\";\n+  private static final String DEFAULT_PROPERTIES_FILE = \"spark-defaults.conf\";\n+  static final String DEFAULT_MEM = \"512m\";\n+\n+  String javaHome;\n+  String sparkHome;\n+  String propertiesFile;\n+  final Map<String, String> conf;\n+  final Map<String, String> launcherEnv;\n+\n+  AbstractLauncher() {\n+    this(Collections.<String, String>emptyMap());\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.conf = new HashMap<String, String>();\n+    this.launcherEnv = new HashMap<String, String>(env);\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /** Set a custom JAVA_HOME for launching the Spark application. */\n+  public T setJavaHome(String javaHome) {\n+    checkNotNull(javaHome, \"javaHome\");\n+    this.javaHome = javaHome;\n+    return THIS;\n+  }\n+\n+  /** Set a custom Spark installation location for the application. */\n+  public T setSparkHome(String sparkHome) {\n+    checkNotNull(sparkHome, \"sparkHome\");\n+    launcherEnv.put(ENV_SPARK_HOME, sparkHome);\n+    return THIS;\n+  }\n+\n+  /** Set a custom properties file with Spark configuration for the application. */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /** Set a single configuration value for the application. */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed. This method should\n+   * also update the environment map with any environment variables needed by the child process.\n+   * <p/>\n+   * Note that this method is a no-op in the base class, even though subclasses in this package\n+   * really must implement it. This approach was taken to allow this method to be package private\n+   * while still allowing CommandUtils.scala to extend this class for its use.\n+   *\n+   * @param env Map containing environment variables to set for the Spark job.\n+   */\n+  List<String> buildLauncherCommand(Map<String, String> env) throws IOException {\n+    throw new UnsupportedOperationException(\"Subclasses must implement this method.\");\n+  }\n+\n+  /**\n+   * Prepares the launcher command for execution from a shell script. This is used by the `Main`\n+   * class to service the scripts shipped with the Spark distribution.\n+   */\n+  List<String> buildShellCommand() throws IOException {\n+    Map<String, String> childEnv = new HashMap<String, String>(launcherEnv);\n+    List<String> cmd = buildLauncherCommand(childEnv);\n+    return isWindows() ? prepareForWindows(cmd, childEnv) : prepareForBash(cmd, childEnv);\n+  }\n+\n+  /**\n+   * Loads the configuration file for the application, if it exists. This is  either the"
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "Can this be made package private if we don't expect users to extend it? It looks like every use of this is inside of the launcher package.\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-02-19T01:21:08Z",
    "diffHunk": "@@ -0,0 +1,499 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers - objects that encapsulate knowledge about how to build the\n+ * commands to run a Spark application or service. This class is not meant to be extended by user\n+ * code.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "So, `CommandLauncher` (in CommandUtils.scala) extends it. This is similar to `SparkSubmitArguments` extending `SparkSubmitOptionParser`. For those two cases, I can probably play with the package of those two classes to allow making this package-private. How does that sound?\n\nThe other issue is that this actually provides public APIs that uses can call from `SparkLauncher`. I'm not sure of the exact semantics, but it's weird to expose public methods in a package-private class if they're meant to be used by code outside the package.\n\nI'll take a stab at turning things around, to see if I can have only `SparkLauncher` be public and have others build on top of it instead.\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-02-19T18:50:26Z",
    "diffHunk": "@@ -0,0 +1,499 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers - objects that encapsulate knowledge about how to build the\n+ * commands to run a Spark application or service. This class is not meant to be extended by user\n+ * code.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {"
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "I'm not sure this is ever used.\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-02-19T01:30:09Z",
    "diffHunk": "@@ -0,0 +1,499 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers - objects that encapsulate knowledge about how to build the\n+ * commands to run a Spark application or service. This class is not meant to be extended by user\n+ * code.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  private static final String ENV_SPARK_HOME = \"SPARK_HOME\";\n+  private static final String DEFAULT_PROPERTIES_FILE = \"spark-defaults.conf\";\n+  static final String DEFAULT_MEM = \"512m\";\n+\n+  String javaHome;\n+  String sparkHome;"
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "Should we call this `AbstractCommandBuilder` instead of \"Launcher\"  - the word Launcher is somewhat throwing me off? This class and its extensions do not necessarily launch anything themselves and the doc here really discusses command building functionality.\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-02-19T01:35:02Z",
    "diffHunk": "@@ -0,0 +1,499 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers - objects that encapsulate knowledge about how to build the\n+ * commands to run a Spark application or service. This class is not meant to be extended by user\n+ * code.\n+ */"
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "This method along with `prepareForWindows` and `quoteForBatchScript` all seem like utility methods independent of the state inside of a given instance. Can they be made static?\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-02-19T01:52:32Z",
    "diffHunk": "@@ -0,0 +1,499 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers - objects that encapsulate knowledge about how to build the\n+ * commands to run a Spark application or service. This class is not meant to be extended by user\n+ * code.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  private static final String ENV_SPARK_HOME = \"SPARK_HOME\";\n+  private static final String DEFAULT_PROPERTIES_FILE = \"spark-defaults.conf\";\n+  static final String DEFAULT_MEM = \"512m\";\n+\n+  String javaHome;\n+  String sparkHome;\n+  String propertiesFile;\n+  final Map<String, String> conf;\n+  final Map<String, String> launcherEnv;\n+\n+  AbstractLauncher() {\n+    this(Collections.<String, String>emptyMap());\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.conf = new HashMap<String, String>();\n+    this.launcherEnv = new HashMap<String, String>(env);\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /**\n+   * Set a custom JAVA_HOME for launching the Spark application.\n+   *\n+   * @param javaHome Path to the JAVA_HOME to use.\n+   * @return This launcher.\n+   */\n+  public T setJavaHome(String javaHome) {\n+    checkNotNull(javaHome, \"javaHome\");\n+    this.javaHome = javaHome;\n+    return THIS;\n+  }\n+\n+  /**\n+   * Set a custom Spark installation location for the application.\n+   *\n+   * @param sparkHome Path to the Spark installation to use.\n+   * @return This launcher.\n+   */\n+  public T setSparkHome(String sparkHome) {\n+    checkNotNull(sparkHome, \"sparkHome\");\n+    launcherEnv.put(ENV_SPARK_HOME, sparkHome);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Set a custom properties file with Spark configuration for the application.\n+   *\n+   * @param path Path to custom properties file to use.\n+   * @return This launcher.\n+   */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /**\n+   * Set a single configuration value for the application.\n+   *\n+   * @param key Configuration key.\n+   * @param value The value to use.\n+   * @return This launcher.\n+   */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed. This method should\n+   * also update the environment map with any environment variables needed by the child process.\n+   * <p/>\n+   * Note that this method is a no-op in the base class, even though subclasses in this package\n+   * really must implement it. This approach was taken to allow this method to be package private\n+   * while still allowing CommandUtils.scala to extend this class for its use.\n+   *\n+   * @param env Map containing environment variables to set for the Spark job.\n+   */\n+  List<String> buildLauncherCommand(Map<String, String> env) throws IOException {\n+    throw new UnsupportedOperationException(\"Subclasses must implement this method.\");\n+  }\n+\n+  /**\n+   * Prepares the launcher command for execution from a shell script. This is used by the `Main`\n+   * class to service the scripts shipped with the Spark distribution.\n+   */\n+  List<String> buildShellCommand() throws IOException {\n+    Map<String, String> childEnv = new HashMap<String, String>(launcherEnv);\n+    List<String> cmd = buildLauncherCommand(childEnv);\n+    return isWindows() ? prepareForWindows(cmd, childEnv) : prepareForBash(cmd, childEnv);\n+  }\n+\n+  /**\n+   * Loads the configuration file for the application, if it exists. This is either the\n+   * user-specified properties file, or the spark-defaults.conf file under the Spark configuration\n+   * directory.\n+   */\n+  Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      propsFile = new File(getConfDir(), DEFAULT_PROPERTIES_FILE);\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  String getSparkHome() {\n+    String path = getenv(ENV_SPARK_HOME);\n+    checkState(path != null,\n+      \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> buildJavaCommand(String extraClassPath) throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getConfDir(), \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    cmd.add(\"-cp\");\n+    cmd.add(join(File.pathSeparator, buildClassPath(extraClassPath)));\n+    return cmd;\n+  }\n+\n+  /**\n+   * Adds the default perm gen size option for Spark if the VM requires it and the user hasn't\n+   * set it.\n+   */\n+  protected void addPermGenSizeOpt(List<String> cmd) {\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) > 1 || Integer.parseInt(version[1]) > 7) {\n+      return;\n+    }\n+\n+    for (String arg : cmd) {\n+      if (arg.startsWith(\"-XX:MaxPermSize=\")) {\n+        return;\n+      }\n+    }\n+\n+    cmd.add(\"-XX:MaxPermSize=128m\");\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    addToClassPath(cp, getConfDir());\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+\n+      // Add this path to include jars that are shaded in the final deliverable created during\n+      // the maven build. These jars are copied to this directory during the build.\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars.\n+    //\n+    // This block also serves as a check for SPARK-1703, when the assembly jar is built with\n+    // Java 7 and ends up with too many files, causing issues with other JDK versions.\n+    boolean needsDataNucleus = false;\n+    JarFile assemblyJar = null;\n+    try {\n+      assemblyJar = new JarFile(assembly);\n+      needsDataNucleus = assemblyJar.getEntry(\"org/apache/hadoop/hive/ql/exec/\") != null;\n+    } catch (IOException ioe) {\n+      if (ioe.getMessage().indexOf(\"invalid CEN header\") > 0) {\n+        System.err.println(\n+          \"Loading Spark jar failed.\\n\" +\n+          \"This is likely because Spark was compiled with Java 7 and run\\n\" +\n+          \"with Java 6 (see SPARK-1703). Please use Java 7 to run Spark\\n\" +\n+          \"or build Spark with Java 6.\");\n+        System.exit(1);\n+      } else {\n+        throw ioe;\n+      }\n+    } finally {\n+      if (assemblyJar != null) {\n+        try {\n+          assemblyJar.close();\n+        } catch (IOException e) {\n+          // Ignore.\n+        }\n+      }\n+    }\n+\n+    if (needsDataNucleus) {\n+      System.err.println(\"Spark assembly has been built with Hive, including Datanucleus jars \" +\n+        \"in classpath.\");\n+      File libdir;\n+      if (new File(sparkHome, \"RELEASE\").isFile()) {\n+        libdir = new File(sparkHome, \"lib\");\n+      } else {\n+        libdir = new File(sparkHome, \"lib_managed/jars\");\n+      }\n+\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+        libdir.getAbsolutePath());\n+      for (File jar : libdir.listFiles()) {\n+        if (jar.getName().startsWith(\"datanucleus-\")) {\n+          addToClassPath(cp, jar.getAbsolutePath());\n+        }\n+      }\n+    }\n+\n+    addToClassPath(cp, getenv(\"HADOOP_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"YARN_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"SPARK_DIST_CLASSPATH\"));\n+    return cp;\n+  }\n+\n+  /**\n+   * Adds entries to the classpath.\n+   *\n+   * @param cp List where to appended the new classpath entries.\n+   * @param entries New classpath entries (separated by File.pathSeparator).\n+   */\n+  private void addToClassPath(List<String> cp, String entries) {\n+    if (isEmpty(entries)) {\n+      return;\n+    }\n+    String[] split = entries.split(Pattern.quote(File.pathSeparator));\n+    for (String entry : split) {\n+      if (!isEmpty(entry)) {\n+        if (new File(entry).isDirectory() && !entry.endsWith(File.separator)) {\n+          entry += File.separator;\n+        }\n+        cp.add(entry);\n+      }\n+    }\n+  }\n+\n+  String getScalaVersion() {\n+    String scala = getenv(\"SPARK_SCALA_VERSION\");\n+    if (scala != null) {\n+      return scala;\n+    }\n+\n+    String sparkHome = getSparkHome();\n+    File scala210 = new File(sparkHome, \"assembly/target/scala-2.10\");\n+    File scala211 = new File(sparkHome, \"assembly/target/scala-2.11\");\n+    if (scala210.isDirectory() && scala211.isDirectory()) {\n+      checkState(false,\n+        \"Presence of build for both scala versions (2.10 and 2.11) detected.\\n\" +\n+        \"Either clean one of them or set SPARK_SCALA_VERSION in your environment.\");\n+    } else if (scala210.isDirectory()) {\n+      return \"2.10\";\n+    } else {\n+      checkState(scala211.isDirectory(), \"Cannot find any assembly build directories.\");\n+      return \"2.11\";\n+    }\n+\n+    throw new IllegalStateException(\"Should not reach here.\");\n+  }\n+\n+  private String findAssembly(String scalaVersion) {\n+    String sparkHome = getSparkHome();\n+    File libdir;\n+    if (new File(sparkHome, \"RELEASE\").isFile()) {\n+      libdir = new File(sparkHome, \"lib\");\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+          libdir.getAbsolutePath());\n+    } else {\n+      libdir = new File(sparkHome, String.format(\"assembly/target/scala-%s\", scalaVersion));\n+    }\n+\n+    final Pattern re = Pattern.compile(\"spark-assembly.*hadoop.*\\\\.jar\");\n+    FileFilter filter = new FileFilter() {\n+      @Override\n+      public boolean accept(File file) {\n+        return file.isFile() && re.matcher(file.getName()).matches();\n+      }\n+    };\n+    File[] assemblies = libdir.listFiles(filter);\n+    checkState(assemblies != null && assemblies.length > 0, \"No assemblies found in '%s'.\", libdir);\n+    checkState(assemblies.length == 1, \"Multiple assemblies found in '%s'.\", libdir);\n+    return assemblies[0].getAbsolutePath();\n+  }\n+\n+  private String getenv(String key) {\n+    return firstNonEmpty(launcherEnv.get(key), System.getenv(key));\n+  }\n+\n+  private String getConfDir() {\n+    String confDir = getenv(\"SPARK_CONF_DIR\");\n+    return confDir != null ? confDir : join(File.separator, getSparkHome(), \"conf\");\n+  }\n+\n+  /**\n+   * Prepare the command for execution from a bash script. The final command will have commands to\n+   * set up any needed environment variables needed by the child process.\n+   */\n+  private List<String> prepareForBash(List<String> cmd, Map<String, String> childEnv) {"
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "Should this one be a static utility method?\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-02-19T01:54:23Z",
    "diffHunk": "@@ -0,0 +1,499 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers - objects that encapsulate knowledge about how to build the\n+ * commands to run a Spark application or service. This class is not meant to be extended by user\n+ * code.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  private static final String ENV_SPARK_HOME = \"SPARK_HOME\";\n+  private static final String DEFAULT_PROPERTIES_FILE = \"spark-defaults.conf\";\n+  static final String DEFAULT_MEM = \"512m\";\n+\n+  String javaHome;\n+  String sparkHome;\n+  String propertiesFile;\n+  final Map<String, String> conf;\n+  final Map<String, String> launcherEnv;\n+\n+  AbstractLauncher() {\n+    this(Collections.<String, String>emptyMap());\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.conf = new HashMap<String, String>();\n+    this.launcherEnv = new HashMap<String, String>(env);\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /**\n+   * Set a custom JAVA_HOME for launching the Spark application.\n+   *\n+   * @param javaHome Path to the JAVA_HOME to use.\n+   * @return This launcher.\n+   */\n+  public T setJavaHome(String javaHome) {\n+    checkNotNull(javaHome, \"javaHome\");\n+    this.javaHome = javaHome;\n+    return THIS;\n+  }\n+\n+  /**\n+   * Set a custom Spark installation location for the application.\n+   *\n+   * @param sparkHome Path to the Spark installation to use.\n+   * @return This launcher.\n+   */\n+  public T setSparkHome(String sparkHome) {\n+    checkNotNull(sparkHome, \"sparkHome\");\n+    launcherEnv.put(ENV_SPARK_HOME, sparkHome);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Set a custom properties file with Spark configuration for the application.\n+   *\n+   * @param path Path to custom properties file to use.\n+   * @return This launcher.\n+   */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /**\n+   * Set a single configuration value for the application.\n+   *\n+   * @param key Configuration key.\n+   * @param value The value to use.\n+   * @return This launcher.\n+   */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed. This method should\n+   * also update the environment map with any environment variables needed by the child process.\n+   * <p/>\n+   * Note that this method is a no-op in the base class, even though subclasses in this package\n+   * really must implement it. This approach was taken to allow this method to be package private\n+   * while still allowing CommandUtils.scala to extend this class for its use.\n+   *\n+   * @param env Map containing environment variables to set for the Spark job.\n+   */\n+  List<String> buildLauncherCommand(Map<String, String> env) throws IOException {\n+    throw new UnsupportedOperationException(\"Subclasses must implement this method.\");\n+  }\n+\n+  /**\n+   * Prepares the launcher command for execution from a shell script. This is used by the `Main`\n+   * class to service the scripts shipped with the Spark distribution.\n+   */\n+  List<String> buildShellCommand() throws IOException {\n+    Map<String, String> childEnv = new HashMap<String, String>(launcherEnv);\n+    List<String> cmd = buildLauncherCommand(childEnv);\n+    return isWindows() ? prepareForWindows(cmd, childEnv) : prepareForBash(cmd, childEnv);\n+  }\n+\n+  /**\n+   * Loads the configuration file for the application, if it exists. This is either the\n+   * user-specified properties file, or the spark-defaults.conf file under the Spark configuration\n+   * directory.\n+   */\n+  Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      propsFile = new File(getConfDir(), DEFAULT_PROPERTIES_FILE);\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  String getSparkHome() {\n+    String path = getenv(ENV_SPARK_HOME);\n+    checkState(path != null,\n+      \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> buildJavaCommand(String extraClassPath) throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getConfDir(), \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    cmd.add(\"-cp\");\n+    cmd.add(join(File.pathSeparator, buildClassPath(extraClassPath)));\n+    return cmd;\n+  }\n+\n+  /**\n+   * Adds the default perm gen size option for Spark if the VM requires it and the user hasn't\n+   * set it.\n+   */\n+  protected void addPermGenSizeOpt(List<String> cmd) {\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) > 1 || Integer.parseInt(version[1]) > 7) {\n+      return;\n+    }\n+\n+    for (String arg : cmd) {\n+      if (arg.startsWith(\"-XX:MaxPermSize=\")) {\n+        return;\n+      }\n+    }\n+\n+    cmd.add(\"-XX:MaxPermSize=128m\");\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    addToClassPath(cp, getConfDir());\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+\n+      // Add this path to include jars that are shaded in the final deliverable created during\n+      // the maven build. These jars are copied to this directory during the build.\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars.\n+    //\n+    // This block also serves as a check for SPARK-1703, when the assembly jar is built with\n+    // Java 7 and ends up with too many files, causing issues with other JDK versions.\n+    boolean needsDataNucleus = false;\n+    JarFile assemblyJar = null;\n+    try {\n+      assemblyJar = new JarFile(assembly);\n+      needsDataNucleus = assemblyJar.getEntry(\"org/apache/hadoop/hive/ql/exec/\") != null;\n+    } catch (IOException ioe) {\n+      if (ioe.getMessage().indexOf(\"invalid CEN header\") > 0) {\n+        System.err.println(\n+          \"Loading Spark jar failed.\\n\" +\n+          \"This is likely because Spark was compiled with Java 7 and run\\n\" +\n+          \"with Java 6 (see SPARK-1703). Please use Java 7 to run Spark\\n\" +\n+          \"or build Spark with Java 6.\");\n+        System.exit(1);\n+      } else {\n+        throw ioe;\n+      }\n+    } finally {\n+      if (assemblyJar != null) {\n+        try {\n+          assemblyJar.close();\n+        } catch (IOException e) {\n+          // Ignore.\n+        }\n+      }\n+    }\n+\n+    if (needsDataNucleus) {\n+      System.err.println(\"Spark assembly has been built with Hive, including Datanucleus jars \" +\n+        \"in classpath.\");\n+      File libdir;\n+      if (new File(sparkHome, \"RELEASE\").isFile()) {\n+        libdir = new File(sparkHome, \"lib\");\n+      } else {\n+        libdir = new File(sparkHome, \"lib_managed/jars\");\n+      }\n+\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+        libdir.getAbsolutePath());\n+      for (File jar : libdir.listFiles()) {\n+        if (jar.getName().startsWith(\"datanucleus-\")) {\n+          addToClassPath(cp, jar.getAbsolutePath());\n+        }\n+      }\n+    }\n+\n+    addToClassPath(cp, getenv(\"HADOOP_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"YARN_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"SPARK_DIST_CLASSPATH\"));\n+    return cp;\n+  }\n+\n+  /**\n+   * Adds entries to the classpath.\n+   *\n+   * @param cp List where to appended the new classpath entries.\n+   * @param entries New classpath entries (separated by File.pathSeparator).\n+   */\n+  private void addToClassPath(List<String> cp, String entries) {"
  }, {
    "author": {
      "login": "pwendell"
    },
    "body": "Overall, separating the methods that rely on or mutate internal state vs the ones that don't would help make this class easier to follow.\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-02-19T02:09:06Z",
    "diffHunk": "@@ -0,0 +1,499 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers - objects that encapsulate knowledge about how to build the\n+ * commands to run a Spark application or service. This class is not meant to be extended by user\n+ * code.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  private static final String ENV_SPARK_HOME = \"SPARK_HOME\";\n+  private static final String DEFAULT_PROPERTIES_FILE = \"spark-defaults.conf\";\n+  static final String DEFAULT_MEM = \"512m\";\n+\n+  String javaHome;\n+  String sparkHome;\n+  String propertiesFile;\n+  final Map<String, String> conf;\n+  final Map<String, String> launcherEnv;\n+\n+  AbstractLauncher() {\n+    this(Collections.<String, String>emptyMap());\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.conf = new HashMap<String, String>();\n+    this.launcherEnv = new HashMap<String, String>(env);\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /**\n+   * Set a custom JAVA_HOME for launching the Spark application.\n+   *\n+   * @param javaHome Path to the JAVA_HOME to use.\n+   * @return This launcher.\n+   */\n+  public T setJavaHome(String javaHome) {\n+    checkNotNull(javaHome, \"javaHome\");\n+    this.javaHome = javaHome;\n+    return THIS;\n+  }\n+\n+  /**\n+   * Set a custom Spark installation location for the application.\n+   *\n+   * @param sparkHome Path to the Spark installation to use.\n+   * @return This launcher.\n+   */\n+  public T setSparkHome(String sparkHome) {\n+    checkNotNull(sparkHome, \"sparkHome\");\n+    launcherEnv.put(ENV_SPARK_HOME, sparkHome);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Set a custom properties file with Spark configuration for the application.\n+   *\n+   * @param path Path to custom properties file to use.\n+   * @return This launcher.\n+   */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /**\n+   * Set a single configuration value for the application.\n+   *\n+   * @param key Configuration key.\n+   * @param value The value to use.\n+   * @return This launcher.\n+   */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed. This method should\n+   * also update the environment map with any environment variables needed by the child process.\n+   * <p/>\n+   * Note that this method is a no-op in the base class, even though subclasses in this package\n+   * really must implement it. This approach was taken to allow this method to be package private\n+   * while still allowing CommandUtils.scala to extend this class for its use.\n+   *\n+   * @param env Map containing environment variables to set for the Spark job.\n+   */\n+  List<String> buildLauncherCommand(Map<String, String> env) throws IOException {\n+    throw new UnsupportedOperationException(\"Subclasses must implement this method.\");\n+  }\n+\n+  /**\n+   * Prepares the launcher command for execution from a shell script. This is used by the `Main`\n+   * class to service the scripts shipped with the Spark distribution.\n+   */\n+  List<String> buildShellCommand() throws IOException {\n+    Map<String, String> childEnv = new HashMap<String, String>(launcherEnv);\n+    List<String> cmd = buildLauncherCommand(childEnv);\n+    return isWindows() ? prepareForWindows(cmd, childEnv) : prepareForBash(cmd, childEnv);\n+  }\n+\n+  /**\n+   * Loads the configuration file for the application, if it exists. This is either the\n+   * user-specified properties file, or the spark-defaults.conf file under the Spark configuration\n+   * directory.\n+   */\n+  Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      propsFile = new File(getConfDir(), DEFAULT_PROPERTIES_FILE);\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  String getSparkHome() {\n+    String path = getenv(ENV_SPARK_HOME);\n+    checkState(path != null,\n+      \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> buildJavaCommand(String extraClassPath) throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getConfDir(), \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    cmd.add(\"-cp\");\n+    cmd.add(join(File.pathSeparator, buildClassPath(extraClassPath)));\n+    return cmd;\n+  }\n+\n+  /**\n+   * Adds the default perm gen size option for Spark if the VM requires it and the user hasn't\n+   * set it.\n+   */\n+  protected void addPermGenSizeOpt(List<String> cmd) {\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) > 1 || Integer.parseInt(version[1]) > 7) {\n+      return;\n+    }\n+\n+    for (String arg : cmd) {\n+      if (arg.startsWith(\"-XX:MaxPermSize=\")) {\n+        return;\n+      }\n+    }\n+\n+    cmd.add(\"-XX:MaxPermSize=128m\");\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    addToClassPath(cp, getConfDir());\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+\n+      // Add this path to include jars that are shaded in the final deliverable created during\n+      // the maven build. These jars are copied to this directory during the build.\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars.\n+    //\n+    // This block also serves as a check for SPARK-1703, when the assembly jar is built with\n+    // Java 7 and ends up with too many files, causing issues with other JDK versions.\n+    boolean needsDataNucleus = false;\n+    JarFile assemblyJar = null;\n+    try {\n+      assemblyJar = new JarFile(assembly);\n+      needsDataNucleus = assemblyJar.getEntry(\"org/apache/hadoop/hive/ql/exec/\") != null;\n+    } catch (IOException ioe) {\n+      if (ioe.getMessage().indexOf(\"invalid CEN header\") > 0) {\n+        System.err.println(\n+          \"Loading Spark jar failed.\\n\" +\n+          \"This is likely because Spark was compiled with Java 7 and run\\n\" +\n+          \"with Java 6 (see SPARK-1703). Please use Java 7 to run Spark\\n\" +\n+          \"or build Spark with Java 6.\");\n+        System.exit(1);\n+      } else {\n+        throw ioe;\n+      }\n+    } finally {\n+      if (assemblyJar != null) {\n+        try {\n+          assemblyJar.close();\n+        } catch (IOException e) {\n+          // Ignore.\n+        }\n+      }\n+    }\n+\n+    if (needsDataNucleus) {\n+      System.err.println(\"Spark assembly has been built with Hive, including Datanucleus jars \" +\n+        \"in classpath.\");\n+      File libdir;\n+      if (new File(sparkHome, \"RELEASE\").isFile()) {\n+        libdir = new File(sparkHome, \"lib\");\n+      } else {\n+        libdir = new File(sparkHome, \"lib_managed/jars\");\n+      }\n+\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+        libdir.getAbsolutePath());\n+      for (File jar : libdir.listFiles()) {\n+        if (jar.getName().startsWith(\"datanucleus-\")) {\n+          addToClassPath(cp, jar.getAbsolutePath());\n+        }\n+      }\n+    }\n+\n+    addToClassPath(cp, getenv(\"HADOOP_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"YARN_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"SPARK_DIST_CLASSPATH\"));\n+    return cp;\n+  }\n+\n+  /**\n+   * Adds entries to the classpath.\n+   *\n+   * @param cp List where to appended the new classpath entries.\n+   * @param entries New classpath entries (separated by File.pathSeparator).\n+   */\n+  private void addToClassPath(List<String> cp, String entries) {"
  }, {
    "author": {
      "login": "pwendell"
    },
    "body": "Or maybe all of these should be moved to `LauncherCommon`?\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-02-19T02:09:48Z",
    "diffHunk": "@@ -0,0 +1,499 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers - objects that encapsulate knowledge about how to build the\n+ * commands to run a Spark application or service. This class is not meant to be extended by user\n+ * code.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  private static final String ENV_SPARK_HOME = \"SPARK_HOME\";\n+  private static final String DEFAULT_PROPERTIES_FILE = \"spark-defaults.conf\";\n+  static final String DEFAULT_MEM = \"512m\";\n+\n+  String javaHome;\n+  String sparkHome;\n+  String propertiesFile;\n+  final Map<String, String> conf;\n+  final Map<String, String> launcherEnv;\n+\n+  AbstractLauncher() {\n+    this(Collections.<String, String>emptyMap());\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.conf = new HashMap<String, String>();\n+    this.launcherEnv = new HashMap<String, String>(env);\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /**\n+   * Set a custom JAVA_HOME for launching the Spark application.\n+   *\n+   * @param javaHome Path to the JAVA_HOME to use.\n+   * @return This launcher.\n+   */\n+  public T setJavaHome(String javaHome) {\n+    checkNotNull(javaHome, \"javaHome\");\n+    this.javaHome = javaHome;\n+    return THIS;\n+  }\n+\n+  /**\n+   * Set a custom Spark installation location for the application.\n+   *\n+   * @param sparkHome Path to the Spark installation to use.\n+   * @return This launcher.\n+   */\n+  public T setSparkHome(String sparkHome) {\n+    checkNotNull(sparkHome, \"sparkHome\");\n+    launcherEnv.put(ENV_SPARK_HOME, sparkHome);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Set a custom properties file with Spark configuration for the application.\n+   *\n+   * @param path Path to custom properties file to use.\n+   * @return This launcher.\n+   */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /**\n+   * Set a single configuration value for the application.\n+   *\n+   * @param key Configuration key.\n+   * @param value The value to use.\n+   * @return This launcher.\n+   */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed. This method should\n+   * also update the environment map with any environment variables needed by the child process.\n+   * <p/>\n+   * Note that this method is a no-op in the base class, even though subclasses in this package\n+   * really must implement it. This approach was taken to allow this method to be package private\n+   * while still allowing CommandUtils.scala to extend this class for its use.\n+   *\n+   * @param env Map containing environment variables to set for the Spark job.\n+   */\n+  List<String> buildLauncherCommand(Map<String, String> env) throws IOException {\n+    throw new UnsupportedOperationException(\"Subclasses must implement this method.\");\n+  }\n+\n+  /**\n+   * Prepares the launcher command for execution from a shell script. This is used by the `Main`\n+   * class to service the scripts shipped with the Spark distribution.\n+   */\n+  List<String> buildShellCommand() throws IOException {\n+    Map<String, String> childEnv = new HashMap<String, String>(launcherEnv);\n+    List<String> cmd = buildLauncherCommand(childEnv);\n+    return isWindows() ? prepareForWindows(cmd, childEnv) : prepareForBash(cmd, childEnv);\n+  }\n+\n+  /**\n+   * Loads the configuration file for the application, if it exists. This is either the\n+   * user-specified properties file, or the spark-defaults.conf file under the Spark configuration\n+   * directory.\n+   */\n+  Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      propsFile = new File(getConfDir(), DEFAULT_PROPERTIES_FILE);\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  String getSparkHome() {\n+    String path = getenv(ENV_SPARK_HOME);\n+    checkState(path != null,\n+      \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> buildJavaCommand(String extraClassPath) throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getConfDir(), \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    cmd.add(\"-cp\");\n+    cmd.add(join(File.pathSeparator, buildClassPath(extraClassPath)));\n+    return cmd;\n+  }\n+\n+  /**\n+   * Adds the default perm gen size option for Spark if the VM requires it and the user hasn't\n+   * set it.\n+   */\n+  protected void addPermGenSizeOpt(List<String> cmd) {\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) > 1 || Integer.parseInt(version[1]) > 7) {\n+      return;\n+    }\n+\n+    for (String arg : cmd) {\n+      if (arg.startsWith(\"-XX:MaxPermSize=\")) {\n+        return;\n+      }\n+    }\n+\n+    cmd.add(\"-XX:MaxPermSize=128m\");\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  List<String> buildClassPath(String appClassPath) throws IOException {\n+    String sparkHome = getSparkHome();\n+    String scala = getScalaVersion();\n+\n+    List<String> cp = new ArrayList<String>();\n+    addToClassPath(cp, getenv(\"SPARK_CLASSPATH\"));\n+    addToClassPath(cp, appClassPath);\n+\n+    addToClassPath(cp, getConfDir());\n+\n+    boolean prependClasses = !isEmpty(getenv(\"SPARK_PREPEND_CLASSES\"));\n+    boolean isTesting = \"1\".equals(getenv(\"SPARK_TESTING\"));\n+    if (prependClasses || isTesting) {\n+      List<String> projects = Arrays.asList(\"core\", \"repl\", \"mllib\", \"bagel\", \"graphx\",\n+        \"streaming\", \"tools\", \"sql/catalyst\", \"sql/core\", \"sql/hive\", \"sql/hive-thriftserver\",\n+        \"yarn\", \"launcher\");\n+      if (prependClasses) {\n+        System.err.println(\n+          \"NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of \" +\n+          \"assembly.\");\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/classes\", sparkHome, project,\n+            scala));\n+        }\n+      }\n+      if (isTesting) {\n+        for (String project : projects) {\n+          addToClassPath(cp, String.format(\"%s/%s/target/scala-%s/test-classes\", sparkHome,\n+            project, scala));\n+        }\n+      }\n+\n+      // Add this path to include jars that are shaded in the final deliverable created during\n+      // the maven build. These jars are copied to this directory during the build.\n+      addToClassPath(cp, String.format(\"%s/core/target/jars/*\", sparkHome));\n+    }\n+\n+    String assembly = findAssembly(scala);\n+    addToClassPath(cp, assembly);\n+\n+    // When Hive support is needed, Datanucleus jars must be included on the classpath. Datanucleus\n+    // jars do not work if only included in the uber jar as plugin.xml metadata is lost. Both sbt\n+    // and maven will populate \"lib_managed/jars/\" with the datanucleus jars when Spark is built\n+    // with Hive, so first check if the datanucleus jars exist, and then ensure the current Spark\n+    // assembly is built for Hive, before actually populating the CLASSPATH with the jars.\n+    //\n+    // This block also serves as a check for SPARK-1703, when the assembly jar is built with\n+    // Java 7 and ends up with too many files, causing issues with other JDK versions.\n+    boolean needsDataNucleus = false;\n+    JarFile assemblyJar = null;\n+    try {\n+      assemblyJar = new JarFile(assembly);\n+      needsDataNucleus = assemblyJar.getEntry(\"org/apache/hadoop/hive/ql/exec/\") != null;\n+    } catch (IOException ioe) {\n+      if (ioe.getMessage().indexOf(\"invalid CEN header\") > 0) {\n+        System.err.println(\n+          \"Loading Spark jar failed.\\n\" +\n+          \"This is likely because Spark was compiled with Java 7 and run\\n\" +\n+          \"with Java 6 (see SPARK-1703). Please use Java 7 to run Spark\\n\" +\n+          \"or build Spark with Java 6.\");\n+        System.exit(1);\n+      } else {\n+        throw ioe;\n+      }\n+    } finally {\n+      if (assemblyJar != null) {\n+        try {\n+          assemblyJar.close();\n+        } catch (IOException e) {\n+          // Ignore.\n+        }\n+      }\n+    }\n+\n+    if (needsDataNucleus) {\n+      System.err.println(\"Spark assembly has been built with Hive, including Datanucleus jars \" +\n+        \"in classpath.\");\n+      File libdir;\n+      if (new File(sparkHome, \"RELEASE\").isFile()) {\n+        libdir = new File(sparkHome, \"lib\");\n+      } else {\n+        libdir = new File(sparkHome, \"lib_managed/jars\");\n+      }\n+\n+      checkState(libdir.isDirectory(), \"Library directory '%s' does not exist.\",\n+        libdir.getAbsolutePath());\n+      for (File jar : libdir.listFiles()) {\n+        if (jar.getName().startsWith(\"datanucleus-\")) {\n+          addToClassPath(cp, jar.getAbsolutePath());\n+        }\n+      }\n+    }\n+\n+    addToClassPath(cp, getenv(\"HADOOP_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"YARN_CONF_DIR\"));\n+    addToClassPath(cp, getenv(\"SPARK_DIST_CLASSPATH\"));\n+    return cp;\n+  }\n+\n+  /**\n+   * Adds entries to the classpath.\n+   *\n+   * @param cp List where to appended the new classpath entries.\n+   * @param entries New classpath entries (separated by File.pathSeparator).\n+   */\n+  private void addToClassPath(List<String> cp, String entries) {"
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "It would be helpful if this documents a bit the expected usage of a Launcher. It's something like you create a luncher, set various properties, and then call `buildShellCommand()`.\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-02-19T02:06:59Z",
    "diffHunk": "@@ -0,0 +1,499 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers - objects that encapsulate knowledge about how to build the\n+ * commands to run a Spark application or service. This class is not meant to be extended by user"
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "Could this just be called `buildCommand`?\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-02-19T02:07:33Z",
    "diffHunk": "@@ -0,0 +1,499 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers - objects that encapsulate knowledge about how to build the\n+ * commands to run a Spark application or service. This class is not meant to be extended by user\n+ * code.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  private static final String ENV_SPARK_HOME = \"SPARK_HOME\";\n+  private static final String DEFAULT_PROPERTIES_FILE = \"spark-defaults.conf\";\n+  static final String DEFAULT_MEM = \"512m\";\n+\n+  String javaHome;\n+  String sparkHome;\n+  String propertiesFile;\n+  final Map<String, String> conf;\n+  final Map<String, String> launcherEnv;\n+\n+  AbstractLauncher() {\n+    this(Collections.<String, String>emptyMap());\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.conf = new HashMap<String, String>();\n+    this.launcherEnv = new HashMap<String, String>(env);\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /**\n+   * Set a custom JAVA_HOME for launching the Spark application.\n+   *\n+   * @param javaHome Path to the JAVA_HOME to use.\n+   * @return This launcher.\n+   */\n+  public T setJavaHome(String javaHome) {\n+    checkNotNull(javaHome, \"javaHome\");\n+    this.javaHome = javaHome;\n+    return THIS;\n+  }\n+\n+  /**\n+   * Set a custom Spark installation location for the application.\n+   *\n+   * @param sparkHome Path to the Spark installation to use.\n+   * @return This launcher.\n+   */\n+  public T setSparkHome(String sparkHome) {\n+    checkNotNull(sparkHome, \"sparkHome\");\n+    launcherEnv.put(ENV_SPARK_HOME, sparkHome);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Set a custom properties file with Spark configuration for the application.\n+   *\n+   * @param path Path to custom properties file to use.\n+   * @return This launcher.\n+   */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /**\n+   * Set a single configuration value for the application.\n+   *\n+   * @param key Configuration key.\n+   * @param value The value to use.\n+   * @return This launcher.\n+   */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed. This method should\n+   * also update the environment map with any environment variables needed by the child process.\n+   * <p/>\n+   * Note that this method is a no-op in the base class, even though subclasses in this package\n+   * really must implement it. This approach was taken to allow this method to be package private\n+   * while still allowing CommandUtils.scala to extend this class for its use.\n+   *\n+   * @param env Map containing environment variables to set for the Spark job.\n+   */\n+  List<String> buildLauncherCommand(Map<String, String> env) throws IOException {\n+    throw new UnsupportedOperationException(\"Subclasses must implement this method.\");\n+  }\n+\n+  /**\n+   * Prepares the launcher command for execution from a shell script. This is used by the `Main`\n+   * class to service the scripts shipped with the Spark distribution.\n+   */\n+  List<String> buildShellCommand() throws IOException {"
  }],
  "prId": 3916
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "This seems like a function that will be difficult to make backwards compatible, if the intent is at some point to expose this to users.\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-02-19T02:59:16Z",
    "diffHunk": "@@ -0,0 +1,499 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers - objects that encapsulate knowledge about how to build the\n+ * commands to run a Spark application or service. This class is not meant to be extended by user\n+ * code.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  private static final String ENV_SPARK_HOME = \"SPARK_HOME\";\n+  private static final String DEFAULT_PROPERTIES_FILE = \"spark-defaults.conf\";\n+  static final String DEFAULT_MEM = \"512m\";\n+\n+  String javaHome;\n+  String sparkHome;\n+  String propertiesFile;\n+  final Map<String, String> conf;\n+  final Map<String, String> launcherEnv;\n+\n+  AbstractLauncher() {\n+    this(Collections.<String, String>emptyMap());\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.conf = new HashMap<String, String>();\n+    this.launcherEnv = new HashMap<String, String>(env);\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /**\n+   * Set a custom JAVA_HOME for launching the Spark application.\n+   *\n+   * @param javaHome Path to the JAVA_HOME to use.\n+   * @return This launcher.\n+   */\n+  public T setJavaHome(String javaHome) {\n+    checkNotNull(javaHome, \"javaHome\");\n+    this.javaHome = javaHome;\n+    return THIS;\n+  }\n+\n+  /**\n+   * Set a custom Spark installation location for the application.\n+   *\n+   * @param sparkHome Path to the Spark installation to use.\n+   * @return This launcher.\n+   */\n+  public T setSparkHome(String sparkHome) {\n+    checkNotNull(sparkHome, \"sparkHome\");\n+    launcherEnv.put(ENV_SPARK_HOME, sparkHome);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Set a custom properties file with Spark configuration for the application.\n+   *\n+   * @param path Path to custom properties file to use.\n+   * @return This launcher.\n+   */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /**\n+   * Set a single configuration value for the application.\n+   *\n+   * @param key Configuration key.\n+   * @param value The value to use.\n+   * @return This launcher.\n+   */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed. This method should\n+   * also update the environment map with any environment variables needed by the child process.\n+   * <p/>\n+   * Note that this method is a no-op in the base class, even though subclasses in this package\n+   * really must implement it. This approach was taken to allow this method to be package private\n+   * while still allowing CommandUtils.scala to extend this class for its use.\n+   *\n+   * @param env Map containing environment variables to set for the Spark job.\n+   */\n+  List<String> buildLauncherCommand(Map<String, String> env) throws IOException {\n+    throw new UnsupportedOperationException(\"Subclasses must implement this method.\");\n+  }\n+\n+  /**\n+   * Prepares the launcher command for execution from a shell script. This is used by the `Main`\n+   * class to service the scripts shipped with the Spark distribution.\n+   */\n+  List<String> buildShellCommand() throws IOException {\n+    Map<String, String> childEnv = new HashMap<String, String>(launcherEnv);\n+    List<String> cmd = buildLauncherCommand(childEnv);\n+    return isWindows() ? prepareForWindows(cmd, childEnv) : prepareForBash(cmd, childEnv);\n+  }\n+\n+  /**\n+   * Loads the configuration file for the application, if it exists. This is either the\n+   * user-specified properties file, or the spark-defaults.conf file under the Spark configuration\n+   * directory.\n+   */\n+  Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      propsFile = new File(getConfDir(), DEFAULT_PROPERTIES_FILE);\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  String getSparkHome() {\n+    String path = getenv(ENV_SPARK_HOME);\n+    checkState(path != null,\n+      \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> buildJavaCommand(String extraClassPath) throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getConfDir(), \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    cmd.add(\"-cp\");\n+    cmd.add(join(File.pathSeparator, buildClassPath(extraClassPath)));\n+    return cmd;\n+  }\n+\n+  /**\n+   * Adds the default perm gen size option for Spark if the VM requires it and the user hasn't\n+   * set it.\n+   */\n+  protected void addPermGenSizeOpt(List<String> cmd) {\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) > 1 || Integer.parseInt(version[1]) > 7) {\n+      return;\n+    }\n+\n+    for (String arg : cmd) {\n+      if (arg.startsWith(\"-XX:MaxPermSize=\")) {\n+        return;\n+      }\n+    }\n+\n+    cmd.add(\"-XX:MaxPermSize=128m\");\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  List<String> buildClassPath(String appClassPath) throws IOException {"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "I don't think this particular method is expected to be exposed to users. For backwards compatibility, it may need to be forked if the distribution layout of new Spark versions changes - that means there will be different ways of generating this classpath depending on the version of Spark being launched.\n\nBut that doesn't need to be exposed, it can all be internal to this code.\n",
    "commit": "18c7e4db3b9713c4bc13487e3a15e59b6bf2dc58",
    "createdAt": "2015-02-19T18:54:26Z",
    "diffHunk": "@@ -0,0 +1,499 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.launcher;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileInputStream;\n+import java.io.InputStreamReader;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.jar.JarFile;\n+import java.util.regex.Pattern;\n+\n+/**\n+ * Basic functionality for launchers - objects that encapsulate knowledge about how to build the\n+ * commands to run a Spark application or service. This class is not meant to be extended by user\n+ * code.\n+ */\n+public abstract class AbstractLauncher<T extends AbstractLauncher> extends LauncherCommon {\n+\n+  private static final String ENV_SPARK_HOME = \"SPARK_HOME\";\n+  private static final String DEFAULT_PROPERTIES_FILE = \"spark-defaults.conf\";\n+  static final String DEFAULT_MEM = \"512m\";\n+\n+  String javaHome;\n+  String sparkHome;\n+  String propertiesFile;\n+  final Map<String, String> conf;\n+  final Map<String, String> launcherEnv;\n+\n+  AbstractLauncher() {\n+    this(Collections.<String, String>emptyMap());\n+  }\n+\n+  protected AbstractLauncher(Map<String, String> env) {\n+    this.conf = new HashMap<String, String>();\n+    this.launcherEnv = new HashMap<String, String>(env);\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private final T THIS = (T) this;\n+\n+  /**\n+   * Set a custom JAVA_HOME for launching the Spark application.\n+   *\n+   * @param javaHome Path to the JAVA_HOME to use.\n+   * @return This launcher.\n+   */\n+  public T setJavaHome(String javaHome) {\n+    checkNotNull(javaHome, \"javaHome\");\n+    this.javaHome = javaHome;\n+    return THIS;\n+  }\n+\n+  /**\n+   * Set a custom Spark installation location for the application.\n+   *\n+   * @param sparkHome Path to the Spark installation to use.\n+   * @return This launcher.\n+   */\n+  public T setSparkHome(String sparkHome) {\n+    checkNotNull(sparkHome, \"sparkHome\");\n+    launcherEnv.put(ENV_SPARK_HOME, sparkHome);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Set a custom properties file with Spark configuration for the application.\n+   *\n+   * @param path Path to custom properties file to use.\n+   * @return This launcher.\n+   */\n+  public T setPropertiesFile(String path) {\n+    checkNotNull(path, \"path\");\n+    this.propertiesFile = path;\n+    return THIS;\n+  }\n+\n+  /**\n+   * Set a single configuration value for the application.\n+   *\n+   * @param key Configuration key.\n+   * @param value The value to use.\n+   * @return This launcher.\n+   */\n+  public T setConf(String key, String value) {\n+    checkNotNull(key, \"key\");\n+    checkNotNull(value, \"value\");\n+    checkArgument(key.startsWith(\"spark.\"), \"'key' must start with 'spark.'\");\n+    conf.put(key, value);\n+    return THIS;\n+  }\n+\n+  /**\n+   * Launchers should implement this to create the command to be executed. This method should\n+   * also update the environment map with any environment variables needed by the child process.\n+   * <p/>\n+   * Note that this method is a no-op in the base class, even though subclasses in this package\n+   * really must implement it. This approach was taken to allow this method to be package private\n+   * while still allowing CommandUtils.scala to extend this class for its use.\n+   *\n+   * @param env Map containing environment variables to set for the Spark job.\n+   */\n+  List<String> buildLauncherCommand(Map<String, String> env) throws IOException {\n+    throw new UnsupportedOperationException(\"Subclasses must implement this method.\");\n+  }\n+\n+  /**\n+   * Prepares the launcher command for execution from a shell script. This is used by the `Main`\n+   * class to service the scripts shipped with the Spark distribution.\n+   */\n+  List<String> buildShellCommand() throws IOException {\n+    Map<String, String> childEnv = new HashMap<String, String>(launcherEnv);\n+    List<String> cmd = buildLauncherCommand(childEnv);\n+    return isWindows() ? prepareForWindows(cmd, childEnv) : prepareForBash(cmd, childEnv);\n+  }\n+\n+  /**\n+   * Loads the configuration file for the application, if it exists. This is either the\n+   * user-specified properties file, or the spark-defaults.conf file under the Spark configuration\n+   * directory.\n+   */\n+  Properties loadPropertiesFile() throws IOException {\n+    Properties props = new Properties();\n+    File propsFile;\n+    if (propertiesFile != null) {\n+      propsFile = new File(propertiesFile);\n+      checkArgument(propsFile.isFile(), \"Invalid properties file '%s'.\", propertiesFile);\n+    } else {\n+      propsFile = new File(getConfDir(), DEFAULT_PROPERTIES_FILE);\n+    }\n+\n+    if (propsFile.isFile()) {\n+      FileInputStream fd = null;\n+      try {\n+        fd = new FileInputStream(propsFile);\n+        props.load(new InputStreamReader(fd, \"UTF-8\"));\n+      } finally {\n+        if (fd != null) {\n+          try {\n+            fd.close();\n+          } catch (IOException e) {\n+            // Ignore.\n+          }\n+        }\n+      }\n+    }\n+\n+    return props;\n+  }\n+\n+  String getSparkHome() {\n+    String path = getenv(ENV_SPARK_HOME);\n+    checkState(path != null,\n+      \"Spark home not found; set it explicitly or use the SPARK_HOME environment variable.\");\n+    return path;\n+  }\n+\n+  protected List<String> buildJavaCommand(String extraClassPath) throws IOException {\n+    List<String> cmd = new ArrayList<String>();\n+    if (javaHome == null) {\n+      cmd.add(join(File.separator, System.getProperty(\"java.home\"), \"bin\", \"java\"));\n+    } else {\n+      cmd.add(join(File.separator, javaHome, \"bin\", \"java\"));\n+    }\n+\n+    // Load extra JAVA_OPTS from conf/java-opts, if it exists.\n+    File javaOpts = new File(join(File.separator, getConfDir(), \"java-opts\"));\n+    if (javaOpts.isFile()) {\n+      BufferedReader br = new BufferedReader(new InputStreamReader(\n+          new FileInputStream(javaOpts), \"UTF-8\"));\n+      try {\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          addOptionString(cmd, line);\n+        }\n+      } finally {\n+        br.close();\n+      }\n+    }\n+\n+    cmd.add(\"-cp\");\n+    cmd.add(join(File.pathSeparator, buildClassPath(extraClassPath)));\n+    return cmd;\n+  }\n+\n+  /**\n+   * Adds the default perm gen size option for Spark if the VM requires it and the user hasn't\n+   * set it.\n+   */\n+  protected void addPermGenSizeOpt(List<String> cmd) {\n+    // Don't set MaxPermSize for Java 8 and later.\n+    String[] version = System.getProperty(\"java.version\").split(\"\\\\.\");\n+    if (Integer.parseInt(version[0]) > 1 || Integer.parseInt(version[1]) > 7) {\n+      return;\n+    }\n+\n+    for (String arg : cmd) {\n+      if (arg.startsWith(\"-XX:MaxPermSize=\")) {\n+        return;\n+      }\n+    }\n+\n+    cmd.add(\"-XX:MaxPermSize=128m\");\n+  }\n+\n+  protected void addOptionString(List<String> cmd, String options) {\n+    if (!isEmpty(options)) {\n+      for (String opt : parseOptionString(options)) {\n+        cmd.add(opt);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n+   * each entry is formatted in the way expected by <i>java.net.URLClassLoader</i> (more\n+   * specifically, with trailing slashes for directories).\n+   */\n+  List<String> buildClassPath(String appClassPath) throws IOException {"
  }],
  "prId": 3916
}]