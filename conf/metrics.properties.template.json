[{
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "@dongjoon-hyun Will this swamp the Prometheus server? A lot of metrics of numerous Spark applications will be sent to the central server, for example, `metrics_{$app_id}_driver_BlockManager_disk_diskSpaceUsed_MB_Value 0`\r\n\r\nHave you tried to run it ? How to deal with the high load issues? ",
    "commit": "0fda09667478b5010e53d614be4655fd01b845a7",
    "createdAt": "2019-10-05T23:45:52Z",
    "diffHunk": "@@ -192,4 +201,10 @@\n \n #driver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n \n-#executor.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\\ No newline at end of file\n+#executor.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n+\n+# Example configuration for PrometheusServlet\n+#*.sink.prometheusServlet.class=org.apache.spark.metrics.sink.PrometheusServlet\n+#*.sink.prometheusServlet.path=/metrics/prometheus\n+#master.sink.prometheusServlet.path=/metrics/master/prometheus\n+#applications.sink.prometheusServlet.path=/metrics/applications/prometheus",
    "line": 28
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Actually, I've used this in a long-running spark streaming app in the dedicated Spark standalone cluster.",
    "commit": "0fda09667478b5010e53d614be4655fd01b845a7",
    "createdAt": "2019-10-05T23:50:01Z",
    "diffHunk": "@@ -192,4 +201,10 @@\n \n #driver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n \n-#executor.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\\ No newline at end of file\n+#executor.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n+\n+# Example configuration for PrometheusServlet\n+#*.sink.prometheusServlet.class=org.apache.spark.metrics.sink.PrometheusServlet\n+#*.sink.prometheusServlet.path=/metrics/prometheus\n+#master.sink.prometheusServlet.path=/metrics/master/prometheus\n+#applications.sink.prometheusServlet.path=/metrics/applications/prometheus",
    "line": 28
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "And, I fully understand what you mean.",
    "commit": "0fda09667478b5010e53d614be4655fd01b845a7",
    "createdAt": "2019-10-05T23:51:06Z",
    "diffHunk": "@@ -192,4 +201,10 @@\n \n #driver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n \n-#executor.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\\ No newline at end of file\n+#executor.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n+\n+# Example configuration for PrometheusServlet\n+#*.sink.prometheusServlet.class=org.apache.spark.metrics.sink.PrometheusServlet\n+#*.sink.prometheusServlet.path=/metrics/prometheus\n+#master.sink.prometheusServlet.path=/metrics/master/prometheus\n+#applications.sink.prometheusServlet.path=/metrics/applications/prometheus",
    "line": 28
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "@gatorsmile . Did you try to use `spark.metrics.namespace`?\r\n```\r\n$ bin/spark-shell --conf spark.metrics.namespace=ns\r\n... (running)\r\n```\r\n\r\n```\r\n$ curl -s http://localhost:4040/metrics/prometheus/ | grep driver_BlockManager_disk_diskSpaceUse\r\nmetrics_ns_driver_BlockManager_disk_diskSpaceUsed_MB_Value 0\r\n```",
    "commit": "0fda09667478b5010e53d614be4655fd01b845a7",
    "createdAt": "2019-10-08T20:16:01Z",
    "diffHunk": "@@ -192,4 +201,10 @@\n \n #driver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n \n-#executor.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\\ No newline at end of file\n+#executor.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n+\n+# Example configuration for PrometheusServlet\n+#*.sink.prometheusServlet.class=org.apache.spark.metrics.sink.PrometheusServlet\n+#*.sink.prometheusServlet.path=/metrics/prometheus\n+#master.sink.prometheusServlet.path=/metrics/master/prometheus\n+#applications.sink.prometheusServlet.path=/metrics/applications/prometheus",
    "line": 28
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "The key is generated by `MetricsSystem.buildRegistryName`, not by the individual `Sink`s.\r\n- http://spark.apache.org/docs/latest/monitoring.html#metrics\r\n> However, often times, users want to be able to track the metrics across apps for driver and executors, which is hard to do with application ID (i.e. spark.app.id) since it changes with every invocation of the app. For such use cases, a custom namespace can be specified for metrics reporting using spark.metrics.namespace configuration property. \r\n\r\nI'd like to improve this further according to the additional requirement. Please let me know.",
    "commit": "0fda09667478b5010e53d614be4655fd01b845a7",
    "createdAt": "2019-10-08T20:20:48Z",
    "diffHunk": "@@ -192,4 +201,10 @@\n \n #driver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n \n-#executor.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\\ No newline at end of file\n+#executor.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n+\n+# Example configuration for PrometheusServlet\n+#*.sink.prometheusServlet.class=org.apache.spark.metrics.sink.PrometheusServlet\n+#*.sink.prometheusServlet.path=/metrics/prometheus\n+#master.sink.prometheusServlet.path=/metrics/master/prometheus\n+#applications.sink.prometheusServlet.path=/metrics/applications/prometheus",
    "line": 28
  }],
  "prId": 25769
}]