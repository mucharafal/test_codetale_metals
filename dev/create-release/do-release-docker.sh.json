[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "So we need to do `export UID=xxx` before running this script?",
    "commit": "745342d499ef290d5ce5484bcb2349d2be57a2fb",
    "createdAt": "2018-09-16T04:45:10Z",
    "diffHunk": "@@ -0,0 +1,143 @@\n+#!/usr/bin/env bash\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+#\n+# Creates a Spark release candidate. The script will update versions, tag the branch,\n+# build Spark binary packages and documentation, and upload maven artifacts to a staging\n+# repository. There is also a dry run mode where only local builds are performed, and\n+# nothing is uploaded to the ASF repos.\n+#\n+# Run with \"-h\" for options.\n+#\n+\n+set -e\n+SELF=$(cd $(dirname $0) && pwd)\n+. \"$SELF/release-util.sh\"\n+\n+function usage {\n+  local NAME=$(basename $0)\n+  cat <<EOF\n+Usage: $NAME [options]\n+\n+This script runs the release scripts inside a docker image. The image is hardcoded to be called\n+\"spark-rm\" and will be re-generated (as needed) on every invocation of this script.\n+\n+Options are:\n+\n+  -d [path]   : required: working directory (output will be written to an \"output\" directory in\n+                the working directory).\n+  -n          : dry run mode. Performs checks and local builds, but do not upload anything.\n+  -t [tag]    : tag for the spark-rm docker image to use for building (default: \"latest\").\n+  -j [path]   : path to local JDK installation to use for building. By default the script will\n+                use openjdk8 installed in the docker image.\n+  -s [step]   : runs a single step of the process; valid steps are: tag, build, docs, publish\n+EOF\n+}\n+\n+WORKDIR=\n+IMGTAG=latest\n+JAVA=\n+RELEASE_STEP=\n+while getopts \"d:hj:ns:t:\" opt; do\n+  case $opt in\n+    d) WORKDIR=\"$OPTARG\" ;;\n+    n) DRY_RUN=1 ;;\n+    t) IMGTAG=\"$OPTARG\" ;;\n+    j) JAVA=\"$OPTARG\" ;;\n+    s) RELEASE_STEP=\"$OPTARG\" ;;\n+    h) usage ;;\n+    ?) error \"Invalid option. Run with -h for help.\" ;;\n+  esac\n+done\n+\n+if [ -z \"$WORKDIR\" ] || [ ! -d \"$WORKDIR\" ]; then\n+  error \"Work directory (-d) must be defined and exist. Run with -h for help.\"\n+fi\n+\n+if [ -d \"$WORKDIR/output\" ]; then\n+  read -p \"Output directory already exists. Overwrite and continue? [y/n] \" ANSWER\n+  if [ \"$ANSWER\" != \"y\" ]; then\n+    error \"Exiting.\"\n+  fi\n+fi\n+\n+cd \"$WORKDIR\"\n+rm -rf \"$WORKDIR/output\"\n+mkdir \"$WORKDIR/output\"\n+\n+get_release_info\n+\n+# Place all RM scripts and necessary data in a local directory that must be defined in the command\n+# line. This directory is mounted into the image.\n+for f in \"$SELF\"/*; do\n+  if [ -f \"$f\" ]; then\n+    cp \"$f\" \"$WORKDIR\"\n+  fi\n+done\n+\n+GPG_KEY_FILE=\"$WORKDIR/gpg.key\"\n+fcreate_secure \"$GPG_KEY_FILE\"\n+$GPG --export-secret-key --armor \"$GPG_KEY\" > \"$GPG_KEY_FILE\"\n+\n+run_silent \"Building spark-rm image with tag $IMGTAG...\" \"docker-build.log\" \\\n+  docker build -t \"spark-rm:$IMGTAG\" --build-arg UID=$UID \"$SELF/spark-rm\"",
    "line": 99
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "got it. This is a system variable. So we can't run this script with root user...",
    "commit": "745342d499ef290d5ce5484bcb2349d2be57a2fb",
    "createdAt": "2018-09-16T04:47:55Z",
    "diffHunk": "@@ -0,0 +1,143 @@\n+#!/usr/bin/env bash\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+#\n+# Creates a Spark release candidate. The script will update versions, tag the branch,\n+# build Spark binary packages and documentation, and upload maven artifacts to a staging\n+# repository. There is also a dry run mode where only local builds are performed, and\n+# nothing is uploaded to the ASF repos.\n+#\n+# Run with \"-h\" for options.\n+#\n+\n+set -e\n+SELF=$(cd $(dirname $0) && pwd)\n+. \"$SELF/release-util.sh\"\n+\n+function usage {\n+  local NAME=$(basename $0)\n+  cat <<EOF\n+Usage: $NAME [options]\n+\n+This script runs the release scripts inside a docker image. The image is hardcoded to be called\n+\"spark-rm\" and will be re-generated (as needed) on every invocation of this script.\n+\n+Options are:\n+\n+  -d [path]   : required: working directory (output will be written to an \"output\" directory in\n+                the working directory).\n+  -n          : dry run mode. Performs checks and local builds, but do not upload anything.\n+  -t [tag]    : tag for the spark-rm docker image to use for building (default: \"latest\").\n+  -j [path]   : path to local JDK installation to use for building. By default the script will\n+                use openjdk8 installed in the docker image.\n+  -s [step]   : runs a single step of the process; valid steps are: tag, build, docs, publish\n+EOF\n+}\n+\n+WORKDIR=\n+IMGTAG=latest\n+JAVA=\n+RELEASE_STEP=\n+while getopts \"d:hj:ns:t:\" opt; do\n+  case $opt in\n+    d) WORKDIR=\"$OPTARG\" ;;\n+    n) DRY_RUN=1 ;;\n+    t) IMGTAG=\"$OPTARG\" ;;\n+    j) JAVA=\"$OPTARG\" ;;\n+    s) RELEASE_STEP=\"$OPTARG\" ;;\n+    h) usage ;;\n+    ?) error \"Invalid option. Run with -h for help.\" ;;\n+  esac\n+done\n+\n+if [ -z \"$WORKDIR\" ] || [ ! -d \"$WORKDIR\" ]; then\n+  error \"Work directory (-d) must be defined and exist. Run with -h for help.\"\n+fi\n+\n+if [ -d \"$WORKDIR/output\" ]; then\n+  read -p \"Output directory already exists. Overwrite and continue? [y/n] \" ANSWER\n+  if [ \"$ANSWER\" != \"y\" ]; then\n+    error \"Exiting.\"\n+  fi\n+fi\n+\n+cd \"$WORKDIR\"\n+rm -rf \"$WORKDIR/output\"\n+mkdir \"$WORKDIR/output\"\n+\n+get_release_info\n+\n+# Place all RM scripts and necessary data in a local directory that must be defined in the command\n+# line. This directory is mounted into the image.\n+for f in \"$SELF\"/*; do\n+  if [ -f \"$f\" ]; then\n+    cp \"$f\" \"$WORKDIR\"\n+  fi\n+done\n+\n+GPG_KEY_FILE=\"$WORKDIR/gpg.key\"\n+fcreate_secure \"$GPG_KEY_FILE\"\n+$GPG --export-secret-key --armor \"$GPG_KEY\" > \"$GPG_KEY_FILE\"\n+\n+run_silent \"Building spark-rm image with tag $IMGTAG...\" \"docker-build.log\" \\\n+  docker build -t \"spark-rm:$IMGTAG\" --build-arg UID=$UID \"$SELF/spark-rm\"",
    "line": 99
  }],
  "prId": 21515
}]