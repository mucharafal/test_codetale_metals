[{
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "I like ScalaTest's loan-fixture pattern for this type of cases where different tests need different fixtures that must perform cleanup: http://www.scalatest.org/user_guide/sharing_fixtures#loanFixtureMethods\n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-28T19:58:52Z",
    "diffHunk": "@@ -0,0 +1,221 @@\n+package org.apache.spark.streaming\n+\n+import java.io.File\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.duration._\n+import scala.language.postfixOps\n+import org.scalatest.{BeforeAndAfter, FunSuite, Matchers}\n+import org.scalatest.concurrent.Eventually._\n+\n+import akka.actor.{ActorSystem, Props}\n+import com.google.common.io.Files\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark._\n+import org.apache.spark.network.nio.NioBlockTransferService\n+import org.apache.spark.scheduler.LiveListenerBus\n+import org.apache.spark.serializer.KryoSerializer\n+import org.apache.spark.shuffle.hash.HashShuffleManager\n+import org.apache.spark.storage._\n+import org.apache.spark.streaming.util._\n+import org.apache.spark.streaming.receiver._\n+import org.apache.spark.util.AkkaUtils\n+import WriteAheadLogBasedBlockHandler._\n+import WriteAheadLogSuite._\n+\n+class ReceivedBlockHandlerSuite extends FunSuite with BeforeAndAfter with Matchers with Logging {\n+\n+  val conf = new SparkConf().set(\"spark.streaming.receiver.writeAheadLog.rollingInterval\", \"1\")\n+  val hadoopConf = new Configuration()\n+  val storageLevel = StorageLevel.MEMORY_ONLY_SER\n+  val streamId = 1\n+  val securityMgr = new SecurityManager(conf)\n+  val mapOutputTracker = new MapOutputTrackerMaster(conf)\n+  val shuffleManager = new HashShuffleManager(conf)\n+  val serializer = new KryoSerializer(conf)\n+  val manualClock = new ManualClock\n+  val blockManagerSize = 10000000\n+\n+  var actorSystem: ActorSystem = null\n+  var blockManagerMaster: BlockManagerMaster = null\n+  var blockManager: BlockManager = null\n+  var receivedBlockHandler: ReceivedBlockHandler = null\n+  var tempDirectory: File = null\n+\n+  before {\n+    val (actorSystem, boundPort) = AkkaUtils.createActorSystem(\n+      \"test\", \"localhost\", 0, conf = conf, securityManager = securityMgr)\n+    this.actorSystem = actorSystem\n+    conf.set(\"spark.driver.port\", boundPort.toString)\n+\n+    blockManagerMaster = new BlockManagerMaster(\n+      actorSystem.actorOf(Props(new BlockManagerMasterActor(true, conf, new LiveListenerBus))),\n+      conf, true)\n+\n+    blockManager = new BlockManager(\"bm\", actorSystem, blockManagerMaster, serializer,\n+      blockManagerSize, conf, mapOutputTracker, shuffleManager,\n+      new NioBlockTransferService(conf, securityMgr))\n+\n+    tempDirectory = Files.createTempDir()\n+    manualClock.setTime(0)\n+  }\n+\n+  after {\n+    if (receivedBlockHandler != null) {\n+      if (receivedBlockHandler.isInstanceOf[WriteAheadLogBasedBlockHandler]) {\n+        receivedBlockHandler.asInstanceOf[WriteAheadLogBasedBlockHandler].stop()\n+      }\n+    }\n+    if (blockManager != null) {\n+      blockManager.stop()\n+      blockManager = null\n+    }\n+    if (blockManagerMaster != null) {\n+      blockManagerMaster.stop()\n+      blockManagerMaster = null\n+    }\n+    actorSystem.shutdown()\n+    actorSystem.awaitTermination()\n+    actorSystem = null\n+\n+    if (tempDirectory != null && tempDirectory.exists()) {\n+      FileUtils.deleteDirectory(tempDirectory)\n+      tempDirectory = null\n+    }\n+  }\n+\n+  test(\"BlockManagerBasedBlockHandler - store blocks\") {\n+    createBlockManagerBasedBlockHandler()"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "That is most probably easier to read. Let me try that out. Thanks for the idea!\n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-28T22:09:18Z",
    "diffHunk": "@@ -0,0 +1,221 @@\n+package org.apache.spark.streaming\n+\n+import java.io.File\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.duration._\n+import scala.language.postfixOps\n+import org.scalatest.{BeforeAndAfter, FunSuite, Matchers}\n+import org.scalatest.concurrent.Eventually._\n+\n+import akka.actor.{ActorSystem, Props}\n+import com.google.common.io.Files\n+import org.apache.commons.io.FileUtils\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark._\n+import org.apache.spark.network.nio.NioBlockTransferService\n+import org.apache.spark.scheduler.LiveListenerBus\n+import org.apache.spark.serializer.KryoSerializer\n+import org.apache.spark.shuffle.hash.HashShuffleManager\n+import org.apache.spark.storage._\n+import org.apache.spark.streaming.util._\n+import org.apache.spark.streaming.receiver._\n+import org.apache.spark.util.AkkaUtils\n+import WriteAheadLogBasedBlockHandler._\n+import WriteAheadLogSuite._\n+\n+class ReceivedBlockHandlerSuite extends FunSuite with BeforeAndAfter with Matchers with Logging {\n+\n+  val conf = new SparkConf().set(\"spark.streaming.receiver.writeAheadLog.rollingInterval\", \"1\")\n+  val hadoopConf = new Configuration()\n+  val storageLevel = StorageLevel.MEMORY_ONLY_SER\n+  val streamId = 1\n+  val securityMgr = new SecurityManager(conf)\n+  val mapOutputTracker = new MapOutputTrackerMaster(conf)\n+  val shuffleManager = new HashShuffleManager(conf)\n+  val serializer = new KryoSerializer(conf)\n+  val manualClock = new ManualClock\n+  val blockManagerSize = 10000000\n+\n+  var actorSystem: ActorSystem = null\n+  var blockManagerMaster: BlockManagerMaster = null\n+  var blockManager: BlockManager = null\n+  var receivedBlockHandler: ReceivedBlockHandler = null\n+  var tempDirectory: File = null\n+\n+  before {\n+    val (actorSystem, boundPort) = AkkaUtils.createActorSystem(\n+      \"test\", \"localhost\", 0, conf = conf, securityManager = securityMgr)\n+    this.actorSystem = actorSystem\n+    conf.set(\"spark.driver.port\", boundPort.toString)\n+\n+    blockManagerMaster = new BlockManagerMaster(\n+      actorSystem.actorOf(Props(new BlockManagerMasterActor(true, conf, new LiveListenerBus))),\n+      conf, true)\n+\n+    blockManager = new BlockManager(\"bm\", actorSystem, blockManagerMaster, serializer,\n+      blockManagerSize, conf, mapOutputTracker, shuffleManager,\n+      new NioBlockTransferService(conf, securityMgr))\n+\n+    tempDirectory = Files.createTempDir()\n+    manualClock.setTime(0)\n+  }\n+\n+  after {\n+    if (receivedBlockHandler != null) {\n+      if (receivedBlockHandler.isInstanceOf[WriteAheadLogBasedBlockHandler]) {\n+        receivedBlockHandler.asInstanceOf[WriteAheadLogBasedBlockHandler].stop()\n+      }\n+    }\n+    if (blockManager != null) {\n+      blockManager.stop()\n+      blockManager = null\n+    }\n+    if (blockManagerMaster != null) {\n+      blockManagerMaster.stop()\n+      blockManagerMaster = null\n+    }\n+    actorSystem.shutdown()\n+    actorSystem.awaitTermination()\n+    actorSystem = null\n+\n+    if (tempDirectory != null && tempDirectory.exists()) {\n+      FileUtils.deleteDirectory(tempDirectory)\n+      tempDirectory = null\n+    }\n+  }\n+\n+  test(\"BlockManagerBasedBlockHandler - store blocks\") {\n+    createBlockManagerBasedBlockHandler()"
  }],
  "prId": 2940
}]