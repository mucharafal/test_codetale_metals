[{
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Style nit: place this `i =>` on the previous line.\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-25T01:08:24Z",
    "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.File\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+import org.scalatest.{BeforeAndAfter, FunSuite}\n+\n+import com.google.common.io.Files\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.storage.{BlockId, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, WriteAheadLogWriter}\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+class HDFSBackedBlockRDDSuite extends FunSuite with BeforeAndAfter {\n+  val conf = new SparkConf()\n+    .setMaster(\"local[2]\")\n+    .setAppName(this.getClass.getSimpleName)\n+  val sparkContext = new SparkContext(conf)\n+  val hadoopConf = new Configuration()\n+  val blockManager = sparkContext.env.blockManager\n+  // Since the same BM is reused in all tests, use an atomic int to generate ids\n+  val idGenerator = new AtomicInteger(0)\n+  var file: File = null\n+  var dir: File = null\n+\n+  before {\n+    dir = Files.createTempDir()\n+    file = new File(dir, \"BlockManagerWrite\")\n+  }\n+\n+  after {\n+    file.delete()\n+    dir.delete()\n+  }\n+\n+  test(\"Data available in BM and HDFS\") {\n+    doTestHDFSBackedRDD(5, 5, 20, 5)\n+  }\n+\n+  test(\"Data available in in BM but not in HDFS\") {\n+    doTestHDFSBackedRDD(5, 0, 20, 5)\n+  }\n+\n+  test(\"Data available in in HDFS and not in BM\") {\n+    doTestHDFSBackedRDD(0, 5, 20, 5)\n+  }\n+\n+  test(\"Data partially available in BM, and the rest in HDFS\") {\n+    doTestHDFSBackedRDD(3, 2, 20, 5)\n+  }\n+\n+  /**\n+   * Write a bunch of events into the HDFS Block RDD. Put a part of all of them to the\n+   * BlockManager, so all reads need not happen from HDFS.\n+   * @param total - Total number of Strings to write\n+   * @param blockCount - Number of blocks to write (therefore, total # of events per block =\n+   *                   total/blockCount\n+   */\n+  private def doTestHDFSBackedRDD(\n+      writeToBMCount: Int,\n+      writeToHDFSCount: Int,\n+      total: Int,\n+      blockCount: Int\n+    ) {\n+    val countPerBlock = total / blockCount\n+    val blockIds = (0 until blockCount).map {\n+      i =>"
  }],
  "prId": 2931
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "This should probably extend the `SharedSparkContext` test trait in Spark in order to ensure that the SparkContext is eventually cleaned up properly.\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-25T02:46:38Z",
    "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.File\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+import org.scalatest.{BeforeAndAfter, FunSuite}\n+\n+import com.google.common.io.Files\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.storage.{BlockId, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, WriteAheadLogWriter}\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+class HDFSBackedBlockRDDSuite extends FunSuite with BeforeAndAfter {"
  }, {
    "author": {
      "login": "harishreedharan"
    },
    "body": "Getting LocalSparkContext is actually sort of painful as it is a test class, so we need to create a test-jar in mvn, which sbt does not pull in causing compilation failures even if the test-jar is added to the pom.\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-25T06:37:36Z",
    "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.File\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+import org.scalatest.{BeforeAndAfter, FunSuite}\n+\n+import com.google.common.io.Files\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.storage.{BlockId, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, WriteAheadLogWriter}\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+class HDFSBackedBlockRDDSuite extends FunSuite with BeforeAndAfter {"
  }, {
    "author": {
      "login": "harishreedharan"
    },
    "body": "I copied the code for stopping and clearing the properties into this test though\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-25T06:38:04Z",
    "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.File\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+import org.scalatest.{BeforeAndAfter, FunSuite}\n+\n+import com.google.common.io.Files\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.storage.{BlockId, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, WriteAheadLogWriter}\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+class HDFSBackedBlockRDDSuite extends FunSuite with BeforeAndAfter {"
  }],
  "prId": 2931
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "You don't need a dash here, at least according to the style of the rest of the scaladocs.\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-25T02:47:45Z",
    "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.File\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+import org.scalatest.{BeforeAndAfter, FunSuite}\n+\n+import com.google.common.io.Files\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.storage.{BlockId, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, WriteAheadLogWriter}\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+class HDFSBackedBlockRDDSuite extends FunSuite with BeforeAndAfter {\n+  val conf = new SparkConf()\n+    .setMaster(\"local[2]\")\n+    .setAppName(this.getClass.getSimpleName)\n+  val sparkContext = new SparkContext(conf)\n+  val hadoopConf = new Configuration()\n+  val blockManager = sparkContext.env.blockManager\n+  // Since the same BM is reused in all tests, use an atomic int to generate ids\n+  val idGenerator = new AtomicInteger(0)\n+  var file: File = null\n+  var dir: File = null\n+\n+  before {\n+    dir = Files.createTempDir()\n+    file = new File(dir, \"BlockManagerWrite\")\n+  }\n+\n+  after {\n+    file.delete()\n+    dir.delete()\n+  }\n+\n+  test(\"Data available in BM and HDFS\") {\n+    doTestHDFSBackedRDD(5, 5, 20, 5)\n+  }\n+\n+  test(\"Data available in in BM but not in HDFS\") {\n+    doTestHDFSBackedRDD(5, 0, 20, 5)\n+  }\n+\n+  test(\"Data available in in HDFS and not in BM\") {\n+    doTestHDFSBackedRDD(0, 5, 20, 5)\n+  }\n+\n+  test(\"Data partially available in BM, and the rest in HDFS\") {\n+    doTestHDFSBackedRDD(3, 2, 20, 5)\n+  }\n+\n+  /**\n+   * Write a bunch of events into the HDFS Block RDD. Put a part of all of them to the\n+   * BlockManager, so all reads need not happen from HDFS.\n+   * @param total - Total number of Strings to write"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Removed.\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-27T19:01:36Z",
    "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.File\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+import org.scalatest.{BeforeAndAfter, FunSuite}\n+\n+import com.google.common.io.Files\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.storage.{BlockId, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, WriteAheadLogWriter}\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+class HDFSBackedBlockRDDSuite extends FunSuite with BeforeAndAfter {\n+  val conf = new SparkConf()\n+    .setMaster(\"local[2]\")\n+    .setAppName(this.getClass.getSimpleName)\n+  val sparkContext = new SparkContext(conf)\n+  val hadoopConf = new Configuration()\n+  val blockManager = sparkContext.env.blockManager\n+  // Since the same BM is reused in all tests, use an atomic int to generate ids\n+  val idGenerator = new AtomicInteger(0)\n+  var file: File = null\n+  var dir: File = null\n+\n+  before {\n+    dir = Files.createTempDir()\n+    file = new File(dir, \"BlockManagerWrite\")\n+  }\n+\n+  after {\n+    file.delete()\n+    dir.delete()\n+  }\n+\n+  test(\"Data available in BM and HDFS\") {\n+    doTestHDFSBackedRDD(5, 5, 20, 5)\n+  }\n+\n+  test(\"Data available in in BM but not in HDFS\") {\n+    doTestHDFSBackedRDD(5, 0, 20, 5)\n+  }\n+\n+  test(\"Data available in in HDFS and not in BM\") {\n+    doTestHDFSBackedRDD(0, 5, 20, 5)\n+  }\n+\n+  test(\"Data partially available in BM, and the rest in HDFS\") {\n+    doTestHDFSBackedRDD(3, 2, 20, 5)\n+  }\n+\n+  /**\n+   * Write a bunch of events into the HDFS Block RDD. Put a part of all of them to the\n+   * BlockManager, so all reads need not happen from HDFS.\n+   * @param total - Total number of Strings to write"
  }],
  "prId": 2931
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Extra blank line here.\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-25T02:52:36Z",
    "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.File\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+import org.scalatest.{BeforeAndAfter, FunSuite}\n+\n+import com.google.common.io.Files\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.storage.{BlockId, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, WriteAheadLogWriter}\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+class HDFSBackedBlockRDDSuite extends FunSuite with BeforeAndAfter {\n+  val conf = new SparkConf()\n+    .setMaster(\"local[2]\")\n+    .setAppName(this.getClass.getSimpleName)\n+  val sparkContext = new SparkContext(conf)\n+  val hadoopConf = new Configuration()\n+  val blockManager = sparkContext.env.blockManager\n+  // Since the same BM is reused in all tests, use an atomic int to generate ids\n+  val idGenerator = new AtomicInteger(0)\n+  var file: File = null\n+  var dir: File = null\n+\n+  before {\n+    dir = Files.createTempDir()\n+    file = new File(dir, \"BlockManagerWrite\")\n+  }\n+\n+  after {\n+    file.delete()\n+    dir.delete()\n+  }\n+\n+  test(\"Data available in BM and HDFS\") {\n+    doTestHDFSBackedRDD(5, 5, 20, 5)\n+  }\n+\n+  test(\"Data available in in BM but not in HDFS\") {\n+    doTestHDFSBackedRDD(5, 0, 20, 5)\n+  }\n+\n+  test(\"Data available in in HDFS and not in BM\") {\n+    doTestHDFSBackedRDD(0, 5, 20, 5)\n+  }\n+\n+  test(\"Data partially available in BM, and the rest in HDFS\") {\n+    doTestHDFSBackedRDD(3, 2, 20, 5)\n+  }\n+\n+  /**\n+   * Write a bunch of events into the HDFS Block RDD. Put a part of all of them to the\n+   * BlockManager, so all reads need not happen from HDFS.\n+   * @param total - Total number of Strings to write\n+   * @param blockCount - Number of blocks to write (therefore, total # of events per block =\n+   *                   total/blockCount\n+   */\n+  private def doTestHDFSBackedRDD(\n+      writeToBMCount: Int,\n+      writeToHDFSCount: Int,\n+      total: Int,\n+      blockCount: Int\n+    ) {\n+    val countPerBlock = total / blockCount\n+    val blockIds = (0 until blockCount).map {\n+      i =>\n+        StreamBlockId(idGenerator.incrementAndGet(), idGenerator.incrementAndGet())\n+    }\n+\n+    val writtenStrings = generateData(total, countPerBlock)\n+\n+    if (writeToBMCount != 0) {\n+      (0 until writeToBMCount).foreach { i =>\n+        blockManager\n+          .putIterator(blockIds(i), writtenStrings(i).iterator, StorageLevel.MEMORY_ONLY_SER)\n+      }\n+    }\n+\n+    val segments = new ArrayBuffer[WriteAheadLogFileSegment]\n+    if (writeToHDFSCount != 0) {\n+      // Generate some fake segments for the blocks in BM so the RDD does not complain\n+      segments ++= generateFakeSegments(writeToBMCount)\n+      segments ++= writeDataToHDFS(writtenStrings.slice(writeToBMCount, blockCount),\n+        blockIds.slice(writeToBMCount, blockCount))\n+"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Removed\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-27T19:01:15Z",
    "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.File\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+import org.scalatest.{BeforeAndAfter, FunSuite}\n+\n+import com.google.common.io.Files\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.storage.{BlockId, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, WriteAheadLogWriter}\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+class HDFSBackedBlockRDDSuite extends FunSuite with BeforeAndAfter {\n+  val conf = new SparkConf()\n+    .setMaster(\"local[2]\")\n+    .setAppName(this.getClass.getSimpleName)\n+  val sparkContext = new SparkContext(conf)\n+  val hadoopConf = new Configuration()\n+  val blockManager = sparkContext.env.blockManager\n+  // Since the same BM is reused in all tests, use an atomic int to generate ids\n+  val idGenerator = new AtomicInteger(0)\n+  var file: File = null\n+  var dir: File = null\n+\n+  before {\n+    dir = Files.createTempDir()\n+    file = new File(dir, \"BlockManagerWrite\")\n+  }\n+\n+  after {\n+    file.delete()\n+    dir.delete()\n+  }\n+\n+  test(\"Data available in BM and HDFS\") {\n+    doTestHDFSBackedRDD(5, 5, 20, 5)\n+  }\n+\n+  test(\"Data available in in BM but not in HDFS\") {\n+    doTestHDFSBackedRDD(5, 0, 20, 5)\n+  }\n+\n+  test(\"Data available in in HDFS and not in BM\") {\n+    doTestHDFSBackedRDD(0, 5, 20, 5)\n+  }\n+\n+  test(\"Data partially available in BM, and the rest in HDFS\") {\n+    doTestHDFSBackedRDD(3, 2, 20, 5)\n+  }\n+\n+  /**\n+   * Write a bunch of events into the HDFS Block RDD. Put a part of all of them to the\n+   * BlockManager, so all reads need not happen from HDFS.\n+   * @param total - Total number of Strings to write\n+   * @param blockCount - Number of blocks to write (therefore, total # of events per block =\n+   *                   total/blockCount\n+   */\n+  private def doTestHDFSBackedRDD(\n+      writeToBMCount: Int,\n+      writeToHDFSCount: Int,\n+      total: Int,\n+      blockCount: Int\n+    ) {\n+    val countPerBlock = total / blockCount\n+    val blockIds = (0 until blockCount).map {\n+      i =>\n+        StreamBlockId(idGenerator.incrementAndGet(), idGenerator.incrementAndGet())\n+    }\n+\n+    val writtenStrings = generateData(total, countPerBlock)\n+\n+    if (writeToBMCount != 0) {\n+      (0 until writeToBMCount).foreach { i =>\n+        blockManager\n+          .putIterator(blockIds(i), writtenStrings(i).iterator, StorageLevel.MEMORY_ONLY_SER)\n+      }\n+    }\n+\n+    val segments = new ArrayBuffer[WriteAheadLogFileSegment]\n+    if (writeToHDFSCount != 0) {\n+      // Generate some fake segments for the blocks in BM so the RDD does not complain\n+      segments ++= generateFakeSegments(writeToBMCount)\n+      segments ++= writeDataToHDFS(writtenStrings.slice(writeToBMCount, blockCount),\n+        blockIds.slice(writeToBMCount, blockCount))\n+"
  }],
  "prId": 2931
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "I'd just do `segments = if (writeToHDFSCount)` and return immutable segments from each branch to avoid making `segments` mutable.\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-25T02:53:36Z",
    "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.File\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+import org.scalatest.{BeforeAndAfter, FunSuite}\n+\n+import com.google.common.io.Files\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.storage.{BlockId, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, WriteAheadLogWriter}\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+class HDFSBackedBlockRDDSuite extends FunSuite with BeforeAndAfter {\n+  val conf = new SparkConf()\n+    .setMaster(\"local[2]\")\n+    .setAppName(this.getClass.getSimpleName)\n+  val sparkContext = new SparkContext(conf)\n+  val hadoopConf = new Configuration()\n+  val blockManager = sparkContext.env.blockManager\n+  // Since the same BM is reused in all tests, use an atomic int to generate ids\n+  val idGenerator = new AtomicInteger(0)\n+  var file: File = null\n+  var dir: File = null\n+\n+  before {\n+    dir = Files.createTempDir()\n+    file = new File(dir, \"BlockManagerWrite\")\n+  }\n+\n+  after {\n+    file.delete()\n+    dir.delete()\n+  }\n+\n+  test(\"Data available in BM and HDFS\") {\n+    doTestHDFSBackedRDD(5, 5, 20, 5)\n+  }\n+\n+  test(\"Data available in in BM but not in HDFS\") {\n+    doTestHDFSBackedRDD(5, 0, 20, 5)\n+  }\n+\n+  test(\"Data available in in HDFS and not in BM\") {\n+    doTestHDFSBackedRDD(0, 5, 20, 5)\n+  }\n+\n+  test(\"Data partially available in BM, and the rest in HDFS\") {\n+    doTestHDFSBackedRDD(3, 2, 20, 5)\n+  }\n+\n+  /**\n+   * Write a bunch of events into the HDFS Block RDD. Put a part of all of them to the\n+   * BlockManager, so all reads need not happen from HDFS.\n+   * @param total - Total number of Strings to write\n+   * @param blockCount - Number of blocks to write (therefore, total # of events per block =\n+   *                   total/blockCount\n+   */\n+  private def doTestHDFSBackedRDD(\n+      writeToBMCount: Int,\n+      writeToHDFSCount: Int,\n+      total: Int,\n+      blockCount: Int\n+    ) {\n+    val countPerBlock = total / blockCount\n+    val blockIds = (0 until blockCount).map {\n+      i =>\n+        StreamBlockId(idGenerator.incrementAndGet(), idGenerator.incrementAndGet())\n+    }\n+\n+    val writtenStrings = generateData(total, countPerBlock)\n+\n+    if (writeToBMCount != 0) {\n+      (0 until writeToBMCount).foreach { i =>\n+        blockManager\n+          .putIterator(blockIds(i), writtenStrings(i).iterator, StorageLevel.MEMORY_ONLY_SER)\n+      }\n+    }\n+\n+    val segments = new ArrayBuffer[WriteAheadLogFileSegment]"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Done.\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-27T19:01:20Z",
    "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.File\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable.ArrayBuffer\n+import org.scalatest.{BeforeAndAfter, FunSuite}\n+\n+import com.google.common.io.Files\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.storage.{BlockId, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, WriteAheadLogWriter}\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+class HDFSBackedBlockRDDSuite extends FunSuite with BeforeAndAfter {\n+  val conf = new SparkConf()\n+    .setMaster(\"local[2]\")\n+    .setAppName(this.getClass.getSimpleName)\n+  val sparkContext = new SparkContext(conf)\n+  val hadoopConf = new Configuration()\n+  val blockManager = sparkContext.env.blockManager\n+  // Since the same BM is reused in all tests, use an atomic int to generate ids\n+  val idGenerator = new AtomicInteger(0)\n+  var file: File = null\n+  var dir: File = null\n+\n+  before {\n+    dir = Files.createTempDir()\n+    file = new File(dir, \"BlockManagerWrite\")\n+  }\n+\n+  after {\n+    file.delete()\n+    dir.delete()\n+  }\n+\n+  test(\"Data available in BM and HDFS\") {\n+    doTestHDFSBackedRDD(5, 5, 20, 5)\n+  }\n+\n+  test(\"Data available in in BM but not in HDFS\") {\n+    doTestHDFSBackedRDD(5, 0, 20, 5)\n+  }\n+\n+  test(\"Data available in in HDFS and not in BM\") {\n+    doTestHDFSBackedRDD(0, 5, 20, 5)\n+  }\n+\n+  test(\"Data partially available in BM, and the rest in HDFS\") {\n+    doTestHDFSBackedRDD(3, 2, 20, 5)\n+  }\n+\n+  /**\n+   * Write a bunch of events into the HDFS Block RDD. Put a part of all of them to the\n+   * BlockManager, so all reads need not happen from HDFS.\n+   * @param total - Total number of Strings to write\n+   * @param blockCount - Number of blocks to write (therefore, total # of events per block =\n+   *                   total/blockCount\n+   */\n+  private def doTestHDFSBackedRDD(\n+      writeToBMCount: Int,\n+      writeToHDFSCount: Int,\n+      total: Int,\n+      blockCount: Int\n+    ) {\n+    val countPerBlock = total / blockCount\n+    val blockIds = (0 until blockCount).map {\n+      i =>\n+        StreamBlockId(idGenerator.incrementAndGet(), idGenerator.incrementAndGet())\n+    }\n+\n+    val writtenStrings = generateData(total, countPerBlock)\n+\n+    if (writeToBMCount != 0) {\n+      (0 until writeToBMCount).foreach { i =>\n+        blockManager\n+          .putIterator(blockIds(i), writtenStrings(i).iterator, StorageLevel.MEMORY_ONLY_SER)\n+      }\n+    }\n+\n+    val segments = new ArrayBuffer[WriteAheadLogFileSegment]"
  }],
  "prId": 2931
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "move this to the previous line?\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T06:07:23Z",
    "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.File\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import org.scalatest.{BeforeAndAfterAll, BeforeAndAfter, FunSuite}\n+\n+import com.google.common.io.Files\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.storage.{BlockManager, BlockId, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, WriteAheadLogWriter}\n+\n+class HDFSBackedBlockRDDSuite extends FunSuite with BeforeAndAfter with BeforeAndAfterAll {\n+  val conf = new SparkConf()\n+    .setMaster(\"local[2]\")\n+    .setAppName(this.getClass.getSimpleName)\n+  val hadoopConf = new Configuration()\n+  // Since the same BM is reused in all tests, use an atomic int to generate ids\n+  val idGenerator = new AtomicInteger(0)\n+  \n+  var sparkContext: SparkContext = null\n+  var blockManager: BlockManager = null\n+  var file: File = null\n+  var dir: File = null\n+\n+  before {\n+    blockManager = sparkContext.env.blockManager\n+    dir = Files.createTempDir()\n+    file = new File(dir, \"BlockManagerWrite\")\n+  }\n+\n+  after {\n+    file.delete()\n+    dir.delete()\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    sparkContext = new SparkContext(conf)\n+  }\n+\n+  override def afterAll(): Unit = {\n+    // Copied from LocalSparkContext which can't be imported since spark-core test-jar does not\n+    // get imported properly by sbt even if it is created.\n+    sparkContext.stop()\n+    System.clearProperty(\"spark.driver.port\")\n+  }\n+\n+  test(\"Data available in BM and HDFS\") {\n+    testHDFSBackedRDD(5, 5, 20, 5)\n+  }\n+\n+  test(\"Data available in in BM but not in HDFS\") {\n+    testHDFSBackedRDD(5, 0, 20, 5)\n+  }\n+\n+  test(\"Data available in in HDFS and not in BM\") {\n+    testHDFSBackedRDD(0, 5, 20, 5)\n+  }\n+\n+  test(\"Data partially available in BM, and the rest in HDFS\") {\n+    testHDFSBackedRDD(3, 2, 20, 5)\n+  }\n+\n+  /**\n+   * Write a bunch of events into the HDFS Block RDD. Put a part of all of them to the\n+   * BlockManager, so all reads need not happen from HDFS.\n+   * @param total Total number of Strings to write\n+   * @param blockCount Number of blocks to write (therefore, total # of events per block =\n+   *                   total/blockCount\n+   */\n+  private def testHDFSBackedRDD(\n+      writeToBMCount: Int,\n+      writeToHDFSCount: Int,\n+      total: Int,\n+      blockCount: Int\n+    ) {"
  }],
  "prId": 2931
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "Use Array.tabulate instead or Seq.tabulate\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T06:07:46Z",
    "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.File\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import org.scalatest.{BeforeAndAfterAll, BeforeAndAfter, FunSuite}\n+\n+import com.google.common.io.Files\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.storage.{BlockManager, BlockId, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, WriteAheadLogWriter}\n+\n+class HDFSBackedBlockRDDSuite extends FunSuite with BeforeAndAfter with BeforeAndAfterAll {\n+  val conf = new SparkConf()\n+    .setMaster(\"local[2]\")\n+    .setAppName(this.getClass.getSimpleName)\n+  val hadoopConf = new Configuration()\n+  // Since the same BM is reused in all tests, use an atomic int to generate ids\n+  val idGenerator = new AtomicInteger(0)\n+  \n+  var sparkContext: SparkContext = null\n+  var blockManager: BlockManager = null\n+  var file: File = null\n+  var dir: File = null\n+\n+  before {\n+    blockManager = sparkContext.env.blockManager\n+    dir = Files.createTempDir()\n+    file = new File(dir, \"BlockManagerWrite\")\n+  }\n+\n+  after {\n+    file.delete()\n+    dir.delete()\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    sparkContext = new SparkContext(conf)\n+  }\n+\n+  override def afterAll(): Unit = {\n+    // Copied from LocalSparkContext which can't be imported since spark-core test-jar does not\n+    // get imported properly by sbt even if it is created.\n+    sparkContext.stop()\n+    System.clearProperty(\"spark.driver.port\")\n+  }\n+\n+  test(\"Data available in BM and HDFS\") {\n+    testHDFSBackedRDD(5, 5, 20, 5)\n+  }\n+\n+  test(\"Data available in in BM but not in HDFS\") {\n+    testHDFSBackedRDD(5, 0, 20, 5)\n+  }\n+\n+  test(\"Data available in in HDFS and not in BM\") {\n+    testHDFSBackedRDD(0, 5, 20, 5)\n+  }\n+\n+  test(\"Data partially available in BM, and the rest in HDFS\") {\n+    testHDFSBackedRDD(3, 2, 20, 5)\n+  }\n+\n+  /**\n+   * Write a bunch of events into the HDFS Block RDD. Put a part of all of them to the\n+   * BlockManager, so all reads need not happen from HDFS.\n+   * @param total Total number of Strings to write\n+   * @param blockCount Number of blocks to write (therefore, total # of events per block =\n+   *                   total/blockCount\n+   */\n+  private def testHDFSBackedRDD(\n+      writeToBMCount: Int,\n+      writeToHDFSCount: Int,\n+      total: Int,\n+      blockCount: Int\n+    ) {\n+    val countPerBlock = total / blockCount\n+    val blockIds = (0 until blockCount).map { i =>"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Done.\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T21:40:01Z",
    "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.File\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import org.scalatest.{BeforeAndAfterAll, BeforeAndAfter, FunSuite}\n+\n+import com.google.common.io.Files\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.storage.{BlockManager, BlockId, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, WriteAheadLogWriter}\n+\n+class HDFSBackedBlockRDDSuite extends FunSuite with BeforeAndAfter with BeforeAndAfterAll {\n+  val conf = new SparkConf()\n+    .setMaster(\"local[2]\")\n+    .setAppName(this.getClass.getSimpleName)\n+  val hadoopConf = new Configuration()\n+  // Since the same BM is reused in all tests, use an atomic int to generate ids\n+  val idGenerator = new AtomicInteger(0)\n+  \n+  var sparkContext: SparkContext = null\n+  var blockManager: BlockManager = null\n+  var file: File = null\n+  var dir: File = null\n+\n+  before {\n+    blockManager = sparkContext.env.blockManager\n+    dir = Files.createTempDir()\n+    file = new File(dir, \"BlockManagerWrite\")\n+  }\n+\n+  after {\n+    file.delete()\n+    dir.delete()\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    sparkContext = new SparkContext(conf)\n+  }\n+\n+  override def afterAll(): Unit = {\n+    // Copied from LocalSparkContext which can't be imported since spark-core test-jar does not\n+    // get imported properly by sbt even if it is created.\n+    sparkContext.stop()\n+    System.clearProperty(\"spark.driver.port\")\n+  }\n+\n+  test(\"Data available in BM and HDFS\") {\n+    testHDFSBackedRDD(5, 5, 20, 5)\n+  }\n+\n+  test(\"Data available in in BM but not in HDFS\") {\n+    testHDFSBackedRDD(5, 0, 20, 5)\n+  }\n+\n+  test(\"Data available in in HDFS and not in BM\") {\n+    testHDFSBackedRDD(0, 5, 20, 5)\n+  }\n+\n+  test(\"Data partially available in BM, and the rest in HDFS\") {\n+    testHDFSBackedRDD(3, 2, 20, 5)\n+  }\n+\n+  /**\n+   * Write a bunch of events into the HDFS Block RDD. Put a part of all of them to the\n+   * BlockManager, so all reads need not happen from HDFS.\n+   * @param total Total number of Strings to write\n+   * @param blockCount Number of blocks to write (therefore, total # of events per block =\n+   *                   total/blockCount\n+   */\n+  private def testHDFSBackedRDD(\n+      writeToBMCount: Int,\n+      writeToHDFSCount: Int,\n+      total: Int,\n+      blockCount: Int\n+    ) {\n+    val countPerBlock = total / blockCount\n+    val blockIds = (0 until blockCount).map { i =>"
  }],
  "prId": 2931
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "indent off\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T06:07:55Z",
    "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.File\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import org.scalatest.{BeforeAndAfterAll, BeforeAndAfter, FunSuite}\n+\n+import com.google.common.io.Files\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.storage.{BlockManager, BlockId, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, WriteAheadLogWriter}\n+\n+class HDFSBackedBlockRDDSuite extends FunSuite with BeforeAndAfter with BeforeAndAfterAll {\n+  val conf = new SparkConf()\n+    .setMaster(\"local[2]\")\n+    .setAppName(this.getClass.getSimpleName)\n+  val hadoopConf = new Configuration()\n+  // Since the same BM is reused in all tests, use an atomic int to generate ids\n+  val idGenerator = new AtomicInteger(0)\n+  \n+  var sparkContext: SparkContext = null\n+  var blockManager: BlockManager = null\n+  var file: File = null\n+  var dir: File = null\n+\n+  before {\n+    blockManager = sparkContext.env.blockManager\n+    dir = Files.createTempDir()\n+    file = new File(dir, \"BlockManagerWrite\")\n+  }\n+\n+  after {\n+    file.delete()\n+    dir.delete()\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    sparkContext = new SparkContext(conf)\n+  }\n+\n+  override def afterAll(): Unit = {\n+    // Copied from LocalSparkContext which can't be imported since spark-core test-jar does not\n+    // get imported properly by sbt even if it is created.\n+    sparkContext.stop()\n+    System.clearProperty(\"spark.driver.port\")\n+  }\n+\n+  test(\"Data available in BM and HDFS\") {\n+    testHDFSBackedRDD(5, 5, 20, 5)\n+  }\n+\n+  test(\"Data available in in BM but not in HDFS\") {\n+    testHDFSBackedRDD(5, 0, 20, 5)\n+  }\n+\n+  test(\"Data available in in HDFS and not in BM\") {\n+    testHDFSBackedRDD(0, 5, 20, 5)\n+  }\n+\n+  test(\"Data partially available in BM, and the rest in HDFS\") {\n+    testHDFSBackedRDD(3, 2, 20, 5)\n+  }\n+\n+  /**\n+   * Write a bunch of events into the HDFS Block RDD. Put a part of all of them to the\n+   * BlockManager, so all reads need not happen from HDFS.\n+   * @param total Total number of Strings to write\n+   * @param blockCount Number of blocks to write (therefore, total # of events per block =\n+   *                   total/blockCount\n+   */\n+  private def testHDFSBackedRDD(\n+      writeToBMCount: Int,\n+      writeToHDFSCount: Int,\n+      total: Int,\n+      blockCount: Int\n+    ) {\n+    val countPerBlock = total / blockCount\n+    val blockIds = (0 until blockCount).map { i =>\n+        StreamBlockId(idGenerator.incrementAndGet(), idGenerator.incrementAndGet())"
  }],
  "prId": 2931
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "what do u mean by \"a part of all of them\"?\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T06:08:39Z",
    "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.File\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import org.scalatest.{BeforeAndAfterAll, BeforeAndAfter, FunSuite}\n+\n+import com.google.common.io.Files\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.storage.{BlockManager, BlockId, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, WriteAheadLogWriter}\n+\n+class HDFSBackedBlockRDDSuite extends FunSuite with BeforeAndAfter with BeforeAndAfterAll {\n+  val conf = new SparkConf()\n+    .setMaster(\"local[2]\")\n+    .setAppName(this.getClass.getSimpleName)\n+  val hadoopConf = new Configuration()\n+  // Since the same BM is reused in all tests, use an atomic int to generate ids\n+  val idGenerator = new AtomicInteger(0)\n+  \n+  var sparkContext: SparkContext = null\n+  var blockManager: BlockManager = null\n+  var file: File = null\n+  var dir: File = null\n+\n+  before {\n+    blockManager = sparkContext.env.blockManager\n+    dir = Files.createTempDir()\n+    file = new File(dir, \"BlockManagerWrite\")\n+  }\n+\n+  after {\n+    file.delete()\n+    dir.delete()\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    sparkContext = new SparkContext(conf)\n+  }\n+\n+  override def afterAll(): Unit = {\n+    // Copied from LocalSparkContext which can't be imported since spark-core test-jar does not\n+    // get imported properly by sbt even if it is created.\n+    sparkContext.stop()\n+    System.clearProperty(\"spark.driver.port\")\n+  }\n+\n+  test(\"Data available in BM and HDFS\") {\n+    testHDFSBackedRDD(5, 5, 20, 5)\n+  }\n+\n+  test(\"Data available in in BM but not in HDFS\") {\n+    testHDFSBackedRDD(5, 0, 20, 5)\n+  }\n+\n+  test(\"Data available in in HDFS and not in BM\") {\n+    testHDFSBackedRDD(0, 5, 20, 5)\n+  }\n+\n+  test(\"Data partially available in BM, and the rest in HDFS\") {\n+    testHDFSBackedRDD(3, 2, 20, 5)\n+  }\n+\n+  /**\n+   * Write a bunch of events into the HDFS Block RDD. Put a part of all of them to the"
  }],
  "prId": 2931
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "you should explain more clearly what this test function does.  it is long enough that it is no longer obvious, and your comment doesn't really address that.\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T06:09:21Z",
    "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.File\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import org.scalatest.{BeforeAndAfterAll, BeforeAndAfter, FunSuite}\n+\n+import com.google.common.io.Files\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.storage.{BlockManager, BlockId, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, WriteAheadLogWriter}\n+\n+class HDFSBackedBlockRDDSuite extends FunSuite with BeforeAndAfter with BeforeAndAfterAll {\n+  val conf = new SparkConf()\n+    .setMaster(\"local[2]\")\n+    .setAppName(this.getClass.getSimpleName)\n+  val hadoopConf = new Configuration()\n+  // Since the same BM is reused in all tests, use an atomic int to generate ids\n+  val idGenerator = new AtomicInteger(0)\n+  \n+  var sparkContext: SparkContext = null\n+  var blockManager: BlockManager = null\n+  var file: File = null\n+  var dir: File = null\n+\n+  before {\n+    blockManager = sparkContext.env.blockManager\n+    dir = Files.createTempDir()\n+    file = new File(dir, \"BlockManagerWrite\")\n+  }\n+\n+  after {\n+    file.delete()\n+    dir.delete()\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    sparkContext = new SparkContext(conf)\n+  }\n+\n+  override def afterAll(): Unit = {\n+    // Copied from LocalSparkContext which can't be imported since spark-core test-jar does not\n+    // get imported properly by sbt even if it is created.\n+    sparkContext.stop()\n+    System.clearProperty(\"spark.driver.port\")\n+  }\n+\n+  test(\"Data available in BM and HDFS\") {\n+    testHDFSBackedRDD(5, 5, 20, 5)\n+  }\n+\n+  test(\"Data available in in BM but not in HDFS\") {\n+    testHDFSBackedRDD(5, 0, 20, 5)\n+  }\n+\n+  test(\"Data available in in HDFS and not in BM\") {\n+    testHDFSBackedRDD(0, 5, 20, 5)\n+  }\n+\n+  test(\"Data partially available in BM, and the rest in HDFS\") {\n+    testHDFSBackedRDD(3, 2, 20, 5)\n+  }\n+\n+  /**\n+   * Write a bunch of events into the HDFS Block RDD. Put a part of all of them to the\n+   * BlockManager, so all reads need not happen from HDFS.\n+   * @param total Total number of Strings to write\n+   * @param blockCount Number of blocks to write (therefore, total # of events per block =\n+   *                   total/blockCount\n+   */\n+  private def testHDFSBackedRDD("
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "basically i'm worried the test case is too complicated for others to understand without enough comment, and it will be modified incorrectly in the future.\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T06:12:06Z",
    "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.File\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import org.scalatest.{BeforeAndAfterAll, BeforeAndAfter, FunSuite}\n+\n+import com.google.common.io.Files\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.storage.{BlockManager, BlockId, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, WriteAheadLogWriter}\n+\n+class HDFSBackedBlockRDDSuite extends FunSuite with BeforeAndAfter with BeforeAndAfterAll {\n+  val conf = new SparkConf()\n+    .setMaster(\"local[2]\")\n+    .setAppName(this.getClass.getSimpleName)\n+  val hadoopConf = new Configuration()\n+  // Since the same BM is reused in all tests, use an atomic int to generate ids\n+  val idGenerator = new AtomicInteger(0)\n+  \n+  var sparkContext: SparkContext = null\n+  var blockManager: BlockManager = null\n+  var file: File = null\n+  var dir: File = null\n+\n+  before {\n+    blockManager = sparkContext.env.blockManager\n+    dir = Files.createTempDir()\n+    file = new File(dir, \"BlockManagerWrite\")\n+  }\n+\n+  after {\n+    file.delete()\n+    dir.delete()\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    sparkContext = new SparkContext(conf)\n+  }\n+\n+  override def afterAll(): Unit = {\n+    // Copied from LocalSparkContext which can't be imported since spark-core test-jar does not\n+    // get imported properly by sbt even if it is created.\n+    sparkContext.stop()\n+    System.clearProperty(\"spark.driver.port\")\n+  }\n+\n+  test(\"Data available in BM and HDFS\") {\n+    testHDFSBackedRDD(5, 5, 20, 5)\n+  }\n+\n+  test(\"Data available in in BM but not in HDFS\") {\n+    testHDFSBackedRDD(5, 0, 20, 5)\n+  }\n+\n+  test(\"Data available in in HDFS and not in BM\") {\n+    testHDFSBackedRDD(0, 5, 20, 5)\n+  }\n+\n+  test(\"Data partially available in BM, and the rest in HDFS\") {\n+    testHDFSBackedRDD(3, 2, 20, 5)\n+  }\n+\n+  /**\n+   * Write a bunch of events into the HDFS Block RDD. Put a part of all of them to the\n+   * BlockManager, so all reads need not happen from HDFS.\n+   * @param total Total number of Strings to write\n+   * @param blockCount Number of blocks to write (therefore, total # of events per block =\n+   *                   total/blockCount\n+   */\n+  private def testHDFSBackedRDD("
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "Based on a cursory glance, it looks like this is testing that writing a HDFS-backed BlockRDD then reading it returns the same contents as the original RDD. Maybe add a comment to this effect?\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T16:46:10Z",
    "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.File\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import org.scalatest.{BeforeAndAfterAll, BeforeAndAfter, FunSuite}\n+\n+import com.google.common.io.Files\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.storage.{BlockManager, BlockId, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, WriteAheadLogWriter}\n+\n+class HDFSBackedBlockRDDSuite extends FunSuite with BeforeAndAfter with BeforeAndAfterAll {\n+  val conf = new SparkConf()\n+    .setMaster(\"local[2]\")\n+    .setAppName(this.getClass.getSimpleName)\n+  val hadoopConf = new Configuration()\n+  // Since the same BM is reused in all tests, use an atomic int to generate ids\n+  val idGenerator = new AtomicInteger(0)\n+  \n+  var sparkContext: SparkContext = null\n+  var blockManager: BlockManager = null\n+  var file: File = null\n+  var dir: File = null\n+\n+  before {\n+    blockManager = sparkContext.env.blockManager\n+    dir = Files.createTempDir()\n+    file = new File(dir, \"BlockManagerWrite\")\n+  }\n+\n+  after {\n+    file.delete()\n+    dir.delete()\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    sparkContext = new SparkContext(conf)\n+  }\n+\n+  override def afterAll(): Unit = {\n+    // Copied from LocalSparkContext which can't be imported since spark-core test-jar does not\n+    // get imported properly by sbt even if it is created.\n+    sparkContext.stop()\n+    System.clearProperty(\"spark.driver.port\")\n+  }\n+\n+  test(\"Data available in BM and HDFS\") {\n+    testHDFSBackedRDD(5, 5, 20, 5)\n+  }\n+\n+  test(\"Data available in in BM but not in HDFS\") {\n+    testHDFSBackedRDD(5, 0, 20, 5)\n+  }\n+\n+  test(\"Data available in in HDFS and not in BM\") {\n+    testHDFSBackedRDD(0, 5, 20, 5)\n+  }\n+\n+  test(\"Data partially available in BM, and the rest in HDFS\") {\n+    testHDFSBackedRDD(3, 2, 20, 5)\n+  }\n+\n+  /**\n+   * Write a bunch of events into the HDFS Block RDD. Put a part of all of them to the\n+   * BlockManager, so all reads need not happen from HDFS.\n+   * @param total Total number of Strings to write\n+   * @param blockCount Number of blocks to write (therefore, total # of events per block =\n+   *                   total/blockCount\n+   */\n+  private def testHDFSBackedRDD("
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Added.\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T21:39:12Z",
    "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.File\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import org.scalatest.{BeforeAndAfterAll, BeforeAndAfter, FunSuite}\n+\n+import com.google.common.io.Files\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.storage.{BlockManager, BlockId, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, WriteAheadLogWriter}\n+\n+class HDFSBackedBlockRDDSuite extends FunSuite with BeforeAndAfter with BeforeAndAfterAll {\n+  val conf = new SparkConf()\n+    .setMaster(\"local[2]\")\n+    .setAppName(this.getClass.getSimpleName)\n+  val hadoopConf = new Configuration()\n+  // Since the same BM is reused in all tests, use an atomic int to generate ids\n+  val idGenerator = new AtomicInteger(0)\n+  \n+  var sparkContext: SparkContext = null\n+  var blockManager: BlockManager = null\n+  var file: File = null\n+  var dir: File = null\n+\n+  before {\n+    blockManager = sparkContext.env.blockManager\n+    dir = Files.createTempDir()\n+    file = new File(dir, \"BlockManagerWrite\")\n+  }\n+\n+  after {\n+    file.delete()\n+    dir.delete()\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    sparkContext = new SparkContext(conf)\n+  }\n+\n+  override def afterAll(): Unit = {\n+    // Copied from LocalSparkContext which can't be imported since spark-core test-jar does not\n+    // get imported properly by sbt even if it is created.\n+    sparkContext.stop()\n+    System.clearProperty(\"spark.driver.port\")\n+  }\n+\n+  test(\"Data available in BM and HDFS\") {\n+    testHDFSBackedRDD(5, 5, 20, 5)\n+  }\n+\n+  test(\"Data available in in BM but not in HDFS\") {\n+    testHDFSBackedRDD(5, 0, 20, 5)\n+  }\n+\n+  test(\"Data available in in HDFS and not in BM\") {\n+    testHDFSBackedRDD(0, 5, 20, 5)\n+  }\n+\n+  test(\"Data partially available in BM, and the rest in HDFS\") {\n+    testHDFSBackedRDD(3, 2, 20, 5)\n+  }\n+\n+  /**\n+   * Write a bunch of events into the HDFS Block RDD. Put a part of all of them to the\n+   * BlockManager, so all reads need not happen from HDFS.\n+   * @param total Total number of Strings to write\n+   * @param blockCount Number of blocks to write (therefore, total # of events per block =\n+   *                   total/blockCount\n+   */\n+  private def testHDFSBackedRDD("
  }],
  "prId": 2931
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "Seq.fill\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T06:10:45Z",
    "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.File\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import org.scalatest.{BeforeAndAfterAll, BeforeAndAfter, FunSuite}\n+\n+import com.google.common.io.Files\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.storage.{BlockManager, BlockId, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, WriteAheadLogWriter}\n+\n+class HDFSBackedBlockRDDSuite extends FunSuite with BeforeAndAfter with BeforeAndAfterAll {\n+  val conf = new SparkConf()\n+    .setMaster(\"local[2]\")\n+    .setAppName(this.getClass.getSimpleName)\n+  val hadoopConf = new Configuration()\n+  // Since the same BM is reused in all tests, use an atomic int to generate ids\n+  val idGenerator = new AtomicInteger(0)\n+  \n+  var sparkContext: SparkContext = null\n+  var blockManager: BlockManager = null\n+  var file: File = null\n+  var dir: File = null\n+\n+  before {\n+    blockManager = sparkContext.env.blockManager\n+    dir = Files.createTempDir()\n+    file = new File(dir, \"BlockManagerWrite\")\n+  }\n+\n+  after {\n+    file.delete()\n+    dir.delete()\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    sparkContext = new SparkContext(conf)\n+  }\n+\n+  override def afterAll(): Unit = {\n+    // Copied from LocalSparkContext which can't be imported since spark-core test-jar does not\n+    // get imported properly by sbt even if it is created.\n+    sparkContext.stop()\n+    System.clearProperty(\"spark.driver.port\")\n+  }\n+\n+  test(\"Data available in BM and HDFS\") {\n+    testHDFSBackedRDD(5, 5, 20, 5)\n+  }\n+\n+  test(\"Data available in in BM but not in HDFS\") {\n+    testHDFSBackedRDD(5, 0, 20, 5)\n+  }\n+\n+  test(\"Data available in in HDFS and not in BM\") {\n+    testHDFSBackedRDD(0, 5, 20, 5)\n+  }\n+\n+  test(\"Data partially available in BM, and the rest in HDFS\") {\n+    testHDFSBackedRDD(3, 2, 20, 5)\n+  }\n+\n+  /**\n+   * Write a bunch of events into the HDFS Block RDD. Put a part of all of them to the\n+   * BlockManager, so all reads need not happen from HDFS.\n+   * @param total Total number of Strings to write\n+   * @param blockCount Number of blocks to write (therefore, total # of events per block =\n+   *                   total/blockCount\n+   */\n+  private def testHDFSBackedRDD(\n+      writeToBMCount: Int,\n+      writeToHDFSCount: Int,\n+      total: Int,\n+      blockCount: Int\n+    ) {\n+    val countPerBlock = total / blockCount\n+    val blockIds = (0 until blockCount).map { i =>\n+        StreamBlockId(idGenerator.incrementAndGet(), idGenerator.incrementAndGet())\n+    }\n+\n+    val writtenStrings = generateData(total, countPerBlock)\n+\n+    if (writeToBMCount != 0) {\n+      (0 until writeToBMCount).foreach { i =>\n+        blockManager\n+          .putIterator(blockIds(i), writtenStrings(i).iterator, StorageLevel.MEMORY_ONLY_SER)\n+      }\n+    }\n+\n+    val segments = {\n+      if (writeToHDFSCount != 0) {\n+        // Generate some fake segments for the blocks in BM so the RDD does not complain\n+        generateFakeSegments(writeToBMCount) ++\n+          writeDataToHDFS(writtenStrings.slice(writeToBMCount, blockCount),\n+            blockIds.slice(writeToBMCount, blockCount))\n+      } else {\n+        generateFakeSegments(blockCount)\n+      }\n+    }\n+\n+    val rdd = new HDFSBackedBlockRDD[String](sparkContext, hadoopConf, blockIds.toArray,\n+      segments.toArray, false, StorageLevel.MEMORY_ONLY)\n+\n+    val dataFromRDD = rdd.collect()\n+    // verify each partition is equal to the data pulled out\n+    assert(writtenStrings.flatten === dataFromRDD)\n+  }\n+\n+  /**\n+   * Write data to HDFS and get a list of Seq of Seqs in which each Seq represents the data that\n+   * went into one block.\n+   * @param count Number of Strings to write\n+   * @param countPerBlock Number of Strings per block\n+   * @return Seq of Seqs, each of these Seqs is one block\n+   */\n+  private def generateData(\n+      count: Int,\n+      countPerBlock: Int\n+    ): Seq[Seq[String]] = {\n+    val strings = (0 until count).map { _ => scala.util.Random.nextString(50)}\n+    strings.grouped(countPerBlock).toSeq\n+  }\n+\n+  private def writeDataToHDFS(\n+      blockData: Seq[Seq[String]],\n+      blockIds: Seq[BlockId]\n+    ): Seq[WriteAheadLogFileSegment] = {\n+    assert(blockData.size === blockIds.size)\n+    val segments = new ArrayBuffer[WriteAheadLogFileSegment]()\n+    val writer = new WriteAheadLogWriter(file.toString, hadoopConf)\n+    blockData.zip(blockIds).foreach {\n+      case (data, id) =>\n+        segments += writer.write(blockManager.dataSerialize(id, data.iterator))\n+    }\n+    writer.close()\n+    segments\n+  }\n+\n+  private def generateFakeSegments(count: Int): Seq[WriteAheadLogFileSegment] = {\n+    (0 until count).map { _ => new WriteAheadLogFileSegment(\"random\", 0l, 0) }"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Done.\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T21:39:47Z",
    "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.File\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import org.scalatest.{BeforeAndAfterAll, BeforeAndAfter, FunSuite}\n+\n+import com.google.common.io.Files\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.storage.{BlockManager, BlockId, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, WriteAheadLogWriter}\n+\n+class HDFSBackedBlockRDDSuite extends FunSuite with BeforeAndAfter with BeforeAndAfterAll {\n+  val conf = new SparkConf()\n+    .setMaster(\"local[2]\")\n+    .setAppName(this.getClass.getSimpleName)\n+  val hadoopConf = new Configuration()\n+  // Since the same BM is reused in all tests, use an atomic int to generate ids\n+  val idGenerator = new AtomicInteger(0)\n+  \n+  var sparkContext: SparkContext = null\n+  var blockManager: BlockManager = null\n+  var file: File = null\n+  var dir: File = null\n+\n+  before {\n+    blockManager = sparkContext.env.blockManager\n+    dir = Files.createTempDir()\n+    file = new File(dir, \"BlockManagerWrite\")\n+  }\n+\n+  after {\n+    file.delete()\n+    dir.delete()\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    sparkContext = new SparkContext(conf)\n+  }\n+\n+  override def afterAll(): Unit = {\n+    // Copied from LocalSparkContext which can't be imported since spark-core test-jar does not\n+    // get imported properly by sbt even if it is created.\n+    sparkContext.stop()\n+    System.clearProperty(\"spark.driver.port\")\n+  }\n+\n+  test(\"Data available in BM and HDFS\") {\n+    testHDFSBackedRDD(5, 5, 20, 5)\n+  }\n+\n+  test(\"Data available in in BM but not in HDFS\") {\n+    testHDFSBackedRDD(5, 0, 20, 5)\n+  }\n+\n+  test(\"Data available in in HDFS and not in BM\") {\n+    testHDFSBackedRDD(0, 5, 20, 5)\n+  }\n+\n+  test(\"Data partially available in BM, and the rest in HDFS\") {\n+    testHDFSBackedRDD(3, 2, 20, 5)\n+  }\n+\n+  /**\n+   * Write a bunch of events into the HDFS Block RDD. Put a part of all of them to the\n+   * BlockManager, so all reads need not happen from HDFS.\n+   * @param total Total number of Strings to write\n+   * @param blockCount Number of blocks to write (therefore, total # of events per block =\n+   *                   total/blockCount\n+   */\n+  private def testHDFSBackedRDD(\n+      writeToBMCount: Int,\n+      writeToHDFSCount: Int,\n+      total: Int,\n+      blockCount: Int\n+    ) {\n+    val countPerBlock = total / blockCount\n+    val blockIds = (0 until blockCount).map { i =>\n+        StreamBlockId(idGenerator.incrementAndGet(), idGenerator.incrementAndGet())\n+    }\n+\n+    val writtenStrings = generateData(total, countPerBlock)\n+\n+    if (writeToBMCount != 0) {\n+      (0 until writeToBMCount).foreach { i =>\n+        blockManager\n+          .putIterator(blockIds(i), writtenStrings(i).iterator, StorageLevel.MEMORY_ONLY_SER)\n+      }\n+    }\n+\n+    val segments = {\n+      if (writeToHDFSCount != 0) {\n+        // Generate some fake segments for the blocks in BM so the RDD does not complain\n+        generateFakeSegments(writeToBMCount) ++\n+          writeDataToHDFS(writtenStrings.slice(writeToBMCount, blockCount),\n+            blockIds.slice(writeToBMCount, blockCount))\n+      } else {\n+        generateFakeSegments(blockCount)\n+      }\n+    }\n+\n+    val rdd = new HDFSBackedBlockRDD[String](sparkContext, hadoopConf, blockIds.toArray,\n+      segments.toArray, false, StorageLevel.MEMORY_ONLY)\n+\n+    val dataFromRDD = rdd.collect()\n+    // verify each partition is equal to the data pulled out\n+    assert(writtenStrings.flatten === dataFromRDD)\n+  }\n+\n+  /**\n+   * Write data to HDFS and get a list of Seq of Seqs in which each Seq represents the data that\n+   * went into one block.\n+   * @param count Number of Strings to write\n+   * @param countPerBlock Number of Strings per block\n+   * @return Seq of Seqs, each of these Seqs is one block\n+   */\n+  private def generateData(\n+      count: Int,\n+      countPerBlock: Int\n+    ): Seq[Seq[String]] = {\n+    val strings = (0 until count).map { _ => scala.util.Random.nextString(50)}\n+    strings.grouped(countPerBlock).toSeq\n+  }\n+\n+  private def writeDataToHDFS(\n+      blockData: Seq[Seq[String]],\n+      blockIds: Seq[BlockId]\n+    ): Seq[WriteAheadLogFileSegment] = {\n+    assert(blockData.size === blockIds.size)\n+    val segments = new ArrayBuffer[WriteAheadLogFileSegment]()\n+    val writer = new WriteAheadLogWriter(file.toString, hadoopConf)\n+    blockData.zip(blockIds).foreach {\n+      case (data, id) =>\n+        segments += writer.write(blockManager.dataSerialize(id, data.iterator))\n+    }\n+    writer.close()\n+    segments\n+  }\n+\n+  private def generateFakeSegments(count: Int): Seq[WriteAheadLogFileSegment] = {\n+    (0 until count).map { _ => new WriteAheadLogFileSegment(\"random\", 0l, 0) }"
  }],
  "prId": 2931
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "move case to previous line\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T06:10:53Z",
    "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.File\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import org.scalatest.{BeforeAndAfterAll, BeforeAndAfter, FunSuite}\n+\n+import com.google.common.io.Files\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.storage.{BlockManager, BlockId, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, WriteAheadLogWriter}\n+\n+class HDFSBackedBlockRDDSuite extends FunSuite with BeforeAndAfter with BeforeAndAfterAll {\n+  val conf = new SparkConf()\n+    .setMaster(\"local[2]\")\n+    .setAppName(this.getClass.getSimpleName)\n+  val hadoopConf = new Configuration()\n+  // Since the same BM is reused in all tests, use an atomic int to generate ids\n+  val idGenerator = new AtomicInteger(0)\n+  \n+  var sparkContext: SparkContext = null\n+  var blockManager: BlockManager = null\n+  var file: File = null\n+  var dir: File = null\n+\n+  before {\n+    blockManager = sparkContext.env.blockManager\n+    dir = Files.createTempDir()\n+    file = new File(dir, \"BlockManagerWrite\")\n+  }\n+\n+  after {\n+    file.delete()\n+    dir.delete()\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    sparkContext = new SparkContext(conf)\n+  }\n+\n+  override def afterAll(): Unit = {\n+    // Copied from LocalSparkContext which can't be imported since spark-core test-jar does not\n+    // get imported properly by sbt even if it is created.\n+    sparkContext.stop()\n+    System.clearProperty(\"spark.driver.port\")\n+  }\n+\n+  test(\"Data available in BM and HDFS\") {\n+    testHDFSBackedRDD(5, 5, 20, 5)\n+  }\n+\n+  test(\"Data available in in BM but not in HDFS\") {\n+    testHDFSBackedRDD(5, 0, 20, 5)\n+  }\n+\n+  test(\"Data available in in HDFS and not in BM\") {\n+    testHDFSBackedRDD(0, 5, 20, 5)\n+  }\n+\n+  test(\"Data partially available in BM, and the rest in HDFS\") {\n+    testHDFSBackedRDD(3, 2, 20, 5)\n+  }\n+\n+  /**\n+   * Write a bunch of events into the HDFS Block RDD. Put a part of all of them to the\n+   * BlockManager, so all reads need not happen from HDFS.\n+   * @param total Total number of Strings to write\n+   * @param blockCount Number of blocks to write (therefore, total # of events per block =\n+   *                   total/blockCount\n+   */\n+  private def testHDFSBackedRDD(\n+      writeToBMCount: Int,\n+      writeToHDFSCount: Int,\n+      total: Int,\n+      blockCount: Int\n+    ) {\n+    val countPerBlock = total / blockCount\n+    val blockIds = (0 until blockCount).map { i =>\n+        StreamBlockId(idGenerator.incrementAndGet(), idGenerator.incrementAndGet())\n+    }\n+\n+    val writtenStrings = generateData(total, countPerBlock)\n+\n+    if (writeToBMCount != 0) {\n+      (0 until writeToBMCount).foreach { i =>\n+        blockManager\n+          .putIterator(blockIds(i), writtenStrings(i).iterator, StorageLevel.MEMORY_ONLY_SER)\n+      }\n+    }\n+\n+    val segments = {\n+      if (writeToHDFSCount != 0) {\n+        // Generate some fake segments for the blocks in BM so the RDD does not complain\n+        generateFakeSegments(writeToBMCount) ++\n+          writeDataToHDFS(writtenStrings.slice(writeToBMCount, blockCount),\n+            blockIds.slice(writeToBMCount, blockCount))\n+      } else {\n+        generateFakeSegments(blockCount)\n+      }\n+    }\n+\n+    val rdd = new HDFSBackedBlockRDD[String](sparkContext, hadoopConf, blockIds.toArray,\n+      segments.toArray, false, StorageLevel.MEMORY_ONLY)\n+\n+    val dataFromRDD = rdd.collect()\n+    // verify each partition is equal to the data pulled out\n+    assert(writtenStrings.flatten === dataFromRDD)\n+  }\n+\n+  /**\n+   * Write data to HDFS and get a list of Seq of Seqs in which each Seq represents the data that\n+   * went into one block.\n+   * @param count Number of Strings to write\n+   * @param countPerBlock Number of Strings per block\n+   * @return Seq of Seqs, each of these Seqs is one block\n+   */\n+  private def generateData(\n+      count: Int,\n+      countPerBlock: Int\n+    ): Seq[Seq[String]] = {\n+    val strings = (0 until count).map { _ => scala.util.Random.nextString(50)}\n+    strings.grouped(countPerBlock).toSeq\n+  }\n+\n+  private def writeDataToHDFS(\n+      blockData: Seq[Seq[String]],\n+      blockIds: Seq[BlockId]\n+    ): Seq[WriteAheadLogFileSegment] = {\n+    assert(blockData.size === blockIds.size)\n+    val segments = new ArrayBuffer[WriteAheadLogFileSegment]()\n+    val writer = new WriteAheadLogWriter(file.toString, hadoopConf)\n+    blockData.zip(blockIds).foreach {\n+      case (data, id) =>"
  }],
  "prId": 2931
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "imports out of order\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T06:12:42Z",
    "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.File"
  }],
  "prId": 2931
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "`.tabulate`\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T16:43:16Z",
    "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.File\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import org.scalatest.{BeforeAndAfterAll, BeforeAndAfter, FunSuite}\n+\n+import com.google.common.io.Files\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.storage.{BlockManager, BlockId, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, WriteAheadLogWriter}\n+\n+class HDFSBackedBlockRDDSuite extends FunSuite with BeforeAndAfter with BeforeAndAfterAll {\n+  val conf = new SparkConf()\n+    .setMaster(\"local[2]\")\n+    .setAppName(this.getClass.getSimpleName)\n+  val hadoopConf = new Configuration()\n+  // Since the same BM is reused in all tests, use an atomic int to generate ids\n+  val idGenerator = new AtomicInteger(0)\n+  \n+  var sparkContext: SparkContext = null\n+  var blockManager: BlockManager = null\n+  var file: File = null\n+  var dir: File = null\n+\n+  before {\n+    blockManager = sparkContext.env.blockManager\n+    dir = Files.createTempDir()\n+    file = new File(dir, \"BlockManagerWrite\")\n+  }\n+\n+  after {\n+    file.delete()\n+    dir.delete()\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    sparkContext = new SparkContext(conf)\n+  }\n+\n+  override def afterAll(): Unit = {\n+    // Copied from LocalSparkContext which can't be imported since spark-core test-jar does not\n+    // get imported properly by sbt even if it is created.\n+    sparkContext.stop()\n+    System.clearProperty(\"spark.driver.port\")\n+  }\n+\n+  test(\"Data available in BM and HDFS\") {\n+    testHDFSBackedRDD(5, 5, 20, 5)\n+  }\n+\n+  test(\"Data available in in BM but not in HDFS\") {\n+    testHDFSBackedRDD(5, 0, 20, 5)\n+  }\n+\n+  test(\"Data available in in HDFS and not in BM\") {\n+    testHDFSBackedRDD(0, 5, 20, 5)\n+  }\n+\n+  test(\"Data partially available in BM, and the rest in HDFS\") {\n+    testHDFSBackedRDD(3, 2, 20, 5)\n+  }\n+\n+  /**\n+   * Write a bunch of events into the HDFS Block RDD. Put a part of all of them to the\n+   * BlockManager, so all reads need not happen from HDFS.\n+   * @param total Total number of Strings to write\n+   * @param blockCount Number of blocks to write (therefore, total # of events per block =\n+   *                   total/blockCount\n+   */\n+  private def testHDFSBackedRDD(\n+      writeToBMCount: Int,\n+      writeToHDFSCount: Int,\n+      total: Int,\n+      blockCount: Int\n+    ) {\n+    val countPerBlock = total / blockCount\n+    val blockIds = (0 until blockCount).map { i =>\n+        StreamBlockId(idGenerator.incrementAndGet(), idGenerator.incrementAndGet())\n+    }\n+\n+    val writtenStrings = generateData(total, countPerBlock)\n+\n+    if (writeToBMCount != 0) {\n+      (0 until writeToBMCount).foreach { i =>\n+        blockManager\n+          .putIterator(blockIds(i), writtenStrings(i).iterator, StorageLevel.MEMORY_ONLY_SER)\n+      }\n+    }\n+\n+    val segments = {\n+      if (writeToHDFSCount != 0) {\n+        // Generate some fake segments for the blocks in BM so the RDD does not complain\n+        generateFakeSegments(writeToBMCount) ++\n+          writeDataToHDFS(writtenStrings.slice(writeToBMCount, blockCount),\n+            blockIds.slice(writeToBMCount, blockCount))\n+      } else {\n+        generateFakeSegments(blockCount)\n+      }\n+    }\n+\n+    val rdd = new HDFSBackedBlockRDD[String](sparkContext, hadoopConf, blockIds.toArray,\n+      segments.toArray, false, StorageLevel.MEMORY_ONLY)\n+\n+    val dataFromRDD = rdd.collect()\n+    // verify each partition is equal to the data pulled out\n+    assert(writtenStrings.flatten === dataFromRDD)\n+  }\n+\n+  /**\n+   * Write data to HDFS and get a list of Seq of Seqs in which each Seq represents the data that\n+   * went into one block.\n+   * @param count Number of Strings to write\n+   * @param countPerBlock Number of Strings per block\n+   * @return Seq of Seqs, each of these Seqs is one block\n+   */\n+  private def generateData(\n+      count: Int,\n+      countPerBlock: Int\n+    ): Seq[Seq[String]] = {\n+    val strings = (0 until count).map { _ => scala.util.Random.nextString(50)}"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "refactored.\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T21:41:26Z",
    "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.File\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import org.scalatest.{BeforeAndAfterAll, BeforeAndAfter, FunSuite}\n+\n+import com.google.common.io.Files\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.storage.{BlockManager, BlockId, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, WriteAheadLogWriter}\n+\n+class HDFSBackedBlockRDDSuite extends FunSuite with BeforeAndAfter with BeforeAndAfterAll {\n+  val conf = new SparkConf()\n+    .setMaster(\"local[2]\")\n+    .setAppName(this.getClass.getSimpleName)\n+  val hadoopConf = new Configuration()\n+  // Since the same BM is reused in all tests, use an atomic int to generate ids\n+  val idGenerator = new AtomicInteger(0)\n+  \n+  var sparkContext: SparkContext = null\n+  var blockManager: BlockManager = null\n+  var file: File = null\n+  var dir: File = null\n+\n+  before {\n+    blockManager = sparkContext.env.blockManager\n+    dir = Files.createTempDir()\n+    file = new File(dir, \"BlockManagerWrite\")\n+  }\n+\n+  after {\n+    file.delete()\n+    dir.delete()\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    sparkContext = new SparkContext(conf)\n+  }\n+\n+  override def afterAll(): Unit = {\n+    // Copied from LocalSparkContext which can't be imported since spark-core test-jar does not\n+    // get imported properly by sbt even if it is created.\n+    sparkContext.stop()\n+    System.clearProperty(\"spark.driver.port\")\n+  }\n+\n+  test(\"Data available in BM and HDFS\") {\n+    testHDFSBackedRDD(5, 5, 20, 5)\n+  }\n+\n+  test(\"Data available in in BM but not in HDFS\") {\n+    testHDFSBackedRDD(5, 0, 20, 5)\n+  }\n+\n+  test(\"Data available in in HDFS and not in BM\") {\n+    testHDFSBackedRDD(0, 5, 20, 5)\n+  }\n+\n+  test(\"Data partially available in BM, and the rest in HDFS\") {\n+    testHDFSBackedRDD(3, 2, 20, 5)\n+  }\n+\n+  /**\n+   * Write a bunch of events into the HDFS Block RDD. Put a part of all of them to the\n+   * BlockManager, so all reads need not happen from HDFS.\n+   * @param total Total number of Strings to write\n+   * @param blockCount Number of blocks to write (therefore, total # of events per block =\n+   *                   total/blockCount\n+   */\n+  private def testHDFSBackedRDD(\n+      writeToBMCount: Int,\n+      writeToHDFSCount: Int,\n+      total: Int,\n+      blockCount: Int\n+    ) {\n+    val countPerBlock = total / blockCount\n+    val blockIds = (0 until blockCount).map { i =>\n+        StreamBlockId(idGenerator.incrementAndGet(), idGenerator.incrementAndGet())\n+    }\n+\n+    val writtenStrings = generateData(total, countPerBlock)\n+\n+    if (writeToBMCount != 0) {\n+      (0 until writeToBMCount).foreach { i =>\n+        blockManager\n+          .putIterator(blockIds(i), writtenStrings(i).iterator, StorageLevel.MEMORY_ONLY_SER)\n+      }\n+    }\n+\n+    val segments = {\n+      if (writeToHDFSCount != 0) {\n+        // Generate some fake segments for the blocks in BM so the RDD does not complain\n+        generateFakeSegments(writeToBMCount) ++\n+          writeDataToHDFS(writtenStrings.slice(writeToBMCount, blockCount),\n+            blockIds.slice(writeToBMCount, blockCount))\n+      } else {\n+        generateFakeSegments(blockCount)\n+      }\n+    }\n+\n+    val rdd = new HDFSBackedBlockRDD[String](sparkContext, hadoopConf, blockIds.toArray,\n+      segments.toArray, false, StorageLevel.MEMORY_ONLY)\n+\n+    val dataFromRDD = rdd.collect()\n+    // verify each partition is equal to the data pulled out\n+    assert(writtenStrings.flatten === dataFromRDD)\n+  }\n+\n+  /**\n+   * Write data to HDFS and get a list of Seq of Seqs in which each Seq represents the data that\n+   * went into one block.\n+   * @param count Number of Strings to write\n+   * @param countPerBlock Number of Strings per block\n+   * @return Seq of Seqs, each of these Seqs is one block\n+   */\n+  private def generateData(\n+      count: Int,\n+      countPerBlock: Int\n+    ): Seq[Seq[String]] = {\n+    val strings = (0 until count).map { _ => scala.util.Random.nextString(50)}"
  }],
  "prId": 2931
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "I think `numItems` might be a clearer variable name\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T16:46:54Z",
    "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.File\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import org.scalatest.{BeforeAndAfterAll, BeforeAndAfter, FunSuite}\n+\n+import com.google.common.io.Files\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.storage.{BlockManager, BlockId, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, WriteAheadLogWriter}\n+\n+class HDFSBackedBlockRDDSuite extends FunSuite with BeforeAndAfter with BeforeAndAfterAll {\n+  val conf = new SparkConf()\n+    .setMaster(\"local[2]\")\n+    .setAppName(this.getClass.getSimpleName)\n+  val hadoopConf = new Configuration()\n+  // Since the same BM is reused in all tests, use an atomic int to generate ids\n+  val idGenerator = new AtomicInteger(0)\n+  \n+  var sparkContext: SparkContext = null\n+  var blockManager: BlockManager = null\n+  var file: File = null\n+  var dir: File = null\n+\n+  before {\n+    blockManager = sparkContext.env.blockManager\n+    dir = Files.createTempDir()\n+    file = new File(dir, \"BlockManagerWrite\")\n+  }\n+\n+  after {\n+    file.delete()\n+    dir.delete()\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    sparkContext = new SparkContext(conf)\n+  }\n+\n+  override def afterAll(): Unit = {\n+    // Copied from LocalSparkContext which can't be imported since spark-core test-jar does not\n+    // get imported properly by sbt even if it is created.\n+    sparkContext.stop()\n+    System.clearProperty(\"spark.driver.port\")\n+  }\n+\n+  test(\"Data available in BM and HDFS\") {\n+    testHDFSBackedRDD(5, 5, 20, 5)\n+  }\n+\n+  test(\"Data available in in BM but not in HDFS\") {\n+    testHDFSBackedRDD(5, 0, 20, 5)\n+  }\n+\n+  test(\"Data available in in HDFS and not in BM\") {\n+    testHDFSBackedRDD(0, 5, 20, 5)\n+  }\n+\n+  test(\"Data partially available in BM, and the rest in HDFS\") {\n+    testHDFSBackedRDD(3, 2, 20, 5)\n+  }\n+\n+  /**\n+   * Write a bunch of events into the HDFS Block RDD. Put a part of all of them to the\n+   * BlockManager, so all reads need not happen from HDFS.\n+   * @param total Total number of Strings to write\n+   * @param blockCount Number of blocks to write (therefore, total # of events per block =\n+   *                   total/blockCount\n+   */\n+  private def testHDFSBackedRDD(\n+      writeToBMCount: Int,\n+      writeToHDFSCount: Int,\n+      total: Int,"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "refactored.\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T21:41:18Z",
    "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.File\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import org.scalatest.{BeforeAndAfterAll, BeforeAndAfter, FunSuite}\n+\n+import com.google.common.io.Files\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.storage.{BlockManager, BlockId, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, WriteAheadLogWriter}\n+\n+class HDFSBackedBlockRDDSuite extends FunSuite with BeforeAndAfter with BeforeAndAfterAll {\n+  val conf = new SparkConf()\n+    .setMaster(\"local[2]\")\n+    .setAppName(this.getClass.getSimpleName)\n+  val hadoopConf = new Configuration()\n+  // Since the same BM is reused in all tests, use an atomic int to generate ids\n+  val idGenerator = new AtomicInteger(0)\n+  \n+  var sparkContext: SparkContext = null\n+  var blockManager: BlockManager = null\n+  var file: File = null\n+  var dir: File = null\n+\n+  before {\n+    blockManager = sparkContext.env.blockManager\n+    dir = Files.createTempDir()\n+    file = new File(dir, \"BlockManagerWrite\")\n+  }\n+\n+  after {\n+    file.delete()\n+    dir.delete()\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    sparkContext = new SparkContext(conf)\n+  }\n+\n+  override def afterAll(): Unit = {\n+    // Copied from LocalSparkContext which can't be imported since spark-core test-jar does not\n+    // get imported properly by sbt even if it is created.\n+    sparkContext.stop()\n+    System.clearProperty(\"spark.driver.port\")\n+  }\n+\n+  test(\"Data available in BM and HDFS\") {\n+    testHDFSBackedRDD(5, 5, 20, 5)\n+  }\n+\n+  test(\"Data available in in BM but not in HDFS\") {\n+    testHDFSBackedRDD(5, 0, 20, 5)\n+  }\n+\n+  test(\"Data available in in HDFS and not in BM\") {\n+    testHDFSBackedRDD(0, 5, 20, 5)\n+  }\n+\n+  test(\"Data partially available in BM, and the rest in HDFS\") {\n+    testHDFSBackedRDD(3, 2, 20, 5)\n+  }\n+\n+  /**\n+   * Write a bunch of events into the HDFS Block RDD. Put a part of all of them to the\n+   * BlockManager, so all reads need not happen from HDFS.\n+   * @param total Total number of Strings to write\n+   * @param blockCount Number of blocks to write (therefore, total # of events per block =\n+   *                   total/blockCount\n+   */\n+  private def testHDFSBackedRDD(\n+      writeToBMCount: Int,\n+      writeToHDFSCount: Int,\n+      total: Int,"
  }],
  "prId": 2931
}]