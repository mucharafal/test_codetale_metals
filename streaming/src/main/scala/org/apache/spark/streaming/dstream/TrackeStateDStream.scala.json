[{
  "comments": [{
    "author": {
      "login": "jerryshao"
    },
    "body": "Is it possible `value` will be `null`, would it be better to use `Option(value)`?\n",
    "commit": "ae64786fd937002a2cc1f80518d54e970a6bbb21",
    "createdAt": "2015-10-27T07:13:16Z",
    "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.dstream\n+\n+import java.io.{IOException, ObjectOutputStream}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.{EmptyRDD, RDD}\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming._\n+import org.apache.spark.streaming.util.StateMap\n+import org.apache.spark.util.Utils\n+\n+private[streaming] case class TrackStateRDDRecord[K: ClassTag, S: ClassTag, T: ClassTag](\n+    stateMap: StateMap[K, S], emittedRecords: Seq[T])\n+\n+\n+private[streaming] class TrackStateRDDPartition(\n+    idx: Int,\n+    @transient private var prevStateRDD: RDD[_],\n+    @transient private var partitionedDataRDD: RDD[_]) extends Partition {\n+\n+  private[dstream] var previousSessionRDDPartition: Partition = null\n+  private[dstream] var partitionedDataRDDPartition: Partition = null\n+\n+  override def index: Int = idx\n+  override def hashCode(): Int = idx\n+\n+  @throws(classOf[IOException])\n+  private def writeObject(oos: ObjectOutputStream): Unit = Utils.tryOrIOException {\n+    // Update the reference to parent split at the time of task serialization\n+    previousSessionRDDPartition = prevStateRDD.partitions(index)\n+    partitionedDataRDDPartition = partitionedDataRDD.partitions(index)\n+    oos.defaultWriteObject()\n+  }\n+}\n+\n+private[streaming] class TrackStateRDD[K: ClassTag, V: ClassTag, S: ClassTag, T: ClassTag](\n+    _sc: SparkContext,\n+    private var prevStateRDD: RDD[TrackStateRDDRecord[K, S, T]],\n+    private var partitionedDataRDD: RDD[(K, V)],\n+    trackingFunction: (K, Option[V], State[S]) => Option[T],\n+    currentTime: Long, timeoutThresholdTime: Option[Long]\n+  ) extends RDD[TrackStateRDDRecord[K, S, T]](\n+    _sc,\n+    List(\n+      new OneToOneDependency[TrackStateRDDRecord[K, S, T]](prevStateRDD),\n+      new OneToOneDependency(partitionedDataRDD))\n+  ) {\n+\n+  @volatile private var doFullScan = false\n+\n+  require(partitionedDataRDD.partitioner.nonEmpty)\n+  require(partitionedDataRDD.partitioner == prevStateRDD.partitioner)\n+\n+  override val partitioner = prevStateRDD.partitioner\n+\n+  override def checkpoint(): Unit = {\n+    super.checkpoint()\n+    doFullScan = true\n+  }\n+\n+  override def compute(\n+      partition: Partition, context: TaskContext): Iterator[TrackStateRDDRecord[K, S, T]] = {\n+\n+    val stateRDDPartition = partition.asInstanceOf[TrackStateRDDPartition]\n+    val prevStateRDDIterator = prevStateRDD.iterator(\n+      stateRDDPartition.previousSessionRDDPartition, context)\n+    val dataIterator = partitionedDataRDD.iterator(\n+      stateRDDPartition.partitionedDataRDDPartition, context)\n+    if (!prevStateRDDIterator.hasNext) {\n+      throw new SparkException(s\"Could not find state map in previous state RDD\")\n+    }\n+\n+    val newStateMap = prevStateRDDIterator.next().stateMap.copy()\n+    val emittedRecords = new ArrayBuffer[T]\n+\n+    val wrappedState = new StateImpl[S]()\n+\n+    dataIterator.foreach { case (key, value) =>\n+      wrappedState.wrap(newStateMap.get(key))\n+      val emittedRecord = trackingFunction(key, Some(value), wrappedState)"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "If the parentDStream[(K, V)] generates data such that the corresponding value is null, then the system should respect that null and pass it on. So odd as it may sounds, I think it is right to call the track state function with `Some(null)`.\n\nBut you do raise a good point. I will get more feedback on this.\n",
    "commit": "ae64786fd937002a2cc1f80518d54e970a6bbb21",
    "createdAt": "2015-10-27T08:53:24Z",
    "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.dstream\n+\n+import java.io.{IOException, ObjectOutputStream}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.{EmptyRDD, RDD}\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming._\n+import org.apache.spark.streaming.util.StateMap\n+import org.apache.spark.util.Utils\n+\n+private[streaming] case class TrackStateRDDRecord[K: ClassTag, S: ClassTag, T: ClassTag](\n+    stateMap: StateMap[K, S], emittedRecords: Seq[T])\n+\n+\n+private[streaming] class TrackStateRDDPartition(\n+    idx: Int,\n+    @transient private var prevStateRDD: RDD[_],\n+    @transient private var partitionedDataRDD: RDD[_]) extends Partition {\n+\n+  private[dstream] var previousSessionRDDPartition: Partition = null\n+  private[dstream] var partitionedDataRDDPartition: Partition = null\n+\n+  override def index: Int = idx\n+  override def hashCode(): Int = idx\n+\n+  @throws(classOf[IOException])\n+  private def writeObject(oos: ObjectOutputStream): Unit = Utils.tryOrIOException {\n+    // Update the reference to parent split at the time of task serialization\n+    previousSessionRDDPartition = prevStateRDD.partitions(index)\n+    partitionedDataRDDPartition = partitionedDataRDD.partitions(index)\n+    oos.defaultWriteObject()\n+  }\n+}\n+\n+private[streaming] class TrackStateRDD[K: ClassTag, V: ClassTag, S: ClassTag, T: ClassTag](\n+    _sc: SparkContext,\n+    private var prevStateRDD: RDD[TrackStateRDDRecord[K, S, T]],\n+    private var partitionedDataRDD: RDD[(K, V)],\n+    trackingFunction: (K, Option[V], State[S]) => Option[T],\n+    currentTime: Long, timeoutThresholdTime: Option[Long]\n+  ) extends RDD[TrackStateRDDRecord[K, S, T]](\n+    _sc,\n+    List(\n+      new OneToOneDependency[TrackStateRDDRecord[K, S, T]](prevStateRDD),\n+      new OneToOneDependency(partitionedDataRDD))\n+  ) {\n+\n+  @volatile private var doFullScan = false\n+\n+  require(partitionedDataRDD.partitioner.nonEmpty)\n+  require(partitionedDataRDD.partitioner == prevStateRDD.partitioner)\n+\n+  override val partitioner = prevStateRDD.partitioner\n+\n+  override def checkpoint(): Unit = {\n+    super.checkpoint()\n+    doFullScan = true\n+  }\n+\n+  override def compute(\n+      partition: Partition, context: TaskContext): Iterator[TrackStateRDDRecord[K, S, T]] = {\n+\n+    val stateRDDPartition = partition.asInstanceOf[TrackStateRDDPartition]\n+    val prevStateRDDIterator = prevStateRDD.iterator(\n+      stateRDDPartition.previousSessionRDDPartition, context)\n+    val dataIterator = partitionedDataRDD.iterator(\n+      stateRDDPartition.partitionedDataRDDPartition, context)\n+    if (!prevStateRDDIterator.hasNext) {\n+      throw new SparkException(s\"Could not find state map in previous state RDD\")\n+    }\n+\n+    val newStateMap = prevStateRDDIterator.next().stateMap.copy()\n+    val emittedRecords = new ArrayBuffer[T]\n+\n+    val wrappedState = new StateImpl[S]()\n+\n+    dataIterator.foreach { case (key, value) =>\n+      wrappedState.wrap(newStateMap.get(key))\n+      val emittedRecord = trackingFunction(key, Some(value), wrappedState)"
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "Since we defined the tracking function as `(K, Option[V], State[S]) => Option[T]`, so user will expect `None` rather than `Some(null)` if the input value is null, that is a little misleading and will possibly lead to NPE if used like `v.map(_.toString).getOrElse(...)`.\n",
    "commit": "ae64786fd937002a2cc1f80518d54e970a6bbb21",
    "createdAt": "2015-10-28T08:54:07Z",
    "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.dstream\n+\n+import java.io.{IOException, ObjectOutputStream}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.{EmptyRDD, RDD}\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming._\n+import org.apache.spark.streaming.util.StateMap\n+import org.apache.spark.util.Utils\n+\n+private[streaming] case class TrackStateRDDRecord[K: ClassTag, S: ClassTag, T: ClassTag](\n+    stateMap: StateMap[K, S], emittedRecords: Seq[T])\n+\n+\n+private[streaming] class TrackStateRDDPartition(\n+    idx: Int,\n+    @transient private var prevStateRDD: RDD[_],\n+    @transient private var partitionedDataRDD: RDD[_]) extends Partition {\n+\n+  private[dstream] var previousSessionRDDPartition: Partition = null\n+  private[dstream] var partitionedDataRDDPartition: Partition = null\n+\n+  override def index: Int = idx\n+  override def hashCode(): Int = idx\n+\n+  @throws(classOf[IOException])\n+  private def writeObject(oos: ObjectOutputStream): Unit = Utils.tryOrIOException {\n+    // Update the reference to parent split at the time of task serialization\n+    previousSessionRDDPartition = prevStateRDD.partitions(index)\n+    partitionedDataRDDPartition = partitionedDataRDD.partitions(index)\n+    oos.defaultWriteObject()\n+  }\n+}\n+\n+private[streaming] class TrackStateRDD[K: ClassTag, V: ClassTag, S: ClassTag, T: ClassTag](\n+    _sc: SparkContext,\n+    private var prevStateRDD: RDD[TrackStateRDDRecord[K, S, T]],\n+    private var partitionedDataRDD: RDD[(K, V)],\n+    trackingFunction: (K, Option[V], State[S]) => Option[T],\n+    currentTime: Long, timeoutThresholdTime: Option[Long]\n+  ) extends RDD[TrackStateRDDRecord[K, S, T]](\n+    _sc,\n+    List(\n+      new OneToOneDependency[TrackStateRDDRecord[K, S, T]](prevStateRDD),\n+      new OneToOneDependency(partitionedDataRDD))\n+  ) {\n+\n+  @volatile private var doFullScan = false\n+\n+  require(partitionedDataRDD.partitioner.nonEmpty)\n+  require(partitionedDataRDD.partitioner == prevStateRDD.partitioner)\n+\n+  override val partitioner = prevStateRDD.partitioner\n+\n+  override def checkpoint(): Unit = {\n+    super.checkpoint()\n+    doFullScan = true\n+  }\n+\n+  override def compute(\n+      partition: Partition, context: TaskContext): Iterator[TrackStateRDDRecord[K, S, T]] = {\n+\n+    val stateRDDPartition = partition.asInstanceOf[TrackStateRDDPartition]\n+    val prevStateRDDIterator = prevStateRDD.iterator(\n+      stateRDDPartition.previousSessionRDDPartition, context)\n+    val dataIterator = partitionedDataRDD.iterator(\n+      stateRDDPartition.partitionedDataRDDPartition, context)\n+    if (!prevStateRDDIterator.hasNext) {\n+      throw new SparkException(s\"Could not find state map in previous state RDD\")\n+    }\n+\n+    val newStateMap = prevStateRDDIterator.next().stateMap.copy()\n+    val emittedRecords = new ArrayBuffer[T]\n+\n+    val wrappedState = new StateImpl[S]()\n+\n+    dataIterator.foreach { case (key, value) =>\n+      wrappedState.wrap(newStateMap.get(key))\n+      val emittedRecord = trackingFunction(key, Some(value), wrappedState)"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Yeah. But what if the user actually generates a tuple from upstream code which like (\"some-key\", null) because he want to signify null downstream. That null could be different from not having any data at all. How does one differentiate that? \n\nI guess it boils down to policy, do we respect nulls in the data or not. \n",
    "commit": "ae64786fd937002a2cc1f80518d54e970a6bbb21",
    "createdAt": "2015-10-28T10:21:35Z",
    "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.dstream\n+\n+import java.io.{IOException, ObjectOutputStream}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.{EmptyRDD, RDD}\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming._\n+import org.apache.spark.streaming.util.StateMap\n+import org.apache.spark.util.Utils\n+\n+private[streaming] case class TrackStateRDDRecord[K: ClassTag, S: ClassTag, T: ClassTag](\n+    stateMap: StateMap[K, S], emittedRecords: Seq[T])\n+\n+\n+private[streaming] class TrackStateRDDPartition(\n+    idx: Int,\n+    @transient private var prevStateRDD: RDD[_],\n+    @transient private var partitionedDataRDD: RDD[_]) extends Partition {\n+\n+  private[dstream] var previousSessionRDDPartition: Partition = null\n+  private[dstream] var partitionedDataRDDPartition: Partition = null\n+\n+  override def index: Int = idx\n+  override def hashCode(): Int = idx\n+\n+  @throws(classOf[IOException])\n+  private def writeObject(oos: ObjectOutputStream): Unit = Utils.tryOrIOException {\n+    // Update the reference to parent split at the time of task serialization\n+    previousSessionRDDPartition = prevStateRDD.partitions(index)\n+    partitionedDataRDDPartition = partitionedDataRDD.partitions(index)\n+    oos.defaultWriteObject()\n+  }\n+}\n+\n+private[streaming] class TrackStateRDD[K: ClassTag, V: ClassTag, S: ClassTag, T: ClassTag](\n+    _sc: SparkContext,\n+    private var prevStateRDD: RDD[TrackStateRDDRecord[K, S, T]],\n+    private var partitionedDataRDD: RDD[(K, V)],\n+    trackingFunction: (K, Option[V], State[S]) => Option[T],\n+    currentTime: Long, timeoutThresholdTime: Option[Long]\n+  ) extends RDD[TrackStateRDDRecord[K, S, T]](\n+    _sc,\n+    List(\n+      new OneToOneDependency[TrackStateRDDRecord[K, S, T]](prevStateRDD),\n+      new OneToOneDependency(partitionedDataRDD))\n+  ) {\n+\n+  @volatile private var doFullScan = false\n+\n+  require(partitionedDataRDD.partitioner.nonEmpty)\n+  require(partitionedDataRDD.partitioner == prevStateRDD.partitioner)\n+\n+  override val partitioner = prevStateRDD.partitioner\n+\n+  override def checkpoint(): Unit = {\n+    super.checkpoint()\n+    doFullScan = true\n+  }\n+\n+  override def compute(\n+      partition: Partition, context: TaskContext): Iterator[TrackStateRDDRecord[K, S, T]] = {\n+\n+    val stateRDDPartition = partition.asInstanceOf[TrackStateRDDPartition]\n+    val prevStateRDDIterator = prevStateRDD.iterator(\n+      stateRDDPartition.previousSessionRDDPartition, context)\n+    val dataIterator = partitionedDataRDD.iterator(\n+      stateRDDPartition.partitionedDataRDDPartition, context)\n+    if (!prevStateRDDIterator.hasNext) {\n+      throw new SparkException(s\"Could not find state map in previous state RDD\")\n+    }\n+\n+    val newStateMap = prevStateRDDIterator.next().stateMap.copy()\n+    val emittedRecords = new ArrayBuffer[T]\n+\n+    val wrappedState = new StateImpl[S]()\n+\n+    dataIterator.foreach { case (key, value) =>\n+      wrappedState.wrap(newStateMap.get(key))\n+      val emittedRecord = trackingFunction(key, Some(value), wrappedState)"
  }],
  "prId": 9256
}, {
  "comments": [{
    "author": {
      "login": "jerryshao"
    },
    "body": "It is better not to materialize all the `emittedRecords` in `compute()`, just one by each iteration, from the current code I think it can be achieved, not sure if there's any other concern.\n",
    "commit": "ae64786fd937002a2cc1f80518d54e970a6bbb21",
    "createdAt": "2015-10-27T07:35:45Z",
    "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.dstream\n+\n+import java.io.{IOException, ObjectOutputStream}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.{EmptyRDD, RDD}\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming._\n+import org.apache.spark.streaming.util.StateMap\n+import org.apache.spark.util.Utils\n+\n+private[streaming] case class TrackStateRDDRecord[K: ClassTag, S: ClassTag, T: ClassTag](\n+    stateMap: StateMap[K, S], emittedRecords: Seq[T])\n+\n+\n+private[streaming] class TrackStateRDDPartition(\n+    idx: Int,\n+    @transient private var prevStateRDD: RDD[_],\n+    @transient private var partitionedDataRDD: RDD[_]) extends Partition {\n+\n+  private[dstream] var previousSessionRDDPartition: Partition = null\n+  private[dstream] var partitionedDataRDDPartition: Partition = null\n+\n+  override def index: Int = idx\n+  override def hashCode(): Int = idx\n+\n+  @throws(classOf[IOException])\n+  private def writeObject(oos: ObjectOutputStream): Unit = Utils.tryOrIOException {\n+    // Update the reference to parent split at the time of task serialization\n+    previousSessionRDDPartition = prevStateRDD.partitions(index)\n+    partitionedDataRDDPartition = partitionedDataRDD.partitions(index)\n+    oos.defaultWriteObject()\n+  }\n+}\n+\n+private[streaming] class TrackStateRDD[K: ClassTag, V: ClassTag, S: ClassTag, T: ClassTag](\n+    _sc: SparkContext,\n+    private var prevStateRDD: RDD[TrackStateRDDRecord[K, S, T]],\n+    private var partitionedDataRDD: RDD[(K, V)],\n+    trackingFunction: (K, Option[V], State[S]) => Option[T],\n+    currentTime: Long, timeoutThresholdTime: Option[Long]\n+  ) extends RDD[TrackStateRDDRecord[K, S, T]](\n+    _sc,\n+    List(\n+      new OneToOneDependency[TrackStateRDDRecord[K, S, T]](prevStateRDD),\n+      new OneToOneDependency(partitionedDataRDD))\n+  ) {\n+\n+  @volatile private var doFullScan = false\n+\n+  require(partitionedDataRDD.partitioner.nonEmpty)\n+  require(partitionedDataRDD.partitioner == prevStateRDD.partitioner)\n+\n+  override val partitioner = prevStateRDD.partitioner\n+\n+  override def checkpoint(): Unit = {\n+    super.checkpoint()\n+    doFullScan = true\n+  }\n+\n+  override def compute(\n+      partition: Partition, context: TaskContext): Iterator[TrackStateRDDRecord[K, S, T]] = {\n+\n+    val stateRDDPartition = partition.asInstanceOf[TrackStateRDDPartition]\n+    val prevStateRDDIterator = prevStateRDD.iterator(\n+      stateRDDPartition.previousSessionRDDPartition, context)\n+    val dataIterator = partitionedDataRDD.iterator(\n+      stateRDDPartition.partitionedDataRDDPartition, context)\n+    if (!prevStateRDDIterator.hasNext) {\n+      throw new SparkException(s\"Could not find state map in previous state RDD\")\n+    }\n+\n+    val newStateMap = prevStateRDDIterator.next().stateMap.copy()\n+    val emittedRecords = new ArrayBuffer[T]\n+\n+    val wrappedState = new StateImpl[S]()\n+\n+    dataIterator.foreach { case (key, value) =>\n+      wrappedState.wrap(newStateMap.get(key))\n+      val emittedRecord = trackingFunction(key, Some(value), wrappedState)\n+      if (wrappedState.isRemoved) {\n+        newStateMap.remove(key)\n+      } else if (wrappedState.isUpdated) {\n+        newStateMap.put(key, wrappedState.get(), currentTime)\n+      }\n+      emittedRecords ++= emittedRecord"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "I think you have to materialize the emitted records. Since the combo of [StateMap, emitted records] is a single record for the StateRDD, when the \"record\" is computed AND persisted, all the necessary information needs to be available in memory, and not require recomputing. Once persisted, the record should be usable any number of times. So if the emitted data _inside_ that record is an iterator, it can not be reused any number of times. Which breaks the RDD's specifications.\n",
    "commit": "ae64786fd937002a2cc1f80518d54e970a6bbb21",
    "createdAt": "2015-10-28T00:28:51Z",
    "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.dstream\n+\n+import java.io.{IOException, ObjectOutputStream}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.{EmptyRDD, RDD}\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming._\n+import org.apache.spark.streaming.util.StateMap\n+import org.apache.spark.util.Utils\n+\n+private[streaming] case class TrackStateRDDRecord[K: ClassTag, S: ClassTag, T: ClassTag](\n+    stateMap: StateMap[K, S], emittedRecords: Seq[T])\n+\n+\n+private[streaming] class TrackStateRDDPartition(\n+    idx: Int,\n+    @transient private var prevStateRDD: RDD[_],\n+    @transient private var partitionedDataRDD: RDD[_]) extends Partition {\n+\n+  private[dstream] var previousSessionRDDPartition: Partition = null\n+  private[dstream] var partitionedDataRDDPartition: Partition = null\n+\n+  override def index: Int = idx\n+  override def hashCode(): Int = idx\n+\n+  @throws(classOf[IOException])\n+  private def writeObject(oos: ObjectOutputStream): Unit = Utils.tryOrIOException {\n+    // Update the reference to parent split at the time of task serialization\n+    previousSessionRDDPartition = prevStateRDD.partitions(index)\n+    partitionedDataRDDPartition = partitionedDataRDD.partitions(index)\n+    oos.defaultWriteObject()\n+  }\n+}\n+\n+private[streaming] class TrackStateRDD[K: ClassTag, V: ClassTag, S: ClassTag, T: ClassTag](\n+    _sc: SparkContext,\n+    private var prevStateRDD: RDD[TrackStateRDDRecord[K, S, T]],\n+    private var partitionedDataRDD: RDD[(K, V)],\n+    trackingFunction: (K, Option[V], State[S]) => Option[T],\n+    currentTime: Long, timeoutThresholdTime: Option[Long]\n+  ) extends RDD[TrackStateRDDRecord[K, S, T]](\n+    _sc,\n+    List(\n+      new OneToOneDependency[TrackStateRDDRecord[K, S, T]](prevStateRDD),\n+      new OneToOneDependency(partitionedDataRDD))\n+  ) {\n+\n+  @volatile private var doFullScan = false\n+\n+  require(partitionedDataRDD.partitioner.nonEmpty)\n+  require(partitionedDataRDD.partitioner == prevStateRDD.partitioner)\n+\n+  override val partitioner = prevStateRDD.partitioner\n+\n+  override def checkpoint(): Unit = {\n+    super.checkpoint()\n+    doFullScan = true\n+  }\n+\n+  override def compute(\n+      partition: Partition, context: TaskContext): Iterator[TrackStateRDDRecord[K, S, T]] = {\n+\n+    val stateRDDPartition = partition.asInstanceOf[TrackStateRDDPartition]\n+    val prevStateRDDIterator = prevStateRDD.iterator(\n+      stateRDDPartition.previousSessionRDDPartition, context)\n+    val dataIterator = partitionedDataRDD.iterator(\n+      stateRDDPartition.partitionedDataRDDPartition, context)\n+    if (!prevStateRDDIterator.hasNext) {\n+      throw new SparkException(s\"Could not find state map in previous state RDD\")\n+    }\n+\n+    val newStateMap = prevStateRDDIterator.next().stateMap.copy()\n+    val emittedRecords = new ArrayBuffer[T]\n+\n+    val wrappedState = new StateImpl[S]()\n+\n+    dataIterator.foreach { case (key, value) =>\n+      wrappedState.wrap(newStateMap.get(key))\n+      val emittedRecord = trackingFunction(key, Some(value), wrappedState)\n+      if (wrappedState.isRemoved) {\n+        newStateMap.remove(key)\n+      } else if (wrappedState.isUpdated) {\n+        newStateMap.put(key, wrappedState.get(), currentTime)\n+      }\n+      emittedRecords ++= emittedRecord"
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "I don't think being an iterator cannot be persisted in memory and get reused many times. Oppositely many RDD is designed as iterator like `HadoopRDD`, whereas can be persisted in memory and get used many times. Actually if RDD is chosen to be persisted in memory, it will materialize the iterator by using `unroll` in `MemoryStore`, materialization will be happened at that step, after that this RDD can be reused without recomputing (you can check the code of `CacheManager`) .\n\nI think the problem here is that it is hard to use the iterator because you have to return a combo of (StateMap, emitted records), where StateMap is a materialized one in memory, so being emitted records as iterator is hard to do. Maybe we could refactor this part of code to make it more iterator friendly, otherwise this unmanaged ArrayBuffer will potentially be a OOM issue. Just my thoughts, any misunderstandings please point out :smile: .\n",
    "commit": "ae64786fd937002a2cc1f80518d54e970a6bbb21",
    "createdAt": "2015-10-28T07:25:43Z",
    "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.dstream\n+\n+import java.io.{IOException, ObjectOutputStream}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.{EmptyRDD, RDD}\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming._\n+import org.apache.spark.streaming.util.StateMap\n+import org.apache.spark.util.Utils\n+\n+private[streaming] case class TrackStateRDDRecord[K: ClassTag, S: ClassTag, T: ClassTag](\n+    stateMap: StateMap[K, S], emittedRecords: Seq[T])\n+\n+\n+private[streaming] class TrackStateRDDPartition(\n+    idx: Int,\n+    @transient private var prevStateRDD: RDD[_],\n+    @transient private var partitionedDataRDD: RDD[_]) extends Partition {\n+\n+  private[dstream] var previousSessionRDDPartition: Partition = null\n+  private[dstream] var partitionedDataRDDPartition: Partition = null\n+\n+  override def index: Int = idx\n+  override def hashCode(): Int = idx\n+\n+  @throws(classOf[IOException])\n+  private def writeObject(oos: ObjectOutputStream): Unit = Utils.tryOrIOException {\n+    // Update the reference to parent split at the time of task serialization\n+    previousSessionRDDPartition = prevStateRDD.partitions(index)\n+    partitionedDataRDDPartition = partitionedDataRDD.partitions(index)\n+    oos.defaultWriteObject()\n+  }\n+}\n+\n+private[streaming] class TrackStateRDD[K: ClassTag, V: ClassTag, S: ClassTag, T: ClassTag](\n+    _sc: SparkContext,\n+    private var prevStateRDD: RDD[TrackStateRDDRecord[K, S, T]],\n+    private var partitionedDataRDD: RDD[(K, V)],\n+    trackingFunction: (K, Option[V], State[S]) => Option[T],\n+    currentTime: Long, timeoutThresholdTime: Option[Long]\n+  ) extends RDD[TrackStateRDDRecord[K, S, T]](\n+    _sc,\n+    List(\n+      new OneToOneDependency[TrackStateRDDRecord[K, S, T]](prevStateRDD),\n+      new OneToOneDependency(partitionedDataRDD))\n+  ) {\n+\n+  @volatile private var doFullScan = false\n+\n+  require(partitionedDataRDD.partitioner.nonEmpty)\n+  require(partitionedDataRDD.partitioner == prevStateRDD.partitioner)\n+\n+  override val partitioner = prevStateRDD.partitioner\n+\n+  override def checkpoint(): Unit = {\n+    super.checkpoint()\n+    doFullScan = true\n+  }\n+\n+  override def compute(\n+      partition: Partition, context: TaskContext): Iterator[TrackStateRDDRecord[K, S, T]] = {\n+\n+    val stateRDDPartition = partition.asInstanceOf[TrackStateRDDPartition]\n+    val prevStateRDDIterator = prevStateRDD.iterator(\n+      stateRDDPartition.previousSessionRDDPartition, context)\n+    val dataIterator = partitionedDataRDD.iterator(\n+      stateRDDPartition.partitionedDataRDDPartition, context)\n+    if (!prevStateRDDIterator.hasNext) {\n+      throw new SparkException(s\"Could not find state map in previous state RDD\")\n+    }\n+\n+    val newStateMap = prevStateRDDIterator.next().stateMap.copy()\n+    val emittedRecords = new ArrayBuffer[T]\n+\n+    val wrappedState = new StateImpl[S]()\n+\n+    dataIterator.foreach { case (key, value) =>\n+      wrappedState.wrap(newStateMap.get(key))\n+      val emittedRecord = trackingFunction(key, Some(value), wrappedState)\n+      if (wrappedState.isRemoved) {\n+        newStateMap.remove(key)\n+      } else if (wrappedState.isUpdated) {\n+        newStateMap.put(key, wrappedState.get(), currentTime)\n+      }\n+      emittedRecords ++= emittedRecord"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "The comparison to `HadoopRDD` does not apply here. When cached, the records of `HadoopRDD`'s partitions is grabbed through the iterator, unrolled and stored in MemoryStore. There each record is a (key, value) pair, like (String, String). So the unrolled data is stored in the MemoryStore as a ArrayBuffer[(String, String)](or equivalent), which can be iterated over any number of times. \n\nIn our case, the record type is (StateMap, seq-of-emitted-records), which is not the same as a simple (string, string) record of a `HadoopRDD`s. So if that is persisted, it will be store in MemoryStore a ArrayBuffer[(StateMap, seq-of-emitted-records)]. Now if this sequence of emitted records is an iterator, then it will be ArrayBuffer[(StateMap, Iterator[T]]. You can access the record of tuple (map, iterator) multiple times, but you can unroll the iterator _inside_ the record **only once**. Subsequent accesses of the ArrayBuffer will lead to a (StateMap, empty-iterator), which violates the RDD principle.\n\nRegarding the OOM, storing an RDD in-memory by unrolling in MemoryStore (in an ArrayBuffer), is no different from my storing data in my ArrayBuffer. So I think this is really no worse in terms of possibility of OOMing\n",
    "commit": "ae64786fd937002a2cc1f80518d54e970a6bbb21",
    "createdAt": "2015-10-28T09:21:52Z",
    "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.dstream\n+\n+import java.io.{IOException, ObjectOutputStream}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark._\n+import org.apache.spark.rdd.{EmptyRDD, RDD}\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming._\n+import org.apache.spark.streaming.util.StateMap\n+import org.apache.spark.util.Utils\n+\n+private[streaming] case class TrackStateRDDRecord[K: ClassTag, S: ClassTag, T: ClassTag](\n+    stateMap: StateMap[K, S], emittedRecords: Seq[T])\n+\n+\n+private[streaming] class TrackStateRDDPartition(\n+    idx: Int,\n+    @transient private var prevStateRDD: RDD[_],\n+    @transient private var partitionedDataRDD: RDD[_]) extends Partition {\n+\n+  private[dstream] var previousSessionRDDPartition: Partition = null\n+  private[dstream] var partitionedDataRDDPartition: Partition = null\n+\n+  override def index: Int = idx\n+  override def hashCode(): Int = idx\n+\n+  @throws(classOf[IOException])\n+  private def writeObject(oos: ObjectOutputStream): Unit = Utils.tryOrIOException {\n+    // Update the reference to parent split at the time of task serialization\n+    previousSessionRDDPartition = prevStateRDD.partitions(index)\n+    partitionedDataRDDPartition = partitionedDataRDD.partitions(index)\n+    oos.defaultWriteObject()\n+  }\n+}\n+\n+private[streaming] class TrackStateRDD[K: ClassTag, V: ClassTag, S: ClassTag, T: ClassTag](\n+    _sc: SparkContext,\n+    private var prevStateRDD: RDD[TrackStateRDDRecord[K, S, T]],\n+    private var partitionedDataRDD: RDD[(K, V)],\n+    trackingFunction: (K, Option[V], State[S]) => Option[T],\n+    currentTime: Long, timeoutThresholdTime: Option[Long]\n+  ) extends RDD[TrackStateRDDRecord[K, S, T]](\n+    _sc,\n+    List(\n+      new OneToOneDependency[TrackStateRDDRecord[K, S, T]](prevStateRDD),\n+      new OneToOneDependency(partitionedDataRDD))\n+  ) {\n+\n+  @volatile private var doFullScan = false\n+\n+  require(partitionedDataRDD.partitioner.nonEmpty)\n+  require(partitionedDataRDD.partitioner == prevStateRDD.partitioner)\n+\n+  override val partitioner = prevStateRDD.partitioner\n+\n+  override def checkpoint(): Unit = {\n+    super.checkpoint()\n+    doFullScan = true\n+  }\n+\n+  override def compute(\n+      partition: Partition, context: TaskContext): Iterator[TrackStateRDDRecord[K, S, T]] = {\n+\n+    val stateRDDPartition = partition.asInstanceOf[TrackStateRDDPartition]\n+    val prevStateRDDIterator = prevStateRDD.iterator(\n+      stateRDDPartition.previousSessionRDDPartition, context)\n+    val dataIterator = partitionedDataRDD.iterator(\n+      stateRDDPartition.partitionedDataRDDPartition, context)\n+    if (!prevStateRDDIterator.hasNext) {\n+      throw new SparkException(s\"Could not find state map in previous state RDD\")\n+    }\n+\n+    val newStateMap = prevStateRDDIterator.next().stateMap.copy()\n+    val emittedRecords = new ArrayBuffer[T]\n+\n+    val wrappedState = new StateImpl[S]()\n+\n+    dataIterator.foreach { case (key, value) =>\n+      wrappedState.wrap(newStateMap.get(key))\n+      val emittedRecord = trackingFunction(key, Some(value), wrappedState)\n+      if (wrappedState.isRemoved) {\n+        newStateMap.remove(key)\n+      } else if (wrappedState.isUpdated) {\n+        newStateMap.put(key, wrappedState.get(), currentTime)\n+      }\n+      emittedRecords ++= emittedRecord"
  }],
  "prId": 9256
}]