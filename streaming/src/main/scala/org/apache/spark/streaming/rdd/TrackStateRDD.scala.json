[{
  "comments": [{
    "author": {
      "login": "huitseeker"
    },
    "body": "I'd rather see [procedure syntax avoided](http://docs.scala-lang.org/style/declarations.html).\n",
    "commit": "ae64786fd937002a2cc1f80518d54e970a6bbb21",
    "createdAt": "2015-11-06T21:12:16Z",
    "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.{IOException, ObjectInputStream, ObjectOutputStream}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark.rdd.{MapPartitionsRDD, RDD}\n+import org.apache.spark.streaming.{StateImpl, State}\n+import org.apache.spark.streaming.util.{EmptyStateMap, StateMap}\n+import org.apache.spark.util.Utils\n+import org.apache.spark._\n+\n+private[streaming] case class TrackStateRDDRecord[K, S, T](\n+    var stateMap: StateMap[K, S], var emittedRecords: Seq[T]) {\n+  /*\n+  private def writeObject(outputStream: ObjectOutputStream): Unit = {\n+    outputStream.writeObject(stateMap)\n+    outputStream.writeInt(emittedRecords.size)\n+    val iterator = emittedRecords.iterator\n+    while(iterator.hasNext) {\n+      outputStream.writeObject(iterator.next)\n+    }\n+  }\n+\n+  private def readObject(inputStream: ObjectInputStream): Unit = {\n+    stateMap = inputStream.readObject().asInstanceOf[StateMap[K, S]]\n+    val numEmittedRecords = inputStream.readInt()\n+    val array = new Array[T](numEmittedRecords)\n+    var i = 0\n+    while(i < numEmittedRecords) {\n+      array(i) = inputStream.readObject().asInstanceOf[T]\n+    }\n+    emittedRecords = array.toSeq\n+  }*/\n+}\n+\n+\n+private[streaming] class TrackStateRDDPartition(\n+    idx: Int,\n+    @transient private var prevStateRDD: RDD[_],\n+    @transient private var partitionedDataRDD: RDD[_]) extends Partition {\n+\n+  private[rdd] var previousSessionRDDPartition: Partition = null\n+  private[rdd] var partitionedDataRDDPartition: Partition = null\n+\n+  override def index: Int = idx\n+  override def hashCode(): Int = idx\n+\n+  @throws(classOf[IOException])\n+  private def writeObject(oos: ObjectOutputStream): Unit = Utils.tryOrIOException {\n+    // Update the reference to parent split at the time of task serialization\n+    previousSessionRDDPartition = prevStateRDD.partitions(index)\n+    partitionedDataRDDPartition = partitionedDataRDD.partitions(index)\n+    oos.defaultWriteObject()\n+  }\n+}\n+\n+\n+/**\n+ * RDD storing the keyed-state of trackStateByKey and corresponding emitted records.\n+ * Each partition of this RDD has a single record that contains a StateMap storing\n+ */\n+private[streaming] class TrackStateRDD[K: ClassTag, V: ClassTag, S: ClassTag, T: ClassTag](\n+    private var prevStateRDD: RDD[TrackStateRDDRecord[K, S, T]],\n+    private var partitionedDataRDD: RDD[(K, V)],\n+    trackingFunction: (K, Option[V], State[S]) => Option[T],\n+    currentTime: Long, timeoutThresholdTime: Option[Long]\n+  ) extends RDD[TrackStateRDDRecord[K, S, T]](\n+    partitionedDataRDD.sparkContext,\n+    List(\n+      new OneToOneDependency[TrackStateRDDRecord[K, S, T]](prevStateRDD),\n+      new OneToOneDependency(partitionedDataRDD))\n+  ) {\n+\n+  @volatile private var doFullScan = false\n+\n+  require(prevStateRDD.partitioner.nonEmpty)\n+  require(partitionedDataRDD.partitioner == prevStateRDD.partitioner)\n+\n+  override val partitioner = prevStateRDD.partitioner\n+\n+  override def checkpoint(): Unit = {\n+    super.checkpoint()\n+    doFullScan = true\n+  }\n+\n+  override def compute(\n+      partition: Partition, context: TaskContext): Iterator[TrackStateRDDRecord[K, S, T]] = {\n+\n+    val stateRDDPartition = partition.asInstanceOf[TrackStateRDDPartition]\n+    val prevStateRDDIterator = prevStateRDD.iterator(\n+      stateRDDPartition.previousSessionRDDPartition, context)\n+    val dataIterator = partitionedDataRDD.iterator(\n+      stateRDDPartition.partitionedDataRDDPartition, context)\n+\n+    val newStateMap = if (prevStateRDDIterator.hasNext) {\n+      prevStateRDDIterator.next().stateMap.copy()\n+    } else {\n+      new EmptyStateMap[K, S]()\n+    }\n+\n+    val emittedRecords = new ArrayBuffer[T]\n+\n+    val wrappedState = new StateImpl[S]()\n+\n+    dataIterator.foreach { case (key, value) =>\n+      wrappedState.wrap(newStateMap.get(key))\n+      val emittedRecord = trackingFunction(key, Some(value), wrappedState)\n+      if (wrappedState.isRemoved) {\n+        newStateMap.remove(key)\n+      } else if (wrappedState.isUpdated) {\n+        newStateMap.put(key, wrappedState.get(), currentTime)\n+      }\n+      emittedRecords ++= emittedRecord\n+    }\n+\n+    if (doFullScan) {\n+      if (timeoutThresholdTime.isDefined) {\n+        newStateMap.getByTime(timeoutThresholdTime.get).foreach { case (key, state, _) =>\n+          wrappedState.wrapTiminoutState(state)\n+          val emittedRecord = trackingFunction(key, None, wrappedState)\n+          emittedRecords ++= emittedRecord\n+          newStateMap.remove(key)\n+        }\n+      }\n+    }\n+\n+    Iterator(TrackStateRDDRecord(newStateMap, emittedRecords))\n+  }\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    Array.tabulate(prevStateRDD.partitions.length) { i =>\n+      new TrackStateRDDPartition(i, prevStateRDD, partitionedDataRDD)}\n+  }\n+\n+  override def clearDependencies() {"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "that should have been caught by scala style check. We strictly avoid this.\n",
    "commit": "ae64786fd937002a2cc1f80518d54e970a6bbb21",
    "createdAt": "2015-11-06T23:40:39Z",
    "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.{IOException, ObjectInputStream, ObjectOutputStream}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark.rdd.{MapPartitionsRDD, RDD}\n+import org.apache.spark.streaming.{StateImpl, State}\n+import org.apache.spark.streaming.util.{EmptyStateMap, StateMap}\n+import org.apache.spark.util.Utils\n+import org.apache.spark._\n+\n+private[streaming] case class TrackStateRDDRecord[K, S, T](\n+    var stateMap: StateMap[K, S], var emittedRecords: Seq[T]) {\n+  /*\n+  private def writeObject(outputStream: ObjectOutputStream): Unit = {\n+    outputStream.writeObject(stateMap)\n+    outputStream.writeInt(emittedRecords.size)\n+    val iterator = emittedRecords.iterator\n+    while(iterator.hasNext) {\n+      outputStream.writeObject(iterator.next)\n+    }\n+  }\n+\n+  private def readObject(inputStream: ObjectInputStream): Unit = {\n+    stateMap = inputStream.readObject().asInstanceOf[StateMap[K, S]]\n+    val numEmittedRecords = inputStream.readInt()\n+    val array = new Array[T](numEmittedRecords)\n+    var i = 0\n+    while(i < numEmittedRecords) {\n+      array(i) = inputStream.readObject().asInstanceOf[T]\n+    }\n+    emittedRecords = array.toSeq\n+  }*/\n+}\n+\n+\n+private[streaming] class TrackStateRDDPartition(\n+    idx: Int,\n+    @transient private var prevStateRDD: RDD[_],\n+    @transient private var partitionedDataRDD: RDD[_]) extends Partition {\n+\n+  private[rdd] var previousSessionRDDPartition: Partition = null\n+  private[rdd] var partitionedDataRDDPartition: Partition = null\n+\n+  override def index: Int = idx\n+  override def hashCode(): Int = idx\n+\n+  @throws(classOf[IOException])\n+  private def writeObject(oos: ObjectOutputStream): Unit = Utils.tryOrIOException {\n+    // Update the reference to parent split at the time of task serialization\n+    previousSessionRDDPartition = prevStateRDD.partitions(index)\n+    partitionedDataRDDPartition = partitionedDataRDD.partitions(index)\n+    oos.defaultWriteObject()\n+  }\n+}\n+\n+\n+/**\n+ * RDD storing the keyed-state of trackStateByKey and corresponding emitted records.\n+ * Each partition of this RDD has a single record that contains a StateMap storing\n+ */\n+private[streaming] class TrackStateRDD[K: ClassTag, V: ClassTag, S: ClassTag, T: ClassTag](\n+    private var prevStateRDD: RDD[TrackStateRDDRecord[K, S, T]],\n+    private var partitionedDataRDD: RDD[(K, V)],\n+    trackingFunction: (K, Option[V], State[S]) => Option[T],\n+    currentTime: Long, timeoutThresholdTime: Option[Long]\n+  ) extends RDD[TrackStateRDDRecord[K, S, T]](\n+    partitionedDataRDD.sparkContext,\n+    List(\n+      new OneToOneDependency[TrackStateRDDRecord[K, S, T]](prevStateRDD),\n+      new OneToOneDependency(partitionedDataRDD))\n+  ) {\n+\n+  @volatile private var doFullScan = false\n+\n+  require(prevStateRDD.partitioner.nonEmpty)\n+  require(partitionedDataRDD.partitioner == prevStateRDD.partitioner)\n+\n+  override val partitioner = prevStateRDD.partitioner\n+\n+  override def checkpoint(): Unit = {\n+    super.checkpoint()\n+    doFullScan = true\n+  }\n+\n+  override def compute(\n+      partition: Partition, context: TaskContext): Iterator[TrackStateRDDRecord[K, S, T]] = {\n+\n+    val stateRDDPartition = partition.asInstanceOf[TrackStateRDDPartition]\n+    val prevStateRDDIterator = prevStateRDD.iterator(\n+      stateRDDPartition.previousSessionRDDPartition, context)\n+    val dataIterator = partitionedDataRDD.iterator(\n+      stateRDDPartition.partitionedDataRDDPartition, context)\n+\n+    val newStateMap = if (prevStateRDDIterator.hasNext) {\n+      prevStateRDDIterator.next().stateMap.copy()\n+    } else {\n+      new EmptyStateMap[K, S]()\n+    }\n+\n+    val emittedRecords = new ArrayBuffer[T]\n+\n+    val wrappedState = new StateImpl[S]()\n+\n+    dataIterator.foreach { case (key, value) =>\n+      wrappedState.wrap(newStateMap.get(key))\n+      val emittedRecord = trackingFunction(key, Some(value), wrappedState)\n+      if (wrappedState.isRemoved) {\n+        newStateMap.remove(key)\n+      } else if (wrappedState.isUpdated) {\n+        newStateMap.put(key, wrappedState.get(), currentTime)\n+      }\n+      emittedRecords ++= emittedRecord\n+    }\n+\n+    if (doFullScan) {\n+      if (timeoutThresholdTime.isDefined) {\n+        newStateMap.getByTime(timeoutThresholdTime.get).foreach { case (key, state, _) =>\n+          wrappedState.wrapTiminoutState(state)\n+          val emittedRecord = trackingFunction(key, None, wrappedState)\n+          emittedRecords ++= emittedRecord\n+          newStateMap.remove(key)\n+        }\n+      }\n+    }\n+\n+    Iterator(TrackStateRDDRecord(newStateMap, emittedRecords))\n+  }\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    Array.tabulate(prevStateRDD.partitions.length) { i =>\n+      new TrackStateRDDPartition(i, prevStateRDD, partitionedDataRDD)}\n+  }\n+\n+  override def clearDependencies() {"
  }],
  "prId": 9256
}, {
  "comments": [{
    "author": {
      "login": "huitseeker"
    },
    "body": "Maybe merge those `if`s in a single condition with `&&` ?\n",
    "commit": "ae64786fd937002a2cc1f80518d54e970a6bbb21",
    "createdAt": "2015-11-06T21:13:01Z",
    "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.{IOException, ObjectInputStream, ObjectOutputStream}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark.rdd.{MapPartitionsRDD, RDD}\n+import org.apache.spark.streaming.{StateImpl, State}\n+import org.apache.spark.streaming.util.{EmptyStateMap, StateMap}\n+import org.apache.spark.util.Utils\n+import org.apache.spark._\n+\n+private[streaming] case class TrackStateRDDRecord[K, S, T](\n+    var stateMap: StateMap[K, S], var emittedRecords: Seq[T]) {\n+  /*\n+  private def writeObject(outputStream: ObjectOutputStream): Unit = {\n+    outputStream.writeObject(stateMap)\n+    outputStream.writeInt(emittedRecords.size)\n+    val iterator = emittedRecords.iterator\n+    while(iterator.hasNext) {\n+      outputStream.writeObject(iterator.next)\n+    }\n+  }\n+\n+  private def readObject(inputStream: ObjectInputStream): Unit = {\n+    stateMap = inputStream.readObject().asInstanceOf[StateMap[K, S]]\n+    val numEmittedRecords = inputStream.readInt()\n+    val array = new Array[T](numEmittedRecords)\n+    var i = 0\n+    while(i < numEmittedRecords) {\n+      array(i) = inputStream.readObject().asInstanceOf[T]\n+    }\n+    emittedRecords = array.toSeq\n+  }*/\n+}\n+\n+\n+private[streaming] class TrackStateRDDPartition(\n+    idx: Int,\n+    @transient private var prevStateRDD: RDD[_],\n+    @transient private var partitionedDataRDD: RDD[_]) extends Partition {\n+\n+  private[rdd] var previousSessionRDDPartition: Partition = null\n+  private[rdd] var partitionedDataRDDPartition: Partition = null\n+\n+  override def index: Int = idx\n+  override def hashCode(): Int = idx\n+\n+  @throws(classOf[IOException])\n+  private def writeObject(oos: ObjectOutputStream): Unit = Utils.tryOrIOException {\n+    // Update the reference to parent split at the time of task serialization\n+    previousSessionRDDPartition = prevStateRDD.partitions(index)\n+    partitionedDataRDDPartition = partitionedDataRDD.partitions(index)\n+    oos.defaultWriteObject()\n+  }\n+}\n+\n+\n+/**\n+ * RDD storing the keyed-state of trackStateByKey and corresponding emitted records.\n+ * Each partition of this RDD has a single record that contains a StateMap storing\n+ */\n+private[streaming] class TrackStateRDD[K: ClassTag, V: ClassTag, S: ClassTag, T: ClassTag](\n+    private var prevStateRDD: RDD[TrackStateRDDRecord[K, S, T]],\n+    private var partitionedDataRDD: RDD[(K, V)],\n+    trackingFunction: (K, Option[V], State[S]) => Option[T],\n+    currentTime: Long, timeoutThresholdTime: Option[Long]\n+  ) extends RDD[TrackStateRDDRecord[K, S, T]](\n+    partitionedDataRDD.sparkContext,\n+    List(\n+      new OneToOneDependency[TrackStateRDDRecord[K, S, T]](prevStateRDD),\n+      new OneToOneDependency(partitionedDataRDD))\n+  ) {\n+\n+  @volatile private var doFullScan = false\n+\n+  require(prevStateRDD.partitioner.nonEmpty)\n+  require(partitionedDataRDD.partitioner == prevStateRDD.partitioner)\n+\n+  override val partitioner = prevStateRDD.partitioner\n+\n+  override def checkpoint(): Unit = {\n+    super.checkpoint()\n+    doFullScan = true\n+  }\n+\n+  override def compute(\n+      partition: Partition, context: TaskContext): Iterator[TrackStateRDDRecord[K, S, T]] = {\n+\n+    val stateRDDPartition = partition.asInstanceOf[TrackStateRDDPartition]\n+    val prevStateRDDIterator = prevStateRDD.iterator(\n+      stateRDDPartition.previousSessionRDDPartition, context)\n+    val dataIterator = partitionedDataRDD.iterator(\n+      stateRDDPartition.partitionedDataRDDPartition, context)\n+\n+    val newStateMap = if (prevStateRDDIterator.hasNext) {\n+      prevStateRDDIterator.next().stateMap.copy()\n+    } else {\n+      new EmptyStateMap[K, S]()\n+    }\n+\n+    val emittedRecords = new ArrayBuffer[T]\n+\n+    val wrappedState = new StateImpl[S]()\n+\n+    dataIterator.foreach { case (key, value) =>\n+      wrappedState.wrap(newStateMap.get(key))\n+      val emittedRecord = trackingFunction(key, Some(value), wrappedState)\n+      if (wrappedState.isRemoved) {\n+        newStateMap.remove(key)\n+      } else if (wrappedState.isUpdated) {\n+        newStateMap.put(key, wrappedState.get(), currentTime)\n+      }\n+      emittedRecords ++= emittedRecord\n+    }\n+\n+    if (doFullScan) {\n+      if (timeoutThresholdTime.isDefined) {"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "done.\n",
    "commit": "ae64786fd937002a2cc1f80518d54e970a6bbb21",
    "createdAt": "2015-11-07T01:45:53Z",
    "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.{IOException, ObjectInputStream, ObjectOutputStream}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark.rdd.{MapPartitionsRDD, RDD}\n+import org.apache.spark.streaming.{StateImpl, State}\n+import org.apache.spark.streaming.util.{EmptyStateMap, StateMap}\n+import org.apache.spark.util.Utils\n+import org.apache.spark._\n+\n+private[streaming] case class TrackStateRDDRecord[K, S, T](\n+    var stateMap: StateMap[K, S], var emittedRecords: Seq[T]) {\n+  /*\n+  private def writeObject(outputStream: ObjectOutputStream): Unit = {\n+    outputStream.writeObject(stateMap)\n+    outputStream.writeInt(emittedRecords.size)\n+    val iterator = emittedRecords.iterator\n+    while(iterator.hasNext) {\n+      outputStream.writeObject(iterator.next)\n+    }\n+  }\n+\n+  private def readObject(inputStream: ObjectInputStream): Unit = {\n+    stateMap = inputStream.readObject().asInstanceOf[StateMap[K, S]]\n+    val numEmittedRecords = inputStream.readInt()\n+    val array = new Array[T](numEmittedRecords)\n+    var i = 0\n+    while(i < numEmittedRecords) {\n+      array(i) = inputStream.readObject().asInstanceOf[T]\n+    }\n+    emittedRecords = array.toSeq\n+  }*/\n+}\n+\n+\n+private[streaming] class TrackStateRDDPartition(\n+    idx: Int,\n+    @transient private var prevStateRDD: RDD[_],\n+    @transient private var partitionedDataRDD: RDD[_]) extends Partition {\n+\n+  private[rdd] var previousSessionRDDPartition: Partition = null\n+  private[rdd] var partitionedDataRDDPartition: Partition = null\n+\n+  override def index: Int = idx\n+  override def hashCode(): Int = idx\n+\n+  @throws(classOf[IOException])\n+  private def writeObject(oos: ObjectOutputStream): Unit = Utils.tryOrIOException {\n+    // Update the reference to parent split at the time of task serialization\n+    previousSessionRDDPartition = prevStateRDD.partitions(index)\n+    partitionedDataRDDPartition = partitionedDataRDD.partitions(index)\n+    oos.defaultWriteObject()\n+  }\n+}\n+\n+\n+/**\n+ * RDD storing the keyed-state of trackStateByKey and corresponding emitted records.\n+ * Each partition of this RDD has a single record that contains a StateMap storing\n+ */\n+private[streaming] class TrackStateRDD[K: ClassTag, V: ClassTag, S: ClassTag, T: ClassTag](\n+    private var prevStateRDD: RDD[TrackStateRDDRecord[K, S, T]],\n+    private var partitionedDataRDD: RDD[(K, V)],\n+    trackingFunction: (K, Option[V], State[S]) => Option[T],\n+    currentTime: Long, timeoutThresholdTime: Option[Long]\n+  ) extends RDD[TrackStateRDDRecord[K, S, T]](\n+    partitionedDataRDD.sparkContext,\n+    List(\n+      new OneToOneDependency[TrackStateRDDRecord[K, S, T]](prevStateRDD),\n+      new OneToOneDependency(partitionedDataRDD))\n+  ) {\n+\n+  @volatile private var doFullScan = false\n+\n+  require(prevStateRDD.partitioner.nonEmpty)\n+  require(partitionedDataRDD.partitioner == prevStateRDD.partitioner)\n+\n+  override val partitioner = prevStateRDD.partitioner\n+\n+  override def checkpoint(): Unit = {\n+    super.checkpoint()\n+    doFullScan = true\n+  }\n+\n+  override def compute(\n+      partition: Partition, context: TaskContext): Iterator[TrackStateRDDRecord[K, S, T]] = {\n+\n+    val stateRDDPartition = partition.asInstanceOf[TrackStateRDDPartition]\n+    val prevStateRDDIterator = prevStateRDD.iterator(\n+      stateRDDPartition.previousSessionRDDPartition, context)\n+    val dataIterator = partitionedDataRDD.iterator(\n+      stateRDDPartition.partitionedDataRDDPartition, context)\n+\n+    val newStateMap = if (prevStateRDDIterator.hasNext) {\n+      prevStateRDDIterator.next().stateMap.copy()\n+    } else {\n+      new EmptyStateMap[K, S]()\n+    }\n+\n+    val emittedRecords = new ArrayBuffer[T]\n+\n+    val wrappedState = new StateImpl[S]()\n+\n+    dataIterator.foreach { case (key, value) =>\n+      wrappedState.wrap(newStateMap.get(key))\n+      val emittedRecord = trackingFunction(key, Some(value), wrappedState)\n+      if (wrappedState.isRemoved) {\n+        newStateMap.remove(key)\n+      } else if (wrappedState.isUpdated) {\n+        newStateMap.put(key, wrappedState.get(), currentTime)\n+      }\n+      emittedRecords ++= emittedRecord\n+    }\n+\n+    if (doFullScan) {\n+      if (timeoutThresholdTime.isDefined) {"
  }],
  "prId": 9256
}, {
  "comments": [{
    "author": {
      "login": "mateiz"
    },
    "body": "Do we need these?\n",
    "commit": "ae64786fd937002a2cc1f80518d54e970a6bbb21",
    "createdAt": "2015-11-07T16:27:01Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.{IOException, ObjectInputStream, ObjectOutputStream}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark.rdd.{MapPartitionsRDD, RDD}\n+import org.apache.spark.streaming.{StateImpl, State}\n+import org.apache.spark.streaming.util.{EmptyStateMap, StateMap}\n+import org.apache.spark.util.Utils\n+import org.apache.spark._\n+\n+private[streaming] case class TrackStateRDDRecord[K, S, T](\n+    var stateMap: StateMap[K, S], var emittedRecords: Seq[T]) {\n+  /*\n+  private def writeObject(outputStream: ObjectOutputStream): Unit = {\n+    outputStream.writeObject(stateMap)\n+    outputStream.writeInt(emittedRecords.size)\n+    val iterator = emittedRecords.iterator\n+    while(iterator.hasNext) {\n+      outputStream.writeObject(iterator.next)\n+    }\n+  }\n+\n+  private def readObject(inputStream: ObjectInputStream): Unit = {\n+    stateMap = inputStream.readObject().asInstanceOf[StateMap[K, S]]\n+    val numEmittedRecords = inputStream.readInt()\n+    val array = new Array[T](numEmittedRecords)\n+    var i = 0\n+    while(i < numEmittedRecords) {\n+      array(i) = inputStream.readObject().asInstanceOf[T]\n+    }\n+    emittedRecords = array.toSeq\n+  }*/"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Still a bit of WIP. I wanted to optimized the serialization a little bit, but I will move this work to a different PR. \n",
    "commit": "ae64786fd937002a2cc1f80518d54e970a6bbb21",
    "createdAt": "2015-11-09T00:09:09Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.{IOException, ObjectInputStream, ObjectOutputStream}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark.rdd.{MapPartitionsRDD, RDD}\n+import org.apache.spark.streaming.{StateImpl, State}\n+import org.apache.spark.streaming.util.{EmptyStateMap, StateMap}\n+import org.apache.spark.util.Utils\n+import org.apache.spark._\n+\n+private[streaming] case class TrackStateRDDRecord[K, S, T](\n+    var stateMap: StateMap[K, S], var emittedRecords: Seq[T]) {\n+  /*\n+  private def writeObject(outputStream: ObjectOutputStream): Unit = {\n+    outputStream.writeObject(stateMap)\n+    outputStream.writeInt(emittedRecords.size)\n+    val iterator = emittedRecords.iterator\n+    while(iterator.hasNext) {\n+      outputStream.writeObject(iterator.next)\n+    }\n+  }\n+\n+  private def readObject(inputStream: ObjectInputStream): Unit = {\n+    stateMap = inputStream.readObject().asInstanceOf[StateMap[K, S]]\n+    val numEmittedRecords = inputStream.readInt()\n+    val array = new Array[T](numEmittedRecords)\n+    var i = 0\n+    while(i < numEmittedRecords) {\n+      array(i) = inputStream.readObject().asInstanceOf[T]\n+    }\n+    emittedRecords = array.toSeq\n+  }*/"
  }],
  "prId": 9256
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "nit: `timeoutThresholdTime` should be in a new line.\n",
    "commit": "ae64786fd937002a2cc1f80518d54e970a6bbb21",
    "createdAt": "2015-11-11T05:38:10Z",
    "diffHunk": "@@ -0,0 +1,188 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.{IOException, ObjectInputStream, ObjectOutputStream}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark.rdd.{MapPartitionsRDD, RDD}\n+import org.apache.spark.streaming.{Time, StateImpl, State}\n+import org.apache.spark.streaming.util.{EmptyStateMap, StateMap}\n+import org.apache.spark.util.Utils\n+import org.apache.spark._\n+\n+/**\n+ * Record storing the keyed-state [[TrackStateRDD]]. Each record contains a [[StateMap]] and a\n+ * sequence of records returned by the tracking function of `trackStateByKey`.\n+ */\n+private[streaming] case class TrackStateRDDRecord[K, S, T](\n+    var stateMap: StateMap[K, S], var emittedRecords: Seq[T])\n+\n+/**\n+ * Partition of the [[TrackStateRDD]], which depends on corresponding partitions of prev state\n+ * RDD, and a partitioned keyed-data RDD\n+ */\n+private[streaming] class TrackStateRDDPartition(\n+    idx: Int,\n+    @transient private var prevStateRDD: RDD[_],\n+    @transient private var partitionedDataRDD: RDD[_]) extends Partition {\n+\n+  private[rdd] var previousSessionRDDPartition: Partition = null\n+  private[rdd] var partitionedDataRDDPartition: Partition = null\n+\n+  override def index: Int = idx\n+  override def hashCode(): Int = idx\n+\n+  @throws(classOf[IOException])\n+  private def writeObject(oos: ObjectOutputStream): Unit = Utils.tryOrIOException {\n+    // Update the reference to parent split at the time of task serialization\n+    previousSessionRDDPartition = prevStateRDD.partitions(index)\n+    partitionedDataRDDPartition = partitionedDataRDD.partitions(index)\n+    oos.defaultWriteObject()\n+  }\n+}\n+\n+\n+/**\n+ * RDD storing the keyed-state of `trackStateByKey` and corresponding emitted records.\n+ * Each partition of this RDD has a single record of type [[TrackStateRDDRecord]]. This contains a\n+ * [[StateMap]] (containing the keyed-states) and the sequence of records returned by the tracking\n+ * function of  `trackStateByKey`.\n+ * @param prevStateRDD The previous TrackStateRDD on whose StateMap data `this` RDD will be created\n+ * @param partitionedDataRDD The partitioned data RDD which is used update the previous StateMaps\n+ *                           in the `prevStateRDD` to create `this` RDD\n+ * @param trackingFunction The function that will be used to update state and return new data\n+ * @param batchTime        The time of the batch to which this RDD belongs to. Use to update\n+ */\n+private[streaming] class TrackStateRDD[K: ClassTag, V: ClassTag, S: ClassTag, T: ClassTag](\n+    private var prevStateRDD: RDD[TrackStateRDDRecord[K, S, T]],\n+    private var partitionedDataRDD: RDD[(K, V)],\n+    trackingFunction: (Time, K, Option[V], State[S]) => Option[T],\n+    batchTime: Time, timeoutThresholdTime: Option[Long]",
    "line": 78
  }, {
    "author": {
      "login": "zsxwing"
    },
    "body": "no doc for `timeoutThresholdTime`\n",
    "commit": "ae64786fd937002a2cc1f80518d54e970a6bbb21",
    "createdAt": "2015-11-11T05:38:36Z",
    "diffHunk": "@@ -0,0 +1,188 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.rdd\n+\n+import java.io.{IOException, ObjectInputStream, ObjectOutputStream}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark.rdd.{MapPartitionsRDD, RDD}\n+import org.apache.spark.streaming.{Time, StateImpl, State}\n+import org.apache.spark.streaming.util.{EmptyStateMap, StateMap}\n+import org.apache.spark.util.Utils\n+import org.apache.spark._\n+\n+/**\n+ * Record storing the keyed-state [[TrackStateRDD]]. Each record contains a [[StateMap]] and a\n+ * sequence of records returned by the tracking function of `trackStateByKey`.\n+ */\n+private[streaming] case class TrackStateRDDRecord[K, S, T](\n+    var stateMap: StateMap[K, S], var emittedRecords: Seq[T])\n+\n+/**\n+ * Partition of the [[TrackStateRDD]], which depends on corresponding partitions of prev state\n+ * RDD, and a partitioned keyed-data RDD\n+ */\n+private[streaming] class TrackStateRDDPartition(\n+    idx: Int,\n+    @transient private var prevStateRDD: RDD[_],\n+    @transient private var partitionedDataRDD: RDD[_]) extends Partition {\n+\n+  private[rdd] var previousSessionRDDPartition: Partition = null\n+  private[rdd] var partitionedDataRDDPartition: Partition = null\n+\n+  override def index: Int = idx\n+  override def hashCode(): Int = idx\n+\n+  @throws(classOf[IOException])\n+  private def writeObject(oos: ObjectOutputStream): Unit = Utils.tryOrIOException {\n+    // Update the reference to parent split at the time of task serialization\n+    previousSessionRDDPartition = prevStateRDD.partitions(index)\n+    partitionedDataRDDPartition = partitionedDataRDD.partitions(index)\n+    oos.defaultWriteObject()\n+  }\n+}\n+\n+\n+/**\n+ * RDD storing the keyed-state of `trackStateByKey` and corresponding emitted records.\n+ * Each partition of this RDD has a single record of type [[TrackStateRDDRecord]]. This contains a\n+ * [[StateMap]] (containing the keyed-states) and the sequence of records returned by the tracking\n+ * function of  `trackStateByKey`.\n+ * @param prevStateRDD The previous TrackStateRDD on whose StateMap data `this` RDD will be created\n+ * @param partitionedDataRDD The partitioned data RDD which is used update the previous StateMaps\n+ *                           in the `prevStateRDD` to create `this` RDD\n+ * @param trackingFunction The function that will be used to update state and return new data\n+ * @param batchTime        The time of the batch to which this RDD belongs to. Use to update\n+ */\n+private[streaming] class TrackStateRDD[K: ClassTag, V: ClassTag, S: ClassTag, T: ClassTag](\n+    private var prevStateRDD: RDD[TrackStateRDDRecord[K, S, T]],\n+    private var partitionedDataRDD: RDD[(K, V)],\n+    trackingFunction: (Time, K, Option[V], State[S]) => Option[T],\n+    batchTime: Time, timeoutThresholdTime: Option[Long]",
    "line": 78
  }],
  "prId": 9256
}]