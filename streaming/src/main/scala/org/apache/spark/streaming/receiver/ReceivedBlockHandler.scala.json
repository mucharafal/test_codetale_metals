[{
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "If we only have a single thread, does that limit the number of outstanding futures at any given time? I'm guessing it doesn't, but not very familiar with the scala future API so I wanted to check.\n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-28T19:40:26Z",
    "diffHunk": "@@ -0,0 +1,144 @@\n+package org.apache.spark.streaming.receiver\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{Await, ExecutionContext, Future}\n+import scala.concurrent.duration._\n+import scala.language.{existentials, postfixOps}\n+\n+import WriteAheadLogBasedBlockHandler._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.spark.{SparkException, Logging, SparkConf}\n+import org.apache.spark.storage.{BlockManager, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{Clock, SystemClock, WriteAheadLogManager}\n+import org.apache.spark.util.Utils\n+\n+private[streaming] sealed trait ReceivedBlock\n+private[streaming] case class ArrayBufferBlock(arrayBuffer: ArrayBuffer[_]) extends ReceivedBlock\n+private[streaming] case class IteratorBlock(iterator: Iterator[_]) extends ReceivedBlock\n+private[streaming] case class ByteBufferBlock(byteBuffer: ByteBuffer) extends ReceivedBlock\n+\n+\n+/** Trait that represents a class that handles the storage of blocks received by receiver */\n+private[streaming] trait ReceivedBlockHandler {\n+\n+  /** Store a received block with the given block id */\n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef]\n+\n+  /** Cleanup old blocks older than the given threshold time */\n+  def cleanupOldBlock(threshTime: Long)\n+}\n+\n+/**\n+ * Implementation of a [[org.apache.spark.streaming.receiver.ReceivedBlockHandler]] which\n+ * stores the received blocks into a block manager with the specified storage level.\n+ */\n+private[streaming] class BlockManagerBasedBlockHandler(\n+    blockManager: BlockManager, storageLevel: StorageLevel)\n+  extends ReceivedBlockHandler with Logging {\n+  \n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef] = {\n+    val putResult = receivedBlock match {\n+      case ArrayBufferBlock(arrayBuffer) =>\n+        blockManager.putIterator(blockId, arrayBuffer.iterator, storageLevel, tellMaster = true)\n+      case IteratorBlock(iterator) =>\n+        blockManager.putIterator(blockId, iterator, storageLevel, tellMaster = true)\n+      case ByteBufferBlock(byteBuffer) =>\n+        blockManager.putBytes(blockId, byteBuffer, storageLevel, tellMaster = true)\n+      case _ =>\n+        throw new SparkException(s\"Could not store $blockId to block manager, unexpected block type\")\n+    }\n+    if (!putResult.map { _._1 }.contains(blockId)) {\n+      throw new SparkException(\n+        s\"Could not store $blockId to block manager with storage level $storageLevel\")\n+    }\n+    None\n+  }\n+\n+  def cleanupOldBlock(threshTime: Long) {\n+    // this is not used as blocks inserted into the BlockManager are cleared by DStream's clearing\n+    // of BlockRDDs.\n+  }\n+}\n+\n+/**\n+ * Implementation of a [[org.apache.spark.streaming.receiver.ReceivedBlockHandler]] which\n+ * stores the received blocks in both, a write ahead log and a block manager.\n+ */\n+private[streaming] class WriteAheadLogBasedBlockHandler(\n+    blockManager: BlockManager,\n+    streamId: Int,\n+    storageLevel: StorageLevel,\n+    conf: SparkConf,\n+    hadoopConf: Configuration,\n+    checkpointDir: String,\n+    clock: Clock = new SystemClock\n+  ) extends ReceivedBlockHandler with Logging {\n+\n+  private val blockStoreTimeout = conf.getInt(\n+    \"spark.streaming.receiver.blockStoreTimeout\", 30).seconds\n+  private val rollingInterval = conf.getInt(\n+    \"spark.streaming.receiver.writeAheadLog.rollingInterval\", 60)\n+  private val maxFailures = conf.getInt(\n+    \"spark.streaming.receiver.writeAheadLog.maxFailures\", 3)\n+\n+  private val logManager = new WriteAheadLogManager(\n+    checkpointDirToLogDir(checkpointDir, streamId),\n+    hadoopConf, rollingInterval, maxFailures,\n+    callerName = \"WriteAheadLogBasedBlockHandler\",\n+    clock = clock\n+  )\n+\n+  // For processing futures used in parallel block storing into block manager and write ahead log\n+  implicit private val executionContext = ExecutionContext.fromExecutorService(\n+    Utils.newDaemonFixedThreadPool(1, \"WriteAheadLogBasedBlockHandler\"))"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "I dont have much knowledge either, but my guess is the same as yours.. it does not. \n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-28T21:47:12Z",
    "diffHunk": "@@ -0,0 +1,144 @@\n+package org.apache.spark.streaming.receiver\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{Await, ExecutionContext, Future}\n+import scala.concurrent.duration._\n+import scala.language.{existentials, postfixOps}\n+\n+import WriteAheadLogBasedBlockHandler._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.spark.{SparkException, Logging, SparkConf}\n+import org.apache.spark.storage.{BlockManager, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{Clock, SystemClock, WriteAheadLogManager}\n+import org.apache.spark.util.Utils\n+\n+private[streaming] sealed trait ReceivedBlock\n+private[streaming] case class ArrayBufferBlock(arrayBuffer: ArrayBuffer[_]) extends ReceivedBlock\n+private[streaming] case class IteratorBlock(iterator: Iterator[_]) extends ReceivedBlock\n+private[streaming] case class ByteBufferBlock(byteBuffer: ByteBuffer) extends ReceivedBlock\n+\n+\n+/** Trait that represents a class that handles the storage of blocks received by receiver */\n+private[streaming] trait ReceivedBlockHandler {\n+\n+  /** Store a received block with the given block id */\n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef]\n+\n+  /** Cleanup old blocks older than the given threshold time */\n+  def cleanupOldBlock(threshTime: Long)\n+}\n+\n+/**\n+ * Implementation of a [[org.apache.spark.streaming.receiver.ReceivedBlockHandler]] which\n+ * stores the received blocks into a block manager with the specified storage level.\n+ */\n+private[streaming] class BlockManagerBasedBlockHandler(\n+    blockManager: BlockManager, storageLevel: StorageLevel)\n+  extends ReceivedBlockHandler with Logging {\n+  \n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef] = {\n+    val putResult = receivedBlock match {\n+      case ArrayBufferBlock(arrayBuffer) =>\n+        blockManager.putIterator(blockId, arrayBuffer.iterator, storageLevel, tellMaster = true)\n+      case IteratorBlock(iterator) =>\n+        blockManager.putIterator(blockId, iterator, storageLevel, tellMaster = true)\n+      case ByteBufferBlock(byteBuffer) =>\n+        blockManager.putBytes(blockId, byteBuffer, storageLevel, tellMaster = true)\n+      case _ =>\n+        throw new SparkException(s\"Could not store $blockId to block manager, unexpected block type\")\n+    }\n+    if (!putResult.map { _._1 }.contains(blockId)) {\n+      throw new SparkException(\n+        s\"Could not store $blockId to block manager with storage level $storageLevel\")\n+    }\n+    None\n+  }\n+\n+  def cleanupOldBlock(threshTime: Long) {\n+    // this is not used as blocks inserted into the BlockManager are cleared by DStream's clearing\n+    // of BlockRDDs.\n+  }\n+}\n+\n+/**\n+ * Implementation of a [[org.apache.spark.streaming.receiver.ReceivedBlockHandler]] which\n+ * stores the received blocks in both, a write ahead log and a block manager.\n+ */\n+private[streaming] class WriteAheadLogBasedBlockHandler(\n+    blockManager: BlockManager,\n+    streamId: Int,\n+    storageLevel: StorageLevel,\n+    conf: SparkConf,\n+    hadoopConf: Configuration,\n+    checkpointDir: String,\n+    clock: Clock = new SystemClock\n+  ) extends ReceivedBlockHandler with Logging {\n+\n+  private val blockStoreTimeout = conf.getInt(\n+    \"spark.streaming.receiver.blockStoreTimeout\", 30).seconds\n+  private val rollingInterval = conf.getInt(\n+    \"spark.streaming.receiver.writeAheadLog.rollingInterval\", 60)\n+  private val maxFailures = conf.getInt(\n+    \"spark.streaming.receiver.writeAheadLog.maxFailures\", 3)\n+\n+  private val logManager = new WriteAheadLogManager(\n+    checkpointDirToLogDir(checkpointDir, streamId),\n+    hadoopConf, rollingInterval, maxFailures,\n+    callerName = \"WriteAheadLogBasedBlockHandler\",\n+    clock = clock\n+  )\n+\n+  // For processing futures used in parallel block storing into block manager and write ahead log\n+  implicit private val executionContext = ExecutionContext.fromExecutorService(\n+    Utils.newDaemonFixedThreadPool(1, \"WriteAheadLogBasedBlockHandler\"))"
  }, {
    "author": {
      "login": "harishreedharan"
    },
    "body": "I gave this a try. It does not seem to limit the outstanding futures, but I did some some scheduling delays.\n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-28T22:54:15Z",
    "diffHunk": "@@ -0,0 +1,144 @@\n+package org.apache.spark.streaming.receiver\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{Await, ExecutionContext, Future}\n+import scala.concurrent.duration._\n+import scala.language.{existentials, postfixOps}\n+\n+import WriteAheadLogBasedBlockHandler._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.spark.{SparkException, Logging, SparkConf}\n+import org.apache.spark.storage.{BlockManager, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{Clock, SystemClock, WriteAheadLogManager}\n+import org.apache.spark.util.Utils\n+\n+private[streaming] sealed trait ReceivedBlock\n+private[streaming] case class ArrayBufferBlock(arrayBuffer: ArrayBuffer[_]) extends ReceivedBlock\n+private[streaming] case class IteratorBlock(iterator: Iterator[_]) extends ReceivedBlock\n+private[streaming] case class ByteBufferBlock(byteBuffer: ByteBuffer) extends ReceivedBlock\n+\n+\n+/** Trait that represents a class that handles the storage of blocks received by receiver */\n+private[streaming] trait ReceivedBlockHandler {\n+\n+  /** Store a received block with the given block id */\n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef]\n+\n+  /** Cleanup old blocks older than the given threshold time */\n+  def cleanupOldBlock(threshTime: Long)\n+}\n+\n+/**\n+ * Implementation of a [[org.apache.spark.streaming.receiver.ReceivedBlockHandler]] which\n+ * stores the received blocks into a block manager with the specified storage level.\n+ */\n+private[streaming] class BlockManagerBasedBlockHandler(\n+    blockManager: BlockManager, storageLevel: StorageLevel)\n+  extends ReceivedBlockHandler with Logging {\n+  \n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef] = {\n+    val putResult = receivedBlock match {\n+      case ArrayBufferBlock(arrayBuffer) =>\n+        blockManager.putIterator(blockId, arrayBuffer.iterator, storageLevel, tellMaster = true)\n+      case IteratorBlock(iterator) =>\n+        blockManager.putIterator(blockId, iterator, storageLevel, tellMaster = true)\n+      case ByteBufferBlock(byteBuffer) =>\n+        blockManager.putBytes(blockId, byteBuffer, storageLevel, tellMaster = true)\n+      case _ =>\n+        throw new SparkException(s\"Could not store $blockId to block manager, unexpected block type\")\n+    }\n+    if (!putResult.map { _._1 }.contains(blockId)) {\n+      throw new SparkException(\n+        s\"Could not store $blockId to block manager with storage level $storageLevel\")\n+    }\n+    None\n+  }\n+\n+  def cleanupOldBlock(threshTime: Long) {\n+    // this is not used as blocks inserted into the BlockManager are cleared by DStream's clearing\n+    // of BlockRDDs.\n+  }\n+}\n+\n+/**\n+ * Implementation of a [[org.apache.spark.streaming.receiver.ReceivedBlockHandler]] which\n+ * stores the received blocks in both, a write ahead log and a block manager.\n+ */\n+private[streaming] class WriteAheadLogBasedBlockHandler(\n+    blockManager: BlockManager,\n+    streamId: Int,\n+    storageLevel: StorageLevel,\n+    conf: SparkConf,\n+    hadoopConf: Configuration,\n+    checkpointDir: String,\n+    clock: Clock = new SystemClock\n+  ) extends ReceivedBlockHandler with Logging {\n+\n+  private val blockStoreTimeout = conf.getInt(\n+    \"spark.streaming.receiver.blockStoreTimeout\", 30).seconds\n+  private val rollingInterval = conf.getInt(\n+    \"spark.streaming.receiver.writeAheadLog.rollingInterval\", 60)\n+  private val maxFailures = conf.getInt(\n+    \"spark.streaming.receiver.writeAheadLog.maxFailures\", 3)\n+\n+  private val logManager = new WriteAheadLogManager(\n+    checkpointDirToLogDir(checkpointDir, streamId),\n+    hadoopConf, rollingInterval, maxFailures,\n+    callerName = \"WriteAheadLogBasedBlockHandler\",\n+    clock = clock\n+  )\n+\n+  // For processing futures used in parallel block storing into block manager and write ahead log\n+  implicit private val executionContext = ExecutionContext.fromExecutorService(\n+    Utils.newDaemonFixedThreadPool(1, \"WriteAheadLogBasedBlockHandler\"))"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Actually, I made a mistake. This should at least be 2, so that saving to BM and saving to WAL can proceed in parallel.\n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-28T22:56:43Z",
    "diffHunk": "@@ -0,0 +1,144 @@\n+package org.apache.spark.streaming.receiver\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{Await, ExecutionContext, Future}\n+import scala.concurrent.duration._\n+import scala.language.{existentials, postfixOps}\n+\n+import WriteAheadLogBasedBlockHandler._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.spark.{SparkException, Logging, SparkConf}\n+import org.apache.spark.storage.{BlockManager, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{Clock, SystemClock, WriteAheadLogManager}\n+import org.apache.spark.util.Utils\n+\n+private[streaming] sealed trait ReceivedBlock\n+private[streaming] case class ArrayBufferBlock(arrayBuffer: ArrayBuffer[_]) extends ReceivedBlock\n+private[streaming] case class IteratorBlock(iterator: Iterator[_]) extends ReceivedBlock\n+private[streaming] case class ByteBufferBlock(byteBuffer: ByteBuffer) extends ReceivedBlock\n+\n+\n+/** Trait that represents a class that handles the storage of blocks received by receiver */\n+private[streaming] trait ReceivedBlockHandler {\n+\n+  /** Store a received block with the given block id */\n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef]\n+\n+  /** Cleanup old blocks older than the given threshold time */\n+  def cleanupOldBlock(threshTime: Long)\n+}\n+\n+/**\n+ * Implementation of a [[org.apache.spark.streaming.receiver.ReceivedBlockHandler]] which\n+ * stores the received blocks into a block manager with the specified storage level.\n+ */\n+private[streaming] class BlockManagerBasedBlockHandler(\n+    blockManager: BlockManager, storageLevel: StorageLevel)\n+  extends ReceivedBlockHandler with Logging {\n+  \n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef] = {\n+    val putResult = receivedBlock match {\n+      case ArrayBufferBlock(arrayBuffer) =>\n+        blockManager.putIterator(blockId, arrayBuffer.iterator, storageLevel, tellMaster = true)\n+      case IteratorBlock(iterator) =>\n+        blockManager.putIterator(blockId, iterator, storageLevel, tellMaster = true)\n+      case ByteBufferBlock(byteBuffer) =>\n+        blockManager.putBytes(blockId, byteBuffer, storageLevel, tellMaster = true)\n+      case _ =>\n+        throw new SparkException(s\"Could not store $blockId to block manager, unexpected block type\")\n+    }\n+    if (!putResult.map { _._1 }.contains(blockId)) {\n+      throw new SparkException(\n+        s\"Could not store $blockId to block manager with storage level $storageLevel\")\n+    }\n+    None\n+  }\n+\n+  def cleanupOldBlock(threshTime: Long) {\n+    // this is not used as blocks inserted into the BlockManager are cleared by DStream's clearing\n+    // of BlockRDDs.\n+  }\n+}\n+\n+/**\n+ * Implementation of a [[org.apache.spark.streaming.receiver.ReceivedBlockHandler]] which\n+ * stores the received blocks in both, a write ahead log and a block manager.\n+ */\n+private[streaming] class WriteAheadLogBasedBlockHandler(\n+    blockManager: BlockManager,\n+    streamId: Int,\n+    storageLevel: StorageLevel,\n+    conf: SparkConf,\n+    hadoopConf: Configuration,\n+    checkpointDir: String,\n+    clock: Clock = new SystemClock\n+  ) extends ReceivedBlockHandler with Logging {\n+\n+  private val blockStoreTimeout = conf.getInt(\n+    \"spark.streaming.receiver.blockStoreTimeout\", 30).seconds\n+  private val rollingInterval = conf.getInt(\n+    \"spark.streaming.receiver.writeAheadLog.rollingInterval\", 60)\n+  private val maxFailures = conf.getInt(\n+    \"spark.streaming.receiver.writeAheadLog.maxFailures\", 3)\n+\n+  private val logManager = new WriteAheadLogManager(\n+    checkpointDirToLogDir(checkpointDir, streamId),\n+    hadoopConf, rollingInterval, maxFailures,\n+    callerName = \"WriteAheadLogBasedBlockHandler\",\n+    clock = clock\n+  )\n+\n+  // For processing futures used in parallel block storing into block manager and write ahead log\n+  implicit private val executionContext = ExecutionContext.fromExecutorService(\n+    Utils.newDaemonFixedThreadPool(1, \"WriteAheadLogBasedBlockHandler\"))"
  }, {
    "author": {
      "login": "harishreedharan"
    },
    "body": "Shouldn't this be higher? If there are multiple threads receiving data, storeBlock could end being called from each of these in parallel. At that point, this would block on threads being available to do the writes correct? \n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-28T23:11:49Z",
    "diffHunk": "@@ -0,0 +1,144 @@\n+package org.apache.spark.streaming.receiver\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{Await, ExecutionContext, Future}\n+import scala.concurrent.duration._\n+import scala.language.{existentials, postfixOps}\n+\n+import WriteAheadLogBasedBlockHandler._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.spark.{SparkException, Logging, SparkConf}\n+import org.apache.spark.storage.{BlockManager, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{Clock, SystemClock, WriteAheadLogManager}\n+import org.apache.spark.util.Utils\n+\n+private[streaming] sealed trait ReceivedBlock\n+private[streaming] case class ArrayBufferBlock(arrayBuffer: ArrayBuffer[_]) extends ReceivedBlock\n+private[streaming] case class IteratorBlock(iterator: Iterator[_]) extends ReceivedBlock\n+private[streaming] case class ByteBufferBlock(byteBuffer: ByteBuffer) extends ReceivedBlock\n+\n+\n+/** Trait that represents a class that handles the storage of blocks received by receiver */\n+private[streaming] trait ReceivedBlockHandler {\n+\n+  /** Store a received block with the given block id */\n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef]\n+\n+  /** Cleanup old blocks older than the given threshold time */\n+  def cleanupOldBlock(threshTime: Long)\n+}\n+\n+/**\n+ * Implementation of a [[org.apache.spark.streaming.receiver.ReceivedBlockHandler]] which\n+ * stores the received blocks into a block manager with the specified storage level.\n+ */\n+private[streaming] class BlockManagerBasedBlockHandler(\n+    blockManager: BlockManager, storageLevel: StorageLevel)\n+  extends ReceivedBlockHandler with Logging {\n+  \n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef] = {\n+    val putResult = receivedBlock match {\n+      case ArrayBufferBlock(arrayBuffer) =>\n+        blockManager.putIterator(blockId, arrayBuffer.iterator, storageLevel, tellMaster = true)\n+      case IteratorBlock(iterator) =>\n+        blockManager.putIterator(blockId, iterator, storageLevel, tellMaster = true)\n+      case ByteBufferBlock(byteBuffer) =>\n+        blockManager.putBytes(blockId, byteBuffer, storageLevel, tellMaster = true)\n+      case _ =>\n+        throw new SparkException(s\"Could not store $blockId to block manager, unexpected block type\")\n+    }\n+    if (!putResult.map { _._1 }.contains(blockId)) {\n+      throw new SparkException(\n+        s\"Could not store $blockId to block manager with storage level $storageLevel\")\n+    }\n+    None\n+  }\n+\n+  def cleanupOldBlock(threshTime: Long) {\n+    // this is not used as blocks inserted into the BlockManager are cleared by DStream's clearing\n+    // of BlockRDDs.\n+  }\n+}\n+\n+/**\n+ * Implementation of a [[org.apache.spark.streaming.receiver.ReceivedBlockHandler]] which\n+ * stores the received blocks in both, a write ahead log and a block manager.\n+ */\n+private[streaming] class WriteAheadLogBasedBlockHandler(\n+    blockManager: BlockManager,\n+    streamId: Int,\n+    storageLevel: StorageLevel,\n+    conf: SparkConf,\n+    hadoopConf: Configuration,\n+    checkpointDir: String,\n+    clock: Clock = new SystemClock\n+  ) extends ReceivedBlockHandler with Logging {\n+\n+  private val blockStoreTimeout = conf.getInt(\n+    \"spark.streaming.receiver.blockStoreTimeout\", 30).seconds\n+  private val rollingInterval = conf.getInt(\n+    \"spark.streaming.receiver.writeAheadLog.rollingInterval\", 60)\n+  private val maxFailures = conf.getInt(\n+    \"spark.streaming.receiver.writeAheadLog.maxFailures\", 3)\n+\n+  private val logManager = new WriteAheadLogManager(\n+    checkpointDirToLogDir(checkpointDir, streamId),\n+    hadoopConf, rollingInterval, maxFailures,\n+    callerName = \"WriteAheadLogBasedBlockHandler\",\n+    clock = clock\n+  )\n+\n+  // For processing futures used in parallel block storing into block manager and write ahead log\n+  implicit private val executionContext = ExecutionContext.fromExecutorService(\n+    Utils.newDaemonFixedThreadPool(1, \"WriteAheadLogBasedBlockHandler\"))"
  }],
  "prId": 2940
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "Is there value to having a shared trait here if you just return `Option[AnyRef]`? Why not just have two types of `ReceivedBlockHandler` that don't share a parent class. The caller cannot interact with them in an interchangeable way anyways, it needs to check the return type.\n\nAlso, could you specify the failure handling semantics for these storeBlock methods? Are they supposed to throw an exception on failure?\n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-28T19:48:43Z",
    "diffHunk": "@@ -0,0 +1,144 @@\n+package org.apache.spark.streaming.receiver\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{Await, ExecutionContext, Future}\n+import scala.concurrent.duration._\n+import scala.language.{existentials, postfixOps}\n+\n+import WriteAheadLogBasedBlockHandler._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.spark.{SparkException, Logging, SparkConf}\n+import org.apache.spark.storage.{BlockManager, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{Clock, SystemClock, WriteAheadLogManager}\n+import org.apache.spark.util.Utils\n+\n+private[streaming] sealed trait ReceivedBlock\n+private[streaming] case class ArrayBufferBlock(arrayBuffer: ArrayBuffer[_]) extends ReceivedBlock\n+private[streaming] case class IteratorBlock(iterator: Iterator[_]) extends ReceivedBlock\n+private[streaming] case class ByteBufferBlock(byteBuffer: ByteBuffer) extends ReceivedBlock\n+\n+\n+/** Trait that represents a class that handles the storage of blocks received by receiver */\n+private[streaming] trait ReceivedBlockHandler {\n+\n+  /** Store a received block with the given block id */\n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef]"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "Alternatively, is there a clean way to introduce type parameters to capture the return type?\n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-28T19:54:17Z",
    "diffHunk": "@@ -0,0 +1,144 @@\n+package org.apache.spark.streaming.receiver\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{Await, ExecutionContext, Future}\n+import scala.concurrent.duration._\n+import scala.language.{existentials, postfixOps}\n+\n+import WriteAheadLogBasedBlockHandler._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.spark.{SparkException, Logging, SparkConf}\n+import org.apache.spark.storage.{BlockManager, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{Clock, SystemClock, WriteAheadLogManager}\n+import org.apache.spark.util.Utils\n+\n+private[streaming] sealed trait ReceivedBlock\n+private[streaming] case class ArrayBufferBlock(arrayBuffer: ArrayBuffer[_]) extends ReceivedBlock\n+private[streaming] case class IteratorBlock(iterator: Iterator[_]) extends ReceivedBlock\n+private[streaming] case class ByteBufferBlock(byteBuffer: ByteBuffer) extends ReceivedBlock\n+\n+\n+/** Trait that represents a class that handles the storage of blocks received by receiver */\n+private[streaming] trait ReceivedBlockHandler {\n+\n+  /** Store a received block with the given block id */\n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef]"
  }, {
    "author": {
      "login": "pwendell"
    },
    "body": "Hm, another approach would be to have this return a type called `StoredBlockInfo` or something. And that could contain information about the BlockId and an optional file segment.\n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-28T19:56:35Z",
    "diffHunk": "@@ -0,0 +1,144 @@\n+package org.apache.spark.streaming.receiver\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{Await, ExecutionContext, Future}\n+import scala.concurrent.duration._\n+import scala.language.{existentials, postfixOps}\n+\n+import WriteAheadLogBasedBlockHandler._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.spark.{SparkException, Logging, SparkConf}\n+import org.apache.spark.storage.{BlockManager, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{Clock, SystemClock, WriteAheadLogManager}\n+import org.apache.spark.util.Utils\n+\n+private[streaming] sealed trait ReceivedBlock\n+private[streaming] case class ArrayBufferBlock(arrayBuffer: ArrayBuffer[_]) extends ReceivedBlock\n+private[streaming] case class IteratorBlock(iterator: Iterator[_]) extends ReceivedBlock\n+private[streaming] case class ByteBufferBlock(byteBuffer: ByteBuffer) extends ReceivedBlock\n+\n+\n+/** Trait that represents a class that handles the storage of blocks received by receiver */\n+private[streaming] trait ReceivedBlockHandler {\n+\n+  /** Store a received block with the given block id */\n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef]"
  }, {
    "author": {
      "login": "pwendell"
    },
    "body": "```\nclass StoredBlockInfo {\n  val blockId: String\n  val fileSegment: Option[FileSegment]\n}\n```\n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-28T19:57:19Z",
    "diffHunk": "@@ -0,0 +1,144 @@\n+package org.apache.spark.streaming.receiver\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{Await, ExecutionContext, Future}\n+import scala.concurrent.duration._\n+import scala.language.{existentials, postfixOps}\n+\n+import WriteAheadLogBasedBlockHandler._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.spark.{SparkException, Logging, SparkConf}\n+import org.apache.spark.storage.{BlockManager, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{Clock, SystemClock, WriteAheadLogManager}\n+import org.apache.spark.util.Utils\n+\n+private[streaming] sealed trait ReceivedBlock\n+private[streaming] case class ArrayBufferBlock(arrayBuffer: ArrayBuffer[_]) extends ReceivedBlock\n+private[streaming] case class IteratorBlock(iterator: Iterator[_]) extends ReceivedBlock\n+private[streaming] case class ByteBufferBlock(byteBuffer: ByteBuffer) extends ReceivedBlock\n+\n+\n+/** Trait that represents a class that handles the storage of blocks received by receiver */\n+private[streaming] trait ReceivedBlockHandler {\n+\n+  /** Store a received block with the given block id */\n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef]"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "I wanted to have a single clean interface out here (kind of like BlockStore, used in BlockManager, which is the base for MemoryStore, DiskStore, TachyonStore). We have only two implementation now, but in future there can be other implementations. For example, one may want to log to Cassandra, which will require a third implementation CassandraBasedBlockHandler. Hence I dont want the return type to have a specific type as the return type will be specific to the implementation. \n\nLet me take up Josh's suggestion of introducing type parameter. Should have done that from the start.\n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-28T21:51:44Z",
    "diffHunk": "@@ -0,0 +1,144 @@\n+package org.apache.spark.streaming.receiver\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{Await, ExecutionContext, Future}\n+import scala.concurrent.duration._\n+import scala.language.{existentials, postfixOps}\n+\n+import WriteAheadLogBasedBlockHandler._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.spark.{SparkException, Logging, SparkConf}\n+import org.apache.spark.storage.{BlockManager, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{Clock, SystemClock, WriteAheadLogManager}\n+import org.apache.spark.util.Utils\n+\n+private[streaming] sealed trait ReceivedBlock\n+private[streaming] case class ArrayBufferBlock(arrayBuffer: ArrayBuffer[_]) extends ReceivedBlock\n+private[streaming] case class IteratorBlock(iterator: Iterator[_]) extends ReceivedBlock\n+private[streaming] case class ByteBufferBlock(byteBuffer: ByteBuffer) extends ReceivedBlock\n+\n+\n+/** Trait that represents a class that handles the storage of blocks received by receiver */\n+private[streaming] trait ReceivedBlockHandler {\n+\n+  /** Store a received block with the given block id */\n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef]"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "I played with the typed traits, and realized that it doesnt help much. Here the points to note. \n- Which implementation of ReceivedBlockHandler will be created is not known at compile time (depends on runtime settings). \n- Users of this ReceivedBlockHandler (that is, ReceiverSupervisor) should treat the metadata returned by ReceivedBlockHandler.storeBlock() as a black box (so as a AnyRef or Any), and not have any implementation specific logic to keep things simple. \n\nHence, having type in the trait and returning specific type in each implementation is kind of moot, and downstream code will treat is as AnyRef or Any. Rather it introduces weirdness in some places. Ex. default implementation BlockManagerBasedBlockHandler.storeBlock always return None and hence does not really need a type. \n\nHence I am keeping the implementation as is.\n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-28T22:46:05Z",
    "diffHunk": "@@ -0,0 +1,144 @@\n+package org.apache.spark.streaming.receiver\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{Await, ExecutionContext, Future}\n+import scala.concurrent.duration._\n+import scala.language.{existentials, postfixOps}\n+\n+import WriteAheadLogBasedBlockHandler._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.spark.{SparkException, Logging, SparkConf}\n+import org.apache.spark.storage.{BlockManager, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{Clock, SystemClock, WriteAheadLogManager}\n+import org.apache.spark.util.Utils\n+\n+private[streaming] sealed trait ReceivedBlock\n+private[streaming] case class ArrayBufferBlock(arrayBuffer: ArrayBuffer[_]) extends ReceivedBlock\n+private[streaming] case class IteratorBlock(iterator: Iterator[_]) extends ReceivedBlock\n+private[streaming] case class ByteBufferBlock(byteBuffer: ByteBuffer) extends ReceivedBlock\n+\n+\n+/** Trait that represents a class that handles the storage of blocks received by receiver */\n+private[streaming] trait ReceivedBlockHandler {\n+\n+  /** Store a received block with the given block id */\n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef]"
  }, {
    "author": {
      "login": "harishreedharan"
    },
    "body": "+1. I agree. Adding type info makes things more complex than it needs to be.\n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-28T22:51:00Z",
    "diffHunk": "@@ -0,0 +1,144 @@\n+package org.apache.spark.streaming.receiver\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{Await, ExecutionContext, Future}\n+import scala.concurrent.duration._\n+import scala.language.{existentials, postfixOps}\n+\n+import WriteAheadLogBasedBlockHandler._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.spark.{SparkException, Logging, SparkConf}\n+import org.apache.spark.storage.{BlockManager, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{Clock, SystemClock, WriteAheadLogManager}\n+import org.apache.spark.util.Utils\n+\n+private[streaming] sealed trait ReceivedBlock\n+private[streaming] case class ArrayBufferBlock(arrayBuffer: ArrayBuffer[_]) extends ReceivedBlock\n+private[streaming] case class IteratorBlock(iterator: Iterator[_]) extends ReceivedBlock\n+private[streaming] case class ByteBufferBlock(byteBuffer: ByteBuffer) extends ReceivedBlock\n+\n+\n+/** Trait that represents a class that handles the storage of blocks received by receiver */\n+private[streaming] trait ReceivedBlockHandler {\n+\n+  /** Store a received block with the given block id */\n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef]"
  }, {
    "author": {
      "login": "pwendell"
    },
    "body": "Why not just do this then - it doesn't involve type traits and it's better than what's there now. Having `Option[Any]` as a return type in our codebase is really bad style.\n\n```\nclass StoredBlockInfo {\n  val blockId: String\n  val fileSegment: Option[FileSegment]\n}\n```\n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-29T03:29:12Z",
    "diffHunk": "@@ -0,0 +1,144 @@\n+package org.apache.spark.streaming.receiver\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{Await, ExecutionContext, Future}\n+import scala.concurrent.duration._\n+import scala.language.{existentials, postfixOps}\n+\n+import WriteAheadLogBasedBlockHandler._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.spark.{SparkException, Logging, SparkConf}\n+import org.apache.spark.storage.{BlockManager, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{Clock, SystemClock, WriteAheadLogManager}\n+import org.apache.spark.util.Utils\n+\n+private[streaming] sealed trait ReceivedBlock\n+private[streaming] case class ArrayBufferBlock(arrayBuffer: ArrayBuffer[_]) extends ReceivedBlock\n+private[streaming] case class IteratorBlock(iterator: Iterator[_]) extends ReceivedBlock\n+private[streaming] case class ByteBufferBlock(byteBuffer: ByteBuffer) extends ReceivedBlock\n+\n+\n+/** Trait that represents a class that handles the storage of blocks received by receiver */\n+private[streaming] trait ReceivedBlockHandler {\n+\n+  /** Store a received block with the given block id */\n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef]"
  }],
  "prId": 2940
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "What are the semantics of this if there is a failure... will it throw an exception?\n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-28T20:01:57Z",
    "diffHunk": "@@ -0,0 +1,144 @@\n+package org.apache.spark.streaming.receiver\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{Await, ExecutionContext, Future}\n+import scala.concurrent.duration._\n+import scala.language.{existentials, postfixOps}\n+\n+import WriteAheadLogBasedBlockHandler._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.spark.{SparkException, Logging, SparkConf}\n+import org.apache.spark.storage.{BlockManager, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{Clock, SystemClock, WriteAheadLogManager}\n+import org.apache.spark.util.Utils\n+\n+private[streaming] sealed trait ReceivedBlock\n+private[streaming] case class ArrayBufferBlock(arrayBuffer: ArrayBuffer[_]) extends ReceivedBlock\n+private[streaming] case class IteratorBlock(iterator: Iterator[_]) extends ReceivedBlock\n+private[streaming] case class ByteBufferBlock(byteBuffer: ByteBuffer) extends ReceivedBlock\n+\n+\n+/** Trait that represents a class that handles the storage of blocks received by receiver */\n+private[streaming] trait ReceivedBlockHandler {\n+\n+  /** Store a received block with the given block id */\n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef]\n+\n+  /** Cleanup old blocks older than the given threshold time */\n+  def cleanupOldBlock(threshTime: Long)\n+}\n+\n+/**\n+ * Implementation of a [[org.apache.spark.streaming.receiver.ReceivedBlockHandler]] which\n+ * stores the received blocks into a block manager with the specified storage level.\n+ */\n+private[streaming] class BlockManagerBasedBlockHandler(\n+    blockManager: BlockManager, storageLevel: StorageLevel)\n+  extends ReceivedBlockHandler with Logging {\n+  \n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef] = {\n+    val putResult = receivedBlock match {\n+      case ArrayBufferBlock(arrayBuffer) =>\n+        blockManager.putIterator(blockId, arrayBuffer.iterator, storageLevel, tellMaster = true)\n+      case IteratorBlock(iterator) =>\n+        blockManager.putIterator(blockId, iterator, storageLevel, tellMaster = true)\n+      case ByteBufferBlock(byteBuffer) =>\n+        blockManager.putBytes(blockId, byteBuffer, storageLevel, tellMaster = true)\n+      case _ =>\n+        throw new SparkException(s\"Could not store $blockId to block manager, unexpected block type\")\n+    }\n+    if (!putResult.map { _._1 }.contains(blockId)) {\n+      throw new SparkException(\n+        s\"Could not store $blockId to block manager with storage level $storageLevel\")\n+    }\n+    None\n+  }\n+\n+  def cleanupOldBlock(threshTime: Long) {\n+    // this is not used as blocks inserted into the BlockManager are cleared by DStream's clearing\n+    // of BlockRDDs.\n+  }\n+}\n+\n+/**\n+ * Implementation of a [[org.apache.spark.streaming.receiver.ReceivedBlockHandler]] which\n+ * stores the received blocks in both, a write ahead log and a block manager.\n+ */\n+private[streaming] class WriteAheadLogBasedBlockHandler(\n+    blockManager: BlockManager,\n+    streamId: Int,\n+    storageLevel: StorageLevel,\n+    conf: SparkConf,\n+    hadoopConf: Configuration,\n+    checkpointDir: String,\n+    clock: Clock = new SystemClock\n+  ) extends ReceivedBlockHandler with Logging {\n+\n+  private val blockStoreTimeout = conf.getInt(\n+    \"spark.streaming.receiver.blockStoreTimeout\", 30).seconds\n+  private val rollingInterval = conf.getInt(\n+    \"spark.streaming.receiver.writeAheadLog.rollingInterval\", 60)\n+  private val maxFailures = conf.getInt(\n+    \"spark.streaming.receiver.writeAheadLog.maxFailures\", 3)\n+\n+  private val logManager = new WriteAheadLogManager(\n+    checkpointDirToLogDir(checkpointDir, streamId),\n+    hadoopConf, rollingInterval, maxFailures,\n+    callerName = \"WriteAheadLogBasedBlockHandler\",\n+    clock = clock\n+  )\n+\n+  // For processing futures used in parallel block storing into block manager and write ahead log\n+  implicit private val executionContext = ExecutionContext.fromExecutorService(\n+    Utils.newDaemonFixedThreadPool(1, \"WriteAheadLogBasedBlockHandler\"))\n+  \n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef] = {\n+\n+    // Serialize the block so that it can be inserted into both\n+    val serializedBlock = receivedBlock match {\n+      case ArrayBufferBlock(arrayBuffer) =>\n+        blockManager.dataSerialize(blockId, arrayBuffer.iterator)\n+      case IteratorBlock(iterator) =>\n+        blockManager.dataSerialize(blockId, iterator)\n+      case ByteBufferBlock(byteBuffer) =>\n+        byteBuffer\n+      case _ =>\n+        throw new Exception(s\"Could not push $blockId to block manager, unexpected block type\")\n+    }\n+\n+    val pushToBlockManagerFuture = Future {\n+      val putResult =\n+        blockManager.putBytes(blockId, serializedBlock, storageLevel, tellMaster = true)\n+      if (!putResult.map { _._1 }.contains(blockId)) {\n+        throw new SparkException(\n+          s\"Could not store $blockId to block manager with storage level $storageLevel\")\n+      }\n+    }\n+    val pushToLogFuture = Future {\n+      logManager.writeToLog(serializedBlock)\n+    }\n+    val combinedFuture = for {\n+      _ <- pushToBlockManagerFuture\n+      fileSegment <- pushToLogFuture\n+    } yield fileSegment\n+\n+    Some(Await.result(combinedFuture, blockStoreTimeout))"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Await.result forwards the exception. The unit test actually tests that, by trying insert a too-large-block, which causes exception in lines 115-118 (right above).\n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-28T22:14:29Z",
    "diffHunk": "@@ -0,0 +1,144 @@\n+package org.apache.spark.streaming.receiver\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{Await, ExecutionContext, Future}\n+import scala.concurrent.duration._\n+import scala.language.{existentials, postfixOps}\n+\n+import WriteAheadLogBasedBlockHandler._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.spark.{SparkException, Logging, SparkConf}\n+import org.apache.spark.storage.{BlockManager, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{Clock, SystemClock, WriteAheadLogManager}\n+import org.apache.spark.util.Utils\n+\n+private[streaming] sealed trait ReceivedBlock\n+private[streaming] case class ArrayBufferBlock(arrayBuffer: ArrayBuffer[_]) extends ReceivedBlock\n+private[streaming] case class IteratorBlock(iterator: Iterator[_]) extends ReceivedBlock\n+private[streaming] case class ByteBufferBlock(byteBuffer: ByteBuffer) extends ReceivedBlock\n+\n+\n+/** Trait that represents a class that handles the storage of blocks received by receiver */\n+private[streaming] trait ReceivedBlockHandler {\n+\n+  /** Store a received block with the given block id */\n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef]\n+\n+  /** Cleanup old blocks older than the given threshold time */\n+  def cleanupOldBlock(threshTime: Long)\n+}\n+\n+/**\n+ * Implementation of a [[org.apache.spark.streaming.receiver.ReceivedBlockHandler]] which\n+ * stores the received blocks into a block manager with the specified storage level.\n+ */\n+private[streaming] class BlockManagerBasedBlockHandler(\n+    blockManager: BlockManager, storageLevel: StorageLevel)\n+  extends ReceivedBlockHandler with Logging {\n+  \n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef] = {\n+    val putResult = receivedBlock match {\n+      case ArrayBufferBlock(arrayBuffer) =>\n+        blockManager.putIterator(blockId, arrayBuffer.iterator, storageLevel, tellMaster = true)\n+      case IteratorBlock(iterator) =>\n+        blockManager.putIterator(blockId, iterator, storageLevel, tellMaster = true)\n+      case ByteBufferBlock(byteBuffer) =>\n+        blockManager.putBytes(blockId, byteBuffer, storageLevel, tellMaster = true)\n+      case _ =>\n+        throw new SparkException(s\"Could not store $blockId to block manager, unexpected block type\")\n+    }\n+    if (!putResult.map { _._1 }.contains(blockId)) {\n+      throw new SparkException(\n+        s\"Could not store $blockId to block manager with storage level $storageLevel\")\n+    }\n+    None\n+  }\n+\n+  def cleanupOldBlock(threshTime: Long) {\n+    // this is not used as blocks inserted into the BlockManager are cleared by DStream's clearing\n+    // of BlockRDDs.\n+  }\n+}\n+\n+/**\n+ * Implementation of a [[org.apache.spark.streaming.receiver.ReceivedBlockHandler]] which\n+ * stores the received blocks in both, a write ahead log and a block manager.\n+ */\n+private[streaming] class WriteAheadLogBasedBlockHandler(\n+    blockManager: BlockManager,\n+    streamId: Int,\n+    storageLevel: StorageLevel,\n+    conf: SparkConf,\n+    hadoopConf: Configuration,\n+    checkpointDir: String,\n+    clock: Clock = new SystemClock\n+  ) extends ReceivedBlockHandler with Logging {\n+\n+  private val blockStoreTimeout = conf.getInt(\n+    \"spark.streaming.receiver.blockStoreTimeout\", 30).seconds\n+  private val rollingInterval = conf.getInt(\n+    \"spark.streaming.receiver.writeAheadLog.rollingInterval\", 60)\n+  private val maxFailures = conf.getInt(\n+    \"spark.streaming.receiver.writeAheadLog.maxFailures\", 3)\n+\n+  private val logManager = new WriteAheadLogManager(\n+    checkpointDirToLogDir(checkpointDir, streamId),\n+    hadoopConf, rollingInterval, maxFailures,\n+    callerName = \"WriteAheadLogBasedBlockHandler\",\n+    clock = clock\n+  )\n+\n+  // For processing futures used in parallel block storing into block manager and write ahead log\n+  implicit private val executionContext = ExecutionContext.fromExecutorService(\n+    Utils.newDaemonFixedThreadPool(1, \"WriteAheadLogBasedBlockHandler\"))\n+  \n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef] = {\n+\n+    // Serialize the block so that it can be inserted into both\n+    val serializedBlock = receivedBlock match {\n+      case ArrayBufferBlock(arrayBuffer) =>\n+        blockManager.dataSerialize(blockId, arrayBuffer.iterator)\n+      case IteratorBlock(iterator) =>\n+        blockManager.dataSerialize(blockId, iterator)\n+      case ByteBufferBlock(byteBuffer) =>\n+        byteBuffer\n+      case _ =>\n+        throw new Exception(s\"Could not push $blockId to block manager, unexpected block type\")\n+    }\n+\n+    val pushToBlockManagerFuture = Future {\n+      val putResult =\n+        blockManager.putBytes(blockId, serializedBlock, storageLevel, tellMaster = true)\n+      if (!putResult.map { _._1 }.contains(blockId)) {\n+        throw new SparkException(\n+          s\"Could not store $blockId to block manager with storage level $storageLevel\")\n+      }\n+    }\n+    val pushToLogFuture = Future {\n+      logManager.writeToLog(serializedBlock)\n+    }\n+    val combinedFuture = for {\n+      _ <- pushToBlockManagerFuture\n+      fileSegment <- pushToLogFuture\n+    } yield fileSegment\n+\n+    Some(Await.result(combinedFuture, blockStoreTimeout))"
  }],
  "prId": 2940
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "For these types of \"unexpected case\" errors, I find it helpful to log the name of the unexpected object's class, e.g.\n\n``` scala\ncase o =>\n  throw new SparkException(s\"... unexpected block type ${o.getClass.getName}\")\n```\n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-28T20:08:09Z",
    "diffHunk": "@@ -0,0 +1,144 @@\n+package org.apache.spark.streaming.receiver\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{Await, ExecutionContext, Future}\n+import scala.concurrent.duration._\n+import scala.language.{existentials, postfixOps}\n+\n+import WriteAheadLogBasedBlockHandler._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.spark.{SparkException, Logging, SparkConf}\n+import org.apache.spark.storage.{BlockManager, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{Clock, SystemClock, WriteAheadLogManager}\n+import org.apache.spark.util.Utils\n+\n+private[streaming] sealed trait ReceivedBlock\n+private[streaming] case class ArrayBufferBlock(arrayBuffer: ArrayBuffer[_]) extends ReceivedBlock\n+private[streaming] case class IteratorBlock(iterator: Iterator[_]) extends ReceivedBlock\n+private[streaming] case class ByteBufferBlock(byteBuffer: ByteBuffer) extends ReceivedBlock\n+\n+\n+/** Trait that represents a class that handles the storage of blocks received by receiver */\n+private[streaming] trait ReceivedBlockHandler {\n+\n+  /** Store a received block with the given block id */\n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef]\n+\n+  /** Cleanup old blocks older than the given threshold time */\n+  def cleanupOldBlock(threshTime: Long)\n+}\n+\n+/**\n+ * Implementation of a [[org.apache.spark.streaming.receiver.ReceivedBlockHandler]] which\n+ * stores the received blocks into a block manager with the specified storage level.\n+ */\n+private[streaming] class BlockManagerBasedBlockHandler(\n+    blockManager: BlockManager, storageLevel: StorageLevel)\n+  extends ReceivedBlockHandler with Logging {\n+  \n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef] = {\n+    val putResult = receivedBlock match {\n+      case ArrayBufferBlock(arrayBuffer) =>\n+        blockManager.putIterator(blockId, arrayBuffer.iterator, storageLevel, tellMaster = true)\n+      case IteratorBlock(iterator) =>\n+        blockManager.putIterator(blockId, iterator, storageLevel, tellMaster = true)\n+      case ByteBufferBlock(byteBuffer) =>\n+        blockManager.putBytes(blockId, byteBuffer, storageLevel, tellMaster = true)\n+      case _ =>\n+        throw new SparkException(s\"Could not store $blockId to block manager, unexpected block type\")"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Good idea. I do too, missed it here.\n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-28T22:15:21Z",
    "diffHunk": "@@ -0,0 +1,144 @@\n+package org.apache.spark.streaming.receiver\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{Await, ExecutionContext, Future}\n+import scala.concurrent.duration._\n+import scala.language.{existentials, postfixOps}\n+\n+import WriteAheadLogBasedBlockHandler._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.spark.{SparkException, Logging, SparkConf}\n+import org.apache.spark.storage.{BlockManager, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{Clock, SystemClock, WriteAheadLogManager}\n+import org.apache.spark.util.Utils\n+\n+private[streaming] sealed trait ReceivedBlock\n+private[streaming] case class ArrayBufferBlock(arrayBuffer: ArrayBuffer[_]) extends ReceivedBlock\n+private[streaming] case class IteratorBlock(iterator: Iterator[_]) extends ReceivedBlock\n+private[streaming] case class ByteBufferBlock(byteBuffer: ByteBuffer) extends ReceivedBlock\n+\n+\n+/** Trait that represents a class that handles the storage of blocks received by receiver */\n+private[streaming] trait ReceivedBlockHandler {\n+\n+  /** Store a received block with the given block id */\n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef]\n+\n+  /** Cleanup old blocks older than the given threshold time */\n+  def cleanupOldBlock(threshTime: Long)\n+}\n+\n+/**\n+ * Implementation of a [[org.apache.spark.streaming.receiver.ReceivedBlockHandler]] which\n+ * stores the received blocks into a block manager with the specified storage level.\n+ */\n+private[streaming] class BlockManagerBasedBlockHandler(\n+    blockManager: BlockManager, storageLevel: StorageLevel)\n+  extends ReceivedBlockHandler with Logging {\n+  \n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef] = {\n+    val putResult = receivedBlock match {\n+      case ArrayBufferBlock(arrayBuffer) =>\n+        blockManager.putIterator(blockId, arrayBuffer.iterator, storageLevel, tellMaster = true)\n+      case IteratorBlock(iterator) =>\n+        blockManager.putIterator(blockId, iterator, storageLevel, tellMaster = true)\n+      case ByteBufferBlock(byteBuffer) =>\n+        blockManager.putBytes(blockId, byteBuffer, storageLevel, tellMaster = true)\n+      case _ =>\n+        throw new SparkException(s\"Could not store $blockId to block manager, unexpected block type\")"
  }],
  "prId": 2940
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Can you add an explicit type annotation here?\n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-28T20:09:27Z",
    "diffHunk": "@@ -0,0 +1,144 @@\n+package org.apache.spark.streaming.receiver\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{Await, ExecutionContext, Future}\n+import scala.concurrent.duration._\n+import scala.language.{existentials, postfixOps}\n+\n+import WriteAheadLogBasedBlockHandler._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.spark.{SparkException, Logging, SparkConf}\n+import org.apache.spark.storage.{BlockManager, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{Clock, SystemClock, WriteAheadLogManager}\n+import org.apache.spark.util.Utils\n+\n+private[streaming] sealed trait ReceivedBlock\n+private[streaming] case class ArrayBufferBlock(arrayBuffer: ArrayBuffer[_]) extends ReceivedBlock\n+private[streaming] case class IteratorBlock(iterator: Iterator[_]) extends ReceivedBlock\n+private[streaming] case class ByteBufferBlock(byteBuffer: ByteBuffer) extends ReceivedBlock\n+\n+\n+/** Trait that represents a class that handles the storage of blocks received by receiver */\n+private[streaming] trait ReceivedBlockHandler {\n+\n+  /** Store a received block with the given block id */\n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef]\n+\n+  /** Cleanup old blocks older than the given threshold time */\n+  def cleanupOldBlock(threshTime: Long)\n+}\n+\n+/**\n+ * Implementation of a [[org.apache.spark.streaming.receiver.ReceivedBlockHandler]] which\n+ * stores the received blocks into a block manager with the specified storage level.\n+ */\n+private[streaming] class BlockManagerBasedBlockHandler(\n+    blockManager: BlockManager, storageLevel: StorageLevel)\n+  extends ReceivedBlockHandler with Logging {\n+  \n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef] = {\n+    val putResult = receivedBlock match {"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Done. Yeah, the return type of `blockManager.putIterator` is complicated and is best shown explicitly for better code readability.\n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-28T22:25:09Z",
    "diffHunk": "@@ -0,0 +1,144 @@\n+package org.apache.spark.streaming.receiver\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{Await, ExecutionContext, Future}\n+import scala.concurrent.duration._\n+import scala.language.{existentials, postfixOps}\n+\n+import WriteAheadLogBasedBlockHandler._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.spark.{SparkException, Logging, SparkConf}\n+import org.apache.spark.storage.{BlockManager, StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.util.{Clock, SystemClock, WriteAheadLogManager}\n+import org.apache.spark.util.Utils\n+\n+private[streaming] sealed trait ReceivedBlock\n+private[streaming] case class ArrayBufferBlock(arrayBuffer: ArrayBuffer[_]) extends ReceivedBlock\n+private[streaming] case class IteratorBlock(iterator: Iterator[_]) extends ReceivedBlock\n+private[streaming] case class ByteBufferBlock(byteBuffer: ByteBuffer) extends ReceivedBlock\n+\n+\n+/** Trait that represents a class that handles the storage of blocks received by receiver */\n+private[streaming] trait ReceivedBlockHandler {\n+\n+  /** Store a received block with the given block id */\n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef]\n+\n+  /** Cleanup old blocks older than the given threshold time */\n+  def cleanupOldBlock(threshTime: Long)\n+}\n+\n+/**\n+ * Implementation of a [[org.apache.spark.streaming.receiver.ReceivedBlockHandler]] which\n+ * stores the received blocks into a block manager with the specified storage level.\n+ */\n+private[streaming] class BlockManagerBasedBlockHandler(\n+    blockManager: BlockManager, storageLevel: StorageLevel)\n+  extends ReceivedBlockHandler with Logging {\n+  \n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[AnyRef] = {\n+    val putResult = receivedBlock match {"
  }],
  "prId": 2940
}, {
  "comments": [{
    "author": {
      "login": "harishreedharan"
    },
    "body": "As I mentioned earlier, this might actually end up being a bottle neck. Since you could write using multiple threads in the same receiver - we are basically blocking more than one write from happening at any point in time. Since the BlockManager can handle more writes in parallel, we should probably use a much higher value than 2.\n\nThat said, the WAL Writer would still be a bottle neck - since the writes to the WAL have to be synchronized. So I am not entirely sure if having more than 2 threads helps a whole lot.\n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-29T00:52:30Z",
    "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.receiver\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{Await, ExecutionContext, Future}\n+import scala.concurrent.duration._\n+import scala.language.{existentials, postfixOps}\n+\n+import WriteAheadLogBasedBlockHandler._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.{Logging, SparkConf, SparkException}\n+import org.apache.spark.storage._\n+import org.apache.spark.streaming.util.{Clock, SystemClock, WriteAheadLogManager}\n+import org.apache.spark.util.Utils\n+\n+private[streaming] sealed trait ReceivedBlock\n+private[streaming] case class ArrayBufferBlock(arrayBuffer: ArrayBuffer[_]) extends ReceivedBlock\n+private[streaming] case class IteratorBlock(iterator: Iterator[_]) extends ReceivedBlock\n+private[streaming] case class ByteBufferBlock(byteBuffer: ByteBuffer) extends ReceivedBlock\n+\n+\n+/** Trait that represents a class that handles the storage of blocks received by receiver */\n+private[streaming] trait ReceivedBlockHandler {\n+\n+  /** Store a received block with the given block id */\n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[Any]\n+\n+  /** Cleanup old blocks older than the given threshold time */\n+  def cleanupOldBlock(threshTime: Long)\n+}\n+\n+/**\n+ * Implementation of a [[org.apache.spark.streaming.receiver.ReceivedBlockHandler]] which\n+ * stores the received blocks into a block manager with the specified storage level.\n+ */\n+private[streaming] class BlockManagerBasedBlockHandler(\n+    blockManager: BlockManager, storageLevel: StorageLevel)\n+  extends ReceivedBlockHandler with Logging {\n+  \n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[Any] = {\n+    val putResult: Seq[(BlockId, BlockStatus)] = receivedBlock match {\n+      case ArrayBufferBlock(arrayBuffer) =>\n+        blockManager.putIterator(blockId, arrayBuffer.iterator, storageLevel, tellMaster = true)\n+      case IteratorBlock(iterator) =>\n+        blockManager.putIterator(blockId, iterator, storageLevel, tellMaster = true)\n+      case ByteBufferBlock(byteBuffer) =>\n+        blockManager.putBytes(blockId, byteBuffer, storageLevel, tellMaster = true)\n+      case o =>\n+        throw new SparkException(\n+          s\"Could not store $blockId to block manager, unexpected block type ${o.getClass.getName}\")\n+    }\n+    if (!putResult.map { _._1 }.contains(blockId)) {\n+      throw new SparkException(\n+        s\"Could not store $blockId to block manager with storage level $storageLevel\")\n+    }\n+    None\n+  }\n+\n+  def cleanupOldBlock(threshTime: Long) {\n+    // this is not used as blocks inserted into the BlockManager are cleared by DStream's clearing\n+    // of BlockRDDs.\n+  }\n+}\n+\n+/**\n+ * Implementation of a [[org.apache.spark.streaming.receiver.ReceivedBlockHandler]] which\n+ * stores the received blocks in both, a write ahead log and a block manager.\n+ */\n+private[streaming] class WriteAheadLogBasedBlockHandler(\n+    blockManager: BlockManager,\n+    streamId: Int,\n+    storageLevel: StorageLevel,\n+    conf: SparkConf,\n+    hadoopConf: Configuration,\n+    checkpointDir: String,\n+    clock: Clock = new SystemClock\n+  ) extends ReceivedBlockHandler with Logging {\n+\n+  private val blockStoreTimeout = conf.getInt(\n+    \"spark.streaming.receiver.blockStoreTimeout\", 30).seconds\n+  private val rollingInterval = conf.getInt(\n+    \"spark.streaming.receiver.writeAheadLog.rollingInterval\", 60)\n+  private val maxFailures = conf.getInt(\n+    \"spark.streaming.receiver.writeAheadLog.maxFailures\", 3)\n+\n+  // Manages rolling log files\n+  private val logManager = new WriteAheadLogManager(\n+    checkpointDirToLogDir(checkpointDir, streamId),\n+    hadoopConf, rollingInterval, maxFailures,\n+    callerName = this.getClass.getSimpleName,\n+    clock = clock\n+  )\n+\n+  // For processing futures used in parallel block storing into block manager and write ahead log\n+  implicit private val executionContext = ExecutionContext.fromExecutorService(\n+    Utils.newDaemonFixedThreadPool(2, this.getClass.getSimpleName))"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "There is a good scope of future optimization here. We can always create multiple WALManagers and write in parallel to multiple WALs. That would improve performance depending on where the bottle neck is. \n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-29T01:17:16Z",
    "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.receiver\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{Await, ExecutionContext, Future}\n+import scala.concurrent.duration._\n+import scala.language.{existentials, postfixOps}\n+\n+import WriteAheadLogBasedBlockHandler._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.{Logging, SparkConf, SparkException}\n+import org.apache.spark.storage._\n+import org.apache.spark.streaming.util.{Clock, SystemClock, WriteAheadLogManager}\n+import org.apache.spark.util.Utils\n+\n+private[streaming] sealed trait ReceivedBlock\n+private[streaming] case class ArrayBufferBlock(arrayBuffer: ArrayBuffer[_]) extends ReceivedBlock\n+private[streaming] case class IteratorBlock(iterator: Iterator[_]) extends ReceivedBlock\n+private[streaming] case class ByteBufferBlock(byteBuffer: ByteBuffer) extends ReceivedBlock\n+\n+\n+/** Trait that represents a class that handles the storage of blocks received by receiver */\n+private[streaming] trait ReceivedBlockHandler {\n+\n+  /** Store a received block with the given block id */\n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[Any]\n+\n+  /** Cleanup old blocks older than the given threshold time */\n+  def cleanupOldBlock(threshTime: Long)\n+}\n+\n+/**\n+ * Implementation of a [[org.apache.spark.streaming.receiver.ReceivedBlockHandler]] which\n+ * stores the received blocks into a block manager with the specified storage level.\n+ */\n+private[streaming] class BlockManagerBasedBlockHandler(\n+    blockManager: BlockManager, storageLevel: StorageLevel)\n+  extends ReceivedBlockHandler with Logging {\n+  \n+  def storeBlock(blockId: StreamBlockId, receivedBlock: ReceivedBlock): Option[Any] = {\n+    val putResult: Seq[(BlockId, BlockStatus)] = receivedBlock match {\n+      case ArrayBufferBlock(arrayBuffer) =>\n+        blockManager.putIterator(blockId, arrayBuffer.iterator, storageLevel, tellMaster = true)\n+      case IteratorBlock(iterator) =>\n+        blockManager.putIterator(blockId, iterator, storageLevel, tellMaster = true)\n+      case ByteBufferBlock(byteBuffer) =>\n+        blockManager.putBytes(blockId, byteBuffer, storageLevel, tellMaster = true)\n+      case o =>\n+        throw new SparkException(\n+          s\"Could not store $blockId to block manager, unexpected block type ${o.getClass.getName}\")\n+    }\n+    if (!putResult.map { _._1 }.contains(blockId)) {\n+      throw new SparkException(\n+        s\"Could not store $blockId to block manager with storage level $storageLevel\")\n+    }\n+    None\n+  }\n+\n+  def cleanupOldBlock(threshTime: Long) {\n+    // this is not used as blocks inserted into the BlockManager are cleared by DStream's clearing\n+    // of BlockRDDs.\n+  }\n+}\n+\n+/**\n+ * Implementation of a [[org.apache.spark.streaming.receiver.ReceivedBlockHandler]] which\n+ * stores the received blocks in both, a write ahead log and a block manager.\n+ */\n+private[streaming] class WriteAheadLogBasedBlockHandler(\n+    blockManager: BlockManager,\n+    streamId: Int,\n+    storageLevel: StorageLevel,\n+    conf: SparkConf,\n+    hadoopConf: Configuration,\n+    checkpointDir: String,\n+    clock: Clock = new SystemClock\n+  ) extends ReceivedBlockHandler with Logging {\n+\n+  private val blockStoreTimeout = conf.getInt(\n+    \"spark.streaming.receiver.blockStoreTimeout\", 30).seconds\n+  private val rollingInterval = conf.getInt(\n+    \"spark.streaming.receiver.writeAheadLog.rollingInterval\", 60)\n+  private val maxFailures = conf.getInt(\n+    \"spark.streaming.receiver.writeAheadLog.maxFailures\", 3)\n+\n+  // Manages rolling log files\n+  private val logManager = new WriteAheadLogManager(\n+    checkpointDirToLogDir(checkpointDir, streamId),\n+    hadoopConf, rollingInterval, maxFailures,\n+    callerName = this.getClass.getSimpleName,\n+    clock = clock\n+  )\n+\n+  // For processing futures used in parallel block storing into block manager and write ahead log\n+  implicit private val executionContext = ExecutionContext.fromExecutorService(\n+    Utils.newDaemonFixedThreadPool(2, this.getClass.getSimpleName))"
  }],
  "prId": 2940
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Given that the `Option[Any]` has provoked so much discussion, maybe we should document the return type in this scaladoc (e.g. say that it's arbitrary metadata or something); currently, it's not clear what's being returned.\n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-29T16:31:40Z",
    "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.receiver\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{Await, ExecutionContext, Future}\n+import scala.concurrent.duration._\n+import scala.language.{existentials, postfixOps}\n+\n+import WriteAheadLogBasedBlockHandler._\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.{Logging, SparkConf, SparkException}\n+import org.apache.spark.storage._\n+import org.apache.spark.streaming.util.{Clock, SystemClock, WriteAheadLogManager}\n+import org.apache.spark.util.Utils\n+\n+private[streaming] sealed trait ReceivedBlock\n+private[streaming] case class ArrayBufferBlock(arrayBuffer: ArrayBuffer[_]) extends ReceivedBlock\n+private[streaming] case class IteratorBlock(iterator: Iterator[_]) extends ReceivedBlock\n+private[streaming] case class ByteBufferBlock(byteBuffer: ByteBuffer) extends ReceivedBlock\n+\n+\n+/** Trait that represents a class that handles the storage of blocks received by receiver */\n+private[streaming] trait ReceivedBlockHandler {\n+\n+  /** Store a received block with the given block id */"
  }],
  "prId": 2940
}]