[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "WIP: this file is going to be updated by @harishreedharan to get rid of the local file customizations.\n",
    "commit": "e4bee2065293d7373c43fe5636dd9971dede257e",
    "createdAt": "2014-10-21T22:46:47Z",
    "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.util\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.ByteBuffer\n+\n+import scala.util.Try\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FSDataOutputStream, FileSystem}\n+\n+/**\n+ * A writer for writing byte-buffers to a write ahead log file.\n+ */\n+private[streaming] class WriteAheadLogWriter(path: String, hadoopConf: Configuration)\n+  extends Closeable {\n+  private val underlyingStream: Either[DataOutputStream, FSDataOutputStream] = {"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "Ah, that makes sense.  I guess you can still use the HDFS API to write to local files for testing purposes.\n",
    "commit": "e4bee2065293d7373c43fe5636dd9971dede257e",
    "createdAt": "2014-10-22T04:28:17Z",
    "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.util\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.ByteBuffer\n+\n+import scala.util.Try\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FSDataOutputStream, FileSystem}\n+\n+/**\n+ * A writer for writing byte-buffers to a write ahead log file.\n+ */\n+private[streaming] class WriteAheadLogWriter(path: String, hadoopConf: Configuration)\n+  extends Closeable {\n+  private val underlyingStream: Either[DataOutputStream, FSDataOutputStream] = {"
  }, {
    "author": {
      "login": "harishreedharan"
    },
    "body": "Yep. And for all tests, we are just going to use Hadoop Minicluster anyway.\n",
    "commit": "e4bee2065293d7373c43fe5636dd9971dede257e",
    "createdAt": "2014-10-22T05:42:31Z",
    "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.util\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.ByteBuffer\n+\n+import scala.util.Try\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FSDataOutputStream, FileSystem}\n+\n+/**\n+ * A writer for writing byte-buffers to a write ahead log file.\n+ */\n+private[streaming] class WriteAheadLogWriter(path: String, hadoopConf: Configuration)\n+  extends Closeable {\n+  private val underlyingStream: Either[DataOutputStream, FSDataOutputStream] = {"
  }],
  "prId": 2882
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Nice Scala one-liner :)\n\nWhy do we need this reflection, though?  Is this necessary to support multiple Hadoop versions?  If so, could you add a one-line comment to explain this?\n",
    "commit": "e4bee2065293d7373c43fe5636dd9971dede257e",
    "createdAt": "2014-10-22T05:33:07Z",
    "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.util\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.ByteBuffer\n+\n+import scala.util.Try\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FSDataOutputStream, FileSystem}\n+\n+/**\n+ * A writer for writing byte-buffers to a write ahead log file.\n+ */\n+private[streaming] class WriteAheadLogWriter(path: String, hadoopConf: Configuration)\n+  extends Closeable {\n+  private val underlyingStream: Either[DataOutputStream, FSDataOutputStream] = {\n+    val uri = new URI(path)\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+\n+    if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+      assert(!new File(uri.getPath).exists)\n+      Left(new DataOutputStream(new BufferedOutputStream(new FileOutputStream(uri.getPath))))\n+    } else {\n+      Right(HdfsUtils.getOutputStream(path, hadoopConf))\n+    }\n+  }\n+\n+  private lazy val hadoopFlushMethod = {\n+    val cls = classOf[FSDataOutputStream]\n+    Try(cls.getMethod(\"hflush\")).orElse(Try(cls.getMethod(\"sync\"))).toOption"
  }, {
    "author": {
      "login": "harishreedharan"
    },
    "body": "Actually we do, since Spark supports Hadoop 1 to Hadoop 2.5.0 right now. In Hadoop 1.x, the \"sync\" method did the same thing hflush does in 2.5.0 - so in short we do.\n",
    "commit": "e4bee2065293d7373c43fe5636dd9971dede257e",
    "createdAt": "2014-10-22T05:43:52Z",
    "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.util\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.ByteBuffer\n+\n+import scala.util.Try\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FSDataOutputStream, FileSystem}\n+\n+/**\n+ * A writer for writing byte-buffers to a write ahead log file.\n+ */\n+private[streaming] class WriteAheadLogWriter(path: String, hadoopConf: Configuration)\n+  extends Closeable {\n+  private val underlyingStream: Either[DataOutputStream, FSDataOutputStream] = {\n+    val uri = new URI(path)\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+\n+    if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+      assert(!new File(uri.getPath).exists)\n+      Left(new DataOutputStream(new BufferedOutputStream(new FileOutputStream(uri.getPath))))\n+    } else {\n+      Right(HdfsUtils.getOutputStream(path, hadoopConf))\n+    }\n+  }\n+\n+  private lazy val hadoopFlushMethod = {\n+    val cls = classOf[FSDataOutputStream]\n+    Try(cls.getMethod(\"hflush\")).orElse(Try(cls.getMethod(\"sync\"))).toOption"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Credit goes to Colin McCabe who wrote this line. \nhttps://github.com/apache/spark/blame/master/core/src/main/scala/org/apache/spark/util/FileLogger.scala#L106\nStole from there.\n",
    "commit": "e4bee2065293d7373c43fe5636dd9971dede257e",
    "createdAt": "2014-10-22T07:33:33Z",
    "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.util\n+\n+import java.io._\n+import java.net.URI\n+import java.nio.ByteBuffer\n+\n+import scala.util.Try\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FSDataOutputStream, FileSystem}\n+\n+/**\n+ * A writer for writing byte-buffers to a write ahead log file.\n+ */\n+private[streaming] class WriteAheadLogWriter(path: String, hadoopConf: Configuration)\n+  extends Closeable {\n+  private val underlyingStream: Either[DataOutputStream, FSDataOutputStream] = {\n+    val uri = new URI(path)\n+    val defaultFs = FileSystem.getDefaultUri(hadoopConf).getScheme\n+    val isDefaultLocal = defaultFs == null || defaultFs == \"file\"\n+\n+    if ((isDefaultLocal && uri.getScheme == null) || uri.getScheme == \"file\") {\n+      assert(!new File(uri.getPath).exists)\n+      Left(new DataOutputStream(new BufferedOutputStream(new FileOutputStream(uri.getPath))))\n+    } else {\n+      Right(HdfsUtils.getOutputStream(path, hadoopConf))\n+    }\n+  }\n+\n+  private lazy val hadoopFlushMethod = {\n+    val cls = classOf[FSDataOutputStream]\n+    Try(cls.getMethod(\"hflush\")).orElse(Try(cls.getMethod(\"sync\"))).toOption"
  }],
  "prId": 2882
}]