[{
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "A bit off topic (and we can deal with this later) - but should we make the checkpoint directory into a `sparkConf` setting? That way we could do this type of validation earlier on. Right now unfortunately we can't distinguish here whether the user didn't call `checkpoint` or whether there was just a bug somewhere in Spark code.\n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-28T19:31:58Z",
    "diffHunk": "@@ -44,12 +41,26 @@ import org.apache.spark.streaming.scheduler.RegisterReceiver\n  */\n private[streaming] class ReceiverSupervisorImpl(\n     receiver: Receiver[_],\n-    env: SparkEnv\n+    env: SparkEnv,\n+    hadoopConf: Configuration,\n+    checkpointDirOption: Option[String]\n   ) extends ReceiverSupervisor(receiver, env.conf) with Logging {\n \n-  private val blockManager = env.blockManager\n+  private val receivedBlockHandler: ReceivedBlockHandler = {\n+    if (env.conf.getBoolean(\"spark.streaming.receiver.writeAheadLog.enable\", false)) {\n+      if (checkpointDirOption.isEmpty) {\n+        throw new SparkException(\n+          \"Cannot enable receiver write-ahead log without checkpoint directory set. \" +",
    "line": 37
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Good point. This is something that requires changes at both spark as well as spark streaming level, and probably further discussed, and hence deferred to the next release deadline.\n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-28T21:46:27Z",
    "diffHunk": "@@ -44,12 +41,26 @@ import org.apache.spark.streaming.scheduler.RegisterReceiver\n  */\n private[streaming] class ReceiverSupervisorImpl(\n     receiver: Receiver[_],\n-    env: SparkEnv\n+    env: SparkEnv,\n+    hadoopConf: Configuration,\n+    checkpointDirOption: Option[String]\n   ) extends ReceiverSupervisor(receiver, env.conf) with Logging {\n \n-  private val blockManager = env.blockManager\n+  private val receivedBlockHandler: ReceivedBlockHandler = {\n+    if (env.conf.getBoolean(\"spark.streaming.receiver.writeAheadLog.enable\", false)) {\n+      if (checkpointDirOption.isEmpty) {\n+        throw new SparkException(\n+          \"Cannot enable receiver write-ahead log without checkpoint directory set. \" +",
    "line": 37
  }],
  "prId": 2940
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "In spark we usually don't add \"Option\" to variable names that are options. You just know it from the type. This is actually not even consistent with this file (optionalMetadata vs fileSegmentOption). I'd prefer to just have it be named `fileSegment`.\n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-28T19:58:32Z",
    "diffHunk": "@@ -134,17 +137,32 @@ private[streaming] class ReceiverSupervisorImpl(\n       optionalMetadata: Option[Any],\n       optionalBlockId: Option[StreamBlockId]\n     ) {\n+    pushAndReportBlock(ByteBufferBlock(bytes), optionalMetadata, optionalBlockId)\n+  }\n+\n+  /** Store block and report it to driver */\n+  def pushAndReportBlock(\n+      receivedBlock: ReceivedBlock,\n+      optionalMetadata: Option[Any],\n+      optionalBlockId: Option[StreamBlockId]\n+    ) {\n     val blockId = optionalBlockId.getOrElse(nextBlockId)\n+    val numRecords = receivedBlock match {\n+      case ArrayBufferBlock(arrayBuffer) => arrayBuffer.size\n+      case _ => -1\n+    }\n+\n     val time = System.currentTimeMillis\n-    blockManager.putBytes(blockId, bytes, storageLevel, tellMaster = true)\n-    logDebug(\"Pushed block \" + blockId + \" in \" + (System.currentTimeMillis - time)  + \" ms\")\n-    reportPushedBlock(blockId, -1, optionalMetadata)\n-  }\n+    val fileSegmentOption = receivedBlockHandler.storeBlock(blockId, receivedBlock) match {"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Personally, I specify Option in the variable name, because it because it make it easier to read code like `somethingOption.getOrElse`.  Otherwise, if I see `something.getOrElse` I have to track back and see whether `something` is a HashMap or Option. And errors like not calling `get` or `getOrElse` when the actual value is necessary become more immediately clear.\n\nAgain, this is a not a very strong logic, just a personal preference of mine.\n\nRegarding consistent naming, if that code is within the scope of the lines that I touch in the PR, I will fix it.\n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-28T22:13:06Z",
    "diffHunk": "@@ -134,17 +137,32 @@ private[streaming] class ReceiverSupervisorImpl(\n       optionalMetadata: Option[Any],\n       optionalBlockId: Option[StreamBlockId]\n     ) {\n+    pushAndReportBlock(ByteBufferBlock(bytes), optionalMetadata, optionalBlockId)\n+  }\n+\n+  /** Store block and report it to driver */\n+  def pushAndReportBlock(\n+      receivedBlock: ReceivedBlock,\n+      optionalMetadata: Option[Any],\n+      optionalBlockId: Option[StreamBlockId]\n+    ) {\n     val blockId = optionalBlockId.getOrElse(nextBlockId)\n+    val numRecords = receivedBlock match {\n+      case ArrayBufferBlock(arrayBuffer) => arrayBuffer.size\n+      case _ => -1\n+    }\n+\n     val time = System.currentTimeMillis\n-    blockManager.putBytes(blockId, bytes, storageLevel, tellMaster = true)\n-    logDebug(\"Pushed block \" + blockId + \" in \" + (System.currentTimeMillis - time)  + \" ms\")\n-    reportPushedBlock(blockId, -1, optionalMetadata)\n-  }\n+    val fileSegmentOption = receivedBlockHandler.storeBlock(blockId, receivedBlock) match {"
  }],
  "prId": 2940
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "I think we might want a blank line here to separate the Spark imports from the third-party libraries.\n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-28T20:11:12Z",
    "diffHunk": "@@ -25,16 +25,13 @@ import scala.concurrent.Await\n \n import akka.actor.{Actor, Props}\n import akka.pattern.ask\n-\n import com.google.common.base.Throwables\n-\n-import org.apache.spark.{Logging, SparkEnv}\n-import org.apache.spark.streaming.scheduler._\n-import org.apache.spark.util.{Utils, AkkaUtils}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.{Logging, SparkEnv, SparkException}",
    "line": 11
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Done. Somehow the import sorter is goofing up. \n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-28T22:26:08Z",
    "diffHunk": "@@ -25,16 +25,13 @@ import scala.concurrent.Await\n \n import akka.actor.{Actor, Props}\n import akka.pattern.ask\n-\n import com.google.common.base.Throwables\n-\n-import org.apache.spark.{Logging, SparkEnv}\n-import org.apache.spark.streaming.scheduler._\n-import org.apache.spark.util.{Utils, AkkaUtils}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.{Logging, SparkEnv, SparkException}",
    "line": 11
  }],
  "prId": 2940
}, {
  "comments": [{
    "author": {
      "login": "harishreedharan"
    },
    "body": "@pwendell mentioned that we don't usually add opt/Option in the variable name, since that is implied from the type. Rename this to checkpointDir?\n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-28T23:01:13Z",
    "diffHunk": "@@ -44,12 +41,26 @@ import org.apache.spark.streaming.scheduler.RegisterReceiver\n  */\n private[streaming] class ReceiverSupervisorImpl(\n     receiver: Receiver[_],\n-    env: SparkEnv\n+    env: SparkEnv,\n+    hadoopConf: Configuration,\n+    checkpointDirOption: Option[String]",
    "line": 29
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "See my comment on that thread, on my preference to have Option in the name. Also spoke to @pwendell offline, and he does not seem to have a strong opinion on this.\n",
    "commit": "78a4aaa21f15f8c8a8ff4a5de1aa5f9562ba96ef",
    "createdAt": "2014-10-28T23:02:56Z",
    "diffHunk": "@@ -44,12 +41,26 @@ import org.apache.spark.streaming.scheduler.RegisterReceiver\n  */\n private[streaming] class ReceiverSupervisorImpl(\n     receiver: Receiver[_],\n-    env: SparkEnv\n+    env: SparkEnv,\n+    hadoopConf: Configuration,\n+    checkpointDirOption: Option[String]",
    "line": 29
  }],
  "prId": 2940
}]