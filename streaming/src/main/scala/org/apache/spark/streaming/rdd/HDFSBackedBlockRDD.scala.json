[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Can you explain how this code gets the block locations of the segment of the file that the partition needs? The offsets dont seem to be passed on to the HDFSUtils.getBlockLocations\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-25T01:00:37Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+  val blockId: BlockId, idx: Int, val segment: WriteAheadLogFileSegment) extends Partition {\n+  val index = idx\n+}\n+\n+private[streaming]\n+class HDFSBackedBlockRDD[T: ClassTag](\n+    @transient sc: SparkContext,\n+    @transient hadoopConfiguration: Configuration,\n+    @transient override val blockIds: Array[BlockId],\n+    @transient val segments: Array[WriteAheadLogFileSegment],\n+    val storeInBlockManager: Boolean,\n+    val storageLevel: StorageLevel\n+  ) extends BlockRDD[T](sc, blockIds) {\n+\n+  if (blockIds.length != segments.length) {\n+    throw new IllegalStateException(\"Number of block ids must be the same as number of segments!\")\n+  }\n+\n+  // Hadoop Configuration is not serializable, so broadcast it as a serializable.\n+  val broadcastedHadoopConf = sc.broadcast(new SerializableWritable(hadoopConfiguration))\n+    .asInstanceOf[Broadcast[SerializableWritable[Configuration]]]\n+  override def getPartitions: Array[Partition] = {\n+    assertValid()\n+    (0 until blockIds.size).map { i =>\n+      new HDFSBackedBlockRDDPartition(blockIds(i), i, segments(i))\n+    }.toArray\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[T] = {\n+    assertValid()\n+    val hadoopConf = broadcastedHadoopConf.value.value\n+    val blockManager = SparkEnv.get.blockManager\n+    val partition = split.asInstanceOf[HDFSBackedBlockRDDPartition]\n+    val blockId = partition.blockId\n+    blockManager.get(blockId) match {\n+      // Data is in Block Manager, grab it from there.\n+      case Some(block) =>\n+        block.data.asInstanceOf[Iterator[T]]\n+      // Data not found in Block Manager, grab it from HDFS\n+      case None =>\n+        logInfo(\"Reading partition data from write ahead log \" + partition.segment.path)\n+        val reader = new WriteAheadLogRandomReader(partition.segment.path, hadoopConf)\n+        val dataRead = reader.read(partition.segment)\n+        reader.close()\n+        // Currently, we support storing the data to BM only in serialized form and not in\n+        // deserialized form\n+        if (storeInBlockManager) {\n+          blockManager.putBytes(blockId, dataRead, storageLevel)\n+        }\n+        dataRead.rewind()\n+        blockManager.dataDeserialize(blockId, dataRead).asInstanceOf[Iterator[T]]\n+    }\n+  }\n+\n+  override def getPreferredLocations(split: Partition): Seq[String] = {\n+    val partition = split.asInstanceOf[HDFSBackedBlockRDDPartition]\n+    val locations = getBlockIdLocations()\n+    locations.getOrElse(partition.blockId,\n+      HdfsUtils.getBlockLocations(partition.segment.path, hadoopConfiguration)"
  }, {
    "author": {
      "login": "harishreedharan"
    },
    "body": "Fixed this one in the PR sent to your repo.\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-25T02:44:44Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+  val blockId: BlockId, idx: Int, val segment: WriteAheadLogFileSegment) extends Partition {\n+  val index = idx\n+}\n+\n+private[streaming]\n+class HDFSBackedBlockRDD[T: ClassTag](\n+    @transient sc: SparkContext,\n+    @transient hadoopConfiguration: Configuration,\n+    @transient override val blockIds: Array[BlockId],\n+    @transient val segments: Array[WriteAheadLogFileSegment],\n+    val storeInBlockManager: Boolean,\n+    val storageLevel: StorageLevel\n+  ) extends BlockRDD[T](sc, blockIds) {\n+\n+  if (blockIds.length != segments.length) {\n+    throw new IllegalStateException(\"Number of block ids must be the same as number of segments!\")\n+  }\n+\n+  // Hadoop Configuration is not serializable, so broadcast it as a serializable.\n+  val broadcastedHadoopConf = sc.broadcast(new SerializableWritable(hadoopConfiguration))\n+    .asInstanceOf[Broadcast[SerializableWritable[Configuration]]]\n+  override def getPartitions: Array[Partition] = {\n+    assertValid()\n+    (0 until blockIds.size).map { i =>\n+      new HDFSBackedBlockRDDPartition(blockIds(i), i, segments(i))\n+    }.toArray\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[T] = {\n+    assertValid()\n+    val hadoopConf = broadcastedHadoopConf.value.value\n+    val blockManager = SparkEnv.get.blockManager\n+    val partition = split.asInstanceOf[HDFSBackedBlockRDDPartition]\n+    val blockId = partition.blockId\n+    blockManager.get(blockId) match {\n+      // Data is in Block Manager, grab it from there.\n+      case Some(block) =>\n+        block.data.asInstanceOf[Iterator[T]]\n+      // Data not found in Block Manager, grab it from HDFS\n+      case None =>\n+        logInfo(\"Reading partition data from write ahead log \" + partition.segment.path)\n+        val reader = new WriteAheadLogRandomReader(partition.segment.path, hadoopConf)\n+        val dataRead = reader.read(partition.segment)\n+        reader.close()\n+        // Currently, we support storing the data to BM only in serialized form and not in\n+        // deserialized form\n+        if (storeInBlockManager) {\n+          blockManager.putBytes(blockId, dataRead, storageLevel)\n+        }\n+        dataRead.rewind()\n+        blockManager.dataDeserialize(blockId, dataRead).asInstanceOf[Iterator[T]]\n+    }\n+  }\n+\n+  override def getPreferredLocations(split: Partition): Seq[String] = {\n+    val partition = split.asInstanceOf[HDFSBackedBlockRDDPartition]\n+    val locations = getBlockIdLocations()\n+    locations.getOrElse(partition.blockId,\n+      HdfsUtils.getBlockLocations(partition.segment.path, hadoopConfiguration)"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Fixed.\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-27T17:43:35Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+  val blockId: BlockId, idx: Int, val segment: WriteAheadLogFileSegment) extends Partition {\n+  val index = idx\n+}\n+\n+private[streaming]\n+class HDFSBackedBlockRDD[T: ClassTag](\n+    @transient sc: SparkContext,\n+    @transient hadoopConfiguration: Configuration,\n+    @transient override val blockIds: Array[BlockId],\n+    @transient val segments: Array[WriteAheadLogFileSegment],\n+    val storeInBlockManager: Boolean,\n+    val storageLevel: StorageLevel\n+  ) extends BlockRDD[T](sc, blockIds) {\n+\n+  if (blockIds.length != segments.length) {\n+    throw new IllegalStateException(\"Number of block ids must be the same as number of segments!\")\n+  }\n+\n+  // Hadoop Configuration is not serializable, so broadcast it as a serializable.\n+  val broadcastedHadoopConf = sc.broadcast(new SerializableWritable(hadoopConfiguration))\n+    .asInstanceOf[Broadcast[SerializableWritable[Configuration]]]\n+  override def getPartitions: Array[Partition] = {\n+    assertValid()\n+    (0 until blockIds.size).map { i =>\n+      new HDFSBackedBlockRDDPartition(blockIds(i), i, segments(i))\n+    }.toArray\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[T] = {\n+    assertValid()\n+    val hadoopConf = broadcastedHadoopConf.value.value\n+    val blockManager = SparkEnv.get.blockManager\n+    val partition = split.asInstanceOf[HDFSBackedBlockRDDPartition]\n+    val blockId = partition.blockId\n+    blockManager.get(blockId) match {\n+      // Data is in Block Manager, grab it from there.\n+      case Some(block) =>\n+        block.data.asInstanceOf[Iterator[T]]\n+      // Data not found in Block Manager, grab it from HDFS\n+      case None =>\n+        logInfo(\"Reading partition data from write ahead log \" + partition.segment.path)\n+        val reader = new WriteAheadLogRandomReader(partition.segment.path, hadoopConf)\n+        val dataRead = reader.read(partition.segment)\n+        reader.close()\n+        // Currently, we support storing the data to BM only in serialized form and not in\n+        // deserialized form\n+        if (storeInBlockManager) {\n+          blockManager.putBytes(blockId, dataRead, storageLevel)\n+        }\n+        dataRead.rewind()\n+        blockManager.dataDeserialize(blockId, dataRead).asInstanceOf[Iterator[T]]\n+    }\n+  }\n+\n+  override def getPreferredLocations(split: Partition): Seq[String] = {\n+    val partition = split.asInstanceOf[HDFSBackedBlockRDDPartition]\n+    val locations = getBlockIdLocations()\n+    locations.getOrElse(partition.blockId,\n+      HdfsUtils.getBlockLocations(partition.segment.path, hadoopConfiguration)"
  }],
  "prId": 2931
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "For classes whose constructors don't fit on a single line, I think our style is to wrap it with one field per line, indented two spaces.\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-25T01:09:52Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+  val blockId: BlockId, idx: Int, val segment: WriteAheadLogFileSegment) extends Partition {"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Done.\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-27T19:14:14Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+  val blockId: BlockId, idx: Int, val segment: WriteAheadLogFileSegment) extends Partition {"
  }],
  "prId": 2931
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Why not just make the constructor `idx` a val and rename it to `index`?\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-25T01:10:17Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+  val blockId: BlockId, idx: Int, val segment: WriteAheadLogFileSegment) extends Partition {\n+  val index = idx"
  }],
  "prId": 2931
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "I usually like `require` for stuff like this.\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-25T01:10:54Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+  val blockId: BlockId, idx: Int, val segment: WriteAheadLogFileSegment) extends Partition {\n+  val index = idx\n+}\n+\n+private[streaming]\n+class HDFSBackedBlockRDD[T: ClassTag](\n+    @transient sc: SparkContext,\n+    @transient hadoopConfiguration: Configuration,\n+    @transient override val blockIds: Array[BlockId],\n+    @transient val segments: Array[WriteAheadLogFileSegment],\n+    val storeInBlockManager: Boolean,\n+    val storageLevel: StorageLevel\n+  ) extends BlockRDD[T](sc, blockIds) {\n+\n+  if (blockIds.length != segments.length) {"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "By the way, I found a neat library that auto-generates really nice error messages for `require` statements: http://www.scalactic.org/user_guide/Requirements\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-25T01:15:41Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+  val blockId: BlockId, idx: Int, val segment: WriteAheadLogFileSegment) extends Partition {\n+  val index = idx\n+}\n+\n+private[streaming]\n+class HDFSBackedBlockRDD[T: ClassTag](\n+    @transient sc: SparkContext,\n+    @transient hadoopConfiguration: Configuration,\n+    @transient override val blockIds: Array[BlockId],\n+    @transient val segments: Array[WriteAheadLogFileSegment],\n+    val storeInBlockManager: Boolean,\n+    val storageLevel: StorageLevel\n+  ) extends BlockRDD[T](sc, blockIds) {\n+\n+  if (blockIds.length != segments.length) {"
  }],
  "prId": 2931
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Over in #2935, @davies is planning to add some code to SerializableWritable to address the Hadoop Configuration constructor thread-safety issue, so you shouldn't have to do it here once we've merged that patch.\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-25T01:23:38Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+  val blockId: BlockId, idx: Int, val segment: WriteAheadLogFileSegment) extends Partition {\n+  val index = idx\n+}\n+\n+private[streaming]\n+class HDFSBackedBlockRDD[T: ClassTag](\n+    @transient sc: SparkContext,\n+    @transient hadoopConfiguration: Configuration,\n+    @transient override val blockIds: Array[BlockId],\n+    @transient val segments: Array[WriteAheadLogFileSegment],\n+    val storeInBlockManager: Boolean,\n+    val storageLevel: StorageLevel\n+  ) extends BlockRDD[T](sc, blockIds) {\n+\n+  if (blockIds.length != segments.length) {\n+    throw new IllegalStateException(\"Number of block ids must be the same as number of segments!\")\n+  }\n+\n+  // Hadoop Configuration is not serializable, so broadcast it as a serializable.\n+  val broadcastedHadoopConf = sc.broadcast(new SerializableWritable(hadoopConfiguration))"
  }, {
    "author": {
      "login": "harishreedharan"
    },
    "body": "Does it make sense to take the SerializableWritable as the argument in the constructor (as being done in #2935) or should we just take the hadoopConf and wrap it in the SerializableWritable once that is merged? We don't want to change the interface later. \n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-25T02:46:23Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+  val blockId: BlockId, idx: Int, val segment: WriteAheadLogFileSegment) extends Partition {\n+  val index = idx\n+}\n+\n+private[streaming]\n+class HDFSBackedBlockRDD[T: ClassTag](\n+    @transient sc: SparkContext,\n+    @transient hadoopConfiguration: Configuration,\n+    @transient override val blockIds: Array[BlockId],\n+    @transient val segments: Array[WriteAheadLogFileSegment],\n+    val storeInBlockManager: Boolean,\n+    val storageLevel: StorageLevel\n+  ) extends BlockRDD[T](sc, blockIds) {\n+\n+  if (blockIds.length != segments.length) {\n+    throw new IllegalStateException(\"Number of block ids must be the same as number of segments!\")\n+  }\n+\n+  // Hadoop Configuration is not serializable, so broadcast it as a serializable.\n+  val broadcastedHadoopConf = sc.broadcast(new SerializableWritable(hadoopConfiguration))"
  }, {
    "author": {
      "login": "harishreedharan"
    },
    "body": "For now I am leaving this as is. Lets revisit this later if needed.\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-25T06:07:15Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+  val blockId: BlockId, idx: Int, val segment: WriteAheadLogFileSegment) extends Partition {\n+  val index = idx\n+}\n+\n+private[streaming]\n+class HDFSBackedBlockRDD[T: ClassTag](\n+    @transient sc: SparkContext,\n+    @transient hadoopConfiguration: Configuration,\n+    @transient override val blockIds: Array[BlockId],\n+    @transient val segments: Array[WriteAheadLogFileSegment],\n+    val storeInBlockManager: Boolean,\n+    val storageLevel: StorageLevel\n+  ) extends BlockRDD[T](sc, blockIds) {\n+\n+  if (blockIds.length != segments.length) {\n+    throw new IllegalStateException(\"Number of block ids must be the same as number of segments!\")\n+  }\n+\n+  // Hadoop Configuration is not serializable, so broadcast it as a serializable.\n+  val broadcastedHadoopConf = sc.broadcast(new SerializableWritable(hadoopConfiguration))"
  }],
  "prId": 2931
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Why do you need this cast?\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-25T01:31:57Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+  val blockId: BlockId, idx: Int, val segment: WriteAheadLogFileSegment) extends Partition {\n+  val index = idx\n+}\n+\n+private[streaming]\n+class HDFSBackedBlockRDD[T: ClassTag](\n+    @transient sc: SparkContext,\n+    @transient hadoopConfiguration: Configuration,\n+    @transient override val blockIds: Array[BlockId],\n+    @transient val segments: Array[WriteAheadLogFileSegment],\n+    val storeInBlockManager: Boolean,\n+    val storageLevel: StorageLevel\n+  ) extends BlockRDD[T](sc, blockIds) {\n+\n+  if (blockIds.length != segments.length) {\n+    throw new IllegalStateException(\"Number of block ids must be the same as number of segments!\")\n+  }\n+\n+  // Hadoop Configuration is not serializable, so broadcast it as a serializable.\n+  val broadcastedHadoopConf = sc.broadcast(new SerializableWritable(hadoopConfiguration))\n+    .asInstanceOf[Broadcast[SerializableWritable[Configuration]]]"
  }, {
    "author": {
      "login": "harishreedharan"
    },
    "body": "We don't. I don't even remember why I added it at that time.\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-25T02:46:45Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+  val blockId: BlockId, idx: Int, val segment: WriteAheadLogFileSegment) extends Partition {\n+  val index = idx\n+}\n+\n+private[streaming]\n+class HDFSBackedBlockRDD[T: ClassTag](\n+    @transient sc: SparkContext,\n+    @transient hadoopConfiguration: Configuration,\n+    @transient override val blockIds: Array[BlockId],\n+    @transient val segments: Array[WriteAheadLogFileSegment],\n+    val storeInBlockManager: Boolean,\n+    val storageLevel: StorageLevel\n+  ) extends BlockRDD[T](sc, blockIds) {\n+\n+  if (blockIds.length != segments.length) {\n+    throw new IllegalStateException(\"Number of block ids must be the same as number of segments!\")\n+  }\n+\n+  // Hadoop Configuration is not serializable, so broadcast it as a serializable.\n+  val broadcastedHadoopConf = sc.broadcast(new SerializableWritable(hadoopConfiguration))\n+    .asInstanceOf[Broadcast[SerializableWritable[Configuration]]]"
  }],
  "prId": 2931
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "these few lines of getOrElse's are way too complicated to use getOrElse.\n\nFor the outer most layer, just create an if/else to make it more clear.\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T06:13:46Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+    val blockId: BlockId,\n+    val index: Int,\n+    val segment: WriteAheadLogFileSegment\n+  ) extends Partition\n+\n+private[streaming]\n+class HDFSBackedBlockRDD[T: ClassTag](\n+    @transient sc: SparkContext,\n+    @transient hadoopConfiguration: Configuration,\n+    @transient override val blockIds: Array[BlockId],\n+    @transient val segments: Array[WriteAheadLogFileSegment],\n+    val storeInBlockManager: Boolean,\n+    val storageLevel: StorageLevel\n+  ) extends BlockRDD[T](sc, blockIds) {\n+\n+  require(blockIds.length == segments.length,\n+    \"Number of block ids must be the same as number of segments!\")\n+\n+  // Hadoop Configuration is not serializable, so broadcast it as a serializable.\n+  val broadcastedHadoopConf = sc.broadcast(new SerializableWritable(hadoopConfiguration))\n+\n+  override def getPartitions: Array[Partition] = {\n+    assertValid()\n+    (0 until blockIds.size).map { i =>\n+      new HDFSBackedBlockRDDPartition(blockIds(i), i, segments(i))\n+    }.toArray\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[T] = {\n+    assertValid()\n+    val hadoopConf = broadcastedHadoopConf.value.value\n+    val blockManager = SparkEnv.get.blockManager\n+    val partition = split.asInstanceOf[HDFSBackedBlockRDDPartition]\n+    val blockId = partition.blockId\n+    blockManager.get(blockId) match {\n+      // Data is in Block Manager, grab it from there.\n+      case Some(block) =>\n+        block.data.asInstanceOf[Iterator[T]]\n+      // Data not found in Block Manager, grab it from HDFS\n+      case None =>\n+        logInfo(\"Reading partition data from write ahead log \" + partition.segment.path)\n+        val reader = new WriteAheadLogRandomReader(partition.segment.path, hadoopConf)\n+        val dataRead = reader.read(partition.segment)\n+        reader.close()\n+        // Currently, we support storing the data to BM only in serialized form and not in\n+        // deserialized form\n+        if (storeInBlockManager) {\n+          blockManager.putBytes(blockId, dataRead, storageLevel)\n+        }\n+        dataRead.rewind()\n+        blockManager.dataDeserialize(blockId, dataRead).asInstanceOf[Iterator[T]]\n+    }\n+  }\n+\n+  override def getPreferredLocations(split: Partition): Seq[String] = {\n+    val partition = split.asInstanceOf[HDFSBackedBlockRDDPartition]\n+    val locations = getBlockIdLocations()\n+    locations.getOrElse(partition.blockId,"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "For very deeply-nested versions of this pattern, where you want to select the first non-None value among many alternatives while lazily computing them, I like doing something like\n\n``` scala\ndef sparkPreferredLocations = getBlockIdLocations().get(partition.blockId)\ndef hdfsPreferredLocations = HdfsUtils.getBlockLocations(...)\nsparkPreferredLocations.orElse(hdfsPreferredLocations).getOrElse(Array[String].empty)\n```\n\nI kind of like this because it flattens the nested structure so the code reads as \"here are the alternatives to choose among, defined in order of their precedence, and here's the line that tries them in order.\"  I'm not sure whether this is more or less confusing than the nesting for new Scala users, though.  There's probably an even nicer version of this without all of the intermediate `orElse` calls, but I guess that's only really necessary if you're picking among _many_ alternatives; it might be excessively confusing here.\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T16:57:31Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+    val blockId: BlockId,\n+    val index: Int,\n+    val segment: WriteAheadLogFileSegment\n+  ) extends Partition\n+\n+private[streaming]\n+class HDFSBackedBlockRDD[T: ClassTag](\n+    @transient sc: SparkContext,\n+    @transient hadoopConfiguration: Configuration,\n+    @transient override val blockIds: Array[BlockId],\n+    @transient val segments: Array[WriteAheadLogFileSegment],\n+    val storeInBlockManager: Boolean,\n+    val storageLevel: StorageLevel\n+  ) extends BlockRDD[T](sc, blockIds) {\n+\n+  require(blockIds.length == segments.length,\n+    \"Number of block ids must be the same as number of segments!\")\n+\n+  // Hadoop Configuration is not serializable, so broadcast it as a serializable.\n+  val broadcastedHadoopConf = sc.broadcast(new SerializableWritable(hadoopConfiguration))\n+\n+  override def getPartitions: Array[Partition] = {\n+    assertValid()\n+    (0 until blockIds.size).map { i =>\n+      new HDFSBackedBlockRDDPartition(blockIds(i), i, segments(i))\n+    }.toArray\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[T] = {\n+    assertValid()\n+    val hadoopConf = broadcastedHadoopConf.value.value\n+    val blockManager = SparkEnv.get.blockManager\n+    val partition = split.asInstanceOf[HDFSBackedBlockRDDPartition]\n+    val blockId = partition.blockId\n+    blockManager.get(blockId) match {\n+      // Data is in Block Manager, grab it from there.\n+      case Some(block) =>\n+        block.data.asInstanceOf[Iterator[T]]\n+      // Data not found in Block Manager, grab it from HDFS\n+      case None =>\n+        logInfo(\"Reading partition data from write ahead log \" + partition.segment.path)\n+        val reader = new WriteAheadLogRandomReader(partition.segment.path, hadoopConf)\n+        val dataRead = reader.read(partition.segment)\n+        reader.close()\n+        // Currently, we support storing the data to BM only in serialized form and not in\n+        // deserialized form\n+        if (storeInBlockManager) {\n+          blockManager.putBytes(blockId, dataRead, storageLevel)\n+        }\n+        dataRead.rewind()\n+        blockManager.dataDeserialize(blockId, dataRead).asInstanceOf[Iterator[T]]\n+    }\n+  }\n+\n+  override def getPreferredLocations(split: Partition): Seq[String] = {\n+    val partition = split.asInstanceOf[HDFSBackedBlockRDDPartition]\n+    val locations = getBlockIdLocations()\n+    locations.getOrElse(partition.blockId,"
  }, {
    "author": {
      "login": "harishreedharan"
    },
    "body": "Correct, but we don't want to get the HDFS locations if we know that the sparkPreferredLocations exists, as looking up the locations from HDFS comes at a non-trivial cost of an RPC call. So I will move it to an if-else, so it is clearer.\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T17:59:12Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+    val blockId: BlockId,\n+    val index: Int,\n+    val segment: WriteAheadLogFileSegment\n+  ) extends Partition\n+\n+private[streaming]\n+class HDFSBackedBlockRDD[T: ClassTag](\n+    @transient sc: SparkContext,\n+    @transient hadoopConfiguration: Configuration,\n+    @transient override val blockIds: Array[BlockId],\n+    @transient val segments: Array[WriteAheadLogFileSegment],\n+    val storeInBlockManager: Boolean,\n+    val storageLevel: StorageLevel\n+  ) extends BlockRDD[T](sc, blockIds) {\n+\n+  require(blockIds.length == segments.length,\n+    \"Number of block ids must be the same as number of segments!\")\n+\n+  // Hadoop Configuration is not serializable, so broadcast it as a serializable.\n+  val broadcastedHadoopConf = sc.broadcast(new SerializableWritable(hadoopConfiguration))\n+\n+  override def getPartitions: Array[Partition] = {\n+    assertValid()\n+    (0 until blockIds.size).map { i =>\n+      new HDFSBackedBlockRDDPartition(blockIds(i), i, segments(i))\n+    }.toArray\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[T] = {\n+    assertValid()\n+    val hadoopConf = broadcastedHadoopConf.value.value\n+    val blockManager = SparkEnv.get.blockManager\n+    val partition = split.asInstanceOf[HDFSBackedBlockRDDPartition]\n+    val blockId = partition.blockId\n+    blockManager.get(blockId) match {\n+      // Data is in Block Manager, grab it from there.\n+      case Some(block) =>\n+        block.data.asInstanceOf[Iterator[T]]\n+      // Data not found in Block Manager, grab it from HDFS\n+      case None =>\n+        logInfo(\"Reading partition data from write ahead log \" + partition.segment.path)\n+        val reader = new WriteAheadLogRandomReader(partition.segment.path, hadoopConf)\n+        val dataRead = reader.read(partition.segment)\n+        reader.close()\n+        // Currently, we support storing the data to BM only in serialized form and not in\n+        // deserialized form\n+        if (storeInBlockManager) {\n+          blockManager.putBytes(blockId, dataRead, storageLevel)\n+        }\n+        dataRead.rewind()\n+        blockManager.dataDeserialize(blockId, dataRead).asInstanceOf[Iterator[T]]\n+    }\n+  }\n+\n+  override def getPreferredLocations(split: Partition): Seq[String] = {\n+    val partition = split.asInstanceOf[HDFSBackedBlockRDDPartition]\n+    val locations = getBlockIdLocations()\n+    locations.getOrElse(partition.blockId,"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "> we don't want to get the HDFS locations if we know that the sparkPreferredLocations exists\n\nThe snippet that I listed actually implements this behavior: if sparkPreferreredLocations is not `None`, then the `orElse` short-circuits and never evaluates `hdfsPreferredLocations`.\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T18:13:48Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+    val blockId: BlockId,\n+    val index: Int,\n+    val segment: WriteAheadLogFileSegment\n+  ) extends Partition\n+\n+private[streaming]\n+class HDFSBackedBlockRDD[T: ClassTag](\n+    @transient sc: SparkContext,\n+    @transient hadoopConfiguration: Configuration,\n+    @transient override val blockIds: Array[BlockId],\n+    @transient val segments: Array[WriteAheadLogFileSegment],\n+    val storeInBlockManager: Boolean,\n+    val storageLevel: StorageLevel\n+  ) extends BlockRDD[T](sc, blockIds) {\n+\n+  require(blockIds.length == segments.length,\n+    \"Number of block ids must be the same as number of segments!\")\n+\n+  // Hadoop Configuration is not serializable, so broadcast it as a serializable.\n+  val broadcastedHadoopConf = sc.broadcast(new SerializableWritable(hadoopConfiguration))\n+\n+  override def getPartitions: Array[Partition] = {\n+    assertValid()\n+    (0 until blockIds.size).map { i =>\n+      new HDFSBackedBlockRDDPartition(blockIds(i), i, segments(i))\n+    }.toArray\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[T] = {\n+    assertValid()\n+    val hadoopConf = broadcastedHadoopConf.value.value\n+    val blockManager = SparkEnv.get.blockManager\n+    val partition = split.asInstanceOf[HDFSBackedBlockRDDPartition]\n+    val blockId = partition.blockId\n+    blockManager.get(blockId) match {\n+      // Data is in Block Manager, grab it from there.\n+      case Some(block) =>\n+        block.data.asInstanceOf[Iterator[T]]\n+      // Data not found in Block Manager, grab it from HDFS\n+      case None =>\n+        logInfo(\"Reading partition data from write ahead log \" + partition.segment.path)\n+        val reader = new WriteAheadLogRandomReader(partition.segment.path, hadoopConf)\n+        val dataRead = reader.read(partition.segment)\n+        reader.close()\n+        // Currently, we support storing the data to BM only in serialized form and not in\n+        // deserialized form\n+        if (storeInBlockManager) {\n+          blockManager.putBytes(blockId, dataRead, storageLevel)\n+        }\n+        dataRead.rewind()\n+        blockManager.dataDeserialize(blockId, dataRead).asInstanceOf[Iterator[T]]\n+    }\n+  }\n+\n+  override def getPreferredLocations(split: Partition): Seq[String] = {\n+    val partition = split.asInstanceOf[HDFSBackedBlockRDDPartition]\n+    val locations = getBlockIdLocations()\n+    locations.getOrElse(partition.blockId,"
  }, {
    "author": {
      "login": "harishreedharan"
    },
    "body": "Ah, didn't notice the def :-)\n\nThanks!\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T18:19:25Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+    val blockId: BlockId,\n+    val index: Int,\n+    val segment: WriteAheadLogFileSegment\n+  ) extends Partition\n+\n+private[streaming]\n+class HDFSBackedBlockRDD[T: ClassTag](\n+    @transient sc: SparkContext,\n+    @transient hadoopConfiguration: Configuration,\n+    @transient override val blockIds: Array[BlockId],\n+    @transient val segments: Array[WriteAheadLogFileSegment],\n+    val storeInBlockManager: Boolean,\n+    val storageLevel: StorageLevel\n+  ) extends BlockRDD[T](sc, blockIds) {\n+\n+  require(blockIds.length == segments.length,\n+    \"Number of block ids must be the same as number of segments!\")\n+\n+  // Hadoop Configuration is not serializable, so broadcast it as a serializable.\n+  val broadcastedHadoopConf = sc.broadcast(new SerializableWritable(hadoopConfiguration))\n+\n+  override def getPartitions: Array[Partition] = {\n+    assertValid()\n+    (0 until blockIds.size).map { i =>\n+      new HDFSBackedBlockRDDPartition(blockIds(i), i, segments(i))\n+    }.toArray\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[T] = {\n+    assertValid()\n+    val hadoopConf = broadcastedHadoopConf.value.value\n+    val blockManager = SparkEnv.get.blockManager\n+    val partition = split.asInstanceOf[HDFSBackedBlockRDDPartition]\n+    val blockId = partition.blockId\n+    blockManager.get(blockId) match {\n+      // Data is in Block Manager, grab it from there.\n+      case Some(block) =>\n+        block.data.asInstanceOf[Iterator[T]]\n+      // Data not found in Block Manager, grab it from HDFS\n+      case None =>\n+        logInfo(\"Reading partition data from write ahead log \" + partition.segment.path)\n+        val reader = new WriteAheadLogRandomReader(partition.segment.path, hadoopConf)\n+        val dataRead = reader.read(partition.segment)\n+        reader.close()\n+        // Currently, we support storing the data to BM only in serialized form and not in\n+        // deserialized form\n+        if (storeInBlockManager) {\n+          blockManager.putBytes(blockId, dataRead, storageLevel)\n+        }\n+        dataRead.rewind()\n+        blockManager.dataDeserialize(blockId, dataRead).asInstanceOf[Iterator[T]]\n+    }\n+  }\n+\n+  override def getPreferredLocations(split: Partition): Seq[String] = {\n+    val partition = split.asInstanceOf[HDFSBackedBlockRDDPartition]\n+    val locations = getBlockIdLocations()\n+    locations.getOrElse(partition.blockId,"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "Let's make this dead simple. There is no need to create two extra functions and call getOrElse.\n\nJust do\n\n``` scala\nlocations.get(partition.blockId)) match {\n  case Some(loc) =>\n    loc\n  case None =>\n    val segment = partition.segment\n    HdfsUtils.getBlockLocations(segment.path, segment.offset, segment.length, hadoopConfiguration).getOrElse(Seq.empty)\n}\n```\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T20:07:32Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+    val blockId: BlockId,\n+    val index: Int,\n+    val segment: WriteAheadLogFileSegment\n+  ) extends Partition\n+\n+private[streaming]\n+class HDFSBackedBlockRDD[T: ClassTag](\n+    @transient sc: SparkContext,\n+    @transient hadoopConfiguration: Configuration,\n+    @transient override val blockIds: Array[BlockId],\n+    @transient val segments: Array[WriteAheadLogFileSegment],\n+    val storeInBlockManager: Boolean,\n+    val storageLevel: StorageLevel\n+  ) extends BlockRDD[T](sc, blockIds) {\n+\n+  require(blockIds.length == segments.length,\n+    \"Number of block ids must be the same as number of segments!\")\n+\n+  // Hadoop Configuration is not serializable, so broadcast it as a serializable.\n+  val broadcastedHadoopConf = sc.broadcast(new SerializableWritable(hadoopConfiguration))\n+\n+  override def getPartitions: Array[Partition] = {\n+    assertValid()\n+    (0 until blockIds.size).map { i =>\n+      new HDFSBackedBlockRDDPartition(blockIds(i), i, segments(i))\n+    }.toArray\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[T] = {\n+    assertValid()\n+    val hadoopConf = broadcastedHadoopConf.value.value\n+    val blockManager = SparkEnv.get.blockManager\n+    val partition = split.asInstanceOf[HDFSBackedBlockRDDPartition]\n+    val blockId = partition.blockId\n+    blockManager.get(blockId) match {\n+      // Data is in Block Manager, grab it from there.\n+      case Some(block) =>\n+        block.data.asInstanceOf[Iterator[T]]\n+      // Data not found in Block Manager, grab it from HDFS\n+      case None =>\n+        logInfo(\"Reading partition data from write ahead log \" + partition.segment.path)\n+        val reader = new WriteAheadLogRandomReader(partition.segment.path, hadoopConf)\n+        val dataRead = reader.read(partition.segment)\n+        reader.close()\n+        // Currently, we support storing the data to BM only in serialized form and not in\n+        // deserialized form\n+        if (storeInBlockManager) {\n+          blockManager.putBytes(blockId, dataRead, storageLevel)\n+        }\n+        dataRead.rewind()\n+        blockManager.dataDeserialize(blockId, dataRead).asInstanceOf[Iterator[T]]\n+    }\n+  }\n+\n+  override def getPreferredLocations(split: Partition): Seq[String] = {\n+    val partition = split.asInstanceOf[HDFSBackedBlockRDDPartition]\n+    val locations = getBlockIdLocations()\n+    locations.getOrElse(partition.blockId,"
  }],
  "prId": 2931
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "i don't think u need to declare this a field. it's already a field in the parent class and you can just use that?\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T06:15:41Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+    val blockId: BlockId,\n+    val index: Int,\n+    val segment: WriteAheadLogFileSegment\n+  ) extends Partition\n+\n+private[streaming]\n+class HDFSBackedBlockRDD[T: ClassTag](\n+    @transient sc: SparkContext,\n+    @transient hadoopConfiguration: Configuration,\n+    @transient override val blockIds: Array[BlockId],"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Didnt quite get it. We do have to pass block IDs into the constructor for BlockRDD, for that block IDs need to be taken as part of the constructor of this class - either with as `override blockIds` or with a different name. Isnt it? Or is there a better pattern that i dont know of?\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T08:20:38Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+    val blockId: BlockId,\n+    val index: Int,\n+    val segment: WriteAheadLogFileSegment\n+  ) extends Partition\n+\n+private[streaming]\n+class HDFSBackedBlockRDD[T: ClassTag](\n+    @transient sc: SparkContext,\n+    @transient hadoopConfiguration: Configuration,\n+    @transient override val blockIds: Array[BlockId],"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "I think that Reynold was asking whether this needs to be declared as `val` here, not whether the constructor needs to accept blockIds.\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T16:36:03Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+    val blockId: BlockId,\n+    val index: Int,\n+    val segment: WriteAheadLogFileSegment\n+  ) extends Partition\n+\n+private[streaming]\n+class HDFSBackedBlockRDD[T: ClassTag](\n+    @transient sc: SparkContext,\n+    @transient hadoopConfiguration: Configuration,\n+    @transient override val blockIds: Array[BlockId],"
  }, {
    "author": {
      "login": "harishreedharan"
    },
    "body": "Done\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T19:01:22Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+    val blockId: BlockId,\n+    val index: Int,\n+    val segment: WriteAheadLogFileSegment\n+  ) extends Partition\n+\n+private[streaming]\n+class HDFSBackedBlockRDD[T: ClassTag](\n+    @transient sc: SparkContext,\n+    @transient hadoopConfiguration: Configuration,\n+    @transient override val blockIds: Array[BlockId],"
  }],
  "prId": 2931
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "can this be private[this]?\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T06:16:48Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+    val blockId: BlockId,\n+    val index: Int,\n+    val segment: WriteAheadLogFileSegment\n+  ) extends Partition\n+\n+private[streaming]\n+class HDFSBackedBlockRDD[T: ClassTag](\n+    @transient sc: SparkContext,\n+    @transient hadoopConfiguration: Configuration,\n+    @transient override val blockIds: Array[BlockId],\n+    @transient val segments: Array[WriteAheadLogFileSegment],\n+    val storeInBlockManager: Boolean,\n+    val storageLevel: StorageLevel\n+  ) extends BlockRDD[T](sc, blockIds) {\n+\n+  require(blockIds.length == segments.length,\n+    \"Number of block ids must be the same as number of segments!\")\n+\n+  // Hadoop Configuration is not serializable, so broadcast it as a serializable.\n+  val broadcastedHadoopConf = sc.broadcast(new SerializableWritable(hadoopConfiguration))"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Should definitely be private, @harishreedharan \n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T08:06:06Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+    val blockId: BlockId,\n+    val index: Int,\n+    val segment: WriteAheadLogFileSegment\n+  ) extends Partition\n+\n+private[streaming]\n+class HDFSBackedBlockRDD[T: ClassTag](\n+    @transient sc: SparkContext,\n+    @transient hadoopConfiguration: Configuration,\n+    @transient override val blockIds: Array[BlockId],\n+    @transient val segments: Array[WriteAheadLogFileSegment],\n+    val storeInBlockManager: Boolean,\n+    val storageLevel: StorageLevel\n+  ) extends BlockRDD[T](sc, blockIds) {\n+\n+  require(blockIds.length == segments.length,\n+    \"Number of block ids must be the same as number of segments!\")\n+\n+  // Hadoop Configuration is not serializable, so broadcast it as a serializable.\n+  val broadcastedHadoopConf = sc.broadcast(new SerializableWritable(hadoopConfiguration))"
  }],
  "prId": 2931
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "Array.tabulate\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T06:17:04Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+    val blockId: BlockId,\n+    val index: Int,\n+    val segment: WriteAheadLogFileSegment\n+  ) extends Partition\n+\n+private[streaming]\n+class HDFSBackedBlockRDD[T: ClassTag](\n+    @transient sc: SparkContext,\n+    @transient hadoopConfiguration: Configuration,\n+    @transient override val blockIds: Array[BlockId],\n+    @transient val segments: Array[WriteAheadLogFileSegment],\n+    val storeInBlockManager: Boolean,\n+    val storageLevel: StorageLevel\n+  ) extends BlockRDD[T](sc, blockIds) {\n+\n+  require(blockIds.length == segments.length,\n+    \"Number of block ids must be the same as number of segments!\")\n+\n+  // Hadoop Configuration is not serializable, so broadcast it as a serializable.\n+  val broadcastedHadoopConf = sc.broadcast(new SerializableWritable(hadoopConfiguration))\n+\n+  override def getPartitions: Array[Partition] = {\n+    assertValid()\n+    (0 until blockIds.size).map { i =>"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Done.\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T21:40:31Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+    val blockId: BlockId,\n+    val index: Int,\n+    val segment: WriteAheadLogFileSegment\n+  ) extends Partition\n+\n+private[streaming]\n+class HDFSBackedBlockRDD[T: ClassTag](\n+    @transient sc: SparkContext,\n+    @transient hadoopConfiguration: Configuration,\n+    @transient override val blockIds: Array[BlockId],\n+    @transient val segments: Array[WriteAheadLogFileSegment],\n+    val storeInBlockManager: Boolean,\n+    val storageLevel: StorageLevel\n+  ) extends BlockRDD[T](sc, blockIds) {\n+\n+  require(blockIds.length == segments.length,\n+    \"Number of block ids must be the same as number of segments!\")\n+\n+  // Hadoop Configuration is not serializable, so broadcast it as a serializable.\n+  val broadcastedHadoopConf = sc.broadcast(new SerializableWritable(hadoopConfiguration))\n+\n+  override def getPartitions: Array[Partition] = {\n+    assertValid()\n+    (0 until blockIds.size).map { i =>"
  }],
  "prId": 2931
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "I don't think it makes sense to tie this to WriteAheadLogFileSegment. On one hand the naming HDFSBackedBlockRDD is supposed to be general, on the other you tie it to recovery through the use of WriteAheadLogFileSegment. \n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T06:23:58Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+    val blockId: BlockId,\n+    val index: Int,\n+    val segment: WriteAheadLogFileSegment\n+  ) extends Partition\n+\n+private[streaming]\n+class HDFSBackedBlockRDD[T: ClassTag]("
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "It would be great to add javadoc explaining what this class is for. If it is used for recovery, why should we put the blocks in block manager after using them? Shouldn't recovery data be used only once during a recovery?\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T06:24:45Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+    val blockId: BlockId,\n+    val index: Int,\n+    val segment: WriteAheadLogFileSegment\n+  ) extends Partition\n+\n+private[streaming]\n+class HDFSBackedBlockRDD[T: ClassTag]("
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Good point, how about renaming this class to more specific `WriteAheadLogBasedBackedBlockRDD` (kind-a-long).\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T08:08:08Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+    val blockId: BlockId,\n+    val index: Int,\n+    val segment: WriteAheadLogFileSegment\n+  ) extends Partition\n+\n+private[streaming]\n+class HDFSBackedBlockRDD[T: ClassTag]("
  }, {
    "author": {
      "login": "harishreedharan"
    },
    "body": "I think we could still have to use the recovered data if the same RDD is used for multiple operations, correct? Maybe I am mistaken, but if I do something like \n\n`rdd1 = hdfsRdd.<blah>`\nand\n`rdd2=hdfsRdd.<blah2>` \n\nwouldn't it be better if the data recovered is now in BlockManager?\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T19:00:16Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+    val blockId: BlockId,\n+    val index: Int,\n+    val segment: WriteAheadLogFileSegment\n+  ) extends Partition\n+\n+private[streaming]\n+class HDFSBackedBlockRDD[T: ClassTag]("
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Changed to `WriteAheadLogBasedBackedBlockRDD`\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T21:40:52Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._\n+\n+private[streaming]\n+class HDFSBackedBlockRDDPartition(\n+    val blockId: BlockId,\n+    val index: Int,\n+    val segment: WriteAheadLogFileSegment\n+  ) extends Partition\n+\n+private[streaming]\n+class HDFSBackedBlockRDD[T: ClassTag]("
  }],
  "prId": 2931
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "import out of order\n",
    "commit": "209e49cf7b7467f5ba10f9352745823104f6a332",
    "createdAt": "2014-10-28T06:25:54Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.rdd\n+\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.conf.Configuration\n+\n+import org.apache.spark.rdd.BlockRDD\n+import org.apache.spark.storage.{BlockId, StorageLevel}\n+import org.apache.spark.streaming.util.{WriteAheadLogFileSegment, HdfsUtils, WriteAheadLogRandomReader}\n+import org.apache.spark._"
  }],
  "prId": 2931
}]