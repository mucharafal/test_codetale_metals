[{
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "This error message should probably be different in order to distinguish it from the EOFException case.\n",
    "commit": "e4bee2065293d7373c43fe5636dd9971dede257e",
    "createdAt": "2014-10-22T04:17:36Z",
    "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.util\n+\n+import java.io.{Closeable, EOFException}\n+import java.nio.ByteBuffer\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.Logging\n+\n+/**\n+ * A reader for reading write ahead log files written using\n+ * [[org.apache.spark.streaming.util.WriteAheadLogWriter]]. This reads\n+ * the records (bytebuffers) in the log file sequentially and return them as an\n+ * iterator of bytebuffers.\n+ */\n+private[streaming] class WriteAheadLogReader(path: String, conf: Configuration)\n+  extends Iterator[ByteBuffer] with Closeable with Logging {\n+\n+  private val instream = HdfsUtils.getInputStream(path, conf)\n+  private var closed = false\n+  private var nextItem: Option[ByteBuffer] = None\n+\n+  override def hasNext: Boolean = synchronized {\n+    if (closed) {\n+      return false\n+    }\n+\n+    if (nextItem.isDefined) { // handle the case where hasNext is called without calling next\n+      true\n+    } else {\n+      try {\n+        val length = instream.readInt()\n+        val buffer = new Array[Byte](length)\n+        instream.readFully(buffer)\n+        nextItem = Some(ByteBuffer.wrap(buffer))\n+        logTrace(\"Read next item \" + nextItem.get)\n+        true\n+      } catch {\n+        case e: EOFException =>\n+          logDebug(\"Error reading next item, EOF reached\", e)\n+          close()\n+          false\n+        case e: Exception =>\n+          logDebug(\"Error reading next item, EOF reached\", e)"
  }, {
    "author": {
      "login": "harishreedharan"
    },
    "body": "Will fix in the next iter\n",
    "commit": "e4bee2065293d7373c43fe5636dd9971dede257e",
    "createdAt": "2014-10-22T05:41:45Z",
    "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.util\n+\n+import java.io.{Closeable, EOFException}\n+import java.nio.ByteBuffer\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.Logging\n+\n+/**\n+ * A reader for reading write ahead log files written using\n+ * [[org.apache.spark.streaming.util.WriteAheadLogWriter]]. This reads\n+ * the records (bytebuffers) in the log file sequentially and return them as an\n+ * iterator of bytebuffers.\n+ */\n+private[streaming] class WriteAheadLogReader(path: String, conf: Configuration)\n+  extends Iterator[ByteBuffer] with Closeable with Logging {\n+\n+  private val instream = HdfsUtils.getInputStream(path, conf)\n+  private var closed = false\n+  private var nextItem: Option[ByteBuffer] = None\n+\n+  override def hasNext: Boolean = synchronized {\n+    if (closed) {\n+      return false\n+    }\n+\n+    if (nextItem.isDefined) { // handle the case where hasNext is called without calling next\n+      true\n+    } else {\n+      try {\n+        val length = instream.readInt()\n+        val buffer = new Array[Byte](length)\n+        instream.readFully(buffer)\n+        nextItem = Some(ByteBuffer.wrap(buffer))\n+        logTrace(\"Read next item \" + nextItem.get)\n+        true\n+      } catch {\n+        case e: EOFException =>\n+          logDebug(\"Error reading next item, EOF reached\", e)\n+          close()\n+          false\n+        case e: Exception =>\n+          logDebug(\"Error reading next item, EOF reached\", e)"
  }],
  "prId": 2882
}]