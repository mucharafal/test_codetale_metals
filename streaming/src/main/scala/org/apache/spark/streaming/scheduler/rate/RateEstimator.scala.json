[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "There should be no need for such an estimator if Options are used. \n",
    "commit": "f168c9476fc7104d3d3f92702793e0d9116117d0",
    "createdAt": "2015-07-23T04:36:06Z",
    "diffHunk": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.scheduler.rate\n+\n+import org.apache.spark.annotation.DeveloperApi\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A component that estimates the rate at wich an InputDStream should ingest\n+ * elements, based on updates at every batch completion.\n+ */\n+@DeveloperApi\n+private[streaming] trait RateEstimator extends Serializable {\n+\n+  /**\n+   * Computes the number of elements the stream attached to this `RateEstimator`\n+   * should ingest per second, given an update on the size and completion\n+   * times of the latest batch.\n+   *\n+   * @param time The timetamp of the current batch interval that just finished\n+   * @param elements The number of elements that were processed in this batch\n+   * @param processingDelay The time in ms that took for the job to complete\n+   * @param schedulingDelay The time in ms that the job spent in the scheduling queue\n+   */\n+  def compute(\n+      time: Long,\n+      elements: Long,\n+      processingDelay: Long,\n+      schedulingDelay: Long): Option[Double]\n+}\n+\n+/**\n+ * The trivial rate estimator never sends an update\n+ */\n+private[streaming] class NoopRateEstimator extends RateEstimator {"
  }],
  "prId": 7600
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Does not make sense to have non-public dev API. \n",
    "commit": "f168c9476fc7104d3d3f92702793e0d9116117d0",
    "createdAt": "2015-07-23T04:41:01Z",
    "diffHunk": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.scheduler.rate\n+\n+import org.apache.spark.annotation.DeveloperApi\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A component that estimates the rate at wich an InputDStream should ingest\n+ * elements, based on updates at every batch completion.\n+ */\n+@DeveloperApi"
  }],
  "prId": 7600
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Let's make the configuration params as follows\n`spark.streaming.backpressure.enable = true/false` to enable/disable the whole feature. false by default in 1.5. Also to the docs/configuration.md.\n`spark.streaming.backpressure.rateEstimator = pid` to specify which algorithm to use for estimating. DO NOT add this to the docs/configuration.\n\nBasically, the scope `spark.streaming.backpressure` will contain on backpressure related configuration. \n\nAccordingly, the RateController in InputDStreams will be created as `RateController.create()`, and the estimator will be created by `RateEstimator.create`.\n",
    "commit": "f168c9476fc7104d3d3f92702793e0d9116117d0",
    "createdAt": "2015-07-24T06:49:59Z",
    "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.scheduler.rate\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.SparkException\n+\n+/**\n+ * A component that estimates the rate at wich an InputDStream should ingest\n+ * elements, based on updates at every batch completion.\n+ */\n+private[streaming] trait RateEstimator extends Serializable {\n+\n+  /**\n+   * Computes the number of elements the stream attached to this `RateEstimator`\n+   * should ingest per second, given an update on the size and completion\n+   * times of the latest batch.\n+   *\n+   * @param time The timetamp of the current batch interval that just finished\n+   * @param elements The number of elements that were processed in this batch\n+   * @param processingDelay The time in ms that took for the job to complete\n+   * @param schedulingDelay The time in ms that the job spent in the scheduling queue\n+   */\n+  def compute(\n+      time: Long,\n+      elements: Long,\n+      processingDelay: Long,\n+      schedulingDelay: Long): Option[Double]\n+}\n+\n+object RateEstimator {\n+\n+  /**\n+   * Return a new RateEstimator based on the value of `spark.streaming.RateEstimator`.\n+   *\n+   * @return None if there is no configured estimator, otherwise an instance of RateEstimator\n+   * @throws IllegalArgumentException if there is a configured RateEstimator that doesn't match any\n+   *         known estimators.\n+   */\n+  def makeEstimator(conf: SparkConf): Option[RateEstimator] =\n+    conf.getOption(\"spark.streaming.RateEstimator\") map { estimator =>"
  }, {
    "author": {
      "login": "dragos"
    },
    "body": "The only problem I see is that we'll have different `RateControllers` in different input streams. For instance\n-  `ReceiverInputDStream` publishes using the receiver tracker, but the KafkaDirect receiver won't.\n- the `ReceiverRateController` needs access to the receiver tracker, so currently it's an inner class of `ReceiverInputDStream`.\n\nThis is the main reason I wanted to have `publish` be a method (`protected[streaming]`) on input streams. Usually the logic of _what to do_ with the rate is closer to the specific type of stream, than the controller. If we moved `publish` to the stream, we could have a single implementation of `RateController`, that can be controlled by a single boolean `spark.streaming.backpressure.enabled`.\n\n@tdas What do you think?\n",
    "commit": "f168c9476fc7104d3d3f92702793e0d9116117d0",
    "createdAt": "2015-07-24T09:52:47Z",
    "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.scheduler.rate\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.SparkException\n+\n+/**\n+ * A component that estimates the rate at wich an InputDStream should ingest\n+ * elements, based on updates at every batch completion.\n+ */\n+private[streaming] trait RateEstimator extends Serializable {\n+\n+  /**\n+   * Computes the number of elements the stream attached to this `RateEstimator`\n+   * should ingest per second, given an update on the size and completion\n+   * times of the latest batch.\n+   *\n+   * @param time The timetamp of the current batch interval that just finished\n+   * @param elements The number of elements that were processed in this batch\n+   * @param processingDelay The time in ms that took for the job to complete\n+   * @param schedulingDelay The time in ms that the job spent in the scheduling queue\n+   */\n+  def compute(\n+      time: Long,\n+      elements: Long,\n+      processingDelay: Long,\n+      schedulingDelay: Long): Option[Double]\n+}\n+\n+object RateEstimator {\n+\n+  /**\n+   * Return a new RateEstimator based on the value of `spark.streaming.RateEstimator`.\n+   *\n+   * @return None if there is no configured estimator, otherwise an instance of RateEstimator\n+   * @throws IllegalArgumentException if there is a configured RateEstimator that doesn't match any\n+   *         known estimators.\n+   */\n+  def makeEstimator(conf: SparkConf): Option[RateEstimator] =\n+    conf.getOption(\"spark.streaming.RateEstimator\") map { estimator =>"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "\"InputDStream.publish\" is not something where its immediately clear from the name on what it means in the context of the InputDstream. In fact, publishing a rate is internal implementation detail of how rate controlling is implemented, and not something that should be fundamentally added as a field to the abstract parent class  InputDStreams. In contrast, the field rateController is something that is more generic and implementation-agnostic and is okay to have as a field in the abstract parent class. \n\nSo I propose this. \n1. Let there be a object method `RateController.isBackpressureEnabled`, which will return true if `spark.streaming.backpressure.enabled`. \n2. Class `ReceiverInputDStream` will create a controller instance of `ReceiverRateController` if the above method returns true. You may or may not wrap that check in an object method, `ReceiverRateController.create(): Option[RateController]`\n3. `DirectKafkaRateController` will create a conroller instance of `DirectKafkaRateController` in the same way. \n",
    "commit": "f168c9476fc7104d3d3f92702793e0d9116117d0",
    "createdAt": "2015-07-27T21:16:12Z",
    "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.scheduler.rate\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.SparkException\n+\n+/**\n+ * A component that estimates the rate at wich an InputDStream should ingest\n+ * elements, based on updates at every batch completion.\n+ */\n+private[streaming] trait RateEstimator extends Serializable {\n+\n+  /**\n+   * Computes the number of elements the stream attached to this `RateEstimator`\n+   * should ingest per second, given an update on the size and completion\n+   * times of the latest batch.\n+   *\n+   * @param time The timetamp of the current batch interval that just finished\n+   * @param elements The number of elements that were processed in this batch\n+   * @param processingDelay The time in ms that took for the job to complete\n+   * @param schedulingDelay The time in ms that the job spent in the scheduling queue\n+   */\n+  def compute(\n+      time: Long,\n+      elements: Long,\n+      processingDelay: Long,\n+      schedulingDelay: Long): Option[Double]\n+}\n+\n+object RateEstimator {\n+\n+  /**\n+   * Return a new RateEstimator based on the value of `spark.streaming.RateEstimator`.\n+   *\n+   * @return None if there is no configured estimator, otherwise an instance of RateEstimator\n+   * @throws IllegalArgumentException if there is a configured RateEstimator that doesn't match any\n+   *         known estimators.\n+   */\n+  def makeEstimator(conf: SparkConf): Option[RateEstimator] =\n+    conf.getOption(\"spark.streaming.RateEstimator\") map { estimator =>"
  }],
  "prId": 7600
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Why should this return an option? Since there is a separate conf that enables/disables RateController, this should always return an instance of RateEstimator even if its the default. But I do understand that since this PR does not have a default one this is probably better. So I am okay to merge this PR as is, as long as the PID estimator PR updates this code to not use Option. Does that make sense?\n",
    "commit": "f168c9476fc7104d3d3f92702793e0d9116117d0",
    "createdAt": "2015-07-28T13:10:10Z",
    "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.scheduler.rate\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.SparkException\n+\n+/**\n+ * A component that estimates the rate at wich an InputDStream should ingest\n+ * elements, based on updates at every batch completion.\n+ */\n+private[streaming] trait RateEstimator extends Serializable {\n+\n+  /**\n+   * Computes the number of elements the stream attached to this `RateEstimator`\n+   * should ingest per second, given an update on the size and completion\n+   * times of the latest batch.\n+   *\n+   * @param time The timetamp of the current batch interval that just finished\n+   * @param elements The number of elements that were processed in this batch\n+   * @param processingDelay The time in ms that took for the job to complete\n+   * @param schedulingDelay The time in ms that the job spent in the scheduling queue\n+   */\n+  def compute(\n+      time: Long,\n+      elements: Long,\n+      processingDelay: Long,\n+      schedulingDelay: Long): Option[Double]\n+}\n+\n+object RateEstimator {\n+\n+  /**\n+   * Return a new RateEstimator based on the value of `spark.streaming.RateEstimator`.\n+   *\n+   * @return None if there is no configured estimator, otherwise an instance of RateEstimator\n+   * @throws IllegalArgumentException if there is a configured RateEstimator that doesn't match any\n+   *         known estimators.\n+   */\n+  def create(conf: SparkConf): Option[RateEstimator] =",
    "line": 55
  }, {
    "author": {
      "login": "dragos"
    },
    "body": "ok\n",
    "commit": "f168c9476fc7104d3d3f92702793e0d9116117d0",
    "createdAt": "2015-07-28T16:27:22Z",
    "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.scheduler.rate\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.SparkException\n+\n+/**\n+ * A component that estimates the rate at wich an InputDStream should ingest\n+ * elements, based on updates at every batch completion.\n+ */\n+private[streaming] trait RateEstimator extends Serializable {\n+\n+  /**\n+   * Computes the number of elements the stream attached to this `RateEstimator`\n+   * should ingest per second, given an update on the size and completion\n+   * times of the latest batch.\n+   *\n+   * @param time The timetamp of the current batch interval that just finished\n+   * @param elements The number of elements that were processed in this batch\n+   * @param processingDelay The time in ms that took for the job to complete\n+   * @param schedulingDelay The time in ms that the job spent in the scheduling queue\n+   */\n+  def compute(\n+      time: Long,\n+      elements: Long,\n+      processingDelay: Long,\n+      schedulingDelay: Long): Option[Double]\n+}\n+\n+object RateEstimator {\n+\n+  /**\n+   * Return a new RateEstimator based on the value of `spark.streaming.RateEstimator`.\n+   *\n+   * @return None if there is no configured estimator, otherwise an instance of RateEstimator\n+   * @throws IllegalArgumentException if there is a configured RateEstimator that doesn't match any\n+   *         known estimators.\n+   */\n+  def create(conf: SparkConf): Option[RateEstimator] =",
    "line": 55
  }],
  "prId": 7600
}]