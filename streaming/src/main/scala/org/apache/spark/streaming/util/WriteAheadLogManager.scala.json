[{
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "I think that `succeeded` implies `fileSegment != null` and vice-versa, so you probably don't need this variable.\n",
    "commit": "e4bee2065293d7373c43fe5636dd9971dede257e",
    "createdAt": "2014-10-22T04:03:21Z",
    "diffHunk": "@@ -0,0 +1,223 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.util\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{ExecutionContext, Future}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.fs.permission.FsPermission\n+import org.apache.spark.Logging\n+import org.apache.spark.util.Utils\n+import WriteAheadLogManager._\n+\n+/**\n+ * This class manages write ahead log files.\n+ * - Writes records (bytebuffers) to periodically rotating log files.\n+ * - Recovers the log files and the reads the recovered records upon failures.\n+ * - Cleans up old log files.\n+ *\n+ * Uses [[org.apache.spark.streaming.util.WriteAheadLogWriter]] to write\n+ * and [[org.apache.spark.streaming.util.WriteAheadLogReader]] to read.\n+ *\n+ *@param logDirectory Directory when rotating log files will be created.\n+ * @param hadoopConf Hadoop configuration for reading/writing log files.\n+ * @param rollingIntervalSecs The interval in seconds with which logs will be rolled over.\n+ *                            Default is one minute.\n+ * @param maxFailures Max number of failures that is tolerated for every attempt to write to log.\n+ *                    Default is three.\n+ * @param callerName Optional name of the class who is using this manager.\n+ * @param clock Optional clock that is used to check for rotation interval.\n+ */\n+private[streaming] class WriteAheadLogManager(\n+    logDirectory: String,\n+    hadoopConf: Configuration,\n+    rollingIntervalSecs: Int = 60,\n+    maxFailures: Int = 3,\n+    callerName: String = \"\",\n+    clock: Clock = new SystemClock\n+  ) extends Logging {\n+\n+  private val pastLogs = new ArrayBuffer[LogInfo]\n+  private val callerNameTag =\n+    if (callerName != null && callerName.nonEmpty) s\" for $callerName\" else \"\"\n+  private val threadpoolName = s\"WriteAheadLogManager $callerNameTag\"\n+  implicit private val executionContext = ExecutionContext.fromExecutorService(\n+    Utils.newDaemonFixedThreadPool(1, threadpoolName))\n+  override protected val logName = s\"WriteAheadLogManager $callerNameTag\"\n+\n+  private var currentLogPath: String = null\n+  private var currentLogWriter: WriteAheadLogWriter = null\n+  private var currentLogWriterStartTime: Long = -1L\n+  private var currentLogWriterStopTime: Long = -1L\n+\n+  initializeOrRecover()\n+\n+  /** Write a byte buffer to the log file */\n+  def writeToLog(byteBuffer: ByteBuffer): FileSegment = synchronized {\n+    var fileSegment: FileSegment = null\n+    var failures = 0\n+    var lastException: Exception = null\n+    var succeeded = false"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "I had thought so, and I usually do it. But for reading I felt its easier to understand `if (!succeeded ...` . Am happy to change it.\n",
    "commit": "e4bee2065293d7373c43fe5636dd9971dede257e",
    "createdAt": "2014-10-22T07:23:38Z",
    "diffHunk": "@@ -0,0 +1,223 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.util\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{ExecutionContext, Future}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.fs.permission.FsPermission\n+import org.apache.spark.Logging\n+import org.apache.spark.util.Utils\n+import WriteAheadLogManager._\n+\n+/**\n+ * This class manages write ahead log files.\n+ * - Writes records (bytebuffers) to periodically rotating log files.\n+ * - Recovers the log files and the reads the recovered records upon failures.\n+ * - Cleans up old log files.\n+ *\n+ * Uses [[org.apache.spark.streaming.util.WriteAheadLogWriter]] to write\n+ * and [[org.apache.spark.streaming.util.WriteAheadLogReader]] to read.\n+ *\n+ *@param logDirectory Directory when rotating log files will be created.\n+ * @param hadoopConf Hadoop configuration for reading/writing log files.\n+ * @param rollingIntervalSecs The interval in seconds with which logs will be rolled over.\n+ *                            Default is one minute.\n+ * @param maxFailures Max number of failures that is tolerated for every attempt to write to log.\n+ *                    Default is three.\n+ * @param callerName Optional name of the class who is using this manager.\n+ * @param clock Optional clock that is used to check for rotation interval.\n+ */\n+private[streaming] class WriteAheadLogManager(\n+    logDirectory: String,\n+    hadoopConf: Configuration,\n+    rollingIntervalSecs: Int = 60,\n+    maxFailures: Int = 3,\n+    callerName: String = \"\",\n+    clock: Clock = new SystemClock\n+  ) extends Logging {\n+\n+  private val pastLogs = new ArrayBuffer[LogInfo]\n+  private val callerNameTag =\n+    if (callerName != null && callerName.nonEmpty) s\" for $callerName\" else \"\"\n+  private val threadpoolName = s\"WriteAheadLogManager $callerNameTag\"\n+  implicit private val executionContext = ExecutionContext.fromExecutorService(\n+    Utils.newDaemonFixedThreadPool(1, threadpoolName))\n+  override protected val logName = s\"WriteAheadLogManager $callerNameTag\"\n+\n+  private var currentLogPath: String = null\n+  private var currentLogWriter: WriteAheadLogWriter = null\n+  private var currentLogWriterStartTime: Long = -1L\n+  private var currentLogWriterStopTime: Long = -1L\n+\n+  initializeOrRecover()\n+\n+  /** Write a byte buffer to the log file */\n+  def writeToLog(byteBuffer: ByteBuffer): FileSegment = synchronized {\n+    var fileSegment: FileSegment = null\n+    var failures = 0\n+    var lastException: Exception = null\n+    var succeeded = false"
  }],
  "prId": 2882
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Extra space after past.\n",
    "commit": "e4bee2065293d7373c43fe5636dd9971dede257e",
    "createdAt": "2014-10-22T04:04:44Z",
    "diffHunk": "@@ -0,0 +1,223 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.util\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{ExecutionContext, Future}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.fs.permission.FsPermission\n+import org.apache.spark.Logging\n+import org.apache.spark.util.Utils\n+import WriteAheadLogManager._\n+\n+/**\n+ * This class manages write ahead log files.\n+ * - Writes records (bytebuffers) to periodically rotating log files.\n+ * - Recovers the log files and the reads the recovered records upon failures.\n+ * - Cleans up old log files.\n+ *\n+ * Uses [[org.apache.spark.streaming.util.WriteAheadLogWriter]] to write\n+ * and [[org.apache.spark.streaming.util.WriteAheadLogReader]] to read.\n+ *\n+ *@param logDirectory Directory when rotating log files will be created.\n+ * @param hadoopConf Hadoop configuration for reading/writing log files.\n+ * @param rollingIntervalSecs The interval in seconds with which logs will be rolled over.\n+ *                            Default is one minute.\n+ * @param maxFailures Max number of failures that is tolerated for every attempt to write to log.\n+ *                    Default is three.\n+ * @param callerName Optional name of the class who is using this manager.\n+ * @param clock Optional clock that is used to check for rotation interval.\n+ */\n+private[streaming] class WriteAheadLogManager(\n+    logDirectory: String,\n+    hadoopConf: Configuration,\n+    rollingIntervalSecs: Int = 60,\n+    maxFailures: Int = 3,\n+    callerName: String = \"\",\n+    clock: Clock = new SystemClock\n+  ) extends Logging {\n+\n+  private val pastLogs = new ArrayBuffer[LogInfo]\n+  private val callerNameTag =\n+    if (callerName != null && callerName.nonEmpty) s\" for $callerName\" else \"\"\n+  private val threadpoolName = s\"WriteAheadLogManager $callerNameTag\"\n+  implicit private val executionContext = ExecutionContext.fromExecutorService(\n+    Utils.newDaemonFixedThreadPool(1, threadpoolName))\n+  override protected val logName = s\"WriteAheadLogManager $callerNameTag\"\n+\n+  private var currentLogPath: String = null\n+  private var currentLogWriter: WriteAheadLogWriter = null\n+  private var currentLogWriterStartTime: Long = -1L\n+  private var currentLogWriterStopTime: Long = -1L\n+\n+  initializeOrRecover()\n+\n+  /** Write a byte buffer to the log file */\n+  def writeToLog(byteBuffer: ByteBuffer): FileSegment = synchronized {\n+    var fileSegment: FileSegment = null\n+    var failures = 0\n+    var lastException: Exception = null\n+    var succeeded = false\n+    while (!succeeded && failures < maxFailures) {\n+      try {\n+        fileSegment = getLogWriter(clock.currentTime).write(byteBuffer)\n+        succeeded = true\n+      } catch {\n+        case ex: Exception =>\n+          lastException = ex\n+          logWarning(\"Failed to write to write ahead log\")\n+          resetWriter()\n+          failures += 1\n+      }\n+    }\n+    if (fileSegment == null) {\n+      logError(s\"Failed to write to write ahead log after $failures failures\")\n+      throw lastException\n+    }\n+    fileSegment\n+  }\n+\n+  /**\n+   * Read all the existing logs from the log directory.\n+   *\n+   * Note that this is typically called when the caller is initializing and wants\n+   * to recover past  state from the write ahead logs (that is, before making any writes)."
  }],
  "prId": 2882
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Why not make `currentLogPath` into an `Option[String]`?  This seems clearer than wrapping a null into an option.\n",
    "commit": "e4bee2065293d7373c43fe5636dd9971dede257e",
    "createdAt": "2014-10-22T04:06:18Z",
    "diffHunk": "@@ -0,0 +1,223 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.util\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{ExecutionContext, Future}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.fs.permission.FsPermission\n+import org.apache.spark.Logging\n+import org.apache.spark.util.Utils\n+import WriteAheadLogManager._\n+\n+/**\n+ * This class manages write ahead log files.\n+ * - Writes records (bytebuffers) to periodically rotating log files.\n+ * - Recovers the log files and the reads the recovered records upon failures.\n+ * - Cleans up old log files.\n+ *\n+ * Uses [[org.apache.spark.streaming.util.WriteAheadLogWriter]] to write\n+ * and [[org.apache.spark.streaming.util.WriteAheadLogReader]] to read.\n+ *\n+ *@param logDirectory Directory when rotating log files will be created.\n+ * @param hadoopConf Hadoop configuration for reading/writing log files.\n+ * @param rollingIntervalSecs The interval in seconds with which logs will be rolled over.\n+ *                            Default is one minute.\n+ * @param maxFailures Max number of failures that is tolerated for every attempt to write to log.\n+ *                    Default is three.\n+ * @param callerName Optional name of the class who is using this manager.\n+ * @param clock Optional clock that is used to check for rotation interval.\n+ */\n+private[streaming] class WriteAheadLogManager(\n+    logDirectory: String,\n+    hadoopConf: Configuration,\n+    rollingIntervalSecs: Int = 60,\n+    maxFailures: Int = 3,\n+    callerName: String = \"\",\n+    clock: Clock = new SystemClock\n+  ) extends Logging {\n+\n+  private val pastLogs = new ArrayBuffer[LogInfo]\n+  private val callerNameTag =\n+    if (callerName != null && callerName.nonEmpty) s\" for $callerName\" else \"\"\n+  private val threadpoolName = s\"WriteAheadLogManager $callerNameTag\"\n+  implicit private val executionContext = ExecutionContext.fromExecutorService(\n+    Utils.newDaemonFixedThreadPool(1, threadpoolName))\n+  override protected val logName = s\"WriteAheadLogManager $callerNameTag\"\n+\n+  private var currentLogPath: String = null\n+  private var currentLogWriter: WriteAheadLogWriter = null\n+  private var currentLogWriterStartTime: Long = -1L\n+  private var currentLogWriterStopTime: Long = -1L\n+\n+  initializeOrRecover()\n+\n+  /** Write a byte buffer to the log file */\n+  def writeToLog(byteBuffer: ByteBuffer): FileSegment = synchronized {\n+    var fileSegment: FileSegment = null\n+    var failures = 0\n+    var lastException: Exception = null\n+    var succeeded = false\n+    while (!succeeded && failures < maxFailures) {\n+      try {\n+        fileSegment = getLogWriter(clock.currentTime).write(byteBuffer)\n+        succeeded = true\n+      } catch {\n+        case ex: Exception =>\n+          lastException = ex\n+          logWarning(\"Failed to write to write ahead log\")\n+          resetWriter()\n+          failures += 1\n+      }\n+    }\n+    if (fileSegment == null) {\n+      logError(s\"Failed to write to write ahead log after $failures failures\")\n+      throw lastException\n+    }\n+    fileSegment\n+  }\n+\n+  /**\n+   * Read all the existing logs from the log directory.\n+   *\n+   * Note that this is typically called when the caller is initializing and wants\n+   * to recover past  state from the write ahead logs (that is, before making any writes).\n+   * If this is called after writes have been made using this manager, then it may not return\n+   * the latest the records. This does not deal with currently active log files, and\n+   * hence the implementation is kept simple.\n+   */\n+  def readFromLog(): Iterator[ByteBuffer] = synchronized {\n+    val logFilesToRead = pastLogs.map{ _.path} ++ Option(currentLogPath)"
  }],
  "prId": 2882
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "I think you can just write `.flatten` instead of flatMapping over `identity`.\n",
    "commit": "e4bee2065293d7373c43fe5636dd9971dede257e",
    "createdAt": "2014-10-22T04:06:55Z",
    "diffHunk": "@@ -0,0 +1,223 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.util\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{ExecutionContext, Future}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.fs.permission.FsPermission\n+import org.apache.spark.Logging\n+import org.apache.spark.util.Utils\n+import WriteAheadLogManager._\n+\n+/**\n+ * This class manages write ahead log files.\n+ * - Writes records (bytebuffers) to periodically rotating log files.\n+ * - Recovers the log files and the reads the recovered records upon failures.\n+ * - Cleans up old log files.\n+ *\n+ * Uses [[org.apache.spark.streaming.util.WriteAheadLogWriter]] to write\n+ * and [[org.apache.spark.streaming.util.WriteAheadLogReader]] to read.\n+ *\n+ *@param logDirectory Directory when rotating log files will be created.\n+ * @param hadoopConf Hadoop configuration for reading/writing log files.\n+ * @param rollingIntervalSecs The interval in seconds with which logs will be rolled over.\n+ *                            Default is one minute.\n+ * @param maxFailures Max number of failures that is tolerated for every attempt to write to log.\n+ *                    Default is three.\n+ * @param callerName Optional name of the class who is using this manager.\n+ * @param clock Optional clock that is used to check for rotation interval.\n+ */\n+private[streaming] class WriteAheadLogManager(\n+    logDirectory: String,\n+    hadoopConf: Configuration,\n+    rollingIntervalSecs: Int = 60,\n+    maxFailures: Int = 3,\n+    callerName: String = \"\",\n+    clock: Clock = new SystemClock\n+  ) extends Logging {\n+\n+  private val pastLogs = new ArrayBuffer[LogInfo]\n+  private val callerNameTag =\n+    if (callerName != null && callerName.nonEmpty) s\" for $callerName\" else \"\"\n+  private val threadpoolName = s\"WriteAheadLogManager $callerNameTag\"\n+  implicit private val executionContext = ExecutionContext.fromExecutorService(\n+    Utils.newDaemonFixedThreadPool(1, threadpoolName))\n+  override protected val logName = s\"WriteAheadLogManager $callerNameTag\"\n+\n+  private var currentLogPath: String = null\n+  private var currentLogWriter: WriteAheadLogWriter = null\n+  private var currentLogWriterStartTime: Long = -1L\n+  private var currentLogWriterStopTime: Long = -1L\n+\n+  initializeOrRecover()\n+\n+  /** Write a byte buffer to the log file */\n+  def writeToLog(byteBuffer: ByteBuffer): FileSegment = synchronized {\n+    var fileSegment: FileSegment = null\n+    var failures = 0\n+    var lastException: Exception = null\n+    var succeeded = false\n+    while (!succeeded && failures < maxFailures) {\n+      try {\n+        fileSegment = getLogWriter(clock.currentTime).write(byteBuffer)\n+        succeeded = true\n+      } catch {\n+        case ex: Exception =>\n+          lastException = ex\n+          logWarning(\"Failed to write to write ahead log\")\n+          resetWriter()\n+          failures += 1\n+      }\n+    }\n+    if (fileSegment == null) {\n+      logError(s\"Failed to write to write ahead log after $failures failures\")\n+      throw lastException\n+    }\n+    fileSegment\n+  }\n+\n+  /**\n+   * Read all the existing logs from the log directory.\n+   *\n+   * Note that this is typically called when the caller is initializing and wants\n+   * to recover past  state from the write ahead logs (that is, before making any writes).\n+   * If this is called after writes have been made using this manager, then it may not return\n+   * the latest the records. This does not deal with currently active log files, and\n+   * hence the implementation is kept simple.\n+   */\n+  def readFromLog(): Iterator[ByteBuffer] = synchronized {\n+    val logFilesToRead = pastLogs.map{ _.path} ++ Option(currentLogPath)\n+    logInfo(\"Reading from the logs: \" + logFilesToRead.mkString(\"\\n\"))\n+    logFilesToRead.iterator.map { file =>\n+        logDebug(s\"Creating log reader with $file\")\n+        new WriteAheadLogReader(file, hadoopConf)\n+    } flatMap { x => x }"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Tried, for some reason cant do it. Iterator does not have flatten. \nhttp://www.scala-lang.org/api/current/index.html#scala.collection.Iterator\n",
    "commit": "e4bee2065293d7373c43fe5636dd9971dede257e",
    "createdAt": "2014-10-22T07:27:08Z",
    "diffHunk": "@@ -0,0 +1,223 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.util\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{ExecutionContext, Future}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.fs.permission.FsPermission\n+import org.apache.spark.Logging\n+import org.apache.spark.util.Utils\n+import WriteAheadLogManager._\n+\n+/**\n+ * This class manages write ahead log files.\n+ * - Writes records (bytebuffers) to periodically rotating log files.\n+ * - Recovers the log files and the reads the recovered records upon failures.\n+ * - Cleans up old log files.\n+ *\n+ * Uses [[org.apache.spark.streaming.util.WriteAheadLogWriter]] to write\n+ * and [[org.apache.spark.streaming.util.WriteAheadLogReader]] to read.\n+ *\n+ *@param logDirectory Directory when rotating log files will be created.\n+ * @param hadoopConf Hadoop configuration for reading/writing log files.\n+ * @param rollingIntervalSecs The interval in seconds with which logs will be rolled over.\n+ *                            Default is one minute.\n+ * @param maxFailures Max number of failures that is tolerated for every attempt to write to log.\n+ *                    Default is three.\n+ * @param callerName Optional name of the class who is using this manager.\n+ * @param clock Optional clock that is used to check for rotation interval.\n+ */\n+private[streaming] class WriteAheadLogManager(\n+    logDirectory: String,\n+    hadoopConf: Configuration,\n+    rollingIntervalSecs: Int = 60,\n+    maxFailures: Int = 3,\n+    callerName: String = \"\",\n+    clock: Clock = new SystemClock\n+  ) extends Logging {\n+\n+  private val pastLogs = new ArrayBuffer[LogInfo]\n+  private val callerNameTag =\n+    if (callerName != null && callerName.nonEmpty) s\" for $callerName\" else \"\"\n+  private val threadpoolName = s\"WriteAheadLogManager $callerNameTag\"\n+  implicit private val executionContext = ExecutionContext.fromExecutorService(\n+    Utils.newDaemonFixedThreadPool(1, threadpoolName))\n+  override protected val logName = s\"WriteAheadLogManager $callerNameTag\"\n+\n+  private var currentLogPath: String = null\n+  private var currentLogWriter: WriteAheadLogWriter = null\n+  private var currentLogWriterStartTime: Long = -1L\n+  private var currentLogWriterStopTime: Long = -1L\n+\n+  initializeOrRecover()\n+\n+  /** Write a byte buffer to the log file */\n+  def writeToLog(byteBuffer: ByteBuffer): FileSegment = synchronized {\n+    var fileSegment: FileSegment = null\n+    var failures = 0\n+    var lastException: Exception = null\n+    var succeeded = false\n+    while (!succeeded && failures < maxFailures) {\n+      try {\n+        fileSegment = getLogWriter(clock.currentTime).write(byteBuffer)\n+        succeeded = true\n+      } catch {\n+        case ex: Exception =>\n+          lastException = ex\n+          logWarning(\"Failed to write to write ahead log\")\n+          resetWriter()\n+          failures += 1\n+      }\n+    }\n+    if (fileSegment == null) {\n+      logError(s\"Failed to write to write ahead log after $failures failures\")\n+      throw lastException\n+    }\n+    fileSegment\n+  }\n+\n+  /**\n+   * Read all the existing logs from the log directory.\n+   *\n+   * Note that this is typically called when the caller is initializing and wants\n+   * to recover past  state from the write ahead logs (that is, before making any writes).\n+   * If this is called after writes have been made using this manager, then it may not return\n+   * the latest the records. This does not deal with currently active log files, and\n+   * hence the implementation is kept simple.\n+   */\n+  def readFromLog(): Iterator[ByteBuffer] = synchronized {\n+    val logFilesToRead = pastLogs.map{ _.path} ++ Option(currentLogPath)\n+    logInfo(\"Reading from the logs: \" + logFilesToRead.mkString(\"\\n\"))\n+    logFilesToRead.iterator.map { file =>\n+        logDebug(s\"Creating log reader with $file\")\n+        new WriteAheadLogReader(file, hadoopConf)\n+    } flatMap { x => x }"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "Ah, that makes sense, actually: `flatMap(identity)` on an iterator here is the equivalent of Python's itertools.chain, whereas `flatten` might imply materializing the flattened collection, which doesn't make sense for an iterator.\n",
    "commit": "e4bee2065293d7373c43fe5636dd9971dede257e",
    "createdAt": "2014-10-24T00:44:02Z",
    "diffHunk": "@@ -0,0 +1,223 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.util\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{ExecutionContext, Future}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.fs.permission.FsPermission\n+import org.apache.spark.Logging\n+import org.apache.spark.util.Utils\n+import WriteAheadLogManager._\n+\n+/**\n+ * This class manages write ahead log files.\n+ * - Writes records (bytebuffers) to periodically rotating log files.\n+ * - Recovers the log files and the reads the recovered records upon failures.\n+ * - Cleans up old log files.\n+ *\n+ * Uses [[org.apache.spark.streaming.util.WriteAheadLogWriter]] to write\n+ * and [[org.apache.spark.streaming.util.WriteAheadLogReader]] to read.\n+ *\n+ *@param logDirectory Directory when rotating log files will be created.\n+ * @param hadoopConf Hadoop configuration for reading/writing log files.\n+ * @param rollingIntervalSecs The interval in seconds with which logs will be rolled over.\n+ *                            Default is one minute.\n+ * @param maxFailures Max number of failures that is tolerated for every attempt to write to log.\n+ *                    Default is three.\n+ * @param callerName Optional name of the class who is using this manager.\n+ * @param clock Optional clock that is used to check for rotation interval.\n+ */\n+private[streaming] class WriteAheadLogManager(\n+    logDirectory: String,\n+    hadoopConf: Configuration,\n+    rollingIntervalSecs: Int = 60,\n+    maxFailures: Int = 3,\n+    callerName: String = \"\",\n+    clock: Clock = new SystemClock\n+  ) extends Logging {\n+\n+  private val pastLogs = new ArrayBuffer[LogInfo]\n+  private val callerNameTag =\n+    if (callerName != null && callerName.nonEmpty) s\" for $callerName\" else \"\"\n+  private val threadpoolName = s\"WriteAheadLogManager $callerNameTag\"\n+  implicit private val executionContext = ExecutionContext.fromExecutorService(\n+    Utils.newDaemonFixedThreadPool(1, threadpoolName))\n+  override protected val logName = s\"WriteAheadLogManager $callerNameTag\"\n+\n+  private var currentLogPath: String = null\n+  private var currentLogWriter: WriteAheadLogWriter = null\n+  private var currentLogWriterStartTime: Long = -1L\n+  private var currentLogWriterStopTime: Long = -1L\n+\n+  initializeOrRecover()\n+\n+  /** Write a byte buffer to the log file */\n+  def writeToLog(byteBuffer: ByteBuffer): FileSegment = synchronized {\n+    var fileSegment: FileSegment = null\n+    var failures = 0\n+    var lastException: Exception = null\n+    var succeeded = false\n+    while (!succeeded && failures < maxFailures) {\n+      try {\n+        fileSegment = getLogWriter(clock.currentTime).write(byteBuffer)\n+        succeeded = true\n+      } catch {\n+        case ex: Exception =>\n+          lastException = ex\n+          logWarning(\"Failed to write to write ahead log\")\n+          resetWriter()\n+          failures += 1\n+      }\n+    }\n+    if (fileSegment == null) {\n+      logError(s\"Failed to write to write ahead log after $failures failures\")\n+      throw lastException\n+    }\n+    fileSegment\n+  }\n+\n+  /**\n+   * Read all the existing logs from the log directory.\n+   *\n+   * Note that this is typically called when the caller is initializing and wants\n+   * to recover past  state from the write ahead logs (that is, before making any writes).\n+   * If this is called after writes have been made using this manager, then it may not return\n+   * the latest the records. This does not deal with currently active log files, and\n+   * hence the implementation is kept simple.\n+   */\n+  def readFromLog(): Iterator[ByteBuffer] = synchronized {\n+    val logFilesToRead = pastLogs.map{ _.path} ++ Option(currentLogPath)\n+    logInfo(\"Reading from the logs: \" + logFilesToRead.mkString(\"\\n\"))\n+    logFilesToRead.iterator.map { file =>\n+        logDebug(s\"Creating log reader with $file\")\n+        new WriteAheadLogReader(file, hadoopConf)\n+    } flatMap { x => x }"
  }],
  "prId": 2882
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Can these permissions be narrower?  Do we need group readability, or would user-readability suffice?  \n",
    "commit": "e4bee2065293d7373c43fe5636dd9971dede257e",
    "createdAt": "2014-10-22T04:12:18Z",
    "diffHunk": "@@ -0,0 +1,223 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.util\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{ExecutionContext, Future}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.fs.permission.FsPermission\n+import org.apache.spark.Logging\n+import org.apache.spark.util.Utils\n+import WriteAheadLogManager._\n+\n+/**\n+ * This class manages write ahead log files.\n+ * - Writes records (bytebuffers) to periodically rotating log files.\n+ * - Recovers the log files and the reads the recovered records upon failures.\n+ * - Cleans up old log files.\n+ *\n+ * Uses [[org.apache.spark.streaming.util.WriteAheadLogWriter]] to write\n+ * and [[org.apache.spark.streaming.util.WriteAheadLogReader]] to read.\n+ *\n+ *@param logDirectory Directory when rotating log files will be created.\n+ * @param hadoopConf Hadoop configuration for reading/writing log files.\n+ * @param rollingIntervalSecs The interval in seconds with which logs will be rolled over.\n+ *                            Default is one minute.\n+ * @param maxFailures Max number of failures that is tolerated for every attempt to write to log.\n+ *                    Default is three.\n+ * @param callerName Optional name of the class who is using this manager.\n+ * @param clock Optional clock that is used to check for rotation interval.\n+ */\n+private[streaming] class WriteAheadLogManager(\n+    logDirectory: String,\n+    hadoopConf: Configuration,\n+    rollingIntervalSecs: Int = 60,\n+    maxFailures: Int = 3,\n+    callerName: String = \"\",\n+    clock: Clock = new SystemClock\n+  ) extends Logging {\n+\n+  private val pastLogs = new ArrayBuffer[LogInfo]\n+  private val callerNameTag =\n+    if (callerName != null && callerName.nonEmpty) s\" for $callerName\" else \"\"\n+  private val threadpoolName = s\"WriteAheadLogManager $callerNameTag\"\n+  implicit private val executionContext = ExecutionContext.fromExecutorService(\n+    Utils.newDaemonFixedThreadPool(1, threadpoolName))\n+  override protected val logName = s\"WriteAheadLogManager $callerNameTag\"\n+\n+  private var currentLogPath: String = null\n+  private var currentLogWriter: WriteAheadLogWriter = null\n+  private var currentLogWriterStartTime: Long = -1L\n+  private var currentLogWriterStopTime: Long = -1L\n+\n+  initializeOrRecover()\n+\n+  /** Write a byte buffer to the log file */\n+  def writeToLog(byteBuffer: ByteBuffer): FileSegment = synchronized {\n+    var fileSegment: FileSegment = null\n+    var failures = 0\n+    var lastException: Exception = null\n+    var succeeded = false\n+    while (!succeeded && failures < maxFailures) {\n+      try {\n+        fileSegment = getLogWriter(clock.currentTime).write(byteBuffer)\n+        succeeded = true\n+      } catch {\n+        case ex: Exception =>\n+          lastException = ex\n+          logWarning(\"Failed to write to write ahead log\")\n+          resetWriter()\n+          failures += 1\n+      }\n+    }\n+    if (fileSegment == null) {\n+      logError(s\"Failed to write to write ahead log after $failures failures\")\n+      throw lastException\n+    }\n+    fileSegment\n+  }\n+\n+  /**\n+   * Read all the existing logs from the log directory.\n+   *\n+   * Note that this is typically called when the caller is initializing and wants\n+   * to recover past  state from the write ahead logs (that is, before making any writes).\n+   * If this is called after writes have been made using this manager, then it may not return\n+   * the latest the records. This does not deal with currently active log files, and\n+   * hence the implementation is kept simple.\n+   */\n+  def readFromLog(): Iterator[ByteBuffer] = synchronized {\n+    val logFilesToRead = pastLogs.map{ _.path} ++ Option(currentLogPath)\n+    logInfo(\"Reading from the logs: \" + logFilesToRead.mkString(\"\\n\"))\n+    logFilesToRead.iterator.map { file =>\n+        logDebug(s\"Creating log reader with $file\")\n+        new WriteAheadLogReader(file, hadoopConf)\n+    } flatMap { x => x }\n+  }\n+\n+  /**\n+   * Delete the log files that are older than the threshold time.\n+   *\n+   * Its important to note that the threshold time is based on the time stamps used in the log\n+   * files, which is usually based on the local system time. So if there is coordination necessary\n+   * between the node calculating the threshTime (say, driver node), and the local system time\n+   * (say, worker node), the caller has to take account of possible time skew.\n+   */\n+  def cleanupOldLogs(threshTime: Long): Unit = {\n+    val oldLogFiles = synchronized { pastLogs.filter { _.endTime < threshTime } }\n+    logInfo(s\"Attempting to clear ${oldLogFiles.size} old log files in $logDirectory \" +\n+      s\"older than $threshTime: ${oldLogFiles.map { _.path }.mkString(\"\\n\")}\")\n+\n+    def deleteFiles() {\n+      oldLogFiles.foreach { logInfo =>\n+        try {\n+          val path = new Path(logInfo.path)\n+          val fs = hadoopConf.synchronized { path.getFileSystem(hadoopConf) }\n+          fs.delete(path, true)\n+          synchronized { pastLogs -= logInfo }\n+          logDebug(s\"Cleared log file $logInfo\")\n+        } catch {\n+          case ex: Exception =>\n+            logWarning(s\"Error clearing write ahead log file $logInfo\", ex)\n+        }\n+      }\n+      logInfo(s\"Cleared log files in $logDirectory older than $threshTime\")\n+    }\n+    if (!executionContext.isShutdown) {\n+      Future { deleteFiles() }\n+    }\n+  }\n+\n+  /** Stop the manager, close any open log writer */\n+  def stop(): Unit = synchronized {\n+    if (currentLogWriter != null) {\n+      currentLogWriter.close()\n+    }\n+    executionContext.shutdown()\n+    logInfo(\"Stopped write ahead log manager\")\n+  }\n+\n+  /** Get the current log writer while taking care of rotation */\n+  private def getLogWriter(currentTime: Long): WriteAheadLogWriter = synchronized {\n+    if (currentLogWriter == null || currentTime > currentLogWriterStopTime) {\n+      resetWriter()\n+      if (currentLogPath != null) {\n+        pastLogs += LogInfo(currentLogWriterStartTime, currentLogWriterStopTime, currentLogPath)\n+      }\n+      currentLogWriterStartTime = currentTime\n+      currentLogWriterStopTime = currentTime + (rollingIntervalSecs * 1000)\n+      val newLogPath = new Path(logDirectory,\n+        timeToLogFile(currentLogWriterStartTime, currentLogWriterStopTime))\n+      currentLogPath = newLogPath.toString\n+      currentLogWriter = new WriteAheadLogWriter(currentLogPath, hadoopConf)\n+    }\n+    currentLogWriter\n+  }\n+\n+  /** Initialize the log directory or recover existing logs inside the directory */\n+  private def initializeOrRecover(): Unit = synchronized {\n+    val logDirectoryPath = new Path(logDirectory)\n+    val fileSystem = logDirectoryPath.getFileSystem(hadoopConf)\n+\n+    if (fileSystem.exists(logDirectoryPath) && fileSystem.getFileStatus(logDirectoryPath).isDir) {\n+      val logFileInfo = logFilesTologInfo(fileSystem.listStatus(logDirectoryPath).map { _.getPath })\n+      pastLogs.clear()\n+      pastLogs ++= logFileInfo\n+      logInfo(s\"Recovered ${logFileInfo.size} write ahead log files from $logDirectory\")\n+      logDebug(s\"Recovered files are:\\n${logFileInfo.map(_.path).mkString(\"\\n\")}\")\n+    } else {\n+      fileSystem.mkdirs(logDirectoryPath,\n+        FsPermission.createImmutable(Integer.parseInt(\"770\", 8).toShort))"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "I copied the permissions from a similar functionality\nhttps://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala#L145\n",
    "commit": "e4bee2065293d7373c43fe5636dd9971dede257e",
    "createdAt": "2014-10-22T07:31:02Z",
    "diffHunk": "@@ -0,0 +1,223 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.util\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{ExecutionContext, Future}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.fs.permission.FsPermission\n+import org.apache.spark.Logging\n+import org.apache.spark.util.Utils\n+import WriteAheadLogManager._\n+\n+/**\n+ * This class manages write ahead log files.\n+ * - Writes records (bytebuffers) to periodically rotating log files.\n+ * - Recovers the log files and the reads the recovered records upon failures.\n+ * - Cleans up old log files.\n+ *\n+ * Uses [[org.apache.spark.streaming.util.WriteAheadLogWriter]] to write\n+ * and [[org.apache.spark.streaming.util.WriteAheadLogReader]] to read.\n+ *\n+ *@param logDirectory Directory when rotating log files will be created.\n+ * @param hadoopConf Hadoop configuration for reading/writing log files.\n+ * @param rollingIntervalSecs The interval in seconds with which logs will be rolled over.\n+ *                            Default is one minute.\n+ * @param maxFailures Max number of failures that is tolerated for every attempt to write to log.\n+ *                    Default is three.\n+ * @param callerName Optional name of the class who is using this manager.\n+ * @param clock Optional clock that is used to check for rotation interval.\n+ */\n+private[streaming] class WriteAheadLogManager(\n+    logDirectory: String,\n+    hadoopConf: Configuration,\n+    rollingIntervalSecs: Int = 60,\n+    maxFailures: Int = 3,\n+    callerName: String = \"\",\n+    clock: Clock = new SystemClock\n+  ) extends Logging {\n+\n+  private val pastLogs = new ArrayBuffer[LogInfo]\n+  private val callerNameTag =\n+    if (callerName != null && callerName.nonEmpty) s\" for $callerName\" else \"\"\n+  private val threadpoolName = s\"WriteAheadLogManager $callerNameTag\"\n+  implicit private val executionContext = ExecutionContext.fromExecutorService(\n+    Utils.newDaemonFixedThreadPool(1, threadpoolName))\n+  override protected val logName = s\"WriteAheadLogManager $callerNameTag\"\n+\n+  private var currentLogPath: String = null\n+  private var currentLogWriter: WriteAheadLogWriter = null\n+  private var currentLogWriterStartTime: Long = -1L\n+  private var currentLogWriterStopTime: Long = -1L\n+\n+  initializeOrRecover()\n+\n+  /** Write a byte buffer to the log file */\n+  def writeToLog(byteBuffer: ByteBuffer): FileSegment = synchronized {\n+    var fileSegment: FileSegment = null\n+    var failures = 0\n+    var lastException: Exception = null\n+    var succeeded = false\n+    while (!succeeded && failures < maxFailures) {\n+      try {\n+        fileSegment = getLogWriter(clock.currentTime).write(byteBuffer)\n+        succeeded = true\n+      } catch {\n+        case ex: Exception =>\n+          lastException = ex\n+          logWarning(\"Failed to write to write ahead log\")\n+          resetWriter()\n+          failures += 1\n+      }\n+    }\n+    if (fileSegment == null) {\n+      logError(s\"Failed to write to write ahead log after $failures failures\")\n+      throw lastException\n+    }\n+    fileSegment\n+  }\n+\n+  /**\n+   * Read all the existing logs from the log directory.\n+   *\n+   * Note that this is typically called when the caller is initializing and wants\n+   * to recover past  state from the write ahead logs (that is, before making any writes).\n+   * If this is called after writes have been made using this manager, then it may not return\n+   * the latest the records. This does not deal with currently active log files, and\n+   * hence the implementation is kept simple.\n+   */\n+  def readFromLog(): Iterator[ByteBuffer] = synchronized {\n+    val logFilesToRead = pastLogs.map{ _.path} ++ Option(currentLogPath)\n+    logInfo(\"Reading from the logs: \" + logFilesToRead.mkString(\"\\n\"))\n+    logFilesToRead.iterator.map { file =>\n+        logDebug(s\"Creating log reader with $file\")\n+        new WriteAheadLogReader(file, hadoopConf)\n+    } flatMap { x => x }\n+  }\n+\n+  /**\n+   * Delete the log files that are older than the threshold time.\n+   *\n+   * Its important to note that the threshold time is based on the time stamps used in the log\n+   * files, which is usually based on the local system time. So if there is coordination necessary\n+   * between the node calculating the threshTime (say, driver node), and the local system time\n+   * (say, worker node), the caller has to take account of possible time skew.\n+   */\n+  def cleanupOldLogs(threshTime: Long): Unit = {\n+    val oldLogFiles = synchronized { pastLogs.filter { _.endTime < threshTime } }\n+    logInfo(s\"Attempting to clear ${oldLogFiles.size} old log files in $logDirectory \" +\n+      s\"older than $threshTime: ${oldLogFiles.map { _.path }.mkString(\"\\n\")}\")\n+\n+    def deleteFiles() {\n+      oldLogFiles.foreach { logInfo =>\n+        try {\n+          val path = new Path(logInfo.path)\n+          val fs = hadoopConf.synchronized { path.getFileSystem(hadoopConf) }\n+          fs.delete(path, true)\n+          synchronized { pastLogs -= logInfo }\n+          logDebug(s\"Cleared log file $logInfo\")\n+        } catch {\n+          case ex: Exception =>\n+            logWarning(s\"Error clearing write ahead log file $logInfo\", ex)\n+        }\n+      }\n+      logInfo(s\"Cleared log files in $logDirectory older than $threshTime\")\n+    }\n+    if (!executionContext.isShutdown) {\n+      Future { deleteFiles() }\n+    }\n+  }\n+\n+  /** Stop the manager, close any open log writer */\n+  def stop(): Unit = synchronized {\n+    if (currentLogWriter != null) {\n+      currentLogWriter.close()\n+    }\n+    executionContext.shutdown()\n+    logInfo(\"Stopped write ahead log manager\")\n+  }\n+\n+  /** Get the current log writer while taking care of rotation */\n+  private def getLogWriter(currentTime: Long): WriteAheadLogWriter = synchronized {\n+    if (currentLogWriter == null || currentTime > currentLogWriterStopTime) {\n+      resetWriter()\n+      if (currentLogPath != null) {\n+        pastLogs += LogInfo(currentLogWriterStartTime, currentLogWriterStopTime, currentLogPath)\n+      }\n+      currentLogWriterStartTime = currentTime\n+      currentLogWriterStopTime = currentTime + (rollingIntervalSecs * 1000)\n+      val newLogPath = new Path(logDirectory,\n+        timeToLogFile(currentLogWriterStartTime, currentLogWriterStopTime))\n+      currentLogPath = newLogPath.toString\n+      currentLogWriter = new WriteAheadLogWriter(currentLogPath, hadoopConf)\n+    }\n+    currentLogWriter\n+  }\n+\n+  /** Initialize the log directory or recover existing logs inside the directory */\n+  private def initializeOrRecover(): Unit = synchronized {\n+    val logDirectoryPath = new Path(logDirectory)\n+    val fileSystem = logDirectoryPath.getFileSystem(hadoopConf)\n+\n+    if (fileSystem.exists(logDirectoryPath) && fileSystem.getFileStatus(logDirectoryPath).isDir) {\n+      val logFileInfo = logFilesTologInfo(fileSystem.listStatus(logDirectoryPath).map { _.getPath })\n+      pastLogs.clear()\n+      pastLogs ++= logFileInfo\n+      logInfo(s\"Recovered ${logFileInfo.size} write ahead log files from $logDirectory\")\n+      logDebug(s\"Recovered files are:\\n${logFileInfo.map(_.path).mkString(\"\\n\")}\")\n+    } else {\n+      fileSystem.mkdirs(logDirectoryPath,\n+        FsPermission.createImmutable(Integer.parseInt(\"770\", 8).toShort))"
  }, {
    "author": {
      "login": "harishreedharan"
    },
    "body": "For now I am removing the mkdirs call. When running on minicluster it fails. Also we don't actually need to create a directory with HDFS. If we simply create a file in the directory, HDFS will create the directory with perms for the current user to read/write.\n",
    "commit": "e4bee2065293d7373c43fe5636dd9971dede257e",
    "createdAt": "2014-10-22T20:29:24Z",
    "diffHunk": "@@ -0,0 +1,223 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.util\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{ExecutionContext, Future}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.fs.permission.FsPermission\n+import org.apache.spark.Logging\n+import org.apache.spark.util.Utils\n+import WriteAheadLogManager._\n+\n+/**\n+ * This class manages write ahead log files.\n+ * - Writes records (bytebuffers) to periodically rotating log files.\n+ * - Recovers the log files and the reads the recovered records upon failures.\n+ * - Cleans up old log files.\n+ *\n+ * Uses [[org.apache.spark.streaming.util.WriteAheadLogWriter]] to write\n+ * and [[org.apache.spark.streaming.util.WriteAheadLogReader]] to read.\n+ *\n+ *@param logDirectory Directory when rotating log files will be created.\n+ * @param hadoopConf Hadoop configuration for reading/writing log files.\n+ * @param rollingIntervalSecs The interval in seconds with which logs will be rolled over.\n+ *                            Default is one minute.\n+ * @param maxFailures Max number of failures that is tolerated for every attempt to write to log.\n+ *                    Default is three.\n+ * @param callerName Optional name of the class who is using this manager.\n+ * @param clock Optional clock that is used to check for rotation interval.\n+ */\n+private[streaming] class WriteAheadLogManager(\n+    logDirectory: String,\n+    hadoopConf: Configuration,\n+    rollingIntervalSecs: Int = 60,\n+    maxFailures: Int = 3,\n+    callerName: String = \"\",\n+    clock: Clock = new SystemClock\n+  ) extends Logging {\n+\n+  private val pastLogs = new ArrayBuffer[LogInfo]\n+  private val callerNameTag =\n+    if (callerName != null && callerName.nonEmpty) s\" for $callerName\" else \"\"\n+  private val threadpoolName = s\"WriteAheadLogManager $callerNameTag\"\n+  implicit private val executionContext = ExecutionContext.fromExecutorService(\n+    Utils.newDaemonFixedThreadPool(1, threadpoolName))\n+  override protected val logName = s\"WriteAheadLogManager $callerNameTag\"\n+\n+  private var currentLogPath: String = null\n+  private var currentLogWriter: WriteAheadLogWriter = null\n+  private var currentLogWriterStartTime: Long = -1L\n+  private var currentLogWriterStopTime: Long = -1L\n+\n+  initializeOrRecover()\n+\n+  /** Write a byte buffer to the log file */\n+  def writeToLog(byteBuffer: ByteBuffer): FileSegment = synchronized {\n+    var fileSegment: FileSegment = null\n+    var failures = 0\n+    var lastException: Exception = null\n+    var succeeded = false\n+    while (!succeeded && failures < maxFailures) {\n+      try {\n+        fileSegment = getLogWriter(clock.currentTime).write(byteBuffer)\n+        succeeded = true\n+      } catch {\n+        case ex: Exception =>\n+          lastException = ex\n+          logWarning(\"Failed to write to write ahead log\")\n+          resetWriter()\n+          failures += 1\n+      }\n+    }\n+    if (fileSegment == null) {\n+      logError(s\"Failed to write to write ahead log after $failures failures\")\n+      throw lastException\n+    }\n+    fileSegment\n+  }\n+\n+  /**\n+   * Read all the existing logs from the log directory.\n+   *\n+   * Note that this is typically called when the caller is initializing and wants\n+   * to recover past  state from the write ahead logs (that is, before making any writes).\n+   * If this is called after writes have been made using this manager, then it may not return\n+   * the latest the records. This does not deal with currently active log files, and\n+   * hence the implementation is kept simple.\n+   */\n+  def readFromLog(): Iterator[ByteBuffer] = synchronized {\n+    val logFilesToRead = pastLogs.map{ _.path} ++ Option(currentLogPath)\n+    logInfo(\"Reading from the logs: \" + logFilesToRead.mkString(\"\\n\"))\n+    logFilesToRead.iterator.map { file =>\n+        logDebug(s\"Creating log reader with $file\")\n+        new WriteAheadLogReader(file, hadoopConf)\n+    } flatMap { x => x }\n+  }\n+\n+  /**\n+   * Delete the log files that are older than the threshold time.\n+   *\n+   * Its important to note that the threshold time is based on the time stamps used in the log\n+   * files, which is usually based on the local system time. So if there is coordination necessary\n+   * between the node calculating the threshTime (say, driver node), and the local system time\n+   * (say, worker node), the caller has to take account of possible time skew.\n+   */\n+  def cleanupOldLogs(threshTime: Long): Unit = {\n+    val oldLogFiles = synchronized { pastLogs.filter { _.endTime < threshTime } }\n+    logInfo(s\"Attempting to clear ${oldLogFiles.size} old log files in $logDirectory \" +\n+      s\"older than $threshTime: ${oldLogFiles.map { _.path }.mkString(\"\\n\")}\")\n+\n+    def deleteFiles() {\n+      oldLogFiles.foreach { logInfo =>\n+        try {\n+          val path = new Path(logInfo.path)\n+          val fs = hadoopConf.synchronized { path.getFileSystem(hadoopConf) }\n+          fs.delete(path, true)\n+          synchronized { pastLogs -= logInfo }\n+          logDebug(s\"Cleared log file $logInfo\")\n+        } catch {\n+          case ex: Exception =>\n+            logWarning(s\"Error clearing write ahead log file $logInfo\", ex)\n+        }\n+      }\n+      logInfo(s\"Cleared log files in $logDirectory older than $threshTime\")\n+    }\n+    if (!executionContext.isShutdown) {\n+      Future { deleteFiles() }\n+    }\n+  }\n+\n+  /** Stop the manager, close any open log writer */\n+  def stop(): Unit = synchronized {\n+    if (currentLogWriter != null) {\n+      currentLogWriter.close()\n+    }\n+    executionContext.shutdown()\n+    logInfo(\"Stopped write ahead log manager\")\n+  }\n+\n+  /** Get the current log writer while taking care of rotation */\n+  private def getLogWriter(currentTime: Long): WriteAheadLogWriter = synchronized {\n+    if (currentLogWriter == null || currentTime > currentLogWriterStopTime) {\n+      resetWriter()\n+      if (currentLogPath != null) {\n+        pastLogs += LogInfo(currentLogWriterStartTime, currentLogWriterStopTime, currentLogPath)\n+      }\n+      currentLogWriterStartTime = currentTime\n+      currentLogWriterStopTime = currentTime + (rollingIntervalSecs * 1000)\n+      val newLogPath = new Path(logDirectory,\n+        timeToLogFile(currentLogWriterStartTime, currentLogWriterStopTime))\n+      currentLogPath = newLogPath.toString\n+      currentLogWriter = new WriteAheadLogWriter(currentLogPath, hadoopConf)\n+    }\n+    currentLogWriter\n+  }\n+\n+  /** Initialize the log directory or recover existing logs inside the directory */\n+  private def initializeOrRecover(): Unit = synchronized {\n+    val logDirectoryPath = new Path(logDirectory)\n+    val fileSystem = logDirectoryPath.getFileSystem(hadoopConf)\n+\n+    if (fileSystem.exists(logDirectoryPath) && fileSystem.getFileStatus(logDirectoryPath).isDir) {\n+      val logFileInfo = logFilesTologInfo(fileSystem.listStatus(logDirectoryPath).map { _.getPath })\n+      pastLogs.clear()\n+      pastLogs ++= logFileInfo\n+      logInfo(s\"Recovered ${logFileInfo.size} write ahead log files from $logDirectory\")\n+      logDebug(s\"Recovered files are:\\n${logFileInfo.map(_.path).mkString(\"\\n\")}\")\n+    } else {\n+      fileSystem.mkdirs(logDirectoryPath,\n+        FsPermission.createImmutable(Integer.parseInt(\"770\", 8).toShort))"
  }],
  "prId": 2882
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Don't need the curly braces for this string interpolation.\n",
    "commit": "e4bee2065293d7373c43fe5636dd9971dede257e",
    "createdAt": "2014-10-22T04:12:49Z",
    "diffHunk": "@@ -0,0 +1,223 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.util\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{ExecutionContext, Future}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.fs.permission.FsPermission\n+import org.apache.spark.Logging\n+import org.apache.spark.util.Utils\n+import WriteAheadLogManager._\n+\n+/**\n+ * This class manages write ahead log files.\n+ * - Writes records (bytebuffers) to periodically rotating log files.\n+ * - Recovers the log files and the reads the recovered records upon failures.\n+ * - Cleans up old log files.\n+ *\n+ * Uses [[org.apache.spark.streaming.util.WriteAheadLogWriter]] to write\n+ * and [[org.apache.spark.streaming.util.WriteAheadLogReader]] to read.\n+ *\n+ *@param logDirectory Directory when rotating log files will be created.\n+ * @param hadoopConf Hadoop configuration for reading/writing log files.\n+ * @param rollingIntervalSecs The interval in seconds with which logs will be rolled over.\n+ *                            Default is one minute.\n+ * @param maxFailures Max number of failures that is tolerated for every attempt to write to log.\n+ *                    Default is three.\n+ * @param callerName Optional name of the class who is using this manager.\n+ * @param clock Optional clock that is used to check for rotation interval.\n+ */\n+private[streaming] class WriteAheadLogManager(\n+    logDirectory: String,\n+    hadoopConf: Configuration,\n+    rollingIntervalSecs: Int = 60,\n+    maxFailures: Int = 3,\n+    callerName: String = \"\",\n+    clock: Clock = new SystemClock\n+  ) extends Logging {\n+\n+  private val pastLogs = new ArrayBuffer[LogInfo]\n+  private val callerNameTag =\n+    if (callerName != null && callerName.nonEmpty) s\" for $callerName\" else \"\"\n+  private val threadpoolName = s\"WriteAheadLogManager $callerNameTag\"\n+  implicit private val executionContext = ExecutionContext.fromExecutorService(\n+    Utils.newDaemonFixedThreadPool(1, threadpoolName))\n+  override protected val logName = s\"WriteAheadLogManager $callerNameTag\"\n+\n+  private var currentLogPath: String = null\n+  private var currentLogWriter: WriteAheadLogWriter = null\n+  private var currentLogWriterStartTime: Long = -1L\n+  private var currentLogWriterStopTime: Long = -1L\n+\n+  initializeOrRecover()\n+\n+  /** Write a byte buffer to the log file */\n+  def writeToLog(byteBuffer: ByteBuffer): FileSegment = synchronized {\n+    var fileSegment: FileSegment = null\n+    var failures = 0\n+    var lastException: Exception = null\n+    var succeeded = false\n+    while (!succeeded && failures < maxFailures) {\n+      try {\n+        fileSegment = getLogWriter(clock.currentTime).write(byteBuffer)\n+        succeeded = true\n+      } catch {\n+        case ex: Exception =>\n+          lastException = ex\n+          logWarning(\"Failed to write to write ahead log\")\n+          resetWriter()\n+          failures += 1\n+      }\n+    }\n+    if (fileSegment == null) {\n+      logError(s\"Failed to write to write ahead log after $failures failures\")\n+      throw lastException\n+    }\n+    fileSegment\n+  }\n+\n+  /**\n+   * Read all the existing logs from the log directory.\n+   *\n+   * Note that this is typically called when the caller is initializing and wants\n+   * to recover past  state from the write ahead logs (that is, before making any writes).\n+   * If this is called after writes have been made using this manager, then it may not return\n+   * the latest the records. This does not deal with currently active log files, and\n+   * hence the implementation is kept simple.\n+   */\n+  def readFromLog(): Iterator[ByteBuffer] = synchronized {\n+    val logFilesToRead = pastLogs.map{ _.path} ++ Option(currentLogPath)\n+    logInfo(\"Reading from the logs: \" + logFilesToRead.mkString(\"\\n\"))\n+    logFilesToRead.iterator.map { file =>\n+        logDebug(s\"Creating log reader with $file\")\n+        new WriteAheadLogReader(file, hadoopConf)\n+    } flatMap { x => x }\n+  }\n+\n+  /**\n+   * Delete the log files that are older than the threshold time.\n+   *\n+   * Its important to note that the threshold time is based on the time stamps used in the log\n+   * files, which is usually based on the local system time. So if there is coordination necessary\n+   * between the node calculating the threshTime (say, driver node), and the local system time\n+   * (say, worker node), the caller has to take account of possible time skew.\n+   */\n+  def cleanupOldLogs(threshTime: Long): Unit = {\n+    val oldLogFiles = synchronized { pastLogs.filter { _.endTime < threshTime } }\n+    logInfo(s\"Attempting to clear ${oldLogFiles.size} old log files in $logDirectory \" +\n+      s\"older than $threshTime: ${oldLogFiles.map { _.path }.mkString(\"\\n\")}\")\n+\n+    def deleteFiles() {\n+      oldLogFiles.foreach { logInfo =>\n+        try {\n+          val path = new Path(logInfo.path)\n+          val fs = hadoopConf.synchronized { path.getFileSystem(hadoopConf) }\n+          fs.delete(path, true)\n+          synchronized { pastLogs -= logInfo }\n+          logDebug(s\"Cleared log file $logInfo\")\n+        } catch {\n+          case ex: Exception =>\n+            logWarning(s\"Error clearing write ahead log file $logInfo\", ex)\n+        }\n+      }\n+      logInfo(s\"Cleared log files in $logDirectory older than $threshTime\")\n+    }\n+    if (!executionContext.isShutdown) {\n+      Future { deleteFiles() }\n+    }\n+  }\n+\n+  /** Stop the manager, close any open log writer */\n+  def stop(): Unit = synchronized {\n+    if (currentLogWriter != null) {\n+      currentLogWriter.close()\n+    }\n+    executionContext.shutdown()\n+    logInfo(\"Stopped write ahead log manager\")\n+  }\n+\n+  /** Get the current log writer while taking care of rotation */\n+  private def getLogWriter(currentTime: Long): WriteAheadLogWriter = synchronized {\n+    if (currentLogWriter == null || currentTime > currentLogWriterStopTime) {\n+      resetWriter()\n+      if (currentLogPath != null) {\n+        pastLogs += LogInfo(currentLogWriterStartTime, currentLogWriterStopTime, currentLogPath)\n+      }\n+      currentLogWriterStartTime = currentTime\n+      currentLogWriterStopTime = currentTime + (rollingIntervalSecs * 1000)\n+      val newLogPath = new Path(logDirectory,\n+        timeToLogFile(currentLogWriterStartTime, currentLogWriterStopTime))\n+      currentLogPath = newLogPath.toString\n+      currentLogWriter = new WriteAheadLogWriter(currentLogPath, hadoopConf)\n+    }\n+    currentLogWriter\n+  }\n+\n+  /** Initialize the log directory or recover existing logs inside the directory */\n+  private def initializeOrRecover(): Unit = synchronized {\n+    val logDirectoryPath = new Path(logDirectory)\n+    val fileSystem = logDirectoryPath.getFileSystem(hadoopConf)\n+\n+    if (fileSystem.exists(logDirectoryPath) && fileSystem.getFileStatus(logDirectoryPath).isDir) {\n+      val logFileInfo = logFilesTologInfo(fileSystem.listStatus(logDirectoryPath).map { _.getPath })\n+      pastLogs.clear()\n+      pastLogs ++= logFileInfo\n+      logInfo(s\"Recovered ${logFileInfo.size} write ahead log files from $logDirectory\")\n+      logDebug(s\"Recovered files are:\\n${logFileInfo.map(_.path).mkString(\"\\n\")}\")\n+    } else {\n+      fileSystem.mkdirs(logDirectoryPath,\n+        FsPermission.createImmutable(Integer.parseInt(\"770\", 8).toShort))\n+      logInfo(s\"Created ${logDirectory} for write ahead log files\")"
  }, {
    "author": {
      "login": "harishreedharan"
    },
    "body": "Removed as part of change above.\n",
    "commit": "e4bee2065293d7373c43fe5636dd9971dede257e",
    "createdAt": "2014-10-22T20:30:54Z",
    "diffHunk": "@@ -0,0 +1,223 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.util\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{ExecutionContext, Future}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.fs.permission.FsPermission\n+import org.apache.spark.Logging\n+import org.apache.spark.util.Utils\n+import WriteAheadLogManager._\n+\n+/**\n+ * This class manages write ahead log files.\n+ * - Writes records (bytebuffers) to periodically rotating log files.\n+ * - Recovers the log files and the reads the recovered records upon failures.\n+ * - Cleans up old log files.\n+ *\n+ * Uses [[org.apache.spark.streaming.util.WriteAheadLogWriter]] to write\n+ * and [[org.apache.spark.streaming.util.WriteAheadLogReader]] to read.\n+ *\n+ *@param logDirectory Directory when rotating log files will be created.\n+ * @param hadoopConf Hadoop configuration for reading/writing log files.\n+ * @param rollingIntervalSecs The interval in seconds with which logs will be rolled over.\n+ *                            Default is one minute.\n+ * @param maxFailures Max number of failures that is tolerated for every attempt to write to log.\n+ *                    Default is three.\n+ * @param callerName Optional name of the class who is using this manager.\n+ * @param clock Optional clock that is used to check for rotation interval.\n+ */\n+private[streaming] class WriteAheadLogManager(\n+    logDirectory: String,\n+    hadoopConf: Configuration,\n+    rollingIntervalSecs: Int = 60,\n+    maxFailures: Int = 3,\n+    callerName: String = \"\",\n+    clock: Clock = new SystemClock\n+  ) extends Logging {\n+\n+  private val pastLogs = new ArrayBuffer[LogInfo]\n+  private val callerNameTag =\n+    if (callerName != null && callerName.nonEmpty) s\" for $callerName\" else \"\"\n+  private val threadpoolName = s\"WriteAheadLogManager $callerNameTag\"\n+  implicit private val executionContext = ExecutionContext.fromExecutorService(\n+    Utils.newDaemonFixedThreadPool(1, threadpoolName))\n+  override protected val logName = s\"WriteAheadLogManager $callerNameTag\"\n+\n+  private var currentLogPath: String = null\n+  private var currentLogWriter: WriteAheadLogWriter = null\n+  private var currentLogWriterStartTime: Long = -1L\n+  private var currentLogWriterStopTime: Long = -1L\n+\n+  initializeOrRecover()\n+\n+  /** Write a byte buffer to the log file */\n+  def writeToLog(byteBuffer: ByteBuffer): FileSegment = synchronized {\n+    var fileSegment: FileSegment = null\n+    var failures = 0\n+    var lastException: Exception = null\n+    var succeeded = false\n+    while (!succeeded && failures < maxFailures) {\n+      try {\n+        fileSegment = getLogWriter(clock.currentTime).write(byteBuffer)\n+        succeeded = true\n+      } catch {\n+        case ex: Exception =>\n+          lastException = ex\n+          logWarning(\"Failed to write to write ahead log\")\n+          resetWriter()\n+          failures += 1\n+      }\n+    }\n+    if (fileSegment == null) {\n+      logError(s\"Failed to write to write ahead log after $failures failures\")\n+      throw lastException\n+    }\n+    fileSegment\n+  }\n+\n+  /**\n+   * Read all the existing logs from the log directory.\n+   *\n+   * Note that this is typically called when the caller is initializing and wants\n+   * to recover past  state from the write ahead logs (that is, before making any writes).\n+   * If this is called after writes have been made using this manager, then it may not return\n+   * the latest the records. This does not deal with currently active log files, and\n+   * hence the implementation is kept simple.\n+   */\n+  def readFromLog(): Iterator[ByteBuffer] = synchronized {\n+    val logFilesToRead = pastLogs.map{ _.path} ++ Option(currentLogPath)\n+    logInfo(\"Reading from the logs: \" + logFilesToRead.mkString(\"\\n\"))\n+    logFilesToRead.iterator.map { file =>\n+        logDebug(s\"Creating log reader with $file\")\n+        new WriteAheadLogReader(file, hadoopConf)\n+    } flatMap { x => x }\n+  }\n+\n+  /**\n+   * Delete the log files that are older than the threshold time.\n+   *\n+   * Its important to note that the threshold time is based on the time stamps used in the log\n+   * files, which is usually based on the local system time. So if there is coordination necessary\n+   * between the node calculating the threshTime (say, driver node), and the local system time\n+   * (say, worker node), the caller has to take account of possible time skew.\n+   */\n+  def cleanupOldLogs(threshTime: Long): Unit = {\n+    val oldLogFiles = synchronized { pastLogs.filter { _.endTime < threshTime } }\n+    logInfo(s\"Attempting to clear ${oldLogFiles.size} old log files in $logDirectory \" +\n+      s\"older than $threshTime: ${oldLogFiles.map { _.path }.mkString(\"\\n\")}\")\n+\n+    def deleteFiles() {\n+      oldLogFiles.foreach { logInfo =>\n+        try {\n+          val path = new Path(logInfo.path)\n+          val fs = hadoopConf.synchronized { path.getFileSystem(hadoopConf) }\n+          fs.delete(path, true)\n+          synchronized { pastLogs -= logInfo }\n+          logDebug(s\"Cleared log file $logInfo\")\n+        } catch {\n+          case ex: Exception =>\n+            logWarning(s\"Error clearing write ahead log file $logInfo\", ex)\n+        }\n+      }\n+      logInfo(s\"Cleared log files in $logDirectory older than $threshTime\")\n+    }\n+    if (!executionContext.isShutdown) {\n+      Future { deleteFiles() }\n+    }\n+  }\n+\n+  /** Stop the manager, close any open log writer */\n+  def stop(): Unit = synchronized {\n+    if (currentLogWriter != null) {\n+      currentLogWriter.close()\n+    }\n+    executionContext.shutdown()\n+    logInfo(\"Stopped write ahead log manager\")\n+  }\n+\n+  /** Get the current log writer while taking care of rotation */\n+  private def getLogWriter(currentTime: Long): WriteAheadLogWriter = synchronized {\n+    if (currentLogWriter == null || currentTime > currentLogWriterStopTime) {\n+      resetWriter()\n+      if (currentLogPath != null) {\n+        pastLogs += LogInfo(currentLogWriterStartTime, currentLogWriterStopTime, currentLogPath)\n+      }\n+      currentLogWriterStartTime = currentTime\n+      currentLogWriterStopTime = currentTime + (rollingIntervalSecs * 1000)\n+      val newLogPath = new Path(logDirectory,\n+        timeToLogFile(currentLogWriterStartTime, currentLogWriterStopTime))\n+      currentLogPath = newLogPath.toString\n+      currentLogWriter = new WriteAheadLogWriter(currentLogPath, hadoopConf)\n+    }\n+    currentLogWriter\n+  }\n+\n+  /** Initialize the log directory or recover existing logs inside the directory */\n+  private def initializeOrRecover(): Unit = synchronized {\n+    val logDirectoryPath = new Path(logDirectory)\n+    val fileSystem = logDirectoryPath.getFileSystem(hadoopConf)\n+\n+    if (fileSystem.exists(logDirectoryPath) && fileSystem.getFileStatus(logDirectoryPath).isDir) {\n+      val logFileInfo = logFilesTologInfo(fileSystem.listStatus(logDirectoryPath).map { _.getPath })\n+      pastLogs.clear()\n+      pastLogs ++= logFileInfo\n+      logInfo(s\"Recovered ${logFileInfo.size} write ahead log files from $logDirectory\")\n+      logDebug(s\"Recovered files are:\\n${logFileInfo.map(_.path).mkString(\"\\n\")}\")\n+    } else {\n+      fileSystem.mkdirs(logDirectoryPath,\n+        FsPermission.createImmutable(Integer.parseInt(\"770\", 8).toShort))\n+      logInfo(s\"Created ${logDirectory} for write ahead log files\")"
  }],
  "prId": 2882
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "It might be nice to explicitly state that this write is synchronous / blocking, if that's the case.  If I call this and it returns, am I guaranteed that my log record is durably stored in HDFS and readable during recovery?\n",
    "commit": "e4bee2065293d7373c43fe5636dd9971dede257e",
    "createdAt": "2014-10-22T05:41:25Z",
    "diffHunk": "@@ -0,0 +1,223 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.util\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{ExecutionContext, Future}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.fs.permission.FsPermission\n+import org.apache.spark.Logging\n+import org.apache.spark.util.Utils\n+import WriteAheadLogManager._\n+\n+/**\n+ * This class manages write ahead log files.\n+ * - Writes records (bytebuffers) to periodically rotating log files.\n+ * - Recovers the log files and the reads the recovered records upon failures.\n+ * - Cleans up old log files.\n+ *\n+ * Uses [[org.apache.spark.streaming.util.WriteAheadLogWriter]] to write\n+ * and [[org.apache.spark.streaming.util.WriteAheadLogReader]] to read.\n+ *\n+ *@param logDirectory Directory when rotating log files will be created.\n+ * @param hadoopConf Hadoop configuration for reading/writing log files.\n+ * @param rollingIntervalSecs The interval in seconds with which logs will be rolled over.\n+ *                            Default is one minute.\n+ * @param maxFailures Max number of failures that is tolerated for every attempt to write to log.\n+ *                    Default is three.\n+ * @param callerName Optional name of the class who is using this manager.\n+ * @param clock Optional clock that is used to check for rotation interval.\n+ */\n+private[streaming] class WriteAheadLogManager(\n+    logDirectory: String,\n+    hadoopConf: Configuration,\n+    rollingIntervalSecs: Int = 60,\n+    maxFailures: Int = 3,\n+    callerName: String = \"\",\n+    clock: Clock = new SystemClock\n+  ) extends Logging {\n+\n+  private val pastLogs = new ArrayBuffer[LogInfo]\n+  private val callerNameTag =\n+    if (callerName != null && callerName.nonEmpty) s\" for $callerName\" else \"\"\n+  private val threadpoolName = s\"WriteAheadLogManager $callerNameTag\"\n+  implicit private val executionContext = ExecutionContext.fromExecutorService(\n+    Utils.newDaemonFixedThreadPool(1, threadpoolName))\n+  override protected val logName = s\"WriteAheadLogManager $callerNameTag\"\n+\n+  private var currentLogPath: String = null\n+  private var currentLogWriter: WriteAheadLogWriter = null\n+  private var currentLogWriterStartTime: Long = -1L\n+  private var currentLogWriterStopTime: Long = -1L\n+\n+  initializeOrRecover()\n+\n+  /** Write a byte buffer to the log file */"
  }, {
    "author": {
      "login": "harishreedharan"
    },
    "body": "Yes, that is right. I believe we are lacking Scaladocs everywhere - we should do it I guess.\n",
    "commit": "e4bee2065293d7373c43fe5636dd9971dede257e",
    "createdAt": "2014-10-22T05:45:55Z",
    "diffHunk": "@@ -0,0 +1,223 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.util\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{ExecutionContext, Future}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.fs.permission.FsPermission\n+import org.apache.spark.Logging\n+import org.apache.spark.util.Utils\n+import WriteAheadLogManager._\n+\n+/**\n+ * This class manages write ahead log files.\n+ * - Writes records (bytebuffers) to periodically rotating log files.\n+ * - Recovers the log files and the reads the recovered records upon failures.\n+ * - Cleans up old log files.\n+ *\n+ * Uses [[org.apache.spark.streaming.util.WriteAheadLogWriter]] to write\n+ * and [[org.apache.spark.streaming.util.WriteAheadLogReader]] to read.\n+ *\n+ *@param logDirectory Directory when rotating log files will be created.\n+ * @param hadoopConf Hadoop configuration for reading/writing log files.\n+ * @param rollingIntervalSecs The interval in seconds with which logs will be rolled over.\n+ *                            Default is one minute.\n+ * @param maxFailures Max number of failures that is tolerated for every attempt to write to log.\n+ *                    Default is three.\n+ * @param callerName Optional name of the class who is using this manager.\n+ * @param clock Optional clock that is used to check for rotation interval.\n+ */\n+private[streaming] class WriteAheadLogManager(\n+    logDirectory: String,\n+    hadoopConf: Configuration,\n+    rollingIntervalSecs: Int = 60,\n+    maxFailures: Int = 3,\n+    callerName: String = \"\",\n+    clock: Clock = new SystemClock\n+  ) extends Logging {\n+\n+  private val pastLogs = new ArrayBuffer[LogInfo]\n+  private val callerNameTag =\n+    if (callerName != null && callerName.nonEmpty) s\" for $callerName\" else \"\"\n+  private val threadpoolName = s\"WriteAheadLogManager $callerNameTag\"\n+  implicit private val executionContext = ExecutionContext.fromExecutorService(\n+    Utils.newDaemonFixedThreadPool(1, threadpoolName))\n+  override protected val logName = s\"WriteAheadLogManager $callerNameTag\"\n+\n+  private var currentLogPath: String = null\n+  private var currentLogWriter: WriteAheadLogWriter = null\n+  private var currentLogWriterStartTime: Long = -1L\n+  private var currentLogWriterStopTime: Long = -1L\n+\n+  initializeOrRecover()\n+\n+  /** Write a byte buffer to the log file */"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Yes, I agree. Will make it more clear. \n",
    "commit": "e4bee2065293d7373c43fe5636dd9971dede257e",
    "createdAt": "2014-10-22T07:18:09Z",
    "diffHunk": "@@ -0,0 +1,223 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.util\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{ExecutionContext, Future}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.fs.permission.FsPermission\n+import org.apache.spark.Logging\n+import org.apache.spark.util.Utils\n+import WriteAheadLogManager._\n+\n+/**\n+ * This class manages write ahead log files.\n+ * - Writes records (bytebuffers) to periodically rotating log files.\n+ * - Recovers the log files and the reads the recovered records upon failures.\n+ * - Cleans up old log files.\n+ *\n+ * Uses [[org.apache.spark.streaming.util.WriteAheadLogWriter]] to write\n+ * and [[org.apache.spark.streaming.util.WriteAheadLogReader]] to read.\n+ *\n+ *@param logDirectory Directory when rotating log files will be created.\n+ * @param hadoopConf Hadoop configuration for reading/writing log files.\n+ * @param rollingIntervalSecs The interval in seconds with which logs will be rolled over.\n+ *                            Default is one minute.\n+ * @param maxFailures Max number of failures that is tolerated for every attempt to write to log.\n+ *                    Default is three.\n+ * @param callerName Optional name of the class who is using this manager.\n+ * @param clock Optional clock that is used to check for rotation interval.\n+ */\n+private[streaming] class WriteAheadLogManager(\n+    logDirectory: String,\n+    hadoopConf: Configuration,\n+    rollingIntervalSecs: Int = 60,\n+    maxFailures: Int = 3,\n+    callerName: String = \"\",\n+    clock: Clock = new SystemClock\n+  ) extends Logging {\n+\n+  private val pastLogs = new ArrayBuffer[LogInfo]\n+  private val callerNameTag =\n+    if (callerName != null && callerName.nonEmpty) s\" for $callerName\" else \"\"\n+  private val threadpoolName = s\"WriteAheadLogManager $callerNameTag\"\n+  implicit private val executionContext = ExecutionContext.fromExecutorService(\n+    Utils.newDaemonFixedThreadPool(1, threadpoolName))\n+  override protected val logName = s\"WriteAheadLogManager $callerNameTag\"\n+\n+  private var currentLogPath: String = null\n+  private var currentLogWriter: WriteAheadLogWriter = null\n+  private var currentLogWriterStartTime: Long = -1L\n+  private var currentLogWriterStopTime: Long = -1L\n+\n+  initializeOrRecover()\n+\n+  /** Write a byte buffer to the log file */"
  }],
  "prId": 2882
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Need an extra space before `@param`\n",
    "commit": "e4bee2065293d7373c43fe5636dd9971dede257e",
    "createdAt": "2014-10-24T00:38:31Z",
    "diffHunk": "@@ -0,0 +1,224 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.util\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{ExecutionContext, Future}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.fs.permission.FsPermission\n+import org.apache.spark.Logging\n+import org.apache.spark.util.Utils\n+import WriteAheadLogManager._\n+\n+/**\n+ * This class manages write ahead log files.\n+ * - Writes records (bytebuffers) to periodically rotating log files.\n+ * - Recovers the log files and the reads the recovered records upon failures.\n+ * - Cleans up old log files.\n+ *\n+ * Uses [[org.apache.spark.streaming.util.WriteAheadLogWriter]] to write\n+ * and [[org.apache.spark.streaming.util.WriteAheadLogReader]] to read.\n+ *\n+ *@param logDirectory Directory when rotating log files will be created."
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Done.\n",
    "commit": "e4bee2065293d7373c43fe5636dd9971dede257e",
    "createdAt": "2014-10-24T00:57:02Z",
    "diffHunk": "@@ -0,0 +1,224 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.util\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{ExecutionContext, Future}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.fs.permission.FsPermission\n+import org.apache.spark.Logging\n+import org.apache.spark.util.Utils\n+import WriteAheadLogManager._\n+\n+/**\n+ * This class manages write ahead log files.\n+ * - Writes records (bytebuffers) to periodically rotating log files.\n+ * - Recovers the log files and the reads the recovered records upon failures.\n+ * - Cleans up old log files.\n+ *\n+ * Uses [[org.apache.spark.streaming.util.WriteAheadLogWriter]] to write\n+ * and [[org.apache.spark.streaming.util.WriteAheadLogReader]] to read.\n+ *\n+ *@param logDirectory Directory when rotating log files will be created."
  }],
  "prId": 2882
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "How could callerName be null?\n",
    "commit": "e4bee2065293d7373c43fe5636dd9971dede257e",
    "createdAt": "2014-10-24T00:40:25Z",
    "diffHunk": "@@ -0,0 +1,224 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.util\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{ExecutionContext, Future}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.fs.permission.FsPermission\n+import org.apache.spark.Logging\n+import org.apache.spark.util.Utils\n+import WriteAheadLogManager._\n+\n+/**\n+ * This class manages write ahead log files.\n+ * - Writes records (bytebuffers) to periodically rotating log files.\n+ * - Recovers the log files and the reads the recovered records upon failures.\n+ * - Cleans up old log files.\n+ *\n+ * Uses [[org.apache.spark.streaming.util.WriteAheadLogWriter]] to write\n+ * and [[org.apache.spark.streaming.util.WriteAheadLogReader]] to read.\n+ *\n+ *@param logDirectory Directory when rotating log files will be created.\n+ * @param hadoopConf Hadoop configuration for reading/writing log files.\n+ * @param rollingIntervalSecs The interval in seconds with which logs will be rolled over.\n+ *                            Default is one minute.\n+ * @param maxFailures Max number of failures that is tolerated for every attempt to write to log.\n+ *                    Default is three.\n+ * @param callerName Optional name of the class who is using this manager.\n+ * @param clock Optional clock that is used to check for rotation interval.\n+ */\n+private[streaming] class WriteAheadLogManager(\n+    logDirectory: String,\n+    hadoopConf: Configuration,\n+    rollingIntervalSecs: Int = 60,\n+    maxFailures: Int = 3,\n+    callerName: String = \"\",\n+    clock: Clock = new SystemClock\n+  ) extends Logging {\n+\n+  private val pastLogs = new ArrayBuffer[LogInfo]\n+  private val callerNameTag =\n+    if (callerName != null && callerName.nonEmpty) s\" for $callerName\" else \"\""
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "If someone explicitly passes null. But checking that can be avoided, why would any one do that.\n",
    "commit": "e4bee2065293d7373c43fe5636dd9971dede257e",
    "createdAt": "2014-10-24T00:47:30Z",
    "diffHunk": "@@ -0,0 +1,224 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.util\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{ExecutionContext, Future}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.fs.permission.FsPermission\n+import org.apache.spark.Logging\n+import org.apache.spark.util.Utils\n+import WriteAheadLogManager._\n+\n+/**\n+ * This class manages write ahead log files.\n+ * - Writes records (bytebuffers) to periodically rotating log files.\n+ * - Recovers the log files and the reads the recovered records upon failures.\n+ * - Cleans up old log files.\n+ *\n+ * Uses [[org.apache.spark.streaming.util.WriteAheadLogWriter]] to write\n+ * and [[org.apache.spark.streaming.util.WriteAheadLogReader]] to read.\n+ *\n+ *@param logDirectory Directory when rotating log files will be created.\n+ * @param hadoopConf Hadoop configuration for reading/writing log files.\n+ * @param rollingIntervalSecs The interval in seconds with which logs will be rolled over.\n+ *                            Default is one minute.\n+ * @param maxFailures Max number of failures that is tolerated for every attempt to write to log.\n+ *                    Default is three.\n+ * @param callerName Optional name of the class who is using this manager.\n+ * @param clock Optional clock that is used to check for rotation interval.\n+ */\n+private[streaming] class WriteAheadLogManager(\n+    logDirectory: String,\n+    hadoopConf: Configuration,\n+    rollingIntervalSecs: Int = 60,\n+    maxFailures: Int = 3,\n+    callerName: String = \"\",\n+    clock: Clock = new SystemClock\n+  ) extends Logging {\n+\n+  private val pastLogs = new ArrayBuffer[LogInfo]\n+  private val callerNameTag =\n+    if (callerName != null && callerName.nonEmpty) s\" for $callerName\" else \"\""
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Removed.\n",
    "commit": "e4bee2065293d7373c43fe5636dd9971dede257e",
    "createdAt": "2014-10-24T00:57:09Z",
    "diffHunk": "@@ -0,0 +1,224 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.util\n+\n+import java.nio.ByteBuffer\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.{ExecutionContext, Future}\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.fs.permission.FsPermission\n+import org.apache.spark.Logging\n+import org.apache.spark.util.Utils\n+import WriteAheadLogManager._\n+\n+/**\n+ * This class manages write ahead log files.\n+ * - Writes records (bytebuffers) to periodically rotating log files.\n+ * - Recovers the log files and the reads the recovered records upon failures.\n+ * - Cleans up old log files.\n+ *\n+ * Uses [[org.apache.spark.streaming.util.WriteAheadLogWriter]] to write\n+ * and [[org.apache.spark.streaming.util.WriteAheadLogReader]] to read.\n+ *\n+ *@param logDirectory Directory when rotating log files will be created.\n+ * @param hadoopConf Hadoop configuration for reading/writing log files.\n+ * @param rollingIntervalSecs The interval in seconds with which logs will be rolled over.\n+ *                            Default is one minute.\n+ * @param maxFailures Max number of failures that is tolerated for every attempt to write to log.\n+ *                    Default is three.\n+ * @param callerName Optional name of the class who is using this manager.\n+ * @param clock Optional clock that is used to check for rotation interval.\n+ */\n+private[streaming] class WriteAheadLogManager(\n+    logDirectory: String,\n+    hadoopConf: Configuration,\n+    rollingIntervalSecs: Int = 60,\n+    maxFailures: Int = 3,\n+    callerName: String = \"\",\n+    clock: Clock = new SystemClock\n+  ) extends Logging {\n+\n+  private val pastLogs = new ArrayBuffer[LogInfo]\n+  private val callerNameTag =\n+    if (callerName != null && callerName.nonEmpty) s\" for $callerName\" else \"\""
  }],
  "prId": 2882
}]