[{
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Can delete this Scaladoc.\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-26T01:22:22Z",
    "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.api.python\n+\n+import java.util.{ArrayList => JArrayList}\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.api.java._\n+import org.apache.spark.api.python._\n+import org.apache.spark.streaming.{Duration, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.api.java._\n+\n+\n+/**\n+ * Interface for Python callback function\n+ */\n+trait PythonRDDFunction {\n+  def call(rdd: JavaRDD[_], rdd2: JavaRDD[_], time: Long): JavaRDD[Array[Byte]]\n+}\n+\n+\n+/**\n+ * Transformed DStream in Python.\n+ *\n+ * If the result RDD is PythonRDD, then it will cache it as an template for future use,\n+ * this can reduce the Python callbacks.\n+ *\n+ * @param parent"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "`wrapRDD` and `some` does not seem to be using the `pfunc` and is therefore stateless. Shouldnt they be moved to an object? \n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-30T01:03:21Z",
    "diffHunk": "@@ -0,0 +1,261 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.api.python\n+\n+import java.util.{ArrayList => JArrayList, List => JList}\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.api.java._\n+import org.apache.spark.api.python._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{Interval, Duration, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.api.java._\n+\n+\n+/**\n+ * Interface for Python callback function with three arguments\n+ */\n+trait PythonRDDFunction {\n+  def call(time: Long, rdds: JList[_]): JavaRDD[Array[Byte]]\n+}\n+\n+/**\n+ * Wrapper for PythonRDDFunction\n+ */\n+private[python] class RDDFunction(pfunc: PythonRDDFunction)\n+  extends function.Function2[JList[JavaRDD[_]], Time, JavaRDD[Array[Byte]]] with Serializable {\n+\n+  def wrapRDD(rdd: Option[RDD[_]]): JavaRDD[_] = {"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Since they are not used outside `RDDFunction`, they should at least be private to this class. \n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-30T06:40:22Z",
    "diffHunk": "@@ -0,0 +1,261 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.api.python\n+\n+import java.util.{ArrayList => JArrayList, List => JList}\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.api.java._\n+import org.apache.spark.api.python._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{Interval, Duration, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.api.java._\n+\n+\n+/**\n+ * Interface for Python callback function with three arguments\n+ */\n+trait PythonRDDFunction {\n+  def call(time: Long, rdds: JList[_]): JavaRDD[Array[Byte]]\n+}\n+\n+/**\n+ * Wrapper for PythonRDDFunction\n+ */\n+private[python] class RDDFunction(pfunc: PythonRDDFunction)\n+  extends function.Function2[JList[JavaRDD[_]], Time, JavaRDD[Array[Byte]]] with Serializable {\n+\n+  def wrapRDD(rdd: Option[RDD[_]]): JavaRDD[_] = {"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Please add Scala/Java docs for these classes (even if they are internal classes). Helps one understand the code, which is anyways a little complex for PySpark stuff.\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-30T01:04:44Z",
    "diffHunk": "@@ -0,0 +1,261 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.api.python\n+\n+import java.util.{ArrayList => JArrayList, List => JList}\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.api.java._\n+import org.apache.spark.api.python._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{Interval, Duration, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.api.java._\n+\n+\n+/**\n+ * Interface for Python callback function with three arguments\n+ */\n+trait PythonRDDFunction {\n+  def call(time: Long, rdds: JList[_]): JavaRDD[Array[Byte]]\n+}\n+\n+/**\n+ * Wrapper for PythonRDDFunction\n+ */\n+private[python] class RDDFunction(pfunc: PythonRDDFunction)\n+  extends function.Function2[JList[JavaRDD[_]], Time, JavaRDD[Array[Byte]]] with Serializable {\n+\n+  def wrapRDD(rdd: Option[RDD[_]]): JavaRDD[_] = {\n+    if (rdd.isDefined) {\n+      JavaRDD.fromRDD(rdd.get)\n+    } else {\n+      null\n+    }\n+  }\n+\n+  def some(jrdd: JavaRDD[Array[Byte]]): Option[RDD[Array[Byte]]] = {\n+    if (jrdd != null) {\n+      Some(jrdd.rdd)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  def apply(rdd: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    some(pfunc.call(time.milliseconds, List(wrapRDD(rdd)).asJava))\n+  }\n+\n+  def apply(rdd: Option[RDD[_]], rdd2: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    some(pfunc.call(time.milliseconds, List(wrapRDD(rdd), wrapRDD(rdd2)).asJava))\n+  }\n+\n+  // for JFunction2\n+  def call(rdds: JList[JavaRDD[_]], time: Time): JavaRDD[Array[Byte]] = {\n+    pfunc.call(time.milliseconds, rdds)\n+  }\n+}\n+\n+private[python]"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "What do you mean by result RDD? \n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-30T01:05:16Z",
    "diffHunk": "@@ -0,0 +1,261 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.api.python\n+\n+import java.util.{ArrayList => JArrayList, List => JList}\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.api.java._\n+import org.apache.spark.api.python._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{Interval, Duration, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.api.java._\n+\n+\n+/**\n+ * Interface for Python callback function with three arguments\n+ */\n+trait PythonRDDFunction {\n+  def call(time: Long, rdds: JList[_]): JavaRDD[Array[Byte]]\n+}\n+\n+/**\n+ * Wrapper for PythonRDDFunction\n+ */\n+private[python] class RDDFunction(pfunc: PythonRDDFunction)\n+  extends function.Function2[JList[JavaRDD[_]], Time, JavaRDD[Array[Byte]]] with Serializable {\n+\n+  def wrapRDD(rdd: Option[RDD[_]]): JavaRDD[_] = {\n+    if (rdd.isDefined) {\n+      JavaRDD.fromRDD(rdd.get)\n+    } else {\n+      null\n+    }\n+  }\n+\n+  def some(jrdd: JavaRDD[Array[Byte]]): Option[RDD[Array[Byte]]] = {\n+    if (jrdd != null) {\n+      Some(jrdd.rdd)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  def apply(rdd: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    some(pfunc.call(time.milliseconds, List(wrapRDD(rdd)).asJava))\n+  }\n+\n+  def apply(rdd: Option[RDD[_]], rdd2: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    some(pfunc.call(time.milliseconds, List(wrapRDD(rdd), wrapRDD(rdd2)).asJava))\n+  }\n+\n+  // for JFunction2\n+  def call(rdds: JList[JavaRDD[_]], time: Time): JavaRDD[Array[Byte]] = {\n+    pfunc.call(time.milliseconds, rdds)\n+  }\n+}\n+\n+private[python]\n+abstract class PythonDStream(parent: DStream[_]) extends DStream[Array[Byte]] (parent.ssc) {\n+\n+  override def dependencies = List(parent)\n+\n+  override def slideDuration: Duration = parent.slideDuration\n+\n+  val asJavaDStream  = JavaDStream.fromDStream(this)\n+}\n+\n+private[spark] object PythonDStream {\n+\n+  // helper function for DStream.foreachRDD(),\n+  // cannot be `foreachRDD`, it will confusing py4j\n+  def callForeachRDD(jdstream: JavaDStream[Array[Byte]], pyfunc: PythonRDDFunction){\n+    val func = new RDDFunction(pyfunc)\n+    jdstream.dstream.foreachRDD((rdd, time) => func(Some(rdd), time))\n+  }\n+\n+  // helper function for ssc.transform()\n+  def callTransform(ssc: JavaStreamingContext, jdsteams: JList[JavaDStream[_]],\n+                    pyfunc: PythonRDDFunction)\n+    :JavaDStream[Array[Byte]] = {\n+    val func = new RDDFunction(pyfunc)\n+    ssc.transform(jdsteams, func)\n+  }\n+\n+  // convert list of RDD into queue of RDDs, for ssc.queueStream()\n+  def toRDDQueue(rdds: JArrayList[JavaRDD[Array[Byte]]]): java.util.Queue[JavaRDD[Array[Byte]]] = {\n+    val queue = new java.util.LinkedList[JavaRDD[Array[Byte]]]\n+    rdds.forall(queue.add(_))\n+    queue\n+  }\n+}\n+\n+/**\n+ * Transformed DStream in Python.\n+ *\n+ * If the result RDD is PythonRDD, then it will cache it as an template for future use,"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Please add some comments on what this method does, especially regarding reuse and all. Also, how is `lastResult` used?\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-30T01:08:43Z",
    "diffHunk": "@@ -0,0 +1,261 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.api.python\n+\n+import java.util.{ArrayList => JArrayList, List => JList}\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.api.java._\n+import org.apache.spark.api.python._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{Interval, Duration, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.api.java._\n+\n+\n+/**\n+ * Interface for Python callback function with three arguments\n+ */\n+trait PythonRDDFunction {\n+  def call(time: Long, rdds: JList[_]): JavaRDD[Array[Byte]]\n+}\n+\n+/**\n+ * Wrapper for PythonRDDFunction\n+ */\n+private[python] class RDDFunction(pfunc: PythonRDDFunction)\n+  extends function.Function2[JList[JavaRDD[_]], Time, JavaRDD[Array[Byte]]] with Serializable {\n+\n+  def wrapRDD(rdd: Option[RDD[_]]): JavaRDD[_] = {\n+    if (rdd.isDefined) {\n+      JavaRDD.fromRDD(rdd.get)\n+    } else {\n+      null\n+    }\n+  }\n+\n+  def some(jrdd: JavaRDD[Array[Byte]]): Option[RDD[Array[Byte]]] = {\n+    if (jrdd != null) {\n+      Some(jrdd.rdd)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  def apply(rdd: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    some(pfunc.call(time.milliseconds, List(wrapRDD(rdd)).asJava))\n+  }\n+\n+  def apply(rdd: Option[RDD[_]], rdd2: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    some(pfunc.call(time.milliseconds, List(wrapRDD(rdd), wrapRDD(rdd2)).asJava))\n+  }\n+\n+  // for JFunction2\n+  def call(rdds: JList[JavaRDD[_]], time: Time): JavaRDD[Array[Byte]] = {\n+    pfunc.call(time.milliseconds, rdds)\n+  }\n+}\n+\n+private[python]\n+abstract class PythonDStream(parent: DStream[_]) extends DStream[Array[Byte]] (parent.ssc) {\n+\n+  override def dependencies = List(parent)\n+\n+  override def slideDuration: Duration = parent.slideDuration\n+\n+  val asJavaDStream  = JavaDStream.fromDStream(this)\n+}\n+\n+private[spark] object PythonDStream {\n+\n+  // helper function for DStream.foreachRDD(),\n+  // cannot be `foreachRDD`, it will confusing py4j\n+  def callForeachRDD(jdstream: JavaDStream[Array[Byte]], pyfunc: PythonRDDFunction){\n+    val func = new RDDFunction(pyfunc)\n+    jdstream.dstream.foreachRDD((rdd, time) => func(Some(rdd), time))\n+  }\n+\n+  // helper function for ssc.transform()\n+  def callTransform(ssc: JavaStreamingContext, jdsteams: JList[JavaDStream[_]],\n+                    pyfunc: PythonRDDFunction)\n+    :JavaDStream[Array[Byte]] = {\n+    val func = new RDDFunction(pyfunc)\n+    ssc.transform(jdsteams, func)\n+  }\n+\n+  // convert list of RDD into queue of RDDs, for ssc.queueStream()\n+  def toRDDQueue(rdds: JArrayList[JavaRDD[Array[Byte]]]): java.util.Queue[JavaRDD[Array[Byte]]] = {\n+    val queue = new java.util.LinkedList[JavaRDD[Array[Byte]]]\n+    rdds.forall(queue.add(_))\n+    queue\n+  }\n+}\n+\n+/**\n+ * Transformed DStream in Python.\n+ *\n+ * If the result RDD is PythonRDD, then it will cache it as an template for future use,\n+ * this can reduce the Python callbacks.\n+ */\n+private[spark]\n+class PythonTransformedDStream (parent: DStream[_], pfunc: PythonRDDFunction,\n+                                var reuse: Boolean = false)\n+  extends PythonDStream(parent) {\n+\n+  val func = new RDDFunction(pfunc)\n+  var lastResult: PythonRDD = _\n+\n+  override def compute(validTime: Time): Option[RDD[Array[Byte]]] = {\n+    val rdd1 = parent.getOrCompute(validTime)"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Should be private to python package.\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-30T01:17:23Z",
    "diffHunk": "@@ -0,0 +1,261 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.api.python\n+\n+import java.util.{ArrayList => JArrayList, List => JList}\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.api.java._\n+import org.apache.spark.api.python._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{Interval, Duration, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.api.java._\n+\n+\n+/**\n+ * Interface for Python callback function with three arguments\n+ */\n+trait PythonRDDFunction {"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "The name is very non-intuitive. This is just an interface for defining functions that can be used in `Dstream.transform`. A more intuitive name (at least to me) is `DStreamTransformFunction`. What do you think?\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-30T06:37:30Z",
    "diffHunk": "@@ -0,0 +1,261 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.api.python\n+\n+import java.util.{ArrayList => JArrayList, List => JList}\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.api.java._\n+import org.apache.spark.api.python._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{Interval, Duration, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.api.java._\n+\n+\n+/**\n+ * Interface for Python callback function with three arguments\n+ */\n+trait PythonRDDFunction {"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "What is the need for both Scala functions - RDDFunction and PythonRDDFunction?\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-30T01:31:04Z",
    "diffHunk": "@@ -0,0 +1,261 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.api.python\n+\n+import java.util.{ArrayList => JArrayList, List => JList}\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.api.java._\n+import org.apache.spark.api.python._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{Interval, Duration, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.api.java._\n+\n+\n+/**\n+ * Interface for Python callback function with three arguments\n+ */\n+trait PythonRDDFunction {\n+  def call(time: Long, rdds: JList[_]): JavaRDD[Array[Byte]]\n+}\n+\n+/**\n+ * Wrapper for PythonRDDFunction\n+ */\n+private[python] class RDDFunction(pfunc: PythonRDDFunction)"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "I think I get it. You need a simple trait / interface (`PythonRDDFunction`) that is used as the interface for creating python function objects through py4j. And then you need to convert it to a `Function2` object (using `RDDFunction` class) so that it can be passed on to `DStream.foreachRDD`. However, I am not sure this justifies a separate class with a confusing name. How about static conversions added to `object PythonRDDFunction` like \n\n```\nobject PythonRDDFunction {\n   implicit def toFunction2(func: PythonRDDFunction): Option[Function2[....]] = {\n      new Function2[...]\n   }\n}\n```\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-30T06:34:25Z",
    "diffHunk": "@@ -0,0 +1,261 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.api.python\n+\n+import java.util.{ArrayList => JArrayList, List => JList}\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.api.java._\n+import org.apache.spark.api.python._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{Interval, Duration, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.api.java._\n+\n+\n+/**\n+ * Interface for Python callback function with three arguments\n+ */\n+trait PythonRDDFunction {\n+  def call(time: Long, rdds: JList[_]): JavaRDD[Array[Byte]]\n+}\n+\n+/**\n+ * Wrapper for PythonRDDFunction\n+ */\n+private[python] class RDDFunction(pfunc: PythonRDDFunction)"
  }, {
    "author": {
      "login": "davies"
    },
    "body": "I had tried implicit conversion, but failed. In the case of func(rdd, rdd2, time), scala can not create an RDDFunction for it automatically.\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-30T07:55:58Z",
    "diffHunk": "@@ -0,0 +1,261 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.api.python\n+\n+import java.util.{ArrayList => JArrayList, List => JList}\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.api.java._\n+import org.apache.spark.api.python._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{Interval, Duration, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.api.java._\n+\n+\n+/**\n+ * Interface for Python callback function with three arguments\n+ */\n+trait PythonRDDFunction {\n+  def call(time: Long, rdds: JList[_]): JavaRDD[Array[Byte]]\n+}\n+\n+/**\n+ * Wrapper for PythonRDDFunction\n+ */\n+private[python] class RDDFunction(pfunc: PythonRDDFunction)"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Why `private[spark]` and not `private[python]` ?\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-30T06:41:47Z",
    "diffHunk": "@@ -0,0 +1,261 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.api.python\n+\n+import java.util.{ArrayList => JArrayList, List => JList}\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.api.java._\n+import org.apache.spark.api.python._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{Interval, Duration, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.api.java._\n+\n+\n+/**\n+ * Interface for Python callback function with three arguments\n+ */\n+trait PythonRDDFunction {\n+  def call(time: Long, rdds: JList[_]): JavaRDD[Array[Byte]]\n+}\n+\n+/**\n+ * Wrapper for PythonRDDFunction\n+ */\n+private[python] class RDDFunction(pfunc: PythonRDDFunction)\n+  extends function.Function2[JList[JavaRDD[_]], Time, JavaRDD[Array[Byte]]] with Serializable {\n+\n+  def wrapRDD(rdd: Option[RDD[_]]): JavaRDD[_] = {\n+    if (rdd.isDefined) {\n+      JavaRDD.fromRDD(rdd.get)\n+    } else {\n+      null\n+    }\n+  }\n+\n+  def some(jrdd: JavaRDD[Array[Byte]]): Option[RDD[Array[Byte]]] = {\n+    if (jrdd != null) {\n+      Some(jrdd.rdd)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  def apply(rdd: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    some(pfunc.call(time.milliseconds, List(wrapRDD(rdd)).asJava))\n+  }\n+\n+  def apply(rdd: Option[RDD[_]], rdd2: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    some(pfunc.call(time.milliseconds, List(wrapRDD(rdd), wrapRDD(rdd2)).asJava))\n+  }\n+\n+  // for JFunction2\n+  def call(rdds: JList[JavaRDD[_]], time: Time): JavaRDD[Array[Byte]] = {\n+    pfunc.call(time.milliseconds, rdds)\n+  }\n+}\n+\n+private[python]\n+abstract class PythonDStream(parent: DStream[_]) extends DStream[Array[Byte]] (parent.ssc) {\n+\n+  override def dependencies = List(parent)\n+\n+  override def slideDuration: Duration = parent.slideDuration\n+\n+  val asJavaDStream  = JavaDStream.fromDStream(this)\n+}\n+\n+private[spark] object PythonDStream {"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Why `private[spark]` and not `private[python]`?\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-30T06:42:22Z",
    "diffHunk": "@@ -0,0 +1,261 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.api.python\n+\n+import java.util.{ArrayList => JArrayList, List => JList}\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.api.java._\n+import org.apache.spark.api.python._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{Interval, Duration, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.api.java._\n+\n+\n+/**\n+ * Interface for Python callback function with three arguments\n+ */\n+trait PythonRDDFunction {\n+  def call(time: Long, rdds: JList[_]): JavaRDD[Array[Byte]]\n+}\n+\n+/**\n+ * Wrapper for PythonRDDFunction\n+ */\n+private[python] class RDDFunction(pfunc: PythonRDDFunction)\n+  extends function.Function2[JList[JavaRDD[_]], Time, JavaRDD[Array[Byte]]] with Serializable {\n+\n+  def wrapRDD(rdd: Option[RDD[_]]): JavaRDD[_] = {\n+    if (rdd.isDefined) {\n+      JavaRDD.fromRDD(rdd.get)\n+    } else {\n+      null\n+    }\n+  }\n+\n+  def some(jrdd: JavaRDD[Array[Byte]]): Option[RDD[Array[Byte]]] = {\n+    if (jrdd != null) {\n+      Some(jrdd.rdd)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  def apply(rdd: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    some(pfunc.call(time.milliseconds, List(wrapRDD(rdd)).asJava))\n+  }\n+\n+  def apply(rdd: Option[RDD[_]], rdd2: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    some(pfunc.call(time.milliseconds, List(wrapRDD(rdd), wrapRDD(rdd2)).asJava))\n+  }\n+\n+  // for JFunction2\n+  def call(rdds: JList[JavaRDD[_]], time: Time): JavaRDD[Array[Byte]] = {\n+    pfunc.call(time.milliseconds, rdds)\n+  }\n+}\n+\n+private[python]\n+abstract class PythonDStream(parent: DStream[_]) extends DStream[Array[Byte]] (parent.ssc) {\n+\n+  override def dependencies = List(parent)\n+\n+  override def slideDuration: Duration = parent.slideDuration\n+\n+  val asJavaDStream  = JavaDStream.fromDStream(this)\n+}\n+\n+private[spark] object PythonDStream {\n+\n+  // helper function for DStream.foreachRDD(),\n+  // cannot be `foreachRDD`, it will confusing py4j\n+  def callForeachRDD(jdstream: JavaDStream[Array[Byte]], pyfunc: PythonRDDFunction){\n+    val func = new RDDFunction(pyfunc)\n+    jdstream.dstream.foreachRDD((rdd, time) => func(Some(rdd), time))\n+  }\n+\n+  // helper function for ssc.transform()\n+  def callTransform(ssc: JavaStreamingContext, jdsteams: JList[JavaDStream[_]],\n+                    pyfunc: PythonRDDFunction)\n+    :JavaDStream[Array[Byte]] = {\n+    val func = new RDDFunction(pyfunc)\n+    ssc.transform(jdsteams, func)\n+  }\n+\n+  // convert list of RDD into queue of RDDs, for ssc.queueStream()\n+  def toRDDQueue(rdds: JArrayList[JavaRDD[Array[Byte]]]): java.util.Queue[JavaRDD[Array[Byte]]] = {\n+    val queue = new java.util.LinkedList[JavaRDD[Array[Byte]]]\n+    rdds.forall(queue.add(_))\n+    queue\n+  }\n+}\n+\n+/**\n+ * Transformed DStream in Python.\n+ *\n+ * If the result RDD is PythonRDD, then it will cache it as an template for future use,\n+ * this can reduce the Python callbacks.\n+ */\n+private[spark]"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Why `private[spark]` and not `private[python]`?\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-30T06:42:33Z",
    "diffHunk": "@@ -0,0 +1,261 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.api.python\n+\n+import java.util.{ArrayList => JArrayList, List => JList}\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.api.java._\n+import org.apache.spark.api.python._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{Interval, Duration, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.api.java._\n+\n+\n+/**\n+ * Interface for Python callback function with three arguments\n+ */\n+trait PythonRDDFunction {\n+  def call(time: Long, rdds: JList[_]): JavaRDD[Array[Byte]]\n+}\n+\n+/**\n+ * Wrapper for PythonRDDFunction\n+ */\n+private[python] class RDDFunction(pfunc: PythonRDDFunction)\n+  extends function.Function2[JList[JavaRDD[_]], Time, JavaRDD[Array[Byte]]] with Serializable {\n+\n+  def wrapRDD(rdd: Option[RDD[_]]): JavaRDD[_] = {\n+    if (rdd.isDefined) {\n+      JavaRDD.fromRDD(rdd.get)\n+    } else {\n+      null\n+    }\n+  }\n+\n+  def some(jrdd: JavaRDD[Array[Byte]]): Option[RDD[Array[Byte]]] = {\n+    if (jrdd != null) {\n+      Some(jrdd.rdd)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  def apply(rdd: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    some(pfunc.call(time.milliseconds, List(wrapRDD(rdd)).asJava))\n+  }\n+\n+  def apply(rdd: Option[RDD[_]], rdd2: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    some(pfunc.call(time.milliseconds, List(wrapRDD(rdd), wrapRDD(rdd2)).asJava))\n+  }\n+\n+  // for JFunction2\n+  def call(rdds: JList[JavaRDD[_]], time: Time): JavaRDD[Array[Byte]] = {\n+    pfunc.call(time.milliseconds, rdds)\n+  }\n+}\n+\n+private[python]\n+abstract class PythonDStream(parent: DStream[_]) extends DStream[Array[Byte]] (parent.ssc) {\n+\n+  override def dependencies = List(parent)\n+\n+  override def slideDuration: Duration = parent.slideDuration\n+\n+  val asJavaDStream  = JavaDStream.fromDStream(this)\n+}\n+\n+private[spark] object PythonDStream {\n+\n+  // helper function for DStream.foreachRDD(),\n+  // cannot be `foreachRDD`, it will confusing py4j\n+  def callForeachRDD(jdstream: JavaDStream[Array[Byte]], pyfunc: PythonRDDFunction){\n+    val func = new RDDFunction(pyfunc)\n+    jdstream.dstream.foreachRDD((rdd, time) => func(Some(rdd), time))\n+  }\n+\n+  // helper function for ssc.transform()\n+  def callTransform(ssc: JavaStreamingContext, jdsteams: JList[JavaDStream[_]],\n+                    pyfunc: PythonRDDFunction)\n+    :JavaDStream[Array[Byte]] = {\n+    val func = new RDDFunction(pyfunc)\n+    ssc.transform(jdsteams, func)\n+  }\n+\n+  // convert list of RDD into queue of RDDs, for ssc.queueStream()\n+  def toRDDQueue(rdds: JArrayList[JavaRDD[Array[Byte]]]): java.util.Queue[JavaRDD[Array[Byte]]] = {\n+    val queue = new java.util.LinkedList[JavaRDD[Array[Byte]]]\n+    rdds.forall(queue.add(_))\n+    queue\n+  }\n+}\n+\n+/**\n+ * Transformed DStream in Python.\n+ *\n+ * If the result RDD is PythonRDD, then it will cache it as an template for future use,\n+ * this can reduce the Python callbacks.\n+ */\n+private[spark]\n+class PythonTransformedDStream (parent: DStream[_], pfunc: PythonRDDFunction,\n+                                var reuse: Boolean = false)\n+  extends PythonDStream(parent) {\n+\n+  val func = new RDDFunction(pfunc)\n+  var lastResult: PythonRDD = _\n+\n+  override def compute(validTime: Time): Option[RDD[Array[Byte]]] = {\n+    val rdd1 = parent.getOrCompute(validTime)\n+    if (rdd1.isEmpty) {\n+      return None\n+    }\n+    if (reuse && lastResult != null) {\n+      Some(lastResult.copyTo(rdd1.get))\n+    } else {\n+      val r = func(rdd1, validTime)\n+      if (reuse && r.isDefined && lastResult == null) {\n+        r.get match {\n+          case rdd: PythonRDD =>\n+            if (rdd.parent(0) == rdd1) {\n+              // only one PythonRDD\n+              lastResult = rdd\n+            } else {\n+              // may have multiple stages\n+              reuse = false\n+            }\n+        }\n+      }\n+      r\n+    }\n+  }\n+}\n+\n+/**\n+ * Transformed from two DStreams in Python.\n+ */\n+private[spark]\n+class PythonTransformed2DStream(parent: DStream[_], parent2: DStream[_],\n+                                pfunc: PythonRDDFunction)"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Why `private[spark]` and not `private[python]`?\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-30T06:42:43Z",
    "diffHunk": "@@ -0,0 +1,261 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.api.python\n+\n+import java.util.{ArrayList => JArrayList, List => JList}\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.api.java._\n+import org.apache.spark.api.python._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{Interval, Duration, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.api.java._\n+\n+\n+/**\n+ * Interface for Python callback function with three arguments\n+ */\n+trait PythonRDDFunction {\n+  def call(time: Long, rdds: JList[_]): JavaRDD[Array[Byte]]\n+}\n+\n+/**\n+ * Wrapper for PythonRDDFunction\n+ */\n+private[python] class RDDFunction(pfunc: PythonRDDFunction)\n+  extends function.Function2[JList[JavaRDD[_]], Time, JavaRDD[Array[Byte]]] with Serializable {\n+\n+  def wrapRDD(rdd: Option[RDD[_]]): JavaRDD[_] = {\n+    if (rdd.isDefined) {\n+      JavaRDD.fromRDD(rdd.get)\n+    } else {\n+      null\n+    }\n+  }\n+\n+  def some(jrdd: JavaRDD[Array[Byte]]): Option[RDD[Array[Byte]]] = {\n+    if (jrdd != null) {\n+      Some(jrdd.rdd)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  def apply(rdd: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    some(pfunc.call(time.milliseconds, List(wrapRDD(rdd)).asJava))\n+  }\n+\n+  def apply(rdd: Option[RDD[_]], rdd2: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    some(pfunc.call(time.milliseconds, List(wrapRDD(rdd), wrapRDD(rdd2)).asJava))\n+  }\n+\n+  // for JFunction2\n+  def call(rdds: JList[JavaRDD[_]], time: Time): JavaRDD[Array[Byte]] = {\n+    pfunc.call(time.milliseconds, rdds)\n+  }\n+}\n+\n+private[python]\n+abstract class PythonDStream(parent: DStream[_]) extends DStream[Array[Byte]] (parent.ssc) {\n+\n+  override def dependencies = List(parent)\n+\n+  override def slideDuration: Duration = parent.slideDuration\n+\n+  val asJavaDStream  = JavaDStream.fromDStream(this)\n+}\n+\n+private[spark] object PythonDStream {\n+\n+  // helper function for DStream.foreachRDD(),\n+  // cannot be `foreachRDD`, it will confusing py4j\n+  def callForeachRDD(jdstream: JavaDStream[Array[Byte]], pyfunc: PythonRDDFunction){\n+    val func = new RDDFunction(pyfunc)\n+    jdstream.dstream.foreachRDD((rdd, time) => func(Some(rdd), time))\n+  }\n+\n+  // helper function for ssc.transform()\n+  def callTransform(ssc: JavaStreamingContext, jdsteams: JList[JavaDStream[_]],\n+                    pyfunc: PythonRDDFunction)\n+    :JavaDStream[Array[Byte]] = {\n+    val func = new RDDFunction(pyfunc)\n+    ssc.transform(jdsteams, func)\n+  }\n+\n+  // convert list of RDD into queue of RDDs, for ssc.queueStream()\n+  def toRDDQueue(rdds: JArrayList[JavaRDD[Array[Byte]]]): java.util.Queue[JavaRDD[Array[Byte]]] = {\n+    val queue = new java.util.LinkedList[JavaRDD[Array[Byte]]]\n+    rdds.forall(queue.add(_))\n+    queue\n+  }\n+}\n+\n+/**\n+ * Transformed DStream in Python.\n+ *\n+ * If the result RDD is PythonRDD, then it will cache it as an template for future use,\n+ * this can reduce the Python callbacks.\n+ */\n+private[spark]\n+class PythonTransformedDStream (parent: DStream[_], pfunc: PythonRDDFunction,\n+                                var reuse: Boolean = false)\n+  extends PythonDStream(parent) {\n+\n+  val func = new RDDFunction(pfunc)\n+  var lastResult: PythonRDD = _\n+\n+  override def compute(validTime: Time): Option[RDD[Array[Byte]]] = {\n+    val rdd1 = parent.getOrCompute(validTime)\n+    if (rdd1.isEmpty) {\n+      return None\n+    }\n+    if (reuse && lastResult != null) {\n+      Some(lastResult.copyTo(rdd1.get))\n+    } else {\n+      val r = func(rdd1, validTime)\n+      if (reuse && r.isDefined && lastResult == null) {\n+        r.get match {\n+          case rdd: PythonRDD =>\n+            if (rdd.parent(0) == rdd1) {\n+              // only one PythonRDD\n+              lastResult = rdd\n+            } else {\n+              // may have multiple stages\n+              reuse = false\n+            }\n+        }\n+      }\n+      r\n+    }\n+  }\n+}\n+\n+/**\n+ * Transformed from two DStreams in Python.\n+ */\n+private[spark]\n+class PythonTransformed2DStream(parent: DStream[_], parent2: DStream[_],\n+                                pfunc: PythonRDDFunction)\n+  extends DStream[Array[Byte]] (parent.ssc) {\n+\n+  val func = new RDDFunction(pfunc)\n+\n+  override def slideDuration: Duration = parent.slideDuration\n+\n+  override def dependencies = List(parent, parent2)\n+\n+  override def compute(validTime: Time): Option[RDD[Array[Byte]]] = {\n+    func(parent.getOrCompute(validTime), parent2.getOrCompute(validTime), validTime)\n+  }\n+\n+  val asJavaDStream  = JavaDStream.fromDStream(this)\n+}\n+\n+/**\n+ * similar to StateDStream\n+ */\n+private[spark]\n+class PythonStateDStream(parent: DStream[Array[Byte]], preduceFunc: PythonRDDFunction)\n+  extends PythonDStream(parent) {"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Why `private[spark]` and not `private[python]`?\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-30T06:42:51Z",
    "diffHunk": "@@ -0,0 +1,261 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.api.python\n+\n+import java.util.{ArrayList => JArrayList, List => JList}\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.api.java._\n+import org.apache.spark.api.python._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{Interval, Duration, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.api.java._\n+\n+\n+/**\n+ * Interface for Python callback function with three arguments\n+ */\n+trait PythonRDDFunction {\n+  def call(time: Long, rdds: JList[_]): JavaRDD[Array[Byte]]\n+}\n+\n+/**\n+ * Wrapper for PythonRDDFunction\n+ */\n+private[python] class RDDFunction(pfunc: PythonRDDFunction)\n+  extends function.Function2[JList[JavaRDD[_]], Time, JavaRDD[Array[Byte]]] with Serializable {\n+\n+  def wrapRDD(rdd: Option[RDD[_]]): JavaRDD[_] = {\n+    if (rdd.isDefined) {\n+      JavaRDD.fromRDD(rdd.get)\n+    } else {\n+      null\n+    }\n+  }\n+\n+  def some(jrdd: JavaRDD[Array[Byte]]): Option[RDD[Array[Byte]]] = {\n+    if (jrdd != null) {\n+      Some(jrdd.rdd)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  def apply(rdd: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    some(pfunc.call(time.milliseconds, List(wrapRDD(rdd)).asJava))\n+  }\n+\n+  def apply(rdd: Option[RDD[_]], rdd2: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    some(pfunc.call(time.milliseconds, List(wrapRDD(rdd), wrapRDD(rdd2)).asJava))\n+  }\n+\n+  // for JFunction2\n+  def call(rdds: JList[JavaRDD[_]], time: Time): JavaRDD[Array[Byte]] = {\n+    pfunc.call(time.milliseconds, rdds)\n+  }\n+}\n+\n+private[python]\n+abstract class PythonDStream(parent: DStream[_]) extends DStream[Array[Byte]] (parent.ssc) {\n+\n+  override def dependencies = List(parent)\n+\n+  override def slideDuration: Duration = parent.slideDuration\n+\n+  val asJavaDStream  = JavaDStream.fromDStream(this)\n+}\n+\n+private[spark] object PythonDStream {\n+\n+  // helper function for DStream.foreachRDD(),\n+  // cannot be `foreachRDD`, it will confusing py4j\n+  def callForeachRDD(jdstream: JavaDStream[Array[Byte]], pyfunc: PythonRDDFunction){\n+    val func = new RDDFunction(pyfunc)\n+    jdstream.dstream.foreachRDD((rdd, time) => func(Some(rdd), time))\n+  }\n+\n+  // helper function for ssc.transform()\n+  def callTransform(ssc: JavaStreamingContext, jdsteams: JList[JavaDStream[_]],\n+                    pyfunc: PythonRDDFunction)\n+    :JavaDStream[Array[Byte]] = {\n+    val func = new RDDFunction(pyfunc)\n+    ssc.transform(jdsteams, func)\n+  }\n+\n+  // convert list of RDD into queue of RDDs, for ssc.queueStream()\n+  def toRDDQueue(rdds: JArrayList[JavaRDD[Array[Byte]]]): java.util.Queue[JavaRDD[Array[Byte]]] = {\n+    val queue = new java.util.LinkedList[JavaRDD[Array[Byte]]]\n+    rdds.forall(queue.add(_))\n+    queue\n+  }\n+}\n+\n+/**\n+ * Transformed DStream in Python.\n+ *\n+ * If the result RDD is PythonRDD, then it will cache it as an template for future use,\n+ * this can reduce the Python callbacks.\n+ */\n+private[spark]\n+class PythonTransformedDStream (parent: DStream[_], pfunc: PythonRDDFunction,\n+                                var reuse: Boolean = false)\n+  extends PythonDStream(parent) {\n+\n+  val func = new RDDFunction(pfunc)\n+  var lastResult: PythonRDD = _\n+\n+  override def compute(validTime: Time): Option[RDD[Array[Byte]]] = {\n+    val rdd1 = parent.getOrCompute(validTime)\n+    if (rdd1.isEmpty) {\n+      return None\n+    }\n+    if (reuse && lastResult != null) {\n+      Some(lastResult.copyTo(rdd1.get))\n+    } else {\n+      val r = func(rdd1, validTime)\n+      if (reuse && r.isDefined && lastResult == null) {\n+        r.get match {\n+          case rdd: PythonRDD =>\n+            if (rdd.parent(0) == rdd1) {\n+              // only one PythonRDD\n+              lastResult = rdd\n+            } else {\n+              // may have multiple stages\n+              reuse = false\n+            }\n+        }\n+      }\n+      r\n+    }\n+  }\n+}\n+\n+/**\n+ * Transformed from two DStreams in Python.\n+ */\n+private[spark]\n+class PythonTransformed2DStream(parent: DStream[_], parent2: DStream[_],\n+                                pfunc: PythonRDDFunction)\n+  extends DStream[Array[Byte]] (parent.ssc) {\n+\n+  val func = new RDDFunction(pfunc)\n+\n+  override def slideDuration: Duration = parent.slideDuration\n+\n+  override def dependencies = List(parent, parent2)\n+\n+  override def compute(validTime: Time): Option[RDD[Array[Byte]]] = {\n+    func(parent.getOrCompute(validTime), parent2.getOrCompute(validTime), validTime)\n+  }\n+\n+  val asJavaDStream  = JavaDStream.fromDStream(this)\n+}\n+\n+/**\n+ * similar to StateDStream\n+ */\n+private[spark]\n+class PythonStateDStream(parent: DStream[Array[Byte]], preduceFunc: PythonRDDFunction)\n+  extends PythonDStream(parent) {\n+\n+  val reduceFunc = new RDDFunction(preduceFunc)\n+\n+  super.persist(StorageLevel.MEMORY_ONLY)\n+  override val mustCheckpoint = true\n+\n+  override def compute(validTime: Time): Option[RDD[Array[Byte]]] = {\n+    val lastState = getOrCompute(validTime - slideDuration)\n+    val rdd = parent.getOrCompute(validTime)\n+    if (rdd.isDefined) {\n+      reduceFunc(lastState, rdd, validTime)\n+    } else {\n+      lastState\n+    }\n+  }\n+}\n+\n+/**\n+ * similar to ReducedWindowedDStream\n+ */\n+private[spark]\n+class PythonReducedWindowedDStream(parent: DStream[Array[Byte]],"
  }, {
    "author": {
      "login": "davies"
    },
    "body": "it's called in Python, should private[spark] also work?\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-30T07:49:57Z",
    "diffHunk": "@@ -0,0 +1,261 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.api.python\n+\n+import java.util.{ArrayList => JArrayList, List => JList}\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.api.java._\n+import org.apache.spark.api.python._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{Interval, Duration, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.api.java._\n+\n+\n+/**\n+ * Interface for Python callback function with three arguments\n+ */\n+trait PythonRDDFunction {\n+  def call(time: Long, rdds: JList[_]): JavaRDD[Array[Byte]]\n+}\n+\n+/**\n+ * Wrapper for PythonRDDFunction\n+ */\n+private[python] class RDDFunction(pfunc: PythonRDDFunction)\n+  extends function.Function2[JList[JavaRDD[_]], Time, JavaRDD[Array[Byte]]] with Serializable {\n+\n+  def wrapRDD(rdd: Option[RDD[_]]): JavaRDD[_] = {\n+    if (rdd.isDefined) {\n+      JavaRDD.fromRDD(rdd.get)\n+    } else {\n+      null\n+    }\n+  }\n+\n+  def some(jrdd: JavaRDD[Array[Byte]]): Option[RDD[Array[Byte]]] = {\n+    if (jrdd != null) {\n+      Some(jrdd.rdd)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  def apply(rdd: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    some(pfunc.call(time.milliseconds, List(wrapRDD(rdd)).asJava))\n+  }\n+\n+  def apply(rdd: Option[RDD[_]], rdd2: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    some(pfunc.call(time.milliseconds, List(wrapRDD(rdd), wrapRDD(rdd2)).asJava))\n+  }\n+\n+  // for JFunction2\n+  def call(rdds: JList[JavaRDD[_]], time: Time): JavaRDD[Array[Byte]] = {\n+    pfunc.call(time.milliseconds, rdds)\n+  }\n+}\n+\n+private[python]\n+abstract class PythonDStream(parent: DStream[_]) extends DStream[Array[Byte]] (parent.ssc) {\n+\n+  override def dependencies = List(parent)\n+\n+  override def slideDuration: Duration = parent.slideDuration\n+\n+  val asJavaDStream  = JavaDStream.fromDStream(this)\n+}\n+\n+private[spark] object PythonDStream {\n+\n+  // helper function for DStream.foreachRDD(),\n+  // cannot be `foreachRDD`, it will confusing py4j\n+  def callForeachRDD(jdstream: JavaDStream[Array[Byte]], pyfunc: PythonRDDFunction){\n+    val func = new RDDFunction(pyfunc)\n+    jdstream.dstream.foreachRDD((rdd, time) => func(Some(rdd), time))\n+  }\n+\n+  // helper function for ssc.transform()\n+  def callTransform(ssc: JavaStreamingContext, jdsteams: JList[JavaDStream[_]],\n+                    pyfunc: PythonRDDFunction)\n+    :JavaDStream[Array[Byte]] = {\n+    val func = new RDDFunction(pyfunc)\n+    ssc.transform(jdsteams, func)\n+  }\n+\n+  // convert list of RDD into queue of RDDs, for ssc.queueStream()\n+  def toRDDQueue(rdds: JArrayList[JavaRDD[Array[Byte]]]): java.util.Queue[JavaRDD[Array[Byte]]] = {\n+    val queue = new java.util.LinkedList[JavaRDD[Array[Byte]]]\n+    rdds.forall(queue.add(_))\n+    queue\n+  }\n+}\n+\n+/**\n+ * Transformed DStream in Python.\n+ *\n+ * If the result RDD is PythonRDD, then it will cache it as an template for future use,\n+ * this can reduce the Python callbacks.\n+ */\n+private[spark]\n+class PythonTransformedDStream (parent: DStream[_], pfunc: PythonRDDFunction,\n+                                var reuse: Boolean = false)\n+  extends PythonDStream(parent) {\n+\n+  val func = new RDDFunction(pfunc)\n+  var lastResult: PythonRDD = _\n+\n+  override def compute(validTime: Time): Option[RDD[Array[Byte]]] = {\n+    val rdd1 = parent.getOrCompute(validTime)\n+    if (rdd1.isEmpty) {\n+      return None\n+    }\n+    if (reuse && lastResult != null) {\n+      Some(lastResult.copyTo(rdd1.get))\n+    } else {\n+      val r = func(rdd1, validTime)\n+      if (reuse && r.isDefined && lastResult == null) {\n+        r.get match {\n+          case rdd: PythonRDD =>\n+            if (rdd.parent(0) == rdd1) {\n+              // only one PythonRDD\n+              lastResult = rdd\n+            } else {\n+              // may have multiple stages\n+              reuse = false\n+            }\n+        }\n+      }\n+      r\n+    }\n+  }\n+}\n+\n+/**\n+ * Transformed from two DStreams in Python.\n+ */\n+private[spark]\n+class PythonTransformed2DStream(parent: DStream[_], parent2: DStream[_],\n+                                pfunc: PythonRDDFunction)\n+  extends DStream[Array[Byte]] (parent.ssc) {\n+\n+  val func = new RDDFunction(pfunc)\n+\n+  override def slideDuration: Duration = parent.slideDuration\n+\n+  override def dependencies = List(parent, parent2)\n+\n+  override def compute(validTime: Time): Option[RDD[Array[Byte]]] = {\n+    func(parent.getOrCompute(validTime), parent2.getOrCompute(validTime), validTime)\n+  }\n+\n+  val asJavaDStream  = JavaDStream.fromDStream(this)\n+}\n+\n+/**\n+ * similar to StateDStream\n+ */\n+private[spark]\n+class PythonStateDStream(parent: DStream[Array[Byte]], preduceFunc: PythonRDDFunction)\n+  extends PythonDStream(parent) {\n+\n+  val reduceFunc = new RDDFunction(preduceFunc)\n+\n+  super.persist(StorageLevel.MEMORY_ONLY)\n+  override val mustCheckpoint = true\n+\n+  override def compute(validTime: Time): Option[RDD[Array[Byte]]] = {\n+    val lastState = getOrCompute(validTime - slideDuration)\n+    val rdd = parent.getOrCompute(validTime)\n+    if (rdd.isDefined) {\n+      reduceFunc(lastState, rdd, validTime)\n+    } else {\n+      lastState\n+    }\n+  }\n+}\n+\n+/**\n+ * similar to ReducedWindowedDStream\n+ */\n+private[spark]\n+class PythonReducedWindowedDStream(parent: DStream[Array[Byte]],"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Isnt this function essentially, `Option(jrdd).map { _.rdd }` ? It might be worth using that directly.\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-30T18:01:50Z",
    "diffHunk": "@@ -0,0 +1,275 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.api.python\n+\n+import java.util.{ArrayList => JArrayList, List => JList}\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.api.java._\n+import org.apache.spark.api.python._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{Interval, Duration, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.api.java._\n+\n+\n+/**\n+ * Interface for Python callback function with three arguments\n+ */\n+private[python] trait PythonRDDFunction {\n+  // callback in Python\n+  def call(time: Long, rdds: JList[_]): JavaRDD[Array[Byte]]\n+}\n+\n+/**\n+ * Wrapper for PythonRDDFunction\n+ */\n+private[python] class RDDFunction(pfunc: PythonRDDFunction)\n+  extends function.Function2[JList[JavaRDD[_]], Time, JavaRDD[Array[Byte]]] with Serializable {\n+\n+  def apply(rdd: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    PythonDStream.some(pfunc.call(time.milliseconds, List(PythonDStream.wrapRDD(rdd)).asJava))\n+  }\n+\n+  def apply(rdd: Option[RDD[_]], rdd2: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    val rdds = List(PythonDStream.wrapRDD(rdd), PythonDStream.wrapRDD(rdd2)).asJava\n+    PythonDStream.some(pfunc.call(time.milliseconds, rdds))\n+  }\n+\n+  // for function.Function2\n+  def call(rdds: JList[JavaRDD[_]], time: Time): JavaRDD[Array[Byte]] = {\n+    pfunc.call(time.milliseconds, rdds)\n+  }\n+}\n+\n+\n+/**\n+ * Base class for PythonDStream with some common methods\n+ */\n+private[python]\n+abstract class PythonDStream(parent: DStream[_], pfunc: PythonRDDFunction)\n+  extends DStream[Array[Byte]] (parent.ssc) {\n+\n+  val func = new RDDFunction(pfunc)\n+\n+  override def dependencies = List(parent)\n+\n+  override def slideDuration: Duration = parent.slideDuration\n+\n+  val asJavaDStream  = JavaDStream.fromDStream(this)\n+}\n+\n+/**\n+ * Helper functions\n+ */\n+private[python] object PythonDStream {\n+\n+  // convert Option[RDD[_]] to JavaRDD, handle null gracefully\n+  def wrapRDD(rdd: Option[RDD[_]]): JavaRDD[_] = {\n+    if (rdd.isDefined) {\n+      JavaRDD.fromRDD(rdd.get)\n+    } else {\n+      null\n+    }\n+  }\n+\n+  // convert JavaRDD to Option[RDD[Array[Byte]]] to , handle null gracefully\n+  def some(jrdd: JavaRDD[Array[Byte]]): Option[RDD[Array[Byte]]] = {"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Isnt this just `rdd.map(JavaRDD.fromRDD).orNull` ? Why not use this directly? Sorry should have observed that earlier :)\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-30T18:03:03Z",
    "diffHunk": "@@ -0,0 +1,275 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.api.python\n+\n+import java.util.{ArrayList => JArrayList, List => JList}\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.api.java._\n+import org.apache.spark.api.python._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{Interval, Duration, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.api.java._\n+\n+\n+/**\n+ * Interface for Python callback function with three arguments\n+ */\n+private[python] trait PythonRDDFunction {\n+  // callback in Python\n+  def call(time: Long, rdds: JList[_]): JavaRDD[Array[Byte]]\n+}\n+\n+/**\n+ * Wrapper for PythonRDDFunction\n+ */\n+private[python] class RDDFunction(pfunc: PythonRDDFunction)\n+  extends function.Function2[JList[JavaRDD[_]], Time, JavaRDD[Array[Byte]]] with Serializable {\n+\n+  def apply(rdd: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    PythonDStream.some(pfunc.call(time.milliseconds, List(PythonDStream.wrapRDD(rdd)).asJava))\n+  }\n+\n+  def apply(rdd: Option[RDD[_]], rdd2: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    val rdds = List(PythonDStream.wrapRDD(rdd), PythonDStream.wrapRDD(rdd2)).asJava\n+    PythonDStream.some(pfunc.call(time.milliseconds, rdds))\n+  }\n+\n+  // for function.Function2\n+  def call(rdds: JList[JavaRDD[_]], time: Time): JavaRDD[Array[Byte]] = {\n+    pfunc.call(time.milliseconds, rdds)\n+  }\n+}\n+\n+\n+/**\n+ * Base class for PythonDStream with some common methods\n+ */\n+private[python]\n+abstract class PythonDStream(parent: DStream[_], pfunc: PythonRDDFunction)\n+  extends DStream[Array[Byte]] (parent.ssc) {\n+\n+  val func = new RDDFunction(pfunc)\n+\n+  override def dependencies = List(parent)\n+\n+  override def slideDuration: Duration = parent.slideDuration\n+\n+  val asJavaDStream  = JavaDStream.fromDStream(this)\n+}\n+\n+/**\n+ * Helper functions\n+ */\n+private[python] object PythonDStream {\n+\n+  // convert Option[RDD[_]] to JavaRDD, handle null gracefully\n+  def wrapRDD(rdd: Option[RDD[_]]): JavaRDD[_] = {"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "The style is wrong I think, there should be space between `:` and `JavaDStream`. How about\n\n```\ndef callTransform(\n      ssc: JavaStreamingContext, \n      jdsteams: JList[JavaDStream[_]], \n      pyfunc: PythonRDDFunction\n   ): JavaDStream[Array[Byte]] = {\n```\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-30T18:06:03Z",
    "diffHunk": "@@ -0,0 +1,275 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.api.python\n+\n+import java.util.{ArrayList => JArrayList, List => JList}\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.api.java._\n+import org.apache.spark.api.python._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{Interval, Duration, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.api.java._\n+\n+\n+/**\n+ * Interface for Python callback function with three arguments\n+ */\n+private[python] trait PythonRDDFunction {\n+  // callback in Python\n+  def call(time: Long, rdds: JList[_]): JavaRDD[Array[Byte]]\n+}\n+\n+/**\n+ * Wrapper for PythonRDDFunction\n+ */\n+private[python] class RDDFunction(pfunc: PythonRDDFunction)\n+  extends function.Function2[JList[JavaRDD[_]], Time, JavaRDD[Array[Byte]]] with Serializable {\n+\n+  def apply(rdd: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    PythonDStream.some(pfunc.call(time.milliseconds, List(PythonDStream.wrapRDD(rdd)).asJava))\n+  }\n+\n+  def apply(rdd: Option[RDD[_]], rdd2: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    val rdds = List(PythonDStream.wrapRDD(rdd), PythonDStream.wrapRDD(rdd2)).asJava\n+    PythonDStream.some(pfunc.call(time.milliseconds, rdds))\n+  }\n+\n+  // for function.Function2\n+  def call(rdds: JList[JavaRDD[_]], time: Time): JavaRDD[Array[Byte]] = {\n+    pfunc.call(time.milliseconds, rdds)\n+  }\n+}\n+\n+\n+/**\n+ * Base class for PythonDStream with some common methods\n+ */\n+private[python]\n+abstract class PythonDStream(parent: DStream[_], pfunc: PythonRDDFunction)\n+  extends DStream[Array[Byte]] (parent.ssc) {\n+\n+  val func = new RDDFunction(pfunc)\n+\n+  override def dependencies = List(parent)\n+\n+  override def slideDuration: Duration = parent.slideDuration\n+\n+  val asJavaDStream  = JavaDStream.fromDStream(this)\n+}\n+\n+/**\n+ * Helper functions\n+ */\n+private[python] object PythonDStream {\n+\n+  // convert Option[RDD[_]] to JavaRDD, handle null gracefully\n+  def wrapRDD(rdd: Option[RDD[_]]): JavaRDD[_] = {\n+    if (rdd.isDefined) {\n+      JavaRDD.fromRDD(rdd.get)\n+    } else {\n+      null\n+    }\n+  }\n+\n+  // convert JavaRDD to Option[RDD[Array[Byte]]] to , handle null gracefully\n+  def some(jrdd: JavaRDD[Array[Byte]]): Option[RDD[Array[Byte]]] = {\n+    if (jrdd != null) {\n+      Some(jrdd.rdd)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  // helper function for DStream.foreachRDD(),\n+  // cannot be `foreachRDD`, it will confusing py4j\n+  def callForeachRDD(jdstream: JavaDStream[Array[Byte]], pfunc: PythonRDDFunction){\n+    val func = new RDDFunction((pfunc))\n+    jdstream.dstream.foreachRDD((rdd, time) => func(Some(rdd), time))\n+  }\n+\n+  // helper function for ssc.transform()\n+  def callTransform(ssc: JavaStreamingContext, jdsteams: JList[JavaDStream[_]],\n+                    pyfunc: PythonRDDFunction)\n+    :JavaDStream[Array[Byte]] = {"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Need space between `)` and `{`\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-30T18:06:46Z",
    "diffHunk": "@@ -0,0 +1,275 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.api.python\n+\n+import java.util.{ArrayList => JArrayList, List => JList}\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.api.java._\n+import org.apache.spark.api.python._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{Interval, Duration, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.api.java._\n+\n+\n+/**\n+ * Interface for Python callback function with three arguments\n+ */\n+private[python] trait PythonRDDFunction {\n+  // callback in Python\n+  def call(time: Long, rdds: JList[_]): JavaRDD[Array[Byte]]\n+}\n+\n+/**\n+ * Wrapper for PythonRDDFunction\n+ */\n+private[python] class RDDFunction(pfunc: PythonRDDFunction)\n+  extends function.Function2[JList[JavaRDD[_]], Time, JavaRDD[Array[Byte]]] with Serializable {\n+\n+  def apply(rdd: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    PythonDStream.some(pfunc.call(time.milliseconds, List(PythonDStream.wrapRDD(rdd)).asJava))\n+  }\n+\n+  def apply(rdd: Option[RDD[_]], rdd2: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    val rdds = List(PythonDStream.wrapRDD(rdd), PythonDStream.wrapRDD(rdd2)).asJava\n+    PythonDStream.some(pfunc.call(time.milliseconds, rdds))\n+  }\n+\n+  // for function.Function2\n+  def call(rdds: JList[JavaRDD[_]], time: Time): JavaRDD[Array[Byte]] = {\n+    pfunc.call(time.milliseconds, rdds)\n+  }\n+}\n+\n+\n+/**\n+ * Base class for PythonDStream with some common methods\n+ */\n+private[python]\n+abstract class PythonDStream(parent: DStream[_], pfunc: PythonRDDFunction)\n+  extends DStream[Array[Byte]] (parent.ssc) {\n+\n+  val func = new RDDFunction(pfunc)\n+\n+  override def dependencies = List(parent)\n+\n+  override def slideDuration: Duration = parent.slideDuration\n+\n+  val asJavaDStream  = JavaDStream.fromDStream(this)\n+}\n+\n+/**\n+ * Helper functions\n+ */\n+private[python] object PythonDStream {\n+\n+  // convert Option[RDD[_]] to JavaRDD, handle null gracefully\n+  def wrapRDD(rdd: Option[RDD[_]]): JavaRDD[_] = {\n+    if (rdd.isDefined) {\n+      JavaRDD.fromRDD(rdd.get)\n+    } else {\n+      null\n+    }\n+  }\n+\n+  // convert JavaRDD to Option[RDD[Array[Byte]]] to , handle null gracefully\n+  def some(jrdd: JavaRDD[Array[Byte]]): Option[RDD[Array[Byte]]] = {\n+    if (jrdd != null) {\n+      Some(jrdd.rdd)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  // helper function for DStream.foreachRDD(),\n+  // cannot be `foreachRDD`, it will confusing py4j\n+  def callForeachRDD(jdstream: JavaDStream[Array[Byte]], pfunc: PythonRDDFunction){"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "extra space.\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-30T18:07:18Z",
    "diffHunk": "@@ -0,0 +1,275 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.api.python\n+\n+import java.util.{ArrayList => JArrayList, List => JList}\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.spark.api.java._\n+import org.apache.spark.api.python._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{Interval, Duration, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.api.java._\n+\n+\n+/**\n+ * Interface for Python callback function with three arguments\n+ */\n+private[python] trait PythonRDDFunction {\n+  // callback in Python\n+  def call(time: Long, rdds: JList[_]): JavaRDD[Array[Byte]]\n+}\n+\n+/**\n+ * Wrapper for PythonRDDFunction\n+ */\n+private[python] class RDDFunction(pfunc: PythonRDDFunction)\n+  extends function.Function2[JList[JavaRDD[_]], Time, JavaRDD[Array[Byte]]] with Serializable {\n+\n+  def apply(rdd: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    PythonDStream.some(pfunc.call(time.milliseconds, List(PythonDStream.wrapRDD(rdd)).asJava))\n+  }\n+\n+  def apply(rdd: Option[RDD[_]], rdd2: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    val rdds = List(PythonDStream.wrapRDD(rdd), PythonDStream.wrapRDD(rdd2)).asJava\n+    PythonDStream.some(pfunc.call(time.milliseconds, rdds))\n+  }\n+\n+  // for function.Function2\n+  def call(rdds: JList[JavaRDD[_]], time: Time): JavaRDD[Array[Byte]] = {\n+    pfunc.call(time.milliseconds, rdds)\n+  }\n+}\n+\n+\n+/**\n+ * Base class for PythonDStream with some common methods\n+ */\n+private[python]\n+abstract class PythonDStream(parent: DStream[_], pfunc: PythonRDDFunction)\n+  extends DStream[Array[Byte]] (parent.ssc) {\n+\n+  val func = new RDDFunction(pfunc)\n+\n+  override def dependencies = List(parent)\n+\n+  override def slideDuration: Duration = parent.slideDuration\n+\n+  val asJavaDStream  = JavaDStream.fromDStream(this)\n+}\n+\n+/**\n+ * Helper functions\n+ */\n+private[python] object PythonDStream {\n+\n+  // convert Option[RDD[_]] to JavaRDD, handle null gracefully\n+  def wrapRDD(rdd: Option[RDD[_]]): JavaRDD[_] = {\n+    if (rdd.isDefined) {\n+      JavaRDD.fromRDD(rdd.get)\n+    } else {\n+      null\n+    }\n+  }\n+\n+  // convert JavaRDD to Option[RDD[Array[Byte]]] to , handle null gracefully\n+  def some(jrdd: JavaRDD[Array[Byte]]): Option[RDD[Array[Byte]]] = {\n+    if (jrdd != null) {\n+      Some(jrdd.rdd)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  // helper function for DStream.foreachRDD(),\n+  // cannot be `foreachRDD`, it will confusing py4j\n+  def callForeachRDD(jdstream: JavaDStream[Array[Byte]], pfunc: PythonRDDFunction){\n+    val func = new RDDFunction((pfunc))\n+    jdstream.dstream.foreachRDD((rdd, time) => func(Some(rdd), time))\n+  }\n+\n+  // helper function for ssc.transform()\n+  def callTransform(ssc: JavaStreamingContext, jdsteams: JList[JavaDStream[_]],\n+                    pyfunc: PythonRDDFunction)\n+    :JavaDStream[Array[Byte]] = {\n+    val func = new RDDFunction(pyfunc)\n+    ssc.transform(jdsteams, func)\n+  }\n+\n+  // convert list of RDD into queue of RDDs, for ssc.queueStream()\n+  def toRDDQueue(rdds: JArrayList[JavaRDD[Array[Byte]]]): java.util.Queue[JavaRDD[Array[Byte]]] = {\n+    val queue = new java.util.LinkedList[JavaRDD[Array[Byte]]]\n+    rdds.forall(queue.add(_))\n+    queue\n+  }\n+}\n+\n+/**\n+ * Transformed DStream in Python.\n+ *\n+ * If `reuse` is true and the result of the `func` is an PythonRDD, then it will cache it\n+ * as an template for future use, this can reduce the Python callbacks.\n+ */\n+private[python]\n+class PythonTransformedDStream (parent: DStream[_], pfunc: PythonRDDFunction,\n+                                var reuse: Boolean = false)\n+  extends PythonDStream(parent, pfunc) {\n+\n+  // rdd returned by func\n+  var lastResult: PythonRDD = _\n+\n+  override def compute(validTime: Time): Option[RDD[Array[Byte]]] = {\n+    val rdd = parent.getOrCompute(validTime)\n+    if (rdd.isEmpty) {\n+      return None\n+    }\n+    if (reuse && lastResult != null) {\n+      // use the previous result as the template to generate new RDD\n+      Some(lastResult.copyTo(rdd.get))\n+    } else {\n+      val r = func(rdd, validTime)\n+      if (reuse && r.isDefined && lastResult == null) {\n+        // try to use the result as a template\n+        r.get match {\n+          case pyrdd: PythonRDD =>\n+            if (pyrdd.parent(0) == rdd) {\n+              // only one PythonRDD\n+              lastResult = pyrdd\n+            } else {\n+              // maybe have multiple stages, don't check it anymore\n+              reuse = false\n+            }\n+        }\n+      }\n+      r\n+    }\n+  }\n+}\n+\n+/**\n+ * Transformed from two DStreams in Python.\n+ */\n+private[python]\n+class PythonTransformed2DStream(parent: DStream[_], parent2: DStream[_],\n+                                pfunc: PythonRDDFunction)\n+  extends DStream[Array[Byte]] (parent.ssc) {\n+\n+  val func = new RDDFunction(pfunc)\n+\n+  override def slideDuration: Duration = parent.slideDuration\n+\n+  override def dependencies = List(parent, parent2)\n+\n+  override def compute(validTime: Time): Option[RDD[Array[Byte]]] = {\n+    func(parent.getOrCompute(validTime), parent2.getOrCompute(validTime), validTime)\n+  }\n+\n+  val asJavaDStream  = JavaDStream.fromDStream(this)"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Incorrect scala style. See http://docs.scala-lang.org/style/declarations.html\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-30T22:25:08Z",
    "diffHunk": "@@ -0,0 +1,301 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.api.python\n+\n+import java.io.{ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.Proxy\n+import java.util.{ArrayList => JArrayList, List => JList}\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.api.java._\n+import org.apache.spark.api.python._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{Interval, Duration, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.api.java._\n+\n+\n+/**\n+ * Interface for Python callback function with three arguments\n+ */\n+private[python] trait PythonRDDFunction {\n+  def call(time: Long, rdds: JList[_]): JavaRDD[Array[Byte]]\n+}\n+\n+/**\n+ * Wrapper for PythonRDDFunction\n+ * TODO: support checkpoint\n+ */\n+private[python] class RDDFunction(@transient var pfunc: PythonRDDFunction)\n+  extends function.Function2[JList[JavaRDD[_]], Time, JavaRDD[Array[Byte]]] with Serializable {\n+\n+  def apply(rdd: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    Option(pfunc.call(time.milliseconds, List(rdd.map(JavaRDD.fromRDD(_)).orNull).asJava)).map(_.rdd)\n+  }\n+\n+  def apply(rdd: Option[RDD[_]], rdd2: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    val rdds = List(rdd.map(JavaRDD.fromRDD(_)).orNull, rdd2.map(JavaRDD.fromRDD(_)).orNull).asJava\n+    Option(pfunc.call(time.milliseconds, rdds)).map(_.rdd)\n+  }\n+\n+  // for function.Function2\n+  def call(rdds: JList[JavaRDD[_]], time: Time): JavaRDD[Array[Byte]] = {\n+    pfunc.call(time.milliseconds, rdds)\n+  }\n+\n+  private def writeObject(out: ObjectOutputStream): Unit = {\n+    assert(PythonDStream.serializer != null, \"Serializer has not been registered!\")\n+    val bytes = PythonDStream.serializer.serialize(pfunc)\n+    out.writeInt(bytes.length)\n+    out.write(bytes)\n+  }\n+\n+  private def readObject(in: ObjectInputStream): Unit = {\n+    assert(PythonDStream.serializer != null, \"Serializer has not been registered!\")\n+    val length = in.readInt()\n+    val bytes = new Array[Byte](length)\n+    in.readFully(bytes)\n+    pfunc = PythonDStream.serializer.deserialize(bytes)\n+  }\n+}\n+\n+/**\n+ * Inferface for Python Serializer to serialize PythonRDDFunction\n+ */\n+private[python] trait PythonRDDFunctionSerializer {\n+  def dumps(id: String): Array[Byte]  //\n+  def loads(bytes: Array[Byte]): PythonRDDFunction\n+}\n+\n+/**\n+ * Wrapper for PythonRDDFunctionSerializer\n+ */\n+private[python] class RDDFunctionSerializer(pser: PythonRDDFunctionSerializer) {\n+  def serialize(func: PythonRDDFunction): Array[Byte] = {\n+    // get the id of PythonRDDFunction in py4j\n+    val h = Proxy.getInvocationHandler(func.asInstanceOf[Proxy])\n+    val f = h.getClass().getDeclaredField(\"id\");\n+    f.setAccessible(true);\n+    val id = f.get(h).asInstanceOf[String];\n+    pser.dumps(id)\n+  }\n+\n+  def deserialize(bytes: Array[Byte]): PythonRDDFunction = {\n+    pser.loads(bytes)\n+  }\n+}\n+\n+/**\n+ * Helper functions\n+ */\n+private[python] object PythonDStream {\n+\n+  // A serializer in Python, used to serialize PythonRDDFunction\n+  var serializer: RDDFunctionSerializer = _\n+\n+  // Register a serializer from Python, should be called during initialization\n+  def registerSerializer(ser: PythonRDDFunctionSerializer) = {\n+    serializer = new RDDFunctionSerializer(ser)\n+  }\n+\n+  // helper function for DStream.foreachRDD(),\n+  // cannot be `foreachRDD`, it will confusing py4j\n+  def callForeachRDD(jdstream: JavaDStream[Array[Byte]], pfunc: PythonRDDFunction) {\n+    val func = new RDDFunction((pfunc))\n+    jdstream.dstream.foreachRDD((rdd, time) => func(Some(rdd), time))\n+  }\n+\n+  // convert list of RDD into queue of RDDs, for ssc.queueStream()\n+  def toRDDQueue(rdds: JArrayList[JavaRDD[Array[Byte]]]): java.util.Queue[JavaRDD[Array[Byte]]] = {\n+    val queue = new java.util.LinkedList[JavaRDD[Array[Byte]]]\n+    rdds.forall(queue.add(_))\n+    queue\n+  }\n+}\n+\n+/**\n+ * Base class for PythonDStream with some common methods\n+ */\n+private[python]\n+abstract class PythonDStream(parent: DStream[_], @transient pfunc: PythonRDDFunction)\n+  extends DStream[Array[Byte]] (parent.ssc) {\n+\n+  val func = new RDDFunction(pfunc)\n+\n+  override def dependencies = List(parent)\n+\n+  override def slideDuration: Duration = parent.slideDuration\n+\n+  val asJavaDStream  = JavaDStream.fromDStream(this)\n+}\n+\n+/**\n+ * Transformed DStream in Python.\n+ *\n+ * If `reuse` is true and the result of the `func` is an PythonRDD, then it will cache it\n+ * as an template for future use, this can reduce the Python callbacks.\n+ */\n+private[python]\n+class PythonTransformedDStream (parent: DStream[_], @transient pfunc: PythonRDDFunction,\n+                                var reuse: Boolean = false)\n+  extends PythonDStream(parent, pfunc) {\n+\n+  // rdd returned by func\n+  var lastResult: PythonRDD = _\n+\n+  override def compute(validTime: Time): Option[RDD[Array[Byte]]] = {\n+    val rdd = parent.getOrCompute(validTime)\n+    if (rdd.isEmpty) {\n+      return None\n+    }\n+    if (reuse && lastResult != null) {\n+      // use the previous result as the template to generate new RDD\n+      Some(lastResult.copyTo(rdd.get))\n+    } else {\n+      val r = func(rdd, validTime)\n+      if (reuse && r.isDefined && lastResult == null) {\n+        // try to use the result as a template\n+        r.get match {\n+          case pyrdd: PythonRDD =>\n+            if (pyrdd.parent(0) == rdd) {\n+              // only one PythonRDD\n+              lastResult = pyrdd\n+            } else {\n+              // maybe have multiple stages, don't check it anymore\n+              reuse = false\n+            }\n+        }\n+      }\n+      r\n+    }\n+  }\n+}\n+\n+/**\n+ * Transformed from two DStreams in Python.\n+ */\n+private[python]\n+class PythonTransformed2DStream(parent: DStream[_], parent2: DStream[_],\n+                                @transient pfunc: PythonRDDFunction)\n+  extends DStream[Array[Byte]] (parent.ssc) {\n+\n+  val func = new RDDFunction(pfunc)\n+\n+  override def slideDuration: Duration = parent.slideDuration\n+\n+  override def dependencies = List(parent, parent2)\n+\n+  override def compute(validTime: Time): Option[RDD[Array[Byte]]] = {\n+    func(parent.getOrCompute(validTime), parent2.getOrCompute(validTime), validTime)\n+  }\n+\n+  val asJavaDStream = JavaDStream.fromDStream(this)\n+}\n+\n+/**\n+ * similar to StateDStream\n+ */\n+private[python]\n+class PythonStateDStream(parent: DStream[Array[Byte]], @transient reduceFunc: PythonRDDFunction)\n+  extends PythonDStream(parent, reduceFunc) {\n+\n+  super.persist(StorageLevel.MEMORY_ONLY)\n+  override val mustCheckpoint = true\n+\n+  override def compute(validTime: Time): Option[RDD[Array[Byte]]] = {\n+    val lastState = getOrCompute(validTime - slideDuration)\n+    val rdd = parent.getOrCompute(validTime)\n+    if (rdd.isDefined) {\n+      func(lastState, rdd, validTime)\n+    } else {\n+      lastState\n+    }\n+  }\n+}\n+\n+/**\n+ * similar to ReducedWindowedDStream\n+ */\n+private[python]\n+class PythonReducedWindowedDStream(parent: DStream[Array[Byte]],"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "style issue. please dont add a comment in the middle of a continuing line\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-30T22:26:05Z",
    "diffHunk": "@@ -0,0 +1,301 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.api.python\n+\n+import java.io.{ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.Proxy\n+import java.util.{ArrayList => JArrayList, List => JList}\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.api.java._\n+import org.apache.spark.api.python._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{Interval, Duration, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.api.java._\n+\n+\n+/**\n+ * Interface for Python callback function with three arguments\n+ */\n+private[python] trait PythonRDDFunction {\n+  def call(time: Long, rdds: JList[_]): JavaRDD[Array[Byte]]\n+}\n+\n+/**\n+ * Wrapper for PythonRDDFunction\n+ * TODO: support checkpoint\n+ */\n+private[python] class RDDFunction(@transient var pfunc: PythonRDDFunction)\n+  extends function.Function2[JList[JavaRDD[_]], Time, JavaRDD[Array[Byte]]] with Serializable {\n+\n+  def apply(rdd: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    Option(pfunc.call(time.milliseconds, List(rdd.map(JavaRDD.fromRDD(_)).orNull).asJava)).map(_.rdd)\n+  }\n+\n+  def apply(rdd: Option[RDD[_]], rdd2: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    val rdds = List(rdd.map(JavaRDD.fromRDD(_)).orNull, rdd2.map(JavaRDD.fromRDD(_)).orNull).asJava\n+    Option(pfunc.call(time.milliseconds, rdds)).map(_.rdd)\n+  }\n+\n+  // for function.Function2\n+  def call(rdds: JList[JavaRDD[_]], time: Time): JavaRDD[Array[Byte]] = {\n+    pfunc.call(time.milliseconds, rdds)\n+  }\n+\n+  private def writeObject(out: ObjectOutputStream): Unit = {\n+    assert(PythonDStream.serializer != null, \"Serializer has not been registered!\")\n+    val bytes = PythonDStream.serializer.serialize(pfunc)\n+    out.writeInt(bytes.length)\n+    out.write(bytes)\n+  }\n+\n+  private def readObject(in: ObjectInputStream): Unit = {\n+    assert(PythonDStream.serializer != null, \"Serializer has not been registered!\")\n+    val length = in.readInt()\n+    val bytes = new Array[Byte](length)\n+    in.readFully(bytes)\n+    pfunc = PythonDStream.serializer.deserialize(bytes)\n+  }\n+}\n+\n+/**\n+ * Inferface for Python Serializer to serialize PythonRDDFunction\n+ */\n+private[python] trait PythonRDDFunctionSerializer {\n+  def dumps(id: String): Array[Byte]  //\n+  def loads(bytes: Array[Byte]): PythonRDDFunction\n+}\n+\n+/**\n+ * Wrapper for PythonRDDFunctionSerializer\n+ */\n+private[python] class RDDFunctionSerializer(pser: PythonRDDFunctionSerializer) {\n+  def serialize(func: PythonRDDFunction): Array[Byte] = {\n+    // get the id of PythonRDDFunction in py4j\n+    val h = Proxy.getInvocationHandler(func.asInstanceOf[Proxy])\n+    val f = h.getClass().getDeclaredField(\"id\");\n+    f.setAccessible(true);\n+    val id = f.get(h).asInstanceOf[String];\n+    pser.dumps(id)\n+  }\n+\n+  def deserialize(bytes: Array[Byte]): PythonRDDFunction = {\n+    pser.loads(bytes)\n+  }\n+}\n+\n+/**\n+ * Helper functions\n+ */\n+private[python] object PythonDStream {\n+\n+  // A serializer in Python, used to serialize PythonRDDFunction\n+  var serializer: RDDFunctionSerializer = _\n+\n+  // Register a serializer from Python, should be called during initialization\n+  def registerSerializer(ser: PythonRDDFunctionSerializer) = {\n+    serializer = new RDDFunctionSerializer(ser)\n+  }\n+\n+  // helper function for DStream.foreachRDD(),\n+  // cannot be `foreachRDD`, it will confusing py4j\n+  def callForeachRDD(jdstream: JavaDStream[Array[Byte]], pfunc: PythonRDDFunction) {\n+    val func = new RDDFunction((pfunc))\n+    jdstream.dstream.foreachRDD((rdd, time) => func(Some(rdd), time))\n+  }\n+\n+  // convert list of RDD into queue of RDDs, for ssc.queueStream()\n+  def toRDDQueue(rdds: JArrayList[JavaRDD[Array[Byte]]]): java.util.Queue[JavaRDD[Array[Byte]]] = {\n+    val queue = new java.util.LinkedList[JavaRDD[Array[Byte]]]\n+    rdds.forall(queue.add(_))\n+    queue\n+  }\n+}\n+\n+/**\n+ * Base class for PythonDStream with some common methods\n+ */\n+private[python]\n+abstract class PythonDStream(parent: DStream[_], @transient pfunc: PythonRDDFunction)\n+  extends DStream[Array[Byte]] (parent.ssc) {\n+\n+  val func = new RDDFunction(pfunc)\n+\n+  override def dependencies = List(parent)\n+\n+  override def slideDuration: Duration = parent.slideDuration\n+\n+  val asJavaDStream  = JavaDStream.fromDStream(this)\n+}\n+\n+/**\n+ * Transformed DStream in Python.\n+ *\n+ * If `reuse` is true and the result of the `func` is an PythonRDD, then it will cache it\n+ * as an template for future use, this can reduce the Python callbacks.\n+ */\n+private[python]\n+class PythonTransformedDStream (parent: DStream[_], @transient pfunc: PythonRDDFunction,\n+                                var reuse: Boolean = false)\n+  extends PythonDStream(parent, pfunc) {\n+\n+  // rdd returned by func\n+  var lastResult: PythonRDD = _\n+\n+  override def compute(validTime: Time): Option[RDD[Array[Byte]]] = {\n+    val rdd = parent.getOrCompute(validTime)\n+    if (rdd.isEmpty) {\n+      return None\n+    }\n+    if (reuse && lastResult != null) {\n+      // use the previous result as the template to generate new RDD\n+      Some(lastResult.copyTo(rdd.get))\n+    } else {\n+      val r = func(rdd, validTime)\n+      if (reuse && r.isDefined && lastResult == null) {\n+        // try to use the result as a template\n+        r.get match {\n+          case pyrdd: PythonRDD =>\n+            if (pyrdd.parent(0) == rdd) {\n+              // only one PythonRDD\n+              lastResult = pyrdd\n+            } else {\n+              // maybe have multiple stages, don't check it anymore\n+              reuse = false\n+            }\n+        }\n+      }\n+      r\n+    }\n+  }\n+}\n+\n+/**\n+ * Transformed from two DStreams in Python.\n+ */\n+private[python]\n+class PythonTransformed2DStream(parent: DStream[_], parent2: DStream[_],\n+                                @transient pfunc: PythonRDDFunction)\n+  extends DStream[Array[Byte]] (parent.ssc) {\n+\n+  val func = new RDDFunction(pfunc)\n+\n+  override def slideDuration: Duration = parent.slideDuration\n+\n+  override def dependencies = List(parent, parent2)\n+\n+  override def compute(validTime: Time): Option[RDD[Array[Byte]]] = {\n+    func(parent.getOrCompute(validTime), parent2.getOrCompute(validTime), validTime)\n+  }\n+\n+  val asJavaDStream = JavaDStream.fromDStream(this)\n+}\n+\n+/**\n+ * similar to StateDStream\n+ */\n+private[python]\n+class PythonStateDStream(parent: DStream[Array[Byte]], @transient reduceFunc: PythonRDDFunction)\n+  extends PythonDStream(parent, reduceFunc) {\n+\n+  super.persist(StorageLevel.MEMORY_ONLY)\n+  override val mustCheckpoint = true\n+\n+  override def compute(validTime: Time): Option[RDD[Array[Byte]]] = {\n+    val lastState = getOrCompute(validTime - slideDuration)\n+    val rdd = parent.getOrCompute(validTime)\n+    if (rdd.isDefined) {\n+      func(lastState, rdd, validTime)\n+    } else {\n+      lastState\n+    }\n+  }\n+}\n+\n+/**\n+ * similar to ReducedWindowedDStream\n+ */\n+private[python]\n+class PythonReducedWindowedDStream(parent: DStream[Array[Byte]],\n+                                   @transient preduceFunc: PythonRDDFunction,\n+                                   @transient pinvReduceFunc: PythonRDDFunction,\n+                                   _windowDuration: Duration,\n+                                   _slideDuration: Duration\n+                                   ) extends PythonDStream(parent, preduceFunc) {\n+\n+  super.persist(StorageLevel.MEMORY_ONLY)\n+  override val mustCheckpoint = true\n+\n+  val invReduceFunc = new RDDFunction(pinvReduceFunc)\n+\n+  def windowDuration: Duration = _windowDuration\n+  override def slideDuration: Duration = _slideDuration\n+  override def parentRememberDuration: Duration = rememberDuration + windowDuration\n+\n+  override def compute(validTime: Time): Option[RDD[Array[Byte]]] = {\n+    val currentTime = validTime\n+    val current = new Interval(currentTime - windowDuration,\n+      currentTime)\n+    val previous = current - slideDuration\n+\n+    //  _____________________________\n+    // |  previous window   _________|___________________\n+    // |___________________|       current window        |  --------------> Time\n+    //                     |_____________________________|\n+    //\n+    // |________ _________|          |________ _________|\n+    //          |                             |\n+    //          V                             V\n+    //       old RDDs                     new RDDs\n+    //\n+\n+    val previousRDD = getOrCompute(previous.endTime)\n+\n+    if (pinvReduceFunc != null && previousRDD.isDefined\n+        // for small window, reduce once will be better than twice"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "I still find the name very confusing, even after looking at this code twice. Its more intuitive to call is TransformFunction (that is, function for transform operation) that RDDFunction (makes me think function of RDD operation, which is wrong). Please rename `RDDFunction` to `TransformFunction` and `PythonRDDFunction` to `PythonTransformFunction`. \n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-30T22:30:39Z",
    "diffHunk": "@@ -0,0 +1,301 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.api.python\n+\n+import java.io.{ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.Proxy\n+import java.util.{ArrayList => JArrayList, List => JList}\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.api.java._\n+import org.apache.spark.api.python._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{Interval, Duration, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.api.java._\n+\n+\n+/**\n+ * Interface for Python callback function with three arguments\n+ */\n+private[python] trait PythonRDDFunction {\n+  def call(time: Long, rdds: JList[_]): JavaRDD[Array[Byte]]\n+}\n+\n+/**\n+ * Wrapper for PythonRDDFunction\n+ * TODO: support checkpoint\n+ */\n+private[python] class RDDFunction(@transient var pfunc: PythonRDDFunction)"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Function2 is already Serializable.\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-10-02T01:12:39Z",
    "diffHunk": "@@ -0,0 +1,304 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.api.python\n+\n+import java.io.{ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.Proxy\n+import java.util.{ArrayList => JArrayList, List => JList}\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.api.java._\n+import org.apache.spark.api.python._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{Interval, Duration, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.api.java._\n+\n+\n+/**\n+ * Interface for Python callback function with three arguments\n+ */\n+private[python] trait PythonTransformFunction {\n+  def call(time: Long, rdds: JList[_]): JavaRDD[Array[Byte]]\n+}\n+\n+/**\n+ * Wrapper for PythonTransformFunction\n+ * TODO: support checkpoint\n+ */\n+private[python] class TransformFunction(@transient var pfunc: PythonTransformFunction)\n+  extends function.Function2[JList[JavaRDD[_]], Time, JavaRDD[Array[Byte]]] with Serializable {"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Extra `//`\nIts more intuitive (at least on Java/Scala side) to call these methods `serialize/deserialize`\nnit: move this trait to be near `PythonTransformFunction`\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-10-02T01:13:47Z",
    "diffHunk": "@@ -0,0 +1,304 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.api.python\n+\n+import java.io.{ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.Proxy\n+import java.util.{ArrayList => JArrayList, List => JList}\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.api.java._\n+import org.apache.spark.api.python._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{Interval, Duration, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.api.java._\n+\n+\n+/**\n+ * Interface for Python callback function with three arguments\n+ */\n+private[python] trait PythonTransformFunction {\n+  def call(time: Long, rdds: JList[_]): JavaRDD[Array[Byte]]\n+}\n+\n+/**\n+ * Wrapper for PythonTransformFunction\n+ * TODO: support checkpoint\n+ */\n+private[python] class TransformFunction(@transient var pfunc: PythonTransformFunction)\n+  extends function.Function2[JList[JavaRDD[_]], Time, JavaRDD[Array[Byte]]] with Serializable {\n+\n+  def apply(rdd: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    Option(pfunc.call(time.milliseconds, List(rdd.map(JavaRDD.fromRDD(_)).orNull).asJava))\n+      .map(_.rdd)\n+  }\n+\n+  def apply(rdd: Option[RDD[_]], rdd2: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    val rdds = List(rdd.map(JavaRDD.fromRDD(_)).orNull, rdd2.map(JavaRDD.fromRDD(_)).orNull).asJava\n+    Option(pfunc.call(time.milliseconds, rdds)).map(_.rdd)\n+  }\n+\n+  // for function.Function2\n+  def call(rdds: JList[JavaRDD[_]], time: Time): JavaRDD[Array[Byte]] = {\n+    pfunc.call(time.milliseconds, rdds)\n+  }\n+\n+  private def writeObject(out: ObjectOutputStream): Unit = {\n+    assert(PythonDStream.serializer != null, \"Serializer has not been registered!\")\n+    val bytes = PythonDStream.serializer.serialize(pfunc)\n+    out.writeInt(bytes.length)\n+    out.write(bytes)\n+  }\n+\n+  private def readObject(in: ObjectInputStream): Unit = {\n+    assert(PythonDStream.serializer != null, \"Serializer has not been registered!\")\n+    val length = in.readInt()\n+    val bytes = new Array[Byte](length)\n+    in.readFully(bytes)\n+    pfunc = PythonDStream.serializer.deserialize(bytes)\n+  }\n+}\n+\n+/**\n+ * Interface for Python Serializer to serialize PythonTransformFunction\n+ */\n+private[python] trait PythonTransformFunctionSerializer {\n+  def dumps(id: String): Array[Byte]  //"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "It looks like this now only takes two arguments.\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-10-03T01:57:10Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.api.python\n+\n+import java.io.{ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.Proxy\n+import java.util.{ArrayList => JArrayList, List => JList}\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+import scala.language.existentials\n+\n+import py4j.GatewayServer\n+\n+import org.apache.spark.api.java._\n+import org.apache.spark.api.python._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{Interval, Duration, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.api.java._\n+\n+\n+/**\n+ * Interface for Python callback function with three arguments"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "How about something like this for the docstring:\n\n> Wraps a PythonTransformFunction (which is a Python object accessed through Py4J) so that it looks like a Scala function and can be transparently serialized and deserialized by Java.\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-10-07T22:46:33Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.api.python\n+\n+import java.io.{ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.Proxy\n+import java.util.{ArrayList => JArrayList, List => JList}\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+import scala.language.existentials\n+\n+import py4j.GatewayServer\n+\n+import org.apache.spark.api.java._\n+import org.apache.spark.api.python._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{Interval, Duration, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.api.java._\n+\n+\n+/**\n+ * Interface for Python callback function which is used to transform RDDs\n+ */\n+private[python] trait PythonTransformFunction {\n+  def call(time: Long, rdds: JList[_]): JavaRDD[Array[Byte]]\n+}\n+\n+/**\n+ * Interface for Python Serializer to serialize PythonTransformFunction\n+ */\n+private[python] trait PythonTransformFunctionSerializer {\n+  def dumps(id: String): Array[Byte]\n+  def loads(bytes: Array[Byte]): PythonTransformFunction\n+}\n+\n+/**\n+ * Wrapper for PythonTransformFunction"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "I'd maybe add a note here saying that PythonTransformFunctionSerializer is logically a singleton that's happens to be implemented as a Python object.\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-10-07T22:54:14Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.api.python\n+\n+import java.io.{ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.Proxy\n+import java.util.{ArrayList => JArrayList, List => JList}\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+import scala.language.existentials\n+\n+import py4j.GatewayServer\n+\n+import org.apache.spark.api.java._\n+import org.apache.spark.api.python._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{Interval, Duration, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.api.java._\n+\n+\n+/**\n+ * Interface for Python callback function which is used to transform RDDs\n+ */\n+private[python] trait PythonTransformFunction {\n+  def call(time: Long, rdds: JList[_]): JavaRDD[Array[Byte]]\n+}\n+\n+/**\n+ * Interface for Python Serializer to serialize PythonTransformFunction\n+ */\n+private[python] trait PythonTransformFunctionSerializer {\n+  def dumps(id: String): Array[Byte]\n+  def loads(bytes: Array[Byte]): PythonTransformFunction\n+}\n+\n+/**\n+ * Wrapper for PythonTransformFunction\n+ */\n+private[python] class TransformFunction(@transient var pfunc: PythonTransformFunction)\n+  extends function.Function2[JList[JavaRDD[_]], Time, JavaRDD[Array[Byte]]] with Serializable {\n+\n+  def apply(rdd: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    Option(pfunc.call(time.milliseconds, List(rdd.map(JavaRDD.fromRDD(_)).orNull).asJava))\n+      .map(_.rdd)\n+  }\n+\n+  def apply(rdd: Option[RDD[_]], rdd2: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    val rdds = List(rdd.map(JavaRDD.fromRDD(_)).orNull, rdd2.map(JavaRDD.fromRDD(_)).orNull).asJava\n+    Option(pfunc.call(time.milliseconds, rdds)).map(_.rdd)\n+  }\n+\n+  // for function.Function2\n+  def call(rdds: JList[JavaRDD[_]], time: Time): JavaRDD[Array[Byte]] = {\n+    pfunc.call(time.milliseconds, rdds)\n+  }\n+\n+  private def writeObject(out: ObjectOutputStream): Unit = {\n+    val bytes = PythonTransformFunctionSerializer.serialize(pfunc)\n+    out.writeInt(bytes.length)\n+    out.write(bytes)\n+  }\n+\n+  private def readObject(in: ObjectInputStream): Unit = {\n+    val length = in.readInt()\n+    val bytes = new Array[Byte](length)\n+    in.readFully(bytes)\n+    pfunc = PythonTransformFunctionSerializer.deserialize(bytes)\n+  }\n+}\n+\n+/**\n+ * Helpers for PythonTransformFunctionSerializer",
    "line": 91
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Maybe add a note here saying that these functions are called from Python via Py4J.\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-10-07T22:55:56Z",
    "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.api.python\n+\n+import java.io.{ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.Proxy\n+import java.util.{ArrayList => JArrayList, List => JList}\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+import scala.language.existentials\n+\n+import py4j.GatewayServer\n+\n+import org.apache.spark.api.java._\n+import org.apache.spark.api.python._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{Interval, Duration, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.api.java._\n+\n+\n+/**\n+ * Interface for Python callback function which is used to transform RDDs\n+ */\n+private[python] trait PythonTransformFunction {\n+  def call(time: Long, rdds: JList[_]): JavaRDD[Array[Byte]]\n+}\n+\n+/**\n+ * Interface for Python Serializer to serialize PythonTransformFunction\n+ */\n+private[python] trait PythonTransformFunctionSerializer {\n+  def dumps(id: String): Array[Byte]\n+  def loads(bytes: Array[Byte]): PythonTransformFunction\n+}\n+\n+/**\n+ * Wrapper for PythonTransformFunction\n+ */\n+private[python] class TransformFunction(@transient var pfunc: PythonTransformFunction)\n+  extends function.Function2[JList[JavaRDD[_]], Time, JavaRDD[Array[Byte]]] with Serializable {\n+\n+  def apply(rdd: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    Option(pfunc.call(time.milliseconds, List(rdd.map(JavaRDD.fromRDD(_)).orNull).asJava))\n+      .map(_.rdd)\n+  }\n+\n+  def apply(rdd: Option[RDD[_]], rdd2: Option[RDD[_]], time: Time): Option[RDD[Array[Byte]]] = {\n+    val rdds = List(rdd.map(JavaRDD.fromRDD(_)).orNull, rdd2.map(JavaRDD.fromRDD(_)).orNull).asJava\n+    Option(pfunc.call(time.milliseconds, rdds)).map(_.rdd)\n+  }\n+\n+  // for function.Function2\n+  def call(rdds: JList[JavaRDD[_]], time: Time): JavaRDD[Array[Byte]] = {\n+    pfunc.call(time.milliseconds, rdds)\n+  }\n+\n+  private def writeObject(out: ObjectOutputStream): Unit = {\n+    val bytes = PythonTransformFunctionSerializer.serialize(pfunc)\n+    out.writeInt(bytes.length)\n+    out.write(bytes)\n+  }\n+\n+  private def readObject(in: ObjectInputStream): Unit = {\n+    val length = in.readInt()\n+    val bytes = new Array[Byte](length)\n+    in.readFully(bytes)\n+    pfunc = PythonTransformFunctionSerializer.deserialize(bytes)\n+  }\n+}\n+\n+/**\n+ * Helpers for PythonTransformFunctionSerializer\n+ */\n+private[python] object PythonTransformFunctionSerializer {\n+\n+  /**\n+   * A serializer in Python, used to serialize PythonTransformFunction\n+    */\n+  private var serializer: PythonTransformFunctionSerializer = _\n+\n+  /*\n+   * Register a serializer from Python, should be called during initialization\n+   */\n+  def register(ser: PythonTransformFunctionSerializer): Unit = {\n+    serializer = ser\n+  }\n+\n+  def serialize(func: PythonTransformFunction): Array[Byte] = {\n+    assert(serializer != null, \"Serializer has not been registered!\")\n+    // get the id of PythonTransformFunction in py4j\n+    val h = Proxy.getInvocationHandler(func.asInstanceOf[Proxy])\n+    val f = h.getClass().getDeclaredField(\"id\")\n+    f.setAccessible(true)\n+    val id = f.get(h).asInstanceOf[String]\n+    serializer.dumps(id)\n+  }\n+\n+  def deserialize(bytes: Array[Byte]): PythonTransformFunction = {\n+    assert(serializer != null, \"Serializer has not been registered!\")\n+    serializer.loads(bytes)\n+  }\n+}\n+\n+/**\n+ * Helper functions"
  }],
  "prId": 2538
}]