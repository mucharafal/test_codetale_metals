[{
  "comments": [{
    "author": {
      "login": "nchammas"
    },
    "body": "Typo: By the sbt build.\n",
    "commit": "83d936870ad0651fc2622593e53d3e31d7eb8d4b",
    "createdAt": "2016-03-28T18:46:07Z",
    "diffHunk": "@@ -0,0 +1,141 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one or more\n+  ~ contributor license agreements.  See the NOTICE file distributed with\n+  ~ this work for additional information regarding copyright ownership.\n+  ~ The ASF licenses this file to You under the Apache License, Version 2.0\n+  ~ (the \"License\"); you may not use this file except in compliance with\n+  ~ the License.  You may obtain a copy of the License at\n+  ~\n+  ~    http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing, software\n+  ~ distributed under the License is distributed on an \"AS IS\" BASIS,\n+  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  ~ See the License for the specific language governing permissions and\n+  ~ limitations under the License.\n+  -->\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+  <modelVersion>4.0.0</modelVersion>\n+  <parent>\n+    <groupId>org.apache.spark</groupId>\n+    <artifactId>spark-parent_2.11</artifactId>\n+    <version>2.0.0-SNAPSHOT</version>\n+    <relativePath>../pom.xml</relativePath>\n+  </parent>\n+\n+  <artifactId>spark-cloud_2.11</artifactId>\n+  <packaging>jar</packaging>\n+  <name>Spark Project cloud integration</name>\n+  <description>Contains support for cloud infrastructures, including the Hadoop JARs and\n+    transitive dependencies needed to interact with the infrastructures.\n+    When included in the spark-assembly JAR, the hadoop artifacts are included, but not\n+    any of the 3rd party libraries needed, such as those from Amazon (for AWS) and Microsoft (Azure).\n+    These will need to be explicitly added to the classpath of any application interacting\n+    with the services.\n+    \n+    Any project which explicitly depends upon the spark-cloud artifact will get the dependencies;\n+    the exact versions of which will depend upon the hadoop version Spark was compiled against.\n+\n+    Hadoop 2.5 and earlier: jets3t.\n+    Hadoop 2.6: hadoop-aws and aws-java-sdk JARs\n+    Hadoop 2.7+: hadoop-aws, aws-java-sdk-s3, hadoop-azure and azure-storage JARs\n+\n+    WARNING: the signatures of methods in aws-java-sdk/aws-java-sdk-s3 can change between versions:\n+    use the same version against which Hadoop was compiled.\n+  </description>\n+  <properties>\n+    <sbt.project.name>cloud</sbt.project.name>\n+  </properties>\n+\n+  <dependencies>\n+    <dependency>\n+      <groupId>org.apache.spark</groupId>\n+      <artifactId>spark-core_${scala.binary.version}</artifactId>\n+      <version>${project.version}</version>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.spark</groupId>\n+      <artifactId>spark-core_${scala.binary.version}</artifactId>\n+      <version>${project.version}</version>\n+      <type>test-jar</type>\n+      <scope>test</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.spark</groupId>\n+      <artifactId>spark-test-tags_${scala.binary.version}</artifactId>\n+    </dependency>\n+    <dependency>\n+      <groupId>net.java.dev.jets3t</groupId>\n+      <artifactId>jets3t</artifactId>\n+    </dependency>\n+\n+    <!-- Explicit listing of transitive deps that are shaded. Otherwise, odd compiler crashes. -->\n+    <dependency>\n+      <groupId>com.google.guava</groupId>\n+      <artifactId>guava</artifactId>\n+    </dependency>\n+    <!-- End of shaded deps. -->\n+  </dependencies>\n+  <build>\n+    <outputDirectory>target/scala-${scala.binary.version}/classes</outputDirectory>\n+    <testOutputDirectory>target/scala-${scala.binary.version}/test-classes</testOutputDirectory>\n+  </build>\n+\n+  <profiles>\n+    \n+    <!--\n+      This profile is enabled automatically by the sbt built. It changes the scope for the guava"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "funny: that was a comment I lifted with the dependency cargo cult style; it'll need fixing in in the original too..\n",
    "commit": "83d936870ad0651fc2622593e53d3e31d7eb8d4b",
    "createdAt": "2016-03-28T18:54:08Z",
    "diffHunk": "@@ -0,0 +1,141 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one or more\n+  ~ contributor license agreements.  See the NOTICE file distributed with\n+  ~ this work for additional information regarding copyright ownership.\n+  ~ The ASF licenses this file to You under the Apache License, Version 2.0\n+  ~ (the \"License\"); you may not use this file except in compliance with\n+  ~ the License.  You may obtain a copy of the License at\n+  ~\n+  ~    http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing, software\n+  ~ distributed under the License is distributed on an \"AS IS\" BASIS,\n+  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  ~ See the License for the specific language governing permissions and\n+  ~ limitations under the License.\n+  -->\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+  <modelVersion>4.0.0</modelVersion>\n+  <parent>\n+    <groupId>org.apache.spark</groupId>\n+    <artifactId>spark-parent_2.11</artifactId>\n+    <version>2.0.0-SNAPSHOT</version>\n+    <relativePath>../pom.xml</relativePath>\n+  </parent>\n+\n+  <artifactId>spark-cloud_2.11</artifactId>\n+  <packaging>jar</packaging>\n+  <name>Spark Project cloud integration</name>\n+  <description>Contains support for cloud infrastructures, including the Hadoop JARs and\n+    transitive dependencies needed to interact with the infrastructures.\n+    When included in the spark-assembly JAR, the hadoop artifacts are included, but not\n+    any of the 3rd party libraries needed, such as those from Amazon (for AWS) and Microsoft (Azure).\n+    These will need to be explicitly added to the classpath of any application interacting\n+    with the services.\n+    \n+    Any project which explicitly depends upon the spark-cloud artifact will get the dependencies;\n+    the exact versions of which will depend upon the hadoop version Spark was compiled against.\n+\n+    Hadoop 2.5 and earlier: jets3t.\n+    Hadoop 2.6: hadoop-aws and aws-java-sdk JARs\n+    Hadoop 2.7+: hadoop-aws, aws-java-sdk-s3, hadoop-azure and azure-storage JARs\n+\n+    WARNING: the signatures of methods in aws-java-sdk/aws-java-sdk-s3 can change between versions:\n+    use the same version against which Hadoop was compiled.\n+  </description>\n+  <properties>\n+    <sbt.project.name>cloud</sbt.project.name>\n+  </properties>\n+\n+  <dependencies>\n+    <dependency>\n+      <groupId>org.apache.spark</groupId>\n+      <artifactId>spark-core_${scala.binary.version}</artifactId>\n+      <version>${project.version}</version>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.spark</groupId>\n+      <artifactId>spark-core_${scala.binary.version}</artifactId>\n+      <version>${project.version}</version>\n+      <type>test-jar</type>\n+      <scope>test</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.spark</groupId>\n+      <artifactId>spark-test-tags_${scala.binary.version}</artifactId>\n+    </dependency>\n+    <dependency>\n+      <groupId>net.java.dev.jets3t</groupId>\n+      <artifactId>jets3t</artifactId>\n+    </dependency>\n+\n+    <!-- Explicit listing of transitive deps that are shaded. Otherwise, odd compiler crashes. -->\n+    <dependency>\n+      <groupId>com.google.guava</groupId>\n+      <artifactId>guava</artifactId>\n+    </dependency>\n+    <!-- End of shaded deps. -->\n+  </dependencies>\n+  <build>\n+    <outputDirectory>target/scala-${scala.binary.version}/classes</outputDirectory>\n+    <testOutputDirectory>target/scala-${scala.binary.version}/test-classes</testOutputDirectory>\n+  </build>\n+\n+  <profiles>\n+    \n+    <!--\n+      This profile is enabled automatically by the sbt built. It changes the scope for the guava"
  }],
  "prId": 12004
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "OK, so the idea here is that these dependencies used to be baked in to other artifacts in Hadoop before 2.7? or weren't available at all before?\r\n\r\nI am guessing there is no license issue here, coming from Hadoop.\r\n",
    "commit": "83d936870ad0651fc2622593e53d3e31d7eb8d4b",
    "createdAt": "2016-11-22T14:59:33Z",
    "diffHunk": "@@ -0,0 +1,158 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one or more\n+  ~ contributor license agreements.  See the NOTICE file distributed with\n+  ~ this work for additional information regarding copyright ownership.\n+  ~ The ASF licenses this file to You under the Apache License, Version 2.0\n+  ~ (the \"License\"); you may not use this file except in compliance with\n+  ~ the License.  You may obtain a copy of the License at\n+  ~\n+  ~    http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing, software\n+  ~ distributed under the License is distributed on an \"AS IS\" BASIS,\n+  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  ~ See the License for the specific language governing permissions and\n+  ~ limitations under the License.\n+  -->\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+  <modelVersion>4.0.0</modelVersion>\n+  <parent>\n+    <groupId>org.apache.spark</groupId>\n+    <artifactId>spark-parent_2.11</artifactId>\n+    <version>2.1.0-SNAPSHOT</version>\n+    <relativePath>../pom.xml</relativePath>\n+  </parent>\n+\n+  <artifactId>spark-cloud_2.11</artifactId>\n+  <packaging>jar</packaging>\n+  <name>Spark Project Cloud Integration</name>\n+  <description>\n+    Contains support for cloud infrastructures, specifically the Hadoop JARs and\n+    transitive dependencies needed to interact with the infrastructures.\n+\n+    Any project which explicitly depends upon the spark-cloud artifact will get the dependencies;\n+    the exact versions of which will depend upon the hadoop version Spark was compiled against.\n+\n+    Hadoop 2.7:\n+      hadoop-aws\n+      aws-java-sdk-s3\n+      hadoop-azure\n+      azure-storage\n+      hadoop-openstack\n+\n+    WARNING: the signatures of methods in aws-java-sdk/aws-java-sdk-s3 can change between versions:\n+    use the same version against which Hadoop was compiled.\n+\n+  </description>\n+  <properties>\n+    <sbt.project.name>cloud</sbt.project.name>\n+  </properties>\n+\n+  <dependencies>\n+    <dependency>\n+      <groupId>org.apache.spark</groupId>\n+      <artifactId>spark-core_${scala.binary.version}</artifactId>\n+      <version>${project.version}</version>\n+    </dependency>\n+\n+    <!--Used for test classes -->\n+    <dependency>\n+      <groupId>org.apache.spark</groupId>\n+      <artifactId>spark-core_${scala.binary.version}</artifactId>\n+      <version>${project.version}</version>\n+      <type>test-jar</type>\n+      <scope>test</scope>\n+    </dependency>\n+\n+\n+    <!-- Jets3t is needed for s3n and s3 classic to work-->\n+    <dependency>\n+      <groupId>net.java.dev.jets3t</groupId>\n+      <artifactId>jets3t</artifactId>\n+    </dependency>\n+\n+    <!-- Explicit listing of transitive deps that are shaded. Otherwise, odd compiler crashes. -->\n+    <dependency>\n+      <groupId>com.google.guava</groupId>\n+      <artifactId>guava</artifactId>\n+    </dependency>\n+    <!-- End of shaded deps. -->\n+  </dependencies>\n+\n+  <build>\n+    <outputDirectory>target/scala-${scala.binary.version}/classes</outputDirectory>\n+    <testOutputDirectory>target/scala-${scala.binary.version}/test-classes</testOutputDirectory>\n+  </build>\n+\n+  <profiles>\n+\n+    <!--\n+      This profile is enabled automatically by the sbt build. It changes the scope for the guava\n+      dependency, since we don't shade it in the artifacts generated by the sbt build.\n+    -->\n+    <profile>\n+      <id>sbt</id>\n+      <dependencies>\n+        <dependency>\n+          <groupId>com.google.guava</groupId>\n+          <artifactId>guava</artifactId>\n+          <scope>compile</scope>\n+        </dependency>\n+      </dependencies>\n+    </profile>\n+\n+    <profile>\n+      <id>hadoop-2.7</id>\n+        <dependencies>\n+          <dependency>\n+            <groupId>org.apache.hadoop</groupId>\n+            <artifactId>hadoop-aws</artifactId>"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "1. s3n was in hadoop-common for 2.5 and earlier; moved into the new hadoop-aws JAR alongside s3 and s3a in 2.6. wasb:// only came in hadoop 2.7. If people really wanted 2.6.x + cloud, it'd get more complex as the cloud module would need 2.6 and 2.7 profiling internally. \r\n\r\n1. Regarding Licensing. There's a small licensing problem right now: [HADOOP-13794](https://issues.apache.org/jira/browse/HADOOP-13794). We can't release any new ASF artifacts with the versions of the amazon-aws SDK currently in Hadoop as it has a version of the org.json artifacts, which are now license-non-grata. For Hadoop 2.9 we're moving to newer AWS SDKs (more specifically, bumping up Jackson so the SDKs work), but don't want to force in a jackson update to 2.7.x. Either I build/release a patched AWS SDK JAR *or* a version of the latest AWS with its dependencies shaded. The jackson issue matters less in Spark as it has a version of jackson which works, but as every AWS SDK release tends to be API incompatible, switching is non trivial. This is why it'd be good to keep a `cloud` profile separate; avoid worrying about this while I sort out what to do about Hadoop 2.7.4",
    "commit": "83d936870ad0651fc2622593e53d3e31d7eb8d4b",
    "createdAt": "2016-11-22T20:33:38Z",
    "diffHunk": "@@ -0,0 +1,158 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one or more\n+  ~ contributor license agreements.  See the NOTICE file distributed with\n+  ~ this work for additional information regarding copyright ownership.\n+  ~ The ASF licenses this file to You under the Apache License, Version 2.0\n+  ~ (the \"License\"); you may not use this file except in compliance with\n+  ~ the License.  You may obtain a copy of the License at\n+  ~\n+  ~    http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing, software\n+  ~ distributed under the License is distributed on an \"AS IS\" BASIS,\n+  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  ~ See the License for the specific language governing permissions and\n+  ~ limitations under the License.\n+  -->\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+  <modelVersion>4.0.0</modelVersion>\n+  <parent>\n+    <groupId>org.apache.spark</groupId>\n+    <artifactId>spark-parent_2.11</artifactId>\n+    <version>2.1.0-SNAPSHOT</version>\n+    <relativePath>../pom.xml</relativePath>\n+  </parent>\n+\n+  <artifactId>spark-cloud_2.11</artifactId>\n+  <packaging>jar</packaging>\n+  <name>Spark Project Cloud Integration</name>\n+  <description>\n+    Contains support for cloud infrastructures, specifically the Hadoop JARs and\n+    transitive dependencies needed to interact with the infrastructures.\n+\n+    Any project which explicitly depends upon the spark-cloud artifact will get the dependencies;\n+    the exact versions of which will depend upon the hadoop version Spark was compiled against.\n+\n+    Hadoop 2.7:\n+      hadoop-aws\n+      aws-java-sdk-s3\n+      hadoop-azure\n+      azure-storage\n+      hadoop-openstack\n+\n+    WARNING: the signatures of methods in aws-java-sdk/aws-java-sdk-s3 can change between versions:\n+    use the same version against which Hadoop was compiled.\n+\n+  </description>\n+  <properties>\n+    <sbt.project.name>cloud</sbt.project.name>\n+  </properties>\n+\n+  <dependencies>\n+    <dependency>\n+      <groupId>org.apache.spark</groupId>\n+      <artifactId>spark-core_${scala.binary.version}</artifactId>\n+      <version>${project.version}</version>\n+    </dependency>\n+\n+    <!--Used for test classes -->\n+    <dependency>\n+      <groupId>org.apache.spark</groupId>\n+      <artifactId>spark-core_${scala.binary.version}</artifactId>\n+      <version>${project.version}</version>\n+      <type>test-jar</type>\n+      <scope>test</scope>\n+    </dependency>\n+\n+\n+    <!-- Jets3t is needed for s3n and s3 classic to work-->\n+    <dependency>\n+      <groupId>net.java.dev.jets3t</groupId>\n+      <artifactId>jets3t</artifactId>\n+    </dependency>\n+\n+    <!-- Explicit listing of transitive deps that are shaded. Otherwise, odd compiler crashes. -->\n+    <dependency>\n+      <groupId>com.google.guava</groupId>\n+      <artifactId>guava</artifactId>\n+    </dependency>\n+    <!-- End of shaded deps. -->\n+  </dependencies>\n+\n+  <build>\n+    <outputDirectory>target/scala-${scala.binary.version}/classes</outputDirectory>\n+    <testOutputDirectory>target/scala-${scala.binary.version}/test-classes</testOutputDirectory>\n+  </build>\n+\n+  <profiles>\n+\n+    <!--\n+      This profile is enabled automatically by the sbt build. It changes the scope for the guava\n+      dependency, since we don't shade it in the artifacts generated by the sbt build.\n+    -->\n+    <profile>\n+      <id>sbt</id>\n+      <dependencies>\n+        <dependency>\n+          <groupId>com.google.guava</groupId>\n+          <artifactId>guava</artifactId>\n+          <scope>compile</scope>\n+        </dependency>\n+      </dependencies>\n+    </profile>\n+\n+    <profile>\n+      <id>hadoop-2.7</id>\n+        <dependencies>\n+          <dependency>\n+            <groupId>org.apache.hadoop</groupId>\n+            <artifactId>hadoop-aws</artifactId>"
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "Does any of this change now that we only support Hadoop 2.6+? I assume that's good news if anything. Only a `hadoop-2.7` profile is defined here so what would this do for 2.6? You mention that the salient packaging change occurred in 2.6.",
    "commit": "83d936870ad0651fc2622593e53d3e31d7eb8d4b",
    "createdAt": "2017-03-04T12:32:00Z",
    "diffHunk": "@@ -0,0 +1,158 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one or more\n+  ~ contributor license agreements.  See the NOTICE file distributed with\n+  ~ this work for additional information regarding copyright ownership.\n+  ~ The ASF licenses this file to You under the Apache License, Version 2.0\n+  ~ (the \"License\"); you may not use this file except in compliance with\n+  ~ the License.  You may obtain a copy of the License at\n+  ~\n+  ~    http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing, software\n+  ~ distributed under the License is distributed on an \"AS IS\" BASIS,\n+  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  ~ See the License for the specific language governing permissions and\n+  ~ limitations under the License.\n+  -->\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+  <modelVersion>4.0.0</modelVersion>\n+  <parent>\n+    <groupId>org.apache.spark</groupId>\n+    <artifactId>spark-parent_2.11</artifactId>\n+    <version>2.1.0-SNAPSHOT</version>\n+    <relativePath>../pom.xml</relativePath>\n+  </parent>\n+\n+  <artifactId>spark-cloud_2.11</artifactId>\n+  <packaging>jar</packaging>\n+  <name>Spark Project Cloud Integration</name>\n+  <description>\n+    Contains support for cloud infrastructures, specifically the Hadoop JARs and\n+    transitive dependencies needed to interact with the infrastructures.\n+\n+    Any project which explicitly depends upon the spark-cloud artifact will get the dependencies;\n+    the exact versions of which will depend upon the hadoop version Spark was compiled against.\n+\n+    Hadoop 2.7:\n+      hadoop-aws\n+      aws-java-sdk-s3\n+      hadoop-azure\n+      azure-storage\n+      hadoop-openstack\n+\n+    WARNING: the signatures of methods in aws-java-sdk/aws-java-sdk-s3 can change between versions:\n+    use the same version against which Hadoop was compiled.\n+\n+  </description>\n+  <properties>\n+    <sbt.project.name>cloud</sbt.project.name>\n+  </properties>\n+\n+  <dependencies>\n+    <dependency>\n+      <groupId>org.apache.spark</groupId>\n+      <artifactId>spark-core_${scala.binary.version}</artifactId>\n+      <version>${project.version}</version>\n+    </dependency>\n+\n+    <!--Used for test classes -->\n+    <dependency>\n+      <groupId>org.apache.spark</groupId>\n+      <artifactId>spark-core_${scala.binary.version}</artifactId>\n+      <version>${project.version}</version>\n+      <type>test-jar</type>\n+      <scope>test</scope>\n+    </dependency>\n+\n+\n+    <!-- Jets3t is needed for s3n and s3 classic to work-->\n+    <dependency>\n+      <groupId>net.java.dev.jets3t</groupId>\n+      <artifactId>jets3t</artifactId>\n+    </dependency>\n+\n+    <!-- Explicit listing of transitive deps that are shaded. Otherwise, odd compiler crashes. -->\n+    <dependency>\n+      <groupId>com.google.guava</groupId>\n+      <artifactId>guava</artifactId>\n+    </dependency>\n+    <!-- End of shaded deps. -->\n+  </dependencies>\n+\n+  <build>\n+    <outputDirectory>target/scala-${scala.binary.version}/classes</outputDirectory>\n+    <testOutputDirectory>target/scala-${scala.binary.version}/test-classes</testOutputDirectory>\n+  </build>\n+\n+  <profiles>\n+\n+    <!--\n+      This profile is enabled automatically by the sbt build. It changes the scope for the guava\n+      dependency, since we don't shade it in the artifacts generated by the sbt build.\n+    -->\n+    <profile>\n+      <id>sbt</id>\n+      <dependencies>\n+        <dependency>\n+          <groupId>com.google.guava</groupId>\n+          <artifactId>guava</artifactId>\n+          <scope>compile</scope>\n+        </dependency>\n+      </dependencies>\n+    </profile>\n+\n+    <profile>\n+      <id>hadoop-2.7</id>\n+        <dependencies>\n+          <dependency>\n+            <groupId>org.apache.hadoop</groupId>\n+            <artifactId>hadoop-aws</artifactId>"
  }],
  "prId": 12004
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Where does an end user need to act on this -- the profile is in theory setting all this up correctly right? ",
    "commit": "83d936870ad0651fc2622593e53d3e31d7eb8d4b",
    "createdAt": "2017-03-04T12:30:12Z",
    "diffHunk": "@@ -0,0 +1,158 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one or more\n+  ~ contributor license agreements.  See the NOTICE file distributed with\n+  ~ this work for additional information regarding copyright ownership.\n+  ~ The ASF licenses this file to You under the Apache License, Version 2.0\n+  ~ (the \"License\"); you may not use this file except in compliance with\n+  ~ the License.  You may obtain a copy of the License at\n+  ~\n+  ~    http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing, software\n+  ~ distributed under the License is distributed on an \"AS IS\" BASIS,\n+  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  ~ See the License for the specific language governing permissions and\n+  ~ limitations under the License.\n+  -->\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+  <modelVersion>4.0.0</modelVersion>\n+  <parent>\n+    <groupId>org.apache.spark</groupId>\n+    <artifactId>spark-parent_2.11</artifactId>\n+    <version>2.2.0-SNAPSHOT</version>\n+    <relativePath>../pom.xml</relativePath>\n+  </parent>\n+\n+  <artifactId>spark-hadoop-cloud_2.11</artifactId>\n+  <packaging>jar</packaging>\n+  <name>Spark Project Cloud Integration</name>\n+  <description>\n+    Contains support for cloud infrastructures, specifically the Hadoop JARs and\n+    transitive dependencies needed to interact with the infrastructures.\n+\n+    Any project which explicitly depends upon the spark-hadoop-cloud artifact will get the\n+    dependencies; the exact versions of which will depend upon the hadoop version Spark was compiled\n+    against.\n+\n+    The imports of transitive dependencies are managed to make them consistent\n+    with those of the Spark build.\n+\n+    WARNING: the signatures of methods in the AWS and Azure SDKs do change between",
    "line": 43
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "I would only include the first sentence here. The description here should be short since nobody will likely read it. Anything substantive could go in docs.",
    "commit": "83d936870ad0651fc2622593e53d3e31d7eb8d4b",
    "createdAt": "2017-04-27T13:26:51Z",
    "diffHunk": "@@ -0,0 +1,158 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one or more\n+  ~ contributor license agreements.  See the NOTICE file distributed with\n+  ~ this work for additional information regarding copyright ownership.\n+  ~ The ASF licenses this file to You under the Apache License, Version 2.0\n+  ~ (the \"License\"); you may not use this file except in compliance with\n+  ~ the License.  You may obtain a copy of the License at\n+  ~\n+  ~    http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing, software\n+  ~ distributed under the License is distributed on an \"AS IS\" BASIS,\n+  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  ~ See the License for the specific language governing permissions and\n+  ~ limitations under the License.\n+  -->\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+  <modelVersion>4.0.0</modelVersion>\n+  <parent>\n+    <groupId>org.apache.spark</groupId>\n+    <artifactId>spark-parent_2.11</artifactId>\n+    <version>2.2.0-SNAPSHOT</version>\n+    <relativePath>../pom.xml</relativePath>\n+  </parent>\n+\n+  <artifactId>spark-hadoop-cloud_2.11</artifactId>\n+  <packaging>jar</packaging>\n+  <name>Spark Project Cloud Integration</name>\n+  <description>\n+    Contains support for cloud infrastructures, specifically the Hadoop JARs and\n+    transitive dependencies needed to interact with the infrastructures.\n+\n+    Any project which explicitly depends upon the spark-hadoop-cloud artifact will get the\n+    dependencies; the exact versions of which will depend upon the hadoop version Spark was compiled\n+    against.\n+\n+    The imports of transitive dependencies are managed to make them consistent\n+    with those of the Spark build.\n+\n+    WARNING: the signatures of methods in the AWS and Azure SDKs do change between",
    "line": 43
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "Cutting back to the first line, it can be covered in docs. \r\n\r\nOne option with the docs is to trim them back and say \"consult the [Hadoop documentation](http://hadoop.apache.org/docs/r2.8.0/hadoop-aws/tools/hadoop-aws/index.html#S3A) for object store setup, and I can be more explicit there on version pain. ",
    "commit": "83d936870ad0651fc2622593e53d3e31d7eb8d4b",
    "createdAt": "2017-04-27T16:58:06Z",
    "diffHunk": "@@ -0,0 +1,158 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one or more\n+  ~ contributor license agreements.  See the NOTICE file distributed with\n+  ~ this work for additional information regarding copyright ownership.\n+  ~ The ASF licenses this file to You under the Apache License, Version 2.0\n+  ~ (the \"License\"); you may not use this file except in compliance with\n+  ~ the License.  You may obtain a copy of the License at\n+  ~\n+  ~    http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing, software\n+  ~ distributed under the License is distributed on an \"AS IS\" BASIS,\n+  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  ~ See the License for the specific language governing permissions and\n+  ~ limitations under the License.\n+  -->\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+  <modelVersion>4.0.0</modelVersion>\n+  <parent>\n+    <groupId>org.apache.spark</groupId>\n+    <artifactId>spark-parent_2.11</artifactId>\n+    <version>2.2.0-SNAPSHOT</version>\n+    <relativePath>../pom.xml</relativePath>\n+  </parent>\n+\n+  <artifactId>spark-hadoop-cloud_2.11</artifactId>\n+  <packaging>jar</packaging>\n+  <name>Spark Project Cloud Integration</name>\n+  <description>\n+    Contains support for cloud infrastructures, specifically the Hadoop JARs and\n+    transitive dependencies needed to interact with the infrastructures.\n+\n+    Any project which explicitly depends upon the spark-hadoop-cloud artifact will get the\n+    dependencies; the exact versions of which will depend upon the hadoop version Spark was compiled\n+    against.\n+\n+    The imports of transitive dependencies are managed to make them consistent\n+    with those of the Spark build.\n+\n+    WARNING: the signatures of methods in the AWS and Azure SDKs do change between",
    "line": 43
  }],
  "prId": 12004
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Why this dependency if there's no source code? in fact shouldn't most of the dependencies be `runtime`? and possibly respect `hadoop.deps.scope`?",
    "commit": "83d936870ad0651fc2622593e53d3e31d7eb8d4b",
    "createdAt": "2017-03-04T12:33:14Z",
    "diffHunk": "@@ -0,0 +1,158 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one or more\n+  ~ contributor license agreements.  See the NOTICE file distributed with\n+  ~ this work for additional information regarding copyright ownership.\n+  ~ The ASF licenses this file to You under the Apache License, Version 2.0\n+  ~ (the \"License\"); you may not use this file except in compliance with\n+  ~ the License.  You may obtain a copy of the License at\n+  ~\n+  ~    http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing, software\n+  ~ distributed under the License is distributed on an \"AS IS\" BASIS,\n+  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  ~ See the License for the specific language governing permissions and\n+  ~ limitations under the License.\n+  -->\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+  <modelVersion>4.0.0</modelVersion>\n+  <parent>\n+    <groupId>org.apache.spark</groupId>\n+    <artifactId>spark-parent_2.11</artifactId>\n+    <version>2.2.0-SNAPSHOT</version>\n+    <relativePath>../pom.xml</relativePath>\n+  </parent>\n+\n+  <artifactId>spark-hadoop-cloud_2.11</artifactId>\n+  <packaging>jar</packaging>\n+  <name>Spark Project Cloud Integration</name>\n+  <description>\n+    Contains support for cloud infrastructures, specifically the Hadoop JARs and\n+    transitive dependencies needed to interact with the infrastructures.\n+\n+    Any project which explicitly depends upon the spark-hadoop-cloud artifact will get the\n+    dependencies; the exact versions of which will depend upon the hadoop version Spark was compiled\n+    against.\n+\n+    The imports of transitive dependencies are managed to make them consistent\n+    with those of the Spark build.\n+\n+    WARNING: the signatures of methods in the AWS and Azure SDKs do change between\n+    versions: use exactly the same version with which the Hadoop JARs were\n+    built.\n+  </description>\n+  <properties>\n+    <sbt.project.name>hadoop-cloud</sbt.project.name>\n+  </properties>\n+\n+  <dependencies>\n+    <dependency>\n+      <groupId>org.apache.spark</groupId>\n+      <artifactId>spark-core_${scala.binary.version}</artifactId>\n+      <version>${project.version}</version>"
  }],
  "prId": 12004
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Is this patching over a problem in the Hadoop or Hive POMs?",
    "commit": "83d936870ad0651fc2622593e53d3e31d7eb8d4b",
    "createdAt": "2017-03-04T12:33:57Z",
    "diffHunk": "@@ -0,0 +1,158 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one or more\n+  ~ contributor license agreements.  See the NOTICE file distributed with\n+  ~ this work for additional information regarding copyright ownership.\n+  ~ The ASF licenses this file to You under the Apache License, Version 2.0\n+  ~ (the \"License\"); you may not use this file except in compliance with\n+  ~ the License.  You may obtain a copy of the License at\n+  ~\n+  ~    http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing, software\n+  ~ distributed under the License is distributed on an \"AS IS\" BASIS,\n+  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  ~ See the License for the specific language governing permissions and\n+  ~ limitations under the License.\n+  -->\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+  <modelVersion>4.0.0</modelVersion>\n+  <parent>\n+    <groupId>org.apache.spark</groupId>\n+    <artifactId>spark-parent_2.11</artifactId>\n+    <version>2.2.0-SNAPSHOT</version>\n+    <relativePath>../pom.xml</relativePath>\n+  </parent>\n+\n+  <artifactId>spark-hadoop-cloud_2.11</artifactId>\n+  <packaging>jar</packaging>\n+  <name>Spark Project Cloud Integration</name>\n+  <description>\n+    Contains support for cloud infrastructures, specifically the Hadoop JARs and\n+    transitive dependencies needed to interact with the infrastructures.\n+\n+    Any project which explicitly depends upon the spark-hadoop-cloud artifact will get the\n+    dependencies; the exact versions of which will depend upon the hadoop version Spark was compiled\n+    against.\n+\n+    The imports of transitive dependencies are managed to make them consistent\n+    with those of the Spark build.\n+\n+    WARNING: the signatures of methods in the AWS and Azure SDKs do change between\n+    versions: use exactly the same version with which the Hadoop JARs were\n+    built.\n+  </description>\n+  <properties>\n+    <sbt.project.name>hadoop-cloud</sbt.project.name>\n+  </properties>\n+\n+  <dependencies>\n+    <dependency>\n+      <groupId>org.apache.spark</groupId>\n+      <artifactId>spark-core_${scala.binary.version}</artifactId>\n+      <version>${project.version}</version>\n+    </dependency>\n+\n+    <!--Used for test classes -->\n+    <dependency>\n+      <groupId>org.apache.spark</groupId>\n+      <artifactId>spark-core_${scala.binary.version}</artifactId>\n+      <version>${project.version}</version>\n+      <type>test-jar</type>\n+      <scope>test</scope>\n+    </dependency>\n+\n+    <!-- Jets3t is needed for s3n and s3 classic to work-->\n+    <dependency>\n+      <groupId>net.java.dev.jets3t</groupId>\n+      <artifactId>jets3t</artifactId>\n+    </dependency>\n+\n+    <!-- Explicit listing of transitive deps that are shaded. Otherwise, odd compiler crashes. -->\n+    <dependency>\n+      <groupId>com.google.guava</groupId>\n+      <artifactId>guava</artifactId>\n+    </dependency>\n+    <!-- End of shaded deps. -->\n+  </dependencies>\n+\n+  <build>\n+    <outputDirectory>target/scala-${scala.binary.version}/classes</outputDirectory>\n+    <testOutputDirectory>target/scala-${scala.binary.version}/test-classes</testOutputDirectory>\n+  </build>\n+\n+  <profiles>\n+\n+    <!--\n+      This profile is enabled automatically by the sbt build. It changes the scope for the guava\n+      dependency, since we don't shade it in the artifacts generated by the sbt build.\n+    -->\n+    <profile>\n+      <id>sbt</id>\n+      <dependencies>\n+        <dependency>\n+          <groupId>com.google.guava</groupId>\n+          <artifactId>guava</artifactId>\n+          <scope>compile</scope>\n+        </dependency>\n+      </dependencies>\n+    </profile>\n+\n+    <profile>\n+      <id>hadoop-2.7</id>\n+        <dependencies>\n+          <dependency>\n+            <groupId>org.apache.hadoop</groupId>\n+            <artifactId>hadoop-aws</artifactId>\n+            <scope>${hadoop.deps.scope}</scope>\n+          </dependency>\n+          <dependency>\n+            <groupId>org.apache.hadoop</groupId>\n+            <artifactId>hadoop-azure</artifactId>\n+            <scope>${hadoop.deps.scope}</scope>\n+          </dependency>\n+          <dependency>\n+            <groupId>org.apache.hadoop</groupId>\n+            <artifactId>hadoop-openstack</artifactId>\n+            <scope>${hadoop.deps.scope}</scope>\n+          </dependency>\n+          <!--\n+          Add joda time to ensure that anything downstream which doesn't pull in spark-hive\n+          gets the correct joda time artifact, so doesn't have auth failures on later Java 8 JVMs\n+          -->\n+          <dependency>\n+            <groupId>joda-time</groupId>"
  }],
  "prId": 12004
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "It shouldn't be necessary to do this if you just mean to manage the version, but that's done in the parent.",
    "commit": "83d936870ad0651fc2622593e53d3e31d7eb8d4b",
    "createdAt": "2017-03-04T12:34:28Z",
    "diffHunk": "@@ -0,0 +1,158 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one or more\n+  ~ contributor license agreements.  See the NOTICE file distributed with\n+  ~ this work for additional information regarding copyright ownership.\n+  ~ The ASF licenses this file to You under the Apache License, Version 2.0\n+  ~ (the \"License\"); you may not use this file except in compliance with\n+  ~ the License.  You may obtain a copy of the License at\n+  ~\n+  ~    http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing, software\n+  ~ distributed under the License is distributed on an \"AS IS\" BASIS,\n+  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  ~ See the License for the specific language governing permissions and\n+  ~ limitations under the License.\n+  -->\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+  <modelVersion>4.0.0</modelVersion>\n+  <parent>\n+    <groupId>org.apache.spark</groupId>\n+    <artifactId>spark-parent_2.11</artifactId>\n+    <version>2.2.0-SNAPSHOT</version>\n+    <relativePath>../pom.xml</relativePath>\n+  </parent>\n+\n+  <artifactId>spark-hadoop-cloud_2.11</artifactId>\n+  <packaging>jar</packaging>\n+  <name>Spark Project Cloud Integration</name>\n+  <description>\n+    Contains support for cloud infrastructures, specifically the Hadoop JARs and\n+    transitive dependencies needed to interact with the infrastructures.\n+\n+    Any project which explicitly depends upon the spark-hadoop-cloud artifact will get the\n+    dependencies; the exact versions of which will depend upon the hadoop version Spark was compiled\n+    against.\n+\n+    The imports of transitive dependencies are managed to make them consistent\n+    with those of the Spark build.\n+\n+    WARNING: the signatures of methods in the AWS and Azure SDKs do change between\n+    versions: use exactly the same version with which the Hadoop JARs were\n+    built.\n+  </description>\n+  <properties>\n+    <sbt.project.name>hadoop-cloud</sbt.project.name>\n+  </properties>\n+\n+  <dependencies>\n+    <dependency>\n+      <groupId>org.apache.spark</groupId>\n+      <artifactId>spark-core_${scala.binary.version}</artifactId>\n+      <version>${project.version}</version>\n+    </dependency>\n+\n+    <!--Used for test classes -->\n+    <dependency>\n+      <groupId>org.apache.spark</groupId>\n+      <artifactId>spark-core_${scala.binary.version}</artifactId>\n+      <version>${project.version}</version>\n+      <type>test-jar</type>\n+      <scope>test</scope>\n+    </dependency>\n+\n+    <!-- Jets3t is needed for s3n and s3 classic to work-->\n+    <dependency>\n+      <groupId>net.java.dev.jets3t</groupId>\n+      <artifactId>jets3t</artifactId>\n+    </dependency>\n+\n+    <!-- Explicit listing of transitive deps that are shaded. Otherwise, odd compiler crashes. -->\n+    <dependency>\n+      <groupId>com.google.guava</groupId>\n+      <artifactId>guava</artifactId>\n+    </dependency>\n+    <!-- End of shaded deps. -->\n+  </dependencies>\n+\n+  <build>\n+    <outputDirectory>target/scala-${scala.binary.version}/classes</outputDirectory>\n+    <testOutputDirectory>target/scala-${scala.binary.version}/test-classes</testOutputDirectory>\n+  </build>\n+\n+  <profiles>\n+\n+    <!--\n+      This profile is enabled automatically by the sbt build. It changes the scope for the guava\n+      dependency, since we don't shade it in the artifacts generated by the sbt build.\n+    -->\n+    <profile>\n+      <id>sbt</id>\n+      <dependencies>\n+        <dependency>\n+          <groupId>com.google.guava</groupId>\n+          <artifactId>guava</artifactId>\n+          <scope>compile</scope>\n+        </dependency>\n+      </dependencies>\n+    </profile>\n+\n+    <profile>\n+      <id>hadoop-2.7</id>\n+        <dependencies>\n+          <dependency>\n+            <groupId>org.apache.hadoop</groupId>\n+            <artifactId>hadoop-aws</artifactId>\n+            <scope>${hadoop.deps.scope}</scope>\n+          </dependency>\n+          <dependency>\n+            <groupId>org.apache.hadoop</groupId>\n+            <artifactId>hadoop-azure</artifactId>\n+            <scope>${hadoop.deps.scope}</scope>\n+          </dependency>\n+          <dependency>\n+            <groupId>org.apache.hadoop</groupId>\n+            <artifactId>hadoop-openstack</artifactId>\n+            <scope>${hadoop.deps.scope}</scope>\n+          </dependency>\n+          <!--\n+          Add joda time to ensure that anything downstream which doesn't pull in spark-hive\n+          gets the correct joda time artifact, so doesn't have auth failures on later Java 8 JVMs\n+          -->\n+          <dependency>\n+            <groupId>joda-time</groupId>\n+            <artifactId>joda-time</artifactId>\n+          </dependency>\n+          <!-- explicitly declare the jackson artifacts desired -->\n+          <dependency>\n+            <groupId>com.fasterxml.jackson.core</groupId>\n+            <artifactId>jackson-databind</artifactId>\n+            <scope>${hadoop.deps.scope}</scope>\n+          </dependency>\n+          <dependency>\n+            <groupId>com.fasterxml.jackson.core</groupId>\n+            <artifactId>jackson-annotations</artifactId>\n+            <scope>${hadoop.deps.scope}</scope>\n+          </dependency>\n+          <dependency>\n+            <groupId>com.fasterxml.jackson.dataformat</groupId>\n+            <artifactId>jackson-dataformat-cbor</artifactId>\n+            <scope>${hadoop.deps.scope}</scope>\n+          </dependency>\n+          <!--Explicit declaration to force in Spark version into transitive dependencies -->\n+          <dependency>"
  }],
  "prId": 12004
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "We may need to make this 2.3.0-SNAPSHOT now, because that's what's correct for master, then change it if it back-ports.",
    "commit": "83d936870ad0651fc2622593e53d3e31d7eb8d4b",
    "createdAt": "2017-04-27T13:26:12Z",
    "diffHunk": "@@ -0,0 +1,117 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one or more\n+  ~ contributor license agreements.  See the NOTICE file distributed with\n+  ~ this work for additional information regarding copyright ownership.\n+  ~ The ASF licenses this file to You under the Apache License, Version 2.0\n+  ~ (the \"License\"); you may not use this file except in compliance with\n+  ~ the License.  You may obtain a copy of the License at\n+  ~\n+  ~    http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing, software\n+  ~ distributed under the License is distributed on an \"AS IS\" BASIS,\n+  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  ~ See the License for the specific language governing permissions and\n+  ~ limitations under the License.\n+  -->\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n+  xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n+  xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+  <modelVersion>4.0.0</modelVersion>\n+  <parent>\n+    <groupId>org.apache.spark</groupId>\n+    <artifactId>spark-parent_2.11</artifactId>\n+    <version>2.2.0-SNAPSHOT</version>",
    "line": 25
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "I'd noticed that this morning....",
    "commit": "83d936870ad0651fc2622593e53d3e31d7eb8d4b",
    "createdAt": "2017-04-27T15:18:30Z",
    "diffHunk": "@@ -0,0 +1,117 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one or more\n+  ~ contributor license agreements.  See the NOTICE file distributed with\n+  ~ this work for additional information regarding copyright ownership.\n+  ~ The ASF licenses this file to You under the Apache License, Version 2.0\n+  ~ (the \"License\"); you may not use this file except in compliance with\n+  ~ the License.  You may obtain a copy of the License at\n+  ~\n+  ~    http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing, software\n+  ~ distributed under the License is distributed on an \"AS IS\" BASIS,\n+  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  ~ See the License for the specific language governing permissions and\n+  ~ limitations under the License.\n+  -->\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n+  xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n+  xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+  <modelVersion>4.0.0</modelVersion>\n+  <parent>\n+    <groupId>org.apache.spark</groupId>\n+    <artifactId>spark-parent_2.11</artifactId>\n+    <version>2.2.0-SNAPSHOT</version>",
    "line": 25
  }],
  "prId": 12004
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "So this only needs to come in for Hadoop 2.7+, not 2.6?",
    "commit": "83d936870ad0651fc2622593e53d3e31d7eb8d4b",
    "createdAt": "2017-04-27T13:27:29Z",
    "diffHunk": "@@ -0,0 +1,117 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one or more\n+  ~ contributor license agreements.  See the NOTICE file distributed with\n+  ~ this work for additional information regarding copyright ownership.\n+  ~ The ASF licenses this file to You under the Apache License, Version 2.0\n+  ~ (the \"License\"); you may not use this file except in compliance with\n+  ~ the License.  You may obtain a copy of the License at\n+  ~\n+  ~    http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing, software\n+  ~ distributed under the License is distributed on an \"AS IS\" BASIS,\n+  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  ~ See the License for the specific language governing permissions and\n+  ~ limitations under the License.\n+  -->\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n+  xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n+  xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+  <modelVersion>4.0.0</modelVersion>\n+  <parent>\n+    <groupId>org.apache.spark</groupId>\n+    <artifactId>spark-parent_2.11</artifactId>\n+    <version>2.2.0-SNAPSHOT</version>\n+    <relativePath>../pom.xml</relativePath>\n+  </parent>\n+\n+  <artifactId>spark-hadoop-cloud_2.11</artifactId>\n+  <packaging>jar</packaging>\n+  <name>Spark Project Cloud Integration</name>\n+  <description>\n+    Contains support for cloud infrastructures, specifically the Hadoop JARs and\n+    transitive dependencies needed to interact with the infrastructures.\n+\n+    Any project which explicitly depends upon the spark-hadoop-cloud artifact will get the\n+    dependencies; the exact versions of which will depend upon the hadoop version Spark was compiled\n+    against.\n+\n+    The imports of transitive dependencies are managed to make them consistent\n+    with those of the Spark build.\n+\n+    WARNING: the signatures of methods in the AWS and Azure SDKs do change between\n+    versions: use exactly the same version with which the Hadoop JARs were\n+    built.\n+  </description>\n+  <properties>\n+    <sbt.project.name>hadoop-cloud</sbt.project.name>\n+  </properties>\n+\n+  <dependencies>\n+    <dependency>\n+      <groupId>org.apache.hadoop</groupId>\n+      <artifactId>hadoop-aws</artifactId>\n+      <scope>${hadoop.deps.scope}</scope>\n+    </dependency>\n+\n+    <dependency>\n+      <groupId>org.apache.hadoop</groupId>\n+      <artifactId>hadoop-openstack</artifactId>\n+      <scope>${hadoop.deps.scope}</scope>\n+    </dependency>\n+    <!--\n+    Add joda time to ensure that anything downstream which doesn't pull in spark-hive\n+    gets the correct joda time artifact, so doesn't have auth failures on later Java 8 JVMs\n+    -->\n+    <dependency>\n+      <groupId>joda-time</groupId>\n+      <artifactId>joda-time</artifactId>\n+      <scope>${hadoop.deps.scope}</scope>\n+    </dependency>\n+    <!-- explicitly declare the jackson artifacts desired -->\n+    <dependency>\n+      <groupId>com.fasterxml.jackson.core</groupId>\n+      <artifactId>jackson-databind</artifactId>\n+      <scope>${hadoop.deps.scope}</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>com.fasterxml.jackson.core</groupId>\n+      <artifactId>jackson-annotations</artifactId>\n+      <scope>${hadoop.deps.scope}</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>com.fasterxml.jackson.dataformat</groupId>\n+      <artifactId>jackson-dataformat-cbor</artifactId>\n+      <scope>${hadoop.deps.scope}</scope>\n+    </dependency>\n+    <!--Explicit declaration to force in Spark version into transitive dependencies -->\n+    <dependency>\n+      <groupId>org.apache.httpcomponents</groupId>\n+      <artifactId>httpclient</artifactId>\n+      <scope>${hadoop.deps.scope}</scope>\n+    </dependency>\n+    <!--Explicit declaration to force in Spark version into transitive dependencies -->\n+    <dependency>\n+      <groupId>org.apache.httpcomponents</groupId>\n+      <artifactId>httpcore</artifactId>\n+      <scope>${hadoop.deps.scope}</scope>\n+    </dependency>\n+  </dependencies>\n+\n+  <profiles>\n+\n+    <profile>\n+      <id>hadoop-2.7</id>",
    "line": 105
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "yes\r\n\r\n* 2.7 adds `hadoop-azure` for `wasb:`\r\n* 2.8 adds `hadoop-azure-datalake` for `adl:`\r\n\r\nThere's going to be an aggregate POM in trunk, `hadoop-cloud-storage`, which declares all transitive stuff, ideally stripping down cruft we don't need. That way if new things go in, anything pulling that JAR shouldn't have to add new declarations. There's still the problem of transitive breakage of JARs (i.e. Jackson)",
    "commit": "83d936870ad0651fc2622593e53d3e31d7eb8d4b",
    "createdAt": "2017-04-27T16:59:41Z",
    "diffHunk": "@@ -0,0 +1,117 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one or more\n+  ~ contributor license agreements.  See the NOTICE file distributed with\n+  ~ this work for additional information regarding copyright ownership.\n+  ~ The ASF licenses this file to You under the Apache License, Version 2.0\n+  ~ (the \"License\"); you may not use this file except in compliance with\n+  ~ the License.  You may obtain a copy of the License at\n+  ~\n+  ~    http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing, software\n+  ~ distributed under the License is distributed on an \"AS IS\" BASIS,\n+  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  ~ See the License for the specific language governing permissions and\n+  ~ limitations under the License.\n+  -->\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n+  xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n+  xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+  <modelVersion>4.0.0</modelVersion>\n+  <parent>\n+    <groupId>org.apache.spark</groupId>\n+    <artifactId>spark-parent_2.11</artifactId>\n+    <version>2.2.0-SNAPSHOT</version>\n+    <relativePath>../pom.xml</relativePath>\n+  </parent>\n+\n+  <artifactId>spark-hadoop-cloud_2.11</artifactId>\n+  <packaging>jar</packaging>\n+  <name>Spark Project Cloud Integration</name>\n+  <description>\n+    Contains support for cloud infrastructures, specifically the Hadoop JARs and\n+    transitive dependencies needed to interact with the infrastructures.\n+\n+    Any project which explicitly depends upon the spark-hadoop-cloud artifact will get the\n+    dependencies; the exact versions of which will depend upon the hadoop version Spark was compiled\n+    against.\n+\n+    The imports of transitive dependencies are managed to make them consistent\n+    with those of the Spark build.\n+\n+    WARNING: the signatures of methods in the AWS and Azure SDKs do change between\n+    versions: use exactly the same version with which the Hadoop JARs were\n+    built.\n+  </description>\n+  <properties>\n+    <sbt.project.name>hadoop-cloud</sbt.project.name>\n+  </properties>\n+\n+  <dependencies>\n+    <dependency>\n+      <groupId>org.apache.hadoop</groupId>\n+      <artifactId>hadoop-aws</artifactId>\n+      <scope>${hadoop.deps.scope}</scope>\n+    </dependency>\n+\n+    <dependency>\n+      <groupId>org.apache.hadoop</groupId>\n+      <artifactId>hadoop-openstack</artifactId>\n+      <scope>${hadoop.deps.scope}</scope>\n+    </dependency>\n+    <!--\n+    Add joda time to ensure that anything downstream which doesn't pull in spark-hive\n+    gets the correct joda time artifact, so doesn't have auth failures on later Java 8 JVMs\n+    -->\n+    <dependency>\n+      <groupId>joda-time</groupId>\n+      <artifactId>joda-time</artifactId>\n+      <scope>${hadoop.deps.scope}</scope>\n+    </dependency>\n+    <!-- explicitly declare the jackson artifacts desired -->\n+    <dependency>\n+      <groupId>com.fasterxml.jackson.core</groupId>\n+      <artifactId>jackson-databind</artifactId>\n+      <scope>${hadoop.deps.scope}</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>com.fasterxml.jackson.core</groupId>\n+      <artifactId>jackson-annotations</artifactId>\n+      <scope>${hadoop.deps.scope}</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>com.fasterxml.jackson.dataformat</groupId>\n+      <artifactId>jackson-dataformat-cbor</artifactId>\n+      <scope>${hadoop.deps.scope}</scope>\n+    </dependency>\n+    <!--Explicit declaration to force in Spark version into transitive dependencies -->\n+    <dependency>\n+      <groupId>org.apache.httpcomponents</groupId>\n+      <artifactId>httpclient</artifactId>\n+      <scope>${hadoop.deps.scope}</scope>\n+    </dependency>\n+    <!--Explicit declaration to force in Spark version into transitive dependencies -->\n+    <dependency>\n+      <groupId>org.apache.httpcomponents</groupId>\n+      <artifactId>httpcore</artifactId>\n+      <scope>${hadoop.deps.scope}</scope>\n+    </dependency>\n+  </dependencies>\n+\n+  <profiles>\n+\n+    <profile>\n+      <id>hadoop-2.7</id>",
    "line": 105
  }],
  "prId": 12004
}]