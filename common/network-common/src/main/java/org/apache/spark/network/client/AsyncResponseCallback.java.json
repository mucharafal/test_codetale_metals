[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "I'm not sure why you need this interface? What in the existing `RpcResponseCallback` interface makes it not suitable for asynchronous replies?",
    "commit": "da93837805efc76c83511eed6a71d49851b34945",
    "createdAt": "2019-11-13T23:36:11Z",
    "diffHunk": "@@ -0,0 +1,32 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.client;\n+\n+/**\n+ * Callback for the result of a single call.\n+ * This will be invoked once with either success or failure.\n+ */\n+public interface AsyncResponseCallback<T> {"
  }, {
    "author": {
      "login": "attilapiros"
    },
    "body": "I prefer this solution because the asynchronous access of local dirs for executors not just goes trough multiple classes but also the asynchronous result itself is used in multiple places.\r\n\r\nIt starts from https://github.com/apache/spark/blob/eafeb478aab411b3517b1e2b4b66099225c4b8c9/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala#L492 and which goes to https://github.com/apache/spark/blob/eafeb478aab411b3517b1e2b4b66099225c4b8c9/core/src/main/scala/org/apache/spark/storage/BlockManager.scala#L142\r\n\r\nBoth places are using the asynchronous result: the first one to access the blocks the second one for caching the directories. With `AsyncResponseCallback[java.util.Map[String, Array[String]]]` I can avoid decoding the message again and again and it is more type safe (I would even say typed) as with `RpcResponseCallback` there is only just a simple `ByteBuffer` which after decoding needs to be casted to the exact `BlockTransferMessage` to get the directories.",
    "commit": "da93837805efc76c83511eed6a71d49851b34945",
    "createdAt": "2019-11-14T11:23:42Z",
    "diffHunk": "@@ -0,0 +1,32 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.client;\n+\n+/**\n+ * Callback for the result of a single call.\n+ * This will be invoked once with either success or failure.\n+ */\n+public interface AsyncResponseCallback<T> {"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "That's not my question though. My question is what is different? Because to me, other than the name, this interface is exactly the same as the existing one. I don't see any special handling of it in your code, or really anything that tells me it has different functionality.\r\n\r\nIn fact, in all the places I noticed, `onSuccess` and `onFailure` are being called just like in a regular `RpcResponseCallback`, so I'm not even seeing anything \"async\" about its use at all.\r\n\r\nSo why does it have to exist?",
    "commit": "da93837805efc76c83511eed6a71d49851b34945",
    "createdAt": "2019-11-14T16:24:05Z",
    "diffHunk": "@@ -0,0 +1,32 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.client;\n+\n+/**\n+ * Callback for the result of a single call.\n+ * This will be invoked once with either success or failure.\n+ */\n+public interface AsyncResponseCallback<T> {"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Ok, the only difference I noticed is that this has a type parameter. So it has nothing to do with it being async, but with deserialization. Which makes me think that it doesn't belong here, since this library intentionally does not expose serialization functionality to its users.\r\n\r\n(The fact that no method in this library even uses this type directly is another tell that it doesn't really belong here.)\r\n\r\nInstead, you should have something in core if you want to have a typed response callback and reuse it in those two methods. e.g. a `Promise`.",
    "commit": "da93837805efc76c83511eed6a71d49851b34945",
    "createdAt": "2019-11-14T16:29:20Z",
    "diffHunk": "@@ -0,0 +1,32 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.client;\n+\n+/**\n+ * Callback for the result of a single call.\n+ * This will be invoked once with either success or failure.\n+ */\n+public interface AsyncResponseCallback<T> {"
  }, {
    "author": {
      "login": "attilapiros"
    },
    "body": "> (The fact that no method in this library even uses this type directly is another tell that it doesn't really belong here.)\r\n\r\nIt is both used by core and network-shuffle and currently `RpcResponseCallback` extends `AsyncResponseCallback` as the generic type there is simply `ByteBuffer ` so this is why I put it there. \r\n",
    "commit": "da93837805efc76c83511eed6a71d49851b34945",
    "createdAt": "2019-11-14T16:55:14Z",
    "diffHunk": "@@ -0,0 +1,32 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.client;\n+\n+/**\n+ * Callback for the result of a single call.\n+ * This will be invoked once with either success or failure.\n+ */\n+public interface AsyncResponseCallback<T> {"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Ok I missed that it's used in `ExternalBlockStoreClient` which is not in core. Still, it doesn't look like it belongs in this module for the reason I explained - it's exposing functionality that is intentionally not provided by this module.\r\n\r\nLooking at it, it seems that `ExternalBlockStoreClient.getHostLocalDirs` could be implemented in `BlockManager`, thus avoiding this. It's just sending an RPC, so all you need are the request and response types defined in this library, and the code to actually call `sendRpc` could be in `BlockManager`. Then you could just use a `Promise`, which is both async (if needed) and typed, in the call sites.",
    "commit": "da93837805efc76c83511eed6a71d49851b34945",
    "createdAt": "2019-11-14T17:08:27Z",
    "diffHunk": "@@ -0,0 +1,32 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.client;\n+\n+/**\n+ * Callback for the result of a single call.\n+ * This will be invoked once with either success or failure.\n+ */\n+public interface AsyncResponseCallback<T> {"
  }, {
    "author": {
      "login": "attilapiros"
    },
    "body": "> sendRpc could be in BlockManager\r\n\r\nBut the `TransportClientFactory` for creating the clients to send the RPC is in `ExternalBlockStoreClient`. What would be your preference regarding that? ",
    "commit": "da93837805efc76c83511eed6a71d49851b34945",
    "createdAt": "2019-11-14T17:29:57Z",
    "diffHunk": "@@ -0,0 +1,32 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.client;\n+\n+/**\n+ * Callback for the result of a single call.\n+ * This will be invoked once with either success or failure.\n+ */\n+public interface AsyncResponseCallback<T> {"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Do this then.\r\n\r\n- Get rid of this interface. It doesn't really belong in network-common anyway. At worst it would belong in network-shuffle, but I think it's not needed.\r\n- Make `getHostLocalDirs` return a `java.util.concurrent.Future`, and internally use a `java.util.concurrent.CompletableFuture` to update it.\r\n\r\nYou should also use `sendRpc` instead of `sendRpcSync` + a thread pool. That makes retries a little more complicated (but not much; `fetchBlocks` handles that, and in this case should be much simpler).\r\n\r\nOr, to simplify, you could just avoid retries altogether. How often will the call fail, really? And if it does, you just lose an optimization; you can try again next time there is a shuffle.\r\n\r\nOn the Scala side, just wrap the Java future:\r\n```\r\nval scalaFuture = Future { javaFuture.get }\r\n```\r\n\r\nOr expose `CompletableFuture` and use its callback methods directly.",
    "commit": "da93837805efc76c83511eed6a71d49851b34945",
    "createdAt": "2019-11-14T17:54:19Z",
    "diffHunk": "@@ -0,0 +1,32 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.client;\n+\n+/**\n+ * Callback for the result of a single call.\n+ * This will be invoked once with either success or failure.\n+ */\n+public interface AsyncResponseCallback<T> {"
  }, {
    "author": {
      "login": "attilapiros"
    },
    "body": "Yes, without the retries it is much simpler. Thanks for the ideas. ",
    "commit": "da93837805efc76c83511eed6a71d49851b34945",
    "createdAt": "2019-11-14T18:14:32Z",
    "diffHunk": "@@ -0,0 +1,32 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.client;\n+\n+/**\n+ * Callback for the result of a single call.\n+ * This will be invoked once with either success or failure.\n+ */\n+public interface AsyncResponseCallback<T> {"
  }],
  "prId": 25299
}]