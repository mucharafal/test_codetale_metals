[{
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "nit: indent more",
    "commit": "faa583f88b410fc7ededafe36bcb1ef878482d44",
    "createdAt": "2019-05-07T19:45:28Z",
    "diffHunk": "@@ -156,66 +156,50 @@ private static void assertStillThere(TestShuffleDataContext dataContext) {\n     }\n   }\n \n-  private static FilenameFilter filter = new FilenameFilter() {\n-    @Override\n-    public boolean accept(File dir, String name) {\n-      // Don't delete shuffle data or shuffle index files.\n-      return !name.endsWith(\".index\") && !name.endsWith(\".data\");\n-    }\n+  private static FilenameFilter filter = (dir, name) -> {\n+    // Don't delete shuffle data or shuffle index files.\n+    return !name.endsWith(\".index\") && !name.endsWith(\".data\") && !name.startsWith(\"rdd_\");\n   };\n \n-  private static boolean assertOnlyShuffleDataInDir(File[] dirs) {\n+  private static boolean assertOnlyFilesToKeepInDir(File[] dirs) {\n     for (File dir : dirs) {\n       assertTrue(dir.getName() + \" wasn't cleaned up\", !dir.exists() ||\n-        dir.listFiles(filter).length == 0 || assertOnlyShuffleDataInDir(dir.listFiles()));\n+        dir.listFiles(filter).length == 0 || assertOnlyFilesToKeepInDir(dir.listFiles()));\n     }\n     return true;\n   }\n \n   private static void assertCleanedUp(TestShuffleDataContext dataContext) {\n     for (String localDir : dataContext.localDirs) {\n       File[] dirs = new File[] {new File(localDir)};\n-      assertOnlyShuffleDataInDir(dirs);\n+      assertOnlyFilesToKeepInDir(dirs);\n     }\n   }\n \n-  private static TestShuffleDataContext initDataContext(boolean withShuffleFiles)\n-      throws IOException {\n-    if (withShuffleFiles) {\n-      return initDataContextWithShuffleFiles();\n-    } else {\n-      return initDataContextWithoutShuffleFiles();\n-    }\n-  }\n-\n-  private static TestShuffleDataContext initDataContextWithShuffleFiles() throws IOException {\n-    TestShuffleDataContext dataContext = createDataContext();\n-    createShuffleFiles(dataContext);\n-    createNonShuffleFiles(dataContext);\n-    return dataContext;\n-  }\n-\n-  private static TestShuffleDataContext initDataContextWithoutShuffleFiles() throws IOException {\n-    TestShuffleDataContext dataContext = createDataContext();\n-    createNonShuffleFiles(dataContext);\n-    return dataContext;\n-  }\n-\n-  private static TestShuffleDataContext createDataContext() {\n+  private static TestShuffleDataContext initDataContext(boolean withFilesToKeep)\n+    throws IOException {"
  }],
  "prId": 24499
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "It would be good to have this config name in a constant (e.g. some new class in `org.apache.spark.network.shuffle`).",
    "commit": "faa583f88b410fc7ededafe36bcb1ef878482d44",
    "createdAt": "2019-05-20T18:03:45Z",
    "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.shuffle;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.*;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import com.google.common.util.concurrent.MoreExecutors;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+import org.apache.spark.network.util.MapConfigProvider;\n+import org.apache.spark.network.util.TransportConf;\n+\n+public class CleanupNonShuffleServiceServedFilesSuite {\n+\n+  // Same-thread Executor used to ensure cleanup happens synchronously in test thread.\n+  private Executor sameThreadExecutor = MoreExecutors.sameThreadExecutor();\n+\n+  private static final String SORT_MANAGER = \"org.apache.spark.shuffle.sort.SortShuffleManager\";\n+\n+  private static Set<String> expectedShuffleFilesToKeep =\n+    ImmutableSet.of(\"shuffle_782_450_0.index\", \"shuffle_782_450_0.data\");\n+\n+  private static Set<String> expectedShuffleAndRddFilesToKeep =\n+    ImmutableSet.of(\"shuffle_782_450_0.index\", \"shuffle_782_450_0.data\", \"rdd_12_34\");\n+\n+  private TransportConf getConf(boolean isFetchRddEnabled) {\n+    return new TransportConf(\n+      \"shuffle\",\n+      new MapConfigProvider(ImmutableMap.of(\n+        \"spark.shuffle.service.fetch.rdd.enabled\","
  }, {
    "author": {
      "login": "attilapiros"
    },
    "body": "What about creating a new class `Constant` in that package? \r\n\r\nShould I only add `spark.shuffle.service.fetch.rdd.enabled` or check within this PR for other possible constants to extract? I am happy to help and for me extracting constants seams to me a low risk refactoring but I would like to avoid making the reviewer's life harder. What is preference here? ",
    "commit": "faa583f88b410fc7ededafe36bcb1ef878482d44",
    "createdAt": "2019-05-20T21:26:17Z",
    "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.shuffle;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.*;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import com.google.common.util.concurrent.MoreExecutors;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+import org.apache.spark.network.util.MapConfigProvider;\n+import org.apache.spark.network.util.TransportConf;\n+\n+public class CleanupNonShuffleServiceServedFilesSuite {\n+\n+  // Same-thread Executor used to ensure cleanup happens synchronously in test thread.\n+  private Executor sameThreadExecutor = MoreExecutors.sameThreadExecutor();\n+\n+  private static final String SORT_MANAGER = \"org.apache.spark.shuffle.sort.SortShuffleManager\";\n+\n+  private static Set<String> expectedShuffleFilesToKeep =\n+    ImmutableSet.of(\"shuffle_782_450_0.index\", \"shuffle_782_450_0.data\");\n+\n+  private static Set<String> expectedShuffleAndRddFilesToKeep =\n+    ImmutableSet.of(\"shuffle_782_450_0.index\", \"shuffle_782_450_0.data\", \"rdd_12_34\");\n+\n+  private TransportConf getConf(boolean isFetchRddEnabled) {\n+    return new TransportConf(\n+      \"shuffle\",\n+      new MapConfigProvider(ImmutableMap.of(\n+        \"spark.shuffle.service.fetch.rdd.enabled\","
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Just change this config. Others can be changed later as needed.",
    "commit": "faa583f88b410fc7ededafe36bcb1ef878482d44",
    "createdAt": "2019-05-20T21:34:39Z",
    "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.shuffle;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.*;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import com.google.common.util.concurrent.MoreExecutors;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+import org.apache.spark.network.util.MapConfigProvider;\n+import org.apache.spark.network.util.TransportConf;\n+\n+public class CleanupNonShuffleServiceServedFilesSuite {\n+\n+  // Same-thread Executor used to ensure cleanup happens synchronously in test thread.\n+  private Executor sameThreadExecutor = MoreExecutors.sameThreadExecutor();\n+\n+  private static final String SORT_MANAGER = \"org.apache.spark.shuffle.sort.SortShuffleManager\";\n+\n+  private static Set<String> expectedShuffleFilesToKeep =\n+    ImmutableSet.of(\"shuffle_782_450_0.index\", \"shuffle_782_450_0.data\");\n+\n+  private static Set<String> expectedShuffleAndRddFilesToKeep =\n+    ImmutableSet.of(\"shuffle_782_450_0.index\", \"shuffle_782_450_0.data\", \"rdd_12_34\");\n+\n+  private TransportConf getConf(boolean isFetchRddEnabled) {\n+    return new TransportConf(\n+      \"shuffle\",\n+      new MapConfigProvider(ImmutableMap.of(\n+        \"spark.shuffle.service.fetch.rdd.enabled\","
  }],
  "prId": 24499
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Comment sounds weird.\r\n\r\n\"Executor which only captures whether it's being used, without executing anything.\"",
    "commit": "faa583f88b410fc7ededafe36bcb1ef878482d44",
    "createdAt": "2019-05-20T18:09:07Z",
    "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.shuffle;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.*;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import com.google.common.util.concurrent.MoreExecutors;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+import org.apache.spark.network.util.MapConfigProvider;\n+import org.apache.spark.network.util.TransportConf;\n+\n+public class CleanupNonShuffleServiceServedFilesSuite {\n+\n+  // Same-thread Executor used to ensure cleanup happens synchronously in test thread.\n+  private Executor sameThreadExecutor = MoreExecutors.sameThreadExecutor();\n+\n+  private static final String SORT_MANAGER = \"org.apache.spark.shuffle.sort.SortShuffleManager\";\n+\n+  private static Set<String> expectedShuffleFilesToKeep =\n+    ImmutableSet.of(\"shuffle_782_450_0.index\", \"shuffle_782_450_0.data\");\n+\n+  private static Set<String> expectedShuffleAndRddFilesToKeep =\n+    ImmutableSet.of(\"shuffle_782_450_0.index\", \"shuffle_782_450_0.data\", \"rdd_12_34\");\n+\n+  private TransportConf getConf(boolean isFetchRddEnabled) {\n+    return new TransportConf(\n+      \"shuffle\",\n+      new MapConfigProvider(ImmutableMap.of(\n+        \"spark.shuffle.service.fetch.rdd.enabled\",\n+        Boolean.toString(isFetchRddEnabled))));\n+  }\n+\n+  @Test\n+  public void cleanupOnRemovedExecutorWithFilesToKeepFetchRddEnabled() throws IOException {\n+    cleanupOnRemovedExecutor(true, getConf(true), expectedShuffleAndRddFilesToKeep);\n+  }\n+\n+  @Test\n+  public void cleanupOnRemovedExecutorWithFilesToKeepFetchRddDisabled() throws IOException {\n+    cleanupOnRemovedExecutor(true, getConf(false), expectedShuffleFilesToKeep);\n+  }\n+\n+  @Test\n+  public void cleanupOnRemovedExecutorWithoutFilesToKeep() throws IOException {\n+    cleanupOnRemovedExecutor(false, getConf(true), Collections.emptySet());\n+  }\n+\n+  private void cleanupOnRemovedExecutor(\n+      boolean withFilesToKeep,\n+      TransportConf conf,\n+      Set<String> expectedFilesKept) throws IOException {\n+    TestShuffleDataContext dataContext = initDataContext(withFilesToKeep);\n+\n+    ExternalShuffleBlockResolver resolver =\n+      new ExternalShuffleBlockResolver(conf, null, sameThreadExecutor);\n+    resolver.registerExecutor(\"app\", \"exec0\", dataContext.createExecutorInfo(SORT_MANAGER));\n+    resolver.executorRemoved(\"exec0\", \"app\");\n+\n+    assertContainedFilenames(dataContext, expectedFilesKept);\n+  }\n+\n+  @Test\n+  public void cleanupUsesExecutorWithFilesToKeep() throws IOException {\n+    cleanupUsesExecutor(true);\n+  }\n+\n+  @Test\n+  public void cleanupUsesExecutorWithoutFilesToKeep() throws IOException {\n+    cleanupUsesExecutor(false);\n+  }\n+\n+  private void cleanupUsesExecutor(boolean withFilesToKeep) throws IOException {\n+    TestShuffleDataContext dataContext = initDataContext(withFilesToKeep);\n+\n+    AtomicBoolean cleanupCalled = new AtomicBoolean(false);\n+\n+    // Executor which does nothing to ensure we're actually using it."
  }, {
    "author": {
      "login": "attilapiros"
    },
    "body": "Comes from the old code. But sure I will fix this.",
    "commit": "faa583f88b410fc7ededafe36bcb1ef878482d44",
    "createdAt": "2019-05-20T20:45:22Z",
    "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.shuffle;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.*;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import com.google.common.util.concurrent.MoreExecutors;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+import org.apache.spark.network.util.MapConfigProvider;\n+import org.apache.spark.network.util.TransportConf;\n+\n+public class CleanupNonShuffleServiceServedFilesSuite {\n+\n+  // Same-thread Executor used to ensure cleanup happens synchronously in test thread.\n+  private Executor sameThreadExecutor = MoreExecutors.sameThreadExecutor();\n+\n+  private static final String SORT_MANAGER = \"org.apache.spark.shuffle.sort.SortShuffleManager\";\n+\n+  private static Set<String> expectedShuffleFilesToKeep =\n+    ImmutableSet.of(\"shuffle_782_450_0.index\", \"shuffle_782_450_0.data\");\n+\n+  private static Set<String> expectedShuffleAndRddFilesToKeep =\n+    ImmutableSet.of(\"shuffle_782_450_0.index\", \"shuffle_782_450_0.data\", \"rdd_12_34\");\n+\n+  private TransportConf getConf(boolean isFetchRddEnabled) {\n+    return new TransportConf(\n+      \"shuffle\",\n+      new MapConfigProvider(ImmutableMap.of(\n+        \"spark.shuffle.service.fetch.rdd.enabled\",\n+        Boolean.toString(isFetchRddEnabled))));\n+  }\n+\n+  @Test\n+  public void cleanupOnRemovedExecutorWithFilesToKeepFetchRddEnabled() throws IOException {\n+    cleanupOnRemovedExecutor(true, getConf(true), expectedShuffleAndRddFilesToKeep);\n+  }\n+\n+  @Test\n+  public void cleanupOnRemovedExecutorWithFilesToKeepFetchRddDisabled() throws IOException {\n+    cleanupOnRemovedExecutor(true, getConf(false), expectedShuffleFilesToKeep);\n+  }\n+\n+  @Test\n+  public void cleanupOnRemovedExecutorWithoutFilesToKeep() throws IOException {\n+    cleanupOnRemovedExecutor(false, getConf(true), Collections.emptySet());\n+  }\n+\n+  private void cleanupOnRemovedExecutor(\n+      boolean withFilesToKeep,\n+      TransportConf conf,\n+      Set<String> expectedFilesKept) throws IOException {\n+    TestShuffleDataContext dataContext = initDataContext(withFilesToKeep);\n+\n+    ExternalShuffleBlockResolver resolver =\n+      new ExternalShuffleBlockResolver(conf, null, sameThreadExecutor);\n+    resolver.registerExecutor(\"app\", \"exec0\", dataContext.createExecutorInfo(SORT_MANAGER));\n+    resolver.executorRemoved(\"exec0\", \"app\");\n+\n+    assertContainedFilenames(dataContext, expectedFilesKept);\n+  }\n+\n+  @Test\n+  public void cleanupUsesExecutorWithFilesToKeep() throws IOException {\n+    cleanupUsesExecutor(true);\n+  }\n+\n+  @Test\n+  public void cleanupUsesExecutorWithoutFilesToKeep() throws IOException {\n+    cleanupUsesExecutor(false);\n+  }\n+\n+  private void cleanupUsesExecutor(boolean withFilesToKeep) throws IOException {\n+    TestShuffleDataContext dataContext = initDataContext(withFilesToKeep);\n+\n+    AtomicBoolean cleanupCalled = new AtomicBoolean(false);\n+\n+    // Executor which does nothing to ensure we're actually using it."
  }],
  "prId": 24499
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "\"dummyExecutor\"?",
    "commit": "faa583f88b410fc7ededafe36bcb1ef878482d44",
    "createdAt": "2019-05-20T18:09:28Z",
    "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.shuffle;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.*;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import com.google.common.util.concurrent.MoreExecutors;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+import org.apache.spark.network.util.MapConfigProvider;\n+import org.apache.spark.network.util.TransportConf;\n+\n+public class CleanupNonShuffleServiceServedFilesSuite {\n+\n+  // Same-thread Executor used to ensure cleanup happens synchronously in test thread.\n+  private Executor sameThreadExecutor = MoreExecutors.sameThreadExecutor();\n+\n+  private static final String SORT_MANAGER = \"org.apache.spark.shuffle.sort.SortShuffleManager\";\n+\n+  private static Set<String> expectedShuffleFilesToKeep =\n+    ImmutableSet.of(\"shuffle_782_450_0.index\", \"shuffle_782_450_0.data\");\n+\n+  private static Set<String> expectedShuffleAndRddFilesToKeep =\n+    ImmutableSet.of(\"shuffle_782_450_0.index\", \"shuffle_782_450_0.data\", \"rdd_12_34\");\n+\n+  private TransportConf getConf(boolean isFetchRddEnabled) {\n+    return new TransportConf(\n+      \"shuffle\",\n+      new MapConfigProvider(ImmutableMap.of(\n+        \"spark.shuffle.service.fetch.rdd.enabled\",\n+        Boolean.toString(isFetchRddEnabled))));\n+  }\n+\n+  @Test\n+  public void cleanupOnRemovedExecutorWithFilesToKeepFetchRddEnabled() throws IOException {\n+    cleanupOnRemovedExecutor(true, getConf(true), expectedShuffleAndRddFilesToKeep);\n+  }\n+\n+  @Test\n+  public void cleanupOnRemovedExecutorWithFilesToKeepFetchRddDisabled() throws IOException {\n+    cleanupOnRemovedExecutor(true, getConf(false), expectedShuffleFilesToKeep);\n+  }\n+\n+  @Test\n+  public void cleanupOnRemovedExecutorWithoutFilesToKeep() throws IOException {\n+    cleanupOnRemovedExecutor(false, getConf(true), Collections.emptySet());\n+  }\n+\n+  private void cleanupOnRemovedExecutor(\n+      boolean withFilesToKeep,\n+      TransportConf conf,\n+      Set<String> expectedFilesKept) throws IOException {\n+    TestShuffleDataContext dataContext = initDataContext(withFilesToKeep);\n+\n+    ExternalShuffleBlockResolver resolver =\n+      new ExternalShuffleBlockResolver(conf, null, sameThreadExecutor);\n+    resolver.registerExecutor(\"app\", \"exec0\", dataContext.createExecutorInfo(SORT_MANAGER));\n+    resolver.executorRemoved(\"exec0\", \"app\");\n+\n+    assertContainedFilenames(dataContext, expectedFilesKept);\n+  }\n+\n+  @Test\n+  public void cleanupUsesExecutorWithFilesToKeep() throws IOException {\n+    cleanupUsesExecutor(true);\n+  }\n+\n+  @Test\n+  public void cleanupUsesExecutorWithoutFilesToKeep() throws IOException {\n+    cleanupUsesExecutor(false);\n+  }\n+\n+  private void cleanupUsesExecutor(boolean withFilesToKeep) throws IOException {\n+    TestShuffleDataContext dataContext = initDataContext(withFilesToKeep);\n+\n+    AtomicBoolean cleanupCalled = new AtomicBoolean(false);\n+\n+    // Executor which does nothing to ensure we're actually using it.\n+    Executor noThreadExecutor = runnable -> cleanupCalled.set(true);"
  }, {
    "author": {
      "login": "attilapiros"
    },
    "body": "Comes from the old code. But sure I will rename.",
    "commit": "faa583f88b410fc7ededafe36bcb1ef878482d44",
    "createdAt": "2019-05-20T20:45:04Z",
    "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.shuffle;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.*;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import com.google.common.util.concurrent.MoreExecutors;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+import org.apache.spark.network.util.MapConfigProvider;\n+import org.apache.spark.network.util.TransportConf;\n+\n+public class CleanupNonShuffleServiceServedFilesSuite {\n+\n+  // Same-thread Executor used to ensure cleanup happens synchronously in test thread.\n+  private Executor sameThreadExecutor = MoreExecutors.sameThreadExecutor();\n+\n+  private static final String SORT_MANAGER = \"org.apache.spark.shuffle.sort.SortShuffleManager\";\n+\n+  private static Set<String> expectedShuffleFilesToKeep =\n+    ImmutableSet.of(\"shuffle_782_450_0.index\", \"shuffle_782_450_0.data\");\n+\n+  private static Set<String> expectedShuffleAndRddFilesToKeep =\n+    ImmutableSet.of(\"shuffle_782_450_0.index\", \"shuffle_782_450_0.data\", \"rdd_12_34\");\n+\n+  private TransportConf getConf(boolean isFetchRddEnabled) {\n+    return new TransportConf(\n+      \"shuffle\",\n+      new MapConfigProvider(ImmutableMap.of(\n+        \"spark.shuffle.service.fetch.rdd.enabled\",\n+        Boolean.toString(isFetchRddEnabled))));\n+  }\n+\n+  @Test\n+  public void cleanupOnRemovedExecutorWithFilesToKeepFetchRddEnabled() throws IOException {\n+    cleanupOnRemovedExecutor(true, getConf(true), expectedShuffleAndRddFilesToKeep);\n+  }\n+\n+  @Test\n+  public void cleanupOnRemovedExecutorWithFilesToKeepFetchRddDisabled() throws IOException {\n+    cleanupOnRemovedExecutor(true, getConf(false), expectedShuffleFilesToKeep);\n+  }\n+\n+  @Test\n+  public void cleanupOnRemovedExecutorWithoutFilesToKeep() throws IOException {\n+    cleanupOnRemovedExecutor(false, getConf(true), Collections.emptySet());\n+  }\n+\n+  private void cleanupOnRemovedExecutor(\n+      boolean withFilesToKeep,\n+      TransportConf conf,\n+      Set<String> expectedFilesKept) throws IOException {\n+    TestShuffleDataContext dataContext = initDataContext(withFilesToKeep);\n+\n+    ExternalShuffleBlockResolver resolver =\n+      new ExternalShuffleBlockResolver(conf, null, sameThreadExecutor);\n+    resolver.registerExecutor(\"app\", \"exec0\", dataContext.createExecutorInfo(SORT_MANAGER));\n+    resolver.executorRemoved(\"exec0\", \"app\");\n+\n+    assertContainedFilenames(dataContext, expectedFilesKept);\n+  }\n+\n+  @Test\n+  public void cleanupUsesExecutorWithFilesToKeep() throws IOException {\n+    cleanupUsesExecutor(true);\n+  }\n+\n+  @Test\n+  public void cleanupUsesExecutorWithoutFilesToKeep() throws IOException {\n+    cleanupUsesExecutor(false);\n+  }\n+\n+  private void cleanupUsesExecutor(boolean withFilesToKeep) throws IOException {\n+    TestShuffleDataContext dataContext = initDataContext(withFilesToKeep);\n+\n+    AtomicBoolean cleanupCalled = new AtomicBoolean(false);\n+\n+    // Executor which does nothing to ensure we're actually using it.\n+    Executor noThreadExecutor = runnable -> cleanupCalled.set(true);"
  }],
  "prId": 24499
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: indent more",
    "commit": "faa583f88b410fc7ededafe36bcb1ef878482d44",
    "createdAt": "2019-05-21T22:16:27Z",
    "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.shuffle;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.*;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import com.google.common.util.concurrent.MoreExecutors;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+import org.apache.spark.network.util.MapConfigProvider;\n+import org.apache.spark.network.util.TransportConf;\n+\n+public class CleanupNonShuffleServiceServedFilesSuite {\n+\n+  // Same-thread Executor used to ensure cleanup happens synchronously in test thread.\n+  private Executor sameThreadExecutor = MoreExecutors.sameThreadExecutor();\n+\n+  private static final String SORT_MANAGER = \"org.apache.spark.shuffle.sort.SortShuffleManager\";\n+\n+  private static Set<String> expectedShuffleFilesToKeep =\n+    ImmutableSet.of(\"shuffle_782_450_0.index\", \"shuffle_782_450_0.data\");\n+\n+  private static Set<String> expectedShuffleAndRddFilesToKeep =\n+    ImmutableSet.of(\"shuffle_782_450_0.index\", \"shuffle_782_450_0.data\", \"rdd_12_34\");\n+\n+  private TransportConf getConf(boolean isFetchRddEnabled) {\n+    return new TransportConf(\n+      \"shuffle\",\n+      new MapConfigProvider(ImmutableMap.of(\n+        Constant.SHUFFLE_SERVICE_FETCH_RDD_ENABLED,\n+        Boolean.toString(isFetchRddEnabled))));\n+  }\n+\n+  @Test\n+  public void cleanupOnRemovedExecutorWithFilesToKeepFetchRddEnabled() throws IOException {\n+    cleanupOnRemovedExecutor(true, getConf(true), expectedShuffleAndRddFilesToKeep);\n+  }\n+\n+  @Test\n+  public void cleanupOnRemovedExecutorWithFilesToKeepFetchRddDisabled() throws IOException {\n+    cleanupOnRemovedExecutor(true, getConf(false), expectedShuffleFilesToKeep);\n+  }\n+\n+  @Test\n+  public void cleanupOnRemovedExecutorWithoutFilesToKeep() throws IOException {\n+    cleanupOnRemovedExecutor(false, getConf(true), Collections.emptySet());\n+  }\n+\n+  private void cleanupOnRemovedExecutor(\n+      boolean withFilesToKeep,\n+      TransportConf conf,\n+      Set<String> expectedFilesKept) throws IOException {\n+    TestShuffleDataContext dataContext = initDataContext(withFilesToKeep);\n+\n+    ExternalShuffleBlockResolver resolver =\n+      new ExternalShuffleBlockResolver(conf, null, sameThreadExecutor);\n+    resolver.registerExecutor(\"app\", \"exec0\", dataContext.createExecutorInfo(SORT_MANAGER));\n+    resolver.executorRemoved(\"exec0\", \"app\");\n+\n+    assertContainedFilenames(dataContext, expectedFilesKept);\n+  }\n+\n+  @Test\n+  public void cleanupUsesExecutorWithFilesToKeep() throws IOException {\n+    cleanupUsesExecutor(true);\n+  }\n+\n+  @Test\n+  public void cleanupUsesExecutorWithoutFilesToKeep() throws IOException {\n+    cleanupUsesExecutor(false);\n+  }\n+\n+  private void cleanupUsesExecutor(boolean withFilesToKeep) throws IOException {\n+    TestShuffleDataContext dataContext = initDataContext(withFilesToKeep);\n+\n+    AtomicBoolean cleanupCalled = new AtomicBoolean(false);\n+\n+    // Executor which only captures whether it's being used, without executing anything.\n+    Executor dummyExecutor = runnable -> cleanupCalled.set(true);\n+\n+    ExternalShuffleBlockResolver manager =\n+      new ExternalShuffleBlockResolver(getConf(true), null, dummyExecutor);\n+\n+    manager.registerExecutor(\"app\", \"exec0\", dataContext.createExecutorInfo(SORT_MANAGER));\n+    manager.executorRemoved(\"exec0\", \"app\");\n+\n+    assertTrue(cleanupCalled.get());\n+    assertStillThere(dataContext);\n+  }\n+\n+  @Test\n+  public void cleanupOnlyRemovedExecutorWithFilesToKeepFetchRddEnabled() throws IOException {\n+    cleanupOnlyRemovedExecutor(true, getConf(true), expectedShuffleAndRddFilesToKeep);\n+  }\n+\n+  @Test\n+  public void cleanupOnlyRemovedExecutorWithFilesToKeepFetchRddDisabled() throws IOException {\n+    cleanupOnlyRemovedExecutor(true, getConf(false), expectedShuffleFilesToKeep);\n+  }\n+\n+  @Test\n+  public void cleanupOnlyRemovedExecutorWithoutFilesToKeep() throws IOException {\n+    cleanupOnlyRemovedExecutor(false, getConf(true) , Collections.emptySet());\n+  }\n+\n+  private void cleanupOnlyRemovedExecutor(\n+      boolean withFilesToKeep,\n+      TransportConf conf,\n+      Set<String> expectedFilesKept) throws IOException {\n+    TestShuffleDataContext dataContext0 = initDataContext(withFilesToKeep);\n+    TestShuffleDataContext dataContext1 = initDataContext(withFilesToKeep);\n+\n+    ExternalShuffleBlockResolver resolver =\n+      new ExternalShuffleBlockResolver(conf, null, sameThreadExecutor);\n+    resolver.registerExecutor(\"app\", \"exec0\", dataContext0.createExecutorInfo(SORT_MANAGER));\n+    resolver.registerExecutor(\"app\", \"exec1\", dataContext1.createExecutorInfo(SORT_MANAGER));\n+\n+\n+    resolver.executorRemoved(\"exec-nonexistent\", \"app\");\n+    assertStillThere(dataContext0);\n+    assertStillThere(dataContext1);\n+\n+    resolver.executorRemoved(\"exec0\", \"app\");\n+    assertContainedFilenames(dataContext0, expectedFilesKept);\n+    assertStillThere(dataContext1);\n+\n+    resolver.executorRemoved(\"exec1\", \"app\");\n+    assertContainedFilenames(dataContext0, expectedFilesKept);\n+    assertContainedFilenames(dataContext1, expectedFilesKept);\n+\n+    // Make sure it's not an error to cleanup multiple times\n+    resolver.executorRemoved(\"exec1\", \"app\");\n+    assertContainedFilenames(dataContext0, expectedFilesKept);\n+    assertContainedFilenames(dataContext1, expectedFilesKept);\n+  }\n+\n+  @Test\n+  public void cleanupOnlyRegisteredExecutorWithFilesToKeepFetchRddEnabled() throws IOException {\n+    cleanupOnlyRegisteredExecutor(true, getConf(true), expectedShuffleAndRddFilesToKeep);\n+  }\n+\n+  @Test\n+  public void cleanupOnlyRegisteredExecutorWithFilesToKeepFetchRddDisabled() throws IOException {\n+    cleanupOnlyRegisteredExecutor(true, getConf(false), expectedShuffleFilesToKeep);\n+  }\n+\n+  @Test\n+  public void cleanupOnlyRegisteredExecutorWithoutFilesToKeep() throws IOException {\n+    cleanupOnlyRegisteredExecutor(false, getConf(true), Collections.emptySet());\n+  }\n+\n+  private void cleanupOnlyRegisteredExecutor(\n+      boolean withFilesToKeep,\n+      TransportConf conf,\n+      Set<String> expectedFilesKept) throws IOException {\n+    TestShuffleDataContext dataContext = initDataContext(withFilesToKeep);\n+\n+    ExternalShuffleBlockResolver resolver =\n+      new ExternalShuffleBlockResolver(conf, null, sameThreadExecutor);\n+    resolver.registerExecutor(\"app\", \"exec0\", dataContext.createExecutorInfo(SORT_MANAGER));\n+\n+    resolver.executorRemoved(\"exec1\", \"app\");\n+    assertStillThere(dataContext);\n+\n+    resolver.executorRemoved(\"exec0\", \"app\");\n+    assertContainedFilenames(dataContext, expectedFilesKept);\n+  }\n+\n+  private static void assertStillThere(TestShuffleDataContext dataContext) {\n+    for (String localDir : dataContext.localDirs) {\n+      assertTrue(localDir + \" was cleaned up prematurely\", new File(localDir).exists());\n+    }\n+  }\n+\n+  private static Set<String> collectFilenames(File[] files) throws IOException {\n+    Set<String> result = new HashSet<>();\n+    for (File file : files) {\n+      if (file.exists()) {\n+        try (Stream<Path> walk = Files.walk(file.toPath())) {\n+          result.addAll(walk\n+            .filter(Files::isRegularFile)\n+            .map(x -> x.toFile().getName())\n+            .collect(Collectors.toSet()));\n+        }\n+      }\n+    }\n+    return result;\n+  }\n+\n+  private static void assertContainedFilenames(\n+      TestShuffleDataContext dataContext,\n+      Set<String> expectedFilenames) throws IOException {\n+    Set<String> collectedFilenames = new HashSet<>();\n+    for (String localDir : dataContext.localDirs) {\n+      File[] dirs = new File[] {new File(localDir)};\n+      collectedFilenames.addAll(collectFilenames(dirs));\n+    }\n+    assertEquals(expectedFilenames, collectedFilenames);\n+  }\n+\n+  private static TestShuffleDataContext initDataContext(boolean withFilesToKeep)\n+      throws IOException {\n+    TestShuffleDataContext dataContext = new TestShuffleDataContext(10, 5);\n+    dataContext.create();\n+    if (withFilesToKeep) {\n+      createFilesToKeep(dataContext);\n+    } else {\n+      createRemovableTestFiles(dataContext);\n+    }\n+    return dataContext;\n+  }\n+\n+  private static void createFilesToKeep(TestShuffleDataContext dataContext) throws IOException {\n+    Random rand = new Random(123);\n+    dataContext.insertSortShuffleData(rand.nextInt(1000), rand.nextInt(1000), new byte[][] {\n+        \"ABC\".getBytes(StandardCharsets.UTF_8),\n+        \"DEF\".getBytes(StandardCharsets.UTF_8)});\n+    dataContext.insertCachedRddData(12, 34, new byte[] { 42 });\n+  }\n+\n+  private static void createRemovableTestFiles(TestShuffleDataContext dataContext)\n+    throws IOException {"
  }],
  "prId": 24499
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: spaces around `{ }`",
    "commit": "faa583f88b410fc7ededafe36bcb1ef878482d44",
    "createdAt": "2019-05-22T21:32:53Z",
    "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.network.shuffle;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.*;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import com.google.common.util.concurrent.MoreExecutors;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+import org.apache.spark.network.util.MapConfigProvider;\n+import org.apache.spark.network.util.TransportConf;\n+\n+public class CleanupNonShuffleServiceServedFilesSuite {\n+\n+  // Same-thread Executor used to ensure cleanup happens synchronously in test thread.\n+  private Executor sameThreadExecutor = MoreExecutors.sameThreadExecutor();\n+\n+  private static final String SORT_MANAGER = \"org.apache.spark.shuffle.sort.SortShuffleManager\";\n+\n+  private static Set<String> expectedShuffleFilesToKeep =\n+    ImmutableSet.of(\"shuffle_782_450_0.index\", \"shuffle_782_450_0.data\");\n+\n+  private static Set<String> expectedShuffleAndRddFilesToKeep =\n+    ImmutableSet.of(\"shuffle_782_450_0.index\", \"shuffle_782_450_0.data\", \"rdd_12_34\");\n+\n+  private TransportConf getConf(boolean isFetchRddEnabled) {\n+    return new TransportConf(\n+      \"shuffle\",\n+      new MapConfigProvider(ImmutableMap.of(\n+        Constants.SHUFFLE_SERVICE_FETCH_RDD_ENABLED,\n+        Boolean.toString(isFetchRddEnabled))));\n+  }\n+\n+  @Test\n+  public void cleanupOnRemovedExecutorWithFilesToKeepFetchRddEnabled() throws IOException {\n+    cleanupOnRemovedExecutor(true, getConf(true), expectedShuffleAndRddFilesToKeep);\n+  }\n+\n+  @Test\n+  public void cleanupOnRemovedExecutorWithFilesToKeepFetchRddDisabled() throws IOException {\n+    cleanupOnRemovedExecutor(true, getConf(false), expectedShuffleFilesToKeep);\n+  }\n+\n+  @Test\n+  public void cleanupOnRemovedExecutorWithoutFilesToKeep() throws IOException {\n+    cleanupOnRemovedExecutor(false, getConf(true), Collections.emptySet());\n+  }\n+\n+  private void cleanupOnRemovedExecutor(\n+      boolean withFilesToKeep,\n+      TransportConf conf,\n+      Set<String> expectedFilesKept) throws IOException {\n+    TestShuffleDataContext dataContext = initDataContext(withFilesToKeep);\n+\n+    ExternalShuffleBlockResolver resolver =\n+      new ExternalShuffleBlockResolver(conf, null, sameThreadExecutor);\n+    resolver.registerExecutor(\"app\", \"exec0\", dataContext.createExecutorInfo(SORT_MANAGER));\n+    resolver.executorRemoved(\"exec0\", \"app\");\n+\n+    assertContainedFilenames(dataContext, expectedFilesKept);\n+  }\n+\n+  @Test\n+  public void cleanupUsesExecutorWithFilesToKeep() throws IOException {\n+    cleanupUsesExecutor(true);\n+  }\n+\n+  @Test\n+  public void cleanupUsesExecutorWithoutFilesToKeep() throws IOException {\n+    cleanupUsesExecutor(false);\n+  }\n+\n+  private void cleanupUsesExecutor(boolean withFilesToKeep) throws IOException {\n+    TestShuffleDataContext dataContext = initDataContext(withFilesToKeep);\n+\n+    AtomicBoolean cleanupCalled = new AtomicBoolean(false);\n+\n+    // Executor which only captures whether it's being used, without executing anything.\n+    Executor dummyExecutor = runnable -> cleanupCalled.set(true);\n+\n+    ExternalShuffleBlockResolver manager =\n+      new ExternalShuffleBlockResolver(getConf(true), null, dummyExecutor);\n+\n+    manager.registerExecutor(\"app\", \"exec0\", dataContext.createExecutorInfo(SORT_MANAGER));\n+    manager.executorRemoved(\"exec0\", \"app\");\n+\n+    assertTrue(cleanupCalled.get());\n+    assertStillThere(dataContext);\n+  }\n+\n+  @Test\n+  public void cleanupOnlyRemovedExecutorWithFilesToKeepFetchRddEnabled() throws IOException {\n+    cleanupOnlyRemovedExecutor(true, getConf(true), expectedShuffleAndRddFilesToKeep);\n+  }\n+\n+  @Test\n+  public void cleanupOnlyRemovedExecutorWithFilesToKeepFetchRddDisabled() throws IOException {\n+    cleanupOnlyRemovedExecutor(true, getConf(false), expectedShuffleFilesToKeep);\n+  }\n+\n+  @Test\n+  public void cleanupOnlyRemovedExecutorWithoutFilesToKeep() throws IOException {\n+    cleanupOnlyRemovedExecutor(false, getConf(true) , Collections.emptySet());\n+  }\n+\n+  private void cleanupOnlyRemovedExecutor(\n+      boolean withFilesToKeep,\n+      TransportConf conf,\n+      Set<String> expectedFilesKept) throws IOException {\n+    TestShuffleDataContext dataContext0 = initDataContext(withFilesToKeep);\n+    TestShuffleDataContext dataContext1 = initDataContext(withFilesToKeep);\n+\n+    ExternalShuffleBlockResolver resolver =\n+      new ExternalShuffleBlockResolver(conf, null, sameThreadExecutor);\n+    resolver.registerExecutor(\"app\", \"exec0\", dataContext0.createExecutorInfo(SORT_MANAGER));\n+    resolver.registerExecutor(\"app\", \"exec1\", dataContext1.createExecutorInfo(SORT_MANAGER));\n+\n+\n+    resolver.executorRemoved(\"exec-nonexistent\", \"app\");\n+    assertStillThere(dataContext0);\n+    assertStillThere(dataContext1);\n+\n+    resolver.executorRemoved(\"exec0\", \"app\");\n+    assertContainedFilenames(dataContext0, expectedFilesKept);\n+    assertStillThere(dataContext1);\n+\n+    resolver.executorRemoved(\"exec1\", \"app\");\n+    assertContainedFilenames(dataContext0, expectedFilesKept);\n+    assertContainedFilenames(dataContext1, expectedFilesKept);\n+\n+    // Make sure it's not an error to cleanup multiple times\n+    resolver.executorRemoved(\"exec1\", \"app\");\n+    assertContainedFilenames(dataContext0, expectedFilesKept);\n+    assertContainedFilenames(dataContext1, expectedFilesKept);\n+  }\n+\n+  @Test\n+  public void cleanupOnlyRegisteredExecutorWithFilesToKeepFetchRddEnabled() throws IOException {\n+    cleanupOnlyRegisteredExecutor(true, getConf(true), expectedShuffleAndRddFilesToKeep);\n+  }\n+\n+  @Test\n+  public void cleanupOnlyRegisteredExecutorWithFilesToKeepFetchRddDisabled() throws IOException {\n+    cleanupOnlyRegisteredExecutor(true, getConf(false), expectedShuffleFilesToKeep);\n+  }\n+\n+  @Test\n+  public void cleanupOnlyRegisteredExecutorWithoutFilesToKeep() throws IOException {\n+    cleanupOnlyRegisteredExecutor(false, getConf(true), Collections.emptySet());\n+  }\n+\n+  private void cleanupOnlyRegisteredExecutor(\n+      boolean withFilesToKeep,\n+      TransportConf conf,\n+      Set<String> expectedFilesKept) throws IOException {\n+    TestShuffleDataContext dataContext = initDataContext(withFilesToKeep);\n+\n+    ExternalShuffleBlockResolver resolver =\n+      new ExternalShuffleBlockResolver(conf, null, sameThreadExecutor);\n+    resolver.registerExecutor(\"app\", \"exec0\", dataContext.createExecutorInfo(SORT_MANAGER));\n+\n+    resolver.executorRemoved(\"exec1\", \"app\");\n+    assertStillThere(dataContext);\n+\n+    resolver.executorRemoved(\"exec0\", \"app\");\n+    assertContainedFilenames(dataContext, expectedFilesKept);\n+  }\n+\n+  private static void assertStillThere(TestShuffleDataContext dataContext) {\n+    for (String localDir : dataContext.localDirs) {\n+      assertTrue(localDir + \" was cleaned up prematurely\", new File(localDir).exists());\n+    }\n+  }\n+\n+  private static Set<String> collectFilenames(File[] files) throws IOException {\n+    Set<String> result = new HashSet<>();\n+    for (File file : files) {\n+      if (file.exists()) {\n+        try (Stream<Path> walk = Files.walk(file.toPath())) {\n+          result.addAll(walk\n+            .filter(Files::isRegularFile)\n+            .map(x -> x.toFile().getName())\n+            .collect(Collectors.toSet()));\n+        }\n+      }\n+    }\n+    return result;\n+  }\n+\n+  private static void assertContainedFilenames(\n+      TestShuffleDataContext dataContext,\n+      Set<String> expectedFilenames) throws IOException {\n+    Set<String> collectedFilenames = new HashSet<>();\n+    for (String localDir : dataContext.localDirs) {\n+      File[] dirs = new File[] {new File(localDir)};"
  }],
  "prId": 24499
}]