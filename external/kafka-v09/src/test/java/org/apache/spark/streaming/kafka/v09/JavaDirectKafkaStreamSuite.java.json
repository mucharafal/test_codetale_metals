[{
  "comments": [{
    "author": {
      "login": "praveend"
    },
    "body": "probably writing 0L would be better rather than (long) casting\n",
    "commit": "0fcda2881b196265239fd38044b23f7ddcb53b45",
    "createdAt": "2016-01-04T09:10:07Z",
    "diffHunk": "@@ -0,0 +1,181 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka.v09;\n+\n+\n+import kafka.common.TopicAndPartition;\n+import kafka.message.MessageAndMetadata;\n+import kafka.serializer.StringDecoder;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.function.Function;\n+import org.apache.spark.streaming.Durations;\n+import org.apache.spark.streaming.api.java.JavaDStream;\n+import org.apache.spark.streaming.api.java.JavaStreamingContext;\n+import org.apache.spark.streaming.kafka.v09.HasOffsetRanges;\n+import org.apache.spark.streaming.kafka.v09.KafkaTestUtils;\n+import org.apache.spark.streaming.kafka.v09.KafkaUtils;\n+import org.apache.spark.streaming.kafka.v09.OffsetRange;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import scala.Tuple2;\n+\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+public class JavaDirectKafkaStreamSuite implements Serializable {\n+  private transient JavaStreamingContext ssc = null;\n+  private transient KafkaTestUtils kafkaTestUtils = null;\n+\n+  @Before\n+  public void setUp() {\n+    kafkaTestUtils = new KafkaTestUtils();\n+    kafkaTestUtils.setup();\n+    SparkConf sparkConf = new SparkConf()\n+        .setMaster(\"local[4]\").setAppName(this.getClass().getSimpleName());\n+    ssc = new JavaStreamingContext(sparkConf, Durations.milliseconds(200));\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    if (ssc != null) {\n+      ssc.stop();\n+      ssc = null;\n+    }\n+\n+    if (kafkaTestUtils != null) {\n+      kafkaTestUtils.teardown();\n+      kafkaTestUtils = null;\n+    }\n+  }\n+\n+  @Test\n+  public void testKafkaStream() throws InterruptedException {\n+    final String topic1 = \"new_topic1_testKafkaDirectStream\";\n+    final String topic2 = \"new_topic2_testKafkaDirectStream\";\n+    // hold a reference to the current offset ranges, so it can be used downstream\n+    final AtomicReference<OffsetRange[]> offsetRanges = new AtomicReference<>();\n+\n+    String[] topic1data = createTopicAndSendData(topic1);\n+    String[] topic2data = createTopicAndSendData(topic2);\n+\n+    HashSet<String> sent = new HashSet<String>();\n+    sent.addAll(Arrays.asList(topic1data));\n+    sent.addAll(Arrays.asList(topic2data));\n+\n+    HashMap<String, String> kafkaParams = new HashMap<String, String>();\n+    kafkaParams.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaTestUtils.brokerAddress());\n+    kafkaParams.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+    kafkaParams.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringDeserializer\");\n+    kafkaParams.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringDeserializer\");\n+    kafkaParams.put(\"spark.kafka.poll.time\", \"1000\");\n+\n+    JavaDStream<String> stream1 = KafkaUtils.createDirectStream(\n+        ssc,\n+        String.class,\n+        String.class,\n+        kafkaParams,\n+        topicToSet(topic1)\n+    ).transformToPair(\n+        // Make sure you can get offset ranges from the rdd\n+        new Function<JavaPairRDD<String, String>, JavaPairRDD<String, String>>() {\n+          @Override\n+          public JavaPairRDD<String, String> call(JavaPairRDD<String, String> rdd) throws Exception {\n+            OffsetRange[] offsets = ((HasOffsetRanges) rdd.rdd()).offsetRanges();\n+            offsetRanges.set(offsets);\n+            Assert.assertEquals(offsets[0].topic(), topic1);\n+            return rdd;\n+          }\n+        }\n+    ).map(\n+        new Function<Tuple2<String, String>, String>() {\n+          @Override\n+          public String call(Tuple2<String, String> kv) throws Exception {\n+            return kv._2();\n+          }\n+        }\n+    );\n+\n+    JavaDStream<String> stream2 = KafkaUtils.createDirectStream(\n+        ssc,\n+        String.class,\n+        String.class,\n+        String.class,\n+        kafkaParams,\n+        topicOffsetToMap(topic2, (long) 0),"
  }],
  "prId": 10294
}, {
  "comments": [{
    "author": {
      "login": "praveend"
    },
    "body": "Any reason why you removed static qualifier from these 2 methods (this and below one)? Also, in signature you are using specific class HashSet/HashMap rather than generic one [Set/Map]...any reason?\n",
    "commit": "0fcda2881b196265239fd38044b23f7ddcb53b45",
    "createdAt": "2016-01-04T09:12:12Z",
    "diffHunk": "@@ -0,0 +1,181 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka.v09;\n+\n+\n+import kafka.common.TopicAndPartition;\n+import kafka.message.MessageAndMetadata;\n+import kafka.serializer.StringDecoder;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.function.Function;\n+import org.apache.spark.streaming.Durations;\n+import org.apache.spark.streaming.api.java.JavaDStream;\n+import org.apache.spark.streaming.api.java.JavaStreamingContext;\n+import org.apache.spark.streaming.kafka.v09.HasOffsetRanges;\n+import org.apache.spark.streaming.kafka.v09.KafkaTestUtils;\n+import org.apache.spark.streaming.kafka.v09.KafkaUtils;\n+import org.apache.spark.streaming.kafka.v09.OffsetRange;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import scala.Tuple2;\n+\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+public class JavaDirectKafkaStreamSuite implements Serializable {\n+  private transient JavaStreamingContext ssc = null;\n+  private transient KafkaTestUtils kafkaTestUtils = null;\n+\n+  @Before\n+  public void setUp() {\n+    kafkaTestUtils = new KafkaTestUtils();\n+    kafkaTestUtils.setup();\n+    SparkConf sparkConf = new SparkConf()\n+        .setMaster(\"local[4]\").setAppName(this.getClass().getSimpleName());\n+    ssc = new JavaStreamingContext(sparkConf, Durations.milliseconds(200));\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    if (ssc != null) {\n+      ssc.stop();\n+      ssc = null;\n+    }\n+\n+    if (kafkaTestUtils != null) {\n+      kafkaTestUtils.teardown();\n+      kafkaTestUtils = null;\n+    }\n+  }\n+\n+  @Test\n+  public void testKafkaStream() throws InterruptedException {\n+    final String topic1 = \"new_topic1_testKafkaDirectStream\";\n+    final String topic2 = \"new_topic2_testKafkaDirectStream\";\n+    // hold a reference to the current offset ranges, so it can be used downstream\n+    final AtomicReference<OffsetRange[]> offsetRanges = new AtomicReference<>();\n+\n+    String[] topic1data = createTopicAndSendData(topic1);\n+    String[] topic2data = createTopicAndSendData(topic2);\n+\n+    HashSet<String> sent = new HashSet<String>();\n+    sent.addAll(Arrays.asList(topic1data));\n+    sent.addAll(Arrays.asList(topic2data));\n+\n+    HashMap<String, String> kafkaParams = new HashMap<String, String>();\n+    kafkaParams.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaTestUtils.brokerAddress());\n+    kafkaParams.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+    kafkaParams.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringDeserializer\");\n+    kafkaParams.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringDeserializer\");\n+    kafkaParams.put(\"spark.kafka.poll.time\", \"1000\");\n+\n+    JavaDStream<String> stream1 = KafkaUtils.createDirectStream(\n+        ssc,\n+        String.class,\n+        String.class,\n+        kafkaParams,\n+        topicToSet(topic1)\n+    ).transformToPair(\n+        // Make sure you can get offset ranges from the rdd\n+        new Function<JavaPairRDD<String, String>, JavaPairRDD<String, String>>() {\n+          @Override\n+          public JavaPairRDD<String, String> call(JavaPairRDD<String, String> rdd) throws Exception {\n+            OffsetRange[] offsets = ((HasOffsetRanges) rdd.rdd()).offsetRanges();\n+            offsetRanges.set(offsets);\n+            Assert.assertEquals(offsets[0].topic(), topic1);\n+            return rdd;\n+          }\n+        }\n+    ).map(\n+        new Function<Tuple2<String, String>, String>() {\n+          @Override\n+          public String call(Tuple2<String, String> kv) throws Exception {\n+            return kv._2();\n+          }\n+        }\n+    );\n+\n+    JavaDStream<String> stream2 = KafkaUtils.createDirectStream(\n+        ssc,\n+        String.class,\n+        String.class,\n+        String.class,\n+        kafkaParams,\n+        topicOffsetToMap(topic2, (long) 0),\n+        new Function<ConsumerRecord<String, String>, String>() {\n+          @Override\n+          public String call(ConsumerRecord<String, String> consumerRecord) throws Exception {\n+            return consumerRecord.value();\n+          }\n+        }\n+    );\n+    JavaDStream<String> unifiedStream = stream1.union(stream2);\n+\n+    final Set<String> result = Collections.synchronizedSet(new HashSet<String>());\n+    unifiedStream.foreachRDD(\n+        new Function<JavaRDD<String>, Void>() {\n+          @Override\n+          public Void call(JavaRDD<String> rdd) throws Exception {\n+            result.addAll(rdd.collect());\n+            for (OffsetRange o : offsetRanges.get()) {\n+              System.out.println(\n+                  o.topic() + \" \" + o.partition() + \" \" + o.fromOffset() + \" \" + o.untilOffset()\n+              );\n+            }\n+            return null;\n+          }\n+        }\n+    );\n+    ssc.start();\n+    long startTime = System.currentTimeMillis();\n+    boolean matches = false;\n+    while (!matches && System.currentTimeMillis() - startTime < 20000) {\n+      matches = sent.size() == result.size();\n+      Thread.sleep(50);\n+    }\n+    Assert.assertEquals(sent, result);\n+    ssc.stop();\n+  }\n+\n+  private HashSet<String> topicToSet(String topic) {"
  }, {
    "author": {
      "login": "nikit-os"
    },
    "body": "This is my mistake. I began to remake Kafka using version 1.5.2, but then moved to 1.6.0 and copied the old code without adaptation\n",
    "commit": "0fcda2881b196265239fd38044b23f7ddcb53b45",
    "createdAt": "2016-01-08T16:48:03Z",
    "diffHunk": "@@ -0,0 +1,181 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka.v09;\n+\n+\n+import kafka.common.TopicAndPartition;\n+import kafka.message.MessageAndMetadata;\n+import kafka.serializer.StringDecoder;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.function.Function;\n+import org.apache.spark.streaming.Durations;\n+import org.apache.spark.streaming.api.java.JavaDStream;\n+import org.apache.spark.streaming.api.java.JavaStreamingContext;\n+import org.apache.spark.streaming.kafka.v09.HasOffsetRanges;\n+import org.apache.spark.streaming.kafka.v09.KafkaTestUtils;\n+import org.apache.spark.streaming.kafka.v09.KafkaUtils;\n+import org.apache.spark.streaming.kafka.v09.OffsetRange;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import scala.Tuple2;\n+\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+public class JavaDirectKafkaStreamSuite implements Serializable {\n+  private transient JavaStreamingContext ssc = null;\n+  private transient KafkaTestUtils kafkaTestUtils = null;\n+\n+  @Before\n+  public void setUp() {\n+    kafkaTestUtils = new KafkaTestUtils();\n+    kafkaTestUtils.setup();\n+    SparkConf sparkConf = new SparkConf()\n+        .setMaster(\"local[4]\").setAppName(this.getClass().getSimpleName());\n+    ssc = new JavaStreamingContext(sparkConf, Durations.milliseconds(200));\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    if (ssc != null) {\n+      ssc.stop();\n+      ssc = null;\n+    }\n+\n+    if (kafkaTestUtils != null) {\n+      kafkaTestUtils.teardown();\n+      kafkaTestUtils = null;\n+    }\n+  }\n+\n+  @Test\n+  public void testKafkaStream() throws InterruptedException {\n+    final String topic1 = \"new_topic1_testKafkaDirectStream\";\n+    final String topic2 = \"new_topic2_testKafkaDirectStream\";\n+    // hold a reference to the current offset ranges, so it can be used downstream\n+    final AtomicReference<OffsetRange[]> offsetRanges = new AtomicReference<>();\n+\n+    String[] topic1data = createTopicAndSendData(topic1);\n+    String[] topic2data = createTopicAndSendData(topic2);\n+\n+    HashSet<String> sent = new HashSet<String>();\n+    sent.addAll(Arrays.asList(topic1data));\n+    sent.addAll(Arrays.asList(topic2data));\n+\n+    HashMap<String, String> kafkaParams = new HashMap<String, String>();\n+    kafkaParams.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaTestUtils.brokerAddress());\n+    kafkaParams.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+    kafkaParams.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringDeserializer\");\n+    kafkaParams.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringDeserializer\");\n+    kafkaParams.put(\"spark.kafka.poll.time\", \"1000\");\n+\n+    JavaDStream<String> stream1 = KafkaUtils.createDirectStream(\n+        ssc,\n+        String.class,\n+        String.class,\n+        kafkaParams,\n+        topicToSet(topic1)\n+    ).transformToPair(\n+        // Make sure you can get offset ranges from the rdd\n+        new Function<JavaPairRDD<String, String>, JavaPairRDD<String, String>>() {\n+          @Override\n+          public JavaPairRDD<String, String> call(JavaPairRDD<String, String> rdd) throws Exception {\n+            OffsetRange[] offsets = ((HasOffsetRanges) rdd.rdd()).offsetRanges();\n+            offsetRanges.set(offsets);\n+            Assert.assertEquals(offsets[0].topic(), topic1);\n+            return rdd;\n+          }\n+        }\n+    ).map(\n+        new Function<Tuple2<String, String>, String>() {\n+          @Override\n+          public String call(Tuple2<String, String> kv) throws Exception {\n+            return kv._2();\n+          }\n+        }\n+    );\n+\n+    JavaDStream<String> stream2 = KafkaUtils.createDirectStream(\n+        ssc,\n+        String.class,\n+        String.class,\n+        String.class,\n+        kafkaParams,\n+        topicOffsetToMap(topic2, (long) 0),\n+        new Function<ConsumerRecord<String, String>, String>() {\n+          @Override\n+          public String call(ConsumerRecord<String, String> consumerRecord) throws Exception {\n+            return consumerRecord.value();\n+          }\n+        }\n+    );\n+    JavaDStream<String> unifiedStream = stream1.union(stream2);\n+\n+    final Set<String> result = Collections.synchronizedSet(new HashSet<String>());\n+    unifiedStream.foreachRDD(\n+        new Function<JavaRDD<String>, Void>() {\n+          @Override\n+          public Void call(JavaRDD<String> rdd) throws Exception {\n+            result.addAll(rdd.collect());\n+            for (OffsetRange o : offsetRanges.get()) {\n+              System.out.println(\n+                  o.topic() + \" \" + o.partition() + \" \" + o.fromOffset() + \" \" + o.untilOffset()\n+              );\n+            }\n+            return null;\n+          }\n+        }\n+    );\n+    ssc.start();\n+    long startTime = System.currentTimeMillis();\n+    boolean matches = false;\n+    while (!matches && System.currentTimeMillis() - startTime < 20000) {\n+      matches = sent.size() == result.size();\n+      Thread.sleep(50);\n+    }\n+    Assert.assertEquals(sent, result);\n+    ssc.stop();\n+  }\n+\n+  private HashSet<String> topicToSet(String topic) {"
  }],
  "prId": 10294
}]