[{
  "comments": [{
    "author": {
      "login": "praveend"
    },
    "body": "We could probably remove this section of defining broker and leaders as we no longer expose the concept of brokers and leaders to end users\n",
    "commit": "0fcda2881b196265239fd38044b23f7ddcb53b45",
    "createdAt": "2016-01-04T09:21:56Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka.v09;\n+\n+import java.io.Serializable;\n+import java.util.HashMap;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.spark.streaming.kafka.v09.Broker;\n+import org.apache.spark.streaming.kafka.v09.KafkaTestUtils;\n+import org.apache.spark.streaming.kafka.v09.KafkaUtils;\n+import org.apache.spark.streaming.kafka.v09.OffsetRange;\n+import scala.Tuple2;\n+\n+import kafka.common.TopicAndPartition;\n+import kafka.message.MessageAndMetadata;\n+import kafka.serializer.StringDecoder;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.Function;\n+\n+public class JavaKafkaRDDSuite implements Serializable {\n+  private transient JavaSparkContext sc = null;\n+  private transient KafkaTestUtils kafkaTestUtils = null;\n+\n+  @Before\n+  public void setUp() {\n+    kafkaTestUtils = new KafkaTestUtils();\n+    kafkaTestUtils.setup();\n+    SparkConf sparkConf = new SparkConf()\n+        .setMaster(\"local[4]\").setAppName(this.getClass().getSimpleName());\n+    sc = new JavaSparkContext(sparkConf);\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    if (sc != null) {\n+      sc.stop();\n+      sc = null;\n+    }\n+\n+    if (kafkaTestUtils != null) {\n+      kafkaTestUtils.teardown();\n+      kafkaTestUtils = null;\n+    }\n+  }\n+\n+  @Test\n+  public void testKafkaRDD() throws InterruptedException {\n+    String topic1 = \"new_topic1_testKafkaRDD\";\n+    String topic2 = \"new_topic2_testKafkaRDD\";\n+\n+    String[] topic1data = createTopicAndSendData(topic1);\n+    String[] topic2data = createTopicAndSendData(topic2);\n+\n+    HashMap<String, String> kafkaParams = new HashMap<String, String>();\n+    kafkaParams.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaTestUtils.brokerAddress());\n+    kafkaParams.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringDeserializer\");\n+    kafkaParams.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringDeserializer\");\n+    kafkaParams.put(\"spark.kafka.poll.time\", \"1000\");\n+\n+    OffsetRange[] offsetRanges = {\n+        OffsetRange.create(topic1, 0, 0, 1),\n+        OffsetRange.create(topic2, 0, 0, 1)\n+    };\n+\n+    HashMap<TopicAndPartition, Broker> emptyLeaders = new HashMap<TopicAndPartition, Broker>();"
  }],
  "prId": 10294
}]