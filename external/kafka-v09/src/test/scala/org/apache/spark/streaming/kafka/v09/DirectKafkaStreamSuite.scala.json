[{
  "comments": [{
    "author": {
      "login": "praveend"
    },
    "body": "given that getLatestOffsets method returns a Map now and a get on that would return Option....we will need to change this line to \n\nkc.getLatestOffsets(Set(topicPartition)).get(topicPartition).getOrElse(0)\n",
    "commit": "0fcda2881b196265239fd38044b23f7ddcb53b45",
    "createdAt": "2016-01-04T09:45:39Z",
    "diffHunk": "@@ -0,0 +1,503 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka.v09\n+\n+import java.io.File\n+import java.util.concurrent.atomic.AtomicLong\n+\n+import kafka.common.TopicAndPartition\n+import org.apache.kafka.clients.consumer.{ ConsumerRecord, ConsumerConfig }\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.streaming.dstream.DStream\n+import org.apache.spark.streaming.scheduler.rate.RateEstimator\n+import org.apache.spark.streaming.scheduler.{ StreamingListenerBatchCompleted, StreamingListenerBatchStarted, StreamingListenerBatchSubmitted, StreamingListener }\n+import org.apache.spark.streaming.{ Time, Milliseconds, StreamingContext }\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{ SparkContext, SparkConf, Logging, SparkFunSuite }\n+import org.scalatest.concurrent.Eventually\n+import org.scalatest.{ BeforeAndAfter, BeforeAndAfterAll }\n+\n+import scala.collection.mutable\n+import scala.collection.mutable.ArrayBuffer\n+import scala.concurrent.duration._\n+import scala.language.postfixOps\n+\n+class DirectKafkaStreamSuite\n+  extends SparkFunSuite\n+  with BeforeAndAfter\n+  with BeforeAndAfterAll\n+  with Eventually\n+  with Logging {\n+  val sparkConf = new SparkConf()\n+    .setMaster(\"local[4]\")\n+    .setAppName(this.getClass.getSimpleName)\n+\n+  private var sc: SparkContext = _\n+  private var ssc: StreamingContext = _\n+  private var testDir: File = _\n+\n+  private var kafkaTestUtils: KafkaTestUtils = _\n+\n+  override def beforeAll {\n+    kafkaTestUtils = new KafkaTestUtils\n+    kafkaTestUtils.setup()\n+  }\n+\n+  override def afterAll {\n+    if (kafkaTestUtils != null) {\n+      kafkaTestUtils.teardown()\n+      kafkaTestUtils = null\n+    }\n+  }\n+\n+  after {\n+    if (ssc != null) {\n+      ssc.stop()\n+      sc = null\n+    }\n+    if (sc != null) {\n+      sc.stop()\n+    }\n+    if (testDir != null) {\n+      Utils.deleteRecursively(testDir)\n+    }\n+  }\n+\n+  test(\"basic stream receiving with multiple topics and earliest starting offset\") {\n+    val topics = Set(\"new_basic1\", \"new_basic2\", \"new_basic3\")\n+    val data = Map(\"a\" -> 7, \"b\" -> 9)\n+    topics.foreach { t =>\n+      kafkaTestUtils.createTopic(t)\n+      kafkaTestUtils.sendMessages(t, data)\n+    }\n+    val totalSent = data.values.sum * topics.size\n+    val kafkaParams = Map(\n+      ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -> kafkaTestUtils.brokerAddress,\n+      ConsumerConfig.AUTO_OFFSET_RESET_CONFIG -> \"earliest\",\n+      ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG ->\n+        \"org.apache.kafka.common.serialization.StringDeserializer\",\n+      ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG ->\n+        \"org.apache.kafka.common.serialization.StringDeserializer\",\n+      \"spark.kafka.poll.time\" -> \"1000\")\n+\n+    ssc = new StreamingContext(sparkConf, Milliseconds(200))\n+    val stream = withClue(\"Error creating direct stream\") {\n+      KafkaUtils.createDirectStream[String, String](\n+        ssc, kafkaParams, topics)\n+    }\n+\n+    val allReceived =\n+      new ArrayBuffer[(String, String)] with mutable.SynchronizedBuffer[(String, String)]\n+\n+    // hold a reference to the current offset ranges, so it can be used downstream\n+    var offsetRanges = Array[OffsetRange]()\n+\n+    stream.transform { rdd =>\n+      // Get the offset ranges in the RDD\n+      offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges\n+      rdd\n+    }.foreachRDD { rdd =>\n+      for (o <- offsetRanges) {\n+        log.info(s\"${rdd.id} | ${o.topic} ${o.partition} ${o.fromOffset} ${o.untilOffset}\")\n+      }\n+      val collected = rdd.mapPartitionsWithIndex { (i, iter) =>\n+        // For each partition, get size of the range in the partition,\n+        // and the number of items in the partition\n+        val off = offsetRanges(i)\n+        val all = iter.toSeq\n+        val partSize = all.size\n+        val rangeSize = off.untilOffset - off.fromOffset\n+        Iterator((partSize, rangeSize))\n+      }.collect\n+\n+      // Verify whether number of elements in each partition\n+      // matches with the corresponding offset range\n+      collected.foreach {\n+        case (partSize, rangeSize) =>\n+          assert(partSize === rangeSize, \"offset ranges are wrong\")\n+      }\n+    }\n+    stream.foreachRDD { rdd => allReceived ++= rdd.collect() }\n+    ssc.start()\n+    eventually(timeout(20000.milliseconds), interval(200.milliseconds)) {\n+      assert(allReceived.size === totalSent,\n+        \"didn't get expected number of messages, messages:\\n\" + allReceived.mkString(\"\\n\"))\n+    }\n+    ssc.stop()\n+  }\n+\n+  test(\"receiving from latest starting offset\") {\n+    val topic = \"new_latest\"\n+    val topicPartition = TopicAndPartition(topic, 0)\n+    val data = Map(\"a\" -> 10)\n+    kafkaTestUtils.createTopic(topic)\n+    val kafkaParams = Map(\n+      ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -> kafkaTestUtils.brokerAddress,\n+      ConsumerConfig.AUTO_OFFSET_RESET_CONFIG -> \"latest\",\n+      ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG ->\n+        \"org.apache.kafka.common.serialization.StringDeserializer\",\n+      ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG ->\n+        \"org.apache.kafka.common.serialization.StringDeserializer\",\n+      \"spark.kafka.poll.time\" -> \"100\")\n+    val kc = new KafkaCluster(kafkaParams)\n+    def getLatestOffset(): Long = {\n+      kc.getLatestOffsets(Set(topicPartition)).get(topicPartition).get"
  }],
  "prId": 10294
}]