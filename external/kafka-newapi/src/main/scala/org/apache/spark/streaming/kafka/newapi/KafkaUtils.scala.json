[{
  "comments": [{
    "author": {
      "login": "mariobriggs"
    },
    "body": "this method and the import of TopicAndPartition has to be knocked off\n",
    "commit": "229b773f9c5c894088a29937f241ad0db48991ba",
    "createdAt": "2016-01-29T14:00:52Z",
    "diffHunk": "@@ -0,0 +1,601 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka.newapi\n+\n+import java.io.OutputStream\n+import java.lang.{ Integer => JInt, Long => JLong }\n+import java.util.{ List => JList, Map => JMap, Set => JSet }\n+\n+import scala.collection.JavaConverters._\n+import scala.reflect.ClassTag\n+\n+import com.google.common.base.Charsets.UTF_8\n+import kafka.common.TopicAndPartition\n+import net.razorvine.pickle.{ IObjectPickler, Opcodes, Pickler }\n+import org.apache.kafka.clients.CommonClientConfigs\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.config.SslConfigs\n+\n+import org.apache.spark.{ SparkContext, SparkException, SSLOptions }\n+import org.apache.spark.api.java.{ JavaPairRDD, JavaRDD, JavaSparkContext }\n+import org.apache.spark.api.java.function.{ Function => JFunction }\n+import org.apache.spark.api.python.SerDeUtil\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.streaming.StreamingContext\n+import org.apache.spark.streaming.api.java._\n+import org.apache.spark.streaming.dstream.{ DStream, InputDStream }\n+\n+object KafkaUtils {\n+\n+  def addSSLOptions(\n+      kafkaParams: Map[String, String],\n+      sc: SparkContext): Map[String, String] = {\n+    val sparkConf = sc.getConf\n+    val defaultSSLOptions = SSLOptions.parse(sparkConf, \"spark.ssl\", None)\n+    val kafkaSSLOptions = SSLOptions.parse(sparkConf, \"spark.ssl.kafka\", Some(defaultSSLOptions))\n+\n+    if (kafkaSSLOptions.enabled) {\n+      val sslParams = Map[String, Option[_]](\n+        CommonClientConfigs.SECURITY_PROTOCOL_CONFIG -> Some(\"SSL\"),\n+        SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG -> kafkaSSLOptions.trustStore,\n+        SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG -> kafkaSSLOptions.trustStorePassword,\n+        SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG -> kafkaSSLOptions.keyStore,\n+        SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG -> kafkaSSLOptions.keyStorePassword,\n+        SslConfigs.SSL_KEY_PASSWORD_CONFIG -> kafkaSSLOptions.keyPassword)\n+      kafkaParams ++ sslParams.filter(_._2.isDefined).mapValues(_.get.toString)\n+    } else {\n+      kafkaParams\n+    }\n+\n+  }\n+\n+  /** Make sure offsets are available in kafka, or throw an exception */\n+  private def checkOffsets(\n+      kafkaParams: Map[String, String],\n+      offsetRanges: Array[OffsetRange]): Array[OffsetRange] = {\n+    val kc = new KafkaCluster(kafkaParams)\n+    try {\n+      val topics = offsetRanges.map(_.topicPartition).toSet\n+      val low = kc.getEarliestOffsets(topics)\n+      val high = kc.getLatestOffsetsWithLeaders(topics)\n+\n+      val result = offsetRanges.filterNot { o =>\n+        low(o.topicPartition()) <= o.fromOffset &&\n+          o.untilOffset <= high(o.topicPartition()).offset\n+      }\n+\n+      if (!result.isEmpty) {\n+        throw new SparkException(\"Offsets not available in Kafka: \" + result.mkString(\",\"))\n+      }\n+\n+      offsetRanges.map { o =>\n+        OffsetRange(o.topic, o.partition, o.fromOffset, o.untilOffset,\n+          high(o.topicPartition()).host)\n+      }\n+    } finally {\n+      kc.close()\n+    }\n+  }\n+\n+  /**\n+   * Create a RDD from Kafka using offset ranges for each topic and partition.\n+   *\n+   * @param sc SparkContext object\n+   * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+   *                    configuration parameters</a>. Requires \"bootstrap.servers\"\n+   *                    to be set with Kafka broker(s) (NOT zookeeper servers) specified in\n+   *                    host1:port1,host2:port2 form.\n+   * @param offsetRanges Each OffsetRange in the batch corresponds to a\n+   *                     range of offsets for a given Kafka topic/partition\n+   * @tparam K type of Kafka message key\n+   * @tparam V type of Kafka message value\n+   * @return RDD of (Kafka message key, Kafka message value)\n+   */\n+  def createRDD[K: ClassTag, V: ClassTag](\n+      sc: SparkContext,\n+      kafkaParams: Map[String, String],\n+      offsetRanges: Array[OffsetRange]): RDD[(K, V)] = sc.withScope {\n+    val messageHandler = (cr: ConsumerRecord[K, V]) => (cr.key, cr.value)\n+    new KafkaRDD[K, V, (K, V)](\n+      sc,\n+      addSSLOptions(kafkaParams, sc),\n+      checkOffsets(kafkaParams, offsetRanges),\n+      messageHandler)\n+  }\n+\n+  /**\n+   * Create a RDD from Kafka using offset ranges for each topic and partition. This allows you\n+   * specify the Kafka leader to connect to (to optimize fetching) and access the message as well\n+   * as the metadata.\n+   *\n+   * @param sc SparkContext object\n+   * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+   *                    configuration parameters</a>. Requires \"bootstrap.servers\"\n+   *                    to be set with Kafka broker(s) (NOT zookeeper servers) specified in\n+   *                    host1:port1,host2:port2 form.\n+   * @param offsetRanges Each OffsetRange in the batch corresponds to a\n+   *                     range of offsets for a given Kafka topic/partition\n+   * @param messageHandler Function for translating each message and metadata into the desired type\n+   *                       * @tparam K type of Kafka message key\n+   * @tparam K type of Kafka message key\n+   * @tparam V type of Kafka message value\n+   * @tparam R type returned by messageHandler\n+   * @return RDD of R\n+   */\n+  def createRDD[K: ClassTag, V: ClassTag, R: ClassTag](\n+      sc: SparkContext,\n+      kafkaParams: Map[String, String],\n+      offsetRanges: Array[OffsetRange],\n+      messageHandler: ConsumerRecord[K, V] => R): RDD[R] = sc.withScope {\n+    val kc = new KafkaCluster[K, V](addSSLOptions(kafkaParams, sc))\n+    val cleanedHandler = sc.clean(messageHandler)\n+    new KafkaRDD[K, V, R](sc,\n+      addSSLOptions(kafkaParams, sc),\n+      checkOffsets(kafkaParams, offsetRanges),\n+      cleanedHandler)\n+  }\n+\n+  /**\n+   * Create a RDD from Kafka using offset ranges for each topic and partition.\n+   *\n+   * @param jsc JavaSparkContext object\n+   * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+   *                    configuration parameters</a>. Requires \"bootstrap.servers\"\n+   *                    specified in host1:port1,host2:port2 form.\n+   * @param offsetRanges Each OffsetRange in the batch corresponds to a\n+   *                     range of offsets for a given Kafka topic/partition\n+   * @param keyClass type of Kafka message key\n+   * @param valueClass type of Kafka message value\n+   * @tparam K type of Kafka message key\n+   * @tparam V type of Kafka message value\n+   * @return RDD of (Kafka message key, Kafka message value)\n+   */\n+  def createRDD[K, V](\n+      jsc: JavaSparkContext,\n+      keyClass: Class[K],\n+      valueClass: Class[V],\n+      kafkaParams: JMap[String, String],\n+      offsetRanges: Array[OffsetRange]): JavaPairRDD[K, V] = jsc.sc.withScope {\n+    implicit val keyCmt: ClassTag[K] = ClassTag(keyClass)\n+    implicit val valueCmt: ClassTag[V] = ClassTag(valueClass)\n+    new JavaPairRDD(createRDD[K, V](\n+      jsc.sc, Map(kafkaParams.asScala.toSeq: _*), offsetRanges))\n+  }\n+\n+  /**\n+   * Create a RDD from Kafka using offset ranges for each topic and partition. This allows you\n+   * specify the Kafka leader to connect to (to optimize fetching) and access the message as well\n+   * as the metadata.\n+   *\n+   * @param jsc JavaSparkContext object\n+   * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+   *                    configuration parameters</a>. Requires \"bootstrap.servers\"\n+   *                    specified in host1:port1,host2:port2 form.\n+   * @param offsetRanges Each OffsetRange in the batch corresponds to a\n+   *                     range of offsets for a given Kafka topic/partition\n+   * @param messageHandler Function for translating each message and metadata into the desired type\n+   * @tparam K type of Kafka message key\n+   * @tparam V type of Kafka message value\n+   * @tparam R type returned by messageHandler\n+   * @return RDD of R\n+   */\n+  def createRDD[K, V, R](\n+      jsc: JavaSparkContext,\n+      keyClass: Class[K],\n+      valueClass: Class[V],\n+      recordClass: Class[R],\n+      kafkaParams: JMap[String, String],\n+      offsetRanges: Array[OffsetRange],\n+      messageHandler: JFunction[ConsumerRecord[K, V], R]): JavaRDD[R] = jsc.sc.withScope {\n+    implicit val keyCmt: ClassTag[K] = ClassTag(keyClass)\n+    implicit val valueCmt: ClassTag[V] = ClassTag(valueClass)\n+    implicit val recordCmt: ClassTag[R] = ClassTag(recordClass)\n+    createRDD[K, V, R](\n+      jsc.sc, Map(kafkaParams.asScala.toSeq: _*), offsetRanges, messageHandler.call _)\n+  }\n+\n+  /**\n+   * Create an input stream that directly pulls messages from Kafka Brokers\n+   * without using any receiver. This stream can guarantee that each message\n+   * from Kafka is included in transformations exactly once (see points below).\n+   *\n+   * Points to note:\n+   * - No receivers: This stream does not use any receiver. It directly queries Kafka\n+   * - Offsets: This does not use Zookeeper to store offsets. The consumed offsets are tracked\n+   * by the stream itself.\n+   * You can access the offsets used in each batch from the generated RDDs (see\n+   * [[HasOffsetRanges]]).\n+   * - Failure Recovery: To recover from driver failures, you have to enable checkpointing\n+   * in the [[StreamingContext]]. The information on consumed offset can be\n+   * recovered from the checkpoint. See the programming guide for details (constraints, etc.).\n+   * - End-to-end semantics: This stream ensures that every records is effectively received and\n+   * transformed exactly once, but gives no guarantees on whether the transformed data are\n+   * outputted exactly once. For end-to-end exactly-once semantics, you have to either ensure\n+   * that the output operation is idempotent, or use transactions to output records atomically.\n+   * See the programming guide for more details.\n+   *\n+   * @param ssc StreamingContext object\n+   * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+   *                    configuration parameters</a>. Requires \"bootstrap.servers\"\n+   *                    to be set with Kafka broker(s) (NOT zookeeper servers) specified in\n+   *                    host1:port1,host2:port2 form.\n+   * @param fromOffsets Per-topic/partition Kafka offsets defining the (inclusive)\n+   *                    starting point of the stream\n+   * @param messageHandler Function for translating each message and metadata into the desired type\n+   * @tparam K type of Kafka message key\n+   * @tparam V type of Kafka message value\n+   * @tparam R type returned by messageHandler\n+   * @return DStream of R\n+   */\n+  def createDirectStream[K: ClassTag, V: ClassTag, R: ClassTag](\n+      ssc: StreamingContext,\n+      kafkaParams: Map[String, String],\n+      fromOffsets: Map[TopicPartition, Long],\n+      messageHandler: ConsumerRecord[K, V] => R): InputDStream[R] = {\n+    val cleanedHandler = ssc.sc.clean(messageHandler)\n+    new DirectKafkaInputDStream[K, V, R](\n+      ssc, addSSLOptions(kafkaParams, ssc.sparkContext), fromOffsets, messageHandler)\n+  }\n+\n+  /**\n+   * Create an input stream that directly pulls messages from Kafka Brokers\n+   * without using any receiver. This stream can guarantee that each message\n+   * from Kafka is included in transformations exactly once (see points below).\n+   *\n+   * Points to note:\n+   * - No receivers: This stream does not use any receiver. It directly queries Kafka\n+   * - Offsets: This does not use Zookeeper to store offsets. The consumed offsets are tracked\n+   * by the stream itself.\n+   * You can access the offsets used in each batch from the generated RDDs (see\n+   * [[HasOffsetRanges]]).\n+   * - Failure Recovery: To recover from driver failures, you have to enable checkpointing\n+   * in the [[StreamingContext]]. The information on consumed offset can be\n+   * recovered from the checkpoint. See the programming guide for details (constraints, etc.).\n+   * - End-to-end semantics: This stream ensures that every records is effectively received and\n+   * transformed exactly once, but gives no guarantees on whether the transformed data are\n+   * outputted exactly once. For end-to-end exactly-once semantics, you have to either ensure\n+   * that the output operation is idempotent, or use transactions to output records atomically.\n+   * See the programming guide for more details.\n+   *\n+   * @param ssc StreamingContext object\n+   * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+   *                    configuration parameters</a>. Requires \"bootstrap.servers\"\n+   *                    to be set with Kafka broker(s) (NOT zookeeper servers), specified in\n+   *                    host1:port1,host2:port2 form.\n+   *                    If not starting from a checkpoint, \"auto.offset.reset\" may be set to\n+   *                    \"earliest\" or \"latest\" to determine where the stream starts\n+   *                    (defaults to \"latest\")\n+   * @param topics Names of the topics to consume\n+   * @tparam K type of Kafka message key\n+   * @tparam V type of Kafka message value\n+   * @return DStream of (Kafka message key, Kafka message value)\n+   */\n+  def createDirectStream[K: ClassTag, V: ClassTag](\n+      ssc: StreamingContext,\n+      kafkaParams: Map[String, String],\n+      topics: Set[String]): InputDStream[(K, V)] = {\n+    val messageHandler = (cr: ConsumerRecord[K, V]) => (cr.key, cr.value)\n+    val fromOffsets = getFromOffsets(kafkaParams, topics)\n+\n+    new DirectKafkaInputDStream[K, V, (K, V)](\n+      ssc, addSSLOptions(kafkaParams, ssc.sparkContext), fromOffsets, messageHandler)\n+  }\n+\n+  /**\n+   * Create an input stream that directly pulls messages from Kafka Brokers\n+   * without using any receiver. This stream can guarantee that each message\n+   * from Kafka is included in transformations exactly once (see points below).\n+   *\n+   * Points to note:\n+   * - No receivers: This stream does not use any receiver. It directly queries Kafka\n+   * - Offsets: This does not use Zookeeper to store offsets. The consumed offsets are tracked\n+   * by the stream itself.\n+   * You can access the offsets used in each batch from the generated RDDs (see\n+   * [[HasOffsetRanges]]).\n+   * - Failure Recovery: To recover from driver failures, you have to enable checkpointing\n+   * in the [[StreamingContext]]. The information on consumed offset can be\n+   * recovered from the checkpoint. See the programming guide for details (constraints, etc.).\n+   * - End-to-end semantics: This stream ensures that every records is effectively received and\n+   * transformed exactly once, but gives no guarantees on whether the transformed data are\n+   * outputted exactly once. For end-to-end exactly-once semantics, you have to either ensure\n+   * that the output operation is idempotent, or use transactions to output records atomically.\n+   * See the programming guide for more details.\n+   *\n+   * @param jssc JavaStreamingContext object\n+   * @param keyClass Class of the keys in the Kafka records\n+   * @param valueClass Class of the values in the Kafka records\n+   * @param recordClass Class of the records in DStream\n+   * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+   *                    configuration parameters</a>. Requires \"bootstrap.servers\"\n+   *                    specified in host1:port1,host2:port2 form.\n+   * @param fromOffsets Per-topic/partition Kafka offsets defining the (inclusive)\n+   *                    starting point of the stream\n+   * @param messageHandler Function for translating each message and metadata into the desired type\n+   * @tparam K type of Kafka message key\n+   * @tparam V type of Kafka message value\n+   * @tparam R type returned by messageHandler\n+   * @return DStream of R\n+   */\n+  def createDirectStream[K, V, R](\n+    jssc: JavaStreamingContext,\n+    keyClass: Class[K],\n+    valueClass: Class[V],\n+    recordClass: Class[R],\n+    kafkaParams: JMap[String, String],\n+    fromOffsets: JMap[TopicPartition, JLong],\n+    messageHandler: JFunction[ConsumerRecord[K, V], R]): JavaInputDStream[R] = {\n+    implicit val keyCmt: ClassTag[K] = ClassTag(keyClass)\n+    implicit val valueCmt: ClassTag[V] = ClassTag(valueClass)\n+    implicit val recordCmt: ClassTag[R] = ClassTag(recordClass)\n+    val cleanedHandler = jssc.sparkContext.clean(messageHandler.call _)\n+    createDirectStream[K, V, R](\n+      jssc.ssc,\n+      Map(kafkaParams.asScala.toSeq: _*),\n+      Map(fromOffsets.asScala.mapValues {\n+        _.longValue()\n+      }.toSeq: _*),\n+      cleanedHandler)\n+  }\n+\n+  /**\n+   * Create an input stream that directly pulls messages from Kafka Brokers\n+   * without using any receiver. This stream can guarantee that each message\n+   * from Kafka is included in transformations exactly once (see points below).\n+   *\n+   * Points to note:\n+   * - No receivers: This stream does not use any receiver. It directly queries Kafka\n+   * - Offsets: This does not use Zookeeper to store offsets. The consumed offsets are tracked\n+   * by the stream itself.\n+   * You can access the offsets used in each batch from the generated RDDs (see\n+   * [[HasOffsetRanges]]).\n+   * - Failure Recovery: To recover from driver failures, you have to enable checkpointing\n+   * in the [[StreamingContext]]. The information on consumed offset can be\n+   * recovered from the checkpoint. See the programming guide for details (constraints, etc.).\n+   * - End-to-end semantics: This stream ensures that every records is effectively received and\n+   * transformed exactly once, but gives no guarantees on whether the transformed data are\n+   * outputted exactly once. For end-to-end exactly-once semantics, you have to either ensure\n+   * that the output operation is idempotent, or use transactions to output records atomically.\n+   * See the programming guide for more details.\n+   *\n+   * @param jssc JavaStreamingContext object\n+   * @param keyClass Class of the keys in the Kafka records\n+   * @param valueClass Class of the values in the Kafka records\n+   * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+   *                    configuration parameters</a>. Requires \"bootstrap.servers\"\n+   *                    to be set with Kafka broker(s) (NOT zookeeper servers), specified in\n+   *                    host1:port1,host2:port2 form.\n+   *                    If not starting from a checkpoint, \"auto.offset.reset\" may be set\n+   *                    to \"latest\" or \"earliest\" to determine where the stream starts\n+   *                    (defaults to \"latest\")\n+   * @param topics Names of the topics to consume\n+   * @tparam K type of Kafka message key\n+   * @tparam V type of Kafka message value\n+   * @return DStream of (Kafka message key, Kafka message value)\n+   */\n+  def createDirectStream[K, V](\n+      jssc: JavaStreamingContext,\n+      keyClass: Class[K],\n+      valueClass: Class[V],\n+      kafkaParams: JMap[String, String],\n+      topics: JSet[String]): JavaPairInputDStream[K, V] = {\n+    implicit val keyCmt: ClassTag[K] = ClassTag(keyClass)\n+    implicit val valueCmt: ClassTag[V] = ClassTag(valueClass)\n+    createDirectStream[K, V](\n+      jssc.ssc,\n+      Map(kafkaParams.asScala.toSeq: _*),\n+      Set(topics.asScala.toSeq: _*))\n+  }\n+\n+  def createOffsetRange(topic: String, partition: JInt, fromOffset: JLong, untilOffset: JLong):\n+    OffsetRange = OffsetRange.create(topic, partition, fromOffset, untilOffset)\n+\n+  def createTopicAndPartition(topic: String, partition: JInt): TopicAndPartition ="
  }],
  "prId": 10953
}]