[{
  "comments": [{
    "author": {
      "login": "harishreedharan"
    },
    "body": "Why use both `ssc` and `ssc_`?\n",
    "commit": "229b773f9c5c894088a29937f241ad0db48991ba",
    "createdAt": "2016-02-03T22:35:23Z",
    "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka.newapi\n+\n+import scala.collection.mutable\n+import scala.reflect.ClassTag\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.kafka.newapi.KafkaCluster.LeaderOffset\n+import org.apache.spark.streaming.scheduler.{RateController, StreamInputInfo}\n+import org.apache.spark.streaming.scheduler.rate.RateEstimator\n+\n+/**\n+ * A stream of {@link org.apache.spark.streaming.kafka.KafkaRDD} where\n+ * each given Kafka topic/partition corresponds to an RDD partition.\n+ * The spark configuration spark.streaming.kafka.maxRatePerPartition gives the maximum number\n+ * of messages\n+ * per second that each '''partition''' will accept.\n+ * Starting offsets are specified in advance,\n+ * and this DStream is not responsible for committing offsets,\n+ * so that you can control exactly-once semantics.\n+ *\n+ * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+ *                    configuration parameters</a>.\n+ *                    Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set\n+ *                    with Kafka broker(s),\n+ *                    NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+ * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+ *                    starting point of the stream\n+ */\n+private[streaming]\n+class DirectKafkaInputDStream[\n+  K: ClassTag,\n+  V: ClassTag,\n+  R: ClassTag](\n+    @transient ssc_ : StreamingContext,\n+    val kafkaParams: Map[String, String],\n+    @transient val fromOffsets: Map[TopicPartition, Long],\n+    messageHandler: ConsumerRecord[K, V] => R\n+  ) extends InputDStream[R](ssc_) with Logging {\n+\n+  val maxRetries = context.sparkContext.getConf.getInt(\n+    \"spark.streaming.kafka.maxRetries\", 1)\n+\n+  // Keep this consistent with how other streams are named (e.g. \"Flume polling stream [2]\")\n+  private[streaming] override def name: String = s\"Kafka 0.9 direct stream [$id]\"\n+\n+  protected[streaming] override val checkpointData =\n+    new DirectKafkaInputDStreamCheckpointData\n+\n+\n+  /**\n+   * Asynchronously maintains & sends new rate limits to the receiver through the receiver tracker.\n+   */\n+  override protected[streaming] val rateController: Option[RateController] = {\n+    if (RateController.isBackPressureEnabled(ssc.conf)) {\n+      Some(new DirectKafkaRateController(id,\n+        RateEstimator.create(ssc.conf, ssc_.graph.batchDuration)))"
  }],
  "prId": 10953
}]