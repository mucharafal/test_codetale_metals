[{
  "comments": [{
    "author": {
      "login": "brkyvz"
    },
    "body": "cleaner to do \r\n`val Seq(shardToMerge, adjShared) = splitOpenShards`",
    "commit": "17a5c3a0b74414d2f65cfdf33d813599b1697804",
    "createdAt": "2016-12-09T19:33:56Z",
    "diffHunk": "@@ -225,6 +225,74 @@ abstract class KinesisStreamTests(aggregateTestData: Boolean) extends KinesisFun\n     ssc.stop(stopSparkContext = false)\n   }\n \n+  testIfEnabled(\"split and merge shards in a stream\") {\n+    // Since this test tries to split and merge shards in a stream, we create another\n+    // temporary stream and then remove it when finished.\n+    val localAppName = s\"KinesisStreamSuite-${math.abs(Random.nextLong())}\"\n+    val localTestUtils = new KPLBasedKinesisTestUtils(1)\n+    localTestUtils.createStream()\n+    try {\n+      val awsCredentials = KinesisTestUtils.getAWSCredentials()\n+      val stream = KinesisUtils.createStream(ssc, localAppName, localTestUtils.streamName,\n+        localTestUtils.endpointUrl, localTestUtils.regionName, InitialPositionInStream.LATEST,\n+        Seconds(10), StorageLevel.MEMORY_ONLY,\n+        awsCredentials.getAWSAccessKeyId, awsCredentials.getAWSSecretKey)\n+\n+      val collected = new mutable.HashSet[Int]\n+      stream.map { bytes => new String(bytes).toInt }.foreachRDD { rdd =>\n+        collected.synchronized {\n+          collected ++= rdd.collect()\n+          logInfo(\"Collected = \" + collected.mkString(\", \"))\n+        }\n+      }\n+      ssc.start()\n+\n+      val (testData1, testData2, testData3) = (1 to 10, 11 to 20, 21 to 30)\n+\n+      eventually(timeout(60 seconds), interval(10 second)) {\n+        localTestUtils.pushData(testData1, aggregateTestData)\n+        assert(collected.synchronized { collected === testData1.toSet },\n+          \"\\nData received does not match data sent\")\n+      }\n+\n+      val shardToSplit = localTestUtils.getShards().head\n+      localTestUtils.splitShard(shardToSplit.getShardId)\n+      val (splitOpenShards, splitCloseShards) = localTestUtils.getShards().partition { shard =>\n+        shard.getSequenceNumberRange.getEndingSequenceNumber == null\n+      }\n+\n+      // We should have one closed shard and two open shards\n+      assert(splitCloseShards.size == 1)\n+      assert(splitOpenShards.size == 2)\n+\n+      eventually(timeout(60 seconds), interval(10 second)) {\n+        localTestUtils.pushData(testData2, aggregateTestData)\n+        assert(collected.synchronized { collected === (testData1 ++ testData2).toSet },\n+          \"\\nData received does not match data sent after splitting a shard\")\n+      }\n+\n+      val (shardToMerge, adjShared) = splitOpenShards match { case Seq(e1, e2) => (e1, e2) }"
  }, {
    "author": {
      "login": "brkyvz"
    },
    "body": "probably you also meant `adjShard`. I would use `adjacentShard`. For a moment I was thinking `adjective`, shared adjective, what does that mean :P",
    "commit": "17a5c3a0b74414d2f65cfdf33d813599b1697804",
    "createdAt": "2016-12-09T19:34:52Z",
    "diffHunk": "@@ -225,6 +225,74 @@ abstract class KinesisStreamTests(aggregateTestData: Boolean) extends KinesisFun\n     ssc.stop(stopSparkContext = false)\n   }\n \n+  testIfEnabled(\"split and merge shards in a stream\") {\n+    // Since this test tries to split and merge shards in a stream, we create another\n+    // temporary stream and then remove it when finished.\n+    val localAppName = s\"KinesisStreamSuite-${math.abs(Random.nextLong())}\"\n+    val localTestUtils = new KPLBasedKinesisTestUtils(1)\n+    localTestUtils.createStream()\n+    try {\n+      val awsCredentials = KinesisTestUtils.getAWSCredentials()\n+      val stream = KinesisUtils.createStream(ssc, localAppName, localTestUtils.streamName,\n+        localTestUtils.endpointUrl, localTestUtils.regionName, InitialPositionInStream.LATEST,\n+        Seconds(10), StorageLevel.MEMORY_ONLY,\n+        awsCredentials.getAWSAccessKeyId, awsCredentials.getAWSSecretKey)\n+\n+      val collected = new mutable.HashSet[Int]\n+      stream.map { bytes => new String(bytes).toInt }.foreachRDD { rdd =>\n+        collected.synchronized {\n+          collected ++= rdd.collect()\n+          logInfo(\"Collected = \" + collected.mkString(\", \"))\n+        }\n+      }\n+      ssc.start()\n+\n+      val (testData1, testData2, testData3) = (1 to 10, 11 to 20, 21 to 30)\n+\n+      eventually(timeout(60 seconds), interval(10 second)) {\n+        localTestUtils.pushData(testData1, aggregateTestData)\n+        assert(collected.synchronized { collected === testData1.toSet },\n+          \"\\nData received does not match data sent\")\n+      }\n+\n+      val shardToSplit = localTestUtils.getShards().head\n+      localTestUtils.splitShard(shardToSplit.getShardId)\n+      val (splitOpenShards, splitCloseShards) = localTestUtils.getShards().partition { shard =>\n+        shard.getSequenceNumberRange.getEndingSequenceNumber == null\n+      }\n+\n+      // We should have one closed shard and two open shards\n+      assert(splitCloseShards.size == 1)\n+      assert(splitOpenShards.size == 2)\n+\n+      eventually(timeout(60 seconds), interval(10 second)) {\n+        localTestUtils.pushData(testData2, aggregateTestData)\n+        assert(collected.synchronized { collected === (testData1 ++ testData2).toSet },\n+          \"\\nData received does not match data sent after splitting a shard\")\n+      }\n+\n+      val (shardToMerge, adjShared) = splitOpenShards match { case Seq(e1, e2) => (e1, e2) }"
  }, {
    "author": {
      "login": "maropu"
    },
    "body": "Yea, I tried to use `adjacentShard`, but it returned `null`. I didn't why it behaved like this though, do you know that?",
    "commit": "17a5c3a0b74414d2f65cfdf33d813599b1697804",
    "createdAt": "2016-12-10T05:59:48Z",
    "diffHunk": "@@ -225,6 +225,74 @@ abstract class KinesisStreamTests(aggregateTestData: Boolean) extends KinesisFun\n     ssc.stop(stopSparkContext = false)\n   }\n \n+  testIfEnabled(\"split and merge shards in a stream\") {\n+    // Since this test tries to split and merge shards in a stream, we create another\n+    // temporary stream and then remove it when finished.\n+    val localAppName = s\"KinesisStreamSuite-${math.abs(Random.nextLong())}\"\n+    val localTestUtils = new KPLBasedKinesisTestUtils(1)\n+    localTestUtils.createStream()\n+    try {\n+      val awsCredentials = KinesisTestUtils.getAWSCredentials()\n+      val stream = KinesisUtils.createStream(ssc, localAppName, localTestUtils.streamName,\n+        localTestUtils.endpointUrl, localTestUtils.regionName, InitialPositionInStream.LATEST,\n+        Seconds(10), StorageLevel.MEMORY_ONLY,\n+        awsCredentials.getAWSAccessKeyId, awsCredentials.getAWSSecretKey)\n+\n+      val collected = new mutable.HashSet[Int]\n+      stream.map { bytes => new String(bytes).toInt }.foreachRDD { rdd =>\n+        collected.synchronized {\n+          collected ++= rdd.collect()\n+          logInfo(\"Collected = \" + collected.mkString(\", \"))\n+        }\n+      }\n+      ssc.start()\n+\n+      val (testData1, testData2, testData3) = (1 to 10, 11 to 20, 21 to 30)\n+\n+      eventually(timeout(60 seconds), interval(10 second)) {\n+        localTestUtils.pushData(testData1, aggregateTestData)\n+        assert(collected.synchronized { collected === testData1.toSet },\n+          \"\\nData received does not match data sent\")\n+      }\n+\n+      val shardToSplit = localTestUtils.getShards().head\n+      localTestUtils.splitShard(shardToSplit.getShardId)\n+      val (splitOpenShards, splitCloseShards) = localTestUtils.getShards().partition { shard =>\n+        shard.getSequenceNumberRange.getEndingSequenceNumber == null\n+      }\n+\n+      // We should have one closed shard and two open shards\n+      assert(splitCloseShards.size == 1)\n+      assert(splitOpenShards.size == 2)\n+\n+      eventually(timeout(60 seconds), interval(10 second)) {\n+        localTestUtils.pushData(testData2, aggregateTestData)\n+        assert(collected.synchronized { collected === (testData1 ++ testData2).toSet },\n+          \"\\nData received does not match data sent after splitting a shard\")\n+      }\n+\n+      val (shardToMerge, adjShared) = splitOpenShards match { case Seq(e1, e2) => (e1, e2) }"
  }],
  "prId": 16213
}, {
  "comments": [{
    "author": {
      "login": "brkyvz"
    },
    "body": "nit: instead of doing a tuple expansion, mind making these new lines",
    "commit": "17a5c3a0b74414d2f65cfdf33d813599b1697804",
    "createdAt": "2017-01-18T01:18:19Z",
    "diffHunk": "@@ -225,6 +225,74 @@ abstract class KinesisStreamTests(aggregateTestData: Boolean) extends KinesisFun\n     ssc.stop(stopSparkContext = false)\n   }\n \n+  testIfEnabled(\"split and merge shards in a stream\") {\n+    // Since this test tries to split and merge shards in a stream, we create another\n+    // temporary stream and then remove it when finished.\n+    val localAppName = s\"KinesisStreamSuite-${math.abs(Random.nextLong())}\"\n+    val localTestUtils = new KPLBasedKinesisTestUtils(1)\n+    localTestUtils.createStream()\n+    try {\n+      val awsCredentials = KinesisTestUtils.getAWSCredentials()\n+      val stream = KinesisUtils.createStream(ssc, localAppName, localTestUtils.streamName,\n+        localTestUtils.endpointUrl, localTestUtils.regionName, InitialPositionInStream.LATEST,\n+        Seconds(10), StorageLevel.MEMORY_ONLY,\n+        awsCredentials.getAWSAccessKeyId, awsCredentials.getAWSSecretKey)\n+\n+      val collected = new mutable.HashSet[Int]\n+      stream.map { bytes => new String(bytes).toInt }.foreachRDD { rdd =>\n+        collected.synchronized {\n+          collected ++= rdd.collect()\n+          logInfo(\"Collected = \" + collected.mkString(\", \"))\n+        }\n+      }\n+      ssc.start()\n+\n+      val (testData1, testData2, testData3) = (1 to 10, 11 to 20, 21 to 30)"
  }, {
    "author": {
      "login": "maropu"
    },
    "body": "okay",
    "commit": "17a5c3a0b74414d2f65cfdf33d813599b1697804",
    "createdAt": "2017-01-18T02:25:37Z",
    "diffHunk": "@@ -225,6 +225,74 @@ abstract class KinesisStreamTests(aggregateTestData: Boolean) extends KinesisFun\n     ssc.stop(stopSparkContext = false)\n   }\n \n+  testIfEnabled(\"split and merge shards in a stream\") {\n+    // Since this test tries to split and merge shards in a stream, we create another\n+    // temporary stream and then remove it when finished.\n+    val localAppName = s\"KinesisStreamSuite-${math.abs(Random.nextLong())}\"\n+    val localTestUtils = new KPLBasedKinesisTestUtils(1)\n+    localTestUtils.createStream()\n+    try {\n+      val awsCredentials = KinesisTestUtils.getAWSCredentials()\n+      val stream = KinesisUtils.createStream(ssc, localAppName, localTestUtils.streamName,\n+        localTestUtils.endpointUrl, localTestUtils.regionName, InitialPositionInStream.LATEST,\n+        Seconds(10), StorageLevel.MEMORY_ONLY,\n+        awsCredentials.getAWSAccessKeyId, awsCredentials.getAWSSecretKey)\n+\n+      val collected = new mutable.HashSet[Int]\n+      stream.map { bytes => new String(bytes).toInt }.foreachRDD { rdd =>\n+        collected.synchronized {\n+          collected ++= rdd.collect()\n+          logInfo(\"Collected = \" + collected.mkString(\", \"))\n+        }\n+      }\n+      ssc.start()\n+\n+      val (testData1, testData2, testData3) = (1 to 10, 11 to 20, 21 to 30)"
  }],
  "prId": 16213
}, {
  "comments": [{
    "author": {
      "login": "brkyvz"
    },
    "body": "nit: `adjShared` -> `adjShard`?",
    "commit": "17a5c3a0b74414d2f65cfdf33d813599b1697804",
    "createdAt": "2017-01-18T01:19:25Z",
    "diffHunk": "@@ -225,6 +225,74 @@ abstract class KinesisStreamTests(aggregateTestData: Boolean) extends KinesisFun\n     ssc.stop(stopSparkContext = false)\n   }\n \n+  testIfEnabled(\"split and merge shards in a stream\") {\n+    // Since this test tries to split and merge shards in a stream, we create another\n+    // temporary stream and then remove it when finished.\n+    val localAppName = s\"KinesisStreamSuite-${math.abs(Random.nextLong())}\"\n+    val localTestUtils = new KPLBasedKinesisTestUtils(1)\n+    localTestUtils.createStream()\n+    try {\n+      val awsCredentials = KinesisTestUtils.getAWSCredentials()\n+      val stream = KinesisUtils.createStream(ssc, localAppName, localTestUtils.streamName,\n+        localTestUtils.endpointUrl, localTestUtils.regionName, InitialPositionInStream.LATEST,\n+        Seconds(10), StorageLevel.MEMORY_ONLY,\n+        awsCredentials.getAWSAccessKeyId, awsCredentials.getAWSSecretKey)\n+\n+      val collected = new mutable.HashSet[Int]\n+      stream.map { bytes => new String(bytes).toInt }.foreachRDD { rdd =>\n+        collected.synchronized {\n+          collected ++= rdd.collect()\n+          logInfo(\"Collected = \" + collected.mkString(\", \"))\n+        }\n+      }\n+      ssc.start()\n+\n+      val (testData1, testData2, testData3) = (1 to 10, 11 to 20, 21 to 30)\n+\n+      eventually(timeout(60 seconds), interval(10 second)) {\n+        localTestUtils.pushData(testData1, aggregateTestData)\n+        assert(collected.synchronized { collected === testData1.toSet },\n+          \"\\nData received does not match data sent\")\n+      }\n+\n+      val shardToSplit = localTestUtils.getShards().head\n+      localTestUtils.splitShard(shardToSplit.getShardId)\n+      val (splitOpenShards, splitCloseShards) = localTestUtils.getShards().partition { shard =>\n+        shard.getSequenceNumberRange.getEndingSequenceNumber == null\n+      }\n+\n+      // We should have one closed shard and two open shards\n+      assert(splitCloseShards.size == 1)\n+      assert(splitOpenShards.size == 2)\n+\n+      eventually(timeout(60 seconds), interval(10 second)) {\n+        localTestUtils.pushData(testData2, aggregateTestData)\n+        assert(collected.synchronized { collected === (testData1 ++ testData2).toSet },\n+          \"\\nData received does not match data sent after splitting a shard\")\n+      }\n+\n+      val Seq(shardToMerge, adjShared) = splitOpenShards"
  }],
  "prId": 16213
}]