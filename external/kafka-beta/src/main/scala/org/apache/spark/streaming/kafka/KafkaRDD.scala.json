[{
  "comments": [{
    "author": {
      "login": "markgrover"
    },
    "body": "Unrelated to your change but the Scaladoc is pretty out of date.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-04-15T23:48:30Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "I figure we can fix up scaladoc once the general approach has some agreement.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-04-18T16:22:28Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD"
  }, {
    "author": {
      "login": "markgrover"
    },
    "body": "Ok.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-04-19T23:09:37Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD"
  }],
  "prId": 11863
}, {
  "comments": [{
    "author": {
      "login": "markgrover"
    },
    "body": "Did you, perhaps, mean to use ENABLE_AUTO_COMMIT_CONFIG here?\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-04-15T23:50:26Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD\n+ */\n+\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag] private[spark] (\n+    sc: SparkContext,\n+    val kafkaParams: ju.Map[String, Object],\n+    val offsetRanges: Array[OffsetRange],\n+    val preferredHosts: ju.Map[TopicPartition, String]\n+) extends RDD[ConsumerRecord[K, V]](sc, Nil) with Logging with HasOffsetRanges {\n+\n+  assert(\"none\" ==\n+    kafkaParams.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).asInstanceOf[String],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to none for executor kafka params, else messages may not match offsetRange\")\n+\n+  assert(false ==\n+    kafkaParams.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG).asInstanceOf[Boolean],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +"
  }, {
    "author": {
      "login": "markgrover"
    },
    "body": "Also, if these properties are not defined, we'd NPEs, no? I understand the object is the right way to create this RDD, but do you think it would make sense to add checks to assert these properties exist?\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-04-15T23:52:45Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD\n+ */\n+\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag] private[spark] (\n+    sc: SparkContext,\n+    val kafkaParams: ju.Map[String, Object],\n+    val offsetRanges: Array[OffsetRange],\n+    val preferredHosts: ju.Map[TopicPartition, String]\n+) extends RDD[ConsumerRecord[K, V]](sc, Nil) with Logging with HasOffsetRanges {\n+\n+  assert(\"none\" ==\n+    kafkaParams.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).asInstanceOf[String],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to none for executor kafka params, else messages may not match offsetRange\")\n+\n+  assert(false ==\n+    kafkaParams.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG).asInstanceOf[Boolean],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +"
  }],
  "prId": 11863
}, {
  "comments": [{
    "author": {
      "login": "markgrover"
    },
    "body": "This is not a suggestion, just a note about something I noticed. We are not namespacing property names to differentiate between the new and old consumer API implementations. For example, a user can't tell just by looking at the property name 'spark.streaming.kafka.consumer.poll.ms' if it applies to the old implementation or the new implementation. And, may be that's ok and even desirable if there are a good chunk of spark.streaming.kafka.\\* properties that'd be shared between the two implementations. But, if not, it may not be a bad idea to create a separate namespace for these properties.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-04-15T23:55:43Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD\n+ */\n+\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag] private[spark] (\n+    sc: SparkContext,\n+    val kafkaParams: ju.Map[String, Object],\n+    val offsetRanges: Array[OffsetRange],\n+    val preferredHosts: ju.Map[TopicPartition, String]\n+) extends RDD[ConsumerRecord[K, V]](sc, Nil) with Logging with HasOffsetRanges {\n+\n+  assert(\"none\" ==\n+    kafkaParams.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).asInstanceOf[String],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to none for executor kafka params, else messages may not match offsetRange\")\n+\n+  assert(false ==\n+    kafkaParams.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG).asInstanceOf[Boolean],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to false for executor kafka params, else offsets may commit before processing\")\n+\n+  // TODO is it necessary to have separate configs for initial poll time vs ongoing poll time?\n+  private val pollTimeout = conf.getLong(\"spark.streaming.kafka.consumer.poll.ms\", 256)"
  }],
  "prId": 11863
}, {
  "comments": [{
    "author": {
      "login": "markgrover"
    },
    "body": "Nit: you could potentially take out the condition on num < 1 and put it before the nonEmptyPartitions statement.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-04-16T00:00:25Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD\n+ */\n+\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag] private[spark] (\n+    sc: SparkContext,\n+    val kafkaParams: ju.Map[String, Object],\n+    val offsetRanges: Array[OffsetRange],\n+    val preferredHosts: ju.Map[TopicPartition, String]\n+) extends RDD[ConsumerRecord[K, V]](sc, Nil) with Logging with HasOffsetRanges {\n+\n+  assert(\"none\" ==\n+    kafkaParams.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).asInstanceOf[String],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to none for executor kafka params, else messages may not match offsetRange\")\n+\n+  assert(false ==\n+    kafkaParams.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG).asInstanceOf[Boolean],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to false for executor kafka params, else offsets may commit before processing\")\n+\n+  // TODO is it necessary to have separate configs for initial poll time vs ongoing poll time?\n+  private val pollTimeout = conf.getLong(\"spark.streaming.kafka.consumer.poll.ms\", 256)\n+  private val cacheInitialCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.initialCapacity\", 16)\n+  private val cacheMaxCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.maxCapacity\", 64)\n+  private val cacheLoadFactor =\n+    conf.getDouble(\"spark.streaming.kafka.consumer.cache.loadFactor\", 0.75).toFloat\n+\n+  override def persist(newLevel: StorageLevel): this.type = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  override def getPartitions: Array[Partition] = {\n+    offsetRanges.zipWithIndex.map { case (o, i) =>\n+        new KafkaRDDPartition(i, o.topic, o.partition, o.fromOffset, o.untilOffset)\n+    }.toArray\n+  }\n+\n+  override def count(): Long = offsetRanges.map(_.count).sum\n+\n+  override def countApprox(\n+      timeout: Long,\n+      confidence: Double = 0.95\n+  ): PartialResult[BoundedDouble] = {\n+    val c = count\n+    new PartialResult(new BoundedDouble(c, 1.0, c, c), true)\n+  }\n+\n+  override def isEmpty(): Boolean = count == 0L\n+\n+  override def take(num: Int): Array[ConsumerRecord[K, V]] = {\n+    val nonEmptyPartitions = this.partitions\n+      .map(_.asInstanceOf[KafkaRDDPartition])\n+      .filter(_.count > 0)\n+\n+    if (num < 1 || nonEmptyPartitions.size < 1) {"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "I don't really want to make style changes to code that's identical to the existing working consumer.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-04-18T16:24:58Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD\n+ */\n+\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag] private[spark] (\n+    sc: SparkContext,\n+    val kafkaParams: ju.Map[String, Object],\n+    val offsetRanges: Array[OffsetRange],\n+    val preferredHosts: ju.Map[TopicPartition, String]\n+) extends RDD[ConsumerRecord[K, V]](sc, Nil) with Logging with HasOffsetRanges {\n+\n+  assert(\"none\" ==\n+    kafkaParams.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).asInstanceOf[String],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to none for executor kafka params, else messages may not match offsetRange\")\n+\n+  assert(false ==\n+    kafkaParams.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG).asInstanceOf[Boolean],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to false for executor kafka params, else offsets may commit before processing\")\n+\n+  // TODO is it necessary to have separate configs for initial poll time vs ongoing poll time?\n+  private val pollTimeout = conf.getLong(\"spark.streaming.kafka.consumer.poll.ms\", 256)\n+  private val cacheInitialCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.initialCapacity\", 16)\n+  private val cacheMaxCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.maxCapacity\", 64)\n+  private val cacheLoadFactor =\n+    conf.getDouble(\"spark.streaming.kafka.consumer.cache.loadFactor\", 0.75).toFloat\n+\n+  override def persist(newLevel: StorageLevel): this.type = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  override def getPartitions: Array[Partition] = {\n+    offsetRanges.zipWithIndex.map { case (o, i) =>\n+        new KafkaRDDPartition(i, o.topic, o.partition, o.fromOffset, o.untilOffset)\n+    }.toArray\n+  }\n+\n+  override def count(): Long = offsetRanges.map(_.count).sum\n+\n+  override def countApprox(\n+      timeout: Long,\n+      confidence: Double = 0.95\n+  ): PartialResult[BoundedDouble] = {\n+    val c = count\n+    new PartialResult(new BoundedDouble(c, 1.0, c, c), true)\n+  }\n+\n+  override def isEmpty(): Boolean = count == 0L\n+\n+  override def take(num: Int): Array[ConsumerRecord[K, V]] = {\n+    val nonEmptyPartitions = this.partitions\n+      .map(_.asInstanceOf[KafkaRDDPartition])\n+      .filter(_.count > 0)\n+\n+    if (num < 1 || nonEmptyPartitions.size < 1) {"
  }],
  "prId": 11863
}, {
  "comments": [{
    "author": {
      "login": "markgrover"
    },
    "body": "Nit: moving the last argument to the next line would make it easier to read.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-04-16T00:00:55Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD\n+ */\n+\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag] private[spark] (\n+    sc: SparkContext,\n+    val kafkaParams: ju.Map[String, Object],\n+    val offsetRanges: Array[OffsetRange],\n+    val preferredHosts: ju.Map[TopicPartition, String]\n+) extends RDD[ConsumerRecord[K, V]](sc, Nil) with Logging with HasOffsetRanges {\n+\n+  assert(\"none\" ==\n+    kafkaParams.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).asInstanceOf[String],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to none for executor kafka params, else messages may not match offsetRange\")\n+\n+  assert(false ==\n+    kafkaParams.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG).asInstanceOf[Boolean],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to false for executor kafka params, else offsets may commit before processing\")\n+\n+  // TODO is it necessary to have separate configs for initial poll time vs ongoing poll time?\n+  private val pollTimeout = conf.getLong(\"spark.streaming.kafka.consumer.poll.ms\", 256)\n+  private val cacheInitialCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.initialCapacity\", 16)\n+  private val cacheMaxCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.maxCapacity\", 64)\n+  private val cacheLoadFactor =\n+    conf.getDouble(\"spark.streaming.kafka.consumer.cache.loadFactor\", 0.75).toFloat\n+\n+  override def persist(newLevel: StorageLevel): this.type = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  override def getPartitions: Array[Partition] = {\n+    offsetRanges.zipWithIndex.map { case (o, i) =>\n+        new KafkaRDDPartition(i, o.topic, o.partition, o.fromOffset, o.untilOffset)\n+    }.toArray\n+  }\n+\n+  override def count(): Long = offsetRanges.map(_.count).sum\n+\n+  override def countApprox(\n+      timeout: Long,\n+      confidence: Double = 0.95\n+  ): PartialResult[BoundedDouble] = {\n+    val c = count\n+    new PartialResult(new BoundedDouble(c, 1.0, c, c), true)\n+  }\n+\n+  override def isEmpty(): Boolean = count == 0L\n+\n+  override def take(num: Int): Array[ConsumerRecord[K, V]] = {\n+    val nonEmptyPartitions = this.partitions\n+      .map(_.asInstanceOf[KafkaRDDPartition])\n+      .filter(_.count > 0)\n+\n+    if (num < 1 || nonEmptyPartitions.size < 1) {\n+      return new Array[ConsumerRecord[K, V]](0)\n+    }\n+\n+    // Determine in advance how many messages need to be taken from each partition\n+    val parts = nonEmptyPartitions.foldLeft(Map[Int, Int]()) { (result, part) =>\n+      val remain = num - result.values.sum\n+      if (remain > 0) {\n+        val taken = Math.min(remain, part.count)\n+        result + (part.index -> taken.toInt)\n+      } else {\n+        result\n+      }\n+    }\n+\n+    val buf = new ArrayBuffer[ConsumerRecord[K, V]]\n+    val res = context.runJob(\n+      this,\n+      (tc: TaskContext, it: Iterator[ConsumerRecord[K, V]]) =>\n+      it.take(parts(tc.partitionId)).toArray, parts.keys.toArray"
  }],
  "prId": 11863
}, {
  "comments": [{
    "author": {
      "login": "markgrover"
    },
    "body": "Would it make sense to have a default value of an empty Map here in case the user doesn't care about preference?\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-04-16T00:03:09Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD\n+ */\n+\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag] private[spark] (\n+    sc: SparkContext,\n+    val kafkaParams: ju.Map[String, Object],\n+    val offsetRanges: Array[OffsetRange],\n+    val preferredHosts: ju.Map[TopicPartition, String]\n+) extends RDD[ConsumerRecord[K, V]](sc, Nil) with Logging with HasOffsetRanges {\n+\n+  assert(\"none\" ==\n+    kafkaParams.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).asInstanceOf[String],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to none for executor kafka params, else messages may not match offsetRange\")\n+\n+  assert(false ==\n+    kafkaParams.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG).asInstanceOf[Boolean],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to false for executor kafka params, else offsets may commit before processing\")\n+\n+  // TODO is it necessary to have separate configs for initial poll time vs ongoing poll time?\n+  private val pollTimeout = conf.getLong(\"spark.streaming.kafka.consumer.poll.ms\", 256)\n+  private val cacheInitialCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.initialCapacity\", 16)\n+  private val cacheMaxCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.maxCapacity\", 64)\n+  private val cacheLoadFactor =\n+    conf.getDouble(\"spark.streaming.kafka.consumer.cache.loadFactor\", 0.75).toFloat\n+\n+  override def persist(newLevel: StorageLevel): this.type = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  override def getPartitions: Array[Partition] = {\n+    offsetRanges.zipWithIndex.map { case (o, i) =>\n+        new KafkaRDDPartition(i, o.topic, o.partition, o.fromOffset, o.untilOffset)\n+    }.toArray\n+  }\n+\n+  override def count(): Long = offsetRanges.map(_.count).sum\n+\n+  override def countApprox(\n+      timeout: Long,\n+      confidence: Double = 0.95\n+  ): PartialResult[BoundedDouble] = {\n+    val c = count\n+    new PartialResult(new BoundedDouble(c, 1.0, c, c), true)\n+  }\n+\n+  override def isEmpty(): Boolean = count == 0L\n+\n+  override def take(num: Int): Array[ConsumerRecord[K, V]] = {\n+    val nonEmptyPartitions = this.partitions\n+      .map(_.asInstanceOf[KafkaRDDPartition])\n+      .filter(_.count > 0)\n+\n+    if (num < 1 || nonEmptyPartitions.size < 1) {\n+      return new Array[ConsumerRecord[K, V]](0)\n+    }\n+\n+    // Determine in advance how many messages need to be taken from each partition\n+    val parts = nonEmptyPartitions.foldLeft(Map[Int, Int]()) { (result, part) =>\n+      val remain = num - result.values.sum\n+      if (remain > 0) {\n+        val taken = Math.min(remain, part.count)\n+        result + (part.index -> taken.toInt)\n+      } else {\n+        result\n+      }\n+    }\n+\n+    val buf = new ArrayBuffer[ConsumerRecord[K, V]]\n+    val res = context.runJob(\n+      this,\n+      (tc: TaskContext, it: Iterator[ConsumerRecord[K, V]]) =>\n+      it.take(parts(tc.partitionId)).toArray, parts.keys.toArray\n+    )\n+    res.foreach(buf ++= _)\n+    buf.toArray\n+  }\n+\n+  private def executors(): Array[ExecutorCacheTaskLocation] = {\n+    val bm = sparkContext.env.blockManager\n+    bm.master.getPeers(bm.blockManagerId).toArray\n+      .map(x => ExecutorCacheTaskLocation(x.host, x.executorId))\n+      .sortWith((a, b) => a.host > b.host || a.executorId > b.executorId)\n+  }\n+\n+  // non-negative modulus, from java 8 math\n+  private def floorMod(a: Int, b: Int): Int = ((a % b) + b) % b\n+\n+  override def getPreferredLocations(thePart: Partition): Seq[String] = {\n+    // TODO what about hosts specified by ip vs name\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    val allExecs = executors()\n+    val tp = part.topicPartition\n+    val prefHost = preferredHosts.get(tp)\n+    val prefExecs = if (null == prefHost) allExecs else allExecs.filter(_.host == prefHost)\n+    val execs = if (prefExecs.isEmpty) allExecs else prefExecs\n+    if (execs.isEmpty) {\n+      Seq()\n+    } else {\n+      val index = this.floorMod(tp.hashCode, execs.length)\n+      val chosen = execs(index)\n+\n+      Seq(chosen.toString)\n+    }\n+  }\n+\n+  private def errBeginAfterEnd(part: KafkaRDDPartition): String =\n+    s\"Beginning offset ${part.fromOffset} is after the ending offset ${part.untilOffset} \" +\n+      s\"for topic ${part.topic} partition ${part.partition}. \" +\n+      \"You either provided an invalid fromOffset, or the Kafka topic has been damaged\"\n+\n+  override def compute(thePart: Partition, context: TaskContext): Iterator[ConsumerRecord[K, V]] = {\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    assert(part.fromOffset <= part.untilOffset, errBeginAfterEnd(part))\n+    if (part.fromOffset == part.untilOffset) {\n+      log.info(s\"Beginning offset ${part.fromOffset} is the same as ending offset \" +\n+        s\"skipping ${part.topic} ${part.partition}\")\n+      Iterator.empty\n+    } else {\n+      new KafkaRDDIterator(part, context)\n+    }\n+  }\n+\n+  private class KafkaRDDIterator(\n+      part: KafkaRDDPartition,\n+      context: TaskContext) extends Iterator[ConsumerRecord[K, V]] {\n+\n+    log.info(s\"Computing topic ${part.topic}, partition ${part.partition} \" +\n+      s\"offsets ${part.fromOffset} -> ${part.untilOffset}\")\n+\n+    val groupId = kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG).asInstanceOf[String]\n+\n+    val consumer = {\n+      CachedKafkaConsumer.init(cacheInitialCapacity, cacheMaxCapacity, cacheLoadFactor)\n+      if (context.attemptNumber > 1) {\n+        // just in case the prior attempt failures were cache related\n+        CachedKafkaConsumer.remove(groupId, part.topic, part.partition)\n+      }\n+      CachedKafkaConsumer.get[K, V](groupId, part.topic, part.partition, kafkaParams)\n+    }\n+\n+    var requestOffset = part.fromOffset\n+\n+    override def hasNext(): Boolean = requestOffset < part.untilOffset\n+\n+    override def next(): ConsumerRecord[K, V] = {\n+      assert(hasNext(), \"Can't call getNext() once untilOffset has been reached\")\n+      val r = consumer.get(requestOffset, pollTimeout)\n+      requestOffset += 1\n+      r\n+    }\n+  }\n+}\n+\n+object KafkaRDD extends Logging {\n+  import org.apache.spark.api.java.{ JavaRDD, JavaSparkContext }\n+\n+  private[kafka] def fixKafkaParams(kafkaParams: ju.HashMap[String, Object]): Unit = {\n+    log.warn(s\"overriding ${ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG} to false for executor\")\n+    kafkaParams.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false: java.lang.Boolean)\n+\n+    log.warn(s\"overriding ${ConsumerConfig.AUTO_OFFSET_RESET_CONFIG} to none for executor\")\n+    kafkaParams.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"none\")\n+\n+    // driver and executor should be in different consumer groups\n+    val groupId = \"spark-executor-\" + kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG)\n+    log.warn(s\"overriding executor ${ConsumerConfig.GROUP_ID_CONFIG} to ${groupId}\")\n+    kafkaParams.put(ConsumerConfig.GROUP_ID_CONFIG, groupId)\n+\n+    // possible workaround for KAFKA-3135\n+    val rbb = kafkaParams.get(ConsumerConfig.RECEIVE_BUFFER_CONFIG)\n+    if (null == rbb || rbb.asInstanceOf[java.lang.Integer] < 65536) {\n+      log.warn(s\"overriding ${ConsumerConfig.RECEIVE_BUFFER_CONFIG} to 65536 see KAFKA-3135\")\n+      kafkaParams.put(ConsumerConfig.RECEIVE_BUFFER_CONFIG, 65536: java.lang.Integer)\n+    }\n+  }\n+\n+  def apply[K: ClassTag, V: ClassTag](\n+      sc: SparkContext,\n+      kafkaParams: ju.Map[String, Object],\n+      offsetRanges: Array[OffsetRange],\n+      preferredHosts: ju.Map[TopicPartition, String]"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "The companion object has a couple of named maps to make communicating preferences clearer.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-04-18T16:26:08Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD\n+ */\n+\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag] private[spark] (\n+    sc: SparkContext,\n+    val kafkaParams: ju.Map[String, Object],\n+    val offsetRanges: Array[OffsetRange],\n+    val preferredHosts: ju.Map[TopicPartition, String]\n+) extends RDD[ConsumerRecord[K, V]](sc, Nil) with Logging with HasOffsetRanges {\n+\n+  assert(\"none\" ==\n+    kafkaParams.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).asInstanceOf[String],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to none for executor kafka params, else messages may not match offsetRange\")\n+\n+  assert(false ==\n+    kafkaParams.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG).asInstanceOf[Boolean],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to false for executor kafka params, else offsets may commit before processing\")\n+\n+  // TODO is it necessary to have separate configs for initial poll time vs ongoing poll time?\n+  private val pollTimeout = conf.getLong(\"spark.streaming.kafka.consumer.poll.ms\", 256)\n+  private val cacheInitialCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.initialCapacity\", 16)\n+  private val cacheMaxCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.maxCapacity\", 64)\n+  private val cacheLoadFactor =\n+    conf.getDouble(\"spark.streaming.kafka.consumer.cache.loadFactor\", 0.75).toFloat\n+\n+  override def persist(newLevel: StorageLevel): this.type = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  override def getPartitions: Array[Partition] = {\n+    offsetRanges.zipWithIndex.map { case (o, i) =>\n+        new KafkaRDDPartition(i, o.topic, o.partition, o.fromOffset, o.untilOffset)\n+    }.toArray\n+  }\n+\n+  override def count(): Long = offsetRanges.map(_.count).sum\n+\n+  override def countApprox(\n+      timeout: Long,\n+      confidence: Double = 0.95\n+  ): PartialResult[BoundedDouble] = {\n+    val c = count\n+    new PartialResult(new BoundedDouble(c, 1.0, c, c), true)\n+  }\n+\n+  override def isEmpty(): Boolean = count == 0L\n+\n+  override def take(num: Int): Array[ConsumerRecord[K, V]] = {\n+    val nonEmptyPartitions = this.partitions\n+      .map(_.asInstanceOf[KafkaRDDPartition])\n+      .filter(_.count > 0)\n+\n+    if (num < 1 || nonEmptyPartitions.size < 1) {\n+      return new Array[ConsumerRecord[K, V]](0)\n+    }\n+\n+    // Determine in advance how many messages need to be taken from each partition\n+    val parts = nonEmptyPartitions.foldLeft(Map[Int, Int]()) { (result, part) =>\n+      val remain = num - result.values.sum\n+      if (remain > 0) {\n+        val taken = Math.min(remain, part.count)\n+        result + (part.index -> taken.toInt)\n+      } else {\n+        result\n+      }\n+    }\n+\n+    val buf = new ArrayBuffer[ConsumerRecord[K, V]]\n+    val res = context.runJob(\n+      this,\n+      (tc: TaskContext, it: Iterator[ConsumerRecord[K, V]]) =>\n+      it.take(parts(tc.partitionId)).toArray, parts.keys.toArray\n+    )\n+    res.foreach(buf ++= _)\n+    buf.toArray\n+  }\n+\n+  private def executors(): Array[ExecutorCacheTaskLocation] = {\n+    val bm = sparkContext.env.blockManager\n+    bm.master.getPeers(bm.blockManagerId).toArray\n+      .map(x => ExecutorCacheTaskLocation(x.host, x.executorId))\n+      .sortWith((a, b) => a.host > b.host || a.executorId > b.executorId)\n+  }\n+\n+  // non-negative modulus, from java 8 math\n+  private def floorMod(a: Int, b: Int): Int = ((a % b) + b) % b\n+\n+  override def getPreferredLocations(thePart: Partition): Seq[String] = {\n+    // TODO what about hosts specified by ip vs name\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    val allExecs = executors()\n+    val tp = part.topicPartition\n+    val prefHost = preferredHosts.get(tp)\n+    val prefExecs = if (null == prefHost) allExecs else allExecs.filter(_.host == prefHost)\n+    val execs = if (prefExecs.isEmpty) allExecs else prefExecs\n+    if (execs.isEmpty) {\n+      Seq()\n+    } else {\n+      val index = this.floorMod(tp.hashCode, execs.length)\n+      val chosen = execs(index)\n+\n+      Seq(chosen.toString)\n+    }\n+  }\n+\n+  private def errBeginAfterEnd(part: KafkaRDDPartition): String =\n+    s\"Beginning offset ${part.fromOffset} is after the ending offset ${part.untilOffset} \" +\n+      s\"for topic ${part.topic} partition ${part.partition}. \" +\n+      \"You either provided an invalid fromOffset, or the Kafka topic has been damaged\"\n+\n+  override def compute(thePart: Partition, context: TaskContext): Iterator[ConsumerRecord[K, V]] = {\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    assert(part.fromOffset <= part.untilOffset, errBeginAfterEnd(part))\n+    if (part.fromOffset == part.untilOffset) {\n+      log.info(s\"Beginning offset ${part.fromOffset} is the same as ending offset \" +\n+        s\"skipping ${part.topic} ${part.partition}\")\n+      Iterator.empty\n+    } else {\n+      new KafkaRDDIterator(part, context)\n+    }\n+  }\n+\n+  private class KafkaRDDIterator(\n+      part: KafkaRDDPartition,\n+      context: TaskContext) extends Iterator[ConsumerRecord[K, V]] {\n+\n+    log.info(s\"Computing topic ${part.topic}, partition ${part.partition} \" +\n+      s\"offsets ${part.fromOffset} -> ${part.untilOffset}\")\n+\n+    val groupId = kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG).asInstanceOf[String]\n+\n+    val consumer = {\n+      CachedKafkaConsumer.init(cacheInitialCapacity, cacheMaxCapacity, cacheLoadFactor)\n+      if (context.attemptNumber > 1) {\n+        // just in case the prior attempt failures were cache related\n+        CachedKafkaConsumer.remove(groupId, part.topic, part.partition)\n+      }\n+      CachedKafkaConsumer.get[K, V](groupId, part.topic, part.partition, kafkaParams)\n+    }\n+\n+    var requestOffset = part.fromOffset\n+\n+    override def hasNext(): Boolean = requestOffset < part.untilOffset\n+\n+    override def next(): ConsumerRecord[K, V] = {\n+      assert(hasNext(), \"Can't call getNext() once untilOffset has been reached\")\n+      val r = consumer.get(requestOffset, pollTimeout)\n+      requestOffset += 1\n+      r\n+    }\n+  }\n+}\n+\n+object KafkaRDD extends Logging {\n+  import org.apache.spark.api.java.{ JavaRDD, JavaSparkContext }\n+\n+  private[kafka] def fixKafkaParams(kafkaParams: ju.HashMap[String, Object]): Unit = {\n+    log.warn(s\"overriding ${ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG} to false for executor\")\n+    kafkaParams.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false: java.lang.Boolean)\n+\n+    log.warn(s\"overriding ${ConsumerConfig.AUTO_OFFSET_RESET_CONFIG} to none for executor\")\n+    kafkaParams.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"none\")\n+\n+    // driver and executor should be in different consumer groups\n+    val groupId = \"spark-executor-\" + kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG)\n+    log.warn(s\"overriding executor ${ConsumerConfig.GROUP_ID_CONFIG} to ${groupId}\")\n+    kafkaParams.put(ConsumerConfig.GROUP_ID_CONFIG, groupId)\n+\n+    // possible workaround for KAFKA-3135\n+    val rbb = kafkaParams.get(ConsumerConfig.RECEIVE_BUFFER_CONFIG)\n+    if (null == rbb || rbb.asInstanceOf[java.lang.Integer] < 65536) {\n+      log.warn(s\"overriding ${ConsumerConfig.RECEIVE_BUFFER_CONFIG} to 65536 see KAFKA-3135\")\n+      kafkaParams.put(ConsumerConfig.RECEIVE_BUFFER_CONFIG, 65536: java.lang.Integer)\n+    }\n+  }\n+\n+  def apply[K: ClassTag, V: ClassTag](\n+      sc: SparkContext,\n+      kafkaParams: ju.Map[String, Object],\n+      offsetRanges: Array[OffsetRange],\n+      preferredHosts: ju.Map[TopicPartition, String]"
  }],
  "prId": 11863
}, {
  "comments": [{
    "author": {
      "login": "markgrover"
    },
    "body": "Out of curiosity, why not store the preferred host info in the partition as well?\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-04-16T00:03:51Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD\n+ */\n+\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag] private[spark] (\n+    sc: SparkContext,\n+    val kafkaParams: ju.Map[String, Object],\n+    val offsetRanges: Array[OffsetRange],\n+    val preferredHosts: ju.Map[TopicPartition, String]\n+) extends RDD[ConsumerRecord[K, V]](sc, Nil) with Logging with HasOffsetRanges {\n+\n+  assert(\"none\" ==\n+    kafkaParams.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).asInstanceOf[String],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to none for executor kafka params, else messages may not match offsetRange\")\n+\n+  assert(false ==\n+    kafkaParams.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG).asInstanceOf[Boolean],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to false for executor kafka params, else offsets may commit before processing\")\n+\n+  // TODO is it necessary to have separate configs for initial poll time vs ongoing poll time?\n+  private val pollTimeout = conf.getLong(\"spark.streaming.kafka.consumer.poll.ms\", 256)\n+  private val cacheInitialCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.initialCapacity\", 16)\n+  private val cacheMaxCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.maxCapacity\", 64)\n+  private val cacheLoadFactor =\n+    conf.getDouble(\"spark.streaming.kafka.consumer.cache.loadFactor\", 0.75).toFloat\n+\n+  override def persist(newLevel: StorageLevel): this.type = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  override def getPartitions: Array[Partition] = {\n+    offsetRanges.zipWithIndex.map { case (o, i) =>\n+        new KafkaRDDPartition(i, o.topic, o.partition, o.fromOffset, o.untilOffset)"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "Because the preferred host might change\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-04-18T16:26:35Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD\n+ */\n+\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag] private[spark] (\n+    sc: SparkContext,\n+    val kafkaParams: ju.Map[String, Object],\n+    val offsetRanges: Array[OffsetRange],\n+    val preferredHosts: ju.Map[TopicPartition, String]\n+) extends RDD[ConsumerRecord[K, V]](sc, Nil) with Logging with HasOffsetRanges {\n+\n+  assert(\"none\" ==\n+    kafkaParams.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).asInstanceOf[String],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to none for executor kafka params, else messages may not match offsetRange\")\n+\n+  assert(false ==\n+    kafkaParams.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG).asInstanceOf[Boolean],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to false for executor kafka params, else offsets may commit before processing\")\n+\n+  // TODO is it necessary to have separate configs for initial poll time vs ongoing poll time?\n+  private val pollTimeout = conf.getLong(\"spark.streaming.kafka.consumer.poll.ms\", 256)\n+  private val cacheInitialCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.initialCapacity\", 16)\n+  private val cacheMaxCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.maxCapacity\", 64)\n+  private val cacheLoadFactor =\n+    conf.getDouble(\"spark.streaming.kafka.consumer.cache.loadFactor\", 0.75).toFloat\n+\n+  override def persist(newLevel: StorageLevel): this.type = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  override def getPartitions: Array[Partition] = {\n+    offsetRanges.zipWithIndex.map { case (o, i) =>\n+        new KafkaRDDPartition(i, o.topic, o.partition, o.fromOffset, o.untilOffset)"
  }],
  "prId": 11863
}, {
  "comments": [{
    "author": {
      "login": "markgrover"
    },
    "body": "Nit: return Array of hosts since that's all we seem to care about.\nAlso, do we need a special case for local here, like is done in ReceiverTracker.scala?\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-04-16T00:04:46Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD\n+ */\n+\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag] private[spark] (\n+    sc: SparkContext,\n+    val kafkaParams: ju.Map[String, Object],\n+    val offsetRanges: Array[OffsetRange],\n+    val preferredHosts: ju.Map[TopicPartition, String]\n+) extends RDD[ConsumerRecord[K, V]](sc, Nil) with Logging with HasOffsetRanges {\n+\n+  assert(\"none\" ==\n+    kafkaParams.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).asInstanceOf[String],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to none for executor kafka params, else messages may not match offsetRange\")\n+\n+  assert(false ==\n+    kafkaParams.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG).asInstanceOf[Boolean],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to false for executor kafka params, else offsets may commit before processing\")\n+\n+  // TODO is it necessary to have separate configs for initial poll time vs ongoing poll time?\n+  private val pollTimeout = conf.getLong(\"spark.streaming.kafka.consumer.poll.ms\", 256)\n+  private val cacheInitialCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.initialCapacity\", 16)\n+  private val cacheMaxCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.maxCapacity\", 64)\n+  private val cacheLoadFactor =\n+    conf.getDouble(\"spark.streaming.kafka.consumer.cache.loadFactor\", 0.75).toFloat\n+\n+  override def persist(newLevel: StorageLevel): this.type = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  override def getPartitions: Array[Partition] = {\n+    offsetRanges.zipWithIndex.map { case (o, i) =>\n+        new KafkaRDDPartition(i, o.topic, o.partition, o.fromOffset, o.untilOffset)\n+    }.toArray\n+  }\n+\n+  override def count(): Long = offsetRanges.map(_.count).sum\n+\n+  override def countApprox(\n+      timeout: Long,\n+      confidence: Double = 0.95\n+  ): PartialResult[BoundedDouble] = {\n+    val c = count\n+    new PartialResult(new BoundedDouble(c, 1.0, c, c), true)\n+  }\n+\n+  override def isEmpty(): Boolean = count == 0L\n+\n+  override def take(num: Int): Array[ConsumerRecord[K, V]] = {\n+    val nonEmptyPartitions = this.partitions\n+      .map(_.asInstanceOf[KafkaRDDPartition])\n+      .filter(_.count > 0)\n+\n+    if (num < 1 || nonEmptyPartitions.size < 1) {\n+      return new Array[ConsumerRecord[K, V]](0)\n+    }\n+\n+    // Determine in advance how many messages need to be taken from each partition\n+    val parts = nonEmptyPartitions.foldLeft(Map[Int, Int]()) { (result, part) =>\n+      val remain = num - result.values.sum\n+      if (remain > 0) {\n+        val taken = Math.min(remain, part.count)\n+        result + (part.index -> taken.toInt)\n+      } else {\n+        result\n+      }\n+    }\n+\n+    val buf = new ArrayBuffer[ConsumerRecord[K, V]]\n+    val res = context.runJob(\n+      this,\n+      (tc: TaskContext, it: Iterator[ConsumerRecord[K, V]]) =>\n+      it.take(parts(tc.partitionId)).toArray, parts.keys.toArray\n+    )\n+    res.foreach(buf ++= _)\n+    buf.toArray\n+  }\n+\n+  private def executors(): Array[ExecutorCacheTaskLocation] = {\n+    val bm = sparkContext.env.blockManager\n+    bm.master.getPeers(bm.blockManagerId).toArray\n+      .map(x => ExecutorCacheTaskLocation(x.host, x.executorId))\n+      .sortWith((a, b) => a.host > b.host || a.executorId > b.executorId)"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "It's not just host.  The to string method on an ExecutorCacheTaskLocation has a special prefix that is handled by the scheduling code.\n\nI don't think there's a necessity to special case local, because things should work regardless if you end up with an empty array, or an array with a single host.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-04-18T16:28:49Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD\n+ */\n+\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag] private[spark] (\n+    sc: SparkContext,\n+    val kafkaParams: ju.Map[String, Object],\n+    val offsetRanges: Array[OffsetRange],\n+    val preferredHosts: ju.Map[TopicPartition, String]\n+) extends RDD[ConsumerRecord[K, V]](sc, Nil) with Logging with HasOffsetRanges {\n+\n+  assert(\"none\" ==\n+    kafkaParams.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).asInstanceOf[String],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to none for executor kafka params, else messages may not match offsetRange\")\n+\n+  assert(false ==\n+    kafkaParams.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG).asInstanceOf[Boolean],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to false for executor kafka params, else offsets may commit before processing\")\n+\n+  // TODO is it necessary to have separate configs for initial poll time vs ongoing poll time?\n+  private val pollTimeout = conf.getLong(\"spark.streaming.kafka.consumer.poll.ms\", 256)\n+  private val cacheInitialCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.initialCapacity\", 16)\n+  private val cacheMaxCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.maxCapacity\", 64)\n+  private val cacheLoadFactor =\n+    conf.getDouble(\"spark.streaming.kafka.consumer.cache.loadFactor\", 0.75).toFloat\n+\n+  override def persist(newLevel: StorageLevel): this.type = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  override def getPartitions: Array[Partition] = {\n+    offsetRanges.zipWithIndex.map { case (o, i) =>\n+        new KafkaRDDPartition(i, o.topic, o.partition, o.fromOffset, o.untilOffset)\n+    }.toArray\n+  }\n+\n+  override def count(): Long = offsetRanges.map(_.count).sum\n+\n+  override def countApprox(\n+      timeout: Long,\n+      confidence: Double = 0.95\n+  ): PartialResult[BoundedDouble] = {\n+    val c = count\n+    new PartialResult(new BoundedDouble(c, 1.0, c, c), true)\n+  }\n+\n+  override def isEmpty(): Boolean = count == 0L\n+\n+  override def take(num: Int): Array[ConsumerRecord[K, V]] = {\n+    val nonEmptyPartitions = this.partitions\n+      .map(_.asInstanceOf[KafkaRDDPartition])\n+      .filter(_.count > 0)\n+\n+    if (num < 1 || nonEmptyPartitions.size < 1) {\n+      return new Array[ConsumerRecord[K, V]](0)\n+    }\n+\n+    // Determine in advance how many messages need to be taken from each partition\n+    val parts = nonEmptyPartitions.foldLeft(Map[Int, Int]()) { (result, part) =>\n+      val remain = num - result.values.sum\n+      if (remain > 0) {\n+        val taken = Math.min(remain, part.count)\n+        result + (part.index -> taken.toInt)\n+      } else {\n+        result\n+      }\n+    }\n+\n+    val buf = new ArrayBuffer[ConsumerRecord[K, V]]\n+    val res = context.runJob(\n+      this,\n+      (tc: TaskContext, it: Iterator[ConsumerRecord[K, V]]) =>\n+      it.take(parts(tc.partitionId)).toArray, parts.keys.toArray\n+    )\n+    res.foreach(buf ++= _)\n+    buf.toArray\n+  }\n+\n+  private def executors(): Array[ExecutorCacheTaskLocation] = {\n+    val bm = sparkContext.env.blockManager\n+    bm.master.getPeers(bm.blockManagerId).toArray\n+      .map(x => ExecutorCacheTaskLocation(x.host, x.executorId))\n+      .sortWith((a, b) => a.host > b.host || a.executorId > b.executorId)"
  }],
  "prId": 11863
}, {
  "comments": [{
    "author": {
      "login": "markgrover"
    },
    "body": "Would it make sense to have an allExecs member so we don't have to compute it more than once?\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-04-16T00:05:51Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD\n+ */\n+\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag] private[spark] (\n+    sc: SparkContext,\n+    val kafkaParams: ju.Map[String, Object],\n+    val offsetRanges: Array[OffsetRange],\n+    val preferredHosts: ju.Map[TopicPartition, String]\n+) extends RDD[ConsumerRecord[K, V]](sc, Nil) with Logging with HasOffsetRanges {\n+\n+  assert(\"none\" ==\n+    kafkaParams.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).asInstanceOf[String],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to none for executor kafka params, else messages may not match offsetRange\")\n+\n+  assert(false ==\n+    kafkaParams.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG).asInstanceOf[Boolean],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to false for executor kafka params, else offsets may commit before processing\")\n+\n+  // TODO is it necessary to have separate configs for initial poll time vs ongoing poll time?\n+  private val pollTimeout = conf.getLong(\"spark.streaming.kafka.consumer.poll.ms\", 256)\n+  private val cacheInitialCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.initialCapacity\", 16)\n+  private val cacheMaxCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.maxCapacity\", 64)\n+  private val cacheLoadFactor =\n+    conf.getDouble(\"spark.streaming.kafka.consumer.cache.loadFactor\", 0.75).toFloat\n+\n+  override def persist(newLevel: StorageLevel): this.type = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  override def getPartitions: Array[Partition] = {\n+    offsetRanges.zipWithIndex.map { case (o, i) =>\n+        new KafkaRDDPartition(i, o.topic, o.partition, o.fromOffset, o.untilOffset)\n+    }.toArray\n+  }\n+\n+  override def count(): Long = offsetRanges.map(_.count).sum\n+\n+  override def countApprox(\n+      timeout: Long,\n+      confidence: Double = 0.95\n+  ): PartialResult[BoundedDouble] = {\n+    val c = count\n+    new PartialResult(new BoundedDouble(c, 1.0, c, c), true)\n+  }\n+\n+  override def isEmpty(): Boolean = count == 0L\n+\n+  override def take(num: Int): Array[ConsumerRecord[K, V]] = {\n+    val nonEmptyPartitions = this.partitions\n+      .map(_.asInstanceOf[KafkaRDDPartition])\n+      .filter(_.count > 0)\n+\n+    if (num < 1 || nonEmptyPartitions.size < 1) {\n+      return new Array[ConsumerRecord[K, V]](0)\n+    }\n+\n+    // Determine in advance how many messages need to be taken from each partition\n+    val parts = nonEmptyPartitions.foldLeft(Map[Int, Int]()) { (result, part) =>\n+      val remain = num - result.values.sum\n+      if (remain > 0) {\n+        val taken = Math.min(remain, part.count)\n+        result + (part.index -> taken.toInt)\n+      } else {\n+        result\n+      }\n+    }\n+\n+    val buf = new ArrayBuffer[ConsumerRecord[K, V]]\n+    val res = context.runJob(\n+      this,\n+      (tc: TaskContext, it: Iterator[ConsumerRecord[K, V]]) =>\n+      it.take(parts(tc.partitionId)).toArray, parts.keys.toArray\n+    )\n+    res.foreach(buf ++= _)\n+    buf.toArray\n+  }\n+\n+  private def executors(): Array[ExecutorCacheTaskLocation] = {\n+    val bm = sparkContext.env.blockManager\n+    bm.master.getPeers(bm.blockManagerId).toArray\n+      .map(x => ExecutorCacheTaskLocation(x.host, x.executorId))\n+      .sortWith((a, b) => a.host > b.host || a.executorId > b.executorId)\n+  }\n+\n+  // non-negative modulus, from java 8 math\n+  private def floorMod(a: Int, b: Int): Int = ((a % b) + b) % b\n+\n+  override def getPreferredLocations(thePart: Partition): Seq[String] = {\n+    // TODO what about hosts specified by ip vs name\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    val allExecs = executors()"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "If it doesn't get computed more than once, it doesn't need a member.\n\nIf it does get computed more than once, the correct answer might have changed, in which case we want the most up to date info.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-04-18T16:29:41Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD\n+ */\n+\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag] private[spark] (\n+    sc: SparkContext,\n+    val kafkaParams: ju.Map[String, Object],\n+    val offsetRanges: Array[OffsetRange],\n+    val preferredHosts: ju.Map[TopicPartition, String]\n+) extends RDD[ConsumerRecord[K, V]](sc, Nil) with Logging with HasOffsetRanges {\n+\n+  assert(\"none\" ==\n+    kafkaParams.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).asInstanceOf[String],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to none for executor kafka params, else messages may not match offsetRange\")\n+\n+  assert(false ==\n+    kafkaParams.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG).asInstanceOf[Boolean],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to false for executor kafka params, else offsets may commit before processing\")\n+\n+  // TODO is it necessary to have separate configs for initial poll time vs ongoing poll time?\n+  private val pollTimeout = conf.getLong(\"spark.streaming.kafka.consumer.poll.ms\", 256)\n+  private val cacheInitialCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.initialCapacity\", 16)\n+  private val cacheMaxCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.maxCapacity\", 64)\n+  private val cacheLoadFactor =\n+    conf.getDouble(\"spark.streaming.kafka.consumer.cache.loadFactor\", 0.75).toFloat\n+\n+  override def persist(newLevel: StorageLevel): this.type = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  override def getPartitions: Array[Partition] = {\n+    offsetRanges.zipWithIndex.map { case (o, i) =>\n+        new KafkaRDDPartition(i, o.topic, o.partition, o.fromOffset, o.untilOffset)\n+    }.toArray\n+  }\n+\n+  override def count(): Long = offsetRanges.map(_.count).sum\n+\n+  override def countApprox(\n+      timeout: Long,\n+      confidence: Double = 0.95\n+  ): PartialResult[BoundedDouble] = {\n+    val c = count\n+    new PartialResult(new BoundedDouble(c, 1.0, c, c), true)\n+  }\n+\n+  override def isEmpty(): Boolean = count == 0L\n+\n+  override def take(num: Int): Array[ConsumerRecord[K, V]] = {\n+    val nonEmptyPartitions = this.partitions\n+      .map(_.asInstanceOf[KafkaRDDPartition])\n+      .filter(_.count > 0)\n+\n+    if (num < 1 || nonEmptyPartitions.size < 1) {\n+      return new Array[ConsumerRecord[K, V]](0)\n+    }\n+\n+    // Determine in advance how many messages need to be taken from each partition\n+    val parts = nonEmptyPartitions.foldLeft(Map[Int, Int]()) { (result, part) =>\n+      val remain = num - result.values.sum\n+      if (remain > 0) {\n+        val taken = Math.min(remain, part.count)\n+        result + (part.index -> taken.toInt)\n+      } else {\n+        result\n+      }\n+    }\n+\n+    val buf = new ArrayBuffer[ConsumerRecord[K, V]]\n+    val res = context.runJob(\n+      this,\n+      (tc: TaskContext, it: Iterator[ConsumerRecord[K, V]]) =>\n+      it.take(parts(tc.partitionId)).toArray, parts.keys.toArray\n+    )\n+    res.foreach(buf ++= _)\n+    buf.toArray\n+  }\n+\n+  private def executors(): Array[ExecutorCacheTaskLocation] = {\n+    val bm = sparkContext.env.blockManager\n+    bm.master.getPeers(bm.blockManagerId).toArray\n+      .map(x => ExecutorCacheTaskLocation(x.host, x.executorId))\n+      .sortWith((a, b) => a.host > b.host || a.executorId > b.executorId)\n+  }\n+\n+  // non-negative modulus, from java 8 math\n+  private def floorMod(a: Int, b: Int): Int = ((a % b) + b) % b\n+\n+  override def getPreferredLocations(thePart: Partition): Seq[String] = {\n+    // TODO what about hosts specified by ip vs name\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    val allExecs = executors()"
  }],
  "prId": 11863
}, {
  "comments": [{
    "author": {
      "login": "markgrover"
    },
    "body": "Ok, so this may very well be too late on a Friday but could you help me understand this part please?\nWhy set a preferred host when it seems like there isn't any?\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-04-16T00:06:41Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD\n+ */\n+\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag] private[spark] (\n+    sc: SparkContext,\n+    val kafkaParams: ju.Map[String, Object],\n+    val offsetRanges: Array[OffsetRange],\n+    val preferredHosts: ju.Map[TopicPartition, String]\n+) extends RDD[ConsumerRecord[K, V]](sc, Nil) with Logging with HasOffsetRanges {\n+\n+  assert(\"none\" ==\n+    kafkaParams.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).asInstanceOf[String],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to none for executor kafka params, else messages may not match offsetRange\")\n+\n+  assert(false ==\n+    kafkaParams.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG).asInstanceOf[Boolean],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to false for executor kafka params, else offsets may commit before processing\")\n+\n+  // TODO is it necessary to have separate configs for initial poll time vs ongoing poll time?\n+  private val pollTimeout = conf.getLong(\"spark.streaming.kafka.consumer.poll.ms\", 256)\n+  private val cacheInitialCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.initialCapacity\", 16)\n+  private val cacheMaxCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.maxCapacity\", 64)\n+  private val cacheLoadFactor =\n+    conf.getDouble(\"spark.streaming.kafka.consumer.cache.loadFactor\", 0.75).toFloat\n+\n+  override def persist(newLevel: StorageLevel): this.type = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  override def getPartitions: Array[Partition] = {\n+    offsetRanges.zipWithIndex.map { case (o, i) =>\n+        new KafkaRDDPartition(i, o.topic, o.partition, o.fromOffset, o.untilOffset)\n+    }.toArray\n+  }\n+\n+  override def count(): Long = offsetRanges.map(_.count).sum\n+\n+  override def countApprox(\n+      timeout: Long,\n+      confidence: Double = 0.95\n+  ): PartialResult[BoundedDouble] = {\n+    val c = count\n+    new PartialResult(new BoundedDouble(c, 1.0, c, c), true)\n+  }\n+\n+  override def isEmpty(): Boolean = count == 0L\n+\n+  override def take(num: Int): Array[ConsumerRecord[K, V]] = {\n+    val nonEmptyPartitions = this.partitions\n+      .map(_.asInstanceOf[KafkaRDDPartition])\n+      .filter(_.count > 0)\n+\n+    if (num < 1 || nonEmptyPartitions.size < 1) {\n+      return new Array[ConsumerRecord[K, V]](0)\n+    }\n+\n+    // Determine in advance how many messages need to be taken from each partition\n+    val parts = nonEmptyPartitions.foldLeft(Map[Int, Int]()) { (result, part) =>\n+      val remain = num - result.values.sum\n+      if (remain > 0) {\n+        val taken = Math.min(remain, part.count)\n+        result + (part.index -> taken.toInt)\n+      } else {\n+        result\n+      }\n+    }\n+\n+    val buf = new ArrayBuffer[ConsumerRecord[K, V]]\n+    val res = context.runJob(\n+      this,\n+      (tc: TaskContext, it: Iterator[ConsumerRecord[K, V]]) =>\n+      it.take(parts(tc.partitionId)).toArray, parts.keys.toArray\n+    )\n+    res.foreach(buf ++= _)\n+    buf.toArray\n+  }\n+\n+  private def executors(): Array[ExecutorCacheTaskLocation] = {\n+    val bm = sparkContext.env.blockManager\n+    bm.master.getPeers(bm.blockManagerId).toArray\n+      .map(x => ExecutorCacheTaskLocation(x.host, x.executorId))\n+      .sortWith((a, b) => a.host > b.host || a.executorId > b.executorId)\n+  }\n+\n+  // non-negative modulus, from java 8 math\n+  private def floorMod(a: Int, b: Int): Int = ((a % b) + b) % b\n+\n+  override def getPreferredLocations(thePart: Partition): Seq[String] = {\n+    // TODO what about hosts specified by ip vs name\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    val allExecs = executors()\n+    val tp = part.topicPartition\n+    val prefHost = preferredHosts.get(tp)\n+    val prefExecs = if (null == prefHost) allExecs else allExecs.filter(_.host == prefHost)\n+    val execs = if (prefExecs.isEmpty) allExecs else prefExecs\n+    if (execs.isEmpty) {\n+      Seq()\n+    } else {\n+      val index = this.floorMod(tp.hashCode, execs.length)"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "Default implementation of getPreferredLocations for an RDD is to return nil, indicating no preference.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-04-18T16:30:13Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD\n+ */\n+\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag] private[spark] (\n+    sc: SparkContext,\n+    val kafkaParams: ju.Map[String, Object],\n+    val offsetRanges: Array[OffsetRange],\n+    val preferredHosts: ju.Map[TopicPartition, String]\n+) extends RDD[ConsumerRecord[K, V]](sc, Nil) with Logging with HasOffsetRanges {\n+\n+  assert(\"none\" ==\n+    kafkaParams.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).asInstanceOf[String],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to none for executor kafka params, else messages may not match offsetRange\")\n+\n+  assert(false ==\n+    kafkaParams.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG).asInstanceOf[Boolean],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to false for executor kafka params, else offsets may commit before processing\")\n+\n+  // TODO is it necessary to have separate configs for initial poll time vs ongoing poll time?\n+  private val pollTimeout = conf.getLong(\"spark.streaming.kafka.consumer.poll.ms\", 256)\n+  private val cacheInitialCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.initialCapacity\", 16)\n+  private val cacheMaxCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.maxCapacity\", 64)\n+  private val cacheLoadFactor =\n+    conf.getDouble(\"spark.streaming.kafka.consumer.cache.loadFactor\", 0.75).toFloat\n+\n+  override def persist(newLevel: StorageLevel): this.type = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  override def getPartitions: Array[Partition] = {\n+    offsetRanges.zipWithIndex.map { case (o, i) =>\n+        new KafkaRDDPartition(i, o.topic, o.partition, o.fromOffset, o.untilOffset)\n+    }.toArray\n+  }\n+\n+  override def count(): Long = offsetRanges.map(_.count).sum\n+\n+  override def countApprox(\n+      timeout: Long,\n+      confidence: Double = 0.95\n+  ): PartialResult[BoundedDouble] = {\n+    val c = count\n+    new PartialResult(new BoundedDouble(c, 1.0, c, c), true)\n+  }\n+\n+  override def isEmpty(): Boolean = count == 0L\n+\n+  override def take(num: Int): Array[ConsumerRecord[K, V]] = {\n+    val nonEmptyPartitions = this.partitions\n+      .map(_.asInstanceOf[KafkaRDDPartition])\n+      .filter(_.count > 0)\n+\n+    if (num < 1 || nonEmptyPartitions.size < 1) {\n+      return new Array[ConsumerRecord[K, V]](0)\n+    }\n+\n+    // Determine in advance how many messages need to be taken from each partition\n+    val parts = nonEmptyPartitions.foldLeft(Map[Int, Int]()) { (result, part) =>\n+      val remain = num - result.values.sum\n+      if (remain > 0) {\n+        val taken = Math.min(remain, part.count)\n+        result + (part.index -> taken.toInt)\n+      } else {\n+        result\n+      }\n+    }\n+\n+    val buf = new ArrayBuffer[ConsumerRecord[K, V]]\n+    val res = context.runJob(\n+      this,\n+      (tc: TaskContext, it: Iterator[ConsumerRecord[K, V]]) =>\n+      it.take(parts(tc.partitionId)).toArray, parts.keys.toArray\n+    )\n+    res.foreach(buf ++= _)\n+    buf.toArray\n+  }\n+\n+  private def executors(): Array[ExecutorCacheTaskLocation] = {\n+    val bm = sparkContext.env.blockManager\n+    bm.master.getPeers(bm.blockManagerId).toArray\n+      .map(x => ExecutorCacheTaskLocation(x.host, x.executorId))\n+      .sortWith((a, b) => a.host > b.host || a.executorId > b.executorId)\n+  }\n+\n+  // non-negative modulus, from java 8 math\n+  private def floorMod(a: Int, b: Int): Int = ((a % b) + b) % b\n+\n+  override def getPreferredLocations(thePart: Partition): Seq[String] = {\n+    // TODO what about hosts specified by ip vs name\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    val allExecs = executors()\n+    val tp = part.topicPartition\n+    val prefHost = preferredHosts.get(tp)\n+    val prefExecs = if (null == prefHost) allExecs else allExecs.filter(_.host == prefHost)\n+    val execs = if (prefExecs.isEmpty) allExecs else prefExecs\n+    if (execs.isEmpty) {\n+      Seq()\n+    } else {\n+      val index = this.floorMod(tp.hashCode, execs.length)"
  }],
  "prId": 11863
}, {
  "comments": [{
    "author": {
      "login": "jerryshao"
    },
    "body": "From my understanding, if the cache hit rate of `cached consumer pool` is bad, there's a chance each executor will have lots of open consumers (like here 64), if there's multiple executors (for example 6) on one node (it is pretty common when running on yarn), the total opened consumer number will be relatively large. \n\nI'm wondering if we could choose the cache pool size according to the cores per executor and cores per task.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-05-06T02:45:50Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD\n+ */\n+\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag] private[spark] (\n+    sc: SparkContext,\n+    val kafkaParams: ju.Map[String, Object],\n+    val offsetRanges: Array[OffsetRange],\n+    val preferredHosts: ju.Map[TopicPartition, String]\n+) extends RDD[ConsumerRecord[K, V]](sc, Nil) with Logging with HasOffsetRanges {\n+\n+  assert(\"none\" ==\n+    kafkaParams.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).asInstanceOf[String],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to none for executor kafka params, else messages may not match offsetRange\")\n+\n+  assert(false ==\n+    kafkaParams.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG).asInstanceOf[Boolean],\n+    ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG +\n+      \" must be set to false for executor kafka params, else offsets may commit before processing\")\n+\n+  // TODO is it necessary to have separate configs for initial poll time vs ongoing poll time?\n+  private val pollTimeout = conf.getLong(\"spark.streaming.kafka.consumer.poll.ms\", 256)\n+  private val cacheInitialCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.initialCapacity\", 16)\n+  private val cacheMaxCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.maxCapacity\", 64)"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "The cache size is user configurable.  The way getPreferredLocations is structured should keep the cache from thrashing too badly as long as the cache size is proportional to the number of total partitions / number of executors.  I know it sucks that it's a connection per partition rather than a connection per broker, but with the way the consumer is designed, there isn't an easy way around that (unlike the old simple consumer).\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-05-06T03:17:08Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD\n+ */\n+\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag] private[spark] (\n+    sc: SparkContext,\n+    val kafkaParams: ju.Map[String, Object],\n+    val offsetRanges: Array[OffsetRange],\n+    val preferredHosts: ju.Map[TopicPartition, String]\n+) extends RDD[ConsumerRecord[K, V]](sc, Nil) with Logging with HasOffsetRanges {\n+\n+  assert(\"none\" ==\n+    kafkaParams.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).asInstanceOf[String],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to none for executor kafka params, else messages may not match offsetRange\")\n+\n+  assert(false ==\n+    kafkaParams.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG).asInstanceOf[Boolean],\n+    ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG +\n+      \" must be set to false for executor kafka params, else offsets may commit before processing\")\n+\n+  // TODO is it necessary to have separate configs for initial poll time vs ongoing poll time?\n+  private val pollTimeout = conf.getLong(\"spark.streaming.kafka.consumer.poll.ms\", 256)\n+  private val cacheInitialCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.initialCapacity\", 16)\n+  private val cacheMaxCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.maxCapacity\", 64)"
  }],
  "prId": 11863
}, {
  "comments": [{
    "author": {
      "login": "jerryshao"
    },
    "body": "I think we already override these two configurations, so maybe we don't need to do assert here again, Also there's no chance for user to create this object directly. \n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-05-06T07:59:24Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD\n+ */\n+\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag] private[spark] (\n+    sc: SparkContext,\n+    val kafkaParams: ju.Map[String, Object],\n+    val offsetRanges: Array[OffsetRange],\n+    val preferredHosts: ju.Map[TopicPartition, String]\n+) extends RDD[ConsumerRecord[K, V]](sc, Nil) with Logging with HasOffsetRanges {\n+\n+  assert(\"none\" ==\n+    kafkaParams.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).asInstanceOf[String],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to none for executor kafka params, else messages may not match offsetRange\")\n+\n+  assert(false =="
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "The override is done in the companion object not in this constructor.  And it's still possible for subclasses to construct this.  The real question is whether you'd ever want to allow executors to mess with offsets, and I'm pretty sure the answer is no.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-05-06T14:25:55Z",
    "diffHunk": "@@ -0,0 +1,259 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD\n+ */\n+\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag] private[spark] (\n+    sc: SparkContext,\n+    val kafkaParams: ju.Map[String, Object],\n+    val offsetRanges: Array[OffsetRange],\n+    val preferredHosts: ju.Map[TopicPartition, String]\n+) extends RDD[ConsumerRecord[K, V]](sc, Nil) with Logging with HasOffsetRanges {\n+\n+  assert(\"none\" ==\n+    kafkaParams.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).asInstanceOf[String],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to none for executor kafka params, else messages may not match offsetRange\")\n+\n+  assert(false =="
  }],
  "prId": 11863
}]