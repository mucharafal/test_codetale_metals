[{
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "The wrapping and code are a little hard to follow. Maybe introduce vals with intermediate values to clarify. Also maybe use `case (..., ...) =>` statements for tuples rather than `._1`, etc",
    "commit": "de02de411aa00cbacc94be5b746dc48be0fe77a3",
    "createdAt": "2019-01-13T14:40:34Z",
    "diffHunk": "@@ -258,17 +260,34 @@ class KafkaTestUtils(withBrokerProps: Map[String, Object] = Map.empty) extends L\n       topic: String,\n       messages: Array[String],\n       partition: Option[Int]): Seq[(String, RecordMetadata)] = {\n+    sendMessages(topic, messages.map(m => (m, Array[(String, Array[Byte])]())), partition)\n+  }\n+\n+  /** Send record to the Kafka broker with headers using specified partition */\n+  def sendMessage(topic: String,\n+                  record: (String, Array[(String, Array[Byte])]),\n+                  partition: Option[Int]): Seq[(String, RecordMetadata)] = {\n+    sendMessages(topic, Array(record), partition)\n+  }\n+\n+  /** Send the array of records to the Kafka broker with headers using specified partition */\n+  def sendMessages(topic: String,\n+                   records: Array[(String, Array[(String, Array[Byte])])],\n+                   partition: Option[Int]): Seq[(String, RecordMetadata)] = {\n     producer = new KafkaProducer[String, String](producerConfiguration)\n     val offsets = try {\n-      messages.map { m =>\n+      records.map { r =>\n         val record = partition match {\n-          case Some(p) => new ProducerRecord[String, String](topic, p, null, m)\n-          case None => new ProducerRecord[String, String](topic, m)\n+          case Some(p) => new ProducerRecord[String, String]("
  }],
  "prId": 22282
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Use scalatest `===` macros here not `==` in all the asserts.",
    "commit": "de02de411aa00cbacc94be5b746dc48be0fe77a3",
    "createdAt": "2019-01-13T14:40:57Z",
    "diffHunk": "@@ -485,3 +504,22 @@ class KafkaTestUtils(withBrokerProps: Map[String, Object] = Map.empty) extends L\n   }\n }\n \n+object KafkaTestUtils {\n+\n+  def assertEqual(lhs: (String, Array[(String, Array[Byte])]),\n+                  rhs: (String, Array[(String, Array[Byte])])): Unit = {\n+    assert(lhs._1 == rhs._1)"
  }],
  "prId": 22282
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "I think scalatest will understand tuple comparison already. It should be able to compare arrays too. Check its methods to see how to do deep comparisons. You may not need more utility methods",
    "commit": "de02de411aa00cbacc94be5b746dc48be0fe77a3",
    "createdAt": "2019-01-13T14:41:41Z",
    "diffHunk": "@@ -485,3 +504,22 @@ class KafkaTestUtils(withBrokerProps: Map[String, Object] = Map.empty) extends L\n   }\n }\n \n+object KafkaTestUtils {\n+\n+  def assertEqual(lhs: (String, Array[(String, Array[Byte])]),\n+                  rhs: (String, Array[(String, Array[Byte])])): Unit = {\n+    assert(lhs._1 == rhs._1)\n+    assert(lhs._2.size == rhs._2.size)\n+    (0 until lhs._2.size) foreach { i =>\n+      assert(lhs._2(i)._1 == rhs._2(i)._1)\n+      assert(lhs._2(i)._2.deep == rhs._2(i)._2.deep)\n+    }\n+  }\n+\n+  def assertEqual(lhs: Array[(String, Array[(String, Array[Byte])])],"
  }, {
    "author": {
      "login": "dongjinleekr"
    },
    "body": "I tried it, but it throws an error; One alternative I think is defining `KafkaTest` trait like `AnalysisTest` which extends `SparkFunSuite` and make `KafkaDataConsumerSuite` extend it like `SparkSqlParserSuite` extends `AnalysisTest.` How about this plan?",
    "commit": "de02de411aa00cbacc94be5b746dc48be0fe77a3",
    "createdAt": "2019-02-10T07:06:30Z",
    "diffHunk": "@@ -485,3 +504,22 @@ class KafkaTestUtils(withBrokerProps: Map[String, Object] = Map.empty) extends L\n   }\n }\n \n+object KafkaTestUtils {\n+\n+  def assertEqual(lhs: (String, Array[(String, Array[Byte])]),\n+                  rhs: (String, Array[(String, Array[Byte])])): Unit = {\n+    assert(lhs._1 == rhs._1)\n+    assert(lhs._2.size == rhs._2.size)\n+    (0 until lhs._2.size) foreach { i =>\n+      assert(lhs._2(i)._1 == rhs._2(i)._1)\n+      assert(lhs._2(i)._2.deep == rhs._2(i)._2.deep)\n+    }\n+  }\n+\n+  def assertEqual(lhs: Array[(String, Array[(String, Array[Byte])])],"
  }],
  "prId": 22282
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "nit: let's remove this unless you've fixed checkstyle here.",
    "commit": "de02de411aa00cbacc94be5b746dc48be0fe77a3",
    "createdAt": "2019-07-25T00:06:06Z",
    "diffHunk": "@@ -507,3 +507,4 @@ class KafkaTestUtils(withBrokerProps: Map[String, Object] = Map.empty) extends L\n     }\n   }\n }\n+"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Please ignore this if it's invalid. I reviewed against the last commit and not sure why Github says \"Outdated\".",
    "commit": "de02de411aa00cbacc94be5b746dc48be0fe77a3",
    "createdAt": "2019-07-25T00:19:58Z",
    "diffHunk": "@@ -507,3 +507,4 @@ class KafkaTestUtils(withBrokerProps: Map[String, Object] = Map.empty) extends L\n     }\n   }\n }\n+"
  }],
  "prId": 22282
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "can you make this more readable with something like `.map { case (value, header) =>`?",
    "commit": "de02de411aa00cbacc94be5b746dc48be0fe77a3",
    "createdAt": "2019-08-16T18:06:11Z",
    "diffHunk": "@@ -257,17 +259,34 @@ class KafkaTestUtils(withBrokerProps: Map[String, Object] = Map.empty) extends L\n       topic: String,\n       messages: Array[String],\n       partition: Option[Int]): Seq[(String, RecordMetadata)] = {\n+    sendMessages(topic, messages.map(m => (m, Seq())), partition)\n+  }\n+\n+  /** Send record to the Kafka broker with headers using specified partition */\n+  def sendMessage(topic: String,\n+                  record: (String, Seq[(String, Array[Byte])]),\n+                  partition: Option[Int]): Seq[(String, RecordMetadata)] = {\n+    sendMessages(topic, Array(record).toSeq, partition)\n+  }\n+\n+  /** Send the array of records to the Kafka broker with headers using specified partition */\n+  def sendMessages(topic: String,\n+                   records: Seq[(String, Seq[(String, Array[Byte])])],\n+                   partition: Option[Int]): Seq[(String, RecordMetadata)] = {\n     producer = new KafkaProducer[String, String](producerConfiguration)\n     val offsets = try {\n-      messages.map { m =>\n+      records.map { r =>"
  }],
  "prId": 22282
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Nit: Rather than duplicate this block, can you pass `p.orNull` as the second arg?",
    "commit": "de02de411aa00cbacc94be5b746dc48be0fe77a3",
    "createdAt": "2019-09-10T14:03:24Z",
    "diffHunk": "@@ -369,17 +371,36 @@ class KafkaTestUtils(\n       topic: String,\n       messages: Array[String],\n       partition: Option[Int]): Seq[(String, RecordMetadata)] = {\n+    sendMessages(topic, messages.map(m => (m, Seq())), partition)\n+  }\n+\n+  /** Send record to the Kafka broker with headers using specified partition */\n+  def sendMessage(topic: String,\n+                  record: (String, Seq[(String, Array[Byte])]),\n+                  partition: Option[Int]): Seq[(String, RecordMetadata)] = {\n+    sendMessages(topic, Array(record).toSeq, partition)\n+  }\n+\n+  /** Send the array of records to the Kafka broker with headers using specified partition */\n+  def sendMessages(topic: String,\n+                   records: Seq[(String, Seq[(String, Array[Byte])])],\n+                   partition: Option[Int]): Seq[(String, RecordMetadata)] = {\n     producer = new KafkaProducer[String, String](producerConfiguration)\n     val offsets = try {\n-      messages.map { m =>\n+      records.map { case (value, header) =>\n+        val headers = header.map { case (k, v) =>\n+          new RecordHeader(k, v).asInstanceOf[Header]\n+        }\n         val record = partition match {\n-          case Some(p) => new ProducerRecord[String, String](topic, p, null, m)\n-          case None => new ProducerRecord[String, String](topic, m)\n+          case Some(p) =>\n+            new ProducerRecord[String, String](topic, p, null, value, headers.asJava)",
    "line": 38
  }, {
    "author": {
      "login": "dongjinleekr"
    },
    "body": "We can't; `Option#orNull` returns scala `Null` type, not java `null`. Because of that, the compiler can't resolve the appropriate constructor then.",
    "commit": "de02de411aa00cbacc94be5b746dc48be0fe77a3",
    "createdAt": "2019-09-11T03:26:28Z",
    "diffHunk": "@@ -369,17 +371,36 @@ class KafkaTestUtils(\n       topic: String,\n       messages: Array[String],\n       partition: Option[Int]): Seq[(String, RecordMetadata)] = {\n+    sendMessages(topic, messages.map(m => (m, Seq())), partition)\n+  }\n+\n+  /** Send record to the Kafka broker with headers using specified partition */\n+  def sendMessage(topic: String,\n+                  record: (String, Seq[(String, Array[Byte])]),\n+                  partition: Option[Int]): Seq[(String, RecordMetadata)] = {\n+    sendMessages(topic, Array(record).toSeq, partition)\n+  }\n+\n+  /** Send the array of records to the Kafka broker with headers using specified partition */\n+  def sendMessages(topic: String,\n+                   records: Seq[(String, Seq[(String, Array[Byte])])],\n+                   partition: Option[Int]): Seq[(String, RecordMetadata)] = {\n     producer = new KafkaProducer[String, String](producerConfiguration)\n     val offsets = try {\n-      messages.map { m =>\n+      records.map { case (value, header) =>\n+        val headers = header.map { case (k, v) =>\n+          new RecordHeader(k, v).asInstanceOf[Header]\n+        }\n         val record = partition match {\n-          case Some(p) => new ProducerRecord[String, String](topic, p, null, m)\n-          case None => new ProducerRecord[String, String](topic, m)\n+          case Some(p) =>\n+            new ProducerRecord[String, String](topic, p, null, value, headers.asJava)",
    "line": 38
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "Oh, it's Option[Int], not of an object type. Right OK",
    "commit": "de02de411aa00cbacc94be5b746dc48be0fe77a3",
    "createdAt": "2019-09-11T13:23:59Z",
    "diffHunk": "@@ -369,17 +371,36 @@ class KafkaTestUtils(\n       topic: String,\n       messages: Array[String],\n       partition: Option[Int]): Seq[(String, RecordMetadata)] = {\n+    sendMessages(topic, messages.map(m => (m, Seq())), partition)\n+  }\n+\n+  /** Send record to the Kafka broker with headers using specified partition */\n+  def sendMessage(topic: String,\n+                  record: (String, Seq[(String, Array[Byte])]),\n+                  partition: Option[Int]): Seq[(String, RecordMetadata)] = {\n+    sendMessages(topic, Array(record).toSeq, partition)\n+  }\n+\n+  /** Send the array of records to the Kafka broker with headers using specified partition */\n+  def sendMessages(topic: String,\n+                   records: Seq[(String, Seq[(String, Array[Byte])])],\n+                   partition: Option[Int]): Seq[(String, RecordMetadata)] = {\n     producer = new KafkaProducer[String, String](producerConfiguration)\n     val offsets = try {\n-      messages.map { m =>\n+      records.map { case (value, header) =>\n+        val headers = header.map { case (k, v) =>\n+          new RecordHeader(k, v).asInstanceOf[Header]\n+        }\n         val record = partition match {\n-          case Some(p) => new ProducerRecord[String, String](topic, p, null, m)\n-          case None => new ProducerRecord[String, String](topic, m)\n+          case Some(p) =>\n+            new ProducerRecord[String, String](topic, p, null, value, headers.asJava)",
    "line": 38
  }],
  "prId": 22282
}]