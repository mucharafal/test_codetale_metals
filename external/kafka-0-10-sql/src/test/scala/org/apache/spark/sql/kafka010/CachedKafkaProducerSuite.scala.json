[{
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "`InUseCount` plays key role in this situation so I would check exact number in such situations.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-11T11:03:32Z",
    "diffHunk": "@@ -35,43 +36,72 @@ class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"ack\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Should return the cached instance, even if auth tokens are set up.\") {\n+    // TODO.\n+    // Question: What happens when a delegation token value is changed for a given producer?\n+    // we would need to recreate a kafka producer.\n+  }\n+\n+  test(\"Remove an offending kafka producer from cache.\") {\n+    import testImplicits._\n+    val df = Seq[(String, String)](null.asInstanceOf[String] -> \"1\").toDF(\"topic\", \"value\")\n+    intercept[SparkException] {\n+      // This will fail because the service is not reachable.\n+      df.write\n+        .format(\"kafka\")\n+        .option(\"topic\", \"topic\")\n+        .option(\"retries\", \"1\")\n+        .option(\"max.block.ms\", \"2\")\n+        .option(\"request.timeout.ms\", \"2\")\n+        .option(\"linger.ms\", \"2\")\n+        .option(\"kafka.bootstrap.servers\", \"12.0.0.1:39022\")\n+        .save()\n+    }\n+    // Since offending kafka producer is released on error and also invalidated, it should not be in\n+    // cache.\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 0)\n+  }\n+\n+  test(\"Should not close a producer in-use.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer: CachedKafkaProducer = CachedKafkaProducer.acquire(kafkaParams)\n+    producer.kafkaProducer // initializing the producer.\n+    assert(producer.getInUseCount.intValue() > 0)"
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Query may fail because of corrupted delegation token or any other problems. I would like to handle this in SPARK-27042. I would focus on the following here:\r\n* If new producer created the latest delegation token has to be picked up\r\n* Delegation token shouldn't be part of the key\r\n",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-11T11:09:16Z",
    "diffHunk": "@@ -35,43 +36,72 @@ class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"ack\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Should return the cached instance, even if auth tokens are set up.\") {\n+    // TODO."
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "I see the intention here but this test can pass if a producer never ever created.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-11T11:11:23Z",
    "diffHunk": "@@ -35,43 +36,72 @@ class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"ack\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Should return the cached instance, even if auth tokens are set up.\") {\n+    // TODO.\n+    // Question: What happens when a delegation token value is changed for a given producer?\n+    // we would need to recreate a kafka producer.\n+  }\n+\n+  test(\"Remove an offending kafka producer from cache.\") {"
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "hm.. but since we are calling `save` at the end, why would producer not created? \r\n\r\nIs it because of something else fails the df.write? like wrong parameter config? Wondering what could it be. \r\nAnyways, I can assert the failure message as well to be sure. \r\n",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-12T05:20:12Z",
    "diffHunk": "@@ -35,43 +36,72 @@ class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"ack\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Should return the cached instance, even if auth tokens are set up.\") {\n+    // TODO.\n+    // Question: What happens when a delegation token value is changed for a given producer?\n+    // we would need to recreate a kafka producer.\n+  }\n+\n+  test(\"Remove an offending kafka producer from cache.\") {"
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "* Seems like this timeout has no effect. At least when I've caused deadlock artificially then test never stopped.\r\n* The other concern I have is the consumed time. Spending 30 seconds for this is huge. What TD has written for the consumer part [here](https://github.com/apache/spark/blob/master/external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaDataConsumerSuite.scala#L63) takes about 7 seconds and provides more or less the same coverage. Maybe similar approach can be used.\r\n* It would be good to test failures as well like `CachedKafkaProducer.release(producer, true)`. This would be easy with the referenced other test approach (though with this one also solvable).\r\n",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-20T12:58:06Z",
    "diffHunk": "@@ -18,60 +18,143 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n+import org.scalatest.time.SpanSugar._\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.Trigger\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n-\n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"ack\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Automatically remove a failing kafka producer from cache.\") {\n+    import testImplicits._\n+    val df = Seq[(String, String)](null.asInstanceOf[String] -> \"1\").toDF(\"topic\", \"value\")\n+    val ex = intercept[SparkException] {\n+      // This will fail because the service is not reachable.\n+      df.write\n+        .format(\"kafka\")\n+        .option(\"topic\", \"topic\")\n+        .option(\"kafka.retries\", \"1\")\n+        .option(\"kafka.max.block.ms\", \"2\")\n+        .option(\"kafka.bootstrap.servers\", \"12.0.0.1:39022\")\n+        .save()\n+    }\n+    assert(ex.getMessage.contains(\"org.apache.kafka.common.errors.TimeoutException\"),\n+      \"Spark command should fail due to service not reachable.\")\n+    // Since failing kafka producer is released on error and also invalidated, it should not be in\n+    // cache.\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 0)\n+  }\n+\n+  test(\"Should not close a producer in-use.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer: CachedKafkaProducer = CachedKafkaProducer.acquire(kafkaParams)\n+    producer.kafkaProducer // initializing the producer.\n+    assert(producer.getInUseCount == 1)\n+    // Explicitly cause the producer from guava cache to be evicted.\n+    CachedKafkaProducer.evict(producer.getKafkaParams)\n+    assert(producer.getInUseCount == 1)\n+    assert(!producer.isClosed, \"An in-use producer should not be closed.\")\n+  }\n+\n+  private def generateKafkaParams: ju.HashMap[String, Object] = {\n     val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n+    kafkaParams.put(\"ack\", \"0\")\n     kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n     kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n     kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    kafkaParams.put(\"acks\", \"1\")\n-    val producer2: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    // With updated conf, a new producer instance should be created.\n-    assert(producer != producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map.size == 2)\n+    kafkaParams\n+  }\n+}\n+\n+class CachedKafkaProducerStressSuite extends KafkaContinuousTest with KafkaTest {\n+\n+  override val streamingTimeout = 30.seconds"
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "There is a test, \"Automatically remove a failing kafka producer from cache.\" which covers the `CachedKafkaProducer.release(producer, true)`. Is that what you meant?\r\n",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-25T08:25:26Z",
    "diffHunk": "@@ -18,60 +18,143 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n+import org.scalatest.time.SpanSugar._\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.Trigger\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n-\n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"ack\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Automatically remove a failing kafka producer from cache.\") {\n+    import testImplicits._\n+    val df = Seq[(String, String)](null.asInstanceOf[String] -> \"1\").toDF(\"topic\", \"value\")\n+    val ex = intercept[SparkException] {\n+      // This will fail because the service is not reachable.\n+      df.write\n+        .format(\"kafka\")\n+        .option(\"topic\", \"topic\")\n+        .option(\"kafka.retries\", \"1\")\n+        .option(\"kafka.max.block.ms\", \"2\")\n+        .option(\"kafka.bootstrap.servers\", \"12.0.0.1:39022\")\n+        .save()\n+    }\n+    assert(ex.getMessage.contains(\"org.apache.kafka.common.errors.TimeoutException\"),\n+      \"Spark command should fail due to service not reachable.\")\n+    // Since failing kafka producer is released on error and also invalidated, it should not be in\n+    // cache.\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 0)\n+  }\n+\n+  test(\"Should not close a producer in-use.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer: CachedKafkaProducer = CachedKafkaProducer.acquire(kafkaParams)\n+    producer.kafkaProducer // initializing the producer.\n+    assert(producer.getInUseCount == 1)\n+    // Explicitly cause the producer from guava cache to be evicted.\n+    CachedKafkaProducer.evict(producer.getKafkaParams)\n+    assert(producer.getInUseCount == 1)\n+    assert(!producer.isClosed, \"An in-use producer should not be closed.\")\n+  }\n+\n+  private def generateKafkaParams: ju.HashMap[String, Object] = {\n     val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n+    kafkaParams.put(\"ack\", \"0\")\n     kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n     kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n     kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    kafkaParams.put(\"acks\", \"1\")\n-    val producer2: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    // With updated conf, a new producer instance should be created.\n-    assert(producer != producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map.size == 2)\n+    kafkaParams\n+  }\n+}\n+\n+class CachedKafkaProducerStressSuite extends KafkaContinuousTest with KafkaTest {\n+\n+  override val streamingTimeout = 30.seconds"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "That's good but not what I meant. For example release 2 times.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-25T08:33:43Z",
    "diffHunk": "@@ -18,60 +18,143 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n+import org.scalatest.time.SpanSugar._\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.Trigger\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n-\n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"ack\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Automatically remove a failing kafka producer from cache.\") {\n+    import testImplicits._\n+    val df = Seq[(String, String)](null.asInstanceOf[String] -> \"1\").toDF(\"topic\", \"value\")\n+    val ex = intercept[SparkException] {\n+      // This will fail because the service is not reachable.\n+      df.write\n+        .format(\"kafka\")\n+        .option(\"topic\", \"topic\")\n+        .option(\"kafka.retries\", \"1\")\n+        .option(\"kafka.max.block.ms\", \"2\")\n+        .option(\"kafka.bootstrap.servers\", \"12.0.0.1:39022\")\n+        .save()\n+    }\n+    assert(ex.getMessage.contains(\"org.apache.kafka.common.errors.TimeoutException\"),\n+      \"Spark command should fail due to service not reachable.\")\n+    // Since failing kafka producer is released on error and also invalidated, it should not be in\n+    // cache.\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 0)\n+  }\n+\n+  test(\"Should not close a producer in-use.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer: CachedKafkaProducer = CachedKafkaProducer.acquire(kafkaParams)\n+    producer.kafkaProducer // initializing the producer.\n+    assert(producer.getInUseCount == 1)\n+    // Explicitly cause the producer from guava cache to be evicted.\n+    CachedKafkaProducer.evict(producer.getKafkaParams)\n+    assert(producer.getInUseCount == 1)\n+    assert(!producer.isClosed, \"An in-use producer should not be closed.\")\n+  }\n+\n+  private def generateKafkaParams: ju.HashMap[String, Object] = {\n     val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n+    kafkaParams.put(\"ack\", \"0\")\n     kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n     kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n     kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    kafkaParams.put(\"acks\", \"1\")\n-    val producer2: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    // With updated conf, a new producer instance should be created.\n-    assert(producer != producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map.size == 2)\n+    kafkaParams\n+  }\n+}\n+\n+class CachedKafkaProducerStressSuite extends KafkaContinuousTest with KafkaTest {\n+\n+  override val streamingTimeout = 30.seconds"
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "s/ack/acks",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-27T10:45:15Z",
    "diffHunk": "@@ -18,60 +18,206 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n+import java.util.concurrent.{ConcurrentLinkedQueue, Executors, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.util.Random\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.Trigger\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n \n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"ack\") // mutate the kafka params."
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "s/ack/acks",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-27T10:45:28Z",
    "diffHunk": "@@ -18,60 +18,206 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n+import java.util.concurrent.{ConcurrentLinkedQueue, Executors, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.util.Random\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.Trigger\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n \n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"ack\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Automatically remove a failing kafka producer from cache.\") {\n+    import testImplicits._\n+    val df = Seq[(String, String)](null.asInstanceOf[String] -> \"1\").toDF(\"topic\", \"value\")\n+    val ex = intercept[SparkException] {\n+      // This will fail because the service is not reachable.\n+      df.write\n+        .format(\"kafka\")\n+        .option(\"topic\", \"topic\")\n+        .option(\"kafka.retries\", \"1\")\n+        .option(\"kafka.max.block.ms\", \"2\")\n+        .option(\"kafka.bootstrap.servers\", \"12.0.0.1:39022\")\n+        .save()\n+    }\n+    assert(ex.getMessage.contains(\"org.apache.kafka.common.errors.TimeoutException\"),\n+      \"Spark command should fail due to service not reachable.\")\n+    // Since failing kafka producer is released on error and also invalidated, it should not be in\n+    // cache.\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 0)\n+  }\n+\n+  test(\"Should not close a producer in-use.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer: CachedKafkaProducer = CachedKafkaProducer.acquire(kafkaParams)\n+    producer.kafkaProducer // initializing the producer.\n+    assert(producer.getInUseCount == 1)\n+    // Explicitly cause the producer from guava cache to be evicted.\n+    CachedKafkaProducer.evict(producer.getKafkaParams)\n+    assert(producer.getInUseCount == 1)\n+    assert(!producer.isClosed, \"An in-use producer should not be closed.\")\n+  }\n+\n+  private def generateKafkaParams: ju.HashMap[String, Object] = {\n     val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n+    kafkaParams.put(\"ack\", \"0\")"
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Maybe `TimeoutException` and `service not reachable` enough.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-27T10:49:35Z",
    "diffHunk": "@@ -18,60 +18,206 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n+import java.util.concurrent.{ConcurrentLinkedQueue, Executors, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.util.Random\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.Trigger\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n \n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"ack\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Automatically remove a failing kafka producer from cache.\") {\n+    import testImplicits._\n+    val df = Seq[(String, String)](null.asInstanceOf[String] -> \"1\").toDF(\"topic\", \"value\")\n+    val ex = intercept[SparkException] {\n+      // This will fail because the service is not reachable.\n+      df.write\n+        .format(\"kafka\")\n+        .option(\"topic\", \"topic\")\n+        .option(\"kafka.retries\", \"1\")\n+        .option(\"kafka.max.block.ms\", \"2\")\n+        .option(\"kafka.bootstrap.servers\", \"12.0.0.1:39022\")\n+        .save()\n+    }\n+    assert(ex.getMessage.contains(\"org.apache.kafka.common.errors.TimeoutException\"),"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Neither changed nor comment left here.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-28T13:26:03Z",
    "diffHunk": "@@ -18,60 +18,206 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n+import java.util.concurrent.{ConcurrentLinkedQueue, Executors, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.util.Random\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.Trigger\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n \n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"ack\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Automatically remove a failing kafka producer from cache.\") {\n+    import testImplicits._\n+    val df = Seq[(String, String)](null.asInstanceOf[String] -> \"1\").toDF(\"topic\", \"value\")\n+    val ex = intercept[SparkException] {\n+      // This will fail because the service is not reachable.\n+      df.write\n+        .format(\"kafka\")\n+        .option(\"topic\", \"topic\")\n+        .option(\"kafka.retries\", \"1\")\n+        .option(\"kafka.max.block.ms\", \"2\")\n+        .option(\"kafka.bootstrap.servers\", \"12.0.0.1:39022\")\n+        .save()\n+    }\n+    assert(ex.getMessage.contains(\"org.apache.kafka.common.errors.TimeoutException\"),"
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Nit: Type not required.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-27T11:07:14Z",
    "diffHunk": "@@ -18,60 +18,206 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n+import java.util.concurrent.{ConcurrentLinkedQueue, Executors, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.util.Random\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.Trigger\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n \n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"ack\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Automatically remove a failing kafka producer from cache.\") {\n+    import testImplicits._\n+    val df = Seq[(String, String)](null.asInstanceOf[String] -> \"1\").toDF(\"topic\", \"value\")\n+    val ex = intercept[SparkException] {\n+      // This will fail because the service is not reachable.\n+      df.write\n+        .format(\"kafka\")\n+        .option(\"topic\", \"topic\")\n+        .option(\"kafka.retries\", \"1\")\n+        .option(\"kafka.max.block.ms\", \"2\")\n+        .option(\"kafka.bootstrap.servers\", \"12.0.0.1:39022\")\n+        .save()\n+    }\n+    assert(ex.getMessage.contains(\"org.apache.kafka.common.errors.TimeoutException\"),\n+      \"Spark command should fail due to service not reachable.\")\n+    // Since failing kafka producer is released on error and also invalidated, it should not be in\n+    // cache.\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 0)\n+  }\n+\n+  test(\"Should not close a producer in-use.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer: CachedKafkaProducer = CachedKafkaProducer.acquire(kafkaParams)\n+    producer.kafkaProducer // initializing the producer.\n+    assert(producer.getInUseCount == 1)\n+    // Explicitly cause the producer from guava cache to be evicted.\n+    CachedKafkaProducer.evict(producer.getKafkaParams)\n+    assert(producer.getInUseCount == 1)\n+    assert(!producer.isClosed, \"An in-use producer should not be closed.\")\n+  }\n+\n+  private def generateKafkaParams: ju.HashMap[String, Object] = {\n     val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n+    kafkaParams.put(\"ack\", \"0\")\n     kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n     kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n     kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    kafkaParams.put(\"acks\", \"1\")\n-    val producer2: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    // With updated conf, a new producer instance should be created.\n-    assert(producer != producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map.size == 2)\n+    kafkaParams\n+  }\n+}\n+\n+class CachedKafkaProducerStressSuite extends KafkaContinuousTest with KafkaTest {\n+\n+  override val brokerProps = Map(\"auto.create.topics.enable\" -> \"false\")\n+\n+  override def afterAll(): Unit = {\n+    if (testUtils != null) {\n+      testUtils.teardown()\n+      testUtils = null\n+    }\n+    super.afterAll()\n+  }\n+\n+  override def sparkConf: SparkConf = {\n+    val conf = super.sparkConf\n+    conf.set(\"spark.kafka.producer.cache.timeout\", \"2ms\")\n+  }\n+\n+  test(\"concurrent use of CachedKafkaProducer\") {\n+    val topic = \"topic\" + Random.nextInt()\n+    val data = (1 to 10).map(_.toString)\n+    testUtils.createTopic(topic, 1)\n+    val kafkaParams: Map[String, Object] = Map(\"bootstrap.servers\" -> testUtils.brokerAddress,\n+      \"key.serializer\" -> classOf[ByteArraySerializer].getName,\n+      \"value.serializer\" -> classOf[ByteArraySerializer].getName)\n \n-    CachedKafkaProducer.close(kafkaParams)\n-    val map2 = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map2.size == 1)\n     import scala.collection.JavaConverters._\n-    val (seq: Seq[(String, Object)], _producer: KP) = map2.asScala.toArray.apply(0)\n-    assert(_producer == producer)\n+\n+    val numThreads: Int = 100"
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "`data` unused.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-27T11:07:33Z",
    "diffHunk": "@@ -18,60 +18,206 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n+import java.util.concurrent.{ConcurrentLinkedQueue, Executors, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.util.Random\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.Trigger\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n \n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"ack\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Automatically remove a failing kafka producer from cache.\") {\n+    import testImplicits._\n+    val df = Seq[(String, String)](null.asInstanceOf[String] -> \"1\").toDF(\"topic\", \"value\")\n+    val ex = intercept[SparkException] {\n+      // This will fail because the service is not reachable.\n+      df.write\n+        .format(\"kafka\")\n+        .option(\"topic\", \"topic\")\n+        .option(\"kafka.retries\", \"1\")\n+        .option(\"kafka.max.block.ms\", \"2\")\n+        .option(\"kafka.bootstrap.servers\", \"12.0.0.1:39022\")\n+        .save()\n+    }\n+    assert(ex.getMessage.contains(\"org.apache.kafka.common.errors.TimeoutException\"),\n+      \"Spark command should fail due to service not reachable.\")\n+    // Since failing kafka producer is released on error and also invalidated, it should not be in\n+    // cache.\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 0)\n+  }\n+\n+  test(\"Should not close a producer in-use.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer: CachedKafkaProducer = CachedKafkaProducer.acquire(kafkaParams)\n+    producer.kafkaProducer // initializing the producer.\n+    assert(producer.getInUseCount == 1)\n+    // Explicitly cause the producer from guava cache to be evicted.\n+    CachedKafkaProducer.evict(producer.getKafkaParams)\n+    assert(producer.getInUseCount == 1)\n+    assert(!producer.isClosed, \"An in-use producer should not be closed.\")\n+  }\n+\n+  private def generateKafkaParams: ju.HashMap[String, Object] = {\n     val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n+    kafkaParams.put(\"ack\", \"0\")\n     kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n     kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n     kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    kafkaParams.put(\"acks\", \"1\")\n-    val producer2: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    // With updated conf, a new producer instance should be created.\n-    assert(producer != producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map.size == 2)\n+    kafkaParams\n+  }\n+}\n+\n+class CachedKafkaProducerStressSuite extends KafkaContinuousTest with KafkaTest {\n+\n+  override val brokerProps = Map(\"auto.create.topics.enable\" -> \"false\")\n+\n+  override def afterAll(): Unit = {\n+    if (testUtils != null) {\n+      testUtils.teardown()\n+      testUtils = null\n+    }\n+    super.afterAll()\n+  }\n+\n+  override def sparkConf: SparkConf = {\n+    val conf = super.sparkConf\n+    conf.set(\"spark.kafka.producer.cache.timeout\", \"2ms\")\n+  }\n+\n+  test(\"concurrent use of CachedKafkaProducer\") {\n+    val topic = \"topic\" + Random.nextInt()\n+    val data = (1 to 10).map(_.toString)"
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Nit: s/( 1/(1",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-27T11:11:10Z",
    "diffHunk": "@@ -18,60 +18,206 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n+import java.util.concurrent.{ConcurrentLinkedQueue, Executors, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.util.Random\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.Trigger\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n \n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"ack\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Automatically remove a failing kafka producer from cache.\") {\n+    import testImplicits._\n+    val df = Seq[(String, String)](null.asInstanceOf[String] -> \"1\").toDF(\"topic\", \"value\")\n+    val ex = intercept[SparkException] {\n+      // This will fail because the service is not reachable.\n+      df.write\n+        .format(\"kafka\")\n+        .option(\"topic\", \"topic\")\n+        .option(\"kafka.retries\", \"1\")\n+        .option(\"kafka.max.block.ms\", \"2\")\n+        .option(\"kafka.bootstrap.servers\", \"12.0.0.1:39022\")\n+        .save()\n+    }\n+    assert(ex.getMessage.contains(\"org.apache.kafka.common.errors.TimeoutException\"),\n+      \"Spark command should fail due to service not reachable.\")\n+    // Since failing kafka producer is released on error and also invalidated, it should not be in\n+    // cache.\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 0)\n+  }\n+\n+  test(\"Should not close a producer in-use.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer: CachedKafkaProducer = CachedKafkaProducer.acquire(kafkaParams)\n+    producer.kafkaProducer // initializing the producer.\n+    assert(producer.getInUseCount == 1)\n+    // Explicitly cause the producer from guava cache to be evicted.\n+    CachedKafkaProducer.evict(producer.getKafkaParams)\n+    assert(producer.getInUseCount == 1)\n+    assert(!producer.isClosed, \"An in-use producer should not be closed.\")\n+  }\n+\n+  private def generateKafkaParams: ju.HashMap[String, Object] = {\n     val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n+    kafkaParams.put(\"ack\", \"0\")\n     kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n     kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n     kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    kafkaParams.put(\"acks\", \"1\")\n-    val producer2: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    // With updated conf, a new producer instance should be created.\n-    assert(producer != producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map.size == 2)\n+    kafkaParams\n+  }\n+}\n+\n+class CachedKafkaProducerStressSuite extends KafkaContinuousTest with KafkaTest {\n+\n+  override val brokerProps = Map(\"auto.create.topics.enable\" -> \"false\")\n+\n+  override def afterAll(): Unit = {\n+    if (testUtils != null) {\n+      testUtils.teardown()\n+      testUtils = null\n+    }\n+    super.afterAll()\n+  }\n+\n+  override def sparkConf: SparkConf = {\n+    val conf = super.sparkConf\n+    conf.set(\"spark.kafka.producer.cache.timeout\", \"2ms\")\n+  }\n+\n+  test(\"concurrent use of CachedKafkaProducer\") {\n+    val topic = \"topic\" + Random.nextInt()\n+    val data = (1 to 10).map(_.toString)\n+    testUtils.createTopic(topic, 1)\n+    val kafkaParams: Map[String, Object] = Map(\"bootstrap.servers\" -> testUtils.brokerAddress,\n+      \"key.serializer\" -> classOf[ByteArraySerializer].getName,\n+      \"value.serializer\" -> classOf[ByteArraySerializer].getName)\n \n-    CachedKafkaProducer.close(kafkaParams)\n-    val map2 = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map2.size == 1)\n     import scala.collection.JavaConverters._\n-    val (seq: Seq[(String, Object)], _producer: KP) = map2.asScala.toArray.apply(0)\n-    assert(_producer == producer)\n+\n+    val numThreads: Int = 100\n+    val numConcurrentProducers: Int = 1000\n+\n+    val kafkaParamsUniqueMap = mutable.HashMap.empty[Int, ju.Map[String, Object]]\n+    ( 1 to numConcurrentProducers).map {"
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Like the direction but maybe some fine tuning would be good because I receive this exception on my local machine:\r\n```\r\nCaused by: org.apache.kafka.common.KafkaException: java.io.IOException: Too many open files\r\n```\r\nOn a beefy executor maybe 20 different TopicPartition is realistic. I think the number of usages can be set high just like in the consumer side test.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-27T11:18:59Z",
    "diffHunk": "@@ -18,60 +18,206 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n+import java.util.concurrent.{ConcurrentLinkedQueue, Executors, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.util.Random\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.Trigger\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n \n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"ack\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Automatically remove a failing kafka producer from cache.\") {\n+    import testImplicits._\n+    val df = Seq[(String, String)](null.asInstanceOf[String] -> \"1\").toDF(\"topic\", \"value\")\n+    val ex = intercept[SparkException] {\n+      // This will fail because the service is not reachable.\n+      df.write\n+        .format(\"kafka\")\n+        .option(\"topic\", \"topic\")\n+        .option(\"kafka.retries\", \"1\")\n+        .option(\"kafka.max.block.ms\", \"2\")\n+        .option(\"kafka.bootstrap.servers\", \"12.0.0.1:39022\")\n+        .save()\n+    }\n+    assert(ex.getMessage.contains(\"org.apache.kafka.common.errors.TimeoutException\"),\n+      \"Spark command should fail due to service not reachable.\")\n+    // Since failing kafka producer is released on error and also invalidated, it should not be in\n+    // cache.\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 0)\n+  }\n+\n+  test(\"Should not close a producer in-use.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer: CachedKafkaProducer = CachedKafkaProducer.acquire(kafkaParams)\n+    producer.kafkaProducer // initializing the producer.\n+    assert(producer.getInUseCount == 1)\n+    // Explicitly cause the producer from guava cache to be evicted.\n+    CachedKafkaProducer.evict(producer.getKafkaParams)\n+    assert(producer.getInUseCount == 1)\n+    assert(!producer.isClosed, \"An in-use producer should not be closed.\")\n+  }\n+\n+  private def generateKafkaParams: ju.HashMap[String, Object] = {\n     val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n+    kafkaParams.put(\"ack\", \"0\")\n     kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n     kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n     kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    kafkaParams.put(\"acks\", \"1\")\n-    val producer2: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    // With updated conf, a new producer instance should be created.\n-    assert(producer != producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map.size == 2)\n+    kafkaParams\n+  }\n+}\n+\n+class CachedKafkaProducerStressSuite extends KafkaContinuousTest with KafkaTest {",
    "line": 118
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Not sure whether its forgotten or under implementation but this hasn't been resolved and I still have the same issue.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-28T13:25:17Z",
    "diffHunk": "@@ -18,60 +18,206 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n+import java.util.concurrent.{ConcurrentLinkedQueue, Executors, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.util.Random\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.Trigger\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n \n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"ack\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Automatically remove a failing kafka producer from cache.\") {\n+    import testImplicits._\n+    val df = Seq[(String, String)](null.asInstanceOf[String] -> \"1\").toDF(\"topic\", \"value\")\n+    val ex = intercept[SparkException] {\n+      // This will fail because the service is not reachable.\n+      df.write\n+        .format(\"kafka\")\n+        .option(\"topic\", \"topic\")\n+        .option(\"kafka.retries\", \"1\")\n+        .option(\"kafka.max.block.ms\", \"2\")\n+        .option(\"kafka.bootstrap.servers\", \"12.0.0.1:39022\")\n+        .save()\n+    }\n+    assert(ex.getMessage.contains(\"org.apache.kafka.common.errors.TimeoutException\"),\n+      \"Spark command should fail due to service not reachable.\")\n+    // Since failing kafka producer is released on error and also invalidated, it should not be in\n+    // cache.\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 0)\n+  }\n+\n+  test(\"Should not close a producer in-use.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer: CachedKafkaProducer = CachedKafkaProducer.acquire(kafkaParams)\n+    producer.kafkaProducer // initializing the producer.\n+    assert(producer.getInUseCount == 1)\n+    // Explicitly cause the producer from guava cache to be evicted.\n+    CachedKafkaProducer.evict(producer.getKafkaParams)\n+    assert(producer.getInUseCount == 1)\n+    assert(!producer.isClosed, \"An in-use producer should not be closed.\")\n+  }\n+\n+  private def generateKafkaParams: ju.HashMap[String, Object] = {\n     val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n+    kafkaParams.put(\"ack\", \"0\")\n     kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n     kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n     kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    kafkaParams.put(\"acks\", \"1\")\n-    val producer2: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    // With updated conf, a new producer instance should be created.\n-    assert(producer != producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map.size == 2)\n+    kafkaParams\n+  }\n+}\n+\n+class CachedKafkaProducerStressSuite extends KafkaContinuousTest with KafkaTest {",
    "line": 118
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "I was not able to see that error, on my laptop(which has very low spec). My guess is, it happens due to too many producer object creation. I can reduce it, in the test.\r\nBut maybe you can increase the ulimit on your system, the default is a bit low.\r\n\r\nA side note, the test does not fail, even without synchronisation blocks, on my system. Am I missing something?",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-29T05:25:17Z",
    "diffHunk": "@@ -18,60 +18,206 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n+import java.util.concurrent.{ConcurrentLinkedQueue, Executors, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.util.Random\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.Trigger\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n \n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"ack\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Automatically remove a failing kafka producer from cache.\") {\n+    import testImplicits._\n+    val df = Seq[(String, String)](null.asInstanceOf[String] -> \"1\").toDF(\"topic\", \"value\")\n+    val ex = intercept[SparkException] {\n+      // This will fail because the service is not reachable.\n+      df.write\n+        .format(\"kafka\")\n+        .option(\"topic\", \"topic\")\n+        .option(\"kafka.retries\", \"1\")\n+        .option(\"kafka.max.block.ms\", \"2\")\n+        .option(\"kafka.bootstrap.servers\", \"12.0.0.1:39022\")\n+        .save()\n+    }\n+    assert(ex.getMessage.contains(\"org.apache.kafka.common.errors.TimeoutException\"),\n+      \"Spark command should fail due to service not reachable.\")\n+    // Since failing kafka producer is released on error and also invalidated, it should not be in\n+    // cache.\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 0)\n+  }\n+\n+  test(\"Should not close a producer in-use.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer: CachedKafkaProducer = CachedKafkaProducer.acquire(kafkaParams)\n+    producer.kafkaProducer // initializing the producer.\n+    assert(producer.getInUseCount == 1)\n+    // Explicitly cause the producer from guava cache to be evicted.\n+    CachedKafkaProducer.evict(producer.getKafkaParams)\n+    assert(producer.getInUseCount == 1)\n+    assert(!producer.isClosed, \"An in-use producer should not be closed.\")\n+  }\n+\n+  private def generateKafkaParams: ju.HashMap[String, Object] = {\n     val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n+    kafkaParams.put(\"ack\", \"0\")\n     kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n     kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n     kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    kafkaParams.put(\"acks\", \"1\")\n-    val producer2: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    // With updated conf, a new producer instance should be created.\n-    assert(producer != producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map.size == 2)\n+    kafkaParams\n+  }\n+}\n+\n+class CachedKafkaProducerStressSuite extends KafkaContinuousTest with KafkaTest {",
    "line": 118
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "I have a MacOS Mojave with default settings. I've not done any deadlock or other magic when this issue came. We've tried it on 2 other machines and produced the same exception.\r\n\r\n> But maybe you can increase the ulimit on your system, the default is a bit low.\r\n\r\nExpecting maybe several thousand contributors to adapt the system to a single test is not a really classical approach. Will check the the updated test...",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-29T11:30:22Z",
    "diffHunk": "@@ -18,60 +18,206 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n+import java.util.concurrent.{ConcurrentLinkedQueue, Executors, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.util.Random\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.Trigger\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n \n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"ack\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Automatically remove a failing kafka producer from cache.\") {\n+    import testImplicits._\n+    val df = Seq[(String, String)](null.asInstanceOf[String] -> \"1\").toDF(\"topic\", \"value\")\n+    val ex = intercept[SparkException] {\n+      // This will fail because the service is not reachable.\n+      df.write\n+        .format(\"kafka\")\n+        .option(\"topic\", \"topic\")\n+        .option(\"kafka.retries\", \"1\")\n+        .option(\"kafka.max.block.ms\", \"2\")\n+        .option(\"kafka.bootstrap.servers\", \"12.0.0.1:39022\")\n+        .save()\n+    }\n+    assert(ex.getMessage.contains(\"org.apache.kafka.common.errors.TimeoutException\"),\n+      \"Spark command should fail due to service not reachable.\")\n+    // Since failing kafka producer is released on error and also invalidated, it should not be in\n+    // cache.\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 0)\n+  }\n+\n+  test(\"Should not close a producer in-use.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer: CachedKafkaProducer = CachedKafkaProducer.acquire(kafkaParams)\n+    producer.kafkaProducer // initializing the producer.\n+    assert(producer.getInUseCount == 1)\n+    // Explicitly cause the producer from guava cache to be evicted.\n+    CachedKafkaProducer.evict(producer.getKafkaParams)\n+    assert(producer.getInUseCount == 1)\n+    assert(!producer.isClosed, \"An in-use producer should not be closed.\")\n+  }\n+\n+  private def generateKafkaParams: ju.HashMap[String, Object] = {\n     val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n+    kafkaParams.put(\"ack\", \"0\")\n     kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n     kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n     kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    kafkaParams.put(\"acks\", \"1\")\n-    val producer2: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    // With updated conf, a new producer instance should be created.\n-    assert(producer != producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map.size == 2)\n+    kafkaParams\n+  }\n+}\n+\n+class CachedKafkaProducerStressSuite extends KafkaContinuousTest with KafkaTest {",
    "line": 118
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "Interestingly, I too have a MacOS Mojave, and never saw that error. BTW, are you still getting it with the updated test? In that case, I can lower the number of producers further.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-03T09:44:32Z",
    "diffHunk": "@@ -18,60 +18,206 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n+import java.util.concurrent.{ConcurrentLinkedQueue, Executors, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.util.Random\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.Trigger\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n \n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"ack\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Automatically remove a failing kafka producer from cache.\") {\n+    import testImplicits._\n+    val df = Seq[(String, String)](null.asInstanceOf[String] -> \"1\").toDF(\"topic\", \"value\")\n+    val ex = intercept[SparkException] {\n+      // This will fail because the service is not reachable.\n+      df.write\n+        .format(\"kafka\")\n+        .option(\"topic\", \"topic\")\n+        .option(\"kafka.retries\", \"1\")\n+        .option(\"kafka.max.block.ms\", \"2\")\n+        .option(\"kafka.bootstrap.servers\", \"12.0.0.1:39022\")\n+        .save()\n+    }\n+    assert(ex.getMessage.contains(\"org.apache.kafka.common.errors.TimeoutException\"),\n+      \"Spark command should fail due to service not reachable.\")\n+    // Since failing kafka producer is released on error and also invalidated, it should not be in\n+    // cache.\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 0)\n+  }\n+\n+  test(\"Should not close a producer in-use.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer: CachedKafkaProducer = CachedKafkaProducer.acquire(kafkaParams)\n+    producer.kafkaProducer // initializing the producer.\n+    assert(producer.getInUseCount == 1)\n+    // Explicitly cause the producer from guava cache to be evicted.\n+    CachedKafkaProducer.evict(producer.getKafkaParams)\n+    assert(producer.getInUseCount == 1)\n+    assert(!producer.isClosed, \"An in-use producer should not be closed.\")\n+  }\n+\n+  private def generateKafkaParams: ju.HashMap[String, Object] = {\n     val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n+    kafkaParams.put(\"ack\", \"0\")\n     kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n     kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n     kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    kafkaParams.put(\"acks\", \"1\")\n-    val producer2: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    // With updated conf, a new producer instance should be created.\n-    assert(producer != producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map.size == 2)\n+    kafkaParams\n+  }\n+}\n+\n+class CachedKafkaProducerStressSuite extends KafkaContinuousTest with KafkaTest {",
    "line": 118
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Re-tested `test(\"concurrent use of CachedKafkaProducer\")` and after ~10239 open file descriptors the same exception came. In `test(\"SPARK-23623: concurrent use of KafkaDataConsumer\")` this number is fluctuating around 500. You can check it by adding the following code after `aquire`:\r\n```\r\nimport java.lang.management.ManagementFactory\r\nimport com.sun.management.UnixOperatingSystemMXBean\r\nSystem.out.println(\"Number of open fd: \" + ManagementFactory.getOperatingSystemMXBean.asInstanceOf[UnixOperatingSystemMXBean].getOpenFileDescriptorCount)\r\n```\r\n",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-04T10:39:52Z",
    "diffHunk": "@@ -18,60 +18,206 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n+import java.util.concurrent.{ConcurrentLinkedQueue, Executors, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.util.Random\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.Trigger\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n \n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"ack\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Automatically remove a failing kafka producer from cache.\") {\n+    import testImplicits._\n+    val df = Seq[(String, String)](null.asInstanceOf[String] -> \"1\").toDF(\"topic\", \"value\")\n+    val ex = intercept[SparkException] {\n+      // This will fail because the service is not reachable.\n+      df.write\n+        .format(\"kafka\")\n+        .option(\"topic\", \"topic\")\n+        .option(\"kafka.retries\", \"1\")\n+        .option(\"kafka.max.block.ms\", \"2\")\n+        .option(\"kafka.bootstrap.servers\", \"12.0.0.1:39022\")\n+        .save()\n+    }\n+    assert(ex.getMessage.contains(\"org.apache.kafka.common.errors.TimeoutException\"),\n+      \"Spark command should fail due to service not reachable.\")\n+    // Since failing kafka producer is released on error and also invalidated, it should not be in\n+    // cache.\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 0)\n+  }\n+\n+  test(\"Should not close a producer in-use.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer: CachedKafkaProducer = CachedKafkaProducer.acquire(kafkaParams)\n+    producer.kafkaProducer // initializing the producer.\n+    assert(producer.getInUseCount == 1)\n+    // Explicitly cause the producer from guava cache to be evicted.\n+    CachedKafkaProducer.evict(producer.getKafkaParams)\n+    assert(producer.getInUseCount == 1)\n+    assert(!producer.isClosed, \"An in-use producer should not be closed.\")\n+  }\n+\n+  private def generateKafkaParams: ju.HashMap[String, Object] = {\n     val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n+    kafkaParams.put(\"ack\", \"0\")\n     kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n     kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n     kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    kafkaParams.put(\"acks\", \"1\")\n-    val producer2: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    // With updated conf, a new producer instance should be created.\n-    assert(producer != producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map.size == 2)\n+    kafkaParams\n+  }\n+}\n+\n+class CachedKafkaProducerStressSuite extends KafkaContinuousTest with KafkaTest {",
    "line": 118
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "I've done the following to fix this:\r\n```\r\n    val data = (1 to 100).map(_.toString)\r\n...\r\n      val futuresAcquire = (1 to 10 * numConcurrentProducers).map { i =>\r\n        threadPool.submit(new Runnable {\r\n          override def run(): Unit = {\r\n            val producer = acquire(i % numConcurrentProducers + 1)\r\n            data.foreach { d =>\r\n              val record = new ProducerRecord[Array[Byte], Array[Byte]](topic, 0, null, d.getBytes)\r\n              producer.kafkaProducer.send(record)\r\n            }\r\n            release(producer)\r\n          }\r\n        })\r\n      }\r\n```\r\n`toBeReleasedQueue` removed completely.\r\nThis way open files is fluctuating around 500-600 and more real life scenario.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-04T10:59:45Z",
    "diffHunk": "@@ -18,60 +18,206 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n+import java.util.concurrent.{ConcurrentLinkedQueue, Executors, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.util.Random\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.Trigger\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n \n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"ack\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Automatically remove a failing kafka producer from cache.\") {\n+    import testImplicits._\n+    val df = Seq[(String, String)](null.asInstanceOf[String] -> \"1\").toDF(\"topic\", \"value\")\n+    val ex = intercept[SparkException] {\n+      // This will fail because the service is not reachable.\n+      df.write\n+        .format(\"kafka\")\n+        .option(\"topic\", \"topic\")\n+        .option(\"kafka.retries\", \"1\")\n+        .option(\"kafka.max.block.ms\", \"2\")\n+        .option(\"kafka.bootstrap.servers\", \"12.0.0.1:39022\")\n+        .save()\n+    }\n+    assert(ex.getMessage.contains(\"org.apache.kafka.common.errors.TimeoutException\"),\n+      \"Spark command should fail due to service not reachable.\")\n+    // Since failing kafka producer is released on error and also invalidated, it should not be in\n+    // cache.\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 0)\n+  }\n+\n+  test(\"Should not close a producer in-use.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer: CachedKafkaProducer = CachedKafkaProducer.acquire(kafkaParams)\n+    producer.kafkaProducer // initializing the producer.\n+    assert(producer.getInUseCount == 1)\n+    // Explicitly cause the producer from guava cache to be evicted.\n+    CachedKafkaProducer.evict(producer.getKafkaParams)\n+    assert(producer.getInUseCount == 1)\n+    assert(!producer.isClosed, \"An in-use producer should not be closed.\")\n+  }\n+\n+  private def generateKafkaParams: ju.HashMap[String, Object] = {\n     val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n+    kafkaParams.put(\"ack\", \"0\")\n     kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n     kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n     kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    kafkaParams.put(\"acks\", \"1\")\n-    val producer2: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    // With updated conf, a new producer instance should be created.\n-    assert(producer != producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map.size == 2)\n+    kafkaParams\n+  }\n+}\n+\n+class CachedKafkaProducerStressSuite extends KafkaContinuousTest with KafkaTest {",
    "line": 118
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Maybe Random.nextBoolean() can be used.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-27T11:28:03Z",
    "diffHunk": "@@ -18,60 +18,206 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n+import java.util.concurrent.{ConcurrentLinkedQueue, Executors, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.util.Random\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.Trigger\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n \n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"ack\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Automatically remove a failing kafka producer from cache.\") {\n+    import testImplicits._\n+    val df = Seq[(String, String)](null.asInstanceOf[String] -> \"1\").toDF(\"topic\", \"value\")\n+    val ex = intercept[SparkException] {\n+      // This will fail because the service is not reachable.\n+      df.write\n+        .format(\"kafka\")\n+        .option(\"topic\", \"topic\")\n+        .option(\"kafka.retries\", \"1\")\n+        .option(\"kafka.max.block.ms\", \"2\")\n+        .option(\"kafka.bootstrap.servers\", \"12.0.0.1:39022\")\n+        .save()\n+    }\n+    assert(ex.getMessage.contains(\"org.apache.kafka.common.errors.TimeoutException\"),\n+      \"Spark command should fail due to service not reachable.\")\n+    // Since failing kafka producer is released on error and also invalidated, it should not be in\n+    // cache.\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 0)\n+  }\n+\n+  test(\"Should not close a producer in-use.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer: CachedKafkaProducer = CachedKafkaProducer.acquire(kafkaParams)\n+    producer.kafkaProducer // initializing the producer.\n+    assert(producer.getInUseCount == 1)\n+    // Explicitly cause the producer from guava cache to be evicted.\n+    CachedKafkaProducer.evict(producer.getKafkaParams)\n+    assert(producer.getInUseCount == 1)\n+    assert(!producer.isClosed, \"An in-use producer should not be closed.\")\n+  }\n+\n+  private def generateKafkaParams: ju.HashMap[String, Object] = {\n     val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n+    kafkaParams.put(\"ack\", \"0\")\n     kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n     kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n     kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    kafkaParams.put(\"acks\", \"1\")\n-    val producer2: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    // With updated conf, a new producer instance should be created.\n-    assert(producer != producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map.size == 2)\n+    kafkaParams\n+  }\n+}\n+\n+class CachedKafkaProducerStressSuite extends KafkaContinuousTest with KafkaTest {\n+\n+  override val brokerProps = Map(\"auto.create.topics.enable\" -> \"false\")\n+\n+  override def afterAll(): Unit = {\n+    if (testUtils != null) {\n+      testUtils.teardown()\n+      testUtils = null\n+    }\n+    super.afterAll()\n+  }\n+\n+  override def sparkConf: SparkConf = {\n+    val conf = super.sparkConf\n+    conf.set(\"spark.kafka.producer.cache.timeout\", \"2ms\")\n+  }\n+\n+  test(\"concurrent use of CachedKafkaProducer\") {\n+    val topic = \"topic\" + Random.nextInt()\n+    val data = (1 to 10).map(_.toString)\n+    testUtils.createTopic(topic, 1)\n+    val kafkaParams: Map[String, Object] = Map(\"bootstrap.servers\" -> testUtils.brokerAddress,\n+      \"key.serializer\" -> classOf[ByteArraySerializer].getName,\n+      \"value.serializer\" -> classOf[ByteArraySerializer].getName)\n \n-    CachedKafkaProducer.close(kafkaParams)\n-    val map2 = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map2.size == 1)\n     import scala.collection.JavaConverters._\n-    val (seq: Seq[(String, Object)], _producer: KP) = map2.asScala.toArray.apply(0)\n-    assert(_producer == producer)\n+\n+    val numThreads: Int = 100\n+    val numConcurrentProducers: Int = 1000\n+\n+    val kafkaParamsUniqueMap = mutable.HashMap.empty[Int, ju.Map[String, Object]]\n+    ( 1 to numConcurrentProducers).map {\n+      i => kafkaParamsUniqueMap.put(i, kafkaParams.updated(\"retries\", s\"$i\").asJava)\n+    }\n+    val toBeReleasedQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+    def acquire(i: Int): Unit = {\n+      val producer = CachedKafkaProducer.acquire(kafkaParamsUniqueMap(i))\n+      producer.kafkaProducer // materialize producer for the first time.\n+      assert(!producer.isClosed, \"Acquired producer cannot be closed.\")\n+      toBeReleasedQueue.add(producer)\n+    }\n+\n+    def release(producer: CachedKafkaProducer): Unit = {\n+      if (producer != null) {\n+        CachedKafkaProducer.release(producer, true)"
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "`CachedKafkaProducer.clear()` would be good somewhere because proper clean happens only on happy path.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-27T11:30:58Z",
    "diffHunk": "@@ -18,60 +18,206 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n+import java.util.concurrent.{ConcurrentLinkedQueue, Executors, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.util.Random\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.Trigger\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n \n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"ack\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Automatically remove a failing kafka producer from cache.\") {\n+    import testImplicits._\n+    val df = Seq[(String, String)](null.asInstanceOf[String] -> \"1\").toDF(\"topic\", \"value\")\n+    val ex = intercept[SparkException] {\n+      // This will fail because the service is not reachable.\n+      df.write\n+        .format(\"kafka\")\n+        .option(\"topic\", \"topic\")\n+        .option(\"kafka.retries\", \"1\")\n+        .option(\"kafka.max.block.ms\", \"2\")\n+        .option(\"kafka.bootstrap.servers\", \"12.0.0.1:39022\")\n+        .save()\n+    }\n+    assert(ex.getMessage.contains(\"org.apache.kafka.common.errors.TimeoutException\"),\n+      \"Spark command should fail due to service not reachable.\")\n+    // Since failing kafka producer is released on error and also invalidated, it should not be in\n+    // cache.\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 0)\n+  }\n+\n+  test(\"Should not close a producer in-use.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer: CachedKafkaProducer = CachedKafkaProducer.acquire(kafkaParams)\n+    producer.kafkaProducer // initializing the producer.\n+    assert(producer.getInUseCount == 1)\n+    // Explicitly cause the producer from guava cache to be evicted.\n+    CachedKafkaProducer.evict(producer.getKafkaParams)\n+    assert(producer.getInUseCount == 1)\n+    assert(!producer.isClosed, \"An in-use producer should not be closed.\")\n+  }\n+\n+  private def generateKafkaParams: ju.HashMap[String, Object] = {\n     val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n+    kafkaParams.put(\"ack\", \"0\")\n     kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n     kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n     kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    kafkaParams.put(\"acks\", \"1\")\n-    val producer2: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    // With updated conf, a new producer instance should be created.\n-    assert(producer != producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map.size == 2)\n+    kafkaParams\n+  }\n+}\n+\n+class CachedKafkaProducerStressSuite extends KafkaContinuousTest with KafkaTest {\n+\n+  override val brokerProps = Map(\"auto.create.topics.enable\" -> \"false\")\n+\n+  override def afterAll(): Unit = {\n+    if (testUtils != null) {\n+      testUtils.teardown()\n+      testUtils = null\n+    }\n+    super.afterAll()\n+  }\n+\n+  override def sparkConf: SparkConf = {\n+    val conf = super.sparkConf\n+    conf.set(\"spark.kafka.producer.cache.timeout\", \"2ms\")\n+  }\n+\n+  test(\"concurrent use of CachedKafkaProducer\") {\n+    val topic = \"topic\" + Random.nextInt()\n+    val data = (1 to 10).map(_.toString)\n+    testUtils.createTopic(topic, 1)\n+    val kafkaParams: Map[String, Object] = Map(\"bootstrap.servers\" -> testUtils.brokerAddress,\n+      \"key.serializer\" -> classOf[ByteArraySerializer].getName,\n+      \"value.serializer\" -> classOf[ByteArraySerializer].getName)\n \n-    CachedKafkaProducer.close(kafkaParams)\n-    val map2 = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map2.size == 1)\n     import scala.collection.JavaConverters._\n-    val (seq: Seq[(String, Object)], _producer: KP) = map2.asScala.toArray.apply(0)\n-    assert(_producer == producer)\n+\n+    val numThreads: Int = 100\n+    val numConcurrentProducers: Int = 1000\n+\n+    val kafkaParamsUniqueMap = mutable.HashMap.empty[Int, ju.Map[String, Object]]\n+    ( 1 to numConcurrentProducers).map {\n+      i => kafkaParamsUniqueMap.put(i, kafkaParams.updated(\"retries\", s\"$i\").asJava)\n+    }\n+    val toBeReleasedQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+    def acquire(i: Int): Unit = {\n+      val producer = CachedKafkaProducer.acquire(kafkaParamsUniqueMap(i))\n+      producer.kafkaProducer // materialize producer for the first time.\n+      assert(!producer.isClosed, \"Acquired producer cannot be closed.\")\n+      toBeReleasedQueue.add(producer)\n+    }\n+\n+    def release(producer: CachedKafkaProducer): Unit = {\n+      if (producer != null) {\n+        CachedKafkaProducer.release(producer, true)\n+        if (producer.getInUseCount > 0) {\n+          assert(!producer.isClosed, \"Should not close an inuse producer.\")\n+        }\n+      }\n+    }\n+    val threadPool = Executors.newFixedThreadPool(numThreads)\n+    try {\n+      val futuresAcquire = (1 to 10 * numConcurrentProducers).map { i =>\n+        threadPool.submit(new Runnable {\n+          override def run(): Unit = {\n+            acquire(i % numConcurrentProducers + 1)\n+          }\n+        })\n+      }\n+      val futuresRelease = (1 to 10 * numConcurrentProducers).map { i =>\n+        val cachedKafkaProducer = toBeReleasedQueue.poll()\n+        // 2x release should not corrupt the state of cache.\n+        (1 to 2).map { j =>\n+          threadPool.submit(new Runnable {\n+            override def run(): Unit = {\n+              release(cachedKafkaProducer)\n+            }\n+          })\n+        }\n+      }\n+      futuresAcquire.foreach(_.get(1, TimeUnit.MINUTES))\n+      futuresRelease.flatten.foreach(_.get(1, TimeUnit.MINUTES))\n+    } finally {\n+      threadPool.shutdown()",
    "line": 182
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "In case of deadlock this test never stopped on my machine.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-27T11:33:52Z",
    "diffHunk": "@@ -18,60 +18,206 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n+import java.util.concurrent.{ConcurrentLinkedQueue, Executors, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.util.Random\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.Trigger\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n \n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"ack\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Automatically remove a failing kafka producer from cache.\") {\n+    import testImplicits._\n+    val df = Seq[(String, String)](null.asInstanceOf[String] -> \"1\").toDF(\"topic\", \"value\")\n+    val ex = intercept[SparkException] {\n+      // This will fail because the service is not reachable.\n+      df.write\n+        .format(\"kafka\")\n+        .option(\"topic\", \"topic\")\n+        .option(\"kafka.retries\", \"1\")\n+        .option(\"kafka.max.block.ms\", \"2\")\n+        .option(\"kafka.bootstrap.servers\", \"12.0.0.1:39022\")\n+        .save()\n+    }\n+    assert(ex.getMessage.contains(\"org.apache.kafka.common.errors.TimeoutException\"),\n+      \"Spark command should fail due to service not reachable.\")\n+    // Since failing kafka producer is released on error and also invalidated, it should not be in\n+    // cache.\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 0)\n+  }\n+\n+  test(\"Should not close a producer in-use.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer: CachedKafkaProducer = CachedKafkaProducer.acquire(kafkaParams)\n+    producer.kafkaProducer // initializing the producer.\n+    assert(producer.getInUseCount == 1)\n+    // Explicitly cause the producer from guava cache to be evicted.\n+    CachedKafkaProducer.evict(producer.getKafkaParams)\n+    assert(producer.getInUseCount == 1)\n+    assert(!producer.isClosed, \"An in-use producer should not be closed.\")\n+  }\n+\n+  private def generateKafkaParams: ju.HashMap[String, Object] = {\n     val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n+    kafkaParams.put(\"ack\", \"0\")\n     kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n     kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n     kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    kafkaParams.put(\"acks\", \"1\")\n-    val producer2: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    // With updated conf, a new producer instance should be created.\n-    assert(producer != producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map.size == 2)\n+    kafkaParams\n+  }\n+}\n+\n+class CachedKafkaProducerStressSuite extends KafkaContinuousTest with KafkaTest {\n+\n+  override val brokerProps = Map(\"auto.create.topics.enable\" -> \"false\")\n+\n+  override def afterAll(): Unit = {\n+    if (testUtils != null) {\n+      testUtils.teardown()\n+      testUtils = null\n+    }\n+    super.afterAll()\n+  }\n+\n+  override def sparkConf: SparkConf = {\n+    val conf = super.sparkConf\n+    conf.set(\"spark.kafka.producer.cache.timeout\", \"2ms\")\n+  }\n+\n+  test(\"concurrent use of CachedKafkaProducer\") {\n+    val topic = \"topic\" + Random.nextInt()\n+    val data = (1 to 10).map(_.toString)\n+    testUtils.createTopic(topic, 1)\n+    val kafkaParams: Map[String, Object] = Map(\"bootstrap.servers\" -> testUtils.brokerAddress,\n+      \"key.serializer\" -> classOf[ByteArraySerializer].getName,\n+      \"value.serializer\" -> classOf[ByteArraySerializer].getName)\n \n-    CachedKafkaProducer.close(kafkaParams)\n-    val map2 = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map2.size == 1)\n     import scala.collection.JavaConverters._\n-    val (seq: Seq[(String, Object)], _producer: KP) = map2.asScala.toArray.apply(0)\n-    assert(_producer == producer)\n+\n+    val numThreads: Int = 100\n+    val numConcurrentProducers: Int = 1000\n+\n+    val kafkaParamsUniqueMap = mutable.HashMap.empty[Int, ju.Map[String, Object]]\n+    ( 1 to numConcurrentProducers).map {\n+      i => kafkaParamsUniqueMap.put(i, kafkaParams.updated(\"retries\", s\"$i\").asJava)\n+    }\n+    val toBeReleasedQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+    def acquire(i: Int): Unit = {\n+      val producer = CachedKafkaProducer.acquire(kafkaParamsUniqueMap(i))\n+      producer.kafkaProducer // materialize producer for the first time.\n+      assert(!producer.isClosed, \"Acquired producer cannot be closed.\")\n+      toBeReleasedQueue.add(producer)\n+    }\n+\n+    def release(producer: CachedKafkaProducer): Unit = {\n+      if (producer != null) {\n+        CachedKafkaProducer.release(producer, true)\n+        if (producer.getInUseCount > 0) {\n+          assert(!producer.isClosed, \"Should not close an inuse producer.\")\n+        }\n+      }\n+    }\n+    val threadPool = Executors.newFixedThreadPool(numThreads)\n+    try {\n+      val futuresAcquire = (1 to 10 * numConcurrentProducers).map { i =>\n+        threadPool.submit(new Runnable {\n+          override def run(): Unit = {\n+            acquire(i % numConcurrentProducers + 1)\n+          }\n+        })\n+      }\n+      val futuresRelease = (1 to 10 * numConcurrentProducers).map { i =>\n+        val cachedKafkaProducer = toBeReleasedQueue.poll()\n+        // 2x release should not corrupt the state of cache.\n+        (1 to 2).map { j =>\n+          threadPool.submit(new Runnable {\n+            override def run(): Unit = {\n+              release(cachedKafkaProducer)\n+            }\n+          })\n+        }\n+      }\n+      futuresAcquire.foreach(_.get(1, TimeUnit.MINUTES))\n+      futuresRelease.flatten.foreach(_.get(1, TimeUnit.MINUTES))\n+    } finally {\n+      threadPool.shutdown()\n+    }\n+  }\n+\n+  /*\n+   * The following stress suite will cause frequent eviction of kafka producers from\n+   * the guava cache. Since these producers remain in use, because they are used by\n+   * multiple tasks, they stay in close queue till they are released finally. This test\n+   * will cause new tasks to use fresh instance of kafka producers and as a result it\n+   * simulates a stress situation, where multiple producers are requested from CachedKafkaProducer\n+   * and at the same time there will be multiple releases. It is supposed to catch a race\n+   * condition if any, due to multiple threads requesting and releasing producers.\n+   */\n+  test(\"Single source and multiple kafka sink with 2ms cache timeout.\") {",
    "line": 196
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "Is this problem resolved?",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-29T04:04:43Z",
    "diffHunk": "@@ -18,60 +18,206 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n+import java.util.concurrent.{ConcurrentLinkedQueue, Executors, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.util.Random\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.Trigger\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n \n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"ack\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Automatically remove a failing kafka producer from cache.\") {\n+    import testImplicits._\n+    val df = Seq[(String, String)](null.asInstanceOf[String] -> \"1\").toDF(\"topic\", \"value\")\n+    val ex = intercept[SparkException] {\n+      // This will fail because the service is not reachable.\n+      df.write\n+        .format(\"kafka\")\n+        .option(\"topic\", \"topic\")\n+        .option(\"kafka.retries\", \"1\")\n+        .option(\"kafka.max.block.ms\", \"2\")\n+        .option(\"kafka.bootstrap.servers\", \"12.0.0.1:39022\")\n+        .save()\n+    }\n+    assert(ex.getMessage.contains(\"org.apache.kafka.common.errors.TimeoutException\"),\n+      \"Spark command should fail due to service not reachable.\")\n+    // Since failing kafka producer is released on error and also invalidated, it should not be in\n+    // cache.\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 0)\n+  }\n+\n+  test(\"Should not close a producer in-use.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer: CachedKafkaProducer = CachedKafkaProducer.acquire(kafkaParams)\n+    producer.kafkaProducer // initializing the producer.\n+    assert(producer.getInUseCount == 1)\n+    // Explicitly cause the producer from guava cache to be evicted.\n+    CachedKafkaProducer.evict(producer.getKafkaParams)\n+    assert(producer.getInUseCount == 1)\n+    assert(!producer.isClosed, \"An in-use producer should not be closed.\")\n+  }\n+\n+  private def generateKafkaParams: ju.HashMap[String, Object] = {\n     val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n+    kafkaParams.put(\"ack\", \"0\")\n     kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n     kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n     kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    kafkaParams.put(\"acks\", \"1\")\n-    val producer2: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    // With updated conf, a new producer instance should be created.\n-    assert(producer != producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map.size == 2)\n+    kafkaParams\n+  }\n+}\n+\n+class CachedKafkaProducerStressSuite extends KafkaContinuousTest with KafkaTest {\n+\n+  override val brokerProps = Map(\"auto.create.topics.enable\" -> \"false\")\n+\n+  override def afterAll(): Unit = {\n+    if (testUtils != null) {\n+      testUtils.teardown()\n+      testUtils = null\n+    }\n+    super.afterAll()\n+  }\n+\n+  override def sparkConf: SparkConf = {\n+    val conf = super.sparkConf\n+    conf.set(\"spark.kafka.producer.cache.timeout\", \"2ms\")\n+  }\n+\n+  test(\"concurrent use of CachedKafkaProducer\") {\n+    val topic = \"topic\" + Random.nextInt()\n+    val data = (1 to 10).map(_.toString)\n+    testUtils.createTopic(topic, 1)\n+    val kafkaParams: Map[String, Object] = Map(\"bootstrap.servers\" -> testUtils.brokerAddress,\n+      \"key.serializer\" -> classOf[ByteArraySerializer].getName,\n+      \"value.serializer\" -> classOf[ByteArraySerializer].getName)\n \n-    CachedKafkaProducer.close(kafkaParams)\n-    val map2 = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map2.size == 1)\n     import scala.collection.JavaConverters._\n-    val (seq: Seq[(String, Object)], _producer: KP) = map2.asScala.toArray.apply(0)\n-    assert(_producer == producer)\n+\n+    val numThreads: Int = 100\n+    val numConcurrentProducers: Int = 1000\n+\n+    val kafkaParamsUniqueMap = mutable.HashMap.empty[Int, ju.Map[String, Object]]\n+    ( 1 to numConcurrentProducers).map {\n+      i => kafkaParamsUniqueMap.put(i, kafkaParams.updated(\"retries\", s\"$i\").asJava)\n+    }\n+    val toBeReleasedQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+    def acquire(i: Int): Unit = {\n+      val producer = CachedKafkaProducer.acquire(kafkaParamsUniqueMap(i))\n+      producer.kafkaProducer // materialize producer for the first time.\n+      assert(!producer.isClosed, \"Acquired producer cannot be closed.\")\n+      toBeReleasedQueue.add(producer)\n+    }\n+\n+    def release(producer: CachedKafkaProducer): Unit = {\n+      if (producer != null) {\n+        CachedKafkaProducer.release(producer, true)\n+        if (producer.getInUseCount > 0) {\n+          assert(!producer.isClosed, \"Should not close an inuse producer.\")\n+        }\n+      }\n+    }\n+    val threadPool = Executors.newFixedThreadPool(numThreads)\n+    try {\n+      val futuresAcquire = (1 to 10 * numConcurrentProducers).map { i =>\n+        threadPool.submit(new Runnable {\n+          override def run(): Unit = {\n+            acquire(i % numConcurrentProducers + 1)\n+          }\n+        })\n+      }\n+      val futuresRelease = (1 to 10 * numConcurrentProducers).map { i =>\n+        val cachedKafkaProducer = toBeReleasedQueue.poll()\n+        // 2x release should not corrupt the state of cache.\n+        (1 to 2).map { j =>\n+          threadPool.submit(new Runnable {\n+            override def run(): Unit = {\n+              release(cachedKafkaProducer)\n+            }\n+          })\n+        }\n+      }\n+      futuresAcquire.foreach(_.get(1, TimeUnit.MINUTES))\n+      futuresRelease.flatten.foreach(_.get(1, TimeUnit.MINUTES))\n+    } finally {\n+      threadPool.shutdown()\n+    }\n+  }\n+\n+  /*\n+   * The following stress suite will cause frequent eviction of kafka producers from\n+   * the guava cache. Since these producers remain in use, because they are used by\n+   * multiple tasks, they stay in close queue till they are released finally. This test\n+   * will cause new tasks to use fresh instance of kafka producers and as a result it\n+   * simulates a stress situation, where multiple producers are requested from CachedKafkaProducer\n+   * and at the same time there will be multiple releases. It is supposed to catch a race\n+   * condition if any, due to multiple threads requesting and releasing producers.\n+   */\n+  test(\"Single source and multiple kafka sink with 2ms cache timeout.\") {",
    "line": 196
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Yeah, now the problem solved and runs fine. Thanks your efforts!",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-10T13:32:25Z",
    "diffHunk": "@@ -18,60 +18,206 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n+import java.util.concurrent.{ConcurrentLinkedQueue, Executors, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.util.Random\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.Trigger\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n \n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"ack\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Automatically remove a failing kafka producer from cache.\") {\n+    import testImplicits._\n+    val df = Seq[(String, String)](null.asInstanceOf[String] -> \"1\").toDF(\"topic\", \"value\")\n+    val ex = intercept[SparkException] {\n+      // This will fail because the service is not reachable.\n+      df.write\n+        .format(\"kafka\")\n+        .option(\"topic\", \"topic\")\n+        .option(\"kafka.retries\", \"1\")\n+        .option(\"kafka.max.block.ms\", \"2\")\n+        .option(\"kafka.bootstrap.servers\", \"12.0.0.1:39022\")\n+        .save()\n+    }\n+    assert(ex.getMessage.contains(\"org.apache.kafka.common.errors.TimeoutException\"),\n+      \"Spark command should fail due to service not reachable.\")\n+    // Since failing kafka producer is released on error and also invalidated, it should not be in\n+    // cache.\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 0)\n+  }\n+\n+  test(\"Should not close a producer in-use.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer: CachedKafkaProducer = CachedKafkaProducer.acquire(kafkaParams)\n+    producer.kafkaProducer // initializing the producer.\n+    assert(producer.getInUseCount == 1)\n+    // Explicitly cause the producer from guava cache to be evicted.\n+    CachedKafkaProducer.evict(producer.getKafkaParams)\n+    assert(producer.getInUseCount == 1)\n+    assert(!producer.isClosed, \"An in-use producer should not be closed.\")\n+  }\n+\n+  private def generateKafkaParams: ju.HashMap[String, Object] = {\n     val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n+    kafkaParams.put(\"ack\", \"0\")\n     kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n     kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n     kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    kafkaParams.put(\"acks\", \"1\")\n-    val producer2: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    // With updated conf, a new producer instance should be created.\n-    assert(producer != producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map.size == 2)\n+    kafkaParams\n+  }\n+}\n+\n+class CachedKafkaProducerStressSuite extends KafkaContinuousTest with KafkaTest {\n+\n+  override val brokerProps = Map(\"auto.create.topics.enable\" -> \"false\")\n+\n+  override def afterAll(): Unit = {\n+    if (testUtils != null) {\n+      testUtils.teardown()\n+      testUtils = null\n+    }\n+    super.afterAll()\n+  }\n+\n+  override def sparkConf: SparkConf = {\n+    val conf = super.sparkConf\n+    conf.set(\"spark.kafka.producer.cache.timeout\", \"2ms\")\n+  }\n+\n+  test(\"concurrent use of CachedKafkaProducer\") {\n+    val topic = \"topic\" + Random.nextInt()\n+    val data = (1 to 10).map(_.toString)\n+    testUtils.createTopic(topic, 1)\n+    val kafkaParams: Map[String, Object] = Map(\"bootstrap.servers\" -> testUtils.brokerAddress,\n+      \"key.serializer\" -> classOf[ByteArraySerializer].getName,\n+      \"value.serializer\" -> classOf[ByteArraySerializer].getName)\n \n-    CachedKafkaProducer.close(kafkaParams)\n-    val map2 = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map2.size == 1)\n     import scala.collection.JavaConverters._\n-    val (seq: Seq[(String, Object)], _producer: KP) = map2.asScala.toArray.apply(0)\n-    assert(_producer == producer)\n+\n+    val numThreads: Int = 100\n+    val numConcurrentProducers: Int = 1000\n+\n+    val kafkaParamsUniqueMap = mutable.HashMap.empty[Int, ju.Map[String, Object]]\n+    ( 1 to numConcurrentProducers).map {\n+      i => kafkaParamsUniqueMap.put(i, kafkaParams.updated(\"retries\", s\"$i\").asJava)\n+    }\n+    val toBeReleasedQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+    def acquire(i: Int): Unit = {\n+      val producer = CachedKafkaProducer.acquire(kafkaParamsUniqueMap(i))\n+      producer.kafkaProducer // materialize producer for the first time.\n+      assert(!producer.isClosed, \"Acquired producer cannot be closed.\")\n+      toBeReleasedQueue.add(producer)\n+    }\n+\n+    def release(producer: CachedKafkaProducer): Unit = {\n+      if (producer != null) {\n+        CachedKafkaProducer.release(producer, true)\n+        if (producer.getInUseCount > 0) {\n+          assert(!producer.isClosed, \"Should not close an inuse producer.\")\n+        }\n+      }\n+    }\n+    val threadPool = Executors.newFixedThreadPool(numThreads)\n+    try {\n+      val futuresAcquire = (1 to 10 * numConcurrentProducers).map { i =>\n+        threadPool.submit(new Runnable {\n+          override def run(): Unit = {\n+            acquire(i % numConcurrentProducers + 1)\n+          }\n+        })\n+      }\n+      val futuresRelease = (1 to 10 * numConcurrentProducers).map { i =>\n+        val cachedKafkaProducer = toBeReleasedQueue.poll()\n+        // 2x release should not corrupt the state of cache.\n+        (1 to 2).map { j =>\n+          threadPool.submit(new Runnable {\n+            override def run(): Unit = {\n+              release(cachedKafkaProducer)\n+            }\n+          })\n+        }\n+      }\n+      futuresAcquire.foreach(_.get(1, TimeUnit.MINUTES))\n+      futuresRelease.flatten.foreach(_.get(1, TimeUnit.MINUTES))\n+    } finally {\n+      threadPool.shutdown()\n+    }\n+  }\n+\n+  /*\n+   * The following stress suite will cause frequent eviction of kafka producers from\n+   * the guava cache. Since these producers remain in use, because they are used by\n+   * multiple tasks, they stay in close queue till they are released finally. This test\n+   * will cause new tasks to use fresh instance of kafka producers and as a result it\n+   * simulates a stress situation, where multiple producers are requested from CachedKafkaProducer\n+   * and at the same time there will be multiple releases. It is supposed to catch a race\n+   * condition if any, due to multiple threads requesting and releasing producers.\n+   */\n+  test(\"Single source and multiple kafka sink with 2ms cache timeout.\") {",
    "line": 196
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Queries won't be stopped if `q.exception.isEmpty` condition not fulfilled, will they?",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-28T13:40:42Z",
    "diffHunk": "@@ -18,60 +18,208 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n+import java.util.concurrent.{ConcurrentLinkedQueue, Executors, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.util.Random\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.Trigger\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n \n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"acks\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Automatically remove a failing kafka producer from cache.\") {\n+    import testImplicits._\n+    val df = Seq[(String, String)](null.asInstanceOf[String] -> \"1\").toDF(\"topic\", \"value\")\n+    val ex = intercept[SparkException] {\n+      // This will fail because the service is not reachable.\n+      df.write\n+        .format(\"kafka\")\n+        .option(\"topic\", \"topic\")\n+        .option(\"kafka.retries\", \"1\")\n+        .option(\"kafka.max.block.ms\", \"2\")\n+        .option(\"kafka.bootstrap.servers\", \"12.0.0.1:39022\")\n+        .save()\n+    }\n+    assert(ex.getMessage.contains(\"org.apache.kafka.common.errors.TimeoutException\"),\n+      \"Spark command should fail due to service not reachable.\")\n+    // Since failing kafka producer is released on error and also invalidated, it should not be in\n+    // cache.\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 0)\n+  }\n+\n+  test(\"Should not close a producer in-use.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer: CachedKafkaProducer = CachedKafkaProducer.acquire(kafkaParams)\n+    producer.kafkaProducer // initializing the producer.\n+    assert(producer.getInUseCount == 1)\n+    // Explicitly cause the producer from guava cache to be evicted.\n+    CachedKafkaProducer.evict(producer.getKafkaParams)\n+    assert(producer.getInUseCount == 1)\n+    assert(!producer.isClosed, \"An in-use producer should not be closed.\")\n+  }\n+\n+  private def generateKafkaParams: ju.HashMap[String, Object] = {\n     val kafkaParams = new ju.HashMap[String, Object]()\n     kafkaParams.put(\"acks\", \"0\")\n     kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n     kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n     kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    kafkaParams.put(\"acks\", \"1\")\n-    val producer2: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    // With updated conf, a new producer instance should be created.\n-    assert(producer != producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map.size == 2)\n+    kafkaParams\n+  }\n+}\n+\n+class CachedKafkaProducerStressSuite extends KafkaContinuousTest with KafkaTest {\n+\n+  override val brokerProps = Map(\"auto.create.topics.enable\" -> \"false\")\n+\n+  override def afterAll(): Unit = {\n+    if (testUtils != null) {\n+      testUtils.teardown()\n+      testUtils = null\n+    }\n+    super.afterAll()\n+  }\n+\n+  override def sparkConf: SparkConf = {\n+    val conf = super.sparkConf\n+    conf.set(\"spark.kafka.producer.cache.timeout\", \"2ms\")\n+  }\n+\n+  test(\"concurrent use of CachedKafkaProducer\") {\n+    val topic = \"topic\" + Random.nextInt()\n+    testUtils.createTopic(topic, 1)\n+    val kafkaParams: Map[String, Object] = Map(\"bootstrap.servers\" -> testUtils.brokerAddress,\n+      \"key.serializer\" -> classOf[ByteArraySerializer].getName,\n+      \"value.serializer\" -> classOf[ByteArraySerializer].getName)\n \n-    CachedKafkaProducer.close(kafkaParams)\n-    val map2 = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map2.size == 1)\n     import scala.collection.JavaConverters._\n-    val (seq: Seq[(String, Object)], _producer: KP) = map2.asScala.toArray.apply(0)\n-    assert(_producer == producer)\n+\n+    val numThreads = 100\n+    val numConcurrentProducers = 1000\n+\n+    val kafkaParamsUniqueMap = mutable.HashMap.empty[Int, ju.Map[String, Object]]\n+    (1 to numConcurrentProducers).map {\n+      i => kafkaParamsUniqueMap.put(i, kafkaParams.updated(\"retries\", s\"$i\").asJava)\n+    }\n+    val toBeReleasedQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+    def acquire(i: Int): Unit = {\n+      val producer = CachedKafkaProducer.acquire(kafkaParamsUniqueMap(i))\n+      producer.kafkaProducer // materialize producer for the first time.\n+      assert(!producer.isClosed, \"Acquired producer cannot be closed.\")\n+      toBeReleasedQueue.add(producer)\n+    }\n+\n+    def release(producer: CachedKafkaProducer): Unit = {\n+      if (producer != null) {\n+        CachedKafkaProducer.release(producer, Random.nextBoolean())\n+        if (producer.getInUseCount > 0) {\n+          assert(!producer.isClosed, \"Should not close an inuse producer.\")\n+        }\n+      }\n+    }\n+    val threadPool = Executors.newFixedThreadPool(numThreads)\n+    try {\n+      val futuresAcquire = (1 to 10 * numConcurrentProducers).map { i =>\n+        threadPool.submit(new Runnable {\n+          override def run(): Unit = {\n+            acquire(i % numConcurrentProducers + 1)\n+          }\n+        })\n+      }\n+      val futuresRelease = (1 to 10 * numConcurrentProducers).map { i =>\n+        val cachedKafkaProducer = toBeReleasedQueue.poll()\n+        // 2x release should not corrupt the state of cache.\n+        (1 to 2).map { j =>\n+          threadPool.submit(new Runnable {\n+            override def run(): Unit = {\n+              release(cachedKafkaProducer)\n+            }\n+          })\n+        }\n+      }\n+      futuresAcquire.foreach(_.get(1, TimeUnit.MINUTES))\n+      futuresRelease.flatten.foreach(_.get(1, TimeUnit.MINUTES))\n+    } finally {\n+      threadPool.shutdown()\n+      CachedKafkaProducer.clear()\n+    }\n+  }\n+\n+  /*\n+   * The following stress suite will cause frequent eviction of kafka producers from\n+   * the guava cache. Since these producers remain in use, because they are used by\n+   * multiple tasks, they stay in close queue till they are released finally. This test\n+   * will cause new tasks to use fresh instance of kafka producers and as a result it\n+   * simulates a stress situation, where multiple producers are requested from CachedKafkaProducer\n+   * and at the same time there will be multiple releases. It is supposed to catch a race\n+   * condition if any, due to multiple threads requesting and releasing producers.\n+   */\n+  test(\"Single source and multiple kafka sink with 2ms cache timeout.\") {\n+\n+    val df = spark.readStream\n+      .format(\"rate\")\n+      .option(\"numPartitions\", \"100\")\n+      .option(\"rowsPerSecond\", \"200\")\n+      .load()\n+      .selectExpr(\"CAST(timestamp AS STRING) key\", \"CAST(value AS STRING) value\")\n+\n+    val checkpointDir = Utils.createTempDir()\n+    val topic = newTopic()\n+    testUtils.createTopic(topic, 100)\n+    failAfter(streamingTimeout) {\n+      val queries = for (i <- 1 to 10) yield {\n+        df.writeStream\n+          .format(\"kafka\")\n+          .option(\"checkpointLocation\", checkpointDir.getCanonicalPath + i)\n+          .option(\"kafka.bootstrap.servers\", testUtils.brokerAddress)\n+          // to make it create 5 unique producers.\n+          .option(\"kafka.max.block.ms\", s\"100${i % 5}\")\n+          .option(\"topic\", topic)\n+          .trigger(Trigger.Continuous(500))\n+          .queryName(s\"kafkaStream$i\")\n+          .start()\n+      }\n+      Thread.sleep(15000)\n+\n+      queries.foreach { q =>\n+        assert(q.exception.isEmpty, \"None of the queries should fail.\")\n+        q.stop()",
    "line": 231
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "There are existing tests, where querry.stop() occurs after the assert statement. Do you want me to ensure that q.stop is called inspite of a failure? ",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-29T04:49:46Z",
    "diffHunk": "@@ -18,60 +18,208 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n+import java.util.concurrent.{ConcurrentLinkedQueue, Executors, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.util.Random\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.Trigger\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n \n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"acks\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Automatically remove a failing kafka producer from cache.\") {\n+    import testImplicits._\n+    val df = Seq[(String, String)](null.asInstanceOf[String] -> \"1\").toDF(\"topic\", \"value\")\n+    val ex = intercept[SparkException] {\n+      // This will fail because the service is not reachable.\n+      df.write\n+        .format(\"kafka\")\n+        .option(\"topic\", \"topic\")\n+        .option(\"kafka.retries\", \"1\")\n+        .option(\"kafka.max.block.ms\", \"2\")\n+        .option(\"kafka.bootstrap.servers\", \"12.0.0.1:39022\")\n+        .save()\n+    }\n+    assert(ex.getMessage.contains(\"org.apache.kafka.common.errors.TimeoutException\"),\n+      \"Spark command should fail due to service not reachable.\")\n+    // Since failing kafka producer is released on error and also invalidated, it should not be in\n+    // cache.\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 0)\n+  }\n+\n+  test(\"Should not close a producer in-use.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer: CachedKafkaProducer = CachedKafkaProducer.acquire(kafkaParams)\n+    producer.kafkaProducer // initializing the producer.\n+    assert(producer.getInUseCount == 1)\n+    // Explicitly cause the producer from guava cache to be evicted.\n+    CachedKafkaProducer.evict(producer.getKafkaParams)\n+    assert(producer.getInUseCount == 1)\n+    assert(!producer.isClosed, \"An in-use producer should not be closed.\")\n+  }\n+\n+  private def generateKafkaParams: ju.HashMap[String, Object] = {\n     val kafkaParams = new ju.HashMap[String, Object]()\n     kafkaParams.put(\"acks\", \"0\")\n     kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n     kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n     kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    kafkaParams.put(\"acks\", \"1\")\n-    val producer2: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    // With updated conf, a new producer instance should be created.\n-    assert(producer != producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map.size == 2)\n+    kafkaParams\n+  }\n+}\n+\n+class CachedKafkaProducerStressSuite extends KafkaContinuousTest with KafkaTest {\n+\n+  override val brokerProps = Map(\"auto.create.topics.enable\" -> \"false\")\n+\n+  override def afterAll(): Unit = {\n+    if (testUtils != null) {\n+      testUtils.teardown()\n+      testUtils = null\n+    }\n+    super.afterAll()\n+  }\n+\n+  override def sparkConf: SparkConf = {\n+    val conf = super.sparkConf\n+    conf.set(\"spark.kafka.producer.cache.timeout\", \"2ms\")\n+  }\n+\n+  test(\"concurrent use of CachedKafkaProducer\") {\n+    val topic = \"topic\" + Random.nextInt()\n+    testUtils.createTopic(topic, 1)\n+    val kafkaParams: Map[String, Object] = Map(\"bootstrap.servers\" -> testUtils.brokerAddress,\n+      \"key.serializer\" -> classOf[ByteArraySerializer].getName,\n+      \"value.serializer\" -> classOf[ByteArraySerializer].getName)\n \n-    CachedKafkaProducer.close(kafkaParams)\n-    val map2 = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map2.size == 1)\n     import scala.collection.JavaConverters._\n-    val (seq: Seq[(String, Object)], _producer: KP) = map2.asScala.toArray.apply(0)\n-    assert(_producer == producer)\n+\n+    val numThreads = 100\n+    val numConcurrentProducers = 1000\n+\n+    val kafkaParamsUniqueMap = mutable.HashMap.empty[Int, ju.Map[String, Object]]\n+    (1 to numConcurrentProducers).map {\n+      i => kafkaParamsUniqueMap.put(i, kafkaParams.updated(\"retries\", s\"$i\").asJava)\n+    }\n+    val toBeReleasedQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+    def acquire(i: Int): Unit = {\n+      val producer = CachedKafkaProducer.acquire(kafkaParamsUniqueMap(i))\n+      producer.kafkaProducer // materialize producer for the first time.\n+      assert(!producer.isClosed, \"Acquired producer cannot be closed.\")\n+      toBeReleasedQueue.add(producer)\n+    }\n+\n+    def release(producer: CachedKafkaProducer): Unit = {\n+      if (producer != null) {\n+        CachedKafkaProducer.release(producer, Random.nextBoolean())\n+        if (producer.getInUseCount > 0) {\n+          assert(!producer.isClosed, \"Should not close an inuse producer.\")\n+        }\n+      }\n+    }\n+    val threadPool = Executors.newFixedThreadPool(numThreads)\n+    try {\n+      val futuresAcquire = (1 to 10 * numConcurrentProducers).map { i =>\n+        threadPool.submit(new Runnable {\n+          override def run(): Unit = {\n+            acquire(i % numConcurrentProducers + 1)\n+          }\n+        })\n+      }\n+      val futuresRelease = (1 to 10 * numConcurrentProducers).map { i =>\n+        val cachedKafkaProducer = toBeReleasedQueue.poll()\n+        // 2x release should not corrupt the state of cache.\n+        (1 to 2).map { j =>\n+          threadPool.submit(new Runnable {\n+            override def run(): Unit = {\n+              release(cachedKafkaProducer)\n+            }\n+          })\n+        }\n+      }\n+      futuresAcquire.foreach(_.get(1, TimeUnit.MINUTES))\n+      futuresRelease.flatten.foreach(_.get(1, TimeUnit.MINUTES))\n+    } finally {\n+      threadPool.shutdown()\n+      CachedKafkaProducer.clear()\n+    }\n+  }\n+\n+  /*\n+   * The following stress suite will cause frequent eviction of kafka producers from\n+   * the guava cache. Since these producers remain in use, because they are used by\n+   * multiple tasks, they stay in close queue till they are released finally. This test\n+   * will cause new tasks to use fresh instance of kafka producers and as a result it\n+   * simulates a stress situation, where multiple producers are requested from CachedKafkaProducer\n+   * and at the same time there will be multiple releases. It is supposed to catch a race\n+   * condition if any, due to multiple threads requesting and releasing producers.\n+   */\n+  test(\"Single source and multiple kafka sink with 2ms cache timeout.\") {\n+\n+    val df = spark.readStream\n+      .format(\"rate\")\n+      .option(\"numPartitions\", \"100\")\n+      .option(\"rowsPerSecond\", \"200\")\n+      .load()\n+      .selectExpr(\"CAST(timestamp AS STRING) key\", \"CAST(value AS STRING) value\")\n+\n+    val checkpointDir = Utils.createTempDir()\n+    val topic = newTopic()\n+    testUtils.createTopic(topic, 100)\n+    failAfter(streamingTimeout) {\n+      val queries = for (i <- 1 to 10) yield {\n+        df.writeStream\n+          .format(\"kafka\")\n+          .option(\"checkpointLocation\", checkpointDir.getCanonicalPath + i)\n+          .option(\"kafka.bootstrap.servers\", testUtils.brokerAddress)\n+          // to make it create 5 unique producers.\n+          .option(\"kafka.max.block.ms\", s\"100${i % 5}\")\n+          .option(\"topic\", topic)\n+          .trigger(Trigger.Continuous(500))\n+          .queryName(s\"kafkaStream$i\")\n+          .start()\n+      }\n+      Thread.sleep(15000)\n+\n+      queries.foreach { q =>\n+        assert(q.exception.isEmpty, \"None of the queries should fail.\")\n+        q.stop()",
    "line": 231
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "This case is different because if the first query gives exception the rest of the queries may run properly and has to be stopped.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-29T09:53:09Z",
    "diffHunk": "@@ -18,60 +18,208 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n+import java.util.concurrent.{ConcurrentLinkedQueue, Executors, TimeUnit}\n+\n+import scala.collection.mutable\n+import scala.util.Random\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.Trigger\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n \n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"acks\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Automatically remove a failing kafka producer from cache.\") {\n+    import testImplicits._\n+    val df = Seq[(String, String)](null.asInstanceOf[String] -> \"1\").toDF(\"topic\", \"value\")\n+    val ex = intercept[SparkException] {\n+      // This will fail because the service is not reachable.\n+      df.write\n+        .format(\"kafka\")\n+        .option(\"topic\", \"topic\")\n+        .option(\"kafka.retries\", \"1\")\n+        .option(\"kafka.max.block.ms\", \"2\")\n+        .option(\"kafka.bootstrap.servers\", \"12.0.0.1:39022\")\n+        .save()\n+    }\n+    assert(ex.getMessage.contains(\"org.apache.kafka.common.errors.TimeoutException\"),\n+      \"Spark command should fail due to service not reachable.\")\n+    // Since failing kafka producer is released on error and also invalidated, it should not be in\n+    // cache.\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 0)\n+  }\n+\n+  test(\"Should not close a producer in-use.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer: CachedKafkaProducer = CachedKafkaProducer.acquire(kafkaParams)\n+    producer.kafkaProducer // initializing the producer.\n+    assert(producer.getInUseCount == 1)\n+    // Explicitly cause the producer from guava cache to be evicted.\n+    CachedKafkaProducer.evict(producer.getKafkaParams)\n+    assert(producer.getInUseCount == 1)\n+    assert(!producer.isClosed, \"An in-use producer should not be closed.\")\n+  }\n+\n+  private def generateKafkaParams: ju.HashMap[String, Object] = {\n     val kafkaParams = new ju.HashMap[String, Object]()\n     kafkaParams.put(\"acks\", \"0\")\n     kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n     kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n     kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    kafkaParams.put(\"acks\", \"1\")\n-    val producer2: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    // With updated conf, a new producer instance should be created.\n-    assert(producer != producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map.size == 2)\n+    kafkaParams\n+  }\n+}\n+\n+class CachedKafkaProducerStressSuite extends KafkaContinuousTest with KafkaTest {\n+\n+  override val brokerProps = Map(\"auto.create.topics.enable\" -> \"false\")\n+\n+  override def afterAll(): Unit = {\n+    if (testUtils != null) {\n+      testUtils.teardown()\n+      testUtils = null\n+    }\n+    super.afterAll()\n+  }\n+\n+  override def sparkConf: SparkConf = {\n+    val conf = super.sparkConf\n+    conf.set(\"spark.kafka.producer.cache.timeout\", \"2ms\")\n+  }\n+\n+  test(\"concurrent use of CachedKafkaProducer\") {\n+    val topic = \"topic\" + Random.nextInt()\n+    testUtils.createTopic(topic, 1)\n+    val kafkaParams: Map[String, Object] = Map(\"bootstrap.servers\" -> testUtils.brokerAddress,\n+      \"key.serializer\" -> classOf[ByteArraySerializer].getName,\n+      \"value.serializer\" -> classOf[ByteArraySerializer].getName)\n \n-    CachedKafkaProducer.close(kafkaParams)\n-    val map2 = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map2.size == 1)\n     import scala.collection.JavaConverters._\n-    val (seq: Seq[(String, Object)], _producer: KP) = map2.asScala.toArray.apply(0)\n-    assert(_producer == producer)\n+\n+    val numThreads = 100\n+    val numConcurrentProducers = 1000\n+\n+    val kafkaParamsUniqueMap = mutable.HashMap.empty[Int, ju.Map[String, Object]]\n+    (1 to numConcurrentProducers).map {\n+      i => kafkaParamsUniqueMap.put(i, kafkaParams.updated(\"retries\", s\"$i\").asJava)\n+    }\n+    val toBeReleasedQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+    def acquire(i: Int): Unit = {\n+      val producer = CachedKafkaProducer.acquire(kafkaParamsUniqueMap(i))\n+      producer.kafkaProducer // materialize producer for the first time.\n+      assert(!producer.isClosed, \"Acquired producer cannot be closed.\")\n+      toBeReleasedQueue.add(producer)\n+    }\n+\n+    def release(producer: CachedKafkaProducer): Unit = {\n+      if (producer != null) {\n+        CachedKafkaProducer.release(producer, Random.nextBoolean())\n+        if (producer.getInUseCount > 0) {\n+          assert(!producer.isClosed, \"Should not close an inuse producer.\")\n+        }\n+      }\n+    }\n+    val threadPool = Executors.newFixedThreadPool(numThreads)\n+    try {\n+      val futuresAcquire = (1 to 10 * numConcurrentProducers).map { i =>\n+        threadPool.submit(new Runnable {\n+          override def run(): Unit = {\n+            acquire(i % numConcurrentProducers + 1)\n+          }\n+        })\n+      }\n+      val futuresRelease = (1 to 10 * numConcurrentProducers).map { i =>\n+        val cachedKafkaProducer = toBeReleasedQueue.poll()\n+        // 2x release should not corrupt the state of cache.\n+        (1 to 2).map { j =>\n+          threadPool.submit(new Runnable {\n+            override def run(): Unit = {\n+              release(cachedKafkaProducer)\n+            }\n+          })\n+        }\n+      }\n+      futuresAcquire.foreach(_.get(1, TimeUnit.MINUTES))\n+      futuresRelease.flatten.foreach(_.get(1, TimeUnit.MINUTES))\n+    } finally {\n+      threadPool.shutdown()\n+      CachedKafkaProducer.clear()\n+    }\n+  }\n+\n+  /*\n+   * The following stress suite will cause frequent eviction of kafka producers from\n+   * the guava cache. Since these producers remain in use, because they are used by\n+   * multiple tasks, they stay in close queue till they are released finally. This test\n+   * will cause new tasks to use fresh instance of kafka producers and as a result it\n+   * simulates a stress situation, where multiple producers are requested from CachedKafkaProducer\n+   * and at the same time there will be multiple releases. It is supposed to catch a race\n+   * condition if any, due to multiple threads requesting and releasing producers.\n+   */\n+  test(\"Single source and multiple kafka sink with 2ms cache timeout.\") {\n+\n+    val df = spark.readStream\n+      .format(\"rate\")\n+      .option(\"numPartitions\", \"100\")\n+      .option(\"rowsPerSecond\", \"200\")\n+      .load()\n+      .selectExpr(\"CAST(timestamp AS STRING) key\", \"CAST(value AS STRING) value\")\n+\n+    val checkpointDir = Utils.createTempDir()\n+    val topic = newTopic()\n+    testUtils.createTopic(topic, 100)\n+    failAfter(streamingTimeout) {\n+      val queries = for (i <- 1 to 10) yield {\n+        df.writeStream\n+          .format(\"kafka\")\n+          .option(\"checkpointLocation\", checkpointDir.getCanonicalPath + i)\n+          .option(\"kafka.bootstrap.servers\", testUtils.brokerAddress)\n+          // to make it create 5 unique producers.\n+          .option(\"kafka.max.block.ms\", s\"100${i % 5}\")\n+          .option(\"topic\", topic)\n+          .trigger(Trigger.Continuous(500))\n+          .queryName(s\"kafkaStream$i\")\n+          .start()\n+      }\n+      Thread.sleep(15000)\n+\n+      queries.foreach { q =>\n+        assert(q.exception.isEmpty, \"None of the queries should fail.\")\n+        q.stop()",
    "line": 231
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Nit: Type is not required.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-10T12:54:48Z",
    "diffHunk": "@@ -18,60 +18,215 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n+import java.util.concurrent.{Executors, TimeUnit}\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n+import scala.collection.mutable\n+import scala.util.Random\n+\n+import org.apache.kafka.clients.producer.ProducerRecord\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.{StreamingQuery, Trigger}\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n \n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams"
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Nit: `generateKafkaParams` can be inlined.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-10T12:55:11Z",
    "diffHunk": "@@ -18,60 +18,215 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n+import java.util.concurrent.{Executors, TimeUnit}\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n+import scala.collection.mutable\n+import scala.util.Random\n+\n+import org.apache.kafka.clients.producer.ProducerRecord\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.{StreamingQuery, Trigger}\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n \n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"acks\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Automatically remove a failing kafka producer from cache.\") {\n+    import testImplicits._\n+    val df = Seq[(String, String)](null.asInstanceOf[String] -> \"1\").toDF(\"topic\", \"value\")\n+    val ex = intercept[SparkException] {\n+      // This will fail because the service is not reachable.\n+      df.write\n+        .format(\"kafka\")\n+        .option(\"topic\", \"topic\")\n+        .option(\"kafka.retries\", \"1\")\n+        .option(\"kafka.max.block.ms\", \"2\")\n+        .option(\"kafka.bootstrap.servers\", \"12.0.0.1:39022\")\n+        .save()\n+    }\n+    assert(ex.getMessage.contains(\"TimeoutException\"),\n+      \"Spark command should fail due to service not reachable.\")\n+    // Kafka first tries to fetch metadata and reports failures as, \" not present in metadata after\n+    // max.block.ms time.\"\n+    assert(ex.getMessage.toLowerCase(ju.Locale.ROOT)\n+      .contains(\"not present in metadata after 2 ms.\"),\n+      \"Spark command should fail due to service not reachable.\")\n+\n+    // Since failing kafka producer is released on error and also invalidated, it should not be in\n+    // cache.\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 0)\n+  }\n+\n+  test(\"Should not close a producer in-use.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams"
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "This already defined in `KafkaSourceTest`. Any reason to have it here?",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-10T12:56:30Z",
    "diffHunk": "@@ -18,60 +18,215 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n+import java.util.concurrent.{Executors, TimeUnit}\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n+import scala.collection.mutable\n+import scala.util.Random\n+\n+import org.apache.kafka.clients.producer.ProducerRecord\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.{StreamingQuery, Trigger}\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n \n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"acks\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Automatically remove a failing kafka producer from cache.\") {\n+    import testImplicits._\n+    val df = Seq[(String, String)](null.asInstanceOf[String] -> \"1\").toDF(\"topic\", \"value\")\n+    val ex = intercept[SparkException] {\n+      // This will fail because the service is not reachable.\n+      df.write\n+        .format(\"kafka\")\n+        .option(\"topic\", \"topic\")\n+        .option(\"kafka.retries\", \"1\")\n+        .option(\"kafka.max.block.ms\", \"2\")\n+        .option(\"kafka.bootstrap.servers\", \"12.0.0.1:39022\")\n+        .save()\n+    }\n+    assert(ex.getMessage.contains(\"TimeoutException\"),\n+      \"Spark command should fail due to service not reachable.\")\n+    // Kafka first tries to fetch metadata and reports failures as, \" not present in metadata after\n+    // max.block.ms time.\"\n+    assert(ex.getMessage.toLowerCase(ju.Locale.ROOT)\n+      .contains(\"not present in metadata after 2 ms.\"),\n+      \"Spark command should fail due to service not reachable.\")\n+\n+    // Since failing kafka producer is released on error and also invalidated, it should not be in\n+    // cache.\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 0)\n+  }\n+\n+  test(\"Should not close a producer in-use.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer: CachedKafkaProducer = CachedKafkaProducer.acquire(kafkaParams)\n+    producer.kafkaProducer // initializing the producer.\n+    assert(producer.getInUseCount == 1)\n+    // Explicitly cause the producer from guava cache to be evicted.\n+    CachedKafkaProducer.evict(producer.getKafkaParams)\n+    assert(producer.getInUseCount == 1)\n+    assert(!producer.isClosed, \"An in-use producer should not be closed.\")\n+  }\n+\n+  private def generateKafkaParams: ju.HashMap[String, Object] = {\n     val kafkaParams = new ju.HashMap[String, Object]()\n     kafkaParams.put(\"acks\", \"0\")\n     kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n     kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n     kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    kafkaParams.put(\"acks\", \"1\")\n-    val producer2: KP = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    // With updated conf, a new producer instance should be created.\n-    assert(producer != producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n-    assert(map.size == 2)\n+    kafkaParams\n+  }\n+}\n+\n+class CachedKafkaProducerStressSuite extends KafkaContinuousTest with KafkaTest {\n+\n+  override val brokerProps = Map(\"auto.create.topics.enable\" -> \"false\")\n+\n+  override def afterAll(): Unit = {"
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Nit: Type is not required.\r\n",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-10T13:00:47Z",
    "diffHunk": "@@ -18,60 +18,215 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n+import java.util.concurrent.{Executors, TimeUnit}\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n+import scala.collection.mutable\n+import scala.util.Random\n+\n+import org.apache.kafka.clients.producer.ProducerRecord\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.{StreamingQuery, Trigger}\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n \n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams: ju.HashMap[String, Object] = generateKafkaParams"
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Maybe the exception is not top level. I've added a helper for such situation: https://github.com/apache/spark/blob/1b232671a8210ed7de5e7e3ce5aecc140db847f2/core/src/main/scala/org/apache/spark/TestUtils.scala#L204",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-07-10T11:30:13Z",
    "diffHunk": "@@ -18,60 +18,207 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n+import java.util.concurrent.{Executors, TimeUnit}\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n+import scala.collection.mutable\n+import scala.util.Random\n+\n+import org.apache.kafka.clients.producer.ProducerRecord\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.{StreamingQuery, Trigger}\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n \n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"acks\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Automatically remove a failing kafka producer from cache.\") {\n+    import testImplicits._\n+    val df = Seq[(String, String)](null.asInstanceOf[String] -> \"1\").toDF(\"topic\", \"value\")\n+    val ex = intercept[SparkException] {\n+      // This will fail because the service is not reachable.\n+      df.write\n+        .format(\"kafka\")\n+        .option(\"topic\", \"topic\")\n+        .option(\"kafka.retries\", \"1\")\n+        .option(\"kafka.max.block.ms\", \"2\")\n+        .option(\"kafka.bootstrap.servers\", \"12.0.0.1:39022\")\n+        .save()\n+    }\n+    assert(ex.getMessage.contains(\"TimeoutException\"),"
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "hm.., exception does get logged in the logs as ERROR, but does not get thrown.\r\n```\r\n19/07/11 12:07:26.016 Executor task launch worker for task 0 ERROR Utils: Aborting task\r\norg.apache.kafka.common.errors.TimeoutException: Topic topic not present in metadata after 2 ms.\r\n19/07/11 12:07:26.019 Executor task launch worker for task 0 ERROR DataWritingSparkTask: Aborting commit for partition 0 (task 0, attempt 0stage 0.0)\r\n\r\n```\r\n\r\nDo you know what is going on, here?",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-07-11T08:44:14Z",
    "diffHunk": "@@ -18,60 +18,207 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n+import java.util.concurrent.{Executors, TimeUnit}\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n+import scala.collection.mutable\n+import scala.util.Random\n+\n+import org.apache.kafka.clients.producer.ProducerRecord\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.{StreamingQuery, Trigger}\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n \n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"acks\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Automatically remove a failing kafka producer from cache.\") {\n+    import testImplicits._\n+    val df = Seq[(String, String)](null.asInstanceOf[String] -> \"1\").toDF(\"topic\", \"value\")\n+    val ex = intercept[SparkException] {\n+      // This will fail because the service is not reachable.\n+      df.write\n+        .format(\"kafka\")\n+        .option(\"topic\", \"topic\")\n+        .option(\"kafka.retries\", \"1\")\n+        .option(\"kafka.max.block.ms\", \"2\")\n+        .option(\"kafka.bootstrap.servers\", \"12.0.0.1:39022\")\n+        .save()\n+    }\n+    assert(ex.getMessage.contains(\"TimeoutException\"),"
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "You can ignore above comment, thanks for the hint given.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-07-11T10:14:14Z",
    "diffHunk": "@@ -18,60 +18,207 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.ConcurrentMap\n+import java.util.concurrent.{Executors, TimeUnit}\n \n-import org.apache.kafka.clients.producer.KafkaProducer\n+import scala.collection.mutable\n+import scala.util.Random\n+\n+import org.apache.kafka.clients.producer.ProducerRecord\n import org.apache.kafka.common.serialization.ByteArraySerializer\n-import org.scalatest.PrivateMethodTester\n \n+import org.apache.spark.{SparkConf, SparkException}\n+import org.apache.spark.sql.streaming.{StreamingQuery, Trigger}\n import org.apache.spark.sql.test.SharedSQLContext\n+import org.apache.spark.util.Utils\n \n-class CachedKafkaProducerSuite extends SharedSQLContext with PrivateMethodTester with KafkaTest {\n \n-  type KP = KafkaProducer[Array[Byte], Array[Byte]]\n+class CachedKafkaProducerSuite extends SharedSQLContext with KafkaTest {\n \n   protected override def beforeEach(): Unit = {\n     super.beforeEach()\n     CachedKafkaProducer.clear()\n   }\n \n-  test(\"Should return the cached instance on calling getOrCreate with same params.\") {\n-    val kafkaParams = new ju.HashMap[String, Object]()\n-    kafkaParams.put(\"acks\", \"0\")\n-    // Here only host should be resolvable, it does not need a running instance of kafka server.\n-    kafkaParams.put(\"bootstrap.servers\", \"127.0.0.1:9022\")\n-    kafkaParams.put(\"key.serializer\", classOf[ByteArraySerializer].getName)\n-    kafkaParams.put(\"value.serializer\", classOf[ByteArraySerializer].getName)\n-    val producer = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    val producer2 = CachedKafkaProducer.getOrCreate(kafkaParams)\n-    assert(producer == producer2)\n-\n-    val cacheMap = PrivateMethod[ConcurrentMap[Seq[(String, Object)], KP]]('getAsMap)\n-    val map = CachedKafkaProducer.invokePrivate(cacheMap())\n+  test(\"Should return the cached instance on calling acquire with same params.\") {\n+    val kafkaParams = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer == producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 2)\n+    val map = CachedKafkaProducer.getAsMap\n     assert(map.size == 1)\n   }\n \n-  test(\"Should close the correct kafka producer for the given kafkaPrams.\") {\n+  test(\"Should return the new instance on calling acquire with different params.\") {\n+    val kafkaParams = generateKafkaParams\n+    val producer = CachedKafkaProducer.acquire(kafkaParams)\n+    kafkaParams.remove(\"acks\") // mutate the kafka params.\n+    val producer2 = CachedKafkaProducer.acquire(kafkaParams)\n+    assert(producer.kafkaProducer != producer2.kafkaProducer)\n+    assert(producer.getInUseCount == 1)\n+    assert(producer2.getInUseCount == 1)\n+    val map = CachedKafkaProducer.getAsMap\n+    assert(map.size == 2)\n+  }\n+\n+  test(\"Automatically remove a failing kafka producer from cache.\") {\n+    import testImplicits._\n+    val df = Seq[(String, String)](null.asInstanceOf[String] -> \"1\").toDF(\"topic\", \"value\")\n+    val ex = intercept[SparkException] {\n+      // This will fail because the service is not reachable.\n+      df.write\n+        .format(\"kafka\")\n+        .option(\"topic\", \"topic\")\n+        .option(\"kafka.retries\", \"1\")\n+        .option(\"kafka.max.block.ms\", \"2\")\n+        .option(\"kafka.bootstrap.servers\", \"12.0.0.1:39022\")\n+        .save()\n+    }\n+    assert(ex.getMessage.contains(\"TimeoutException\"),"
  }],
  "prId": 19096
}]