[{
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "Copied from KafkaMicroBatchSourceSuite.scala. I also moved the set up codes to `KafkaMissingOffsetsTest` to share with KafkaDontFailOnDataLossSuite.",
    "commit": "351527555c53a3977aa86a7057ed0aa12fb0976e",
    "createdAt": "2018-08-23T18:21:51Z",
    "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.util.Properties\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable\n+import scala.util.Random\n+\n+import org.scalatest.time.SpanSugar._\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{DataFrame, Dataset, ForeachWriter}\n+import org.apache.spark.sql.streaming.{StreamTest, Trigger}\n+import org.apache.spark.sql.test.{SharedSQLContext, TestSparkSession}\n+\n+/**\n+ * This is a basic test trait which will set up a Kafka cluster that keeps only several records in\n+ * a topic and ages out records very quickly. This is a helper trait to test\n+ * \"failonDataLoss=false\" case with missing offsets.\n+ *\n+ * Note: there is a hard-code 30 seconds delay (kafka.log.LogManager.InitialTaskDelayMs) to clean up\n+ * records. Hence each class extending this trait needs to wait at least 30 seconds (or even longer\n+ * when running on a slow Jenkins machine) before records start to be removed. To make sure a test\n+ * does see missing offsets, you can check the earliest offset in `eventually` and make sure it's\n+ * not 0 rather than sleeping a hard-code duration.\n+ */\n+trait KafkaMissingOffsetsTest extends SharedSQLContext {\n+\n+  protected var testUtils: KafkaTestUtils = _\n+\n+  override def createSparkSession(): TestSparkSession = {\n+    // Set maxRetries to 3 to handle NPE from `poll` when deleting a topic\n+    new TestSparkSession(new SparkContext(\"local[2,3]\", \"test-sql-context\", sparkConf))\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    testUtils = new KafkaTestUtils {\n+      override def brokerConfiguration: Properties = {\n+        val props = super.brokerConfiguration\n+        // Try to make Kafka clean up messages as fast as possible. However, there is a hard-code\n+        // 30 seconds delay (kafka.log.LogManager.InitialTaskDelayMs) so this test should run at\n+        // least 30 seconds.\n+        props.put(\"log.cleaner.backoff.ms\", \"100\")\n+        // The size of RecordBatch V2 increases to support transactional write.\n+        props.put(\"log.segment.bytes\", \"70\")\n+        props.put(\"log.retention.bytes\", \"40\")\n+        props.put(\"log.retention.check.interval.ms\", \"100\")\n+        props.put(\"delete.retention.ms\", \"10\")\n+        props.put(\"log.flush.scheduler.interval.ms\", \"10\")\n+        props\n+      }\n+    }\n+    testUtils.setup()\n+  }\n+\n+  override def afterAll(): Unit = {\n+    if (testUtils != null) {\n+      testUtils.teardown()\n+      testUtils = null\n+    }\n+    super.afterAll()\n+  }\n+}\n+\n+class KafkaDontFailOnDataLossSuite extends KafkaMissingOffsetsTest {\n+\n+  import testImplicits._\n+\n+  private val topicId = new AtomicInteger(0)\n+\n+  private def newTopic(): String = s\"failOnDataLoss-${topicId.getAndIncrement()}\"\n+\n+  /**\n+   * @param testStreamingQuery whether to test a streaming query or a batch query.\n+   * @param writeToTable the function to write the specified [[DataFrame]] to the given table.\n+   */\n+  private def verifyMissingOffsetsDontCauseDuplicatedRecords(\n+    testStreamingQuery: Boolean)(writeToTable: (DataFrame, String) => Unit): Unit = {\n+    val topic = newTopic()\n+    testUtils.createTopic(topic, partitions = 1)\n+    testUtils.sendMessages(topic, (0 until 50).map(_.toString).toArray)\n+\n+    eventually(timeout(60.seconds)) {\n+      assert(\n+        testUtils.getEarliestOffsets(Set(topic)).head._2 > 0,\n+        \"Kafka didn't delete records after 1 minute\")\n+    }\n+\n+    val table = \"DontFailOnDataLoss\"\n+    withTable(table) {\n+      val df = if (testStreamingQuery) {\n+        spark.readStream\n+          .format(\"kafka\")\n+          .option(\"kafka.bootstrap.servers\", testUtils.brokerAddress)\n+          .option(\"kafka.metadata.max.age.ms\", \"1\")\n+          .option(\"subscribe\", topic)\n+          .option(\"startingOffsets\", s\"\"\"{\"$topic\":{\"0\":0}}\"\"\")\n+          .option(\"failOnDataLoss\", \"false\")\n+          .option(\"kafkaConsumer.pollTimeoutMs\", \"1000\")\n+          .load()\n+      } else {\n+        spark.read\n+          .format(\"kafka\")\n+          .option(\"kafka.bootstrap.servers\", testUtils.brokerAddress)\n+          .option(\"kafka.metadata.max.age.ms\", \"1\")\n+          .option(\"subscribe\", topic)\n+          .option(\"startingOffsets\", s\"\"\"{\"$topic\":{\"0\":0}}\"\"\")\n+          .option(\"failOnDataLoss\", \"false\")\n+          .option(\"kafkaConsumer.pollTimeoutMs\", \"1000\")\n+          .load()\n+      }\n+      writeToTable(df.selectExpr(\"CAST(value AS STRING)\"), table)\n+      val result = spark.table(table).as[String].collect().toList\n+      assert(result.distinct.size === result.size, s\"$result contains duplicated records\")\n+      // Make sure Kafka did remove some records so that this test is valid.\n+      assert(result.size > 0 && result.size < 50)\n+    }\n+  }\n+\n+  test(\"failOnDataLoss=false should not return duplicated records: v1\") {\n+    withSQLConf(\n+      \"spark.sql.streaming.disabledV2MicroBatchReaders\" ->\n+        classOf[KafkaSourceProvider].getCanonicalName) {\n+      verifyMissingOffsetsDontCauseDuplicatedRecords(testStreamingQuery = true) { (df, table) =>\n+        val query = df.writeStream.format(\"memory\").queryName(table).start()\n+        try {\n+          query.processAllAvailable()\n+        } finally {\n+          query.stop()\n+        }\n+      }\n+    }\n+  }\n+\n+  test(\"failOnDataLoss=false should not return duplicated records: v2\") {\n+    verifyMissingOffsetsDontCauseDuplicatedRecords(testStreamingQuery = true) { (df, table) =>\n+      val query = df.writeStream.format(\"memory\").queryName(table).start()\n+      try {\n+        query.processAllAvailable()\n+      } finally {\n+        query.stop()\n+      }\n+    }\n+  }\n+\n+  test(\"failOnDataLoss=false should not return duplicated records: continuous processing\") {\n+    verifyMissingOffsetsDontCauseDuplicatedRecords(testStreamingQuery = true) { (df, table) =>\n+      val query = df.writeStream\n+        .format(\"memory\")\n+        .queryName(table)\n+        .trigger(Trigger.Continuous(100))\n+        .start()\n+      try {\n+        eventually(timeout(60.seconds)) {\n+          assert(spark.table(table).as[String].collect().contains(\"49\"))\n+        }\n+      } finally {\n+        query.stop()\n+      }\n+    }\n+  }\n+\n+  test(\"failOnDataLoss=false should not return duplicated records: batch\") {\n+    verifyMissingOffsetsDontCauseDuplicatedRecords(testStreamingQuery = false) { (df, table) =>\n+      df.write.saveAsTable(table)\n+    }\n+  }\n+}\n+\n+class KafkaSourceStressForDontFailOnDataLossSuite extends StreamTest with KafkaMissingOffsetsTest {",
    "line": 182
  }],
  "prId": 22207
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "How do you ensure that the above configure retention policy will not completely delete all records?",
    "commit": "351527555c53a3977aa86a7057ed0aa12fb0976e",
    "createdAt": "2018-08-24T05:44:37Z",
    "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.util.Properties\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable\n+import scala.util.Random\n+\n+import org.scalatest.time.SpanSugar._\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{DataFrame, Dataset, ForeachWriter}\n+import org.apache.spark.sql.streaming.{StreamTest, Trigger}\n+import org.apache.spark.sql.test.{SharedSQLContext, TestSparkSession}\n+\n+/**\n+ * This is a basic test trait which will set up a Kafka cluster that keeps only several records in\n+ * a topic and ages out records very quickly. This is a helper trait to test\n+ * \"failonDataLoss=false\" case with missing offsets.\n+ *\n+ * Note: there is a hard-code 30 seconds delay (kafka.log.LogManager.InitialTaskDelayMs) to clean up\n+ * records. Hence each class extending this trait needs to wait at least 30 seconds (or even longer\n+ * when running on a slow Jenkins machine) before records start to be removed. To make sure a test\n+ * does see missing offsets, you can check the earliest offset in `eventually` and make sure it's\n+ * not 0 rather than sleeping a hard-code duration.\n+ */\n+trait KafkaMissingOffsetsTest extends SharedSQLContext {\n+\n+  protected var testUtils: KafkaTestUtils = _\n+\n+  override def createSparkSession(): TestSparkSession = {\n+    // Set maxRetries to 3 to handle NPE from `poll` when deleting a topic\n+    new TestSparkSession(new SparkContext(\"local[2,3]\", \"test-sql-context\", sparkConf))\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    testUtils = new KafkaTestUtils {\n+      override def brokerConfiguration: Properties = {\n+        val props = super.brokerConfiguration\n+        // Try to make Kafka clean up messages as fast as possible. However, there is a hard-code\n+        // 30 seconds delay (kafka.log.LogManager.InitialTaskDelayMs) so this test should run at\n+        // least 30 seconds.\n+        props.put(\"log.cleaner.backoff.ms\", \"100\")\n+        // The size of RecordBatch V2 increases to support transactional write.\n+        props.put(\"log.segment.bytes\", \"70\")\n+        props.put(\"log.retention.bytes\", \"40\")\n+        props.put(\"log.retention.check.interval.ms\", \"100\")\n+        props.put(\"delete.retention.ms\", \"10\")\n+        props.put(\"log.flush.scheduler.interval.ms\", \"10\")\n+        props\n+      }\n+    }\n+    testUtils.setup()\n+  }\n+\n+  override def afterAll(): Unit = {\n+    if (testUtils != null) {\n+      testUtils.teardown()\n+      testUtils = null\n+    }\n+    super.afterAll()\n+  }\n+}\n+\n+class KafkaDontFailOnDataLossSuite extends KafkaMissingOffsetsTest {\n+\n+  import testImplicits._\n+\n+  private val topicId = new AtomicInteger(0)\n+\n+  private def newTopic(): String = s\"failOnDataLoss-${topicId.getAndIncrement()}\"\n+\n+  /**\n+   * @param testStreamingQuery whether to test a streaming query or a batch query.\n+   * @param writeToTable the function to write the specified [[DataFrame]] to the given table.\n+   */\n+  private def verifyMissingOffsetsDontCauseDuplicatedRecords(\n+      testStreamingQuery: Boolean)(writeToTable: (DataFrame, String) => Unit): Unit = {\n+    val topic = newTopic()\n+    testUtils.createTopic(topic, partitions = 1)\n+    testUtils.sendMessages(topic, (0 until 50).map(_.toString).toArray)\n+\n+    eventually(timeout(60.seconds)) {\n+      assert(\n+        testUtils.getEarliestOffsets(Set(topic)).head._2 > 0,\n+        \"Kafka didn't delete records after 1 minute\")\n+    }\n+\n+    val table = \"DontFailOnDataLoss\"\n+    withTable(table) {\n+      val df = if (testStreamingQuery) {\n+        spark.readStream\n+          .format(\"kafka\")\n+          .option(\"kafka.bootstrap.servers\", testUtils.brokerAddress)\n+          .option(\"kafka.metadata.max.age.ms\", \"1\")\n+          .option(\"subscribe\", topic)\n+          .option(\"startingOffsets\", s\"\"\"{\"$topic\":{\"0\":0}}\"\"\")\n+          .option(\"failOnDataLoss\", \"false\")\n+          .option(\"kafkaConsumer.pollTimeoutMs\", \"1000\")\n+          .load()\n+      } else {\n+        spark.read\n+          .format(\"kafka\")\n+          .option(\"kafka.bootstrap.servers\", testUtils.brokerAddress)\n+          .option(\"kafka.metadata.max.age.ms\", \"1\")\n+          .option(\"subscribe\", topic)\n+          .option(\"startingOffsets\", s\"\"\"{\"$topic\":{\"0\":0}}\"\"\")\n+          .option(\"failOnDataLoss\", \"false\")\n+          .option(\"kafkaConsumer.pollTimeoutMs\", \"1000\")\n+          .load()\n+      }\n+      writeToTable(df.selectExpr(\"CAST(value AS STRING)\"), table)\n+      val result = spark.table(table).as[String].collect().toList\n+      assert(result.distinct.size === result.size, s\"$result contains duplicated records\")\n+      // Make sure Kafka did remove some records so that this test is valid.\n+      assert(result.size > 0 && result.size < 50)",
    "line": 130
  }, {
    "author": {
      "login": "zsxwing"
    },
    "body": "I checked Kafka codes and it will keep at least one segment for a topic. I also did a simple test to make sure it will not delete all records: Added `Thread.sleep(120000)` after `eventually(timeout(60.seconds)) { assert( testUtils.getEarliestOffsets(Set(topic)).head._2 > 0, \"Kafka didn't delete records after 1 minute\") }` and the assertion still passed.",
    "commit": "351527555c53a3977aa86a7057ed0aa12fb0976e",
    "createdAt": "2018-08-24T17:56:21Z",
    "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.util.Properties\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable\n+import scala.util.Random\n+\n+import org.scalatest.time.SpanSugar._\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{DataFrame, Dataset, ForeachWriter}\n+import org.apache.spark.sql.streaming.{StreamTest, Trigger}\n+import org.apache.spark.sql.test.{SharedSQLContext, TestSparkSession}\n+\n+/**\n+ * This is a basic test trait which will set up a Kafka cluster that keeps only several records in\n+ * a topic and ages out records very quickly. This is a helper trait to test\n+ * \"failonDataLoss=false\" case with missing offsets.\n+ *\n+ * Note: there is a hard-code 30 seconds delay (kafka.log.LogManager.InitialTaskDelayMs) to clean up\n+ * records. Hence each class extending this trait needs to wait at least 30 seconds (or even longer\n+ * when running on a slow Jenkins machine) before records start to be removed. To make sure a test\n+ * does see missing offsets, you can check the earliest offset in `eventually` and make sure it's\n+ * not 0 rather than sleeping a hard-code duration.\n+ */\n+trait KafkaMissingOffsetsTest extends SharedSQLContext {\n+\n+  protected var testUtils: KafkaTestUtils = _\n+\n+  override def createSparkSession(): TestSparkSession = {\n+    // Set maxRetries to 3 to handle NPE from `poll` when deleting a topic\n+    new TestSparkSession(new SparkContext(\"local[2,3]\", \"test-sql-context\", sparkConf))\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    testUtils = new KafkaTestUtils {\n+      override def brokerConfiguration: Properties = {\n+        val props = super.brokerConfiguration\n+        // Try to make Kafka clean up messages as fast as possible. However, there is a hard-code\n+        // 30 seconds delay (kafka.log.LogManager.InitialTaskDelayMs) so this test should run at\n+        // least 30 seconds.\n+        props.put(\"log.cleaner.backoff.ms\", \"100\")\n+        // The size of RecordBatch V2 increases to support transactional write.\n+        props.put(\"log.segment.bytes\", \"70\")\n+        props.put(\"log.retention.bytes\", \"40\")\n+        props.put(\"log.retention.check.interval.ms\", \"100\")\n+        props.put(\"delete.retention.ms\", \"10\")\n+        props.put(\"log.flush.scheduler.interval.ms\", \"10\")\n+        props\n+      }\n+    }\n+    testUtils.setup()\n+  }\n+\n+  override def afterAll(): Unit = {\n+    if (testUtils != null) {\n+      testUtils.teardown()\n+      testUtils = null\n+    }\n+    super.afterAll()\n+  }\n+}\n+\n+class KafkaDontFailOnDataLossSuite extends KafkaMissingOffsetsTest {\n+\n+  import testImplicits._\n+\n+  private val topicId = new AtomicInteger(0)\n+\n+  private def newTopic(): String = s\"failOnDataLoss-${topicId.getAndIncrement()}\"\n+\n+  /**\n+   * @param testStreamingQuery whether to test a streaming query or a batch query.\n+   * @param writeToTable the function to write the specified [[DataFrame]] to the given table.\n+   */\n+  private def verifyMissingOffsetsDontCauseDuplicatedRecords(\n+      testStreamingQuery: Boolean)(writeToTable: (DataFrame, String) => Unit): Unit = {\n+    val topic = newTopic()\n+    testUtils.createTopic(topic, partitions = 1)\n+    testUtils.sendMessages(topic, (0 until 50).map(_.toString).toArray)\n+\n+    eventually(timeout(60.seconds)) {\n+      assert(\n+        testUtils.getEarliestOffsets(Set(topic)).head._2 > 0,\n+        \"Kafka didn't delete records after 1 minute\")\n+    }\n+\n+    val table = \"DontFailOnDataLoss\"\n+    withTable(table) {\n+      val df = if (testStreamingQuery) {\n+        spark.readStream\n+          .format(\"kafka\")\n+          .option(\"kafka.bootstrap.servers\", testUtils.brokerAddress)\n+          .option(\"kafka.metadata.max.age.ms\", \"1\")\n+          .option(\"subscribe\", topic)\n+          .option(\"startingOffsets\", s\"\"\"{\"$topic\":{\"0\":0}}\"\"\")\n+          .option(\"failOnDataLoss\", \"false\")\n+          .option(\"kafkaConsumer.pollTimeoutMs\", \"1000\")\n+          .load()\n+      } else {\n+        spark.read\n+          .format(\"kafka\")\n+          .option(\"kafka.bootstrap.servers\", testUtils.brokerAddress)\n+          .option(\"kafka.metadata.max.age.ms\", \"1\")\n+          .option(\"subscribe\", topic)\n+          .option(\"startingOffsets\", s\"\"\"{\"$topic\":{\"0\":0}}\"\"\")\n+          .option(\"failOnDataLoss\", \"false\")\n+          .option(\"kafkaConsumer.pollTimeoutMs\", \"1000\")\n+          .load()\n+      }\n+      writeToTable(df.selectExpr(\"CAST(value AS STRING)\"), table)\n+      val result = spark.table(table).as[String].collect().toList\n+      assert(result.distinct.size === result.size, s\"$result contains duplicated records\")\n+      // Make sure Kafka did remove some records so that this test is valid.\n+      assert(result.size > 0 && result.size < 50)",
    "line": 130
  }],
  "prId": 22207
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "dedup these options into map... just to make sure they are never in consistent.",
    "commit": "351527555c53a3977aa86a7057ed0aa12fb0976e",
    "createdAt": "2018-08-24T05:46:55Z",
    "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.util.Properties\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable\n+import scala.util.Random\n+\n+import org.scalatest.time.SpanSugar._\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{DataFrame, Dataset, ForeachWriter}\n+import org.apache.spark.sql.streaming.{StreamTest, Trigger}\n+import org.apache.spark.sql.test.{SharedSQLContext, TestSparkSession}\n+\n+/**\n+ * This is a basic test trait which will set up a Kafka cluster that keeps only several records in\n+ * a topic and ages out records very quickly. This is a helper trait to test\n+ * \"failonDataLoss=false\" case with missing offsets.\n+ *\n+ * Note: there is a hard-code 30 seconds delay (kafka.log.LogManager.InitialTaskDelayMs) to clean up\n+ * records. Hence each class extending this trait needs to wait at least 30 seconds (or even longer\n+ * when running on a slow Jenkins machine) before records start to be removed. To make sure a test\n+ * does see missing offsets, you can check the earliest offset in `eventually` and make sure it's\n+ * not 0 rather than sleeping a hard-code duration.\n+ */\n+trait KafkaMissingOffsetsTest extends SharedSQLContext {\n+\n+  protected var testUtils: KafkaTestUtils = _\n+\n+  override def createSparkSession(): TestSparkSession = {\n+    // Set maxRetries to 3 to handle NPE from `poll` when deleting a topic\n+    new TestSparkSession(new SparkContext(\"local[2,3]\", \"test-sql-context\", sparkConf))\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    testUtils = new KafkaTestUtils {\n+      override def brokerConfiguration: Properties = {\n+        val props = super.brokerConfiguration\n+        // Try to make Kafka clean up messages as fast as possible. However, there is a hard-code\n+        // 30 seconds delay (kafka.log.LogManager.InitialTaskDelayMs) so this test should run at\n+        // least 30 seconds.\n+        props.put(\"log.cleaner.backoff.ms\", \"100\")\n+        // The size of RecordBatch V2 increases to support transactional write.\n+        props.put(\"log.segment.bytes\", \"70\")\n+        props.put(\"log.retention.bytes\", \"40\")\n+        props.put(\"log.retention.check.interval.ms\", \"100\")\n+        props.put(\"delete.retention.ms\", \"10\")\n+        props.put(\"log.flush.scheduler.interval.ms\", \"10\")\n+        props\n+      }\n+    }\n+    testUtils.setup()\n+  }\n+\n+  override def afterAll(): Unit = {\n+    if (testUtils != null) {\n+      testUtils.teardown()\n+      testUtils = null\n+    }\n+    super.afterAll()\n+  }\n+}\n+\n+class KafkaDontFailOnDataLossSuite extends KafkaMissingOffsetsTest {\n+\n+  import testImplicits._\n+\n+  private val topicId = new AtomicInteger(0)\n+\n+  private def newTopic(): String = s\"failOnDataLoss-${topicId.getAndIncrement()}\"\n+\n+  /**\n+   * @param testStreamingQuery whether to test a streaming query or a batch query.\n+   * @param writeToTable the function to write the specified [[DataFrame]] to the given table.\n+   */\n+  private def verifyMissingOffsetsDontCauseDuplicatedRecords(\n+      testStreamingQuery: Boolean)(writeToTable: (DataFrame, String) => Unit): Unit = {\n+    val topic = newTopic()\n+    testUtils.createTopic(topic, partitions = 1)\n+    testUtils.sendMessages(topic, (0 until 50).map(_.toString).toArray)\n+\n+    eventually(timeout(60.seconds)) {\n+      assert(\n+        testUtils.getEarliestOffsets(Set(topic)).head._2 > 0,\n+        \"Kafka didn't delete records after 1 minute\")\n+    }\n+\n+    val table = \"DontFailOnDataLoss\"\n+    withTable(table) {\n+      val df = if (testStreamingQuery) {\n+        spark.readStream\n+          .format(\"kafka\")\n+          .option(\"kafka.bootstrap.servers\", testUtils.brokerAddress)\n+          .option(\"kafka.metadata.max.age.ms\", \"1\")\n+          .option(\"subscribe\", topic)\n+          .option(\"startingOffsets\", s\"\"\"{\"$topic\":{\"0\":0}}\"\"\")\n+          .option(\"failOnDataLoss\", \"false\")\n+          .option(\"kafkaConsumer.pollTimeoutMs\", \"1000\")\n+          .load()\n+      } else {\n+        spark.read\n+          .format(\"kafka\")\n+          .option(\"kafka.bootstrap.servers\", testUtils.brokerAddress)"
  }],
  "prId": 22207
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "doesnt processAllAvailable work in continuous processing?",
    "commit": "351527555c53a3977aa86a7057ed0aa12fb0976e",
    "createdAt": "2018-08-24T17:51:28Z",
    "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.util.Properties\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable\n+import scala.util.Random\n+\n+import org.scalatest.time.SpanSugar._\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{DataFrame, Dataset, ForeachWriter}\n+import org.apache.spark.sql.streaming.{StreamTest, Trigger}\n+import org.apache.spark.sql.test.{SharedSQLContext, TestSparkSession}\n+\n+/**\n+ * This is a basic test trait which will set up a Kafka cluster that keeps only several records in\n+ * a topic and ages out records very quickly. This is a helper trait to test\n+ * \"failonDataLoss=false\" case with missing offsets.\n+ *\n+ * Note: there is a hard-code 30 seconds delay (kafka.log.LogManager.InitialTaskDelayMs) to clean up\n+ * records. Hence each class extending this trait needs to wait at least 30 seconds (or even longer\n+ * when running on a slow Jenkins machine) before records start to be removed. To make sure a test\n+ * does see missing offsets, you can check the earliest offset in `eventually` and make sure it's\n+ * not 0 rather than sleeping a hard-code duration.\n+ */\n+trait KafkaMissingOffsetsTest extends SharedSQLContext {\n+\n+  protected var testUtils: KafkaTestUtils = _\n+\n+  override def createSparkSession(): TestSparkSession = {\n+    // Set maxRetries to 3 to handle NPE from `poll` when deleting a topic\n+    new TestSparkSession(new SparkContext(\"local[2,3]\", \"test-sql-context\", sparkConf))\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    testUtils = new KafkaTestUtils {\n+      override def brokerConfiguration: Properties = {\n+        val props = super.brokerConfiguration\n+        // Try to make Kafka clean up messages as fast as possible. However, there is a hard-code\n+        // 30 seconds delay (kafka.log.LogManager.InitialTaskDelayMs) so this test should run at\n+        // least 30 seconds.\n+        props.put(\"log.cleaner.backoff.ms\", \"100\")\n+        // The size of RecordBatch V2 increases to support transactional write.\n+        props.put(\"log.segment.bytes\", \"70\")\n+        props.put(\"log.retention.bytes\", \"40\")\n+        props.put(\"log.retention.check.interval.ms\", \"100\")\n+        props.put(\"delete.retention.ms\", \"10\")\n+        props.put(\"log.flush.scheduler.interval.ms\", \"10\")\n+        props\n+      }\n+    }\n+    testUtils.setup()\n+  }\n+\n+  override def afterAll(): Unit = {\n+    if (testUtils != null) {\n+      testUtils.teardown()\n+      testUtils = null\n+    }\n+    super.afterAll()\n+  }\n+}\n+\n+class KafkaDontFailOnDataLossSuite extends KafkaMissingOffsetsTest {\n+\n+  import testImplicits._\n+\n+  private val topicId = new AtomicInteger(0)\n+\n+  private def newTopic(): String = s\"failOnDataLoss-${topicId.getAndIncrement()}\"\n+\n+  /**\n+   * @param testStreamingQuery whether to test a streaming query or a batch query.\n+   * @param writeToTable the function to write the specified [[DataFrame]] to the given table.\n+   */\n+  private def verifyMissingOffsetsDontCauseDuplicatedRecords(\n+      testStreamingQuery: Boolean)(writeToTable: (DataFrame, String) => Unit): Unit = {\n+    val topic = newTopic()\n+    testUtils.createTopic(topic, partitions = 1)\n+    testUtils.sendMessages(topic, (0 until 50).map(_.toString).toArray)\n+\n+    eventually(timeout(60.seconds)) {\n+      assert(\n+        testUtils.getEarliestOffsets(Set(topic)).head._2 > 0,\n+        \"Kafka didn't delete records after 1 minute\")\n+    }\n+\n+    val table = \"DontFailOnDataLoss\"\n+    withTable(table) {\n+      val df = if (testStreamingQuery) {\n+        spark.readStream\n+          .format(\"kafka\")\n+          .option(\"kafka.bootstrap.servers\", testUtils.brokerAddress)\n+          .option(\"kafka.metadata.max.age.ms\", \"1\")\n+          .option(\"subscribe\", topic)\n+          .option(\"startingOffsets\", s\"\"\"{\"$topic\":{\"0\":0}}\"\"\")\n+          .option(\"failOnDataLoss\", \"false\")\n+          .option(\"kafkaConsumer.pollTimeoutMs\", \"1000\")\n+          .load()\n+      } else {\n+        spark.read\n+          .format(\"kafka\")\n+          .option(\"kafka.bootstrap.servers\", testUtils.brokerAddress)\n+          .option(\"kafka.metadata.max.age.ms\", \"1\")\n+          .option(\"subscribe\", topic)\n+          .option(\"startingOffsets\", s\"\"\"{\"$topic\":{\"0\":0}}\"\"\")\n+          .option(\"failOnDataLoss\", \"false\")\n+          .option(\"kafkaConsumer.pollTimeoutMs\", \"1000\")\n+          .load()\n+      }\n+      writeToTable(df.selectExpr(\"CAST(value AS STRING)\"), table)\n+      val result = spark.table(table).as[String].collect().toList\n+      assert(result.distinct.size === result.size, s\"$result contains duplicated records\")\n+      // Make sure Kafka did remove some records so that this test is valid.\n+      assert(result.size > 0 && result.size < 50)\n+    }\n+  }\n+\n+  test(\"failOnDataLoss=false should not return duplicated records: v1\") {\n+    withSQLConf(\n+      \"spark.sql.streaming.disabledV2MicroBatchReaders\" ->\n+        classOf[KafkaSourceProvider].getCanonicalName) {\n+      verifyMissingOffsetsDontCauseDuplicatedRecords(testStreamingQuery = true) { (df, table) =>\n+        val query = df.writeStream.format(\"memory\").queryName(table).start()\n+        try {\n+          query.processAllAvailable()\n+        } finally {\n+          query.stop()\n+        }\n+      }\n+    }\n+  }\n+\n+  test(\"failOnDataLoss=false should not return duplicated records: v2\") {\n+    verifyMissingOffsetsDontCauseDuplicatedRecords(testStreamingQuery = true) { (df, table) =>\n+      val query = df.writeStream.format(\"memory\").queryName(table).start()\n+      try {\n+        query.processAllAvailable()\n+      } finally {\n+        query.stop()\n+      }\n+    }\n+  }\n+\n+  test(\"failOnDataLoss=false should not return duplicated records: continuous processing\") {\n+    verifyMissingOffsetsDontCauseDuplicatedRecords(testStreamingQuery = true) { (df, table) =>\n+      val query = df.writeStream\n+        .format(\"memory\")\n+        .queryName(table)\n+        .trigger(Trigger.Continuous(100))\n+        .start()\n+      try {\n+        eventually(timeout(60.seconds)) {\n+          assert(spark.table(table).as[String].collect().contains(\"49\"))"
  }, {
    "author": {
      "login": "zsxwing"
    },
    "body": "I didn't know it works!",
    "commit": "351527555c53a3977aa86a7057ed0aa12fb0976e",
    "createdAt": "2018-08-24T18:04:14Z",
    "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.util.Properties\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable\n+import scala.util.Random\n+\n+import org.scalatest.time.SpanSugar._\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{DataFrame, Dataset, ForeachWriter}\n+import org.apache.spark.sql.streaming.{StreamTest, Trigger}\n+import org.apache.spark.sql.test.{SharedSQLContext, TestSparkSession}\n+\n+/**\n+ * This is a basic test trait which will set up a Kafka cluster that keeps only several records in\n+ * a topic and ages out records very quickly. This is a helper trait to test\n+ * \"failonDataLoss=false\" case with missing offsets.\n+ *\n+ * Note: there is a hard-code 30 seconds delay (kafka.log.LogManager.InitialTaskDelayMs) to clean up\n+ * records. Hence each class extending this trait needs to wait at least 30 seconds (or even longer\n+ * when running on a slow Jenkins machine) before records start to be removed. To make sure a test\n+ * does see missing offsets, you can check the earliest offset in `eventually` and make sure it's\n+ * not 0 rather than sleeping a hard-code duration.\n+ */\n+trait KafkaMissingOffsetsTest extends SharedSQLContext {\n+\n+  protected var testUtils: KafkaTestUtils = _\n+\n+  override def createSparkSession(): TestSparkSession = {\n+    // Set maxRetries to 3 to handle NPE from `poll` when deleting a topic\n+    new TestSparkSession(new SparkContext(\"local[2,3]\", \"test-sql-context\", sparkConf))\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    testUtils = new KafkaTestUtils {\n+      override def brokerConfiguration: Properties = {\n+        val props = super.brokerConfiguration\n+        // Try to make Kafka clean up messages as fast as possible. However, there is a hard-code\n+        // 30 seconds delay (kafka.log.LogManager.InitialTaskDelayMs) so this test should run at\n+        // least 30 seconds.\n+        props.put(\"log.cleaner.backoff.ms\", \"100\")\n+        // The size of RecordBatch V2 increases to support transactional write.\n+        props.put(\"log.segment.bytes\", \"70\")\n+        props.put(\"log.retention.bytes\", \"40\")\n+        props.put(\"log.retention.check.interval.ms\", \"100\")\n+        props.put(\"delete.retention.ms\", \"10\")\n+        props.put(\"log.flush.scheduler.interval.ms\", \"10\")\n+        props\n+      }\n+    }\n+    testUtils.setup()\n+  }\n+\n+  override def afterAll(): Unit = {\n+    if (testUtils != null) {\n+      testUtils.teardown()\n+      testUtils = null\n+    }\n+    super.afterAll()\n+  }\n+}\n+\n+class KafkaDontFailOnDataLossSuite extends KafkaMissingOffsetsTest {\n+\n+  import testImplicits._\n+\n+  private val topicId = new AtomicInteger(0)\n+\n+  private def newTopic(): String = s\"failOnDataLoss-${topicId.getAndIncrement()}\"\n+\n+  /**\n+   * @param testStreamingQuery whether to test a streaming query or a batch query.\n+   * @param writeToTable the function to write the specified [[DataFrame]] to the given table.\n+   */\n+  private def verifyMissingOffsetsDontCauseDuplicatedRecords(\n+      testStreamingQuery: Boolean)(writeToTable: (DataFrame, String) => Unit): Unit = {\n+    val topic = newTopic()\n+    testUtils.createTopic(topic, partitions = 1)\n+    testUtils.sendMessages(topic, (0 until 50).map(_.toString).toArray)\n+\n+    eventually(timeout(60.seconds)) {\n+      assert(\n+        testUtils.getEarliestOffsets(Set(topic)).head._2 > 0,\n+        \"Kafka didn't delete records after 1 minute\")\n+    }\n+\n+    val table = \"DontFailOnDataLoss\"\n+    withTable(table) {\n+      val df = if (testStreamingQuery) {\n+        spark.readStream\n+          .format(\"kafka\")\n+          .option(\"kafka.bootstrap.servers\", testUtils.brokerAddress)\n+          .option(\"kafka.metadata.max.age.ms\", \"1\")\n+          .option(\"subscribe\", topic)\n+          .option(\"startingOffsets\", s\"\"\"{\"$topic\":{\"0\":0}}\"\"\")\n+          .option(\"failOnDataLoss\", \"false\")\n+          .option(\"kafkaConsumer.pollTimeoutMs\", \"1000\")\n+          .load()\n+      } else {\n+        spark.read\n+          .format(\"kafka\")\n+          .option(\"kafka.bootstrap.servers\", testUtils.brokerAddress)\n+          .option(\"kafka.metadata.max.age.ms\", \"1\")\n+          .option(\"subscribe\", topic)\n+          .option(\"startingOffsets\", s\"\"\"{\"$topic\":{\"0\":0}}\"\"\")\n+          .option(\"failOnDataLoss\", \"false\")\n+          .option(\"kafkaConsumer.pollTimeoutMs\", \"1000\")\n+          .load()\n+      }\n+      writeToTable(df.selectExpr(\"CAST(value AS STRING)\"), table)\n+      val result = spark.table(table).as[String].collect().toList\n+      assert(result.distinct.size === result.size, s\"$result contains duplicated records\")\n+      // Make sure Kafka did remove some records so that this test is valid.\n+      assert(result.size > 0 && result.size < 50)\n+    }\n+  }\n+\n+  test(\"failOnDataLoss=false should not return duplicated records: v1\") {\n+    withSQLConf(\n+      \"spark.sql.streaming.disabledV2MicroBatchReaders\" ->\n+        classOf[KafkaSourceProvider].getCanonicalName) {\n+      verifyMissingOffsetsDontCauseDuplicatedRecords(testStreamingQuery = true) { (df, table) =>\n+        val query = df.writeStream.format(\"memory\").queryName(table).start()\n+        try {\n+          query.processAllAvailable()\n+        } finally {\n+          query.stop()\n+        }\n+      }\n+    }\n+  }\n+\n+  test(\"failOnDataLoss=false should not return duplicated records: v2\") {\n+    verifyMissingOffsetsDontCauseDuplicatedRecords(testStreamingQuery = true) { (df, table) =>\n+      val query = df.writeStream.format(\"memory\").queryName(table).start()\n+      try {\n+        query.processAllAvailable()\n+      } finally {\n+        query.stop()\n+      }\n+    }\n+  }\n+\n+  test(\"failOnDataLoss=false should not return duplicated records: continuous processing\") {\n+    verifyMissingOffsetsDontCauseDuplicatedRecords(testStreamingQuery = true) { (df, table) =>\n+      val query = df.writeStream\n+        .format(\"memory\")\n+        .queryName(table)\n+        .trigger(Trigger.Continuous(100))\n+        .start()\n+      try {\n+        eventually(timeout(60.seconds)) {\n+          assert(spark.table(table).as[String].collect().contains(\"49\"))"
  }],
  "prId": 22207
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: make single line.",
    "commit": "351527555c53a3977aa86a7057ed0aa12fb0976e",
    "createdAt": "2018-08-24T17:54:20Z",
    "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.util.Properties\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable\n+import scala.util.Random\n+\n+import org.scalatest.time.SpanSugar._\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{DataFrame, Dataset, ForeachWriter}\n+import org.apache.spark.sql.streaming.{StreamTest, Trigger}\n+import org.apache.spark.sql.test.{SharedSQLContext, TestSparkSession}\n+\n+/**\n+ * This is a basic test trait which will set up a Kafka cluster that keeps only several records in\n+ * a topic and ages out records very quickly. This is a helper trait to test\n+ * \"failonDataLoss=false\" case with missing offsets.\n+ *\n+ * Note: there is a hard-code 30 seconds delay (kafka.log.LogManager.InitialTaskDelayMs) to clean up\n+ * records. Hence each class extending this trait needs to wait at least 30 seconds (or even longer\n+ * when running on a slow Jenkins machine) before records start to be removed. To make sure a test\n+ * does see missing offsets, you can check the earliest offset in `eventually` and make sure it's\n+ * not 0 rather than sleeping a hard-code duration.\n+ */\n+trait KafkaMissingOffsetsTest extends SharedSQLContext {\n+\n+  protected var testUtils: KafkaTestUtils = _\n+\n+  override def createSparkSession(): TestSparkSession = {\n+    // Set maxRetries to 3 to handle NPE from `poll` when deleting a topic\n+    new TestSparkSession(new SparkContext(\"local[2,3]\", \"test-sql-context\", sparkConf))\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    testUtils = new KafkaTestUtils {\n+      override def brokerConfiguration: Properties = {\n+        val props = super.brokerConfiguration\n+        // Try to make Kafka clean up messages as fast as possible. However, there is a hard-code\n+        // 30 seconds delay (kafka.log.LogManager.InitialTaskDelayMs) so this test should run at\n+        // least 30 seconds.\n+        props.put(\"log.cleaner.backoff.ms\", \"100\")\n+        // The size of RecordBatch V2 increases to support transactional write.\n+        props.put(\"log.segment.bytes\", \"70\")\n+        props.put(\"log.retention.bytes\", \"40\")\n+        props.put(\"log.retention.check.interval.ms\", \"100\")\n+        props.put(\"delete.retention.ms\", \"10\")\n+        props.put(\"log.flush.scheduler.interval.ms\", \"10\")\n+        props\n+      }\n+    }\n+    testUtils.setup()\n+  }\n+\n+  override def afterAll(): Unit = {\n+    if (testUtils != null) {\n+      testUtils.teardown()\n+      testUtils = null\n+    }\n+    super.afterAll()\n+  }\n+}\n+\n+class KafkaDontFailOnDataLossSuite extends KafkaMissingOffsetsTest {\n+\n+  import testImplicits._\n+\n+  private val topicId = new AtomicInteger(0)\n+\n+  private def newTopic(): String = s\"failOnDataLoss-${topicId.getAndIncrement()}\"\n+\n+  /**\n+   * @param testStreamingQuery whether to test a streaming query or a batch query.\n+   * @param writeToTable the function to write the specified [[DataFrame]] to the given table.\n+   */\n+  private def verifyMissingOffsetsDontCauseDuplicatedRecords(\n+      testStreamingQuery: Boolean)(writeToTable: (DataFrame, String) => Unit): Unit = {\n+    val topic = newTopic()\n+    testUtils.createTopic(topic, partitions = 1)\n+    testUtils.sendMessages(topic, (0 until 50).map(_.toString).toArray)\n+\n+    eventually(timeout(60.seconds)) {\n+      assert(\n+        testUtils.getEarliestOffsets(Set(topic)).head._2 > 0,\n+        \"Kafka didn't delete records after 1 minute\")\n+    }\n+\n+    val table = \"DontFailOnDataLoss\"\n+    withTable(table) {\n+      val df = if (testStreamingQuery) {\n+        spark.readStream\n+          .format(\"kafka\")\n+          .option(\"kafka.bootstrap.servers\", testUtils.brokerAddress)\n+          .option(\"kafka.metadata.max.age.ms\", \"1\")\n+          .option(\"subscribe\", topic)\n+          .option(\"startingOffsets\", s\"\"\"{\"$topic\":{\"0\":0}}\"\"\")\n+          .option(\"failOnDataLoss\", \"false\")\n+          .option(\"kafkaConsumer.pollTimeoutMs\", \"1000\")\n+          .load()\n+      } else {\n+        spark.read\n+          .format(\"kafka\")\n+          .option(\"kafka.bootstrap.servers\", testUtils.brokerAddress)\n+          .option(\"kafka.metadata.max.age.ms\", \"1\")\n+          .option(\"subscribe\", topic)\n+          .option(\"startingOffsets\", s\"\"\"{\"$topic\":{\"0\":0}}\"\"\")\n+          .option(\"failOnDataLoss\", \"false\")\n+          .option(\"kafkaConsumer.pollTimeoutMs\", \"1000\")\n+          .load()\n+      }\n+      writeToTable(df.selectExpr(\"CAST(value AS STRING)\"), table)\n+      val result = spark.table(table).as[String].collect().toList\n+      assert(result.distinct.size === result.size, s\"$result contains duplicated records\")\n+      // Make sure Kafka did remove some records so that this test is valid.\n+      assert(result.size > 0 && result.size < 50)\n+    }\n+  }\n+\n+  test(\"failOnDataLoss=false should not return duplicated records: v1\") {\n+    withSQLConf(\n+      \"spark.sql.streaming.disabledV2MicroBatchReaders\" ->\n+        classOf[KafkaSourceProvider].getCanonicalName) {\n+      verifyMissingOffsetsDontCauseDuplicatedRecords(testStreamingQuery = true) { (df, table) =>\n+        val query = df.writeStream.format(\"memory\").queryName(table).start()\n+        try {\n+          query.processAllAvailable()\n+        } finally {\n+          query.stop()\n+        }\n+      }\n+    }\n+  }\n+\n+  test(\"failOnDataLoss=false should not return duplicated records: v2\") {\n+    verifyMissingOffsetsDontCauseDuplicatedRecords(testStreamingQuery = true) { (df, table) =>\n+      val query = df.writeStream.format(\"memory\").queryName(table).start()\n+      try {\n+        query.processAllAvailable()\n+      } finally {\n+        query.stop()\n+      }\n+    }\n+  }\n+\n+  test(\"failOnDataLoss=false should not return duplicated records: continuous processing\") {\n+    verifyMissingOffsetsDontCauseDuplicatedRecords(testStreamingQuery = true) { (df, table) =>\n+      val query = df.writeStream\n+        .format(\"memory\")\n+        .queryName(table)\n+        .trigger(Trigger.Continuous(100))\n+        .start()\n+      try {\n+        eventually(timeout(60.seconds)) {\n+          assert(spark.table(table).as[String].collect().contains(\"49\"))\n+        }\n+      } finally {\n+        query.stop()\n+      }\n+    }\n+  }\n+\n+  test(\"failOnDataLoss=false should not return duplicated records: batch\") {\n+    verifyMissingOffsetsDontCauseDuplicatedRecords(testStreamingQuery = false) { (df, table) =>\n+      df.write.saveAsTable(table)\n+    }\n+  }\n+}\n+\n+class KafkaSourceStressForDontFailOnDataLossSuite extends StreamTest with KafkaMissingOffsetsTest {\n+\n+  import testImplicits._\n+\n+  private val topicId = new AtomicInteger(0)\n+\n+  private def newTopic(): String = s\"failOnDataLoss-${topicId.getAndIncrement()}\"\n+\n+  protected def startStream(ds: Dataset[Int]) = {\n+    ds.writeStream.foreach(new ForeachWriter[Int] {\n+\n+      override def open(partitionId: Long, version: Long): Boolean = {"
  }],
  "prId": 22207
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: make single line.",
    "commit": "351527555c53a3977aa86a7057ed0aa12fb0976e",
    "createdAt": "2018-08-24T17:54:30Z",
    "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.util.Properties\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.mutable\n+import scala.util.Random\n+\n+import org.scalatest.time.SpanSugar._\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{DataFrame, Dataset, ForeachWriter}\n+import org.apache.spark.sql.streaming.{StreamTest, Trigger}\n+import org.apache.spark.sql.test.{SharedSQLContext, TestSparkSession}\n+\n+/**\n+ * This is a basic test trait which will set up a Kafka cluster that keeps only several records in\n+ * a topic and ages out records very quickly. This is a helper trait to test\n+ * \"failonDataLoss=false\" case with missing offsets.\n+ *\n+ * Note: there is a hard-code 30 seconds delay (kafka.log.LogManager.InitialTaskDelayMs) to clean up\n+ * records. Hence each class extending this trait needs to wait at least 30 seconds (or even longer\n+ * when running on a slow Jenkins machine) before records start to be removed. To make sure a test\n+ * does see missing offsets, you can check the earliest offset in `eventually` and make sure it's\n+ * not 0 rather than sleeping a hard-code duration.\n+ */\n+trait KafkaMissingOffsetsTest extends SharedSQLContext {\n+\n+  protected var testUtils: KafkaTestUtils = _\n+\n+  override def createSparkSession(): TestSparkSession = {\n+    // Set maxRetries to 3 to handle NPE from `poll` when deleting a topic\n+    new TestSparkSession(new SparkContext(\"local[2,3]\", \"test-sql-context\", sparkConf))\n+  }\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    testUtils = new KafkaTestUtils {\n+      override def brokerConfiguration: Properties = {\n+        val props = super.brokerConfiguration\n+        // Try to make Kafka clean up messages as fast as possible. However, there is a hard-code\n+        // 30 seconds delay (kafka.log.LogManager.InitialTaskDelayMs) so this test should run at\n+        // least 30 seconds.\n+        props.put(\"log.cleaner.backoff.ms\", \"100\")\n+        // The size of RecordBatch V2 increases to support transactional write.\n+        props.put(\"log.segment.bytes\", \"70\")\n+        props.put(\"log.retention.bytes\", \"40\")\n+        props.put(\"log.retention.check.interval.ms\", \"100\")\n+        props.put(\"delete.retention.ms\", \"10\")\n+        props.put(\"log.flush.scheduler.interval.ms\", \"10\")\n+        props\n+      }\n+    }\n+    testUtils.setup()\n+  }\n+\n+  override def afterAll(): Unit = {\n+    if (testUtils != null) {\n+      testUtils.teardown()\n+      testUtils = null\n+    }\n+    super.afterAll()\n+  }\n+}\n+\n+class KafkaDontFailOnDataLossSuite extends KafkaMissingOffsetsTest {\n+\n+  import testImplicits._\n+\n+  private val topicId = new AtomicInteger(0)\n+\n+  private def newTopic(): String = s\"failOnDataLoss-${topicId.getAndIncrement()}\"\n+\n+  /**\n+   * @param testStreamingQuery whether to test a streaming query or a batch query.\n+   * @param writeToTable the function to write the specified [[DataFrame]] to the given table.\n+   */\n+  private def verifyMissingOffsetsDontCauseDuplicatedRecords(\n+      testStreamingQuery: Boolean)(writeToTable: (DataFrame, String) => Unit): Unit = {\n+    val topic = newTopic()\n+    testUtils.createTopic(topic, partitions = 1)\n+    testUtils.sendMessages(topic, (0 until 50).map(_.toString).toArray)\n+\n+    eventually(timeout(60.seconds)) {\n+      assert(\n+        testUtils.getEarliestOffsets(Set(topic)).head._2 > 0,\n+        \"Kafka didn't delete records after 1 minute\")\n+    }\n+\n+    val table = \"DontFailOnDataLoss\"\n+    withTable(table) {\n+      val df = if (testStreamingQuery) {\n+        spark.readStream\n+          .format(\"kafka\")\n+          .option(\"kafka.bootstrap.servers\", testUtils.brokerAddress)\n+          .option(\"kafka.metadata.max.age.ms\", \"1\")\n+          .option(\"subscribe\", topic)\n+          .option(\"startingOffsets\", s\"\"\"{\"$topic\":{\"0\":0}}\"\"\")\n+          .option(\"failOnDataLoss\", \"false\")\n+          .option(\"kafkaConsumer.pollTimeoutMs\", \"1000\")\n+          .load()\n+      } else {\n+        spark.read\n+          .format(\"kafka\")\n+          .option(\"kafka.bootstrap.servers\", testUtils.brokerAddress)\n+          .option(\"kafka.metadata.max.age.ms\", \"1\")\n+          .option(\"subscribe\", topic)\n+          .option(\"startingOffsets\", s\"\"\"{\"$topic\":{\"0\":0}}\"\"\")\n+          .option(\"failOnDataLoss\", \"false\")\n+          .option(\"kafkaConsumer.pollTimeoutMs\", \"1000\")\n+          .load()\n+      }\n+      writeToTable(df.selectExpr(\"CAST(value AS STRING)\"), table)\n+      val result = spark.table(table).as[String].collect().toList\n+      assert(result.distinct.size === result.size, s\"$result contains duplicated records\")\n+      // Make sure Kafka did remove some records so that this test is valid.\n+      assert(result.size > 0 && result.size < 50)\n+    }\n+  }\n+\n+  test(\"failOnDataLoss=false should not return duplicated records: v1\") {\n+    withSQLConf(\n+      \"spark.sql.streaming.disabledV2MicroBatchReaders\" ->\n+        classOf[KafkaSourceProvider].getCanonicalName) {\n+      verifyMissingOffsetsDontCauseDuplicatedRecords(testStreamingQuery = true) { (df, table) =>\n+        val query = df.writeStream.format(\"memory\").queryName(table).start()\n+        try {\n+          query.processAllAvailable()\n+        } finally {\n+          query.stop()\n+        }\n+      }\n+    }\n+  }\n+\n+  test(\"failOnDataLoss=false should not return duplicated records: v2\") {\n+    verifyMissingOffsetsDontCauseDuplicatedRecords(testStreamingQuery = true) { (df, table) =>\n+      val query = df.writeStream.format(\"memory\").queryName(table).start()\n+      try {\n+        query.processAllAvailable()\n+      } finally {\n+        query.stop()\n+      }\n+    }\n+  }\n+\n+  test(\"failOnDataLoss=false should not return duplicated records: continuous processing\") {\n+    verifyMissingOffsetsDontCauseDuplicatedRecords(testStreamingQuery = true) { (df, table) =>\n+      val query = df.writeStream\n+        .format(\"memory\")\n+        .queryName(table)\n+        .trigger(Trigger.Continuous(100))\n+        .start()\n+      try {\n+        eventually(timeout(60.seconds)) {\n+          assert(spark.table(table).as[String].collect().contains(\"49\"))\n+        }\n+      } finally {\n+        query.stop()\n+      }\n+    }\n+  }\n+\n+  test(\"failOnDataLoss=false should not return duplicated records: batch\") {\n+    verifyMissingOffsetsDontCauseDuplicatedRecords(testStreamingQuery = false) { (df, table) =>\n+      df.write.saveAsTable(table)\n+    }\n+  }\n+}\n+\n+class KafkaSourceStressForDontFailOnDataLossSuite extends StreamTest with KafkaMissingOffsetsTest {\n+\n+  import testImplicits._\n+\n+  private val topicId = new AtomicInteger(0)\n+\n+  private def newTopic(): String = s\"failOnDataLoss-${topicId.getAndIncrement()}\"\n+\n+  protected def startStream(ds: Dataset[Int]) = {\n+    ds.writeStream.foreach(new ForeachWriter[Int] {\n+\n+      override def open(partitionId: Long, version: Long): Boolean = {\n+        true\n+      }\n+\n+      override def process(value: Int): Unit = {\n+        // Slow down the processing speed so that messages may be aged out.\n+        Thread.sleep(Random.nextInt(500))\n+      }\n+\n+      override def close(errorOrNull: Throwable): Unit = {"
  }],
  "prId": 22207
}]