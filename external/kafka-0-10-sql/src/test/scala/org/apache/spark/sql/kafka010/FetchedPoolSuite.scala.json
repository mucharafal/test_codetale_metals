[{
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "This stays in infinite loop if `startEvictorThread` throws exception.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-04T06:26:02Z",
    "diffHunk": "@@ -0,0 +1,299 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+import org.apache.spark.sql.test.SharedSQLContext\n+\n+class FetchedPoolSuite extends SharedSQLContext {\n+  type Record = ConsumerRecord[Array[Byte], Array[Byte]]\n+\n+  private val dummyBytes = \"dummy\".getBytes\n+\n+  test(\"acquire fresh one\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKey = CacheKey(\"testgroup\", new TopicPartition(\"topic\", 0))\n+\n+    assert(dataPool.getCache.get(cacheKey).isEmpty)\n+\n+    val data = dataPool.acquire(cacheKey, 0)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    data.withNewPoll(testRecords(0, 5).listIterator, 5)\n+\n+    dataPool.release(cacheKey, data)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(!dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"acquire fetched data from multiple keys\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKeys = (0 to 10).map { partId =>\n+      CacheKey(\"testgroup\", new TopicPartition(\"topic\", partId))\n+    }\n+\n+    assert(dataPool.getCache.size === 0)\n+    cacheKeys.foreach { key => assert(dataPool.getCache.get(key).isEmpty) }\n+\n+    val dataList = cacheKeys.map(key => (key, dataPool.acquire(key, 0)))\n+\n+    assert(dataPool.getCache.size === cacheKeys.size)\n+    cacheKeys.map { key =>\n+      assert(dataPool.getCache(key).size === 1)\n+      assert(dataPool.getCache(key).head.inUse)\n+    }\n+\n+    dataList.map { case (_, data) =>\n+      data.withNewPoll(testRecords(0, 5).listIterator, 5)\n+    }\n+\n+    dataList.foreach { case (key, data) =>\n+      dataPool.release(key, data)\n+    }\n+\n+    assert(dataPool.getCache.size === cacheKeys.size)\n+    cacheKeys.map { key =>\n+      assert(dataPool.getCache(key).size === 1)\n+      assert(!dataPool.getCache(key).head.inUse)\n+    }\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"continuous use of fetched data from single key\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKey = CacheKey(\"testgroup\", new TopicPartition(\"topic\", 0))\n+\n+    assert(dataPool.getCache.get(cacheKey).isEmpty)\n+\n+    val data = dataPool.acquire(cacheKey, 0)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    data.withNewPoll(testRecords(0, 5).listIterator, 5)\n+\n+    (0 to 3).foreach { _ => data.next() }\n+\n+    dataPool.release(cacheKey, data)\n+\n+    // suppose next batch\n+\n+    val data2 = dataPool.acquire(cacheKey, data.nextOffsetInFetchedData)\n+\n+    assert(data.eq(data2))\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.release(cacheKey, data2)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(!dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"multiple tasks referring same key continuously using fetched data\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKey = CacheKey(\"testgroup\", new TopicPartition(\"topic\", 0))\n+\n+    assert(dataPool.getCache.get(cacheKey).isEmpty)\n+\n+    val dataFromTask1 = dataPool.acquire(cacheKey, 0)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    val dataFromTask2 = dataPool.acquire(cacheKey, 0)\n+\n+    // it shouldn't give same object as dataFromTask1 though it asks same offset\n+    // it definitely works when offsets are not overlapped: skip adding test for that\n+    assert(dataPool.getCache(cacheKey).size === 2)\n+    assert(dataPool.getCache(cacheKey)(1).inUse)\n+\n+    // reading from task 1\n+    dataFromTask1.withNewPoll(testRecords(0, 5).listIterator, 5)\n+\n+    (0 to 3).foreach { _ => dataFromTask1.next() }\n+\n+    dataPool.release(cacheKey, dataFromTask1)\n+\n+    // reading from task 2\n+    dataFromTask2.withNewPoll(testRecords(0, 30).listIterator, 30)\n+\n+    (0 to 5).foreach { _ => dataFromTask2.next() }\n+\n+    dataPool.release(cacheKey, dataFromTask2)\n+\n+    // suppose next batch for task 1\n+    val data2FromTask1 = dataPool.acquire(cacheKey, dataFromTask1.nextOffsetInFetchedData)\n+    assert(data2FromTask1.eq(dataFromTask1))\n+\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    // suppose next batch for task 2\n+    val data2FromTask2 = dataPool.acquire(cacheKey, dataFromTask2.nextOffsetInFetchedData)\n+    assert(data2FromTask2.eq(dataFromTask2))\n+\n+    assert(dataPool.getCache(cacheKey)(1).inUse)\n+\n+    // release from task 2\n+    dataPool.release(cacheKey, data2FromTask2)\n+    assert(!dataPool.getCache(cacheKey)(1).inUse)\n+\n+    // release from task 1\n+    dataPool.release(cacheKey, data2FromTask1)\n+    assert(!dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"evict idle fetched data\") {"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "I'm not sure I'm seeing the scenario you're referring. Could you elaborate? Don't `eventually` and `assert` avoid such situation?",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-04T13:43:25Z",
    "diffHunk": "@@ -0,0 +1,299 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+import org.apache.spark.sql.test.SharedSQLContext\n+\n+class FetchedPoolSuite extends SharedSQLContext {\n+  type Record = ConsumerRecord[Array[Byte], Array[Byte]]\n+\n+  private val dummyBytes = \"dummy\".getBytes\n+\n+  test(\"acquire fresh one\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKey = CacheKey(\"testgroup\", new TopicPartition(\"topic\", 0))\n+\n+    assert(dataPool.getCache.get(cacheKey).isEmpty)\n+\n+    val data = dataPool.acquire(cacheKey, 0)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    data.withNewPoll(testRecords(0, 5).listIterator, 5)\n+\n+    dataPool.release(cacheKey, data)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(!dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"acquire fetched data from multiple keys\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKeys = (0 to 10).map { partId =>\n+      CacheKey(\"testgroup\", new TopicPartition(\"topic\", partId))\n+    }\n+\n+    assert(dataPool.getCache.size === 0)\n+    cacheKeys.foreach { key => assert(dataPool.getCache.get(key).isEmpty) }\n+\n+    val dataList = cacheKeys.map(key => (key, dataPool.acquire(key, 0)))\n+\n+    assert(dataPool.getCache.size === cacheKeys.size)\n+    cacheKeys.map { key =>\n+      assert(dataPool.getCache(key).size === 1)\n+      assert(dataPool.getCache(key).head.inUse)\n+    }\n+\n+    dataList.map { case (_, data) =>\n+      data.withNewPoll(testRecords(0, 5).listIterator, 5)\n+    }\n+\n+    dataList.foreach { case (key, data) =>\n+      dataPool.release(key, data)\n+    }\n+\n+    assert(dataPool.getCache.size === cacheKeys.size)\n+    cacheKeys.map { key =>\n+      assert(dataPool.getCache(key).size === 1)\n+      assert(!dataPool.getCache(key).head.inUse)\n+    }\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"continuous use of fetched data from single key\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKey = CacheKey(\"testgroup\", new TopicPartition(\"topic\", 0))\n+\n+    assert(dataPool.getCache.get(cacheKey).isEmpty)\n+\n+    val data = dataPool.acquire(cacheKey, 0)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    data.withNewPoll(testRecords(0, 5).listIterator, 5)\n+\n+    (0 to 3).foreach { _ => data.next() }\n+\n+    dataPool.release(cacheKey, data)\n+\n+    // suppose next batch\n+\n+    val data2 = dataPool.acquire(cacheKey, data.nextOffsetInFetchedData)\n+\n+    assert(data.eq(data2))\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.release(cacheKey, data2)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(!dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"multiple tasks referring same key continuously using fetched data\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKey = CacheKey(\"testgroup\", new TopicPartition(\"topic\", 0))\n+\n+    assert(dataPool.getCache.get(cacheKey).isEmpty)\n+\n+    val dataFromTask1 = dataPool.acquire(cacheKey, 0)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    val dataFromTask2 = dataPool.acquire(cacheKey, 0)\n+\n+    // it shouldn't give same object as dataFromTask1 though it asks same offset\n+    // it definitely works when offsets are not overlapped: skip adding test for that\n+    assert(dataPool.getCache(cacheKey).size === 2)\n+    assert(dataPool.getCache(cacheKey)(1).inUse)\n+\n+    // reading from task 1\n+    dataFromTask1.withNewPoll(testRecords(0, 5).listIterator, 5)\n+\n+    (0 to 3).foreach { _ => dataFromTask1.next() }\n+\n+    dataPool.release(cacheKey, dataFromTask1)\n+\n+    // reading from task 2\n+    dataFromTask2.withNewPoll(testRecords(0, 30).listIterator, 30)\n+\n+    (0 to 5).foreach { _ => dataFromTask2.next() }\n+\n+    dataPool.release(cacheKey, dataFromTask2)\n+\n+    // suppose next batch for task 1\n+    val data2FromTask1 = dataPool.acquire(cacheKey, dataFromTask1.nextOffsetInFetchedData)\n+    assert(data2FromTask1.eq(dataFromTask1))\n+\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    // suppose next batch for task 2\n+    val data2FromTask2 = dataPool.acquire(cacheKey, dataFromTask2.nextOffsetInFetchedData)\n+    assert(data2FromTask2.eq(dataFromTask2))\n+\n+    assert(dataPool.getCache(cacheKey)(1).inUse)\n+\n+    // release from task 2\n+    dataPool.release(cacheKey, data2FromTask2)\n+    assert(!dataPool.getCache(cacheKey)(1).inUse)\n+\n+    // release from task 1\n+    dataPool.release(cacheKey, data2FromTask1)\n+    assert(!dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"evict idle fetched data\") {"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Just realized written it with bad function name. I've put `throw SparkException(\"foo\")` into `removeIdleFetchedData` and started the suite.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-05T13:50:46Z",
    "diffHunk": "@@ -0,0 +1,299 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+import org.apache.spark.sql.test.SharedSQLContext\n+\n+class FetchedPoolSuite extends SharedSQLContext {\n+  type Record = ConsumerRecord[Array[Byte], Array[Byte]]\n+\n+  private val dummyBytes = \"dummy\".getBytes\n+\n+  test(\"acquire fresh one\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKey = CacheKey(\"testgroup\", new TopicPartition(\"topic\", 0))\n+\n+    assert(dataPool.getCache.get(cacheKey).isEmpty)\n+\n+    val data = dataPool.acquire(cacheKey, 0)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    data.withNewPoll(testRecords(0, 5).listIterator, 5)\n+\n+    dataPool.release(cacheKey, data)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(!dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"acquire fetched data from multiple keys\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKeys = (0 to 10).map { partId =>\n+      CacheKey(\"testgroup\", new TopicPartition(\"topic\", partId))\n+    }\n+\n+    assert(dataPool.getCache.size === 0)\n+    cacheKeys.foreach { key => assert(dataPool.getCache.get(key).isEmpty) }\n+\n+    val dataList = cacheKeys.map(key => (key, dataPool.acquire(key, 0)))\n+\n+    assert(dataPool.getCache.size === cacheKeys.size)\n+    cacheKeys.map { key =>\n+      assert(dataPool.getCache(key).size === 1)\n+      assert(dataPool.getCache(key).head.inUse)\n+    }\n+\n+    dataList.map { case (_, data) =>\n+      data.withNewPoll(testRecords(0, 5).listIterator, 5)\n+    }\n+\n+    dataList.foreach { case (key, data) =>\n+      dataPool.release(key, data)\n+    }\n+\n+    assert(dataPool.getCache.size === cacheKeys.size)\n+    cacheKeys.map { key =>\n+      assert(dataPool.getCache(key).size === 1)\n+      assert(!dataPool.getCache(key).head.inUse)\n+    }\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"continuous use of fetched data from single key\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKey = CacheKey(\"testgroup\", new TopicPartition(\"topic\", 0))\n+\n+    assert(dataPool.getCache.get(cacheKey).isEmpty)\n+\n+    val data = dataPool.acquire(cacheKey, 0)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    data.withNewPoll(testRecords(0, 5).listIterator, 5)\n+\n+    (0 to 3).foreach { _ => data.next() }\n+\n+    dataPool.release(cacheKey, data)\n+\n+    // suppose next batch\n+\n+    val data2 = dataPool.acquire(cacheKey, data.nextOffsetInFetchedData)\n+\n+    assert(data.eq(data2))\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.release(cacheKey, data2)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(!dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"multiple tasks referring same key continuously using fetched data\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKey = CacheKey(\"testgroup\", new TopicPartition(\"topic\", 0))\n+\n+    assert(dataPool.getCache.get(cacheKey).isEmpty)\n+\n+    val dataFromTask1 = dataPool.acquire(cacheKey, 0)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    val dataFromTask2 = dataPool.acquire(cacheKey, 0)\n+\n+    // it shouldn't give same object as dataFromTask1 though it asks same offset\n+    // it definitely works when offsets are not overlapped: skip adding test for that\n+    assert(dataPool.getCache(cacheKey).size === 2)\n+    assert(dataPool.getCache(cacheKey)(1).inUse)\n+\n+    // reading from task 1\n+    dataFromTask1.withNewPoll(testRecords(0, 5).listIterator, 5)\n+\n+    (0 to 3).foreach { _ => dataFromTask1.next() }\n+\n+    dataPool.release(cacheKey, dataFromTask1)\n+\n+    // reading from task 2\n+    dataFromTask2.withNewPoll(testRecords(0, 30).listIterator, 30)\n+\n+    (0 to 5).foreach { _ => dataFromTask2.next() }\n+\n+    dataPool.release(cacheKey, dataFromTask2)\n+\n+    // suppose next batch for task 1\n+    val data2FromTask1 = dataPool.acquire(cacheKey, dataFromTask1.nextOffsetInFetchedData)\n+    assert(data2FromTask1.eq(dataFromTask1))\n+\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    // suppose next batch for task 2\n+    val data2FromTask2 = dataPool.acquire(cacheKey, dataFromTask2.nextOffsetInFetchedData)\n+    assert(data2FromTask2.eq(dataFromTask2))\n+\n+    assert(dataPool.getCache(cacheKey)(1).inUse)\n+\n+    // release from task 2\n+    dataPool.release(cacheKey, data2FromTask2)\n+    assert(!dataPool.getCache(cacheKey)(1).inUse)\n+\n+    // release from task 1\n+    dataPool.release(cacheKey, data2FromTask1)\n+    assert(!dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"evict idle fetched data\") {"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "To make clear once again, was your finding a false alarm, or there's something to fix which I can reproduce the issue easily?",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-05T15:16:12Z",
    "diffHunk": "@@ -0,0 +1,299 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+import org.apache.spark.sql.test.SharedSQLContext\n+\n+class FetchedPoolSuite extends SharedSQLContext {\n+  type Record = ConsumerRecord[Array[Byte], Array[Byte]]\n+\n+  private val dummyBytes = \"dummy\".getBytes\n+\n+  test(\"acquire fresh one\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKey = CacheKey(\"testgroup\", new TopicPartition(\"topic\", 0))\n+\n+    assert(dataPool.getCache.get(cacheKey).isEmpty)\n+\n+    val data = dataPool.acquire(cacheKey, 0)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    data.withNewPoll(testRecords(0, 5).listIterator, 5)\n+\n+    dataPool.release(cacheKey, data)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(!dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"acquire fetched data from multiple keys\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKeys = (0 to 10).map { partId =>\n+      CacheKey(\"testgroup\", new TopicPartition(\"topic\", partId))\n+    }\n+\n+    assert(dataPool.getCache.size === 0)\n+    cacheKeys.foreach { key => assert(dataPool.getCache.get(key).isEmpty) }\n+\n+    val dataList = cacheKeys.map(key => (key, dataPool.acquire(key, 0)))\n+\n+    assert(dataPool.getCache.size === cacheKeys.size)\n+    cacheKeys.map { key =>\n+      assert(dataPool.getCache(key).size === 1)\n+      assert(dataPool.getCache(key).head.inUse)\n+    }\n+\n+    dataList.map { case (_, data) =>\n+      data.withNewPoll(testRecords(0, 5).listIterator, 5)\n+    }\n+\n+    dataList.foreach { case (key, data) =>\n+      dataPool.release(key, data)\n+    }\n+\n+    assert(dataPool.getCache.size === cacheKeys.size)\n+    cacheKeys.map { key =>\n+      assert(dataPool.getCache(key).size === 1)\n+      assert(!dataPool.getCache(key).head.inUse)\n+    }\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"continuous use of fetched data from single key\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKey = CacheKey(\"testgroup\", new TopicPartition(\"topic\", 0))\n+\n+    assert(dataPool.getCache.get(cacheKey).isEmpty)\n+\n+    val data = dataPool.acquire(cacheKey, 0)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    data.withNewPoll(testRecords(0, 5).listIterator, 5)\n+\n+    (0 to 3).foreach { _ => data.next() }\n+\n+    dataPool.release(cacheKey, data)\n+\n+    // suppose next batch\n+\n+    val data2 = dataPool.acquire(cacheKey, data.nextOffsetInFetchedData)\n+\n+    assert(data.eq(data2))\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.release(cacheKey, data2)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(!dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"multiple tasks referring same key continuously using fetched data\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKey = CacheKey(\"testgroup\", new TopicPartition(\"topic\", 0))\n+\n+    assert(dataPool.getCache.get(cacheKey).isEmpty)\n+\n+    val dataFromTask1 = dataPool.acquire(cacheKey, 0)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    val dataFromTask2 = dataPool.acquire(cacheKey, 0)\n+\n+    // it shouldn't give same object as dataFromTask1 though it asks same offset\n+    // it definitely works when offsets are not overlapped: skip adding test for that\n+    assert(dataPool.getCache(cacheKey).size === 2)\n+    assert(dataPool.getCache(cacheKey)(1).inUse)\n+\n+    // reading from task 1\n+    dataFromTask1.withNewPoll(testRecords(0, 5).listIterator, 5)\n+\n+    (0 to 3).foreach { _ => dataFromTask1.next() }\n+\n+    dataPool.release(cacheKey, dataFromTask1)\n+\n+    // reading from task 2\n+    dataFromTask2.withNewPoll(testRecords(0, 30).listIterator, 30)\n+\n+    (0 to 5).foreach { _ => dataFromTask2.next() }\n+\n+    dataPool.release(cacheKey, dataFromTask2)\n+\n+    // suppose next batch for task 1\n+    val data2FromTask1 = dataPool.acquire(cacheKey, dataFromTask1.nextOffsetInFetchedData)\n+    assert(data2FromTask1.eq(dataFromTask1))\n+\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    // suppose next batch for task 2\n+    val data2FromTask2 = dataPool.acquire(cacheKey, dataFromTask2.nextOffsetInFetchedData)\n+    assert(data2FromTask2.eq(dataFromTask2))\n+\n+    assert(dataPool.getCache(cacheKey)(1).inUse)\n+\n+    // release from task 2\n+    dataPool.release(cacheKey, data2FromTask2)\n+    assert(!dataPool.getCache(cacheKey)(1).inUse)\n+\n+    // release from task 1\n+    dataPool.release(cacheKey, data2FromTask1)\n+    assert(!dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"evict idle fetched data\") {"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "That said just put `throw new SparkException(\"foo\")` at the beginning of `removeIdleFetchedData ` function and the mentioned test never fails. With the new try-catch block one can see the exception but not timing out or proceeding at all.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-06T11:00:24Z",
    "diffHunk": "@@ -0,0 +1,299 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+import org.apache.spark.sql.test.SharedSQLContext\n+\n+class FetchedPoolSuite extends SharedSQLContext {\n+  type Record = ConsumerRecord[Array[Byte], Array[Byte]]\n+\n+  private val dummyBytes = \"dummy\".getBytes\n+\n+  test(\"acquire fresh one\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKey = CacheKey(\"testgroup\", new TopicPartition(\"topic\", 0))\n+\n+    assert(dataPool.getCache.get(cacheKey).isEmpty)\n+\n+    val data = dataPool.acquire(cacheKey, 0)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    data.withNewPoll(testRecords(0, 5).listIterator, 5)\n+\n+    dataPool.release(cacheKey, data)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(!dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"acquire fetched data from multiple keys\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKeys = (0 to 10).map { partId =>\n+      CacheKey(\"testgroup\", new TopicPartition(\"topic\", partId))\n+    }\n+\n+    assert(dataPool.getCache.size === 0)\n+    cacheKeys.foreach { key => assert(dataPool.getCache.get(key).isEmpty) }\n+\n+    val dataList = cacheKeys.map(key => (key, dataPool.acquire(key, 0)))\n+\n+    assert(dataPool.getCache.size === cacheKeys.size)\n+    cacheKeys.map { key =>\n+      assert(dataPool.getCache(key).size === 1)\n+      assert(dataPool.getCache(key).head.inUse)\n+    }\n+\n+    dataList.map { case (_, data) =>\n+      data.withNewPoll(testRecords(0, 5).listIterator, 5)\n+    }\n+\n+    dataList.foreach { case (key, data) =>\n+      dataPool.release(key, data)\n+    }\n+\n+    assert(dataPool.getCache.size === cacheKeys.size)\n+    cacheKeys.map { key =>\n+      assert(dataPool.getCache(key).size === 1)\n+      assert(!dataPool.getCache(key).head.inUse)\n+    }\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"continuous use of fetched data from single key\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKey = CacheKey(\"testgroup\", new TopicPartition(\"topic\", 0))\n+\n+    assert(dataPool.getCache.get(cacheKey).isEmpty)\n+\n+    val data = dataPool.acquire(cacheKey, 0)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    data.withNewPoll(testRecords(0, 5).listIterator, 5)\n+\n+    (0 to 3).foreach { _ => data.next() }\n+\n+    dataPool.release(cacheKey, data)\n+\n+    // suppose next batch\n+\n+    val data2 = dataPool.acquire(cacheKey, data.nextOffsetInFetchedData)\n+\n+    assert(data.eq(data2))\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.release(cacheKey, data2)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(!dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"multiple tasks referring same key continuously using fetched data\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKey = CacheKey(\"testgroup\", new TopicPartition(\"topic\", 0))\n+\n+    assert(dataPool.getCache.get(cacheKey).isEmpty)\n+\n+    val dataFromTask1 = dataPool.acquire(cacheKey, 0)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    val dataFromTask2 = dataPool.acquire(cacheKey, 0)\n+\n+    // it shouldn't give same object as dataFromTask1 though it asks same offset\n+    // it definitely works when offsets are not overlapped: skip adding test for that\n+    assert(dataPool.getCache(cacheKey).size === 2)\n+    assert(dataPool.getCache(cacheKey)(1).inUse)\n+\n+    // reading from task 1\n+    dataFromTask1.withNewPoll(testRecords(0, 5).listIterator, 5)\n+\n+    (0 to 3).foreach { _ => dataFromTask1.next() }\n+\n+    dataPool.release(cacheKey, dataFromTask1)\n+\n+    // reading from task 2\n+    dataFromTask2.withNewPoll(testRecords(0, 30).listIterator, 30)\n+\n+    (0 to 5).foreach { _ => dataFromTask2.next() }\n+\n+    dataPool.release(cacheKey, dataFromTask2)\n+\n+    // suppose next batch for task 1\n+    val data2FromTask1 = dataPool.acquire(cacheKey, dataFromTask1.nextOffsetInFetchedData)\n+    assert(data2FromTask1.eq(dataFromTask1))\n+\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    // suppose next batch for task 2\n+    val data2FromTask2 = dataPool.acquire(cacheKey, dataFromTask2.nextOffsetInFetchedData)\n+    assert(data2FromTask2.eq(dataFromTask2))\n+\n+    assert(dataPool.getCache(cacheKey)(1).inUse)\n+\n+    // release from task 2\n+    dataPool.release(cacheKey, data2FromTask2)\n+    assert(!dataPool.getCache(cacheKey)(1).inUse)\n+\n+    // release from task 1\n+    dataPool.release(cacheKey, data2FromTask1)\n+    assert(!dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"evict idle fetched data\") {"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "As I see it never reaches `eventually`.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-06T11:45:09Z",
    "diffHunk": "@@ -0,0 +1,299 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+import org.apache.spark.sql.test.SharedSQLContext\n+\n+class FetchedPoolSuite extends SharedSQLContext {\n+  type Record = ConsumerRecord[Array[Byte], Array[Byte]]\n+\n+  private val dummyBytes = \"dummy\".getBytes\n+\n+  test(\"acquire fresh one\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKey = CacheKey(\"testgroup\", new TopicPartition(\"topic\", 0))\n+\n+    assert(dataPool.getCache.get(cacheKey).isEmpty)\n+\n+    val data = dataPool.acquire(cacheKey, 0)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    data.withNewPoll(testRecords(0, 5).listIterator, 5)\n+\n+    dataPool.release(cacheKey, data)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(!dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"acquire fetched data from multiple keys\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKeys = (0 to 10).map { partId =>\n+      CacheKey(\"testgroup\", new TopicPartition(\"topic\", partId))\n+    }\n+\n+    assert(dataPool.getCache.size === 0)\n+    cacheKeys.foreach { key => assert(dataPool.getCache.get(key).isEmpty) }\n+\n+    val dataList = cacheKeys.map(key => (key, dataPool.acquire(key, 0)))\n+\n+    assert(dataPool.getCache.size === cacheKeys.size)\n+    cacheKeys.map { key =>\n+      assert(dataPool.getCache(key).size === 1)\n+      assert(dataPool.getCache(key).head.inUse)\n+    }\n+\n+    dataList.map { case (_, data) =>\n+      data.withNewPoll(testRecords(0, 5).listIterator, 5)\n+    }\n+\n+    dataList.foreach { case (key, data) =>\n+      dataPool.release(key, data)\n+    }\n+\n+    assert(dataPool.getCache.size === cacheKeys.size)\n+    cacheKeys.map { key =>\n+      assert(dataPool.getCache(key).size === 1)\n+      assert(!dataPool.getCache(key).head.inUse)\n+    }\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"continuous use of fetched data from single key\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKey = CacheKey(\"testgroup\", new TopicPartition(\"topic\", 0))\n+\n+    assert(dataPool.getCache.get(cacheKey).isEmpty)\n+\n+    val data = dataPool.acquire(cacheKey, 0)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    data.withNewPoll(testRecords(0, 5).listIterator, 5)\n+\n+    (0 to 3).foreach { _ => data.next() }\n+\n+    dataPool.release(cacheKey, data)\n+\n+    // suppose next batch\n+\n+    val data2 = dataPool.acquire(cacheKey, data.nextOffsetInFetchedData)\n+\n+    assert(data.eq(data2))\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.release(cacheKey, data2)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(!dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"multiple tasks referring same key continuously using fetched data\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKey = CacheKey(\"testgroup\", new TopicPartition(\"topic\", 0))\n+\n+    assert(dataPool.getCache.get(cacheKey).isEmpty)\n+\n+    val dataFromTask1 = dataPool.acquire(cacheKey, 0)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    val dataFromTask2 = dataPool.acquire(cacheKey, 0)\n+\n+    // it shouldn't give same object as dataFromTask1 though it asks same offset\n+    // it definitely works when offsets are not overlapped: skip adding test for that\n+    assert(dataPool.getCache(cacheKey).size === 2)\n+    assert(dataPool.getCache(cacheKey)(1).inUse)\n+\n+    // reading from task 1\n+    dataFromTask1.withNewPoll(testRecords(0, 5).listIterator, 5)\n+\n+    (0 to 3).foreach { _ => dataFromTask1.next() }\n+\n+    dataPool.release(cacheKey, dataFromTask1)\n+\n+    // reading from task 2\n+    dataFromTask2.withNewPoll(testRecords(0, 30).listIterator, 30)\n+\n+    (0 to 5).foreach { _ => dataFromTask2.next() }\n+\n+    dataPool.release(cacheKey, dataFromTask2)\n+\n+    // suppose next batch for task 1\n+    val data2FromTask1 = dataPool.acquire(cacheKey, dataFromTask1.nextOffsetInFetchedData)\n+    assert(data2FromTask1.eq(dataFromTask1))\n+\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    // suppose next batch for task 2\n+    val data2FromTask2 = dataPool.acquire(cacheKey, dataFromTask2.nextOffsetInFetchedData)\n+    assert(data2FromTask2.eq(dataFromTask2))\n+\n+    assert(dataPool.getCache(cacheKey)(1).inUse)\n+\n+    // release from task 2\n+    dataPool.release(cacheKey, data2FromTask2)\n+    assert(!dataPool.getCache(cacheKey)(1).inUse)\n+\n+    // release from task 1\n+    dataPool.release(cacheKey, data2FromTask1)\n+    assert(!dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"evict idle fetched data\") {"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "It looks like reaching `eventually`. What I've missed actually is, specify wrong time unit in overall timeout in `eventually` so it waits for 1000x of expected time. After fixing that I can see test failure around 2 seconds.\r\n\r\n```\r\nThe code passed to eventually never returned normally. Attempted 13 times over 2.016476764 seconds. Last failure message: ListBuffer(CachedFetchedData(FetchedData(java.util.AbstractList$ListItr@7426a448,-2,5))) was not empty.\r\n```\r\n\r\nThanks for finding the bug! Will fix.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-07T00:51:52Z",
    "diffHunk": "@@ -0,0 +1,299 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+import org.apache.spark.sql.test.SharedSQLContext\n+\n+class FetchedPoolSuite extends SharedSQLContext {\n+  type Record = ConsumerRecord[Array[Byte], Array[Byte]]\n+\n+  private val dummyBytes = \"dummy\".getBytes\n+\n+  test(\"acquire fresh one\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKey = CacheKey(\"testgroup\", new TopicPartition(\"topic\", 0))\n+\n+    assert(dataPool.getCache.get(cacheKey).isEmpty)\n+\n+    val data = dataPool.acquire(cacheKey, 0)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    data.withNewPoll(testRecords(0, 5).listIterator, 5)\n+\n+    dataPool.release(cacheKey, data)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(!dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"acquire fetched data from multiple keys\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKeys = (0 to 10).map { partId =>\n+      CacheKey(\"testgroup\", new TopicPartition(\"topic\", partId))\n+    }\n+\n+    assert(dataPool.getCache.size === 0)\n+    cacheKeys.foreach { key => assert(dataPool.getCache.get(key).isEmpty) }\n+\n+    val dataList = cacheKeys.map(key => (key, dataPool.acquire(key, 0)))\n+\n+    assert(dataPool.getCache.size === cacheKeys.size)\n+    cacheKeys.map { key =>\n+      assert(dataPool.getCache(key).size === 1)\n+      assert(dataPool.getCache(key).head.inUse)\n+    }\n+\n+    dataList.map { case (_, data) =>\n+      data.withNewPoll(testRecords(0, 5).listIterator, 5)\n+    }\n+\n+    dataList.foreach { case (key, data) =>\n+      dataPool.release(key, data)\n+    }\n+\n+    assert(dataPool.getCache.size === cacheKeys.size)\n+    cacheKeys.map { key =>\n+      assert(dataPool.getCache(key).size === 1)\n+      assert(!dataPool.getCache(key).head.inUse)\n+    }\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"continuous use of fetched data from single key\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKey = CacheKey(\"testgroup\", new TopicPartition(\"topic\", 0))\n+\n+    assert(dataPool.getCache.get(cacheKey).isEmpty)\n+\n+    val data = dataPool.acquire(cacheKey, 0)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    data.withNewPoll(testRecords(0, 5).listIterator, 5)\n+\n+    (0 to 3).foreach { _ => data.next() }\n+\n+    dataPool.release(cacheKey, data)\n+\n+    // suppose next batch\n+\n+    val data2 = dataPool.acquire(cacheKey, data.nextOffsetInFetchedData)\n+\n+    assert(data.eq(data2))\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.release(cacheKey, data2)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(!dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"multiple tasks referring same key continuously using fetched data\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKey = CacheKey(\"testgroup\", new TopicPartition(\"topic\", 0))\n+\n+    assert(dataPool.getCache.get(cacheKey).isEmpty)\n+\n+    val dataFromTask1 = dataPool.acquire(cacheKey, 0)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    val dataFromTask2 = dataPool.acquire(cacheKey, 0)\n+\n+    // it shouldn't give same object as dataFromTask1 though it asks same offset\n+    // it definitely works when offsets are not overlapped: skip adding test for that\n+    assert(dataPool.getCache(cacheKey).size === 2)\n+    assert(dataPool.getCache(cacheKey)(1).inUse)\n+\n+    // reading from task 1\n+    dataFromTask1.withNewPoll(testRecords(0, 5).listIterator, 5)\n+\n+    (0 to 3).foreach { _ => dataFromTask1.next() }\n+\n+    dataPool.release(cacheKey, dataFromTask1)\n+\n+    // reading from task 2\n+    dataFromTask2.withNewPoll(testRecords(0, 30).listIterator, 30)\n+\n+    (0 to 5).foreach { _ => dataFromTask2.next() }\n+\n+    dataPool.release(cacheKey, dataFromTask2)\n+\n+    // suppose next batch for task 1\n+    val data2FromTask1 = dataPool.acquire(cacheKey, dataFromTask1.nextOffsetInFetchedData)\n+    assert(data2FromTask1.eq(dataFromTask1))\n+\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    // suppose next batch for task 2\n+    val data2FromTask2 = dataPool.acquire(cacheKey, dataFromTask2.nextOffsetInFetchedData)\n+    assert(data2FromTask2.eq(dataFromTask2))\n+\n+    assert(dataPool.getCache(cacheKey)(1).inUse)\n+\n+    // release from task 2\n+    dataPool.release(cacheKey, data2FromTask2)\n+    assert(!dataPool.getCache(cacheKey)(1).inUse)\n+\n+    // release from task 1\n+    dataPool.release(cacheKey, data2FromTask1)\n+    assert(!dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"evict idle fetched data\") {"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Works fine, thanks!",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-19T07:48:46Z",
    "diffHunk": "@@ -0,0 +1,299 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+import org.apache.spark.sql.test.SharedSQLContext\n+\n+class FetchedPoolSuite extends SharedSQLContext {\n+  type Record = ConsumerRecord[Array[Byte], Array[Byte]]\n+\n+  private val dummyBytes = \"dummy\".getBytes\n+\n+  test(\"acquire fresh one\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKey = CacheKey(\"testgroup\", new TopicPartition(\"topic\", 0))\n+\n+    assert(dataPool.getCache.get(cacheKey).isEmpty)\n+\n+    val data = dataPool.acquire(cacheKey, 0)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    data.withNewPoll(testRecords(0, 5).listIterator, 5)\n+\n+    dataPool.release(cacheKey, data)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(!dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"acquire fetched data from multiple keys\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKeys = (0 to 10).map { partId =>\n+      CacheKey(\"testgroup\", new TopicPartition(\"topic\", partId))\n+    }\n+\n+    assert(dataPool.getCache.size === 0)\n+    cacheKeys.foreach { key => assert(dataPool.getCache.get(key).isEmpty) }\n+\n+    val dataList = cacheKeys.map(key => (key, dataPool.acquire(key, 0)))\n+\n+    assert(dataPool.getCache.size === cacheKeys.size)\n+    cacheKeys.map { key =>\n+      assert(dataPool.getCache(key).size === 1)\n+      assert(dataPool.getCache(key).head.inUse)\n+    }\n+\n+    dataList.map { case (_, data) =>\n+      data.withNewPoll(testRecords(0, 5).listIterator, 5)\n+    }\n+\n+    dataList.foreach { case (key, data) =>\n+      dataPool.release(key, data)\n+    }\n+\n+    assert(dataPool.getCache.size === cacheKeys.size)\n+    cacheKeys.map { key =>\n+      assert(dataPool.getCache(key).size === 1)\n+      assert(!dataPool.getCache(key).head.inUse)\n+    }\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"continuous use of fetched data from single key\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKey = CacheKey(\"testgroup\", new TopicPartition(\"topic\", 0))\n+\n+    assert(dataPool.getCache.get(cacheKey).isEmpty)\n+\n+    val data = dataPool.acquire(cacheKey, 0)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    data.withNewPoll(testRecords(0, 5).listIterator, 5)\n+\n+    (0 to 3).foreach { _ => data.next() }\n+\n+    dataPool.release(cacheKey, data)\n+\n+    // suppose next batch\n+\n+    val data2 = dataPool.acquire(cacheKey, data.nextOffsetInFetchedData)\n+\n+    assert(data.eq(data2))\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.release(cacheKey, data2)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(!dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"multiple tasks referring same key continuously using fetched data\") {\n+    val dataPool = FetchedDataPool.build\n+\n+    val cacheKey = CacheKey(\"testgroup\", new TopicPartition(\"topic\", 0))\n+\n+    assert(dataPool.getCache.get(cacheKey).isEmpty)\n+\n+    val dataFromTask1 = dataPool.acquire(cacheKey, 0)\n+\n+    assert(dataPool.getCache(cacheKey).size === 1)\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    val dataFromTask2 = dataPool.acquire(cacheKey, 0)\n+\n+    // it shouldn't give same object as dataFromTask1 though it asks same offset\n+    // it definitely works when offsets are not overlapped: skip adding test for that\n+    assert(dataPool.getCache(cacheKey).size === 2)\n+    assert(dataPool.getCache(cacheKey)(1).inUse)\n+\n+    // reading from task 1\n+    dataFromTask1.withNewPoll(testRecords(0, 5).listIterator, 5)\n+\n+    (0 to 3).foreach { _ => dataFromTask1.next() }\n+\n+    dataPool.release(cacheKey, dataFromTask1)\n+\n+    // reading from task 2\n+    dataFromTask2.withNewPoll(testRecords(0, 30).listIterator, 30)\n+\n+    (0 to 5).foreach { _ => dataFromTask2.next() }\n+\n+    dataPool.release(cacheKey, dataFromTask2)\n+\n+    // suppose next batch for task 1\n+    val data2FromTask1 = dataPool.acquire(cacheKey, dataFromTask1.nextOffsetInFetchedData)\n+    assert(data2FromTask1.eq(dataFromTask1))\n+\n+    assert(dataPool.getCache(cacheKey).head.inUse)\n+\n+    // suppose next batch for task 2\n+    val data2FromTask2 = dataPool.acquire(cacheKey, dataFromTask2.nextOffsetInFetchedData)\n+    assert(data2FromTask2.eq(dataFromTask2))\n+\n+    assert(dataPool.getCache(cacheKey)(1).inUse)\n+\n+    // release from task 2\n+    dataPool.release(cacheKey, data2FromTask2)\n+    assert(!dataPool.getCache(cacheKey)(1).inUse)\n+\n+    // release from task 1\n+    dataPool.release(cacheKey, data2FromTask1)\n+    assert(!dataPool.getCache(cacheKey).head.inUse)\n+\n+    dataPool.shutdown()\n+  }\n+\n+  test(\"evict idle fetched data\") {"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Shouldn't it be `FetchedDataPoolSuite.scala`?",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-04T07:07:50Z",
    "diffHunk": "@@ -0,0 +1,299 @@\n+/*"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Nice catch! Will rename.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-04T13:40:35Z",
    "diffHunk": "@@ -0,0 +1,299 @@\n+/*"
  }],
  "prId": 22138
}]