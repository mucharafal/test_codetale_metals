[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "You are not testing all the cases of starting offsets, and other options. `KafkaSourceSuite` tests them, and since this is a whole different code path that microbatch, all of these case should be tested explicitly. \r\n\r\nI think the KafkaSourceSuite can be refactored such that code can be reused. Its some amount of work but without it, we are not confident about testing the all the new code paths.\r\n  \r\nAlso the same with sink, there are combinations of options that are not tested (default topic specified/not-specified, null value in topic column)",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-06T00:32:08Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.util.Properties\n+\n+import org.scalatest.time.SpanSugar._\n+import scala.collection.mutable\n+import scala.util.Random\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{ForeachWriter, Row}\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.execution.streaming.{ContinuousExecutionRelation, StreamingExecutionRelation, StreamingQueryWrapper}\n+import org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution\n+import org.apache.spark.sql.streaming.Trigger\n+import org.apache.spark.sql.test.{SharedSQLContext, TestSparkSession}\n+\n+class KafkaContinuousSuite extends KafkaSourceTest with SharedSQLContext {\n+  import testImplicits._\n+\n+  // We need more than the default local[2] to be able to schedule all partitions simultaneously.\n+  override protected def createSparkSession = new TestSparkSession(\n+    new SparkContext(\n+      \"local[10]\",\n+      \"continuous-stream-test-sql-context\",\n+      sparkConf.set(\"spark.sql.testkey\", \"true\")))\n+\n+  test(\"basic\") {"
  }],
  "prId": 20096
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Add comment on what this method does. It is asserting something, so does not look like it only \"sets\" something.\r\n  ",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-09T20:28:57Z",
    "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.util.Properties\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import org.scalatest.time.SpanSugar._\n+import scala.collection.mutable\n+import scala.util.Random\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{DataFrame, Dataset, ForeachWriter, Row}\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.execution.streaming.StreamExecution\n+import org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution\n+import org.apache.spark.sql.streaming.{StreamTest, Trigger}\n+import org.apache.spark.sql.test.{SharedSQLContext, TestSparkSession}\n+\n+trait KafkaContinuousTest extends KafkaSourceTest {\n+  override val defaultTrigger = Trigger.Continuous(1000)\n+  override val defaultUseV2Sink = true\n+\n+  // We need more than the default local[2] to be able to schedule all partitions simultaneously.\n+  override protected def createSparkSession = new TestSparkSession(\n+    new SparkContext(\n+      \"local[10]\",\n+      \"continuous-stream-test-sql-context\",\n+      sparkConf.set(\"spark.sql.testkey\", \"true\")))\n+\n+  override protected def setTopicPartitions("
  }],
  "prId": 20096
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Add docs to explain what this class if for.",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-09T22:46:30Z",
    "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.util.Properties\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import org.scalatest.time.SpanSugar._\n+import scala.collection.mutable\n+import scala.util.Random\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{DataFrame, Dataset, ForeachWriter, Row}\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.execution.streaming.StreamExecution\n+import org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution\n+import org.apache.spark.sql.streaming.{StreamTest, Trigger}\n+import org.apache.spark.sql.test.{SharedSQLContext, TestSparkSession}\n+\n+trait KafkaContinuousTest extends KafkaSourceTest {"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Also since this is used not just by the source, but also the sink, better to define this in a different file.",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-09T23:22:13Z",
    "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.util.Properties\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import org.scalatest.time.SpanSugar._\n+import scala.collection.mutable\n+import scala.util.Random\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{DataFrame, Dataset, ForeachWriter, Row}\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.execution.streaming.StreamExecution\n+import org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution\n+import org.apache.spark.sql.streaming.{StreamTest, Trigger}\n+import org.apache.spark.sql.test.{SharedSQLContext, TestSparkSession}\n+\n+trait KafkaContinuousTest extends KafkaSourceTest {"
  }],
  "prId": 20096
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Add docs.",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-09T22:46:36Z",
    "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.util.Properties\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import org.scalatest.time.SpanSugar._\n+import scala.collection.mutable\n+import scala.util.Random\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{DataFrame, Dataset, ForeachWriter, Row}\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.execution.streaming.StreamExecution\n+import org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution\n+import org.apache.spark.sql.streaming.{StreamTest, Trigger}\n+import org.apache.spark.sql.test.{SharedSQLContext, TestSparkSession}\n+\n+trait KafkaContinuousTest extends KafkaSourceTest {\n+  override val defaultTrigger = Trigger.Continuous(1000)\n+  override val defaultUseV2Sink = true\n+\n+  // We need more than the default local[2] to be able to schedule all partitions simultaneously.\n+  override protected def createSparkSession = new TestSparkSession(\n+    new SparkContext(\n+      \"local[10]\",\n+      \"continuous-stream-test-sql-context\",\n+      sparkConf.set(\"spark.sql.testkey\", \"true\")))\n+\n+  // In addition to setting the partitions in Kafka, we have to wait until the query has\n+  // reconfigured to the new count so the test framework can hook in properly.\n+  override protected def setTopicPartitions(\n+      topic: String, newCount: Int, query: StreamExecution) = {\n+    testUtils.addPartitions(topic, newCount)\n+    eventually(timeout(streamingTimeout)) {\n+      assert(\n+        query.lastExecution.logical.collectFirst {\n+          case DataSourceV2Relation(_, r: KafkaContinuousReader) => r\n+        }.exists(_.knownPartitions.size == newCount),\n+        s\"query never reconfigured to $newCount partitions\")\n+    }\n+  }\n+\n+  test(\"ensure continuous stream is being used\") {\n+    val query = spark.readStream\n+      .format(\"rate\")\n+      .option(\"numPartitions\", \"1\")\n+      .option(\"rowsPerSecond\", \"1\")\n+      .load()\n+\n+    testStream(query)(\n+      Execute(q => assert(q.isInstanceOf[ContinuousExecution]))\n+    )\n+  }\n+}\n+\n+class KafkaContinuousSourceSuite extends KafkaSourceSuiteBase with KafkaContinuousTest {"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "The `{  }` may not be needed.",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-09T22:47:21Z",
    "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.util.Properties\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import org.scalatest.time.SpanSugar._\n+import scala.collection.mutable\n+import scala.util.Random\n+\n+import org.apache.spark.SparkContext\n+import org.apache.spark.sql.{DataFrame, Dataset, ForeachWriter, Row}\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.execution.streaming.StreamExecution\n+import org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution\n+import org.apache.spark.sql.streaming.{StreamTest, Trigger}\n+import org.apache.spark.sql.test.{SharedSQLContext, TestSparkSession}\n+\n+trait KafkaContinuousTest extends KafkaSourceTest {\n+  override val defaultTrigger = Trigger.Continuous(1000)\n+  override val defaultUseV2Sink = true\n+\n+  // We need more than the default local[2] to be able to schedule all partitions simultaneously.\n+  override protected def createSparkSession = new TestSparkSession(\n+    new SparkContext(\n+      \"local[10]\",\n+      \"continuous-stream-test-sql-context\",\n+      sparkConf.set(\"spark.sql.testkey\", \"true\")))\n+\n+  // In addition to setting the partitions in Kafka, we have to wait until the query has\n+  // reconfigured to the new count so the test framework can hook in properly.\n+  override protected def setTopicPartitions(\n+      topic: String, newCount: Int, query: StreamExecution) = {\n+    testUtils.addPartitions(topic, newCount)\n+    eventually(timeout(streamingTimeout)) {\n+      assert(\n+        query.lastExecution.logical.collectFirst {\n+          case DataSourceV2Relation(_, r: KafkaContinuousReader) => r\n+        }.exists(_.knownPartitions.size == newCount),\n+        s\"query never reconfigured to $newCount partitions\")\n+    }\n+  }\n+\n+  test(\"ensure continuous stream is being used\") {\n+    val query = spark.readStream\n+      .format(\"rate\")\n+      .option(\"numPartitions\", \"1\")\n+      .option(\"rowsPerSecond\", \"1\")\n+      .load()\n+\n+    testStream(query)(\n+      Execute(q => assert(q.isInstanceOf[ContinuousExecution]))\n+    )\n+  }\n+}\n+\n+class KafkaContinuousSourceSuite extends KafkaSourceSuiteBase with KafkaContinuousTest {"
  }],
  "prId": 20096
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Rename this file to KafkaContinuousSourceSuite",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-09T23:21:29Z",
    "diffHunk": "@@ -0,0 +1,135 @@\n+/*"
  }],
  "prId": 20096
}]