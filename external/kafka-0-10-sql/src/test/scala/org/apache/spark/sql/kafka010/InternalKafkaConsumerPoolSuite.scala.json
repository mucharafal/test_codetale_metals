[{
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "I have the feeling `borrow...` tests have similarities.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-07-12T13:38:10Z",
    "diffHunk": "@@ -0,0 +1,316 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig._\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+import org.apache.spark.sql.test.SharedSQLContext\n+\n+class InternalKafkaConsumerPoolSuite extends SharedSQLContext {\n+  import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool.PoolConfig._\n+\n+  test(\"basic multiple borrows and returns for single key\") {\n+    val pool = InternalKafkaConsumerPool.build\n+\n+    val topic = \"topic\"\n+    val partitionId = 0\n+    val topicPartition = new TopicPartition(topic, partitionId)\n+\n+    val kafkaParams: ju.Map[String, Object] = getTestKafkaParams\n+\n+    val key = new CacheKey(topicPartition, kafkaParams)\n+\n+    val pooledObjects = (0 to 2).map { _ =>\n+      val pooledObject = pool.borrowObject(key, kafkaParams)\n+      assertPooledObject(pooledObject, topicPartition, kafkaParams)\n+      pooledObject\n+    }\n+\n+    assertPoolStateForKey(pool, key, numIdle = 0, numActive = 3, numTotal = 3)\n+    assertPoolState(pool, numIdle = 0, numActive = 3, numTotal = 3)\n+\n+    val pooledObject2 = pool.borrowObject(key, kafkaParams)\n+\n+    assertPooledObject(pooledObject2, topicPartition, kafkaParams)\n+    assertPoolStateForKey(pool, key, numIdle = 0, numActive = 4, numTotal = 4)\n+    assertPoolState(pool, numIdle = 0, numActive = 4, numTotal = 4)\n+\n+    pooledObjects.foreach(pool.returnObject)\n+\n+    assertPoolStateForKey(pool, key, numIdle = 3, numActive = 1, numTotal = 4)\n+    assertPoolState(pool, numIdle = 3, numActive = 1, numTotal = 4)\n+\n+    pool.returnObject(pooledObject2)\n+\n+    // we only allow three idle objects per key\n+    assertPoolStateForKey(pool, key, numIdle = 3, numActive = 0, numTotal = 3)\n+    assertPoolState(pool, numIdle = 3, numActive = 0, numTotal = 3)\n+\n+    pool.close()\n+  }\n+\n+  test(\"basic borrow and return for multiple keys\") {\n+    val pool = InternalKafkaConsumerPool.build\n+\n+    val kafkaParams = getTestKafkaParams\n+    val topicPartitions: List[TopicPartition] = for (\n+      topic <- List(\"topic\", \"topic2\");\n+      partitionId <- 0 to 5\n+    ) yield new TopicPartition(topic, partitionId)\n+\n+    val keys: List[CacheKey] = topicPartitions.map { part =>\n+      new CacheKey(part, kafkaParams)\n+    }\n+\n+    // while in loop pool doesn't still exceed total pool size\n+    val keyToPooledObjectPairs = borrowObjectsPerKey(pool, kafkaParams, keys)\n+\n+    assertPoolState(pool, numIdle = 0, numActive = keyToPooledObjectPairs.length,\n+      numTotal = keyToPooledObjectPairs.length)\n+\n+    returnObjects(pool, keyToPooledObjectPairs)\n+\n+    assertPoolState(pool, numIdle = keyToPooledObjectPairs.length, numActive = 0,\n+      numTotal = keyToPooledObjectPairs.length)\n+\n+    pool.close()\n+  }\n+\n+  test(\"borrow more than soft max capacity from pool which is neither free space nor idle object\") {",
    "line": 95
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "I'll revisit and see we can deduplicate something. It seems to have some duplications between tests as you said.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-07-13T00:12:18Z",
    "diffHunk": "@@ -0,0 +1,316 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig._\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+import org.apache.spark.sql.test.SharedSQLContext\n+\n+class InternalKafkaConsumerPoolSuite extends SharedSQLContext {\n+  import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool.PoolConfig._\n+\n+  test(\"basic multiple borrows and returns for single key\") {\n+    val pool = InternalKafkaConsumerPool.build\n+\n+    val topic = \"topic\"\n+    val partitionId = 0\n+    val topicPartition = new TopicPartition(topic, partitionId)\n+\n+    val kafkaParams: ju.Map[String, Object] = getTestKafkaParams\n+\n+    val key = new CacheKey(topicPartition, kafkaParams)\n+\n+    val pooledObjects = (0 to 2).map { _ =>\n+      val pooledObject = pool.borrowObject(key, kafkaParams)\n+      assertPooledObject(pooledObject, topicPartition, kafkaParams)\n+      pooledObject\n+    }\n+\n+    assertPoolStateForKey(pool, key, numIdle = 0, numActive = 3, numTotal = 3)\n+    assertPoolState(pool, numIdle = 0, numActive = 3, numTotal = 3)\n+\n+    val pooledObject2 = pool.borrowObject(key, kafkaParams)\n+\n+    assertPooledObject(pooledObject2, topicPartition, kafkaParams)\n+    assertPoolStateForKey(pool, key, numIdle = 0, numActive = 4, numTotal = 4)\n+    assertPoolState(pool, numIdle = 0, numActive = 4, numTotal = 4)\n+\n+    pooledObjects.foreach(pool.returnObject)\n+\n+    assertPoolStateForKey(pool, key, numIdle = 3, numActive = 1, numTotal = 4)\n+    assertPoolState(pool, numIdle = 3, numActive = 1, numTotal = 4)\n+\n+    pool.returnObject(pooledObject2)\n+\n+    // we only allow three idle objects per key\n+    assertPoolStateForKey(pool, key, numIdle = 3, numActive = 0, numTotal = 3)\n+    assertPoolState(pool, numIdle = 3, numActive = 0, numTotal = 3)\n+\n+    pool.close()\n+  }\n+\n+  test(\"basic borrow and return for multiple keys\") {\n+    val pool = InternalKafkaConsumerPool.build\n+\n+    val kafkaParams = getTestKafkaParams\n+    val topicPartitions: List[TopicPartition] = for (\n+      topic <- List(\"topic\", \"topic2\");\n+      partitionId <- 0 to 5\n+    ) yield new TopicPartition(topic, partitionId)\n+\n+    val keys: List[CacheKey] = topicPartitions.map { part =>\n+      new CacheKey(part, kafkaParams)\n+    }\n+\n+    // while in loop pool doesn't still exceed total pool size\n+    val keyToPooledObjectPairs = borrowObjectsPerKey(pool, kafkaParams, keys)\n+\n+    assertPoolState(pool, numIdle = 0, numActive = keyToPooledObjectPairs.length,\n+      numTotal = keyToPooledObjectPairs.length)\n+\n+    returnObjects(pool, keyToPooledObjectPairs)\n+\n+    assertPoolState(pool, numIdle = keyToPooledObjectPairs.length, numActive = 0,\n+      numTotal = keyToPooledObjectPairs.length)\n+\n+    pool.close()\n+  }\n+\n+  test(\"borrow more than soft max capacity from pool which is neither free space nor idle object\") {",
    "line": 95
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Addressed.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-24T02:51:32Z",
    "diffHunk": "@@ -0,0 +1,316 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig._\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+import org.apache.spark.sql.test.SharedSQLContext\n+\n+class InternalKafkaConsumerPoolSuite extends SharedSQLContext {\n+  import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool.PoolConfig._\n+\n+  test(\"basic multiple borrows and returns for single key\") {\n+    val pool = InternalKafkaConsumerPool.build\n+\n+    val topic = \"topic\"\n+    val partitionId = 0\n+    val topicPartition = new TopicPartition(topic, partitionId)\n+\n+    val kafkaParams: ju.Map[String, Object] = getTestKafkaParams\n+\n+    val key = new CacheKey(topicPartition, kafkaParams)\n+\n+    val pooledObjects = (0 to 2).map { _ =>\n+      val pooledObject = pool.borrowObject(key, kafkaParams)\n+      assertPooledObject(pooledObject, topicPartition, kafkaParams)\n+      pooledObject\n+    }\n+\n+    assertPoolStateForKey(pool, key, numIdle = 0, numActive = 3, numTotal = 3)\n+    assertPoolState(pool, numIdle = 0, numActive = 3, numTotal = 3)\n+\n+    val pooledObject2 = pool.borrowObject(key, kafkaParams)\n+\n+    assertPooledObject(pooledObject2, topicPartition, kafkaParams)\n+    assertPoolStateForKey(pool, key, numIdle = 0, numActive = 4, numTotal = 4)\n+    assertPoolState(pool, numIdle = 0, numActive = 4, numTotal = 4)\n+\n+    pooledObjects.foreach(pool.returnObject)\n+\n+    assertPoolStateForKey(pool, key, numIdle = 3, numActive = 1, numTotal = 4)\n+    assertPoolState(pool, numIdle = 3, numActive = 1, numTotal = 4)\n+\n+    pool.returnObject(pooledObject2)\n+\n+    // we only allow three idle objects per key\n+    assertPoolStateForKey(pool, key, numIdle = 3, numActive = 0, numTotal = 3)\n+    assertPoolState(pool, numIdle = 3, numActive = 0, numTotal = 3)\n+\n+    pool.close()\n+  }\n+\n+  test(\"basic borrow and return for multiple keys\") {\n+    val pool = InternalKafkaConsumerPool.build\n+\n+    val kafkaParams = getTestKafkaParams\n+    val topicPartitions: List[TopicPartition] = for (\n+      topic <- List(\"topic\", \"topic2\");\n+      partitionId <- 0 to 5\n+    ) yield new TopicPartition(topic, partitionId)\n+\n+    val keys: List[CacheKey] = topicPartitions.map { part =>\n+      new CacheKey(part, kafkaParams)\n+    }\n+\n+    // while in loop pool doesn't still exceed total pool size\n+    val keyToPooledObjectPairs = borrowObjectsPerKey(pool, kafkaParams, keys)\n+\n+    assertPoolState(pool, numIdle = 0, numActive = keyToPooledObjectPairs.length,\n+      numTotal = keyToPooledObjectPairs.length)\n+\n+    returnObjects(pool, keyToPooledObjectPairs)\n+\n+    assertPoolState(pool, numIdle = keyToPooledObjectPairs.length, numActive = 0,\n+      numTotal = keyToPooledObjectPairs.length)\n+\n+    pool.close()\n+  }\n+\n+  test(\"borrow more than soft max capacity from pool which is neither free space nor idle object\") {",
    "line": 95
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Nit: indent",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-07-12T13:39:13Z",
    "diffHunk": "@@ -0,0 +1,316 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig._\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+import org.apache.spark.sql.test.SharedSQLContext\n+\n+class InternalKafkaConsumerPoolSuite extends SharedSQLContext {\n+  import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool.PoolConfig._\n+\n+  test(\"basic multiple borrows and returns for single key\") {\n+    val pool = InternalKafkaConsumerPool.build\n+\n+    val topic = \"topic\"\n+    val partitionId = 0\n+    val topicPartition = new TopicPartition(topic, partitionId)\n+\n+    val kafkaParams: ju.Map[String, Object] = getTestKafkaParams\n+\n+    val key = new CacheKey(topicPartition, kafkaParams)\n+\n+    val pooledObjects = (0 to 2).map { _ =>\n+      val pooledObject = pool.borrowObject(key, kafkaParams)\n+      assertPooledObject(pooledObject, topicPartition, kafkaParams)\n+      pooledObject\n+    }\n+\n+    assertPoolStateForKey(pool, key, numIdle = 0, numActive = 3, numTotal = 3)\n+    assertPoolState(pool, numIdle = 0, numActive = 3, numTotal = 3)\n+\n+    val pooledObject2 = pool.borrowObject(key, kafkaParams)\n+\n+    assertPooledObject(pooledObject2, topicPartition, kafkaParams)\n+    assertPoolStateForKey(pool, key, numIdle = 0, numActive = 4, numTotal = 4)\n+    assertPoolState(pool, numIdle = 0, numActive = 4, numTotal = 4)\n+\n+    pooledObjects.foreach(pool.returnObject)\n+\n+    assertPoolStateForKey(pool, key, numIdle = 3, numActive = 1, numTotal = 4)\n+    assertPoolState(pool, numIdle = 3, numActive = 1, numTotal = 4)\n+\n+    pool.returnObject(pooledObject2)\n+\n+    // we only allow three idle objects per key\n+    assertPoolStateForKey(pool, key, numIdle = 3, numActive = 0, numTotal = 3)\n+    assertPoolState(pool, numIdle = 3, numActive = 0, numTotal = 3)\n+\n+    pool.close()\n+  }\n+\n+  test(\"basic borrow and return for multiple keys\") {\n+    val pool = InternalKafkaConsumerPool.build\n+\n+    val kafkaParams = getTestKafkaParams\n+    val topicPartitions: List[TopicPartition] = for (\n+      topic <- List(\"topic\", \"topic2\");\n+      partitionId <- 0 to 5\n+    ) yield new TopicPartition(topic, partitionId)\n+\n+    val keys: List[CacheKey] = topicPartitions.map { part =>\n+      new CacheKey(part, kafkaParams)\n+    }\n+\n+    // while in loop pool doesn't still exceed total pool size\n+    val keyToPooledObjectPairs = borrowObjectsPerKey(pool, kafkaParams, keys)\n+\n+    assertPoolState(pool, numIdle = 0, numActive = keyToPooledObjectPairs.length,\n+      numTotal = keyToPooledObjectPairs.length)\n+\n+    returnObjects(pool, keyToPooledObjectPairs)\n+\n+    assertPoolState(pool, numIdle = keyToPooledObjectPairs.length, numActive = 0,\n+      numTotal = keyToPooledObjectPairs.length)\n+\n+    pool.close()\n+  }\n+\n+  test(\"borrow more than soft max capacity from pool which is neither free space nor idle object\") {\n+    val capacity = 16\n+\n+    val newConf = Seq(\n+      CONFIG_NAME_CAPACITY -> capacity.toString,\n+      CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS -> (-1).toString,\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS -> (-1).toString)\n+\n+    withSparkConf(newConf: _*) {\n+      val pool = InternalKafkaConsumerPool.build\n+\n+      val kafkaParams = getTestKafkaParams\n+      val topicPartitions: List[TopicPartition] = for (\n+        partitionId <- (0 until capacity).toList\n+      ) yield new TopicPartition(\"topic\", partitionId)\n+\n+      val keys: List[CacheKey] = topicPartitions.map { part =>\n+        new CacheKey(part, kafkaParams)\n+      }\n+\n+      // while in loop pool doesn't still exceed soft max pool size\n+      val keyToPooledObjectPairs = borrowObjectsPerKey(pool, kafkaParams, keys)\n+\n+      val moreTopicPartition = new TopicPartition(\"topic2\", 0)\n+      val newCacheKey = new CacheKey(moreTopicPartition, kafkaParams)\n+\n+      // exceeds soft max pool size, and also no idle object for cleaning up\n+      // but pool will borrow a new object\n+      pool.borrowObject(newCacheKey, kafkaParams)\n+\n+      assertPoolState(pool, numIdle = 0, numActive = keyToPooledObjectPairs.length + 1,\n+        numTotal = keyToPooledObjectPairs.length + 1)\n+\n+      pool.close()\n+    }\n+  }\n+\n+  test(\"borrow more than soft max capacity from pool frees up idle objects automatically\") {\n+    val capacity = 16\n+\n+    val newConf = Seq(\n+      CONFIG_NAME_CAPACITY -> capacity.toString,\n+      CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS -> (-1).toString,\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS -> (-1).toString)\n+\n+    withSparkConf(newConf: _*) {\n+      val pool = InternalKafkaConsumerPool.build\n+\n+      val kafkaParams = getTestKafkaParams\n+      val topicPartitions: List[TopicPartition] = for (\n+        partitionId <- (0 until capacity).toList\n+      ) yield new TopicPartition(\"topic\", partitionId)\n+\n+      val keys: List[CacheKey] = topicPartitions.map { part =>\n+        new CacheKey(part, kafkaParams)\n+      }\n+\n+      // borrow objects which makes pool reaching soft capacity\n+      val keyToPooledObjectPairs = borrowObjectsPerKey(pool, kafkaParams, keys)\n+\n+      // return 20% of objects to ensure there're some idle objects to free up later\n+      val numToReturn = (keyToPooledObjectPairs.length * 0.2).toInt\n+      returnObjects(pool, keyToPooledObjectPairs.take(numToReturn))\n+\n+      assertPoolState(pool, numIdle = numToReturn,\n+        numActive = keyToPooledObjectPairs.length - numToReturn,\n+        numTotal = keyToPooledObjectPairs.length)\n+\n+      // borrow a new object: there should be some idle objects to clean up\n+      val moreTopicPartition = new TopicPartition(\"topic2\", 0)\n+      val newCacheKey = new CacheKey(moreTopicPartition, kafkaParams)\n+\n+      val newObject = pool.borrowObject(newCacheKey, kafkaParams)\n+      assertPooledObject(newObject, moreTopicPartition, kafkaParams)\n+      assertPoolStateForKey(pool, newCacheKey, numIdle = 0, numActive = 1, numTotal = 1)\n+\n+      // at least one of idle object should be freed up\n+      assert(pool.getNumIdle < numToReturn)\n+      // we can determine number of active objects correctly\n+      assert(pool.getNumActive === keyToPooledObjectPairs.length - numToReturn + 1)\n+      // total objects should be more than number of active + 1 but can't expect exact number\n+      assert(pool.getTotal > keyToPooledObjectPairs.length - numToReturn + 1)\n+\n+      pool.close()\n+    }\n+  }\n+\n+  test(\"evicting idle objects on background\") {\n+    import org.scalatest.time.SpanSugar._\n+\n+    val minEvictableIdleTimeMillis = 3 * 1000 // 3 seconds\n+    val evictorThreadRunIntervalMillis = 500 // triggering multiple evictions by intention\n+\n+    val newConf = Seq(\n+      CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS -> minEvictableIdleTimeMillis.toString,\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS -> evictorThreadRunIntervalMillis.toString)\n+\n+    withSparkConf(newConf: _*) {\n+      val pool = InternalKafkaConsumerPool.build\n+\n+      val kafkaParams = getTestKafkaParams\n+      val topicPartitions: List[TopicPartition] = for (\n+        partitionId <- (0 until 10).toList\n+      ) yield new TopicPartition(\"topic\", partitionId)\n+\n+      val keys: List[CacheKey] = topicPartitions.map { part =>\n+        new CacheKey(part, kafkaParams)\n+      }\n+\n+      // borrow and return some consumers to ensure some partitions are being idle\n+      // this test covers the use cases: rebalance / topic removal happens while running query\n+      val keyToPooledObjectPairs = borrowObjectsPerKey(pool, kafkaParams, keys)\n+      val objectsToReturn = keyToPooledObjectPairs.filter(_._1.topicPartition.partition() % 2 == 0)\n+      returnObjects(pool, objectsToReturn)\n+\n+      // wait up to twice than minEvictableIdleTimeMillis to ensure evictor thread to clear up\n+      // idle objects\n+      eventually(timeout((minEvictableIdleTimeMillis.toLong * 2).seconds),\n+        interval(evictorThreadRunIntervalMillis.milliseconds)) {\n+        assertPoolState(pool, numIdle = 0, numActive = 5, numTotal = 5)\n+      }\n+\n+      pool.close()\n+    }\n+  }\n+\n+  private def assertPooledObject(\n+      pooledObject: InternalKafkaConsumer,\n+      expectedTopicPartition: TopicPartition,\n+      expectedKafkaParams: ju.Map[String, Object]): Unit = {\n+    assert(pooledObject != null)\n+    assert(pooledObject.kafkaParams === expectedKafkaParams)\n+    assert(pooledObject.topicPartition === expectedTopicPartition)\n+  }\n+\n+  private def assertPoolState(pool: InternalKafkaConsumerPool, numIdle: Int,\n+                              numActive: Int, numTotal: Int): Unit = {\n+    assert(pool.getNumIdle === numIdle)\n+    assert(pool.getNumActive === numActive)\n+    assert(pool.getTotal === numTotal)\n+  }\n+\n+  private def assertPoolStateForKey(pool: InternalKafkaConsumerPool, key: CacheKey,\n+                                    numIdle: Int, numActive: Int, numTotal: Int): Unit = {"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Nit: indent",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-07-12T13:39:22Z",
    "diffHunk": "@@ -0,0 +1,316 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig._\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+import org.apache.spark.sql.test.SharedSQLContext\n+\n+class InternalKafkaConsumerPoolSuite extends SharedSQLContext {\n+  import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool.PoolConfig._\n+\n+  test(\"basic multiple borrows and returns for single key\") {\n+    val pool = InternalKafkaConsumerPool.build\n+\n+    val topic = \"topic\"\n+    val partitionId = 0\n+    val topicPartition = new TopicPartition(topic, partitionId)\n+\n+    val kafkaParams: ju.Map[String, Object] = getTestKafkaParams\n+\n+    val key = new CacheKey(topicPartition, kafkaParams)\n+\n+    val pooledObjects = (0 to 2).map { _ =>\n+      val pooledObject = pool.borrowObject(key, kafkaParams)\n+      assertPooledObject(pooledObject, topicPartition, kafkaParams)\n+      pooledObject\n+    }\n+\n+    assertPoolStateForKey(pool, key, numIdle = 0, numActive = 3, numTotal = 3)\n+    assertPoolState(pool, numIdle = 0, numActive = 3, numTotal = 3)\n+\n+    val pooledObject2 = pool.borrowObject(key, kafkaParams)\n+\n+    assertPooledObject(pooledObject2, topicPartition, kafkaParams)\n+    assertPoolStateForKey(pool, key, numIdle = 0, numActive = 4, numTotal = 4)\n+    assertPoolState(pool, numIdle = 0, numActive = 4, numTotal = 4)\n+\n+    pooledObjects.foreach(pool.returnObject)\n+\n+    assertPoolStateForKey(pool, key, numIdle = 3, numActive = 1, numTotal = 4)\n+    assertPoolState(pool, numIdle = 3, numActive = 1, numTotal = 4)\n+\n+    pool.returnObject(pooledObject2)\n+\n+    // we only allow three idle objects per key\n+    assertPoolStateForKey(pool, key, numIdle = 3, numActive = 0, numTotal = 3)\n+    assertPoolState(pool, numIdle = 3, numActive = 0, numTotal = 3)\n+\n+    pool.close()\n+  }\n+\n+  test(\"basic borrow and return for multiple keys\") {\n+    val pool = InternalKafkaConsumerPool.build\n+\n+    val kafkaParams = getTestKafkaParams\n+    val topicPartitions: List[TopicPartition] = for (\n+      topic <- List(\"topic\", \"topic2\");\n+      partitionId <- 0 to 5\n+    ) yield new TopicPartition(topic, partitionId)\n+\n+    val keys: List[CacheKey] = topicPartitions.map { part =>\n+      new CacheKey(part, kafkaParams)\n+    }\n+\n+    // while in loop pool doesn't still exceed total pool size\n+    val keyToPooledObjectPairs = borrowObjectsPerKey(pool, kafkaParams, keys)\n+\n+    assertPoolState(pool, numIdle = 0, numActive = keyToPooledObjectPairs.length,\n+      numTotal = keyToPooledObjectPairs.length)\n+\n+    returnObjects(pool, keyToPooledObjectPairs)\n+\n+    assertPoolState(pool, numIdle = keyToPooledObjectPairs.length, numActive = 0,\n+      numTotal = keyToPooledObjectPairs.length)\n+\n+    pool.close()\n+  }\n+\n+  test(\"borrow more than soft max capacity from pool which is neither free space nor idle object\") {\n+    val capacity = 16\n+\n+    val newConf = Seq(\n+      CONFIG_NAME_CAPACITY -> capacity.toString,\n+      CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS -> (-1).toString,\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS -> (-1).toString)\n+\n+    withSparkConf(newConf: _*) {\n+      val pool = InternalKafkaConsumerPool.build\n+\n+      val kafkaParams = getTestKafkaParams\n+      val topicPartitions: List[TopicPartition] = for (\n+        partitionId <- (0 until capacity).toList\n+      ) yield new TopicPartition(\"topic\", partitionId)\n+\n+      val keys: List[CacheKey] = topicPartitions.map { part =>\n+        new CacheKey(part, kafkaParams)\n+      }\n+\n+      // while in loop pool doesn't still exceed soft max pool size\n+      val keyToPooledObjectPairs = borrowObjectsPerKey(pool, kafkaParams, keys)\n+\n+      val moreTopicPartition = new TopicPartition(\"topic2\", 0)\n+      val newCacheKey = new CacheKey(moreTopicPartition, kafkaParams)\n+\n+      // exceeds soft max pool size, and also no idle object for cleaning up\n+      // but pool will borrow a new object\n+      pool.borrowObject(newCacheKey, kafkaParams)\n+\n+      assertPoolState(pool, numIdle = 0, numActive = keyToPooledObjectPairs.length + 1,\n+        numTotal = keyToPooledObjectPairs.length + 1)\n+\n+      pool.close()\n+    }\n+  }\n+\n+  test(\"borrow more than soft max capacity from pool frees up idle objects automatically\") {\n+    val capacity = 16\n+\n+    val newConf = Seq(\n+      CONFIG_NAME_CAPACITY -> capacity.toString,\n+      CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS -> (-1).toString,\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS -> (-1).toString)\n+\n+    withSparkConf(newConf: _*) {\n+      val pool = InternalKafkaConsumerPool.build\n+\n+      val kafkaParams = getTestKafkaParams\n+      val topicPartitions: List[TopicPartition] = for (\n+        partitionId <- (0 until capacity).toList\n+      ) yield new TopicPartition(\"topic\", partitionId)\n+\n+      val keys: List[CacheKey] = topicPartitions.map { part =>\n+        new CacheKey(part, kafkaParams)\n+      }\n+\n+      // borrow objects which makes pool reaching soft capacity\n+      val keyToPooledObjectPairs = borrowObjectsPerKey(pool, kafkaParams, keys)\n+\n+      // return 20% of objects to ensure there're some idle objects to free up later\n+      val numToReturn = (keyToPooledObjectPairs.length * 0.2).toInt\n+      returnObjects(pool, keyToPooledObjectPairs.take(numToReturn))\n+\n+      assertPoolState(pool, numIdle = numToReturn,\n+        numActive = keyToPooledObjectPairs.length - numToReturn,\n+        numTotal = keyToPooledObjectPairs.length)\n+\n+      // borrow a new object: there should be some idle objects to clean up\n+      val moreTopicPartition = new TopicPartition(\"topic2\", 0)\n+      val newCacheKey = new CacheKey(moreTopicPartition, kafkaParams)\n+\n+      val newObject = pool.borrowObject(newCacheKey, kafkaParams)\n+      assertPooledObject(newObject, moreTopicPartition, kafkaParams)\n+      assertPoolStateForKey(pool, newCacheKey, numIdle = 0, numActive = 1, numTotal = 1)\n+\n+      // at least one of idle object should be freed up\n+      assert(pool.getNumIdle < numToReturn)\n+      // we can determine number of active objects correctly\n+      assert(pool.getNumActive === keyToPooledObjectPairs.length - numToReturn + 1)\n+      // total objects should be more than number of active + 1 but can't expect exact number\n+      assert(pool.getTotal > keyToPooledObjectPairs.length - numToReturn + 1)\n+\n+      pool.close()\n+    }\n+  }\n+\n+  test(\"evicting idle objects on background\") {\n+    import org.scalatest.time.SpanSugar._\n+\n+    val minEvictableIdleTimeMillis = 3 * 1000 // 3 seconds\n+    val evictorThreadRunIntervalMillis = 500 // triggering multiple evictions by intention\n+\n+    val newConf = Seq(\n+      CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS -> minEvictableIdleTimeMillis.toString,\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS -> evictorThreadRunIntervalMillis.toString)\n+\n+    withSparkConf(newConf: _*) {\n+      val pool = InternalKafkaConsumerPool.build\n+\n+      val kafkaParams = getTestKafkaParams\n+      val topicPartitions: List[TopicPartition] = for (\n+        partitionId <- (0 until 10).toList\n+      ) yield new TopicPartition(\"topic\", partitionId)\n+\n+      val keys: List[CacheKey] = topicPartitions.map { part =>\n+        new CacheKey(part, kafkaParams)\n+      }\n+\n+      // borrow and return some consumers to ensure some partitions are being idle\n+      // this test covers the use cases: rebalance / topic removal happens while running query\n+      val keyToPooledObjectPairs = borrowObjectsPerKey(pool, kafkaParams, keys)\n+      val objectsToReturn = keyToPooledObjectPairs.filter(_._1.topicPartition.partition() % 2 == 0)\n+      returnObjects(pool, objectsToReturn)\n+\n+      // wait up to twice than minEvictableIdleTimeMillis to ensure evictor thread to clear up\n+      // idle objects\n+      eventually(timeout((minEvictableIdleTimeMillis.toLong * 2).seconds),\n+        interval(evictorThreadRunIntervalMillis.milliseconds)) {\n+        assertPoolState(pool, numIdle = 0, numActive = 5, numTotal = 5)\n+      }\n+\n+      pool.close()\n+    }\n+  }\n+\n+  private def assertPooledObject(\n+      pooledObject: InternalKafkaConsumer,\n+      expectedTopicPartition: TopicPartition,\n+      expectedKafkaParams: ju.Map[String, Object]): Unit = {\n+    assert(pooledObject != null)\n+    assert(pooledObject.kafkaParams === expectedKafkaParams)\n+    assert(pooledObject.topicPartition === expectedTopicPartition)\n+  }\n+\n+  private def assertPoolState(pool: InternalKafkaConsumerPool, numIdle: Int,\n+                              numActive: Int, numTotal: Int): Unit = {"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Nit: indent",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-07-12T13:40:20Z",
    "diffHunk": "@@ -0,0 +1,316 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig._\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+import org.apache.spark.sql.test.SharedSQLContext\n+\n+class InternalKafkaConsumerPoolSuite extends SharedSQLContext {\n+  import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool.PoolConfig._\n+\n+  test(\"basic multiple borrows and returns for single key\") {\n+    val pool = InternalKafkaConsumerPool.build\n+\n+    val topic = \"topic\"\n+    val partitionId = 0\n+    val topicPartition = new TopicPartition(topic, partitionId)\n+\n+    val kafkaParams: ju.Map[String, Object] = getTestKafkaParams\n+\n+    val key = new CacheKey(topicPartition, kafkaParams)\n+\n+    val pooledObjects = (0 to 2).map { _ =>\n+      val pooledObject = pool.borrowObject(key, kafkaParams)\n+      assertPooledObject(pooledObject, topicPartition, kafkaParams)\n+      pooledObject\n+    }\n+\n+    assertPoolStateForKey(pool, key, numIdle = 0, numActive = 3, numTotal = 3)\n+    assertPoolState(pool, numIdle = 0, numActive = 3, numTotal = 3)\n+\n+    val pooledObject2 = pool.borrowObject(key, kafkaParams)\n+\n+    assertPooledObject(pooledObject2, topicPartition, kafkaParams)\n+    assertPoolStateForKey(pool, key, numIdle = 0, numActive = 4, numTotal = 4)\n+    assertPoolState(pool, numIdle = 0, numActive = 4, numTotal = 4)\n+\n+    pooledObjects.foreach(pool.returnObject)\n+\n+    assertPoolStateForKey(pool, key, numIdle = 3, numActive = 1, numTotal = 4)\n+    assertPoolState(pool, numIdle = 3, numActive = 1, numTotal = 4)\n+\n+    pool.returnObject(pooledObject2)\n+\n+    // we only allow three idle objects per key\n+    assertPoolStateForKey(pool, key, numIdle = 3, numActive = 0, numTotal = 3)\n+    assertPoolState(pool, numIdle = 3, numActive = 0, numTotal = 3)\n+\n+    pool.close()\n+  }\n+\n+  test(\"basic borrow and return for multiple keys\") {\n+    val pool = InternalKafkaConsumerPool.build\n+\n+    val kafkaParams = getTestKafkaParams\n+    val topicPartitions: List[TopicPartition] = for (\n+      topic <- List(\"topic\", \"topic2\");\n+      partitionId <- 0 to 5\n+    ) yield new TopicPartition(topic, partitionId)\n+\n+    val keys: List[CacheKey] = topicPartitions.map { part =>\n+      new CacheKey(part, kafkaParams)\n+    }\n+\n+    // while in loop pool doesn't still exceed total pool size\n+    val keyToPooledObjectPairs = borrowObjectsPerKey(pool, kafkaParams, keys)\n+\n+    assertPoolState(pool, numIdle = 0, numActive = keyToPooledObjectPairs.length,\n+      numTotal = keyToPooledObjectPairs.length)\n+\n+    returnObjects(pool, keyToPooledObjectPairs)\n+\n+    assertPoolState(pool, numIdle = keyToPooledObjectPairs.length, numActive = 0,\n+      numTotal = keyToPooledObjectPairs.length)\n+\n+    pool.close()\n+  }\n+\n+  test(\"borrow more than soft max capacity from pool which is neither free space nor idle object\") {\n+    val capacity = 16\n+\n+    val newConf = Seq(\n+      CONFIG_NAME_CAPACITY -> capacity.toString,\n+      CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS -> (-1).toString,\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS -> (-1).toString)\n+\n+    withSparkConf(newConf: _*) {\n+      val pool = InternalKafkaConsumerPool.build\n+\n+      val kafkaParams = getTestKafkaParams\n+      val topicPartitions: List[TopicPartition] = for (\n+        partitionId <- (0 until capacity).toList\n+      ) yield new TopicPartition(\"topic\", partitionId)\n+\n+      val keys: List[CacheKey] = topicPartitions.map { part =>\n+        new CacheKey(part, kafkaParams)\n+      }\n+\n+      // while in loop pool doesn't still exceed soft max pool size\n+      val keyToPooledObjectPairs = borrowObjectsPerKey(pool, kafkaParams, keys)\n+\n+      val moreTopicPartition = new TopicPartition(\"topic2\", 0)\n+      val newCacheKey = new CacheKey(moreTopicPartition, kafkaParams)\n+\n+      // exceeds soft max pool size, and also no idle object for cleaning up\n+      // but pool will borrow a new object\n+      pool.borrowObject(newCacheKey, kafkaParams)\n+\n+      assertPoolState(pool, numIdle = 0, numActive = keyToPooledObjectPairs.length + 1,\n+        numTotal = keyToPooledObjectPairs.length + 1)\n+\n+      pool.close()\n+    }\n+  }\n+\n+  test(\"borrow more than soft max capacity from pool frees up idle objects automatically\") {\n+    val capacity = 16\n+\n+    val newConf = Seq(\n+      CONFIG_NAME_CAPACITY -> capacity.toString,\n+      CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS -> (-1).toString,\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS -> (-1).toString)\n+\n+    withSparkConf(newConf: _*) {\n+      val pool = InternalKafkaConsumerPool.build\n+\n+      val kafkaParams = getTestKafkaParams\n+      val topicPartitions: List[TopicPartition] = for (\n+        partitionId <- (0 until capacity).toList\n+      ) yield new TopicPartition(\"topic\", partitionId)\n+\n+      val keys: List[CacheKey] = topicPartitions.map { part =>\n+        new CacheKey(part, kafkaParams)\n+      }\n+\n+      // borrow objects which makes pool reaching soft capacity\n+      val keyToPooledObjectPairs = borrowObjectsPerKey(pool, kafkaParams, keys)\n+\n+      // return 20% of objects to ensure there're some idle objects to free up later\n+      val numToReturn = (keyToPooledObjectPairs.length * 0.2).toInt\n+      returnObjects(pool, keyToPooledObjectPairs.take(numToReturn))\n+\n+      assertPoolState(pool, numIdle = numToReturn,\n+        numActive = keyToPooledObjectPairs.length - numToReturn,\n+        numTotal = keyToPooledObjectPairs.length)\n+\n+      // borrow a new object: there should be some idle objects to clean up\n+      val moreTopicPartition = new TopicPartition(\"topic2\", 0)\n+      val newCacheKey = new CacheKey(moreTopicPartition, kafkaParams)\n+\n+      val newObject = pool.borrowObject(newCacheKey, kafkaParams)\n+      assertPooledObject(newObject, moreTopicPartition, kafkaParams)\n+      assertPoolStateForKey(pool, newCacheKey, numIdle = 0, numActive = 1, numTotal = 1)\n+\n+      // at least one of idle object should be freed up\n+      assert(pool.getNumIdle < numToReturn)\n+      // we can determine number of active objects correctly\n+      assert(pool.getNumActive === keyToPooledObjectPairs.length - numToReturn + 1)\n+      // total objects should be more than number of active + 1 but can't expect exact number\n+      assert(pool.getTotal > keyToPooledObjectPairs.length - numToReturn + 1)\n+\n+      pool.close()\n+    }\n+  }\n+\n+  test(\"evicting idle objects on background\") {\n+    import org.scalatest.time.SpanSugar._\n+\n+    val minEvictableIdleTimeMillis = 3 * 1000 // 3 seconds\n+    val evictorThreadRunIntervalMillis = 500 // triggering multiple evictions by intention\n+\n+    val newConf = Seq(\n+      CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS -> minEvictableIdleTimeMillis.toString,\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS -> evictorThreadRunIntervalMillis.toString)\n+\n+    withSparkConf(newConf: _*) {\n+      val pool = InternalKafkaConsumerPool.build\n+\n+      val kafkaParams = getTestKafkaParams\n+      val topicPartitions: List[TopicPartition] = for (\n+        partitionId <- (0 until 10).toList\n+      ) yield new TopicPartition(\"topic\", partitionId)\n+\n+      val keys: List[CacheKey] = topicPartitions.map { part =>\n+        new CacheKey(part, kafkaParams)\n+      }\n+\n+      // borrow and return some consumers to ensure some partitions are being idle\n+      // this test covers the use cases: rebalance / topic removal happens while running query\n+      val keyToPooledObjectPairs = borrowObjectsPerKey(pool, kafkaParams, keys)\n+      val objectsToReturn = keyToPooledObjectPairs.filter(_._1.topicPartition.partition() % 2 == 0)\n+      returnObjects(pool, objectsToReturn)\n+\n+      // wait up to twice than minEvictableIdleTimeMillis to ensure evictor thread to clear up\n+      // idle objects\n+      eventually(timeout((minEvictableIdleTimeMillis.toLong * 2).seconds),\n+        interval(evictorThreadRunIntervalMillis.milliseconds)) {\n+        assertPoolState(pool, numIdle = 0, numActive = 5, numTotal = 5)\n+      }\n+\n+      pool.close()\n+    }\n+  }\n+\n+  private def assertPooledObject(\n+      pooledObject: InternalKafkaConsumer,\n+      expectedTopicPartition: TopicPartition,\n+      expectedKafkaParams: ju.Map[String, Object]): Unit = {\n+    assert(pooledObject != null)\n+    assert(pooledObject.kafkaParams === expectedKafkaParams)\n+    assert(pooledObject.topicPartition === expectedTopicPartition)\n+  }\n+\n+  private def assertPoolState(pool: InternalKafkaConsumerPool, numIdle: Int,\n+                              numActive: Int, numTotal: Int): Unit = {\n+    assert(pool.getNumIdle === numIdle)\n+    assert(pool.getNumActive === numActive)\n+    assert(pool.getTotal === numTotal)\n+  }\n+\n+  private def assertPoolStateForKey(pool: InternalKafkaConsumerPool, key: CacheKey,\n+                                    numIdle: Int, numActive: Int, numTotal: Int): Unit = {\n+    assert(pool.getNumIdle(key) === numIdle)\n+    assert(pool.getNumActive(key) === numActive)\n+    assert(pool.getTotal(key) === numTotal)\n+  }\n+\n+  private def getTestKafkaParams: ju.Map[String, Object] = Map[String, Object](\n+    GROUP_ID_CONFIG -> \"groupId\",\n+    BOOTSTRAP_SERVERS_CONFIG -> \"PLAINTEXT://localhost:9092\",\n+    KEY_DESERIALIZER_CLASS_CONFIG -> classOf[ByteArrayDeserializer].getName,\n+    VALUE_DESERIALIZER_CLASS_CONFIG -> classOf[ByteArrayDeserializer].getName,\n+    AUTO_OFFSET_RESET_CONFIG -> \"earliest\",\n+    ENABLE_AUTO_COMMIT_CONFIG -> \"false\"\n+  ).asJava\n+\n+  private def borrowObjectsPerKey(\n+      pool: InternalKafkaConsumerPool,\n+      kafkaParams: ju.Map[String, Object],\n+      keys: List[CacheKey]): Seq[(CacheKey, InternalKafkaConsumer)] = {\n+    keys.map { key =>\n+      val numActiveBeforeBorrowing = pool.getNumActive\n+      val numIdleBeforeBorrowing = pool.getNumIdle\n+      val numTotalBeforeBorrowing = pool.getTotal\n+\n+      val pooledObj = pool.borrowObject(key, kafkaParams)\n+\n+      assertPoolStateForKey(pool, key, numIdle = 0, numActive = 1, numTotal = 1)\n+      assertPoolState(pool, numIdle = numIdleBeforeBorrowing,\n+        numActive = numActiveBeforeBorrowing + 1, numTotal = numTotalBeforeBorrowing + 1)\n+\n+      (key, pooledObj)\n+    }\n+  }\n+\n+  private def returnObjects(pool: InternalKafkaConsumerPool,\n+                            objects: Seq[(CacheKey, InternalKafkaConsumer)]): Unit = {"
  }],
  "prId": 22138
}]