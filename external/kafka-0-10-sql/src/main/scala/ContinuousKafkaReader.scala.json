[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Add docs explaining params.",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-03T19:08:45Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.errors.WakeupException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{DataFrame, Row, SparkSession, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeRowWriter}\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.streaming.{HDFSMetadataLog, SerializedOffset}\n+import org.apache.spark.sql.kafka010.KafkaSource.{INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE, INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE, VERSION}\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.sources.v2.streaming.reader.{ContinuousDataReader, ContinuousReader, Offset, PartitionOffset}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+class ContinuousKafkaReader("
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Can we rename this and other classes such that all Kafka class start with \"Kafka\"?",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T02:25:12Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.errors.WakeupException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{DataFrame, Row, SparkSession, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeRowWriter}\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.streaming.{HDFSMetadataLog, SerializedOffset}\n+import org.apache.spark.sql.kafka010.KafkaSource.{INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE, INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE, VERSION}\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.sources.v2.streaming.reader.{ContinuousDataReader, ContinuousReader, Offset, PartitionOffset}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+class ContinuousKafkaReader("
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Also dont forget to rename the file accordingly",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T20:09:51Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.errors.WakeupException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{DataFrame, Row, SparkSession, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeRowWriter}\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.streaming.{HDFSMetadataLog, SerializedOffset}\n+import org.apache.spark.sql.kafka010.KafkaSource.{INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE, INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE, VERSION}\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.sources.v2.streaming.reader.{ContinuousDataReader, ContinuousReader, Offset, PartitionOffset}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+class ContinuousKafkaReader("
  }],
  "prId": 20096
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Please rename this to offsetReader or maybe offsetFetcher to distinguish this from all the Reader classes in DataSourceV2",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T01:02:25Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.errors.WakeupException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{DataFrame, Row, SparkSession, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeRowWriter}\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.streaming.{HDFSMetadataLog, SerializedOffset}\n+import org.apache.spark.sql.kafka010.KafkaSource.{INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE, INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE, VERSION}\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.sources.v2.streaming.reader.{ContinuousDataReader, ContinuousReader, Offset, PartitionOffset}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+class ContinuousKafkaReader(\n+    kafkaReader: KafkaOffsetReader,"
  }],
  "prId": 20096
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "This functions seems to be duplicate of that in the KafkaSource. Can you dedup? Maybe move this into the KafkaOffsetReader?",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T01:28:06Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.errors.WakeupException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{DataFrame, Row, SparkSession, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeRowWriter}\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.streaming.{HDFSMetadataLog, SerializedOffset}\n+import org.apache.spark.sql.kafka010.KafkaSource.{INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE, INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE, VERSION}\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.sources.v2.streaming.reader.{ContinuousDataReader, ContinuousReader, Offset, PartitionOffset}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+class ContinuousKafkaReader(\n+    kafkaReader: KafkaOffsetReader,\n+    executorKafkaParams: java.util.Map[String, Object],\n+    sourceOptions: Map[String, String],\n+    metadataPath: String,\n+    initialOffsets: KafkaOffsetRangeLimit,\n+    failOnDataLoss: Boolean)\n+  extends ContinuousReader with SupportsScanUnsafeRow with Logging {\n+\n+  override def mergeOffsets(offsets: Array[PartitionOffset]): Offset = {\n+    val mergedMap = offsets.map {\n+      case KafkaSourcePartitionOffset(p, o) => Map(p -> o)\n+    }.reduce(_ ++ _)\n+    KafkaSourceOffset(mergedMap)\n+  }\n+\n+  private lazy val session = SparkSession.getActiveSession.get\n+  private lazy val sc = session.sparkContext\n+\n+  private lazy val pollTimeoutMs = sourceOptions.getOrElse(\n+    \"kafkaConsumer.pollTimeoutMs\",\n+    sc.conf.getTimeAsMs(\"spark.network.timeout\", \"120s\").toString\n+  ).toLong\n+\n+  private val maxOffsetsPerTrigger =\n+    sourceOptions.get(\"maxOffsetsPerTrigger\").map(_.toLong)\n+\n+  /**\n+   * Lazily initialize `initialPartitionOffsets` to make sure that `KafkaConsumer.poll` is only\n+   * called in StreamExecutionThread. Otherwise, interrupting a thread while running\n+   * `KafkaConsumer.poll` may hang forever (KAFKA-1894).\n+   */\n+  private lazy val initialPartitionOffsets = {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets.partitionToOffsets\n+  }\n+\n+  private def fetchAndVerify(specificOffsets: Map[TopicPartition, Long]) = {"
  }],
  "prId": 20096
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "is this used anywhere??",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T01:29:43Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.errors.WakeupException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{DataFrame, Row, SparkSession, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeRowWriter}\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.streaming.{HDFSMetadataLog, SerializedOffset}\n+import org.apache.spark.sql.kafka010.KafkaSource.{INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE, INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE, VERSION}\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.sources.v2.streaming.reader.{ContinuousDataReader, ContinuousReader, Offset, PartitionOffset}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+class ContinuousKafkaReader(\n+    kafkaReader: KafkaOffsetReader,\n+    executorKafkaParams: java.util.Map[String, Object],\n+    sourceOptions: Map[String, String],\n+    metadataPath: String,\n+    initialOffsets: KafkaOffsetRangeLimit,\n+    failOnDataLoss: Boolean)\n+  extends ContinuousReader with SupportsScanUnsafeRow with Logging {\n+\n+  override def mergeOffsets(offsets: Array[PartitionOffset]): Offset = {\n+    val mergedMap = offsets.map {\n+      case KafkaSourcePartitionOffset(p, o) => Map(p -> o)\n+    }.reduce(_ ++ _)\n+    KafkaSourceOffset(mergedMap)\n+  }\n+\n+  private lazy val session = SparkSession.getActiveSession.get\n+  private lazy val sc = session.sparkContext\n+\n+  private lazy val pollTimeoutMs = sourceOptions.getOrElse(\n+    \"kafkaConsumer.pollTimeoutMs\",\n+    sc.conf.getTimeAsMs(\"spark.network.timeout\", \"120s\").toString\n+  ).toLong\n+\n+  private val maxOffsetsPerTrigger =\n+    sourceOptions.get(\"maxOffsetsPerTrigger\").map(_.toLong)\n+\n+  /**\n+   * Lazily initialize `initialPartitionOffsets` to make sure that `KafkaConsumer.poll` is only\n+   * called in StreamExecutionThread. Otherwise, interrupting a thread while running\n+   * `KafkaConsumer.poll` may hang forever (KAFKA-1894).\n+   */\n+  private lazy val initialPartitionOffsets = {"
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "oops, no",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-05T01:33:50Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.errors.WakeupException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{DataFrame, Row, SparkSession, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeRowWriter}\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.streaming.{HDFSMetadataLog, SerializedOffset}\n+import org.apache.spark.sql.kafka010.KafkaSource.{INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE, INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE, VERSION}\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.sources.v2.streaming.reader.{ContinuousDataReader, ContinuousReader, Offset, PartitionOffset}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+class ContinuousKafkaReader(\n+    kafkaReader: KafkaOffsetReader,\n+    executorKafkaParams: java.util.Map[String, Object],\n+    sourceOptions: Map[String, String],\n+    metadataPath: String,\n+    initialOffsets: KafkaOffsetRangeLimit,\n+    failOnDataLoss: Boolean)\n+  extends ContinuousReader with SupportsScanUnsafeRow with Logging {\n+\n+  override def mergeOffsets(offsets: Array[PartitionOffset]): Offset = {\n+    val mergedMap = offsets.map {\n+      case KafkaSourcePartitionOffset(p, o) => Map(p -> o)\n+    }.reduce(_ ++ _)\n+    KafkaSourceOffset(mergedMap)\n+  }\n+\n+  private lazy val session = SparkSession.getActiveSession.get\n+  private lazy val sc = session.sparkContext\n+\n+  private lazy val pollTimeoutMs = sourceOptions.getOrElse(\n+    \"kafkaConsumer.pollTimeoutMs\",\n+    sc.conf.getTimeAsMs(\"spark.network.timeout\", \"120s\").toString\n+  ).toLong\n+\n+  private val maxOffsetsPerTrigger =\n+    sourceOptions.get(\"maxOffsetsPerTrigger\").map(_.toLong)\n+\n+  /**\n+   * Lazily initialize `initialPartitionOffsets` to make sure that `KafkaConsumer.poll` is only\n+   * called in StreamExecutionThread. Otherwise, interrupting a thread while running\n+   * `KafkaConsumer.poll` may hang forever (KAFKA-1894).\n+   */\n+  private lazy val initialPartitionOffsets = {"
  }],
  "prId": 20096
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "add docs",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T01:55:28Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.errors.WakeupException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{DataFrame, Row, SparkSession, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeRowWriter}\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.streaming.{HDFSMetadataLog, SerializedOffset}\n+import org.apache.spark.sql.kafka010.KafkaSource.{INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE, INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE, VERSION}\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.sources.v2.streaming.reader.{ContinuousDataReader, ContinuousReader, Offset, PartitionOffset}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+class ContinuousKafkaReader(\n+    kafkaReader: KafkaOffsetReader,\n+    executorKafkaParams: java.util.Map[String, Object],\n+    sourceOptions: Map[String, String],\n+    metadataPath: String,\n+    initialOffsets: KafkaOffsetRangeLimit,\n+    failOnDataLoss: Boolean)\n+  extends ContinuousReader with SupportsScanUnsafeRow with Logging {\n+\n+  override def mergeOffsets(offsets: Array[PartitionOffset]): Offset = {\n+    val mergedMap = offsets.map {\n+      case KafkaSourcePartitionOffset(p, o) => Map(p -> o)\n+    }.reduce(_ ++ _)\n+    KafkaSourceOffset(mergedMap)\n+  }\n+\n+  private lazy val session = SparkSession.getActiveSession.get\n+  private lazy val sc = session.sparkContext\n+\n+  private lazy val pollTimeoutMs = sourceOptions.getOrElse(\n+    \"kafkaConsumer.pollTimeoutMs\",\n+    sc.conf.getTimeAsMs(\"spark.network.timeout\", \"120s\").toString\n+  ).toLong\n+\n+  private val maxOffsetsPerTrigger =\n+    sourceOptions.get(\"maxOffsetsPerTrigger\").map(_.toLong)\n+\n+  /**\n+   * Lazily initialize `initialPartitionOffsets` to make sure that `KafkaConsumer.poll` is only\n+   * called in StreamExecutionThread. Otherwise, interrupting a thread while running\n+   * `KafkaConsumer.poll` may hang forever (KAFKA-1894).\n+   */\n+  private lazy val initialPartitionOffsets = {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets.partitionToOffsets\n+  }\n+\n+  private def fetchAndVerify(specificOffsets: Map[TopicPartition, Long]) = {\n+    val result = kafkaReader.fetchSpecificOffsets(specificOffsets)\n+    specificOffsets.foreach {\n+      case (tp, off) if off != KafkaOffsetRangeLimit.LATEST &&\n+        off != KafkaOffsetRangeLimit.EARLIEST =>\n+        if (result(tp) != off) {\n+          reportDataLoss(\n+            s\"startingOffsets for $tp was $off but consumer reset to ${result(tp)}\")\n+        }\n+      case _ =>\n+      // no real way to check that beginning or end is reasonable\n+    }\n+    KafkaSourceOffset(result)\n+  }\n+\n+  // Initialized when creating read tasks. If this diverges from the partitions at the latest\n+  // offsets, we need to reconfigure.\n+  // Exposed outside this object only for unit tests.\n+  private[sql] var knownPartitions: Set[TopicPartition] = _\n+\n+  override def readSchema: StructType = KafkaOffsetReader.kafkaSchema\n+\n+  private var offset: Offset = _\n+  override def setOffset(start: java.util.Optional[Offset]): Unit = {\n+    offset = start.orElse {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets\n+    }\n+  }\n+\n+  override def getStartOffset(): Offset = offset\n+\n+  override def deserializeOffset(json: String): Offset = {\n+    KafkaSourceOffset(JsonUtils.partitionOffsets(json))\n+  }\n+\n+  override def createUnsafeRowReadTasks(): java.util.List[ReadTask[UnsafeRow]] = {\n+    import scala.collection.JavaConverters._\n+\n+    val oldStartOffsets = KafkaSourceOffset.getPartitionOffsets(offset)\n+\n+    val newPartitions =\n+      kafkaReader.fetchLatestOffsets().keySet.diff(oldStartOffsets.keySet)\n+    val newPartitionOffsets = kafkaReader.fetchEarliestOffsets(newPartitions.toSeq)\n+    val startOffsets = oldStartOffsets ++ newPartitionOffsets\n+\n+    knownPartitions = startOffsets.keySet\n+\n+    startOffsets.toSeq.map {\n+      case (topicPartition, start) =>\n+        ContinuousKafkaReadTask(\n+          topicPartition, start, executorKafkaParams, pollTimeoutMs, failOnDataLoss)\n+          .asInstanceOf[ReadTask[UnsafeRow]]\n+    }.asJava\n+  }\n+\n+  /** Stop this source and free any resources it has allocated. */\n+  def stop(): Unit = synchronized {\n+    kafkaReader.close()\n+  }\n+\n+  override def commit(end: Offset): Unit = {}\n+\n+  override def needsReconfiguration(): Boolean = {\n+    knownPartitions != null && kafkaReader.fetchLatestOffsets().keySet != knownPartitions\n+  }\n+\n+  override def toString(): String = s\"KafkaSource[$kafkaReader]\"\n+\n+  /**\n+   * If `failOnDataLoss` is true, this method will throw an `IllegalStateException`.\n+   * Otherwise, just log a warning.\n+   */\n+  private def reportDataLoss(message: String): Unit = {\n+    if (failOnDataLoss) {\n+      throw new IllegalStateException(message + s\". $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE\")\n+    } else {\n+      logWarning(message + s\". $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE\")\n+    }\n+  }\n+}\n+\n+case class ContinuousKafkaReadTask("
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "rename class to have Kafka at the start to make it easier to find.",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T20:15:10Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.errors.WakeupException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{DataFrame, Row, SparkSession, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeRowWriter}\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.streaming.{HDFSMetadataLog, SerializedOffset}\n+import org.apache.spark.sql.kafka010.KafkaSource.{INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE, INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE, VERSION}\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.sources.v2.streaming.reader.{ContinuousDataReader, ContinuousReader, Offset, PartitionOffset}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+class ContinuousKafkaReader(\n+    kafkaReader: KafkaOffsetReader,\n+    executorKafkaParams: java.util.Map[String, Object],\n+    sourceOptions: Map[String, String],\n+    metadataPath: String,\n+    initialOffsets: KafkaOffsetRangeLimit,\n+    failOnDataLoss: Boolean)\n+  extends ContinuousReader with SupportsScanUnsafeRow with Logging {\n+\n+  override def mergeOffsets(offsets: Array[PartitionOffset]): Offset = {\n+    val mergedMap = offsets.map {\n+      case KafkaSourcePartitionOffset(p, o) => Map(p -> o)\n+    }.reduce(_ ++ _)\n+    KafkaSourceOffset(mergedMap)\n+  }\n+\n+  private lazy val session = SparkSession.getActiveSession.get\n+  private lazy val sc = session.sparkContext\n+\n+  private lazy val pollTimeoutMs = sourceOptions.getOrElse(\n+    \"kafkaConsumer.pollTimeoutMs\",\n+    sc.conf.getTimeAsMs(\"spark.network.timeout\", \"120s\").toString\n+  ).toLong\n+\n+  private val maxOffsetsPerTrigger =\n+    sourceOptions.get(\"maxOffsetsPerTrigger\").map(_.toLong)\n+\n+  /**\n+   * Lazily initialize `initialPartitionOffsets` to make sure that `KafkaConsumer.poll` is only\n+   * called in StreamExecutionThread. Otherwise, interrupting a thread while running\n+   * `KafkaConsumer.poll` may hang forever (KAFKA-1894).\n+   */\n+  private lazy val initialPartitionOffsets = {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets.partitionToOffsets\n+  }\n+\n+  private def fetchAndVerify(specificOffsets: Map[TopicPartition, Long]) = {\n+    val result = kafkaReader.fetchSpecificOffsets(specificOffsets)\n+    specificOffsets.foreach {\n+      case (tp, off) if off != KafkaOffsetRangeLimit.LATEST &&\n+        off != KafkaOffsetRangeLimit.EARLIEST =>\n+        if (result(tp) != off) {\n+          reportDataLoss(\n+            s\"startingOffsets for $tp was $off but consumer reset to ${result(tp)}\")\n+        }\n+      case _ =>\n+      // no real way to check that beginning or end is reasonable\n+    }\n+    KafkaSourceOffset(result)\n+  }\n+\n+  // Initialized when creating read tasks. If this diverges from the partitions at the latest\n+  // offsets, we need to reconfigure.\n+  // Exposed outside this object only for unit tests.\n+  private[sql] var knownPartitions: Set[TopicPartition] = _\n+\n+  override def readSchema: StructType = KafkaOffsetReader.kafkaSchema\n+\n+  private var offset: Offset = _\n+  override def setOffset(start: java.util.Optional[Offset]): Unit = {\n+    offset = start.orElse {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets\n+    }\n+  }\n+\n+  override def getStartOffset(): Offset = offset\n+\n+  override def deserializeOffset(json: String): Offset = {\n+    KafkaSourceOffset(JsonUtils.partitionOffsets(json))\n+  }\n+\n+  override def createUnsafeRowReadTasks(): java.util.List[ReadTask[UnsafeRow]] = {\n+    import scala.collection.JavaConverters._\n+\n+    val oldStartOffsets = KafkaSourceOffset.getPartitionOffsets(offset)\n+\n+    val newPartitions =\n+      kafkaReader.fetchLatestOffsets().keySet.diff(oldStartOffsets.keySet)\n+    val newPartitionOffsets = kafkaReader.fetchEarliestOffsets(newPartitions.toSeq)\n+    val startOffsets = oldStartOffsets ++ newPartitionOffsets\n+\n+    knownPartitions = startOffsets.keySet\n+\n+    startOffsets.toSeq.map {\n+      case (topicPartition, start) =>\n+        ContinuousKafkaReadTask(\n+          topicPartition, start, executorKafkaParams, pollTimeoutMs, failOnDataLoss)\n+          .asInstanceOf[ReadTask[UnsafeRow]]\n+    }.asJava\n+  }\n+\n+  /** Stop this source and free any resources it has allocated. */\n+  def stop(): Unit = synchronized {\n+    kafkaReader.close()\n+  }\n+\n+  override def commit(end: Offset): Unit = {}\n+\n+  override def needsReconfiguration(): Boolean = {\n+    knownPartitions != null && kafkaReader.fetchLatestOffsets().keySet != knownPartitions\n+  }\n+\n+  override def toString(): String = s\"KafkaSource[$kafkaReader]\"\n+\n+  /**\n+   * If `failOnDataLoss` is true, this method will throw an `IllegalStateException`.\n+   * Otherwise, just log a warning.\n+   */\n+  private def reportDataLoss(message: String): Unit = {\n+    if (failOnDataLoss) {\n+      throw new IllegalStateException(message + s\". $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE\")\n+    } else {\n+      logWarning(message + s\". $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE\")\n+    }\n+  }\n+}\n+\n+case class ContinuousKafkaReadTask("
  }],
  "prId": 20096
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "add docs",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T01:55:33Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.errors.WakeupException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{DataFrame, Row, SparkSession, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeRowWriter}\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.streaming.{HDFSMetadataLog, SerializedOffset}\n+import org.apache.spark.sql.kafka010.KafkaSource.{INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE, INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE, VERSION}\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.sources.v2.streaming.reader.{ContinuousDataReader, ContinuousReader, Offset, PartitionOffset}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+class ContinuousKafkaReader(\n+    kafkaReader: KafkaOffsetReader,\n+    executorKafkaParams: java.util.Map[String, Object],\n+    sourceOptions: Map[String, String],\n+    metadataPath: String,\n+    initialOffsets: KafkaOffsetRangeLimit,\n+    failOnDataLoss: Boolean)\n+  extends ContinuousReader with SupportsScanUnsafeRow with Logging {\n+\n+  override def mergeOffsets(offsets: Array[PartitionOffset]): Offset = {\n+    val mergedMap = offsets.map {\n+      case KafkaSourcePartitionOffset(p, o) => Map(p -> o)\n+    }.reduce(_ ++ _)\n+    KafkaSourceOffset(mergedMap)\n+  }\n+\n+  private lazy val session = SparkSession.getActiveSession.get\n+  private lazy val sc = session.sparkContext\n+\n+  private lazy val pollTimeoutMs = sourceOptions.getOrElse(\n+    \"kafkaConsumer.pollTimeoutMs\",\n+    sc.conf.getTimeAsMs(\"spark.network.timeout\", \"120s\").toString\n+  ).toLong\n+\n+  private val maxOffsetsPerTrigger =\n+    sourceOptions.get(\"maxOffsetsPerTrigger\").map(_.toLong)\n+\n+  /**\n+   * Lazily initialize `initialPartitionOffsets` to make sure that `KafkaConsumer.poll` is only\n+   * called in StreamExecutionThread. Otherwise, interrupting a thread while running\n+   * `KafkaConsumer.poll` may hang forever (KAFKA-1894).\n+   */\n+  private lazy val initialPartitionOffsets = {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets.partitionToOffsets\n+  }\n+\n+  private def fetchAndVerify(specificOffsets: Map[TopicPartition, Long]) = {\n+    val result = kafkaReader.fetchSpecificOffsets(specificOffsets)\n+    specificOffsets.foreach {\n+      case (tp, off) if off != KafkaOffsetRangeLimit.LATEST &&\n+        off != KafkaOffsetRangeLimit.EARLIEST =>\n+        if (result(tp) != off) {\n+          reportDataLoss(\n+            s\"startingOffsets for $tp was $off but consumer reset to ${result(tp)}\")\n+        }\n+      case _ =>\n+      // no real way to check that beginning or end is reasonable\n+    }\n+    KafkaSourceOffset(result)\n+  }\n+\n+  // Initialized when creating read tasks. If this diverges from the partitions at the latest\n+  // offsets, we need to reconfigure.\n+  // Exposed outside this object only for unit tests.\n+  private[sql] var knownPartitions: Set[TopicPartition] = _\n+\n+  override def readSchema: StructType = KafkaOffsetReader.kafkaSchema\n+\n+  private var offset: Offset = _\n+  override def setOffset(start: java.util.Optional[Offset]): Unit = {\n+    offset = start.orElse {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets\n+    }\n+  }\n+\n+  override def getStartOffset(): Offset = offset\n+\n+  override def deserializeOffset(json: String): Offset = {\n+    KafkaSourceOffset(JsonUtils.partitionOffsets(json))\n+  }\n+\n+  override def createUnsafeRowReadTasks(): java.util.List[ReadTask[UnsafeRow]] = {\n+    import scala.collection.JavaConverters._\n+\n+    val oldStartOffsets = KafkaSourceOffset.getPartitionOffsets(offset)\n+\n+    val newPartitions =\n+      kafkaReader.fetchLatestOffsets().keySet.diff(oldStartOffsets.keySet)\n+    val newPartitionOffsets = kafkaReader.fetchEarliestOffsets(newPartitions.toSeq)\n+    val startOffsets = oldStartOffsets ++ newPartitionOffsets\n+\n+    knownPartitions = startOffsets.keySet\n+\n+    startOffsets.toSeq.map {\n+      case (topicPartition, start) =>\n+        ContinuousKafkaReadTask(\n+          topicPartition, start, executorKafkaParams, pollTimeoutMs, failOnDataLoss)\n+          .asInstanceOf[ReadTask[UnsafeRow]]\n+    }.asJava\n+  }\n+\n+  /** Stop this source and free any resources it has allocated. */\n+  def stop(): Unit = synchronized {\n+    kafkaReader.close()\n+  }\n+\n+  override def commit(end: Offset): Unit = {}\n+\n+  override def needsReconfiguration(): Boolean = {\n+    knownPartitions != null && kafkaReader.fetchLatestOffsets().keySet != knownPartitions\n+  }\n+\n+  override def toString(): String = s\"KafkaSource[$kafkaReader]\"\n+\n+  /**\n+   * If `failOnDataLoss` is true, this method will throw an `IllegalStateException`.\n+   * Otherwise, just log a warning.\n+   */\n+  private def reportDataLoss(message: String): Unit = {\n+    if (failOnDataLoss) {\n+      throw new IllegalStateException(message + s\". $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE\")\n+    } else {\n+      logWarning(message + s\". $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE\")\n+    }\n+  }\n+}\n+\n+case class ContinuousKafkaReadTask(\n+    topicPartition: TopicPartition,\n+    start: Long,\n+    kafkaParams: java.util.Map[String, Object],\n+    pollTimeoutMs: Long,\n+    failOnDataLoss: Boolean)\n+  extends ReadTask[UnsafeRow] {\n+  override def createDataReader(): ContinuousKafkaDataReader = {\n+    new ContinuousKafkaDataReader(topicPartition, start, kafkaParams, pollTimeoutMs, failOnDataLoss)\n+  }\n+}\n+\n+class ContinuousKafkaDataReader("
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "rename class to begin name with Kafka to make it easy to find.",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T20:14:47Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.errors.WakeupException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{DataFrame, Row, SparkSession, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeRowWriter}\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.streaming.{HDFSMetadataLog, SerializedOffset}\n+import org.apache.spark.sql.kafka010.KafkaSource.{INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE, INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE, VERSION}\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.sources.v2.streaming.reader.{ContinuousDataReader, ContinuousReader, Offset, PartitionOffset}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+class ContinuousKafkaReader(\n+    kafkaReader: KafkaOffsetReader,\n+    executorKafkaParams: java.util.Map[String, Object],\n+    sourceOptions: Map[String, String],\n+    metadataPath: String,\n+    initialOffsets: KafkaOffsetRangeLimit,\n+    failOnDataLoss: Boolean)\n+  extends ContinuousReader with SupportsScanUnsafeRow with Logging {\n+\n+  override def mergeOffsets(offsets: Array[PartitionOffset]): Offset = {\n+    val mergedMap = offsets.map {\n+      case KafkaSourcePartitionOffset(p, o) => Map(p -> o)\n+    }.reduce(_ ++ _)\n+    KafkaSourceOffset(mergedMap)\n+  }\n+\n+  private lazy val session = SparkSession.getActiveSession.get\n+  private lazy val sc = session.sparkContext\n+\n+  private lazy val pollTimeoutMs = sourceOptions.getOrElse(\n+    \"kafkaConsumer.pollTimeoutMs\",\n+    sc.conf.getTimeAsMs(\"spark.network.timeout\", \"120s\").toString\n+  ).toLong\n+\n+  private val maxOffsetsPerTrigger =\n+    sourceOptions.get(\"maxOffsetsPerTrigger\").map(_.toLong)\n+\n+  /**\n+   * Lazily initialize `initialPartitionOffsets` to make sure that `KafkaConsumer.poll` is only\n+   * called in StreamExecutionThread. Otherwise, interrupting a thread while running\n+   * `KafkaConsumer.poll` may hang forever (KAFKA-1894).\n+   */\n+  private lazy val initialPartitionOffsets = {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets.partitionToOffsets\n+  }\n+\n+  private def fetchAndVerify(specificOffsets: Map[TopicPartition, Long]) = {\n+    val result = kafkaReader.fetchSpecificOffsets(specificOffsets)\n+    specificOffsets.foreach {\n+      case (tp, off) if off != KafkaOffsetRangeLimit.LATEST &&\n+        off != KafkaOffsetRangeLimit.EARLIEST =>\n+        if (result(tp) != off) {\n+          reportDataLoss(\n+            s\"startingOffsets for $tp was $off but consumer reset to ${result(tp)}\")\n+        }\n+      case _ =>\n+      // no real way to check that beginning or end is reasonable\n+    }\n+    KafkaSourceOffset(result)\n+  }\n+\n+  // Initialized when creating read tasks. If this diverges from the partitions at the latest\n+  // offsets, we need to reconfigure.\n+  // Exposed outside this object only for unit tests.\n+  private[sql] var knownPartitions: Set[TopicPartition] = _\n+\n+  override def readSchema: StructType = KafkaOffsetReader.kafkaSchema\n+\n+  private var offset: Offset = _\n+  override def setOffset(start: java.util.Optional[Offset]): Unit = {\n+    offset = start.orElse {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets\n+    }\n+  }\n+\n+  override def getStartOffset(): Offset = offset\n+\n+  override def deserializeOffset(json: String): Offset = {\n+    KafkaSourceOffset(JsonUtils.partitionOffsets(json))\n+  }\n+\n+  override def createUnsafeRowReadTasks(): java.util.List[ReadTask[UnsafeRow]] = {\n+    import scala.collection.JavaConverters._\n+\n+    val oldStartOffsets = KafkaSourceOffset.getPartitionOffsets(offset)\n+\n+    val newPartitions =\n+      kafkaReader.fetchLatestOffsets().keySet.diff(oldStartOffsets.keySet)\n+    val newPartitionOffsets = kafkaReader.fetchEarliestOffsets(newPartitions.toSeq)\n+    val startOffsets = oldStartOffsets ++ newPartitionOffsets\n+\n+    knownPartitions = startOffsets.keySet\n+\n+    startOffsets.toSeq.map {\n+      case (topicPartition, start) =>\n+        ContinuousKafkaReadTask(\n+          topicPartition, start, executorKafkaParams, pollTimeoutMs, failOnDataLoss)\n+          .asInstanceOf[ReadTask[UnsafeRow]]\n+    }.asJava\n+  }\n+\n+  /** Stop this source and free any resources it has allocated. */\n+  def stop(): Unit = synchronized {\n+    kafkaReader.close()\n+  }\n+\n+  override def commit(end: Offset): Unit = {}\n+\n+  override def needsReconfiguration(): Boolean = {\n+    knownPartitions != null && kafkaReader.fetchLatestOffsets().keySet != knownPartitions\n+  }\n+\n+  override def toString(): String = s\"KafkaSource[$kafkaReader]\"\n+\n+  /**\n+   * If `failOnDataLoss` is true, this method will throw an `IllegalStateException`.\n+   * Otherwise, just log a warning.\n+   */\n+  private def reportDataLoss(message: String): Unit = {\n+    if (failOnDataLoss) {\n+      throw new IllegalStateException(message + s\". $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE\")\n+    } else {\n+      logWarning(message + s\". $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE\")\n+    }\n+  }\n+}\n+\n+case class ContinuousKafkaReadTask(\n+    topicPartition: TopicPartition,\n+    start: Long,\n+    kafkaParams: java.util.Map[String, Object],\n+    pollTimeoutMs: Long,\n+    failOnDataLoss: Boolean)\n+  extends ReadTask[UnsafeRow] {\n+  override def createDataReader(): ContinuousKafkaDataReader = {\n+    new ContinuousKafkaDataReader(topicPartition, start, kafkaParams, pollTimeoutMs, failOnDataLoss)\n+  }\n+}\n+\n+class ContinuousKafkaDataReader("
  }],
  "prId": 20096
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: move this to prev line.",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T01:55:50Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.errors.WakeupException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{DataFrame, Row, SparkSession, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeRowWriter}\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.streaming.{HDFSMetadataLog, SerializedOffset}\n+import org.apache.spark.sql.kafka010.KafkaSource.{INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE, INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE, VERSION}\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.sources.v2.streaming.reader.{ContinuousDataReader, ContinuousReader, Offset, PartitionOffset}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+class ContinuousKafkaReader(\n+    kafkaReader: KafkaOffsetReader,\n+    executorKafkaParams: java.util.Map[String, Object],\n+    sourceOptions: Map[String, String],\n+    metadataPath: String,\n+    initialOffsets: KafkaOffsetRangeLimit,\n+    failOnDataLoss: Boolean)\n+  extends ContinuousReader with SupportsScanUnsafeRow with Logging {\n+\n+  override def mergeOffsets(offsets: Array[PartitionOffset]): Offset = {\n+    val mergedMap = offsets.map {\n+      case KafkaSourcePartitionOffset(p, o) => Map(p -> o)\n+    }.reduce(_ ++ _)\n+    KafkaSourceOffset(mergedMap)\n+  }\n+\n+  private lazy val session = SparkSession.getActiveSession.get\n+  private lazy val sc = session.sparkContext\n+\n+  private lazy val pollTimeoutMs = sourceOptions.getOrElse(\n+    \"kafkaConsumer.pollTimeoutMs\",\n+    sc.conf.getTimeAsMs(\"spark.network.timeout\", \"120s\").toString\n+  ).toLong\n+\n+  private val maxOffsetsPerTrigger =\n+    sourceOptions.get(\"maxOffsetsPerTrigger\").map(_.toLong)\n+\n+  /**\n+   * Lazily initialize `initialPartitionOffsets` to make sure that `KafkaConsumer.poll` is only\n+   * called in StreamExecutionThread. Otherwise, interrupting a thread while running\n+   * `KafkaConsumer.poll` may hang forever (KAFKA-1894).\n+   */\n+  private lazy val initialPartitionOffsets = {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets.partitionToOffsets\n+  }\n+\n+  private def fetchAndVerify(specificOffsets: Map[TopicPartition, Long]) = {\n+    val result = kafkaReader.fetchSpecificOffsets(specificOffsets)\n+    specificOffsets.foreach {\n+      case (tp, off) if off != KafkaOffsetRangeLimit.LATEST &&\n+        off != KafkaOffsetRangeLimit.EARLIEST =>\n+        if (result(tp) != off) {\n+          reportDataLoss(\n+            s\"startingOffsets for $tp was $off but consumer reset to ${result(tp)}\")\n+        }\n+      case _ =>\n+      // no real way to check that beginning or end is reasonable\n+    }\n+    KafkaSourceOffset(result)\n+  }\n+\n+  // Initialized when creating read tasks. If this diverges from the partitions at the latest\n+  // offsets, we need to reconfigure.\n+  // Exposed outside this object only for unit tests.\n+  private[sql] var knownPartitions: Set[TopicPartition] = _\n+\n+  override def readSchema: StructType = KafkaOffsetReader.kafkaSchema\n+\n+  private var offset: Offset = _\n+  override def setOffset(start: java.util.Optional[Offset]): Unit = {\n+    offset = start.orElse {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets\n+    }\n+  }\n+\n+  override def getStartOffset(): Offset = offset\n+\n+  override def deserializeOffset(json: String): Offset = {\n+    KafkaSourceOffset(JsonUtils.partitionOffsets(json))\n+  }\n+\n+  override def createUnsafeRowReadTasks(): java.util.List[ReadTask[UnsafeRow]] = {\n+    import scala.collection.JavaConverters._\n+\n+    val oldStartOffsets = KafkaSourceOffset.getPartitionOffsets(offset)\n+\n+    val newPartitions =\n+      kafkaReader.fetchLatestOffsets().keySet.diff(oldStartOffsets.keySet)\n+    val newPartitionOffsets = kafkaReader.fetchEarliestOffsets(newPartitions.toSeq)\n+    val startOffsets = oldStartOffsets ++ newPartitionOffsets\n+\n+    knownPartitions = startOffsets.keySet\n+\n+    startOffsets.toSeq.map {\n+      case (topicPartition, start) =>\n+        ContinuousKafkaReadTask(\n+          topicPartition, start, executorKafkaParams, pollTimeoutMs, failOnDataLoss)\n+          .asInstanceOf[ReadTask[UnsafeRow]]\n+    }.asJava\n+  }\n+\n+  /** Stop this source and free any resources it has allocated. */\n+  def stop(): Unit = synchronized {\n+    kafkaReader.close()\n+  }\n+\n+  override def commit(end: Offset): Unit = {}\n+\n+  override def needsReconfiguration(): Boolean = {\n+    knownPartitions != null && kafkaReader.fetchLatestOffsets().keySet != knownPartitions\n+  }\n+\n+  override def toString(): String = s\"KafkaSource[$kafkaReader]\"\n+\n+  /**\n+   * If `failOnDataLoss` is true, this method will throw an `IllegalStateException`.\n+   * Otherwise, just log a warning.\n+   */\n+  private def reportDataLoss(message: String): Unit = {\n+    if (failOnDataLoss) {\n+      throw new IllegalStateException(message + s\". $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE\")\n+    } else {\n+      logWarning(message + s\". $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE\")\n+    }\n+  }\n+}\n+\n+case class ContinuousKafkaReadTask(\n+    topicPartition: TopicPartition,\n+    start: Long,\n+    kafkaParams: java.util.Map[String, Object],\n+    pollTimeoutMs: Long,\n+    failOnDataLoss: Boolean)\n+  extends ReadTask[UnsafeRow] {"
  }],
  "prId": 20096
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: move this to prev line.",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T01:55:59Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.errors.WakeupException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{DataFrame, Row, SparkSession, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeRowWriter}\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.streaming.{HDFSMetadataLog, SerializedOffset}\n+import org.apache.spark.sql.kafka010.KafkaSource.{INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE, INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE, VERSION}\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.sources.v2.streaming.reader.{ContinuousDataReader, ContinuousReader, Offset, PartitionOffset}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+class ContinuousKafkaReader(\n+    kafkaReader: KafkaOffsetReader,\n+    executorKafkaParams: java.util.Map[String, Object],\n+    sourceOptions: Map[String, String],\n+    metadataPath: String,\n+    initialOffsets: KafkaOffsetRangeLimit,\n+    failOnDataLoss: Boolean)\n+  extends ContinuousReader with SupportsScanUnsafeRow with Logging {\n+\n+  override def mergeOffsets(offsets: Array[PartitionOffset]): Offset = {\n+    val mergedMap = offsets.map {\n+      case KafkaSourcePartitionOffset(p, o) => Map(p -> o)\n+    }.reduce(_ ++ _)\n+    KafkaSourceOffset(mergedMap)\n+  }\n+\n+  private lazy val session = SparkSession.getActiveSession.get\n+  private lazy val sc = session.sparkContext\n+\n+  private lazy val pollTimeoutMs = sourceOptions.getOrElse(\n+    \"kafkaConsumer.pollTimeoutMs\",\n+    sc.conf.getTimeAsMs(\"spark.network.timeout\", \"120s\").toString\n+  ).toLong\n+\n+  private val maxOffsetsPerTrigger =\n+    sourceOptions.get(\"maxOffsetsPerTrigger\").map(_.toLong)\n+\n+  /**\n+   * Lazily initialize `initialPartitionOffsets` to make sure that `KafkaConsumer.poll` is only\n+   * called in StreamExecutionThread. Otherwise, interrupting a thread while running\n+   * `KafkaConsumer.poll` may hang forever (KAFKA-1894).\n+   */\n+  private lazy val initialPartitionOffsets = {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets.partitionToOffsets\n+  }\n+\n+  private def fetchAndVerify(specificOffsets: Map[TopicPartition, Long]) = {\n+    val result = kafkaReader.fetchSpecificOffsets(specificOffsets)\n+    specificOffsets.foreach {\n+      case (tp, off) if off != KafkaOffsetRangeLimit.LATEST &&\n+        off != KafkaOffsetRangeLimit.EARLIEST =>\n+        if (result(tp) != off) {\n+          reportDataLoss(\n+            s\"startingOffsets for $tp was $off but consumer reset to ${result(tp)}\")\n+        }\n+      case _ =>\n+      // no real way to check that beginning or end is reasonable\n+    }\n+    KafkaSourceOffset(result)\n+  }\n+\n+  // Initialized when creating read tasks. If this diverges from the partitions at the latest\n+  // offsets, we need to reconfigure.\n+  // Exposed outside this object only for unit tests.\n+  private[sql] var knownPartitions: Set[TopicPartition] = _\n+\n+  override def readSchema: StructType = KafkaOffsetReader.kafkaSchema\n+\n+  private var offset: Offset = _\n+  override def setOffset(start: java.util.Optional[Offset]): Unit = {\n+    offset = start.orElse {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets\n+    }\n+  }\n+\n+  override def getStartOffset(): Offset = offset\n+\n+  override def deserializeOffset(json: String): Offset = {\n+    KafkaSourceOffset(JsonUtils.partitionOffsets(json))\n+  }\n+\n+  override def createUnsafeRowReadTasks(): java.util.List[ReadTask[UnsafeRow]] = {\n+    import scala.collection.JavaConverters._\n+\n+    val oldStartOffsets = KafkaSourceOffset.getPartitionOffsets(offset)\n+\n+    val newPartitions =\n+      kafkaReader.fetchLatestOffsets().keySet.diff(oldStartOffsets.keySet)\n+    val newPartitionOffsets = kafkaReader.fetchEarliestOffsets(newPartitions.toSeq)\n+    val startOffsets = oldStartOffsets ++ newPartitionOffsets\n+\n+    knownPartitions = startOffsets.keySet\n+\n+    startOffsets.toSeq.map {\n+      case (topicPartition, start) =>\n+        ContinuousKafkaReadTask(\n+          topicPartition, start, executorKafkaParams, pollTimeoutMs, failOnDataLoss)\n+          .asInstanceOf[ReadTask[UnsafeRow]]\n+    }.asJava\n+  }\n+\n+  /** Stop this source and free any resources it has allocated. */\n+  def stop(): Unit = synchronized {\n+    kafkaReader.close()\n+  }\n+\n+  override def commit(end: Offset): Unit = {}\n+\n+  override def needsReconfiguration(): Boolean = {\n+    knownPartitions != null && kafkaReader.fetchLatestOffsets().keySet != knownPartitions\n+  }\n+\n+  override def toString(): String = s\"KafkaSource[$kafkaReader]\"\n+\n+  /**\n+   * If `failOnDataLoss` is true, this method will throw an `IllegalStateException`.\n+   * Otherwise, just log a warning.\n+   */\n+  private def reportDataLoss(message: String): Unit = {\n+    if (failOnDataLoss) {\n+      throw new IllegalStateException(message + s\". $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE\")\n+    } else {\n+      logWarning(message + s\". $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE\")\n+    }\n+  }\n+}\n+\n+case class ContinuousKafkaReadTask(\n+    topicPartition: TopicPartition,\n+    start: Long,\n+    kafkaParams: java.util.Map[String, Object],\n+    pollTimeoutMs: Long,\n+    failOnDataLoss: Boolean)\n+  extends ReadTask[UnsafeRow] {\n+  override def createDataReader(): ContinuousKafkaDataReader = {\n+    new ContinuousKafkaDataReader(topicPartition, start, kafkaParams, pollTimeoutMs, failOnDataLoss)\n+  }\n+}\n+\n+class ContinuousKafkaDataReader(\n+    topicPartition: TopicPartition,\n+    start: Long,\n+    kafkaParams: java.util.Map[String, Object],\n+    pollTimeoutMs: Long,\n+    failOnDataLoss: Boolean)\n+  extends ContinuousDataReader[UnsafeRow] {"
  }],
  "prId": 20096
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "startOffset",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T01:57:05Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.errors.WakeupException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{DataFrame, Row, SparkSession, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeRowWriter}\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.streaming.{HDFSMetadataLog, SerializedOffset}\n+import org.apache.spark.sql.kafka010.KafkaSource.{INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE, INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE, VERSION}\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.sources.v2.streaming.reader.{ContinuousDataReader, ContinuousReader, Offset, PartitionOffset}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+class ContinuousKafkaReader(\n+    kafkaReader: KafkaOffsetReader,\n+    executorKafkaParams: java.util.Map[String, Object],\n+    sourceOptions: Map[String, String],\n+    metadataPath: String,\n+    initialOffsets: KafkaOffsetRangeLimit,\n+    failOnDataLoss: Boolean)\n+  extends ContinuousReader with SupportsScanUnsafeRow with Logging {\n+\n+  override def mergeOffsets(offsets: Array[PartitionOffset]): Offset = {\n+    val mergedMap = offsets.map {\n+      case KafkaSourcePartitionOffset(p, o) => Map(p -> o)\n+    }.reduce(_ ++ _)\n+    KafkaSourceOffset(mergedMap)\n+  }\n+\n+  private lazy val session = SparkSession.getActiveSession.get\n+  private lazy val sc = session.sparkContext\n+\n+  private lazy val pollTimeoutMs = sourceOptions.getOrElse(\n+    \"kafkaConsumer.pollTimeoutMs\",\n+    sc.conf.getTimeAsMs(\"spark.network.timeout\", \"120s\").toString\n+  ).toLong\n+\n+  private val maxOffsetsPerTrigger =\n+    sourceOptions.get(\"maxOffsetsPerTrigger\").map(_.toLong)\n+\n+  /**\n+   * Lazily initialize `initialPartitionOffsets` to make sure that `KafkaConsumer.poll` is only\n+   * called in StreamExecutionThread. Otherwise, interrupting a thread while running\n+   * `KafkaConsumer.poll` may hang forever (KAFKA-1894).\n+   */\n+  private lazy val initialPartitionOffsets = {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets.partitionToOffsets\n+  }\n+\n+  private def fetchAndVerify(specificOffsets: Map[TopicPartition, Long]) = {\n+    val result = kafkaReader.fetchSpecificOffsets(specificOffsets)\n+    specificOffsets.foreach {\n+      case (tp, off) if off != KafkaOffsetRangeLimit.LATEST &&\n+        off != KafkaOffsetRangeLimit.EARLIEST =>\n+        if (result(tp) != off) {\n+          reportDataLoss(\n+            s\"startingOffsets for $tp was $off but consumer reset to ${result(tp)}\")\n+        }\n+      case _ =>\n+      // no real way to check that beginning or end is reasonable\n+    }\n+    KafkaSourceOffset(result)\n+  }\n+\n+  // Initialized when creating read tasks. If this diverges from the partitions at the latest\n+  // offsets, we need to reconfigure.\n+  // Exposed outside this object only for unit tests.\n+  private[sql] var knownPartitions: Set[TopicPartition] = _\n+\n+  override def readSchema: StructType = KafkaOffsetReader.kafkaSchema\n+\n+  private var offset: Offset = _\n+  override def setOffset(start: java.util.Optional[Offset]): Unit = {\n+    offset = start.orElse {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets\n+    }\n+  }\n+\n+  override def getStartOffset(): Offset = offset\n+\n+  override def deserializeOffset(json: String): Offset = {\n+    KafkaSourceOffset(JsonUtils.partitionOffsets(json))\n+  }\n+\n+  override def createUnsafeRowReadTasks(): java.util.List[ReadTask[UnsafeRow]] = {\n+    import scala.collection.JavaConverters._\n+\n+    val oldStartOffsets = KafkaSourceOffset.getPartitionOffsets(offset)\n+\n+    val newPartitions =\n+      kafkaReader.fetchLatestOffsets().keySet.diff(oldStartOffsets.keySet)\n+    val newPartitionOffsets = kafkaReader.fetchEarliestOffsets(newPartitions.toSeq)\n+    val startOffsets = oldStartOffsets ++ newPartitionOffsets\n+\n+    knownPartitions = startOffsets.keySet\n+\n+    startOffsets.toSeq.map {\n+      case (topicPartition, start) =>\n+        ContinuousKafkaReadTask(\n+          topicPartition, start, executorKafkaParams, pollTimeoutMs, failOnDataLoss)\n+          .asInstanceOf[ReadTask[UnsafeRow]]\n+    }.asJava\n+  }\n+\n+  /** Stop this source and free any resources it has allocated. */\n+  def stop(): Unit = synchronized {\n+    kafkaReader.close()\n+  }\n+\n+  override def commit(end: Offset): Unit = {}\n+\n+  override def needsReconfiguration(): Boolean = {\n+    knownPartitions != null && kafkaReader.fetchLatestOffsets().keySet != knownPartitions\n+  }\n+\n+  override def toString(): String = s\"KafkaSource[$kafkaReader]\"\n+\n+  /**\n+   * If `failOnDataLoss` is true, this method will throw an `IllegalStateException`.\n+   * Otherwise, just log a warning.\n+   */\n+  private def reportDataLoss(message: String): Unit = {\n+    if (failOnDataLoss) {\n+      throw new IllegalStateException(message + s\". $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE\")\n+    } else {\n+      logWarning(message + s\". $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE\")\n+    }\n+  }\n+}\n+\n+case class ContinuousKafkaReadTask(\n+    topicPartition: TopicPartition,\n+    start: Long,\n+    kafkaParams: java.util.Map[String, Object],\n+    pollTimeoutMs: Long,\n+    failOnDataLoss: Boolean)\n+  extends ReadTask[UnsafeRow] {\n+  override def createDataReader(): ContinuousKafkaDataReader = {\n+    new ContinuousKafkaDataReader(topicPartition, start, kafkaParams, pollTimeoutMs, failOnDataLoss)\n+  }\n+}\n+\n+class ContinuousKafkaDataReader(\n+    topicPartition: TopicPartition,\n+    start: Long,"
  }],
  "prId": 20096
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "startOffset",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T02:01:40Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.errors.WakeupException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{DataFrame, Row, SparkSession, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeRowWriter}\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.streaming.{HDFSMetadataLog, SerializedOffset}\n+import org.apache.spark.sql.kafka010.KafkaSource.{INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE, INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE, VERSION}\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.sources.v2.streaming.reader.{ContinuousDataReader, ContinuousReader, Offset, PartitionOffset}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+class ContinuousKafkaReader(\n+    kafkaReader: KafkaOffsetReader,\n+    executorKafkaParams: java.util.Map[String, Object],\n+    sourceOptions: Map[String, String],\n+    metadataPath: String,\n+    initialOffsets: KafkaOffsetRangeLimit,\n+    failOnDataLoss: Boolean)\n+  extends ContinuousReader with SupportsScanUnsafeRow with Logging {\n+\n+  override def mergeOffsets(offsets: Array[PartitionOffset]): Offset = {\n+    val mergedMap = offsets.map {\n+      case KafkaSourcePartitionOffset(p, o) => Map(p -> o)\n+    }.reduce(_ ++ _)\n+    KafkaSourceOffset(mergedMap)\n+  }\n+\n+  private lazy val session = SparkSession.getActiveSession.get\n+  private lazy val sc = session.sparkContext\n+\n+  private lazy val pollTimeoutMs = sourceOptions.getOrElse(\n+    \"kafkaConsumer.pollTimeoutMs\",\n+    sc.conf.getTimeAsMs(\"spark.network.timeout\", \"120s\").toString\n+  ).toLong\n+\n+  private val maxOffsetsPerTrigger =\n+    sourceOptions.get(\"maxOffsetsPerTrigger\").map(_.toLong)\n+\n+  /**\n+   * Lazily initialize `initialPartitionOffsets` to make sure that `KafkaConsumer.poll` is only\n+   * called in StreamExecutionThread. Otherwise, interrupting a thread while running\n+   * `KafkaConsumer.poll` may hang forever (KAFKA-1894).\n+   */\n+  private lazy val initialPartitionOffsets = {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets.partitionToOffsets\n+  }\n+\n+  private def fetchAndVerify(specificOffsets: Map[TopicPartition, Long]) = {\n+    val result = kafkaReader.fetchSpecificOffsets(specificOffsets)\n+    specificOffsets.foreach {\n+      case (tp, off) if off != KafkaOffsetRangeLimit.LATEST &&\n+        off != KafkaOffsetRangeLimit.EARLIEST =>\n+        if (result(tp) != off) {\n+          reportDataLoss(\n+            s\"startingOffsets for $tp was $off but consumer reset to ${result(tp)}\")\n+        }\n+      case _ =>\n+      // no real way to check that beginning or end is reasonable\n+    }\n+    KafkaSourceOffset(result)\n+  }\n+\n+  // Initialized when creating read tasks. If this diverges from the partitions at the latest\n+  // offsets, we need to reconfigure.\n+  // Exposed outside this object only for unit tests.\n+  private[sql] var knownPartitions: Set[TopicPartition] = _\n+\n+  override def readSchema: StructType = KafkaOffsetReader.kafkaSchema\n+\n+  private var offset: Offset = _\n+  override def setOffset(start: java.util.Optional[Offset]): Unit = {\n+    offset = start.orElse {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets\n+    }\n+  }\n+\n+  override def getStartOffset(): Offset = offset\n+\n+  override def deserializeOffset(json: String): Offset = {\n+    KafkaSourceOffset(JsonUtils.partitionOffsets(json))\n+  }\n+\n+  override def createUnsafeRowReadTasks(): java.util.List[ReadTask[UnsafeRow]] = {\n+    import scala.collection.JavaConverters._\n+\n+    val oldStartOffsets = KafkaSourceOffset.getPartitionOffsets(offset)\n+\n+    val newPartitions =\n+      kafkaReader.fetchLatestOffsets().keySet.diff(oldStartOffsets.keySet)\n+    val newPartitionOffsets = kafkaReader.fetchEarliestOffsets(newPartitions.toSeq)\n+    val startOffsets = oldStartOffsets ++ newPartitionOffsets\n+\n+    knownPartitions = startOffsets.keySet\n+\n+    startOffsets.toSeq.map {\n+      case (topicPartition, start) =>\n+        ContinuousKafkaReadTask(\n+          topicPartition, start, executorKafkaParams, pollTimeoutMs, failOnDataLoss)\n+          .asInstanceOf[ReadTask[UnsafeRow]]\n+    }.asJava\n+  }\n+\n+  /** Stop this source and free any resources it has allocated. */\n+  def stop(): Unit = synchronized {\n+    kafkaReader.close()\n+  }\n+\n+  override def commit(end: Offset): Unit = {}\n+\n+  override def needsReconfiguration(): Boolean = {\n+    knownPartitions != null && kafkaReader.fetchLatestOffsets().keySet != knownPartitions\n+  }\n+\n+  override def toString(): String = s\"KafkaSource[$kafkaReader]\"\n+\n+  /**\n+   * If `failOnDataLoss` is true, this method will throw an `IllegalStateException`.\n+   * Otherwise, just log a warning.\n+   */\n+  private def reportDataLoss(message: String): Unit = {\n+    if (failOnDataLoss) {\n+      throw new IllegalStateException(message + s\". $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE\")\n+    } else {\n+      logWarning(message + s\". $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE\")\n+    }\n+  }\n+}\n+\n+case class ContinuousKafkaReadTask(\n+    topicPartition: TopicPartition,\n+    start: Long,"
  }],
  "prId": 20096
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: oldStartPartitionOffsets",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T02:02:53Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.errors.WakeupException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{DataFrame, Row, SparkSession, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeRowWriter}\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.streaming.{HDFSMetadataLog, SerializedOffset}\n+import org.apache.spark.sql.kafka010.KafkaSource.{INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE, INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE, VERSION}\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.sources.v2.streaming.reader.{ContinuousDataReader, ContinuousReader, Offset, PartitionOffset}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+class ContinuousKafkaReader(\n+    kafkaReader: KafkaOffsetReader,\n+    executorKafkaParams: java.util.Map[String, Object],\n+    sourceOptions: Map[String, String],\n+    metadataPath: String,\n+    initialOffsets: KafkaOffsetRangeLimit,\n+    failOnDataLoss: Boolean)\n+  extends ContinuousReader with SupportsScanUnsafeRow with Logging {\n+\n+  override def mergeOffsets(offsets: Array[PartitionOffset]): Offset = {\n+    val mergedMap = offsets.map {\n+      case KafkaSourcePartitionOffset(p, o) => Map(p -> o)\n+    }.reduce(_ ++ _)\n+    KafkaSourceOffset(mergedMap)\n+  }\n+\n+  private lazy val session = SparkSession.getActiveSession.get\n+  private lazy val sc = session.sparkContext\n+\n+  private lazy val pollTimeoutMs = sourceOptions.getOrElse(\n+    \"kafkaConsumer.pollTimeoutMs\",\n+    sc.conf.getTimeAsMs(\"spark.network.timeout\", \"120s\").toString\n+  ).toLong\n+\n+  private val maxOffsetsPerTrigger =\n+    sourceOptions.get(\"maxOffsetsPerTrigger\").map(_.toLong)\n+\n+  /**\n+   * Lazily initialize `initialPartitionOffsets` to make sure that `KafkaConsumer.poll` is only\n+   * called in StreamExecutionThread. Otherwise, interrupting a thread while running\n+   * `KafkaConsumer.poll` may hang forever (KAFKA-1894).\n+   */\n+  private lazy val initialPartitionOffsets = {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets.partitionToOffsets\n+  }\n+\n+  private def fetchAndVerify(specificOffsets: Map[TopicPartition, Long]) = {\n+    val result = kafkaReader.fetchSpecificOffsets(specificOffsets)\n+    specificOffsets.foreach {\n+      case (tp, off) if off != KafkaOffsetRangeLimit.LATEST &&\n+        off != KafkaOffsetRangeLimit.EARLIEST =>\n+        if (result(tp) != off) {\n+          reportDataLoss(\n+            s\"startingOffsets for $tp was $off but consumer reset to ${result(tp)}\")\n+        }\n+      case _ =>\n+      // no real way to check that beginning or end is reasonable\n+    }\n+    KafkaSourceOffset(result)\n+  }\n+\n+  // Initialized when creating read tasks. If this diverges from the partitions at the latest\n+  // offsets, we need to reconfigure.\n+  // Exposed outside this object only for unit tests.\n+  private[sql] var knownPartitions: Set[TopicPartition] = _\n+\n+  override def readSchema: StructType = KafkaOffsetReader.kafkaSchema\n+\n+  private var offset: Offset = _\n+  override def setOffset(start: java.util.Optional[Offset]): Unit = {\n+    offset = start.orElse {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets\n+    }\n+  }\n+\n+  override def getStartOffset(): Offset = offset\n+\n+  override def deserializeOffset(json: String): Offset = {\n+    KafkaSourceOffset(JsonUtils.partitionOffsets(json))\n+  }\n+\n+  override def createUnsafeRowReadTasks(): java.util.List[ReadTask[UnsafeRow]] = {\n+    import scala.collection.JavaConverters._\n+\n+    val oldStartOffsets = KafkaSourceOffset.getPartitionOffsets(offset)"
  }],
  "prId": 20096
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "I think the resolution should have been done at the time of creating the task. I think that keeps it more consistent -- once the task has been defined, everything has already been resolved and the output is deterministic. ",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T02:05:19Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.errors.WakeupException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{DataFrame, Row, SparkSession, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeRowWriter}\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.streaming.{HDFSMetadataLog, SerializedOffset}\n+import org.apache.spark.sql.kafka010.KafkaSource.{INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE, INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE, VERSION}\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.sources.v2.streaming.reader.{ContinuousDataReader, ContinuousReader, Offset, PartitionOffset}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+class ContinuousKafkaReader(\n+    kafkaReader: KafkaOffsetReader,\n+    executorKafkaParams: java.util.Map[String, Object],\n+    sourceOptions: Map[String, String],\n+    metadataPath: String,\n+    initialOffsets: KafkaOffsetRangeLimit,\n+    failOnDataLoss: Boolean)\n+  extends ContinuousReader with SupportsScanUnsafeRow with Logging {\n+\n+  override def mergeOffsets(offsets: Array[PartitionOffset]): Offset = {\n+    val mergedMap = offsets.map {\n+      case KafkaSourcePartitionOffset(p, o) => Map(p -> o)\n+    }.reduce(_ ++ _)\n+    KafkaSourceOffset(mergedMap)\n+  }\n+\n+  private lazy val session = SparkSession.getActiveSession.get\n+  private lazy val sc = session.sparkContext\n+\n+  private lazy val pollTimeoutMs = sourceOptions.getOrElse(\n+    \"kafkaConsumer.pollTimeoutMs\",\n+    sc.conf.getTimeAsMs(\"spark.network.timeout\", \"120s\").toString\n+  ).toLong\n+\n+  private val maxOffsetsPerTrigger =\n+    sourceOptions.get(\"maxOffsetsPerTrigger\").map(_.toLong)\n+\n+  /**\n+   * Lazily initialize `initialPartitionOffsets` to make sure that `KafkaConsumer.poll` is only\n+   * called in StreamExecutionThread. Otherwise, interrupting a thread while running\n+   * `KafkaConsumer.poll` may hang forever (KAFKA-1894).\n+   */\n+  private lazy val initialPartitionOffsets = {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets.partitionToOffsets\n+  }\n+\n+  private def fetchAndVerify(specificOffsets: Map[TopicPartition, Long]) = {\n+    val result = kafkaReader.fetchSpecificOffsets(specificOffsets)\n+    specificOffsets.foreach {\n+      case (tp, off) if off != KafkaOffsetRangeLimit.LATEST &&\n+        off != KafkaOffsetRangeLimit.EARLIEST =>\n+        if (result(tp) != off) {\n+          reportDataLoss(\n+            s\"startingOffsets for $tp was $off but consumer reset to ${result(tp)}\")\n+        }\n+      case _ =>\n+      // no real way to check that beginning or end is reasonable\n+    }\n+    KafkaSourceOffset(result)\n+  }\n+\n+  // Initialized when creating read tasks. If this diverges from the partitions at the latest\n+  // offsets, we need to reconfigure.\n+  // Exposed outside this object only for unit tests.\n+  private[sql] var knownPartitions: Set[TopicPartition] = _\n+\n+  override def readSchema: StructType = KafkaOffsetReader.kafkaSchema\n+\n+  private var offset: Offset = _\n+  override def setOffset(start: java.util.Optional[Offset]): Unit = {\n+    offset = start.orElse {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets\n+    }\n+  }\n+\n+  override def getStartOffset(): Offset = offset\n+\n+  override def deserializeOffset(json: String): Offset = {\n+    KafkaSourceOffset(JsonUtils.partitionOffsets(json))\n+  }\n+\n+  override def createUnsafeRowReadTasks(): java.util.List[ReadTask[UnsafeRow]] = {\n+    import scala.collection.JavaConverters._\n+\n+    val oldStartOffsets = KafkaSourceOffset.getPartitionOffsets(offset)\n+\n+    val newPartitions =\n+      kafkaReader.fetchLatestOffsets().keySet.diff(oldStartOffsets.keySet)\n+    val newPartitionOffsets = kafkaReader.fetchEarliestOffsets(newPartitions.toSeq)\n+    val startOffsets = oldStartOffsets ++ newPartitionOffsets\n+\n+    knownPartitions = startOffsets.keySet\n+\n+    startOffsets.toSeq.map {\n+      case (topicPartition, start) =>\n+        ContinuousKafkaReadTask(\n+          topicPartition, start, executorKafkaParams, pollTimeoutMs, failOnDataLoss)\n+          .asInstanceOf[ReadTask[UnsafeRow]]\n+    }.asJava\n+  }\n+\n+  /** Stop this source and free any resources it has allocated. */\n+  def stop(): Unit = synchronized {\n+    kafkaReader.close()\n+  }\n+\n+  override def commit(end: Offset): Unit = {}\n+\n+  override def needsReconfiguration(): Boolean = {\n+    knownPartitions != null && kafkaReader.fetchLatestOffsets().keySet != knownPartitions\n+  }\n+\n+  override def toString(): String = s\"KafkaSource[$kafkaReader]\"\n+\n+  /**\n+   * If `failOnDataLoss` is true, this method will throw an `IllegalStateException`.\n+   * Otherwise, just log a warning.\n+   */\n+  private def reportDataLoss(message: String): Unit = {\n+    if (failOnDataLoss) {\n+      throw new IllegalStateException(message + s\". $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE\")\n+    } else {\n+      logWarning(message + s\". $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE\")\n+    }\n+  }\n+}\n+\n+case class ContinuousKafkaReadTask(\n+    topicPartition: TopicPartition,\n+    start: Long,\n+    kafkaParams: java.util.Map[String, Object],\n+    pollTimeoutMs: Long,\n+    failOnDataLoss: Boolean)\n+  extends ReadTask[UnsafeRow] {\n+  override def createDataReader(): ContinuousKafkaDataReader = {\n+    new ContinuousKafkaDataReader(topicPartition, start, kafkaParams, pollTimeoutMs, failOnDataLoss)\n+  }\n+}\n+\n+class ContinuousKafkaDataReader(\n+    topicPartition: TopicPartition,\n+    start: Long,\n+    kafkaParams: java.util.Map[String, Object],\n+    pollTimeoutMs: Long,\n+    failOnDataLoss: Boolean)\n+  extends ContinuousDataReader[UnsafeRow] {\n+  private val topic = topicPartition.topic\n+  private val kafkaPartition = topicPartition.partition\n+  private val consumer = CachedKafkaConsumer.createUncached(topic, kafkaPartition, kafkaParams)\n+\n+  private val closed = new AtomicBoolean(false)\n+\n+  private var nextKafkaOffset = start match {\n+    case s if s >= 0 => s\n+    case KafkaOffsetRangeLimit.EARLIEST => consumer.getAvailableOffsetRange().earliest"
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "I think this resolution actually doesn't do anything at all, because the offsets were already resolved earlier. I think I had the resolution only here originally, and forgot to delete it because it ends up as a no-op. I'll remove it now.",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-05T01:37:54Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.errors.WakeupException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{DataFrame, Row, SparkSession, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeRowWriter}\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.streaming.{HDFSMetadataLog, SerializedOffset}\n+import org.apache.spark.sql.kafka010.KafkaSource.{INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE, INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE, VERSION}\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.sources.v2.streaming.reader.{ContinuousDataReader, ContinuousReader, Offset, PartitionOffset}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+class ContinuousKafkaReader(\n+    kafkaReader: KafkaOffsetReader,\n+    executorKafkaParams: java.util.Map[String, Object],\n+    sourceOptions: Map[String, String],\n+    metadataPath: String,\n+    initialOffsets: KafkaOffsetRangeLimit,\n+    failOnDataLoss: Boolean)\n+  extends ContinuousReader with SupportsScanUnsafeRow with Logging {\n+\n+  override def mergeOffsets(offsets: Array[PartitionOffset]): Offset = {\n+    val mergedMap = offsets.map {\n+      case KafkaSourcePartitionOffset(p, o) => Map(p -> o)\n+    }.reduce(_ ++ _)\n+    KafkaSourceOffset(mergedMap)\n+  }\n+\n+  private lazy val session = SparkSession.getActiveSession.get\n+  private lazy val sc = session.sparkContext\n+\n+  private lazy val pollTimeoutMs = sourceOptions.getOrElse(\n+    \"kafkaConsumer.pollTimeoutMs\",\n+    sc.conf.getTimeAsMs(\"spark.network.timeout\", \"120s\").toString\n+  ).toLong\n+\n+  private val maxOffsetsPerTrigger =\n+    sourceOptions.get(\"maxOffsetsPerTrigger\").map(_.toLong)\n+\n+  /**\n+   * Lazily initialize `initialPartitionOffsets` to make sure that `KafkaConsumer.poll` is only\n+   * called in StreamExecutionThread. Otherwise, interrupting a thread while running\n+   * `KafkaConsumer.poll` may hang forever (KAFKA-1894).\n+   */\n+  private lazy val initialPartitionOffsets = {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets.partitionToOffsets\n+  }\n+\n+  private def fetchAndVerify(specificOffsets: Map[TopicPartition, Long]) = {\n+    val result = kafkaReader.fetchSpecificOffsets(specificOffsets)\n+    specificOffsets.foreach {\n+      case (tp, off) if off != KafkaOffsetRangeLimit.LATEST &&\n+        off != KafkaOffsetRangeLimit.EARLIEST =>\n+        if (result(tp) != off) {\n+          reportDataLoss(\n+            s\"startingOffsets for $tp was $off but consumer reset to ${result(tp)}\")\n+        }\n+      case _ =>\n+      // no real way to check that beginning or end is reasonable\n+    }\n+    KafkaSourceOffset(result)\n+  }\n+\n+  // Initialized when creating read tasks. If this diverges from the partitions at the latest\n+  // offsets, we need to reconfigure.\n+  // Exposed outside this object only for unit tests.\n+  private[sql] var knownPartitions: Set[TopicPartition] = _\n+\n+  override def readSchema: StructType = KafkaOffsetReader.kafkaSchema\n+\n+  private var offset: Offset = _\n+  override def setOffset(start: java.util.Optional[Offset]): Unit = {\n+    offset = start.orElse {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets\n+    }\n+  }\n+\n+  override def getStartOffset(): Offset = offset\n+\n+  override def deserializeOffset(json: String): Offset = {\n+    KafkaSourceOffset(JsonUtils.partitionOffsets(json))\n+  }\n+\n+  override def createUnsafeRowReadTasks(): java.util.List[ReadTask[UnsafeRow]] = {\n+    import scala.collection.JavaConverters._\n+\n+    val oldStartOffsets = KafkaSourceOffset.getPartitionOffsets(offset)\n+\n+    val newPartitions =\n+      kafkaReader.fetchLatestOffsets().keySet.diff(oldStartOffsets.keySet)\n+    val newPartitionOffsets = kafkaReader.fetchEarliestOffsets(newPartitions.toSeq)\n+    val startOffsets = oldStartOffsets ++ newPartitionOffsets\n+\n+    knownPartitions = startOffsets.keySet\n+\n+    startOffsets.toSeq.map {\n+      case (topicPartition, start) =>\n+        ContinuousKafkaReadTask(\n+          topicPartition, start, executorKafkaParams, pollTimeoutMs, failOnDataLoss)\n+          .asInstanceOf[ReadTask[UnsafeRow]]\n+    }.asJava\n+  }\n+\n+  /** Stop this source and free any resources it has allocated. */\n+  def stop(): Unit = synchronized {\n+    kafkaReader.close()\n+  }\n+\n+  override def commit(end: Offset): Unit = {}\n+\n+  override def needsReconfiguration(): Boolean = {\n+    knownPartitions != null && kafkaReader.fetchLatestOffsets().keySet != knownPartitions\n+  }\n+\n+  override def toString(): String = s\"KafkaSource[$kafkaReader]\"\n+\n+  /**\n+   * If `failOnDataLoss` is true, this method will throw an `IllegalStateException`.\n+   * Otherwise, just log a warning.\n+   */\n+  private def reportDataLoss(message: String): Unit = {\n+    if (failOnDataLoss) {\n+      throw new IllegalStateException(message + s\". $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE\")\n+    } else {\n+      logWarning(message + s\". $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE\")\n+    }\n+  }\n+}\n+\n+case class ContinuousKafkaReadTask(\n+    topicPartition: TopicPartition,\n+    start: Long,\n+    kafkaParams: java.util.Map[String, Object],\n+    pollTimeoutMs: Long,\n+    failOnDataLoss: Boolean)\n+  extends ReadTask[UnsafeRow] {\n+  override def createDataReader(): ContinuousKafkaDataReader = {\n+    new ContinuousKafkaDataReader(topicPartition, start, kafkaParams, pollTimeoutMs, failOnDataLoss)\n+  }\n+}\n+\n+class ContinuousKafkaDataReader(\n+    topicPartition: TopicPartition,\n+    start: Long,\n+    kafkaParams: java.util.Map[String, Object],\n+    pollTimeoutMs: Long,\n+    failOnDataLoss: Boolean)\n+  extends ContinuousDataReader[UnsafeRow] {\n+  private val topic = topicPartition.topic\n+  private val kafkaPartition = topicPartition.partition\n+  private val consumer = CachedKafkaConsumer.createUncached(topic, kafkaPartition, kafkaParams)\n+\n+  private val closed = new AtomicBoolean(false)\n+\n+  private var nextKafkaOffset = start match {\n+    case s if s >= 0 => s\n+    case KafkaOffsetRangeLimit.EARLIEST => consumer.getAvailableOffsetRange().earliest"
  }],
  "prId": 20096
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "question: is it guaranteed that the engine will call close() no matter what happens to the task using the ContinuousReader?",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T02:07:21Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.errors.WakeupException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{DataFrame, Row, SparkSession, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeRowWriter}\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.streaming.{HDFSMetadataLog, SerializedOffset}\n+import org.apache.spark.sql.kafka010.KafkaSource.{INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE, INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE, VERSION}\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.sources.v2.streaming.reader.{ContinuousDataReader, ContinuousReader, Offset, PartitionOffset}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+class ContinuousKafkaReader(\n+    kafkaReader: KafkaOffsetReader,\n+    executorKafkaParams: java.util.Map[String, Object],\n+    sourceOptions: Map[String, String],\n+    metadataPath: String,\n+    initialOffsets: KafkaOffsetRangeLimit,\n+    failOnDataLoss: Boolean)\n+  extends ContinuousReader with SupportsScanUnsafeRow with Logging {\n+\n+  override def mergeOffsets(offsets: Array[PartitionOffset]): Offset = {\n+    val mergedMap = offsets.map {\n+      case KafkaSourcePartitionOffset(p, o) => Map(p -> o)\n+    }.reduce(_ ++ _)\n+    KafkaSourceOffset(mergedMap)\n+  }\n+\n+  private lazy val session = SparkSession.getActiveSession.get\n+  private lazy val sc = session.sparkContext\n+\n+  private lazy val pollTimeoutMs = sourceOptions.getOrElse(\n+    \"kafkaConsumer.pollTimeoutMs\",\n+    sc.conf.getTimeAsMs(\"spark.network.timeout\", \"120s\").toString\n+  ).toLong\n+\n+  private val maxOffsetsPerTrigger =\n+    sourceOptions.get(\"maxOffsetsPerTrigger\").map(_.toLong)\n+\n+  /**\n+   * Lazily initialize `initialPartitionOffsets` to make sure that `KafkaConsumer.poll` is only\n+   * called in StreamExecutionThread. Otherwise, interrupting a thread while running\n+   * `KafkaConsumer.poll` may hang forever (KAFKA-1894).\n+   */\n+  private lazy val initialPartitionOffsets = {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets.partitionToOffsets\n+  }\n+\n+  private def fetchAndVerify(specificOffsets: Map[TopicPartition, Long]) = {\n+    val result = kafkaReader.fetchSpecificOffsets(specificOffsets)\n+    specificOffsets.foreach {\n+      case (tp, off) if off != KafkaOffsetRangeLimit.LATEST &&\n+        off != KafkaOffsetRangeLimit.EARLIEST =>\n+        if (result(tp) != off) {\n+          reportDataLoss(\n+            s\"startingOffsets for $tp was $off but consumer reset to ${result(tp)}\")\n+        }\n+      case _ =>\n+      // no real way to check that beginning or end is reasonable\n+    }\n+    KafkaSourceOffset(result)\n+  }\n+\n+  // Initialized when creating read tasks. If this diverges from the partitions at the latest\n+  // offsets, we need to reconfigure.\n+  // Exposed outside this object only for unit tests.\n+  private[sql] var knownPartitions: Set[TopicPartition] = _\n+\n+  override def readSchema: StructType = KafkaOffsetReader.kafkaSchema\n+\n+  private var offset: Offset = _\n+  override def setOffset(start: java.util.Optional[Offset]): Unit = {\n+    offset = start.orElse {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets\n+    }\n+  }\n+\n+  override def getStartOffset(): Offset = offset\n+\n+  override def deserializeOffset(json: String): Offset = {\n+    KafkaSourceOffset(JsonUtils.partitionOffsets(json))\n+  }\n+\n+  override def createUnsafeRowReadTasks(): java.util.List[ReadTask[UnsafeRow]] = {\n+    import scala.collection.JavaConverters._\n+\n+    val oldStartOffsets = KafkaSourceOffset.getPartitionOffsets(offset)\n+\n+    val newPartitions =\n+      kafkaReader.fetchLatestOffsets().keySet.diff(oldStartOffsets.keySet)\n+    val newPartitionOffsets = kafkaReader.fetchEarliestOffsets(newPartitions.toSeq)\n+    val startOffsets = oldStartOffsets ++ newPartitionOffsets\n+\n+    knownPartitions = startOffsets.keySet\n+\n+    startOffsets.toSeq.map {\n+      case (topicPartition, start) =>\n+        ContinuousKafkaReadTask(\n+          topicPartition, start, executorKafkaParams, pollTimeoutMs, failOnDataLoss)\n+          .asInstanceOf[ReadTask[UnsafeRow]]\n+    }.asJava\n+  }\n+\n+  /** Stop this source and free any resources it has allocated. */\n+  def stop(): Unit = synchronized {\n+    kafkaReader.close()\n+  }\n+\n+  override def commit(end: Offset): Unit = {}\n+\n+  override def needsReconfiguration(): Boolean = {\n+    knownPartitions != null && kafkaReader.fetchLatestOffsets().keySet != knownPartitions\n+  }\n+\n+  override def toString(): String = s\"KafkaSource[$kafkaReader]\"\n+\n+  /**\n+   * If `failOnDataLoss` is true, this method will throw an `IllegalStateException`.\n+   * Otherwise, just log a warning.\n+   */\n+  private def reportDataLoss(message: String): Unit = {\n+    if (failOnDataLoss) {\n+      throw new IllegalStateException(message + s\". $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE\")\n+    } else {\n+      logWarning(message + s\". $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE\")\n+    }\n+  }\n+}\n+\n+case class ContinuousKafkaReadTask(\n+    topicPartition: TopicPartition,\n+    start: Long,\n+    kafkaParams: java.util.Map[String, Object],\n+    pollTimeoutMs: Long,\n+    failOnDataLoss: Boolean)\n+  extends ReadTask[UnsafeRow] {\n+  override def createDataReader(): ContinuousKafkaDataReader = {\n+    new ContinuousKafkaDataReader(topicPartition, start, kafkaParams, pollTimeoutMs, failOnDataLoss)\n+  }\n+}\n+\n+class ContinuousKafkaDataReader(\n+    topicPartition: TopicPartition,\n+    start: Long,\n+    kafkaParams: java.util.Map[String, Object],\n+    pollTimeoutMs: Long,\n+    failOnDataLoss: Boolean)\n+  extends ContinuousDataReader[UnsafeRow] {\n+  private val topic = topicPartition.topic\n+  private val kafkaPartition = topicPartition.partition\n+  private val consumer = CachedKafkaConsumer.createUncached(topic, kafkaPartition, kafkaParams)\n+\n+  private val closed = new AtomicBoolean(false)\n+\n+  private var nextKafkaOffset = start match {\n+    case s if s >= 0 => s\n+    case KafkaOffsetRangeLimit.EARLIEST => consumer.getAvailableOffsetRange().earliest\n+    case _ => throw new IllegalArgumentException(s\"Invalid start Kafka offset $start.\")\n+  }\n+  private var currentRecord: ConsumerRecord[Array[Byte], Array[Byte]] = _\n+\n+  override def next(): Boolean = {\n+    var r: ConsumerRecord[Array[Byte], Array[Byte]] = null\n+    while (r == null) {\n+      r = consumer.get(\n+        nextKafkaOffset,\n+        untilOffset = Long.MaxValue,\n+        pollTimeoutMs = Long.MaxValue,\n+        failOnDataLoss)\n+    }\n+    nextKafkaOffset = r.offset + 1\n+    currentRecord = r\n+    true\n+  }\n+\n+  val sharedRow = new UnsafeRow(7)\n+  val bufferHolder = new BufferHolder(sharedRow)\n+  val rowWriter = new UnsafeRowWriter(bufferHolder, 7)\n+\n+  override def get(): UnsafeRow = {\n+    bufferHolder.reset()\n+\n+    if (currentRecord.key == null) {\n+      rowWriter.isNullAt(0)\n+    } else {\n+      rowWriter.write(0, currentRecord.key)\n+    }\n+    rowWriter.write(1, currentRecord.value)\n+    rowWriter.write(2, UTF8String.fromString(currentRecord.topic))\n+    rowWriter.write(3, currentRecord.partition)\n+    rowWriter.write(4, currentRecord.offset)\n+    rowWriter.write(5,\n+      DateTimeUtils.fromJavaTimestamp(new java.sql.Timestamp(currentRecord.timestamp)))\n+    rowWriter.write(6, currentRecord.timestampType.id)\n+    sharedRow.setTotalSize(bufferHolder.totalSize)\n+    sharedRow\n+  }\n+\n+  override def getOffset(): KafkaSourcePartitionOffset = {\n+    KafkaSourcePartitionOffset(topicPartition, nextKafkaOffset)\n+  }\n+\n+  override def close(): Unit = {"
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "close() is called in a terminal finally block for the thread polling the reader, which is ended at task completion.",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-05T01:35:21Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.errors.WakeupException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{DataFrame, Row, SparkSession, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeRowWriter}\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.streaming.{HDFSMetadataLog, SerializedOffset}\n+import org.apache.spark.sql.kafka010.KafkaSource.{INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE, INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE, VERSION}\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.sources.v2.streaming.reader.{ContinuousDataReader, ContinuousReader, Offset, PartitionOffset}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+class ContinuousKafkaReader(\n+    kafkaReader: KafkaOffsetReader,\n+    executorKafkaParams: java.util.Map[String, Object],\n+    sourceOptions: Map[String, String],\n+    metadataPath: String,\n+    initialOffsets: KafkaOffsetRangeLimit,\n+    failOnDataLoss: Boolean)\n+  extends ContinuousReader with SupportsScanUnsafeRow with Logging {\n+\n+  override def mergeOffsets(offsets: Array[PartitionOffset]): Offset = {\n+    val mergedMap = offsets.map {\n+      case KafkaSourcePartitionOffset(p, o) => Map(p -> o)\n+    }.reduce(_ ++ _)\n+    KafkaSourceOffset(mergedMap)\n+  }\n+\n+  private lazy val session = SparkSession.getActiveSession.get\n+  private lazy val sc = session.sparkContext\n+\n+  private lazy val pollTimeoutMs = sourceOptions.getOrElse(\n+    \"kafkaConsumer.pollTimeoutMs\",\n+    sc.conf.getTimeAsMs(\"spark.network.timeout\", \"120s\").toString\n+  ).toLong\n+\n+  private val maxOffsetsPerTrigger =\n+    sourceOptions.get(\"maxOffsetsPerTrigger\").map(_.toLong)\n+\n+  /**\n+   * Lazily initialize `initialPartitionOffsets` to make sure that `KafkaConsumer.poll` is only\n+   * called in StreamExecutionThread. Otherwise, interrupting a thread while running\n+   * `KafkaConsumer.poll` may hang forever (KAFKA-1894).\n+   */\n+  private lazy val initialPartitionOffsets = {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets.partitionToOffsets\n+  }\n+\n+  private def fetchAndVerify(specificOffsets: Map[TopicPartition, Long]) = {\n+    val result = kafkaReader.fetchSpecificOffsets(specificOffsets)\n+    specificOffsets.foreach {\n+      case (tp, off) if off != KafkaOffsetRangeLimit.LATEST &&\n+        off != KafkaOffsetRangeLimit.EARLIEST =>\n+        if (result(tp) != off) {\n+          reportDataLoss(\n+            s\"startingOffsets for $tp was $off but consumer reset to ${result(tp)}\")\n+        }\n+      case _ =>\n+      // no real way to check that beginning or end is reasonable\n+    }\n+    KafkaSourceOffset(result)\n+  }\n+\n+  // Initialized when creating read tasks. If this diverges from the partitions at the latest\n+  // offsets, we need to reconfigure.\n+  // Exposed outside this object only for unit tests.\n+  private[sql] var knownPartitions: Set[TopicPartition] = _\n+\n+  override def readSchema: StructType = KafkaOffsetReader.kafkaSchema\n+\n+  private var offset: Offset = _\n+  override def setOffset(start: java.util.Optional[Offset]): Unit = {\n+    offset = start.orElse {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets\n+    }\n+  }\n+\n+  override def getStartOffset(): Offset = offset\n+\n+  override def deserializeOffset(json: String): Offset = {\n+    KafkaSourceOffset(JsonUtils.partitionOffsets(json))\n+  }\n+\n+  override def createUnsafeRowReadTasks(): java.util.List[ReadTask[UnsafeRow]] = {\n+    import scala.collection.JavaConverters._\n+\n+    val oldStartOffsets = KafkaSourceOffset.getPartitionOffsets(offset)\n+\n+    val newPartitions =\n+      kafkaReader.fetchLatestOffsets().keySet.diff(oldStartOffsets.keySet)\n+    val newPartitionOffsets = kafkaReader.fetchEarliestOffsets(newPartitions.toSeq)\n+    val startOffsets = oldStartOffsets ++ newPartitionOffsets\n+\n+    knownPartitions = startOffsets.keySet\n+\n+    startOffsets.toSeq.map {\n+      case (topicPartition, start) =>\n+        ContinuousKafkaReadTask(\n+          topicPartition, start, executorKafkaParams, pollTimeoutMs, failOnDataLoss)\n+          .asInstanceOf[ReadTask[UnsafeRow]]\n+    }.asJava\n+  }\n+\n+  /** Stop this source and free any resources it has allocated. */\n+  def stop(): Unit = synchronized {\n+    kafkaReader.close()\n+  }\n+\n+  override def commit(end: Offset): Unit = {}\n+\n+  override def needsReconfiguration(): Boolean = {\n+    knownPartitions != null && kafkaReader.fetchLatestOffsets().keySet != knownPartitions\n+  }\n+\n+  override def toString(): String = s\"KafkaSource[$kafkaReader]\"\n+\n+  /**\n+   * If `failOnDataLoss` is true, this method will throw an `IllegalStateException`.\n+   * Otherwise, just log a warning.\n+   */\n+  private def reportDataLoss(message: String): Unit = {\n+    if (failOnDataLoss) {\n+      throw new IllegalStateException(message + s\". $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE\")\n+    } else {\n+      logWarning(message + s\". $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE\")\n+    }\n+  }\n+}\n+\n+case class ContinuousKafkaReadTask(\n+    topicPartition: TopicPartition,\n+    start: Long,\n+    kafkaParams: java.util.Map[String, Object],\n+    pollTimeoutMs: Long,\n+    failOnDataLoss: Boolean)\n+  extends ReadTask[UnsafeRow] {\n+  override def createDataReader(): ContinuousKafkaDataReader = {\n+    new ContinuousKafkaDataReader(topicPartition, start, kafkaParams, pollTimeoutMs, failOnDataLoss)\n+  }\n+}\n+\n+class ContinuousKafkaDataReader(\n+    topicPartition: TopicPartition,\n+    start: Long,\n+    kafkaParams: java.util.Map[String, Object],\n+    pollTimeoutMs: Long,\n+    failOnDataLoss: Boolean)\n+  extends ContinuousDataReader[UnsafeRow] {\n+  private val topic = topicPartition.topic\n+  private val kafkaPartition = topicPartition.partition\n+  private val consumer = CachedKafkaConsumer.createUncached(topic, kafkaPartition, kafkaParams)\n+\n+  private val closed = new AtomicBoolean(false)\n+\n+  private var nextKafkaOffset = start match {\n+    case s if s >= 0 => s\n+    case KafkaOffsetRangeLimit.EARLIEST => consumer.getAvailableOffsetRange().earliest\n+    case _ => throw new IllegalArgumentException(s\"Invalid start Kafka offset $start.\")\n+  }\n+  private var currentRecord: ConsumerRecord[Array[Byte], Array[Byte]] = _\n+\n+  override def next(): Boolean = {\n+    var r: ConsumerRecord[Array[Byte], Array[Byte]] = null\n+    while (r == null) {\n+      r = consumer.get(\n+        nextKafkaOffset,\n+        untilOffset = Long.MaxValue,\n+        pollTimeoutMs = Long.MaxValue,\n+        failOnDataLoss)\n+    }\n+    nextKafkaOffset = r.offset + 1\n+    currentRecord = r\n+    true\n+  }\n+\n+  val sharedRow = new UnsafeRow(7)\n+  val bufferHolder = new BufferHolder(sharedRow)\n+  val rowWriter = new UnsafeRowWriter(bufferHolder, 7)\n+\n+  override def get(): UnsafeRow = {\n+    bufferHolder.reset()\n+\n+    if (currentRecord.key == null) {\n+      rowWriter.isNullAt(0)\n+    } else {\n+      rowWriter.write(0, currentRecord.key)\n+    }\n+    rowWriter.write(1, currentRecord.value)\n+    rowWriter.write(2, UTF8String.fromString(currentRecord.topic))\n+    rowWriter.write(3, currentRecord.partition)\n+    rowWriter.write(4, currentRecord.offset)\n+    rowWriter.write(5,\n+      DateTimeUtils.fromJavaTimestamp(new java.sql.Timestamp(currentRecord.timestamp)))\n+    rowWriter.write(6, currentRecord.timestampType.id)\n+    sharedRow.setTotalSize(bufferHolder.totalSize)\n+    sharedRow\n+  }\n+\n+  override def getOffset(): KafkaSourcePartitionOffset = {\n+    KafkaSourcePartitionOffset(topicPartition, nextKafkaOffset)\n+  }\n+\n+  override def close(): Unit = {"
  }],
  "prId": 20096
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "why is this long value. see what is used in the KafkaSourceRDD of the legacy source\r\n  ",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T02:09:26Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.errors.WakeupException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{DataFrame, Row, SparkSession, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeRowWriter}\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.streaming.{HDFSMetadataLog, SerializedOffset}\n+import org.apache.spark.sql.kafka010.KafkaSource.{INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE, INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE, VERSION}\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.sources.v2.streaming.reader.{ContinuousDataReader, ContinuousReader, Offset, PartitionOffset}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+class ContinuousKafkaReader(\n+    kafkaReader: KafkaOffsetReader,\n+    executorKafkaParams: java.util.Map[String, Object],\n+    sourceOptions: Map[String, String],\n+    metadataPath: String,\n+    initialOffsets: KafkaOffsetRangeLimit,\n+    failOnDataLoss: Boolean)\n+  extends ContinuousReader with SupportsScanUnsafeRow with Logging {\n+\n+  override def mergeOffsets(offsets: Array[PartitionOffset]): Offset = {\n+    val mergedMap = offsets.map {\n+      case KafkaSourcePartitionOffset(p, o) => Map(p -> o)\n+    }.reduce(_ ++ _)\n+    KafkaSourceOffset(mergedMap)\n+  }\n+\n+  private lazy val session = SparkSession.getActiveSession.get\n+  private lazy val sc = session.sparkContext\n+\n+  private lazy val pollTimeoutMs = sourceOptions.getOrElse(\n+    \"kafkaConsumer.pollTimeoutMs\",\n+    sc.conf.getTimeAsMs(\"spark.network.timeout\", \"120s\").toString\n+  ).toLong\n+\n+  private val maxOffsetsPerTrigger =\n+    sourceOptions.get(\"maxOffsetsPerTrigger\").map(_.toLong)\n+\n+  /**\n+   * Lazily initialize `initialPartitionOffsets` to make sure that `KafkaConsumer.poll` is only\n+   * called in StreamExecutionThread. Otherwise, interrupting a thread while running\n+   * `KafkaConsumer.poll` may hang forever (KAFKA-1894).\n+   */\n+  private lazy val initialPartitionOffsets = {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets.partitionToOffsets\n+  }\n+\n+  private def fetchAndVerify(specificOffsets: Map[TopicPartition, Long]) = {\n+    val result = kafkaReader.fetchSpecificOffsets(specificOffsets)\n+    specificOffsets.foreach {\n+      case (tp, off) if off != KafkaOffsetRangeLimit.LATEST &&\n+        off != KafkaOffsetRangeLimit.EARLIEST =>\n+        if (result(tp) != off) {\n+          reportDataLoss(\n+            s\"startingOffsets for $tp was $off but consumer reset to ${result(tp)}\")\n+        }\n+      case _ =>\n+      // no real way to check that beginning or end is reasonable\n+    }\n+    KafkaSourceOffset(result)\n+  }\n+\n+  // Initialized when creating read tasks. If this diverges from the partitions at the latest\n+  // offsets, we need to reconfigure.\n+  // Exposed outside this object only for unit tests.\n+  private[sql] var knownPartitions: Set[TopicPartition] = _\n+\n+  override def readSchema: StructType = KafkaOffsetReader.kafkaSchema\n+\n+  private var offset: Offset = _\n+  override def setOffset(start: java.util.Optional[Offset]): Unit = {\n+    offset = start.orElse {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets\n+    }\n+  }\n+\n+  override def getStartOffset(): Offset = offset\n+\n+  override def deserializeOffset(json: String): Offset = {\n+    KafkaSourceOffset(JsonUtils.partitionOffsets(json))\n+  }\n+\n+  override def createUnsafeRowReadTasks(): java.util.List[ReadTask[UnsafeRow]] = {\n+    import scala.collection.JavaConverters._\n+\n+    val oldStartOffsets = KafkaSourceOffset.getPartitionOffsets(offset)\n+\n+    val newPartitions =\n+      kafkaReader.fetchLatestOffsets().keySet.diff(oldStartOffsets.keySet)\n+    val newPartitionOffsets = kafkaReader.fetchEarliestOffsets(newPartitions.toSeq)\n+    val startOffsets = oldStartOffsets ++ newPartitionOffsets\n+\n+    knownPartitions = startOffsets.keySet\n+\n+    startOffsets.toSeq.map {\n+      case (topicPartition, start) =>\n+        ContinuousKafkaReadTask(\n+          topicPartition, start, executorKafkaParams, pollTimeoutMs, failOnDataLoss)\n+          .asInstanceOf[ReadTask[UnsafeRow]]\n+    }.asJava\n+  }\n+\n+  /** Stop this source and free any resources it has allocated. */\n+  def stop(): Unit = synchronized {\n+    kafkaReader.close()\n+  }\n+\n+  override def commit(end: Offset): Unit = {}\n+\n+  override def needsReconfiguration(): Boolean = {\n+    knownPartitions != null && kafkaReader.fetchLatestOffsets().keySet != knownPartitions\n+  }\n+\n+  override def toString(): String = s\"KafkaSource[$kafkaReader]\"\n+\n+  /**\n+   * If `failOnDataLoss` is true, this method will throw an `IllegalStateException`.\n+   * Otherwise, just log a warning.\n+   */\n+  private def reportDataLoss(message: String): Unit = {\n+    if (failOnDataLoss) {\n+      throw new IllegalStateException(message + s\". $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE\")\n+    } else {\n+      logWarning(message + s\". $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE\")\n+    }\n+  }\n+}\n+\n+case class ContinuousKafkaReadTask(\n+    topicPartition: TopicPartition,\n+    start: Long,\n+    kafkaParams: java.util.Map[String, Object],\n+    pollTimeoutMs: Long,\n+    failOnDataLoss: Boolean)\n+  extends ReadTask[UnsafeRow] {\n+  override def createDataReader(): ContinuousKafkaDataReader = {\n+    new ContinuousKafkaDataReader(topicPartition, start, kafkaParams, pollTimeoutMs, failOnDataLoss)\n+  }\n+}\n+\n+class ContinuousKafkaDataReader(\n+    topicPartition: TopicPartition,\n+    start: Long,\n+    kafkaParams: java.util.Map[String, Object],\n+    pollTimeoutMs: Long,\n+    failOnDataLoss: Boolean)\n+  extends ContinuousDataReader[UnsafeRow] {\n+  private val topic = topicPartition.topic\n+  private val kafkaPartition = topicPartition.partition\n+  private val consumer = CachedKafkaConsumer.createUncached(topic, kafkaPartition, kafkaParams)\n+\n+  private val closed = new AtomicBoolean(false)\n+\n+  private var nextKafkaOffset = start match {\n+    case s if s >= 0 => s\n+    case KafkaOffsetRangeLimit.EARLIEST => consumer.getAvailableOffsetRange().earliest\n+    case _ => throw new IllegalArgumentException(s\"Invalid start Kafka offset $start.\")\n+  }\n+  private var currentRecord: ConsumerRecord[Array[Byte], Array[Byte]] = _\n+\n+  override def next(): Boolean = {\n+    var r: ConsumerRecord[Array[Byte], Array[Byte]] = null\n+    while (r == null) {\n+      r = consumer.get(\n+        nextKafkaOffset,\n+        untilOffset = Long.MaxValue,\n+        pollTimeoutMs = Long.MaxValue,"
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "untilOffset is MaxValue because the read isn't expected to end. pollTimeoutMs is MaxValue because it should wait forever for a new value to show up.",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-05T01:33:57Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.errors.WakeupException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{DataFrame, Row, SparkSession, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeRowWriter}\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.streaming.{HDFSMetadataLog, SerializedOffset}\n+import org.apache.spark.sql.kafka010.KafkaSource.{INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE, INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE, VERSION}\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.sources.v2.streaming.reader.{ContinuousDataReader, ContinuousReader, Offset, PartitionOffset}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+class ContinuousKafkaReader(\n+    kafkaReader: KafkaOffsetReader,\n+    executorKafkaParams: java.util.Map[String, Object],\n+    sourceOptions: Map[String, String],\n+    metadataPath: String,\n+    initialOffsets: KafkaOffsetRangeLimit,\n+    failOnDataLoss: Boolean)\n+  extends ContinuousReader with SupportsScanUnsafeRow with Logging {\n+\n+  override def mergeOffsets(offsets: Array[PartitionOffset]): Offset = {\n+    val mergedMap = offsets.map {\n+      case KafkaSourcePartitionOffset(p, o) => Map(p -> o)\n+    }.reduce(_ ++ _)\n+    KafkaSourceOffset(mergedMap)\n+  }\n+\n+  private lazy val session = SparkSession.getActiveSession.get\n+  private lazy val sc = session.sparkContext\n+\n+  private lazy val pollTimeoutMs = sourceOptions.getOrElse(\n+    \"kafkaConsumer.pollTimeoutMs\",\n+    sc.conf.getTimeAsMs(\"spark.network.timeout\", \"120s\").toString\n+  ).toLong\n+\n+  private val maxOffsetsPerTrigger =\n+    sourceOptions.get(\"maxOffsetsPerTrigger\").map(_.toLong)\n+\n+  /**\n+   * Lazily initialize `initialPartitionOffsets` to make sure that `KafkaConsumer.poll` is only\n+   * called in StreamExecutionThread. Otherwise, interrupting a thread while running\n+   * `KafkaConsumer.poll` may hang forever (KAFKA-1894).\n+   */\n+  private lazy val initialPartitionOffsets = {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets.partitionToOffsets\n+  }\n+\n+  private def fetchAndVerify(specificOffsets: Map[TopicPartition, Long]) = {\n+    val result = kafkaReader.fetchSpecificOffsets(specificOffsets)\n+    specificOffsets.foreach {\n+      case (tp, off) if off != KafkaOffsetRangeLimit.LATEST &&\n+        off != KafkaOffsetRangeLimit.EARLIEST =>\n+        if (result(tp) != off) {\n+          reportDataLoss(\n+            s\"startingOffsets for $tp was $off but consumer reset to ${result(tp)}\")\n+        }\n+      case _ =>\n+      // no real way to check that beginning or end is reasonable\n+    }\n+    KafkaSourceOffset(result)\n+  }\n+\n+  // Initialized when creating read tasks. If this diverges from the partitions at the latest\n+  // offsets, we need to reconfigure.\n+  // Exposed outside this object only for unit tests.\n+  private[sql] var knownPartitions: Set[TopicPartition] = _\n+\n+  override def readSchema: StructType = KafkaOffsetReader.kafkaSchema\n+\n+  private var offset: Offset = _\n+  override def setOffset(start: java.util.Optional[Offset]): Unit = {\n+    offset = start.orElse {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets\n+    }\n+  }\n+\n+  override def getStartOffset(): Offset = offset\n+\n+  override def deserializeOffset(json: String): Offset = {\n+    KafkaSourceOffset(JsonUtils.partitionOffsets(json))\n+  }\n+\n+  override def createUnsafeRowReadTasks(): java.util.List[ReadTask[UnsafeRow]] = {\n+    import scala.collection.JavaConverters._\n+\n+    val oldStartOffsets = KafkaSourceOffset.getPartitionOffsets(offset)\n+\n+    val newPartitions =\n+      kafkaReader.fetchLatestOffsets().keySet.diff(oldStartOffsets.keySet)\n+    val newPartitionOffsets = kafkaReader.fetchEarliestOffsets(newPartitions.toSeq)\n+    val startOffsets = oldStartOffsets ++ newPartitionOffsets\n+\n+    knownPartitions = startOffsets.keySet\n+\n+    startOffsets.toSeq.map {\n+      case (topicPartition, start) =>\n+        ContinuousKafkaReadTask(\n+          topicPartition, start, executorKafkaParams, pollTimeoutMs, failOnDataLoss)\n+          .asInstanceOf[ReadTask[UnsafeRow]]\n+    }.asJava\n+  }\n+\n+  /** Stop this source and free any resources it has allocated. */\n+  def stop(): Unit = synchronized {\n+    kafkaReader.close()\n+  }\n+\n+  override def commit(end: Offset): Unit = {}\n+\n+  override def needsReconfiguration(): Boolean = {\n+    knownPartitions != null && kafkaReader.fetchLatestOffsets().keySet != knownPartitions\n+  }\n+\n+  override def toString(): String = s\"KafkaSource[$kafkaReader]\"\n+\n+  /**\n+   * If `failOnDataLoss` is true, this method will throw an `IllegalStateException`.\n+   * Otherwise, just log a warning.\n+   */\n+  private def reportDataLoss(message: String): Unit = {\n+    if (failOnDataLoss) {\n+      throw new IllegalStateException(message + s\". $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE\")\n+    } else {\n+      logWarning(message + s\". $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE\")\n+    }\n+  }\n+}\n+\n+case class ContinuousKafkaReadTask(\n+    topicPartition: TopicPartition,\n+    start: Long,\n+    kafkaParams: java.util.Map[String, Object],\n+    pollTimeoutMs: Long,\n+    failOnDataLoss: Boolean)\n+  extends ReadTask[UnsafeRow] {\n+  override def createDataReader(): ContinuousKafkaDataReader = {\n+    new ContinuousKafkaDataReader(topicPartition, start, kafkaParams, pollTimeoutMs, failOnDataLoss)\n+  }\n+}\n+\n+class ContinuousKafkaDataReader(\n+    topicPartition: TopicPartition,\n+    start: Long,\n+    kafkaParams: java.util.Map[String, Object],\n+    pollTimeoutMs: Long,\n+    failOnDataLoss: Boolean)\n+  extends ContinuousDataReader[UnsafeRow] {\n+  private val topic = topicPartition.topic\n+  private val kafkaPartition = topicPartition.partition\n+  private val consumer = CachedKafkaConsumer.createUncached(topic, kafkaPartition, kafkaParams)\n+\n+  private val closed = new AtomicBoolean(false)\n+\n+  private var nextKafkaOffset = start match {\n+    case s if s >= 0 => s\n+    case KafkaOffsetRangeLimit.EARLIEST => consumer.getAvailableOffsetRange().earliest\n+    case _ => throw new IllegalArgumentException(s\"Invalid start Kafka offset $start.\")\n+  }\n+  private var currentRecord: ConsumerRecord[Array[Byte], Array[Byte]] = _\n+\n+  override def next(): Boolean = {\n+    var r: ConsumerRecord[Array[Byte], Array[Byte]] = null\n+    while (r == null) {\n+      r = consumer.get(\n+        nextKafkaOffset,\n+        untilOffset = Long.MaxValue,\n+        pollTimeoutMs = Long.MaxValue,"
  }],
  "prId": 20096
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: can we move this up to consolidate all the vals.",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T02:21:55Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.errors.WakeupException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{DataFrame, Row, SparkSession, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeRowWriter}\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.streaming.{HDFSMetadataLog, SerializedOffset}\n+import org.apache.spark.sql.kafka010.KafkaSource.{INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE, INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE, VERSION}\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.sources.v2.streaming.reader.{ContinuousDataReader, ContinuousReader, Offset, PartitionOffset}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+class ContinuousKafkaReader(\n+    kafkaReader: KafkaOffsetReader,\n+    executorKafkaParams: java.util.Map[String, Object],\n+    sourceOptions: Map[String, String],\n+    metadataPath: String,\n+    initialOffsets: KafkaOffsetRangeLimit,\n+    failOnDataLoss: Boolean)\n+  extends ContinuousReader with SupportsScanUnsafeRow with Logging {\n+\n+  override def mergeOffsets(offsets: Array[PartitionOffset]): Offset = {\n+    val mergedMap = offsets.map {\n+      case KafkaSourcePartitionOffset(p, o) => Map(p -> o)\n+    }.reduce(_ ++ _)\n+    KafkaSourceOffset(mergedMap)\n+  }\n+\n+  private lazy val session = SparkSession.getActiveSession.get\n+  private lazy val sc = session.sparkContext\n+\n+  private lazy val pollTimeoutMs = sourceOptions.getOrElse(\n+    \"kafkaConsumer.pollTimeoutMs\",\n+    sc.conf.getTimeAsMs(\"spark.network.timeout\", \"120s\").toString\n+  ).toLong\n+\n+  private val maxOffsetsPerTrigger =\n+    sourceOptions.get(\"maxOffsetsPerTrigger\").map(_.toLong)\n+\n+  /**\n+   * Lazily initialize `initialPartitionOffsets` to make sure that `KafkaConsumer.poll` is only\n+   * called in StreamExecutionThread. Otherwise, interrupting a thread while running\n+   * `KafkaConsumer.poll` may hang forever (KAFKA-1894).\n+   */\n+  private lazy val initialPartitionOffsets = {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets.partitionToOffsets\n+  }\n+\n+  private def fetchAndVerify(specificOffsets: Map[TopicPartition, Long]) = {\n+    val result = kafkaReader.fetchSpecificOffsets(specificOffsets)\n+    specificOffsets.foreach {\n+      case (tp, off) if off != KafkaOffsetRangeLimit.LATEST &&\n+        off != KafkaOffsetRangeLimit.EARLIEST =>\n+        if (result(tp) != off) {\n+          reportDataLoss(\n+            s\"startingOffsets for $tp was $off but consumer reset to ${result(tp)}\")\n+        }\n+      case _ =>\n+      // no real way to check that beginning or end is reasonable\n+    }\n+    KafkaSourceOffset(result)\n+  }\n+\n+  // Initialized when creating read tasks. If this diverges from the partitions at the latest\n+  // offsets, we need to reconfigure.\n+  // Exposed outside this object only for unit tests.\n+  private[sql] var knownPartitions: Set[TopicPartition] = _\n+\n+  override def readSchema: StructType = KafkaOffsetReader.kafkaSchema\n+\n+  private var offset: Offset = _\n+  override def setOffset(start: java.util.Optional[Offset]): Unit = {\n+    offset = start.orElse {\n+      val offsets = initialOffsets match {\n+        case EarliestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())\n+        case LatestOffsetRangeLimit => KafkaSourceOffset(kafkaReader.fetchLatestOffsets())\n+        case SpecificOffsetRangeLimit(p) => fetchAndVerify(p)\n+      }\n+      logInfo(s\"Initial offsets: $offsets\")\n+      offsets\n+    }\n+  }\n+\n+  override def getStartOffset(): Offset = offset\n+\n+  override def deserializeOffset(json: String): Offset = {\n+    KafkaSourceOffset(JsonUtils.partitionOffsets(json))\n+  }\n+\n+  override def createUnsafeRowReadTasks(): java.util.List[ReadTask[UnsafeRow]] = {\n+    import scala.collection.JavaConverters._\n+\n+    val oldStartOffsets = KafkaSourceOffset.getPartitionOffsets(offset)\n+\n+    val newPartitions =\n+      kafkaReader.fetchLatestOffsets().keySet.diff(oldStartOffsets.keySet)\n+    val newPartitionOffsets = kafkaReader.fetchEarliestOffsets(newPartitions.toSeq)\n+    val startOffsets = oldStartOffsets ++ newPartitionOffsets\n+\n+    knownPartitions = startOffsets.keySet\n+\n+    startOffsets.toSeq.map {\n+      case (topicPartition, start) =>\n+        ContinuousKafkaReadTask(\n+          topicPartition, start, executorKafkaParams, pollTimeoutMs, failOnDataLoss)\n+          .asInstanceOf[ReadTask[UnsafeRow]]\n+    }.asJava\n+  }\n+\n+  /** Stop this source and free any resources it has allocated. */\n+  def stop(): Unit = synchronized {\n+    kafkaReader.close()\n+  }\n+\n+  override def commit(end: Offset): Unit = {}\n+\n+  override def needsReconfiguration(): Boolean = {\n+    knownPartitions != null && kafkaReader.fetchLatestOffsets().keySet != knownPartitions\n+  }\n+\n+  override def toString(): String = s\"KafkaSource[$kafkaReader]\"\n+\n+  /**\n+   * If `failOnDataLoss` is true, this method will throw an `IllegalStateException`.\n+   * Otherwise, just log a warning.\n+   */\n+  private def reportDataLoss(message: String): Unit = {\n+    if (failOnDataLoss) {\n+      throw new IllegalStateException(message + s\". $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE\")\n+    } else {\n+      logWarning(message + s\". $INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE\")\n+    }\n+  }\n+}\n+\n+case class ContinuousKafkaReadTask(\n+    topicPartition: TopicPartition,\n+    start: Long,\n+    kafkaParams: java.util.Map[String, Object],\n+    pollTimeoutMs: Long,\n+    failOnDataLoss: Boolean)\n+  extends ReadTask[UnsafeRow] {\n+  override def createDataReader(): ContinuousKafkaDataReader = {\n+    new ContinuousKafkaDataReader(topicPartition, start, kafkaParams, pollTimeoutMs, failOnDataLoss)\n+  }\n+}\n+\n+class ContinuousKafkaDataReader(\n+    topicPartition: TopicPartition,\n+    start: Long,\n+    kafkaParams: java.util.Map[String, Object],\n+    pollTimeoutMs: Long,\n+    failOnDataLoss: Boolean)\n+  extends ContinuousDataReader[UnsafeRow] {\n+  private val topic = topicPartition.topic\n+  private val kafkaPartition = topicPartition.partition\n+  private val consumer = CachedKafkaConsumer.createUncached(topic, kafkaPartition, kafkaParams)\n+\n+  private val closed = new AtomicBoolean(false)\n+\n+  private var nextKafkaOffset = start match {\n+    case s if s >= 0 => s\n+    case KafkaOffsetRangeLimit.EARLIEST => consumer.getAvailableOffsetRange().earliest\n+    case _ => throw new IllegalArgumentException(s\"Invalid start Kafka offset $start.\")\n+  }\n+  private var currentRecord: ConsumerRecord[Array[Byte], Array[Byte]] = _\n+\n+  override def next(): Boolean = {\n+    var r: ConsumerRecord[Array[Byte], Array[Byte]] = null\n+    while (r == null) {\n+      r = consumer.get(\n+        nextKafkaOffset,\n+        untilOffset = Long.MaxValue,\n+        pollTimeoutMs = Long.MaxValue,\n+        failOnDataLoss)\n+    }\n+    nextKafkaOffset = r.offset + 1\n+    currentRecord = r\n+    true\n+  }\n+\n+  val sharedRow = new UnsafeRow(7)\n+  val bufferHolder = new BufferHolder(sharedRow)\n+  val rowWriter = new UnsafeRowWriter(bufferHolder, 7)"
  }],
  "prId": 20096
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Would be nice to do some code organization in this class, e.g. all public methods and vals first, then all the private methods/vals/vars. OR all vals/vars first, then public methods, then private methods. \r\n  ",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T20:11:23Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+import java.util.concurrent.atomic.AtomicBoolean\n+\n+import org.apache.commons.io.IOUtils\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.errors.WakeupException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{DataFrame, Row, SparkSession, SQLContext}\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.catalyst.expressions.codegen.{BufferHolder, UnsafeRowWriter}\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils\n+import org.apache.spark.sql.execution.streaming.{HDFSMetadataLog, SerializedOffset}\n+import org.apache.spark.sql.kafka010.KafkaSource.{INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_FALSE, INSTRUCTION_FOR_FAIL_ON_DATA_LOSS_TRUE, VERSION}\n+import org.apache.spark.sql.sources.v2.reader._\n+import org.apache.spark.sql.sources.v2.streaming.reader.{ContinuousDataReader, ContinuousReader, Offset, PartitionOffset}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+class ContinuousKafkaReader(\n+    kafkaReader: KafkaOffsetReader,\n+    executorKafkaParams: java.util.Map[String, Object],\n+    sourceOptions: Map[String, String],\n+    metadataPath: String,\n+    initialOffsets: KafkaOffsetRangeLimit,\n+    failOnDataLoss: Boolean)\n+  extends ContinuousReader with SupportsScanUnsafeRow with Logging {\n+\n+  override def mergeOffsets(offsets: Array[PartitionOffset]): Offset = {\n+    val mergedMap = offsets.map {\n+      case KafkaSourcePartitionOffset(p, o) => Map(p -> o)\n+    }.reduce(_ ++ _)\n+    KafkaSourceOffset(mergedMap)\n+  }\n+\n+  private lazy val session = SparkSession.getActiveSession.get"
  }],
  "prId": 20096
}]