[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "why this change?",
    "commit": "ea804cfe840196519cc9444be9bedf03d10aa11a",
    "createdAt": "2018-08-22T00:57:52Z",
    "diffHunk": "@@ -337,6 +338,7 @@ private[kafka010] case class KafkaMicroBatchInputPartitionReader(\n       val record = consumer.get(nextOffset, rangeToRead.untilOffset, pollTimeoutMs, failOnDataLoss)\n       if (record != null) {\n         nextRow = converter.toUnsafeRow(record)\n+        nextOffset = record.offset + 1"
  }, {
    "author": {
      "login": "zsxwing"
    },
    "body": " We should update `nextOffset` to `record.offset + 1` rather that `nextOffset + 1`. Otherwise, it may return duplicated records when `failOnDataLoss` is `false`. I will submit another PR to push this fix to 2.3 as it's a correctness issue.\r\n\r\nIn addition, we should change `nextOffset` in the `next` method as the `get` method is designed to be called multiple times.",
    "commit": "ea804cfe840196519cc9444be9bedf03d10aa11a",
    "createdAt": "2018-08-22T17:07:38Z",
    "diffHunk": "@@ -337,6 +338,7 @@ private[kafka010] case class KafkaMicroBatchInputPartitionReader(\n       val record = consumer.get(nextOffset, rangeToRead.untilOffset, pollTimeoutMs, failOnDataLoss)\n       if (record != null) {\n         nextRow = converter.toUnsafeRow(record)\n+        nextOffset = record.offset + 1"
  }],
  "prId": 22042
}]