[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "shall we use `CaseInsensitiveStringMap` which is used a lot in Data Source V2?",
    "commit": "e0e8a8851f975311be990c5a4889d4c9870c3bea",
    "createdAt": "2019-07-25T12:50:07Z",
    "diffHunk": "@@ -22,12 +22,13 @@ import org.apache.kafka.common.TopicPartition\n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n import org.apache.spark.internal.config.Network.NETWORK_TIMEOUT\n+import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n import org.apache.spark.sql.sources.v2.reader.{Batch, InputPartition, PartitionReaderFactory}\n \n \n private[kafka010] class KafkaBatch(\n     strategy: ConsumerStrategy,\n-    sourceOptions: Map[String, String],\n+    sourceOptions: CaseInsensitiveMap[String],",
    "line": 11
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Just back from vacation and syncing. So originally I've considered to use `CaseInsensitiveStringMap` but not chosen because of the following:\r\n* `CaseInsensitiveStringMap` implements java `Map` and if we're passing that around then lot of useful features would disappear (`getOrElse`, `find`, ...) or we just have to add `asScala` to all the places where scala features are used. All in all converting forth and back was overkill from my point of view.\r\n* `KafkaBatch` is private API and though it can be different due to the considerations above.\r\n\r\nAs always if you think it worth then I can adapt...\r\n",
    "commit": "e0e8a8851f975311be990c5a4889d4c9870c3bea",
    "createdAt": "2019-08-05T09:18:20Z",
    "diffHunk": "@@ -22,12 +22,13 @@ import org.apache.kafka.common.TopicPartition\n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n import org.apache.spark.internal.config.Network.NETWORK_TIMEOUT\n+import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n import org.apache.spark.sql.sources.v2.reader.{Batch, InputPartition, PartitionReaderFactory}\n \n \n private[kafka010] class KafkaBatch(\n     strategy: ConsumerStrategy,\n-    sourceOptions: Map[String, String],\n+    sourceOptions: CaseInsensitiveMap[String],",
    "line": 11
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "I think it's fine. Let's keep it as it is.",
    "commit": "e0e8a8851f975311be990c5a4889d4c9870c3bea",
    "createdAt": "2019-08-05T15:44:58Z",
    "diffHunk": "@@ -22,12 +22,13 @@ import org.apache.kafka.common.TopicPartition\n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n import org.apache.spark.internal.config.Network.NETWORK_TIMEOUT\n+import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n import org.apache.spark.sql.sources.v2.reader.{Batch, InputPartition, PartitionReaderFactory}\n \n \n private[kafka010] class KafkaBatch(\n     strategy: ConsumerStrategy,\n-    sourceOptions: Map[String, String],\n+    sourceOptions: CaseInsensitiveMap[String],",
    "line": 11
  }],
  "prId": 24967
}]