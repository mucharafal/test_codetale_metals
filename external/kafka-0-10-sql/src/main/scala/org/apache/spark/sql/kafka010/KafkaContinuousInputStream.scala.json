[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "moved from https://github.com/apache/spark/pull/22547/files#diff-5fa6c9fc023183f4a855f778944d23ebL162",
    "commit": "9f63721677cea627f43f7d536bb32b588cee30a3",
    "createdAt": "2018-09-25T17:00:15Z",
    "diffHunk": "@@ -67,28 +71,29 @@ class KafkaContinuousReadSupport(\n     offsets\n   }\n \n-  override def fullSchema(): StructType = KafkaOffsetReader.kafkaSchema\n-\n-  override def newScanConfigBuilder(start: Offset): ScanConfigBuilder = {\n-    new KafkaContinuousScanConfigBuilder(fullSchema(), start, offsetReader, reportDataLoss)\n-  }\n-\n   override def deserializeOffset(json: String): Offset = {\n     KafkaSourceOffset(JsonUtils.partitionOffsets(json))\n   }\n \n-  override def planInputPartitions(config: ScanConfig): Array[InputPartition] = {\n-    val startOffsets = config.asInstanceOf[KafkaContinuousScanConfig].startOffsets\n-    startOffsets.toSeq.map {\n-      case (topicPartition, start) =>\n-        KafkaContinuousInputPartition(\n-          topicPartition, start, kafkaParams, pollTimeoutMs, failOnDataLoss)\n-    }.toArray\n-  }\n+  override def createContinuousScan(start: Offset): ContinuousScan = {\n+    val oldStartPartitionOffsets = KafkaSourceOffset.getPartitionOffsets(start)",
    "line": 60
  }],
  "prId": 22547
}, {
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Is it possible to break this change into multiple PRs for batch, microbatch, and continuous? It's really large and it would be nice if we could get the changes in incrementally.",
    "commit": "9f63721677cea627f43f7d536bb32b588cee30a3",
    "createdAt": "2018-10-19T21:08:03Z",
    "diffHunk": "@@ -46,17 +45,22 @@ import org.apache.spark.sql.types.StructType\n  *                       scenarios, where some offsets after the specified initial ones can't be\n  *                       properly read.\n  */\n-class KafkaContinuousReadSupport(\n+class KafkaContinuousInputStream(",
    "line": 17
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "+1 for this. A lot of the changes right now are for moving around the streaming code especially, which makes it harder to isolate just the proposed API for review.\r\n\r\nAn alternative is to make this PR separate commits that, while the commits themselves may not compile because of mismatching signatures - but all the commits taken together would compile, and each commit can be reviewed individually for assessing the API and then the implementation.\r\n\r\nFor example I'd propose 3 PRs:\r\n\r\n* Batch reading, with a commit for the interface changes and a separate commit for the implementation changes\r\n* Micro Batch Streaming read, with a commit for the interface changes and a separate commit for the implementation changes\r\n* Continuous streaming read, similar to above\r\n\r\nThoughts?",
    "commit": "9f63721677cea627f43f7d536bb32b588cee30a3",
    "createdAt": "2018-11-02T20:55:18Z",
    "diffHunk": "@@ -46,17 +45,22 @@ import org.apache.spark.sql.types.StructType\n  *                       scenarios, where some offsets after the specified initial ones can't be\n  *                       properly read.\n  */\n-class KafkaContinuousReadSupport(\n+class KafkaContinuousInputStream(",
    "line": 17
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "I'd prefer that the commits themselves compile, but since this is separating the modes I think it could be done incrementally.",
    "commit": "9f63721677cea627f43f7d536bb32b588cee30a3",
    "createdAt": "2018-11-02T22:51:56Z",
    "diffHunk": "@@ -46,17 +45,22 @@ import org.apache.spark.sql.types.StructType\n  *                       scenarios, where some offsets after the specified initial ones can't be\n  *                       properly read.\n  */\n-class KafkaContinuousReadSupport(\n+class KafkaContinuousInputStream(",
    "line": 17
  }, {
    "author": {
      "login": "mccheah"
    },
    "body": "Makes sense. I really consider this to be a blocker on getting this merged and approved. It's difficult to have confidence in a review over such a large change. Thoughts @cloud-fan @rdblue?",
    "commit": "9f63721677cea627f43f7d536bb32b588cee30a3",
    "createdAt": "2018-11-06T01:25:53Z",
    "diffHunk": "@@ -46,17 +45,22 @@ import org.apache.spark.sql.types.StructType\n  *                       scenarios, where some offsets after the specified initial ones can't be\n  *                       properly read.\n  */\n-class KafkaContinuousReadSupport(\n+class KafkaContinuousInputStream(",
    "line": 17
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "Yea I'll separate this PR into 3 smaller ones, after we have agreed on the high-level design at https://docs.google.com/document/d/1uUmKCpWLdh9vHxP7AWJ9EgbwB_U6T3EJYNjhISGmiQg/edit?usp=sharing",
    "commit": "9f63721677cea627f43f7d536bb32b588cee30a3",
    "createdAt": "2018-11-06T03:23:23Z",
    "diffHunk": "@@ -46,17 +45,22 @@ import org.apache.spark.sql.types.StructType\n  *                       scenarios, where some offsets after the specified initial ones can't be\n  *                       properly read.\n  */\n-class KafkaContinuousReadSupport(\n+class KafkaContinuousInputStream(",
    "line": 17
  }],
  "prId": 22547
}]