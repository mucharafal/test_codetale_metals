[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "Moved from https://github.com/apache/spark/pull/22009/files#diff-b35752a92e5ab595a6360d6123c7b7b8L93",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-08T18:05:19Z",
    "diffHunk": "@@ -162,23 +141,52 @@ case class KafkaContinuousInputPartition(\n     startOffset: Long,\n     kafkaParams: ju.Map[String, Object],\n     pollTimeoutMs: Long,\n-    failOnDataLoss: Boolean) extends ContinuousInputPartition[InternalRow] {\n+    failOnDataLoss: Boolean) extends InputPartition\n \n+object KafkaContinuousReaderFactory extends ContinuousPartitionReaderFactory {\n   override def createContinuousReader(\n-      offset: PartitionOffset): InputPartitionReader[InternalRow] = {\n-    val kafkaOffset = offset.asInstanceOf[KafkaSourcePartitionOffset]\n-    require(kafkaOffset.topicPartition == topicPartition,\n-      s\"Expected topicPartition: $topicPartition, but got: ${kafkaOffset.topicPartition}\")\n-    new KafkaContinuousInputPartitionReader(\n-      topicPartition, kafkaOffset.partitionOffset, kafkaParams, pollTimeoutMs, failOnDataLoss)\n+      partition: InputPartition): ContinuousPartitionReader[InternalRow] = {\n+    val p = partition.asInstanceOf[KafkaContinuousInputPartition]\n+    new KafkaContinuousPartitionReader(\n+      p.topicPartition, p.startOffset, p.kafkaParams, p.pollTimeoutMs, p.failOnDataLoss)\n   }\n+}\n+\n+class KafkaContinuousScanConfigBuilder(\n+    schema: StructType,\n+    startOffset: Offset,\n+    offsetReader: KafkaOffsetReader,\n+    reportDataLoss: String => Unit)\n+  extends ScanConfigBuilder {\n+\n+  override def build(): ScanConfig = {\n+    val oldStartPartitionOffsets = KafkaSourceOffset.getPartitionOffsets(startOffset)",
    "line": 156
  }],
  "prId": 22009
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "Moved to `KafkaContinuousScanConfig`",
    "commit": "51cda76897353344427aaa666e29be408263eeb1",
    "createdAt": "2018-08-21T14:31:48Z",
    "diffHunk": "@@ -47,70 +46,49 @@ import org.apache.spark.sql.types.StructType\n  *                       scenarios, where some offsets after the specified initial ones can't be\n  *                       properly read.\n  */\n-class KafkaContinuousReader(\n+class KafkaContinuousReadSupport(\n     offsetReader: KafkaOffsetReader,\n     kafkaParams: ju.Map[String, Object],\n     sourceOptions: Map[String, String],\n     metadataPath: String,\n     initialOffsets: KafkaOffsetRangeLimit,\n     failOnDataLoss: Boolean)\n-  extends ContinuousReader with Logging {\n-\n-  private lazy val session = SparkSession.getActiveSession.get\n-  private lazy val sc = session.sparkContext\n+  extends ContinuousReadSupport with Logging {\n \n   private val pollTimeoutMs = sourceOptions.getOrElse(\"kafkaConsumer.pollTimeoutMs\", \"512\").toLong\n \n-  // Initialized when creating reader factories. If this diverges from the partitions at the latest\n-  // offsets, we need to reconfigure.\n-  // Exposed outside this object only for unit tests.\n-  @volatile private[sql] var knownPartitions: Set[TopicPartition] = _",
    "line": 42
  }],
  "prId": 22009
}]