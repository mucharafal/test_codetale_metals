[{
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Might be better to define a new local value for `record.headers.toArray`, because it creates a new array when `headers` is not empty. It also guarantees consistent view for extracting keys and values, though we know `headers` is unlikely to be modified during this.",
    "commit": "de02de411aa00cbacc94be5b746dc48be0fe77a3",
    "createdAt": "2018-09-03T08:58:34Z",
    "diffHunk": "@@ -44,6 +44,11 @@ private[kafka010] class KafkaRecordToUnsafeRowConverter {\n       5,\n       DateTimeUtils.fromJavaTimestamp(new java.sql.Timestamp(record.timestamp)))\n     rowWriter.write(6, record.timestampType.id)\n+    val keys = record.headers.toArray.map(_.key())"
  }],
  "prId": 22282
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "ditto",
    "commit": "de02de411aa00cbacc94be5b746dc48be0fe77a3",
    "createdAt": "2019-03-18T19:00:29Z",
    "diffHunk": "@@ -44,6 +44,14 @@ private[kafka010] class KafkaRecordToUnsafeRowConverter {\n       5,\n       DateTimeUtils.fromJavaTimestamp(new java.sql.Timestamp(record.timestamp)))\n     rowWriter.write(6, record.timestampType.id)\n+    val headers = record.headers.toArray"
  }],
  "prId": 22282
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "nit: Use `new UnsafeRowWriter(KafkaOffsetReader.kafkaSchema(includeHeaders).size)` to avoid magic numbers.",
    "commit": "de02de411aa00cbacc94be5b746dc48be0fe77a3",
    "createdAt": "2019-07-22T20:20:56Z",
    "diffHunk": "@@ -25,8 +25,12 @@ import org.apache.spark.sql.catalyst.util.DateTimeUtils\n import org.apache.spark.unsafe.types.UTF8String\n \n /** A simple class for converting Kafka ConsumerRecord to UnsafeRow */\n-private[kafka010] class KafkaRecordToUnsafeRowConverter {\n-  private val rowWriter = new UnsafeRowWriter(7)\n+private[kafka010] class KafkaRecordToUnsafeRowConverter(includeHeaders: Boolean) {\n+  private val rowWriter = if (includeHeaders) {"
  }],
  "prId": 22282
}]