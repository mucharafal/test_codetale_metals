[{
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Nit: `kafka-fetched-data-cache-evictor`",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-03T15:04:36Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.mutable\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool {\n+  import FetchedDataPool._\n+\n+  private[kafka010] case class CachedFetchedData(fetchedData: FetchedData) {\n+    var lastReleasedTimestamp: Long = Long.MaxValue\n+    var lastAcquiredTimestamp: Long = Long.MinValue\n+    var inUse: Boolean = false\n+\n+    def getObject: FetchedData = fetchedData\n+  }\n+\n+  private object CachedFetchedData {\n+    def empty(): CachedFetchedData = {\n+      val emptyData = FetchedData(\n+        ju.Collections.emptyListIterator[ConsumerRecord[Array[Byte], Array[Byte]]],\n+        UNKNOWN_OFFSET,\n+        UNKNOWN_OFFSET)\n+\n+      CachedFetchedData(emptyData)\n+    }\n+  }\n+\n+  private type CachedFetchedDataList = mutable.ListBuffer[CachedFetchedData]\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  /** Retrieve internal cache. This method is only for testing. */\n+  private[kafka010] def getCache: mutable.Map[CacheKey, CachedFetchedDataList] = cache\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+      DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+\n+    val evictorThreadInterval = conf.getLong(\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+      DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private val executorService = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"kafka-fetched-data--cache-evictor\")"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Will address.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-04T13:21:30Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.mutable\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool {\n+  import FetchedDataPool._\n+\n+  private[kafka010] case class CachedFetchedData(fetchedData: FetchedData) {\n+    var lastReleasedTimestamp: Long = Long.MaxValue\n+    var lastAcquiredTimestamp: Long = Long.MinValue\n+    var inUse: Boolean = false\n+\n+    def getObject: FetchedData = fetchedData\n+  }\n+\n+  private object CachedFetchedData {\n+    def empty(): CachedFetchedData = {\n+      val emptyData = FetchedData(\n+        ju.Collections.emptyListIterator[ConsumerRecord[Array[Byte], Array[Byte]]],\n+        UNKNOWN_OFFSET,\n+        UNKNOWN_OFFSET)\n+\n+      CachedFetchedData(emptyData)\n+    }\n+  }\n+\n+  private type CachedFetchedDataList = mutable.ListBuffer[CachedFetchedData]\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  /** Retrieve internal cache. This method is only for testing. */\n+  private[kafka010] def getCache: mutable.Map[CacheKey, CachedFetchedDataList] = cache\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+      DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+\n+    val evictorThreadInterval = conf.getLong(\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+      DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private val executorService = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"kafka-fetched-data--cache-evictor\")"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Why is this better then:\r\n`private[kafka010] val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty`",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-03T15:06:16Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.mutable\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool {\n+  import FetchedDataPool._\n+\n+  private[kafka010] case class CachedFetchedData(fetchedData: FetchedData) {\n+    var lastReleasedTimestamp: Long = Long.MaxValue\n+    var lastAcquiredTimestamp: Long = Long.MinValue\n+    var inUse: Boolean = false\n+\n+    def getObject: FetchedData = fetchedData\n+  }\n+\n+  private object CachedFetchedData {\n+    def empty(): CachedFetchedData = {\n+      val emptyData = FetchedData(\n+        ju.Collections.emptyListIterator[ConsumerRecord[Array[Byte], Array[Byte]]],\n+        UNKNOWN_OFFSET,\n+        UNKNOWN_OFFSET)\n+\n+      CachedFetchedData(emptyData)\n+    }\n+  }\n+\n+  private type CachedFetchedDataList = mutable.ListBuffer[CachedFetchedData]\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  /** Retrieve internal cache. This method is only for testing. */\n+  private[kafka010] def getCache: mutable.Map[CacheKey, CachedFetchedDataList] = cache"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "This is to make sure `cache` itself is not accessible from outside, and when callers access `cache` via `getCache`, they will be noted it should not be used other than testing from scaladoc.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-04T13:23:54Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.mutable\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool {\n+  import FetchedDataPool._\n+\n+  private[kafka010] case class CachedFetchedData(fetchedData: FetchedData) {\n+    var lastReleasedTimestamp: Long = Long.MaxValue\n+    var lastAcquiredTimestamp: Long = Long.MinValue\n+    var inUse: Boolean = false\n+\n+    def getObject: FetchedData = fetchedData\n+  }\n+\n+  private object CachedFetchedData {\n+    def empty(): CachedFetchedData = {\n+      val emptyData = FetchedData(\n+        ju.Collections.emptyListIterator[ConsumerRecord[Array[Byte], Array[Byte]]],\n+        UNKNOWN_OFFSET,\n+        UNKNOWN_OFFSET)\n+\n+      CachedFetchedData(emptyData)\n+    }\n+  }\n+\n+  private type CachedFetchedDataList = mutable.ListBuffer[CachedFetchedData]\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  /** Retrieve internal cache. This method is only for testing. */\n+  private[kafka010] def getCache: mutable.Map[CacheKey, CachedFetchedDataList] = cache"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Then `PrivateMethodTester` can be used.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-05T13:29:18Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.mutable\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool {\n+  import FetchedDataPool._\n+\n+  private[kafka010] case class CachedFetchedData(fetchedData: FetchedData) {\n+    var lastReleasedTimestamp: Long = Long.MaxValue\n+    var lastAcquiredTimestamp: Long = Long.MinValue\n+    var inUse: Boolean = false\n+\n+    def getObject: FetchedData = fetchedData\n+  }\n+\n+  private object CachedFetchedData {\n+    def empty(): CachedFetchedData = {\n+      val emptyData = FetchedData(\n+        ju.Collections.emptyListIterator[ConsumerRecord[Array[Byte], Array[Byte]]],\n+        UNKNOWN_OFFSET,\n+        UNKNOWN_OFFSET)\n+\n+      CachedFetchedData(emptyData)\n+    }\n+  }\n+\n+  private type CachedFetchedDataList = mutable.ListBuffer[CachedFetchedData]\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  /** Retrieve internal cache. This method is only for testing. */\n+  private[kafka010] def getCache: mutable.Map[CacheKey, CachedFetchedDataList] = cache"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Ah yes, wasn't aware of PrivateMethodTester. Thanks! Will apply.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-05T15:09:11Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.mutable\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool {\n+  import FetchedDataPool._\n+\n+  private[kafka010] case class CachedFetchedData(fetchedData: FetchedData) {\n+    var lastReleasedTimestamp: Long = Long.MaxValue\n+    var lastAcquiredTimestamp: Long = Long.MinValue\n+    var inUse: Boolean = false\n+\n+    def getObject: FetchedData = fetchedData\n+  }\n+\n+  private object CachedFetchedData {\n+    def empty(): CachedFetchedData = {\n+      val emptyData = FetchedData(\n+        ju.Collections.emptyListIterator[ConsumerRecord[Array[Byte], Array[Byte]]],\n+        UNKNOWN_OFFSET,\n+        UNKNOWN_OFFSET)\n+\n+      CachedFetchedData(emptyData)\n+    }\n+  }\n+\n+  private type CachedFetchedDataList = mutable.ListBuffer[CachedFetchedData]\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  /** Retrieve internal cache. This method is only for testing. */\n+  private[kafka010] def getCache: mutable.Map[CacheKey, CachedFetchedDataList] = cache"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Nit: `executorService.scheduleAtFixedRate(new Runnable {` is enough.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-03T15:08:58Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.mutable\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool {\n+  import FetchedDataPool._\n+\n+  private[kafka010] case class CachedFetchedData(fetchedData: FetchedData) {\n+    var lastReleasedTimestamp: Long = Long.MaxValue\n+    var lastAcquiredTimestamp: Long = Long.MinValue\n+    var inUse: Boolean = false\n+\n+    def getObject: FetchedData = fetchedData\n+  }\n+\n+  private object CachedFetchedData {\n+    def empty(): CachedFetchedData = {\n+      val emptyData = FetchedData(\n+        ju.Collections.emptyListIterator[ConsumerRecord[Array[Byte], Array[Byte]]],\n+        UNKNOWN_OFFSET,\n+        UNKNOWN_OFFSET)\n+\n+      CachedFetchedData(emptyData)\n+    }\n+  }\n+\n+  private type CachedFetchedDataList = mutable.ListBuffer[CachedFetchedData]\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  /** Retrieve internal cache. This method is only for testing. */\n+  private[kafka010] def getCache: mutable.Map[CacheKey, CachedFetchedDataList] = cache\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+      DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+\n+    val evictorThreadInterval = conf.getLong(\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+      DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private val executorService = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"kafka-fetched-data--cache-evictor\")\n+\n+  private def startEvictorThread(): Unit = {\n+    executorService.scheduleAtFixedRate(new Runnable() {"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Will address.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-04T13:21:20Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.mutable\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool {\n+  import FetchedDataPool._\n+\n+  private[kafka010] case class CachedFetchedData(fetchedData: FetchedData) {\n+    var lastReleasedTimestamp: Long = Long.MaxValue\n+    var lastAcquiredTimestamp: Long = Long.MinValue\n+    var inUse: Boolean = false\n+\n+    def getObject: FetchedData = fetchedData\n+  }\n+\n+  private object CachedFetchedData {\n+    def empty(): CachedFetchedData = {\n+      val emptyData = FetchedData(\n+        ju.Collections.emptyListIterator[ConsumerRecord[Array[Byte], Array[Byte]]],\n+        UNKNOWN_OFFSET,\n+        UNKNOWN_OFFSET)\n+\n+      CachedFetchedData(emptyData)\n+    }\n+  }\n+\n+  private type CachedFetchedDataList = mutable.ListBuffer[CachedFetchedData]\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  /** Retrieve internal cache. This method is only for testing. */\n+  private[kafka010] def getCache: mutable.Map[CacheKey, CachedFetchedDataList] = cache\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+      DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+\n+    val evictorThreadInterval = conf.getLong(\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+      DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private val executorService = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"kafka-fetched-data--cache-evictor\")\n+\n+  private def startEvictorThread(): Unit = {\n+    executorService.scheduleAtFixedRate(new Runnable() {"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Any thrown exception or error reaching the executor causes the executor to halt.\r\n`catch` + `log...` would be good.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-03T15:27:57Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.mutable\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool {\n+  import FetchedDataPool._\n+\n+  private[kafka010] case class CachedFetchedData(fetchedData: FetchedData) {\n+    var lastReleasedTimestamp: Long = Long.MaxValue\n+    var lastAcquiredTimestamp: Long = Long.MinValue\n+    var inUse: Boolean = false\n+\n+    def getObject: FetchedData = fetchedData\n+  }\n+\n+  private object CachedFetchedData {\n+    def empty(): CachedFetchedData = {\n+      val emptyData = FetchedData(\n+        ju.Collections.emptyListIterator[ConsumerRecord[Array[Byte], Array[Byte]]],\n+        UNKNOWN_OFFSET,\n+        UNKNOWN_OFFSET)\n+\n+      CachedFetchedData(emptyData)\n+    }\n+  }\n+\n+  private type CachedFetchedDataList = mutable.ListBuffer[CachedFetchedData]\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  /** Retrieve internal cache. This method is only for testing. */\n+  private[kafka010] def getCache: mutable.Map[CacheKey, CachedFetchedDataList] = cache\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+      DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+\n+    val evictorThreadInterval = conf.getLong(\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+      DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private val executorService = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"kafka-fetched-data--cache-evictor\")\n+\n+  private def startEvictorThread(): Unit = {\n+    executorService.scheduleAtFixedRate(new Runnable() {\n+      override def run(): Unit = {\n+        removeIdleFetchedData()"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Nice catch! Will address.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-04T13:21:06Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.mutable\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool {\n+  import FetchedDataPool._\n+\n+  private[kafka010] case class CachedFetchedData(fetchedData: FetchedData) {\n+    var lastReleasedTimestamp: Long = Long.MaxValue\n+    var lastAcquiredTimestamp: Long = Long.MinValue\n+    var inUse: Boolean = false\n+\n+    def getObject: FetchedData = fetchedData\n+  }\n+\n+  private object CachedFetchedData {\n+    def empty(): CachedFetchedData = {\n+      val emptyData = FetchedData(\n+        ju.Collections.emptyListIterator[ConsumerRecord[Array[Byte], Array[Byte]]],\n+        UNKNOWN_OFFSET,\n+        UNKNOWN_OFFSET)\n+\n+      CachedFetchedData(emptyData)\n+    }\n+  }\n+\n+  private type CachedFetchedDataList = mutable.ListBuffer[CachedFetchedData]\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  /** Retrieve internal cache. This method is only for testing. */\n+  private[kafka010] def getCache: mutable.Map[CacheKey, CachedFetchedDataList] = cache\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+      DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+\n+    val evictorThreadInterval = conf.getLong(\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+      DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private val executorService = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"kafka-fetched-data--cache-evictor\")\n+\n+  private def startEvictorThread(): Unit = {\n+    executorService.scheduleAtFixedRate(new Runnable() {\n+      override def run(): Unit = {\n+        removeIdleFetchedData()"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Some warning would be good because it could be a programming failure.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-04T07:53:49Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.mutable\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool {\n+  import FetchedDataPool._\n+\n+  private[kafka010] case class CachedFetchedData(fetchedData: FetchedData) {\n+    var lastReleasedTimestamp: Long = Long.MaxValue\n+    var lastAcquiredTimestamp: Long = Long.MinValue\n+    var inUse: Boolean = false\n+\n+    def getObject: FetchedData = fetchedData\n+  }\n+\n+  private object CachedFetchedData {\n+    def empty(): CachedFetchedData = {\n+      val emptyData = FetchedData(\n+        ju.Collections.emptyListIterator[ConsumerRecord[Array[Byte], Array[Byte]]],\n+        UNKNOWN_OFFSET,\n+        UNKNOWN_OFFSET)\n+\n+      CachedFetchedData(emptyData)\n+    }\n+  }\n+\n+  private type CachedFetchedDataList = mutable.ListBuffer[CachedFetchedData]\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  /** Retrieve internal cache. This method is only for testing. */\n+  private[kafka010] def getCache: mutable.Map[CacheKey, CachedFetchedDataList] = cache\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+      DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+\n+    val evictorThreadInterval = conf.getLong(\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+      DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private val executorService = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"kafka-fetched-data--cache-evictor\")\n+\n+  private def startEvictorThread(): Unit = {\n+    executorService.scheduleAtFixedRate(new Runnable() {\n+      override def run(): Unit = {\n+        removeIdleFetchedData()\n+      }\n+    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\n+  }\n+\n+  startEvictorThread()\n+\n+  def acquire(key: CacheKey, desiredStartOffset: Long): FetchedData = synchronized {\n+    val fetchedDataList = cache.getOrElseUpdate(key, new CachedFetchedDataList())\n+\n+    val cachedFetchedDataOption = fetchedDataList.find { p =>\n+      !p.inUse && p.getObject.nextOffsetInFetchedData == desiredStartOffset\n+    }\n+\n+    var cachedFetchedData: CachedFetchedData = null\n+    if (cachedFetchedDataOption.isDefined) {\n+      cachedFetchedData = cachedFetchedDataOption.get\n+    } else {\n+      cachedFetchedData = CachedFetchedData.empty()\n+      fetchedDataList += cachedFetchedData\n+    }\n+\n+    cachedFetchedData.lastAcquiredTimestamp = System.currentTimeMillis()\n+    cachedFetchedData.inUse = true\n+\n+    cachedFetchedData.getObject\n+  }\n+\n+  def invalidate(key: CacheKey): Unit = synchronized {\n+    cache.remove(key)\n+  }\n+\n+  def release(key: CacheKey, fetchedData: FetchedData): Unit = synchronized {\n+    cache.get(key) match {\n+      case Some(fetchedDataList) =>\n+        val cachedFetchedDataOption = fetchedDataList.find { p =>\n+          p.inUse && p.getObject == fetchedData\n+        }\n+\n+        if (cachedFetchedDataOption.isDefined) {\n+          val cachedFetchedData = cachedFetchedDataOption.get\n+          cachedFetchedData.inUse = false\n+          cachedFetchedData.lastReleasedTimestamp = System.currentTimeMillis()\n+        }\n+\n+      case None =>"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Yeah sometimes yes and sometimes ignorable thing, I intended to avoid the situation which end users are confused with false-negative, but I'm also open to add log message for that.\r\n\r\nLet me add some warning message, and if some reviewer stated the level or removing log, we can revisit it later.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-04T13:19:38Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.mutable\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool {\n+  import FetchedDataPool._\n+\n+  private[kafka010] case class CachedFetchedData(fetchedData: FetchedData) {\n+    var lastReleasedTimestamp: Long = Long.MaxValue\n+    var lastAcquiredTimestamp: Long = Long.MinValue\n+    var inUse: Boolean = false\n+\n+    def getObject: FetchedData = fetchedData\n+  }\n+\n+  private object CachedFetchedData {\n+    def empty(): CachedFetchedData = {\n+      val emptyData = FetchedData(\n+        ju.Collections.emptyListIterator[ConsumerRecord[Array[Byte], Array[Byte]]],\n+        UNKNOWN_OFFSET,\n+        UNKNOWN_OFFSET)\n+\n+      CachedFetchedData(emptyData)\n+    }\n+  }\n+\n+  private type CachedFetchedDataList = mutable.ListBuffer[CachedFetchedData]\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  /** Retrieve internal cache. This method is only for testing. */\n+  private[kafka010] def getCache: mutable.Map[CacheKey, CachedFetchedDataList] = cache\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+      DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+\n+    val evictorThreadInterval = conf.getLong(\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+      DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private val executorService = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"kafka-fetched-data--cache-evictor\")\n+\n+  private def startEvictorThread(): Unit = {\n+    executorService.scheduleAtFixedRate(new Runnable() {\n+      override def run(): Unit = {\n+        removeIdleFetchedData()\n+      }\n+    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\n+  }\n+\n+  startEvictorThread()\n+\n+  def acquire(key: CacheKey, desiredStartOffset: Long): FetchedData = synchronized {\n+    val fetchedDataList = cache.getOrElseUpdate(key, new CachedFetchedDataList())\n+\n+    val cachedFetchedDataOption = fetchedDataList.find { p =>\n+      !p.inUse && p.getObject.nextOffsetInFetchedData == desiredStartOffset\n+    }\n+\n+    var cachedFetchedData: CachedFetchedData = null\n+    if (cachedFetchedDataOption.isDefined) {\n+      cachedFetchedData = cachedFetchedDataOption.get\n+    } else {\n+      cachedFetchedData = CachedFetchedData.empty()\n+      fetchedDataList += cachedFetchedData\n+    }\n+\n+    cachedFetchedData.lastAcquiredTimestamp = System.currentTimeMillis()\n+    cachedFetchedData.inUse = true\n+\n+    cachedFetchedData.getObject\n+  }\n+\n+  def invalidate(key: CacheKey): Unit = synchronized {\n+    cache.remove(key)\n+  }\n+\n+  def release(key: CacheKey, fetchedData: FetchedData): Unit = synchronized {\n+    cache.get(key) match {\n+      case Some(fetchedDataList) =>\n+        val cachedFetchedDataOption = fetchedDataList.find { p =>\n+          p.inUse && p.getObject == fetchedData\n+        }\n+\n+        if (cachedFetchedDataOption.isDefined) {\n+          val cachedFetchedData = cachedFetchedDataOption.get\n+          cachedFetchedData.inUse = false\n+          cachedFetchedData.lastReleasedTimestamp = System.currentTimeMillis()\n+        }\n+\n+      case None =>"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Thanks, yeah just for the record I meant something like this: https://github.com/apache/spark/blob/341b55a58964b1966a1919ac0774c8be5d5e7251/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaDataConsumer.scala#L623\r\n",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-05T14:00:22Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.mutable\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool {\n+  import FetchedDataPool._\n+\n+  private[kafka010] case class CachedFetchedData(fetchedData: FetchedData) {\n+    var lastReleasedTimestamp: Long = Long.MaxValue\n+    var lastAcquiredTimestamp: Long = Long.MinValue\n+    var inUse: Boolean = false\n+\n+    def getObject: FetchedData = fetchedData\n+  }\n+\n+  private object CachedFetchedData {\n+    def empty(): CachedFetchedData = {\n+      val emptyData = FetchedData(\n+        ju.Collections.emptyListIterator[ConsumerRecord[Array[Byte], Array[Byte]]],\n+        UNKNOWN_OFFSET,\n+        UNKNOWN_OFFSET)\n+\n+      CachedFetchedData(emptyData)\n+    }\n+  }\n+\n+  private type CachedFetchedDataList = mutable.ListBuffer[CachedFetchedData]\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  /** Retrieve internal cache. This method is only for testing. */\n+  private[kafka010] def getCache: mutable.Map[CacheKey, CachedFetchedDataList] = cache\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+      DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+\n+    val evictorThreadInterval = conf.getLong(\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+      DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private val executorService = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"kafka-fetched-data--cache-evictor\")\n+\n+  private def startEvictorThread(): Unit = {\n+    executorService.scheduleAtFixedRate(new Runnable() {\n+      override def run(): Unit = {\n+        removeIdleFetchedData()\n+      }\n+    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\n+  }\n+\n+  startEvictorThread()\n+\n+  def acquire(key: CacheKey, desiredStartOffset: Long): FetchedData = synchronized {\n+    val fetchedDataList = cache.getOrElseUpdate(key, new CachedFetchedDataList())\n+\n+    val cachedFetchedDataOption = fetchedDataList.find { p =>\n+      !p.inUse && p.getObject.nextOffsetInFetchedData == desiredStartOffset\n+    }\n+\n+    var cachedFetchedData: CachedFetchedData = null\n+    if (cachedFetchedDataOption.isDefined) {\n+      cachedFetchedData = cachedFetchedDataOption.get\n+    } else {\n+      cachedFetchedData = CachedFetchedData.empty()\n+      fetchedDataList += cachedFetchedData\n+    }\n+\n+    cachedFetchedData.lastAcquiredTimestamp = System.currentTimeMillis()\n+    cachedFetchedData.inUse = true\n+\n+    cachedFetchedData.getObject\n+  }\n+\n+  def invalidate(key: CacheKey): Unit = synchronized {\n+    cache.remove(key)\n+  }\n+\n+  def release(key: CacheKey, fetchedData: FetchedData): Unit = synchronized {\n+    cache.get(key) match {\n+      case Some(fetchedDataList) =>\n+        val cachedFetchedDataOption = fetchedDataList.find { p =>\n+          p.inUse && p.getObject == fetchedData\n+        }\n+\n+        if (cachedFetchedDataOption.isDefined) {\n+          val cachedFetchedData = cachedFetchedDataOption.get\n+          cachedFetchedData.inUse = false\n+          cachedFetchedData.lastReleasedTimestamp = System.currentTimeMillis()\n+        }\n+\n+      case None =>"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Same question re: config constants.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-07-10T23:08:00Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+      DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+\n+    val evictorThreadInterval = conf.getLong(\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+      DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private val executorService = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"kafka-fetched-data-cache-evictor\")\n+\n+  private def startEvictorThread(): ScheduledFuture[_] = {\n+    executorService.scheduleAtFixedRate(new Runnable {\n+      override def run(): Unit = {\n+        try {\n+          removeIdleFetchedData()\n+        } catch {\n+          case NonFatal(e) =>\n+            logWarning(\"Exception occurred while removing idle fetched data.\", e)\n+        }\n+      }\n+    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\n+  }\n+\n+  private var scheduled = startEvictorThread()\n+\n+  private val numCreatedFetchedData = new LongAdder()\n+  private val numTotalElements = new LongAdder()\n+\n+  def getNumCreated: Long = numCreatedFetchedData.sum()\n+  def getNumTotal: Long = numTotalElements.sum()\n+\n+  def acquire(key: CacheKey, desiredStartOffset: Long): FetchedData = synchronized {\n+    val fetchedDataList = cache.getOrElseUpdate(key, new CachedFetchedDataList())\n+\n+    val cachedFetchedDataOption = fetchedDataList.find { p =>\n+      !p.inUse && p.getObject.nextOffsetInFetchedData == desiredStartOffset\n+    }\n+\n+    var cachedFetchedData: CachedFetchedData = null\n+    if (cachedFetchedDataOption.isDefined) {\n+      cachedFetchedData = cachedFetchedDataOption.get\n+    } else {\n+      cachedFetchedData = CachedFetchedData.empty()\n+      fetchedDataList += cachedFetchedData\n+\n+      numCreatedFetchedData.increment()\n+      numTotalElements.increment()\n+    }\n+\n+    cachedFetchedData.lastAcquiredTimestamp = System.currentTimeMillis()\n+    cachedFetchedData.inUse = true\n+\n+    cachedFetchedData.getObject\n+  }\n+\n+  def invalidate(key: CacheKey): Unit = synchronized {\n+    cache.remove(key) match {\n+      case Some(lst) => numTotalElements.add(-1 * lst.size)\n+      case None =>\n+    }\n+  }\n+\n+  def release(key: CacheKey, fetchedData: FetchedData): Unit = synchronized {\n+    cache.get(key) match {\n+      case Some(fetchedDataList) =>\n+        val cachedFetchedDataOption = fetchedDataList.find { p =>\n+          p.inUse && p.getObject == fetchedData\n+        }\n+\n+        if (cachedFetchedDataOption.isDefined) {\n+          val cachedFetchedData = cachedFetchedDataOption.get\n+          cachedFetchedData.inUse = false\n+          cachedFetchedData.lastReleasedTimestamp = System.currentTimeMillis()\n+        }\n+\n+      case None => logWarning(s\"No matching data in pool for $fetchedData in key $key. \" +\n+        \"It might be released before, or it was not a part of pool.\")\n+    }\n+  }\n+\n+  def shutdown(): Unit = {\n+    executorService.shutdownNow()\n+  }\n+\n+  def reset(): Unit = synchronized {\n+    scheduled.cancel(true)\n+\n+    cache.clear()\n+    numTotalElements.reset()\n+    numCreatedFetchedData.reset()\n+\n+    scheduled = startEvictorThread()\n+  }\n+\n+  private def removeIdleFetchedData(): Unit = synchronized {\n+    val timestamp = System.currentTimeMillis()\n+    val maxAllowedIdleTimestamp = timestamp - minEvictableIdleTimeMillis\n+    cache.values.foreach { p: CachedFetchedDataList =>\n+      val idles = p.filter(q => !q.inUse && q.lastReleasedTimestamp < maxAllowedIdleTimestamp)\n+      val lstSize = p.size\n+      idles.foreach(idle => p -= idle)\n+      numTotalElements.add(-1 * (lstSize - p.size))\n+    }\n+  }\n+}\n+\n+private[kafka010] object FetchedDataPool {\n+  private[kafka010] case class CachedFetchedData(fetchedData: FetchedData) {\n+    var lastReleasedTimestamp: Long = Long.MaxValue\n+    var lastAcquiredTimestamp: Long = Long.MinValue\n+    var inUse: Boolean = false\n+\n+    def getObject: FetchedData = fetchedData\n+  }\n+\n+  private object CachedFetchedData {\n+    def empty(): CachedFetchedData = {\n+      val emptyData = FetchedData(\n+        ju.Collections.emptyListIterator[ConsumerRecord[Array[Byte], Array[Byte]]],\n+        UNKNOWN_OFFSET,\n+        UNKNOWN_OFFSET)\n+\n+      CachedFetchedData(emptyData)\n+    }\n+  }\n+\n+  private[kafka010] type CachedFetchedDataList = mutable.ListBuffer[CachedFetchedData]\n+\n+  val CONFIG_NAME_PREFIX = \"spark.sql.kafkaFetchedDataCache.\""
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "In the meantime such configs are prefixed with `spark.kafka.`, for example `spark.kafka.consumer.cache.capacity`. Please see https://github.com/apache/spark/pull/24590.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-07-12T13:06:31Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+      DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+\n+    val evictorThreadInterval = conf.getLong(\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+      DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private val executorService = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"kafka-fetched-data-cache-evictor\")\n+\n+  private def startEvictorThread(): ScheduledFuture[_] = {\n+    executorService.scheduleAtFixedRate(new Runnable {\n+      override def run(): Unit = {\n+        try {\n+          removeIdleFetchedData()\n+        } catch {\n+          case NonFatal(e) =>\n+            logWarning(\"Exception occurred while removing idle fetched data.\", e)\n+        }\n+      }\n+    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\n+  }\n+\n+  private var scheduled = startEvictorThread()\n+\n+  private val numCreatedFetchedData = new LongAdder()\n+  private val numTotalElements = new LongAdder()\n+\n+  def getNumCreated: Long = numCreatedFetchedData.sum()\n+  def getNumTotal: Long = numTotalElements.sum()\n+\n+  def acquire(key: CacheKey, desiredStartOffset: Long): FetchedData = synchronized {\n+    val fetchedDataList = cache.getOrElseUpdate(key, new CachedFetchedDataList())\n+\n+    val cachedFetchedDataOption = fetchedDataList.find { p =>\n+      !p.inUse && p.getObject.nextOffsetInFetchedData == desiredStartOffset\n+    }\n+\n+    var cachedFetchedData: CachedFetchedData = null\n+    if (cachedFetchedDataOption.isDefined) {\n+      cachedFetchedData = cachedFetchedDataOption.get\n+    } else {\n+      cachedFetchedData = CachedFetchedData.empty()\n+      fetchedDataList += cachedFetchedData\n+\n+      numCreatedFetchedData.increment()\n+      numTotalElements.increment()\n+    }\n+\n+    cachedFetchedData.lastAcquiredTimestamp = System.currentTimeMillis()\n+    cachedFetchedData.inUse = true\n+\n+    cachedFetchedData.getObject\n+  }\n+\n+  def invalidate(key: CacheKey): Unit = synchronized {\n+    cache.remove(key) match {\n+      case Some(lst) => numTotalElements.add(-1 * lst.size)\n+      case None =>\n+    }\n+  }\n+\n+  def release(key: CacheKey, fetchedData: FetchedData): Unit = synchronized {\n+    cache.get(key) match {\n+      case Some(fetchedDataList) =>\n+        val cachedFetchedDataOption = fetchedDataList.find { p =>\n+          p.inUse && p.getObject == fetchedData\n+        }\n+\n+        if (cachedFetchedDataOption.isDefined) {\n+          val cachedFetchedData = cachedFetchedDataOption.get\n+          cachedFetchedData.inUse = false\n+          cachedFetchedData.lastReleasedTimestamp = System.currentTimeMillis()\n+        }\n+\n+      case None => logWarning(s\"No matching data in pool for $fetchedData in key $key. \" +\n+        \"It might be released before, or it was not a part of pool.\")\n+    }\n+  }\n+\n+  def shutdown(): Unit = {\n+    executorService.shutdownNow()\n+  }\n+\n+  def reset(): Unit = synchronized {\n+    scheduled.cancel(true)\n+\n+    cache.clear()\n+    numTotalElements.reset()\n+    numCreatedFetchedData.reset()\n+\n+    scheduled = startEvictorThread()\n+  }\n+\n+  private def removeIdleFetchedData(): Unit = synchronized {\n+    val timestamp = System.currentTimeMillis()\n+    val maxAllowedIdleTimestamp = timestamp - minEvictableIdleTimeMillis\n+    cache.values.foreach { p: CachedFetchedDataList =>\n+      val idles = p.filter(q => !q.inUse && q.lastReleasedTimestamp < maxAllowedIdleTimestamp)\n+      val lstSize = p.size\n+      idles.foreach(idle => p -= idle)\n+      numTotalElements.add(-1 * (lstSize - p.size))\n+    }\n+  }\n+}\n+\n+private[kafka010] object FetchedDataPool {\n+  private[kafka010] case class CachedFetchedData(fetchedData: FetchedData) {\n+    var lastReleasedTimestamp: Long = Long.MaxValue\n+    var lastAcquiredTimestamp: Long = Long.MinValue\n+    var inUse: Boolean = false\n+\n+    def getObject: FetchedData = fetchedData\n+  }\n+\n+  private object CachedFetchedData {\n+    def empty(): CachedFetchedData = {\n+      val emptyData = FetchedData(\n+        ju.Collections.emptyListIterator[ConsumerRecord[Array[Byte], Array[Byte]]],\n+        UNKNOWN_OFFSET,\n+        UNKNOWN_OFFSET)\n+\n+      CachedFetchedData(emptyData)\n+    }\n+  }\n+\n+  private[kafka010] type CachedFetchedDataList = mutable.ListBuffer[CachedFetchedData]\n+\n+  val CONFIG_NAME_PREFIX = \"spark.sql.kafkaFetchedDataCache.\""
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "This hasn't been addressed. Seems like you both need to create config constants, and also use the new name.\r\n\r\n(You created constants for the consumer cache, e.g. `CONSUMER_CACHE_MIN_EVICTABLE_IDLE_TIME_MILLIS`, so why not for these?)",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-26T18:26:39Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+      DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+\n+    val evictorThreadInterval = conf.getLong(\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+      DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private val executorService = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"kafka-fetched-data-cache-evictor\")\n+\n+  private def startEvictorThread(): ScheduledFuture[_] = {\n+    executorService.scheduleAtFixedRate(new Runnable {\n+      override def run(): Unit = {\n+        try {\n+          removeIdleFetchedData()\n+        } catch {\n+          case NonFatal(e) =>\n+            logWarning(\"Exception occurred while removing idle fetched data.\", e)\n+        }\n+      }\n+    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\n+  }\n+\n+  private var scheduled = startEvictorThread()\n+\n+  private val numCreatedFetchedData = new LongAdder()\n+  private val numTotalElements = new LongAdder()\n+\n+  def getNumCreated: Long = numCreatedFetchedData.sum()\n+  def getNumTotal: Long = numTotalElements.sum()\n+\n+  def acquire(key: CacheKey, desiredStartOffset: Long): FetchedData = synchronized {\n+    val fetchedDataList = cache.getOrElseUpdate(key, new CachedFetchedDataList())\n+\n+    val cachedFetchedDataOption = fetchedDataList.find { p =>\n+      !p.inUse && p.getObject.nextOffsetInFetchedData == desiredStartOffset\n+    }\n+\n+    var cachedFetchedData: CachedFetchedData = null\n+    if (cachedFetchedDataOption.isDefined) {\n+      cachedFetchedData = cachedFetchedDataOption.get\n+    } else {\n+      cachedFetchedData = CachedFetchedData.empty()\n+      fetchedDataList += cachedFetchedData\n+\n+      numCreatedFetchedData.increment()\n+      numTotalElements.increment()\n+    }\n+\n+    cachedFetchedData.lastAcquiredTimestamp = System.currentTimeMillis()\n+    cachedFetchedData.inUse = true\n+\n+    cachedFetchedData.getObject\n+  }\n+\n+  def invalidate(key: CacheKey): Unit = synchronized {\n+    cache.remove(key) match {\n+      case Some(lst) => numTotalElements.add(-1 * lst.size)\n+      case None =>\n+    }\n+  }\n+\n+  def release(key: CacheKey, fetchedData: FetchedData): Unit = synchronized {\n+    cache.get(key) match {\n+      case Some(fetchedDataList) =>\n+        val cachedFetchedDataOption = fetchedDataList.find { p =>\n+          p.inUse && p.getObject == fetchedData\n+        }\n+\n+        if (cachedFetchedDataOption.isDefined) {\n+          val cachedFetchedData = cachedFetchedDataOption.get\n+          cachedFetchedData.inUse = false\n+          cachedFetchedData.lastReleasedTimestamp = System.currentTimeMillis()\n+        }\n+\n+      case None => logWarning(s\"No matching data in pool for $fetchedData in key $key. \" +\n+        \"It might be released before, or it was not a part of pool.\")\n+    }\n+  }\n+\n+  def shutdown(): Unit = {\n+    executorService.shutdownNow()\n+  }\n+\n+  def reset(): Unit = synchronized {\n+    scheduled.cancel(true)\n+\n+    cache.clear()\n+    numTotalElements.reset()\n+    numCreatedFetchedData.reset()\n+\n+    scheduled = startEvictorThread()\n+  }\n+\n+  private def removeIdleFetchedData(): Unit = synchronized {\n+    val timestamp = System.currentTimeMillis()\n+    val maxAllowedIdleTimestamp = timestamp - minEvictableIdleTimeMillis\n+    cache.values.foreach { p: CachedFetchedDataList =>\n+      val idles = p.filter(q => !q.inUse && q.lastReleasedTimestamp < maxAllowedIdleTimestamp)\n+      val lstSize = p.size\n+      idles.foreach(idle => p -= idle)\n+      numTotalElements.add(-1 * (lstSize - p.size))\n+    }\n+  }\n+}\n+\n+private[kafka010] object FetchedDataPool {\n+  private[kafka010] case class CachedFetchedData(fetchedData: FetchedData) {\n+    var lastReleasedTimestamp: Long = Long.MaxValue\n+    var lastAcquiredTimestamp: Long = Long.MinValue\n+    var inUse: Boolean = false\n+\n+    def getObject: FetchedData = fetchedData\n+  }\n+\n+  private object CachedFetchedData {\n+    def empty(): CachedFetchedData = {\n+      val emptyData = FetchedData(\n+        ju.Collections.emptyListIterator[ConsumerRecord[Array[Byte], Array[Byte]]],\n+        UNKNOWN_OFFSET,\n+        UNKNOWN_OFFSET)\n+\n+      CachedFetchedData(emptyData)\n+    }\n+  }\n+\n+  private[kafka010] type CachedFetchedDataList = mutable.ListBuffer[CachedFetchedData]\n+\n+  val CONFIG_NAME_PREFIX = \"spark.sql.kafkaFetchedDataCache.\""
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Ah I missed it. I'll address. Thanks for reminding.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-26T19:26:59Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+      DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+\n+    val evictorThreadInterval = conf.getLong(\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+      DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private val executorService = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"kafka-fetched-data-cache-evictor\")\n+\n+  private def startEvictorThread(): ScheduledFuture[_] = {\n+    executorService.scheduleAtFixedRate(new Runnable {\n+      override def run(): Unit = {\n+        try {\n+          removeIdleFetchedData()\n+        } catch {\n+          case NonFatal(e) =>\n+            logWarning(\"Exception occurred while removing idle fetched data.\", e)\n+        }\n+      }\n+    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\n+  }\n+\n+  private var scheduled = startEvictorThread()\n+\n+  private val numCreatedFetchedData = new LongAdder()\n+  private val numTotalElements = new LongAdder()\n+\n+  def getNumCreated: Long = numCreatedFetchedData.sum()\n+  def getNumTotal: Long = numTotalElements.sum()\n+\n+  def acquire(key: CacheKey, desiredStartOffset: Long): FetchedData = synchronized {\n+    val fetchedDataList = cache.getOrElseUpdate(key, new CachedFetchedDataList())\n+\n+    val cachedFetchedDataOption = fetchedDataList.find { p =>\n+      !p.inUse && p.getObject.nextOffsetInFetchedData == desiredStartOffset\n+    }\n+\n+    var cachedFetchedData: CachedFetchedData = null\n+    if (cachedFetchedDataOption.isDefined) {\n+      cachedFetchedData = cachedFetchedDataOption.get\n+    } else {\n+      cachedFetchedData = CachedFetchedData.empty()\n+      fetchedDataList += cachedFetchedData\n+\n+      numCreatedFetchedData.increment()\n+      numTotalElements.increment()\n+    }\n+\n+    cachedFetchedData.lastAcquiredTimestamp = System.currentTimeMillis()\n+    cachedFetchedData.inUse = true\n+\n+    cachedFetchedData.getObject\n+  }\n+\n+  def invalidate(key: CacheKey): Unit = synchronized {\n+    cache.remove(key) match {\n+      case Some(lst) => numTotalElements.add(-1 * lst.size)\n+      case None =>\n+    }\n+  }\n+\n+  def release(key: CacheKey, fetchedData: FetchedData): Unit = synchronized {\n+    cache.get(key) match {\n+      case Some(fetchedDataList) =>\n+        val cachedFetchedDataOption = fetchedDataList.find { p =>\n+          p.inUse && p.getObject == fetchedData\n+        }\n+\n+        if (cachedFetchedDataOption.isDefined) {\n+          val cachedFetchedData = cachedFetchedDataOption.get\n+          cachedFetchedData.inUse = false\n+          cachedFetchedData.lastReleasedTimestamp = System.currentTimeMillis()\n+        }\n+\n+      case None => logWarning(s\"No matching data in pool for $fetchedData in key $key. \" +\n+        \"It might be released before, or it was not a part of pool.\")\n+    }\n+  }\n+\n+  def shutdown(): Unit = {\n+    executorService.shutdownNow()\n+  }\n+\n+  def reset(): Unit = synchronized {\n+    scheduled.cancel(true)\n+\n+    cache.clear()\n+    numTotalElements.reset()\n+    numCreatedFetchedData.reset()\n+\n+    scheduled = startEvictorThread()\n+  }\n+\n+  private def removeIdleFetchedData(): Unit = synchronized {\n+    val timestamp = System.currentTimeMillis()\n+    val maxAllowedIdleTimestamp = timestamp - minEvictableIdleTimeMillis\n+    cache.values.foreach { p: CachedFetchedDataList =>\n+      val idles = p.filter(q => !q.inUse && q.lastReleasedTimestamp < maxAllowedIdleTimestamp)\n+      val lstSize = p.size\n+      idles.foreach(idle => p -= idle)\n+      numTotalElements.add(-1 * (lstSize - p.size))\n+    }\n+  }\n+}\n+\n+private[kafka010] object FetchedDataPool {\n+  private[kafka010] case class CachedFetchedData(fetchedData: FetchedData) {\n+    var lastReleasedTimestamp: Long = Long.MaxValue\n+    var lastAcquiredTimestamp: Long = Long.MinValue\n+    var inUse: Boolean = false\n+\n+    def getObject: FetchedData = fetchedData\n+  }\n+\n+  private object CachedFetchedData {\n+    def empty(): CachedFetchedData = {\n+      val emptyData = FetchedData(\n+        ju.Collections.emptyListIterator[ConsumerRecord[Array[Byte], Array[Byte]]],\n+        UNKNOWN_OFFSET,\n+        UNKNOWN_OFFSET)\n+\n+      CachedFetchedData(emptyData)\n+    }\n+  }\n+\n+  private[kafka010] type CachedFetchedDataList = mutable.ListBuffer[CachedFetchedData]\n+\n+  val CONFIG_NAME_PREFIX = \"spark.sql.kafkaFetchedDataCache.\""
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "You could just use `Utils.tryLogNonFatalError` here. The change for `logWarning` to `logError` isn't really a problem here.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-26T18:01:32Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+      DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+\n+    val evictorThreadInterval = conf.getLong(\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+      DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private val executorService = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"kafka-fetched-data-cache-evictor\")\n+\n+  private def startEvictorThread(): ScheduledFuture[_] = {\n+    executorService.scheduleAtFixedRate(new Runnable {\n+      override def run(): Unit = {"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: we generally don't use `get` in getters in the Scala APIs.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-26T18:02:06Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+      DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+\n+    val evictorThreadInterval = conf.getLong(\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+      DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private val executorService = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"kafka-fetched-data-cache-evictor\")\n+\n+  private def startEvictorThread(): ScheduledFuture[_] = {\n+    executorService.scheduleAtFixedRate(new Runnable {\n+      override def run(): Unit = {\n+        try {\n+          removeIdleFetchedData()\n+        } catch {\n+          case NonFatal(e) =>\n+            logWarning(\"Exception occurred while removing idle fetched data.\", e)\n+        }\n+      }\n+    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\n+  }\n+\n+  private var scheduled = startEvictorThread()\n+\n+  private val numCreatedFetchedData = new LongAdder()\n+  private val numTotalElements = new LongAdder()\n+\n+  def getNumCreated: Long = numCreatedFetchedData.sum()"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: move log statement to next line",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-26T18:06:33Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+      DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+\n+    val evictorThreadInterval = conf.getLong(\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+      DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private val executorService = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"kafka-fetched-data-cache-evictor\")\n+\n+  private def startEvictorThread(): ScheduledFuture[_] = {\n+    executorService.scheduleAtFixedRate(new Runnable {\n+      override def run(): Unit = {\n+        try {\n+          removeIdleFetchedData()\n+        } catch {\n+          case NonFatal(e) =>\n+            logWarning(\"Exception occurred while removing idle fetched data.\", e)\n+        }\n+      }\n+    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\n+  }\n+\n+  private var scheduled = startEvictorThread()\n+\n+  private val numCreatedFetchedData = new LongAdder()\n+  private val numTotalElements = new LongAdder()\n+\n+  def getNumCreated: Long = numCreatedFetchedData.sum()\n+  def getNumTotal: Long = numTotalElements.sum()\n+\n+  def acquire(key: CacheKey, desiredStartOffset: Long): FetchedData = synchronized {\n+    val fetchedDataList = cache.getOrElseUpdate(key, new CachedFetchedDataList())\n+\n+    val cachedFetchedDataOption = fetchedDataList.find { p =>\n+      !p.inUse && p.getObject.nextOffsetInFetchedData == desiredStartOffset\n+    }\n+\n+    var cachedFetchedData: CachedFetchedData = null\n+    if (cachedFetchedDataOption.isDefined) {\n+      cachedFetchedData = cachedFetchedDataOption.get\n+    } else {\n+      cachedFetchedData = CachedFetchedData.empty()\n+      fetchedDataList += cachedFetchedData\n+\n+      numCreatedFetchedData.increment()\n+      numTotalElements.increment()\n+    }\n+\n+    cachedFetchedData.lastAcquiredTimestamp = System.currentTimeMillis()\n+    cachedFetchedData.inUse = true\n+\n+    cachedFetchedData.getObject\n+  }\n+\n+  def invalidate(key: CacheKey): Unit = synchronized {\n+    cache.remove(key) match {\n+      case Some(lst) => numTotalElements.add(-1 * lst.size)\n+      case None =>\n+    }\n+  }\n+\n+  def release(key: CacheKey, fetchedData: FetchedData): Unit = synchronized {\n+    cache.get(key) match {\n+      case Some(fetchedDataList) =>\n+        val cachedFetchedDataOption = fetchedDataList.find { p =>\n+          p.inUse && p.getObject == fetchedData\n+        }\n+\n+        if (cachedFetchedDataOption.isDefined) {\n+          val cachedFetchedData = cachedFetchedDataOption.get\n+          cachedFetchedData.inUse = false\n+          cachedFetchedData.lastReleasedTimestamp = System.currentTimeMillis()\n+        }\n+\n+      case None => logWarning(s\"No matching data in pool for $fetchedData in key $key. \" +"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Is it an expected case to try to release an idle entry, or one that is not known? I'm wondering whether there should be an assert or at least a log message, since as far as I can tell that would mean some code is trying to reuse a cached object after it's been released.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-26T18:08:54Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+      DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+\n+    val evictorThreadInterval = conf.getLong(\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+      DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private val executorService = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"kafka-fetched-data-cache-evictor\")\n+\n+  private def startEvictorThread(): ScheduledFuture[_] = {\n+    executorService.scheduleAtFixedRate(new Runnable {\n+      override def run(): Unit = {\n+        try {\n+          removeIdleFetchedData()\n+        } catch {\n+          case NonFatal(e) =>\n+            logWarning(\"Exception occurred while removing idle fetched data.\", e)\n+        }\n+      }\n+    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\n+  }\n+\n+  private var scheduled = startEvictorThread()\n+\n+  private val numCreatedFetchedData = new LongAdder()\n+  private val numTotalElements = new LongAdder()\n+\n+  def getNumCreated: Long = numCreatedFetchedData.sum()\n+  def getNumTotal: Long = numTotalElements.sum()\n+\n+  def acquire(key: CacheKey, desiredStartOffset: Long): FetchedData = synchronized {\n+    val fetchedDataList = cache.getOrElseUpdate(key, new CachedFetchedDataList())\n+\n+    val cachedFetchedDataOption = fetchedDataList.find { p =>\n+      !p.inUse && p.getObject.nextOffsetInFetchedData == desiredStartOffset\n+    }\n+\n+    var cachedFetchedData: CachedFetchedData = null\n+    if (cachedFetchedDataOption.isDefined) {\n+      cachedFetchedData = cachedFetchedDataOption.get\n+    } else {\n+      cachedFetchedData = CachedFetchedData.empty()\n+      fetchedDataList += cachedFetchedData\n+\n+      numCreatedFetchedData.increment()\n+      numTotalElements.increment()\n+    }\n+\n+    cachedFetchedData.lastAcquiredTimestamp = System.currentTimeMillis()\n+    cachedFetchedData.inUse = true\n+\n+    cachedFetchedData.getObject\n+  }\n+\n+  def invalidate(key: CacheKey): Unit = synchronized {\n+    cache.remove(key) match {\n+      case Some(lst) => numTotalElements.add(-1 * lst.size)\n+      case None =>\n+    }\n+  }\n+\n+  def release(key: CacheKey, fetchedData: FetchedData): Unit = synchronized {\n+    cache.get(key) match {\n+      case Some(fetchedDataList) =>\n+        val cachedFetchedDataOption = fetchedDataList.find { p =>\n+          p.inUse && p.getObject == fetchedData",
    "line": 119
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Good point. Agreed it should be assertion or warn log message placed there. Let's add assertion and throw exception as commons-pool does it for now.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-26T19:42:02Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+      DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+\n+    val evictorThreadInterval = conf.getLong(\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+      DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private val executorService = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"kafka-fetched-data-cache-evictor\")\n+\n+  private def startEvictorThread(): ScheduledFuture[_] = {\n+    executorService.scheduleAtFixedRate(new Runnable {\n+      override def run(): Unit = {\n+        try {\n+          removeIdleFetchedData()\n+        } catch {\n+          case NonFatal(e) =>\n+            logWarning(\"Exception occurred while removing idle fetched data.\", e)\n+        }\n+      }\n+    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\n+  }\n+\n+  private var scheduled = startEvictorThread()\n+\n+  private val numCreatedFetchedData = new LongAdder()\n+  private val numTotalElements = new LongAdder()\n+\n+  def getNumCreated: Long = numCreatedFetchedData.sum()\n+  def getNumTotal: Long = numTotalElements.sum()\n+\n+  def acquire(key: CacheKey, desiredStartOffset: Long): FetchedData = synchronized {\n+    val fetchedDataList = cache.getOrElseUpdate(key, new CachedFetchedDataList())\n+\n+    val cachedFetchedDataOption = fetchedDataList.find { p =>\n+      !p.inUse && p.getObject.nextOffsetInFetchedData == desiredStartOffset\n+    }\n+\n+    var cachedFetchedData: CachedFetchedData = null\n+    if (cachedFetchedDataOption.isDefined) {\n+      cachedFetchedData = cachedFetchedDataOption.get\n+    } else {\n+      cachedFetchedData = CachedFetchedData.empty()\n+      fetchedDataList += cachedFetchedData\n+\n+      numCreatedFetchedData.increment()\n+      numTotalElements.increment()\n+    }\n+\n+    cachedFetchedData.lastAcquiredTimestamp = System.currentTimeMillis()\n+    cachedFetchedData.inUse = true\n+\n+    cachedFetchedData.getObject\n+  }\n+\n+  def invalidate(key: CacheKey): Unit = synchronized {\n+    cache.remove(key) match {\n+      case Some(lst) => numTotalElements.add(-1 * lst.size)\n+      case None =>\n+    }\n+  }\n+\n+  def release(key: CacheKey, fetchedData: FetchedData): Unit = synchronized {\n+    cache.get(key) match {\n+      case Some(fetchedDataList) =>\n+        val cachedFetchedDataOption = fetchedDataList.find { p =>\n+          p.inUse && p.getObject == fetchedData",
    "line": 119
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Hmm... that should be same level of warning whether key itself doesn't exist or data is not in pool for the key. I'll deal with logWarning for now.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-26T20:38:59Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+      DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+\n+    val evictorThreadInterval = conf.getLong(\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+      DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private val executorService = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"kafka-fetched-data-cache-evictor\")\n+\n+  private def startEvictorThread(): ScheduledFuture[_] = {\n+    executorService.scheduleAtFixedRate(new Runnable {\n+      override def run(): Unit = {\n+        try {\n+          removeIdleFetchedData()\n+        } catch {\n+          case NonFatal(e) =>\n+            logWarning(\"Exception occurred while removing idle fetched data.\", e)\n+        }\n+      }\n+    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\n+  }\n+\n+  private var scheduled = startEvictorThread()\n+\n+  private val numCreatedFetchedData = new LongAdder()\n+  private val numTotalElements = new LongAdder()\n+\n+  def getNumCreated: Long = numCreatedFetchedData.sum()\n+  def getNumTotal: Long = numTotalElements.sum()\n+\n+  def acquire(key: CacheKey, desiredStartOffset: Long): FetchedData = synchronized {\n+    val fetchedDataList = cache.getOrElseUpdate(key, new CachedFetchedDataList())\n+\n+    val cachedFetchedDataOption = fetchedDataList.find { p =>\n+      !p.inUse && p.getObject.nextOffsetInFetchedData == desiredStartOffset\n+    }\n+\n+    var cachedFetchedData: CachedFetchedData = null\n+    if (cachedFetchedDataOption.isDefined) {\n+      cachedFetchedData = cachedFetchedDataOption.get\n+    } else {\n+      cachedFetchedData = CachedFetchedData.empty()\n+      fetchedDataList += cachedFetchedData\n+\n+      numCreatedFetchedData.increment()\n+      numTotalElements.increment()\n+    }\n+\n+    cachedFetchedData.lastAcquiredTimestamp = System.currentTimeMillis()\n+    cachedFetchedData.inUse = true\n+\n+    cachedFetchedData.getObject\n+  }\n+\n+  def invalidate(key: CacheKey): Unit = synchronized {\n+    cache.remove(key) match {\n+      case Some(lst) => numTotalElements.add(-1 * lst.size)\n+      case None =>\n+    }\n+  }\n+\n+  def release(key: CacheKey, fetchedData: FetchedData): Unit = synchronized {\n+    cache.get(key) match {\n+      case Some(fetchedDataList) =>\n+        val cachedFetchedDataOption = fetchedDataList.find { p =>\n+          p.inUse && p.getObject == fetchedData",
    "line": 119
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Maybe use `ThreadUtils.shutdown`.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-26T18:10:01Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+      DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+\n+    val evictorThreadInterval = conf.getLong(\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+      DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private val executorService = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"kafka-fetched-data-cache-evictor\")\n+\n+  private def startEvictorThread(): ScheduledFuture[_] = {\n+    executorService.scheduleAtFixedRate(new Runnable {\n+      override def run(): Unit = {\n+        try {\n+          removeIdleFetchedData()\n+        } catch {\n+          case NonFatal(e) =>\n+            logWarning(\"Exception occurred while removing idle fetched data.\", e)\n+        }\n+      }\n+    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\n+  }\n+\n+  private var scheduled = startEvictorThread()\n+\n+  private val numCreatedFetchedData = new LongAdder()\n+  private val numTotalElements = new LongAdder()\n+\n+  def getNumCreated: Long = numCreatedFetchedData.sum()\n+  def getNumTotal: Long = numTotalElements.sum()\n+\n+  def acquire(key: CacheKey, desiredStartOffset: Long): FetchedData = synchronized {\n+    val fetchedDataList = cache.getOrElseUpdate(key, new CachedFetchedDataList())\n+\n+    val cachedFetchedDataOption = fetchedDataList.find { p =>\n+      !p.inUse && p.getObject.nextOffsetInFetchedData == desiredStartOffset\n+    }\n+\n+    var cachedFetchedData: CachedFetchedData = null\n+    if (cachedFetchedDataOption.isDefined) {\n+      cachedFetchedData = cachedFetchedDataOption.get\n+    } else {\n+      cachedFetchedData = CachedFetchedData.empty()\n+      fetchedDataList += cachedFetchedData\n+\n+      numCreatedFetchedData.increment()\n+      numTotalElements.increment()\n+    }\n+\n+    cachedFetchedData.lastAcquiredTimestamp = System.currentTimeMillis()\n+    cachedFetchedData.inUse = true\n+\n+    cachedFetchedData.getObject\n+  }\n+\n+  def invalidate(key: CacheKey): Unit = synchronized {\n+    cache.remove(key) match {\n+      case Some(lst) => numTotalElements.add(-1 * lst.size)\n+      case None =>\n+    }\n+  }\n+\n+  def release(key: CacheKey, fetchedData: FetchedData): Unit = synchronized {\n+    cache.get(key) match {\n+      case Some(fetchedDataList) =>\n+        val cachedFetchedDataOption = fetchedDataList.find { p =>\n+          p.inUse && p.getObject == fetchedData\n+        }\n+\n+        if (cachedFetchedDataOption.isDefined) {\n+          val cachedFetchedData = cachedFetchedDataOption.get\n+          cachedFetchedData.inUse = false\n+          cachedFetchedData.lastReleasedTimestamp = System.currentTimeMillis()\n+        }\n+\n+      case None => logWarning(s\"No matching data in pool for $fetchedData in key $key. \" +\n+        \"It might be released before, or it was not a part of pool.\")\n+    }\n+  }\n+\n+  def shutdown(): Unit = {\n+    executorService.shutdownNow()"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Since these timestamps are all being used internally and not exposed in metrics or anything, as far as I can tell, you should probably be using `System.nanoTime`.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-26T18:11:02Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+      DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+\n+    val evictorThreadInterval = conf.getLong(\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+      DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private val executorService = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"kafka-fetched-data-cache-evictor\")\n+\n+  private def startEvictorThread(): ScheduledFuture[_] = {\n+    executorService.scheduleAtFixedRate(new Runnable {\n+      override def run(): Unit = {\n+        try {\n+          removeIdleFetchedData()\n+        } catch {\n+          case NonFatal(e) =>\n+            logWarning(\"Exception occurred while removing idle fetched data.\", e)\n+        }\n+      }\n+    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\n+  }\n+\n+  private var scheduled = startEvictorThread()\n+\n+  private val numCreatedFetchedData = new LongAdder()\n+  private val numTotalElements = new LongAdder()\n+\n+  def getNumCreated: Long = numCreatedFetchedData.sum()\n+  def getNumTotal: Long = numTotalElements.sum()\n+\n+  def acquire(key: CacheKey, desiredStartOffset: Long): FetchedData = synchronized {\n+    val fetchedDataList = cache.getOrElseUpdate(key, new CachedFetchedDataList())\n+\n+    val cachedFetchedDataOption = fetchedDataList.find { p =>\n+      !p.inUse && p.getObject.nextOffsetInFetchedData == desiredStartOffset\n+    }\n+\n+    var cachedFetchedData: CachedFetchedData = null\n+    if (cachedFetchedDataOption.isDefined) {\n+      cachedFetchedData = cachedFetchedDataOption.get\n+    } else {\n+      cachedFetchedData = CachedFetchedData.empty()\n+      fetchedDataList += cachedFetchedData\n+\n+      numCreatedFetchedData.increment()\n+      numTotalElements.increment()\n+    }\n+\n+    cachedFetchedData.lastAcquiredTimestamp = System.currentTimeMillis()\n+    cachedFetchedData.inUse = true\n+\n+    cachedFetchedData.getObject\n+  }\n+\n+  def invalidate(key: CacheKey): Unit = synchronized {\n+    cache.remove(key) match {\n+      case Some(lst) => numTotalElements.add(-1 * lst.size)\n+      case None =>\n+    }\n+  }\n+\n+  def release(key: CacheKey, fetchedData: FetchedData): Unit = synchronized {\n+    cache.get(key) match {\n+      case Some(fetchedDataList) =>\n+        val cachedFetchedDataOption = fetchedDataList.find { p =>\n+          p.inUse && p.getObject == fetchedData\n+        }\n+\n+        if (cachedFetchedDataOption.isDefined) {\n+          val cachedFetchedData = cachedFetchedDataOption.get\n+          cachedFetchedData.inUse = false\n+          cachedFetchedData.lastReleasedTimestamp = System.currentTimeMillis()\n+        }\n+\n+      case None => logWarning(s\"No matching data in pool for $fetchedData in key $key. \" +\n+        \"It might be released before, or it was not a part of pool.\")\n+    }\n+  }\n+\n+  def shutdown(): Unit = {\n+    executorService.shutdownNow()\n+  }\n+\n+  def reset(): Unit = synchronized {\n+    scheduled.cancel(true)\n+\n+    cache.clear()\n+    numTotalElements.reset()\n+    numCreatedFetchedData.reset()\n+\n+    scheduled = startEvictorThread()\n+  }\n+\n+  private def removeIdleFetchedData(): Unit = synchronized {\n+    val timestamp = System.currentTimeMillis()"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: `.filter { q => ... }`\r\n\r\nAlso below in the `foreach`.\r\n\r\nI'd also call the variable `expired` since there may be \"idle\" entries that will not be removed.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-26T18:13:18Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+      DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+\n+    val evictorThreadInterval = conf.getLong(\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+      DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private val executorService = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"kafka-fetched-data-cache-evictor\")\n+\n+  private def startEvictorThread(): ScheduledFuture[_] = {\n+    executorService.scheduleAtFixedRate(new Runnable {\n+      override def run(): Unit = {\n+        try {\n+          removeIdleFetchedData()\n+        } catch {\n+          case NonFatal(e) =>\n+            logWarning(\"Exception occurred while removing idle fetched data.\", e)\n+        }\n+      }\n+    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\n+  }\n+\n+  private var scheduled = startEvictorThread()\n+\n+  private val numCreatedFetchedData = new LongAdder()\n+  private val numTotalElements = new LongAdder()\n+\n+  def getNumCreated: Long = numCreatedFetchedData.sum()\n+  def getNumTotal: Long = numTotalElements.sum()\n+\n+  def acquire(key: CacheKey, desiredStartOffset: Long): FetchedData = synchronized {\n+    val fetchedDataList = cache.getOrElseUpdate(key, new CachedFetchedDataList())\n+\n+    val cachedFetchedDataOption = fetchedDataList.find { p =>\n+      !p.inUse && p.getObject.nextOffsetInFetchedData == desiredStartOffset\n+    }\n+\n+    var cachedFetchedData: CachedFetchedData = null\n+    if (cachedFetchedDataOption.isDefined) {\n+      cachedFetchedData = cachedFetchedDataOption.get\n+    } else {\n+      cachedFetchedData = CachedFetchedData.empty()\n+      fetchedDataList += cachedFetchedData\n+\n+      numCreatedFetchedData.increment()\n+      numTotalElements.increment()\n+    }\n+\n+    cachedFetchedData.lastAcquiredTimestamp = System.currentTimeMillis()\n+    cachedFetchedData.inUse = true\n+\n+    cachedFetchedData.getObject\n+  }\n+\n+  def invalidate(key: CacheKey): Unit = synchronized {\n+    cache.remove(key) match {\n+      case Some(lst) => numTotalElements.add(-1 * lst.size)\n+      case None =>\n+    }\n+  }\n+\n+  def release(key: CacheKey, fetchedData: FetchedData): Unit = synchronized {\n+    cache.get(key) match {\n+      case Some(fetchedDataList) =>\n+        val cachedFetchedDataOption = fetchedDataList.find { p =>\n+          p.inUse && p.getObject == fetchedData\n+        }\n+\n+        if (cachedFetchedDataOption.isDefined) {\n+          val cachedFetchedData = cachedFetchedDataOption.get\n+          cachedFetchedData.inUse = false\n+          cachedFetchedData.lastReleasedTimestamp = System.currentTimeMillis()\n+        }\n+\n+      case None => logWarning(s\"No matching data in pool for $fetchedData in key $key. \" +\n+        \"It might be released before, or it was not a part of pool.\")\n+    }\n+  }\n+\n+  def shutdown(): Unit = {\n+    executorService.shutdownNow()\n+  }\n+\n+  def reset(): Unit = synchronized {\n+    scheduled.cancel(true)\n+\n+    cache.clear()\n+    numTotalElements.reset()\n+    numCreatedFetchedData.reset()\n+\n+    scheduled = startEvictorThread()\n+  }\n+\n+  private def removeIdleFetchedData(): Unit = synchronized {\n+    val timestamp = System.currentTimeMillis()\n+    val maxAllowedIdleTimestamp = timestamp - minEvictableIdleTimeMillis\n+    cache.values.foreach { p: CachedFetchedDataList =>\n+      val idles = p.filter(q => !q.inUse && q.lastReleasedTimestamp < maxAllowedIdleTimestamp)"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "No need for this variable.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-26T18:15:07Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+      DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+\n+    val evictorThreadInterval = conf.getLong(\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+      DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private val executorService = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"kafka-fetched-data-cache-evictor\")\n+\n+  private def startEvictorThread(): ScheduledFuture[_] = {\n+    executorService.scheduleAtFixedRate(new Runnable {\n+      override def run(): Unit = {\n+        try {\n+          removeIdleFetchedData()\n+        } catch {\n+          case NonFatal(e) =>\n+            logWarning(\"Exception occurred while removing idle fetched data.\", e)\n+        }\n+      }\n+    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\n+  }\n+\n+  private var scheduled = startEvictorThread()\n+\n+  private val numCreatedFetchedData = new LongAdder()\n+  private val numTotalElements = new LongAdder()\n+\n+  def getNumCreated: Long = numCreatedFetchedData.sum()\n+  def getNumTotal: Long = numTotalElements.sum()\n+\n+  def acquire(key: CacheKey, desiredStartOffset: Long): FetchedData = synchronized {\n+    val fetchedDataList = cache.getOrElseUpdate(key, new CachedFetchedDataList())\n+\n+    val cachedFetchedDataOption = fetchedDataList.find { p =>\n+      !p.inUse && p.getObject.nextOffsetInFetchedData == desiredStartOffset\n+    }\n+\n+    var cachedFetchedData: CachedFetchedData = null\n+    if (cachedFetchedDataOption.isDefined) {\n+      cachedFetchedData = cachedFetchedDataOption.get\n+    } else {\n+      cachedFetchedData = CachedFetchedData.empty()\n+      fetchedDataList += cachedFetchedData\n+\n+      numCreatedFetchedData.increment()\n+      numTotalElements.increment()\n+    }\n+\n+    cachedFetchedData.lastAcquiredTimestamp = System.currentTimeMillis()\n+    cachedFetchedData.inUse = true\n+\n+    cachedFetchedData.getObject\n+  }\n+\n+  def invalidate(key: CacheKey): Unit = synchronized {\n+    cache.remove(key) match {\n+      case Some(lst) => numTotalElements.add(-1 * lst.size)\n+      case None =>\n+    }\n+  }\n+\n+  def release(key: CacheKey, fetchedData: FetchedData): Unit = synchronized {\n+    cache.get(key) match {\n+      case Some(fetchedDataList) =>\n+        val cachedFetchedDataOption = fetchedDataList.find { p =>\n+          p.inUse && p.getObject == fetchedData\n+        }\n+\n+        if (cachedFetchedDataOption.isDefined) {\n+          val cachedFetchedData = cachedFetchedDataOption.get\n+          cachedFetchedData.inUse = false\n+          cachedFetchedData.lastReleasedTimestamp = System.currentTimeMillis()\n+        }\n+\n+      case None => logWarning(s\"No matching data in pool for $fetchedData in key $key. \" +\n+        \"It might be released before, or it was not a part of pool.\")\n+    }\n+  }\n+\n+  def shutdown(): Unit = {\n+    executorService.shutdownNow()\n+  }\n+\n+  def reset(): Unit = synchronized {\n+    scheduled.cancel(true)\n+\n+    cache.clear()\n+    numTotalElements.reset()\n+    numCreatedFetchedData.reset()\n+\n+    scheduled = startEvictorThread()\n+  }\n+\n+  private def removeIdleFetchedData(): Unit = synchronized {\n+    val timestamp = System.currentTimeMillis()\n+    val maxAllowedIdleTimestamp = timestamp - minEvictableIdleTimeMillis\n+    cache.values.foreach { p: CachedFetchedDataList =>\n+      val idles = p.filter(q => !q.inUse && q.lastReleasedTimestamp < maxAllowedIdleTimestamp)\n+      val lstSize = p.size"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "hmm... shouldn't this be just `-1 * idles.size` (a.k.a. subtract the number of elements being removed)?",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-26T18:15:42Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+      DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+\n+    val evictorThreadInterval = conf.getLong(\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+      DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private val executorService = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"kafka-fetched-data-cache-evictor\")\n+\n+  private def startEvictorThread(): ScheduledFuture[_] = {\n+    executorService.scheduleAtFixedRate(new Runnable {\n+      override def run(): Unit = {\n+        try {\n+          removeIdleFetchedData()\n+        } catch {\n+          case NonFatal(e) =>\n+            logWarning(\"Exception occurred while removing idle fetched data.\", e)\n+        }\n+      }\n+    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\n+  }\n+\n+  private var scheduled = startEvictorThread()\n+\n+  private val numCreatedFetchedData = new LongAdder()\n+  private val numTotalElements = new LongAdder()\n+\n+  def getNumCreated: Long = numCreatedFetchedData.sum()\n+  def getNumTotal: Long = numTotalElements.sum()\n+\n+  def acquire(key: CacheKey, desiredStartOffset: Long): FetchedData = synchronized {\n+    val fetchedDataList = cache.getOrElseUpdate(key, new CachedFetchedDataList())\n+\n+    val cachedFetchedDataOption = fetchedDataList.find { p =>\n+      !p.inUse && p.getObject.nextOffsetInFetchedData == desiredStartOffset\n+    }\n+\n+    var cachedFetchedData: CachedFetchedData = null\n+    if (cachedFetchedDataOption.isDefined) {\n+      cachedFetchedData = cachedFetchedDataOption.get\n+    } else {\n+      cachedFetchedData = CachedFetchedData.empty()\n+      fetchedDataList += cachedFetchedData\n+\n+      numCreatedFetchedData.increment()\n+      numTotalElements.increment()\n+    }\n+\n+    cachedFetchedData.lastAcquiredTimestamp = System.currentTimeMillis()\n+    cachedFetchedData.inUse = true\n+\n+    cachedFetchedData.getObject\n+  }\n+\n+  def invalidate(key: CacheKey): Unit = synchronized {\n+    cache.remove(key) match {\n+      case Some(lst) => numTotalElements.add(-1 * lst.size)\n+      case None =>\n+    }\n+  }\n+\n+  def release(key: CacheKey, fetchedData: FetchedData): Unit = synchronized {\n+    cache.get(key) match {\n+      case Some(fetchedDataList) =>\n+        val cachedFetchedDataOption = fetchedDataList.find { p =>\n+          p.inUse && p.getObject == fetchedData\n+        }\n+\n+        if (cachedFetchedDataOption.isDefined) {\n+          val cachedFetchedData = cachedFetchedDataOption.get\n+          cachedFetchedData.inUse = false\n+          cachedFetchedData.lastReleasedTimestamp = System.currentTimeMillis()\n+        }\n+\n+      case None => logWarning(s\"No matching data in pool for $fetchedData in key $key. \" +\n+        \"It might be released before, or it was not a part of pool.\")\n+    }\n+  }\n+\n+  def shutdown(): Unit = {\n+    executorService.shutdownNow()\n+  }\n+\n+  def reset(): Unit = synchronized {\n+    scheduled.cancel(true)\n+\n+    cache.clear()\n+    numTotalElements.reset()\n+    numCreatedFetchedData.reset()\n+\n+    scheduled = startEvictorThread()\n+  }\n+\n+  private def removeIdleFetchedData(): Unit = synchronized {\n+    val timestamp = System.currentTimeMillis()\n+    val maxAllowedIdleTimestamp = timestamp - minEvictableIdleTimeMillis\n+    cache.values.foreach { p: CachedFetchedDataList =>\n+      val idles = p.filter(q => !q.inUse && q.lastReleasedTimestamp < maxAllowedIdleTimestamp)\n+      val lstSize = p.size\n+      idles.foreach(idle => p -= idle)\n+      numTotalElements.add(-1 * (lstSize - p.size))"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Maybe I was over-thinking and considered not possible case for `p -= idle` doesn't remove anything. As it's synchronized we don't expect the case. Thanks for the correction! Will fix.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-26T19:58:21Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+      DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+\n+    val evictorThreadInterval = conf.getLong(\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+      DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private val executorService = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"kafka-fetched-data-cache-evictor\")\n+\n+  private def startEvictorThread(): ScheduledFuture[_] = {\n+    executorService.scheduleAtFixedRate(new Runnable {\n+      override def run(): Unit = {\n+        try {\n+          removeIdleFetchedData()\n+        } catch {\n+          case NonFatal(e) =>\n+            logWarning(\"Exception occurred while removing idle fetched data.\", e)\n+        }\n+      }\n+    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\n+  }\n+\n+  private var scheduled = startEvictorThread()\n+\n+  private val numCreatedFetchedData = new LongAdder()\n+  private val numTotalElements = new LongAdder()\n+\n+  def getNumCreated: Long = numCreatedFetchedData.sum()\n+  def getNumTotal: Long = numTotalElements.sum()\n+\n+  def acquire(key: CacheKey, desiredStartOffset: Long): FetchedData = synchronized {\n+    val fetchedDataList = cache.getOrElseUpdate(key, new CachedFetchedDataList())\n+\n+    val cachedFetchedDataOption = fetchedDataList.find { p =>\n+      !p.inUse && p.getObject.nextOffsetInFetchedData == desiredStartOffset\n+    }\n+\n+    var cachedFetchedData: CachedFetchedData = null\n+    if (cachedFetchedDataOption.isDefined) {\n+      cachedFetchedData = cachedFetchedDataOption.get\n+    } else {\n+      cachedFetchedData = CachedFetchedData.empty()\n+      fetchedDataList += cachedFetchedData\n+\n+      numCreatedFetchedData.increment()\n+      numTotalElements.increment()\n+    }\n+\n+    cachedFetchedData.lastAcquiredTimestamp = System.currentTimeMillis()\n+    cachedFetchedData.inUse = true\n+\n+    cachedFetchedData.getObject\n+  }\n+\n+  def invalidate(key: CacheKey): Unit = synchronized {\n+    cache.remove(key) match {\n+      case Some(lst) => numTotalElements.add(-1 * lst.size)\n+      case None =>\n+    }\n+  }\n+\n+  def release(key: CacheKey, fetchedData: FetchedData): Unit = synchronized {\n+    cache.get(key) match {\n+      case Some(fetchedDataList) =>\n+        val cachedFetchedDataOption = fetchedDataList.find { p =>\n+          p.inUse && p.getObject == fetchedData\n+        }\n+\n+        if (cachedFetchedDataOption.isDefined) {\n+          val cachedFetchedData = cachedFetchedDataOption.get\n+          cachedFetchedData.inUse = false\n+          cachedFetchedData.lastReleasedTimestamp = System.currentTimeMillis()\n+        }\n+\n+      case None => logWarning(s\"No matching data in pool for $fetchedData in key $key. \" +\n+        \"It might be released before, or it was not a part of pool.\")\n+    }\n+  }\n+\n+  def shutdown(): Unit = {\n+    executorService.shutdownNow()\n+  }\n+\n+  def reset(): Unit = synchronized {\n+    scheduled.cancel(true)\n+\n+    cache.clear()\n+    numTotalElements.reset()\n+    numCreatedFetchedData.reset()\n+\n+    scheduled = startEvictorThread()\n+  }\n+\n+  private def removeIdleFetchedData(): Unit = synchronized {\n+    val timestamp = System.currentTimeMillis()\n+    val maxAllowedIdleTimestamp = timestamp - minEvictableIdleTimeMillis\n+    cache.values.foreach { p: CachedFetchedDataList =>\n+      val idles = p.filter(q => !q.inUse && q.lastReleasedTimestamp < maxAllowedIdleTimestamp)\n+      val lstSize = p.size\n+      idles.foreach(idle => p -= idle)\n+      numTotalElements.add(-1 * (lstSize - p.size))"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Is this method useful? This is not a builder, so just calling the constructor directly should be enough?",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-26T18:19:08Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+      DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+\n+    val evictorThreadInterval = conf.getLong(\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+      DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private val executorService = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"kafka-fetched-data-cache-evictor\")\n+\n+  private def startEvictorThread(): ScheduledFuture[_] = {\n+    executorService.scheduleAtFixedRate(new Runnable {\n+      override def run(): Unit = {\n+        try {\n+          removeIdleFetchedData()\n+        } catch {\n+          case NonFatal(e) =>\n+            logWarning(\"Exception occurred while removing idle fetched data.\", e)\n+        }\n+      }\n+    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\n+  }\n+\n+  private var scheduled = startEvictorThread()\n+\n+  private val numCreatedFetchedData = new LongAdder()\n+  private val numTotalElements = new LongAdder()\n+\n+  def getNumCreated: Long = numCreatedFetchedData.sum()\n+  def getNumTotal: Long = numTotalElements.sum()\n+\n+  def acquire(key: CacheKey, desiredStartOffset: Long): FetchedData = synchronized {\n+    val fetchedDataList = cache.getOrElseUpdate(key, new CachedFetchedDataList())\n+\n+    val cachedFetchedDataOption = fetchedDataList.find { p =>\n+      !p.inUse && p.getObject.nextOffsetInFetchedData == desiredStartOffset\n+    }\n+\n+    var cachedFetchedData: CachedFetchedData = null\n+    if (cachedFetchedDataOption.isDefined) {\n+      cachedFetchedData = cachedFetchedDataOption.get\n+    } else {\n+      cachedFetchedData = CachedFetchedData.empty()\n+      fetchedDataList += cachedFetchedData\n+\n+      numCreatedFetchedData.increment()\n+      numTotalElements.increment()\n+    }\n+\n+    cachedFetchedData.lastAcquiredTimestamp = System.currentTimeMillis()\n+    cachedFetchedData.inUse = true\n+\n+    cachedFetchedData.getObject\n+  }\n+\n+  def invalidate(key: CacheKey): Unit = synchronized {\n+    cache.remove(key) match {\n+      case Some(lst) => numTotalElements.add(-1 * lst.size)\n+      case None =>\n+    }\n+  }\n+\n+  def release(key: CacheKey, fetchedData: FetchedData): Unit = synchronized {\n+    cache.get(key) match {\n+      case Some(fetchedDataList) =>\n+        val cachedFetchedDataOption = fetchedDataList.find { p =>\n+          p.inUse && p.getObject == fetchedData\n+        }\n+\n+        if (cachedFetchedDataOption.isDefined) {\n+          val cachedFetchedData = cachedFetchedDataOption.get\n+          cachedFetchedData.inUse = false\n+          cachedFetchedData.lastReleasedTimestamp = System.currentTimeMillis()\n+        }\n+\n+      case None => logWarning(s\"No matching data in pool for $fetchedData in key $key. \" +\n+        \"It might be released before, or it was not a part of pool.\")\n+    }\n+  }\n+\n+  def shutdown(): Unit = {\n+    executorService.shutdownNow()\n+  }\n+\n+  def reset(): Unit = synchronized {\n+    scheduled.cancel(true)\n+\n+    cache.clear()\n+    numTotalElements.reset()\n+    numCreatedFetchedData.reset()\n+\n+    scheduled = startEvictorThread()\n+  }\n+\n+  private def removeIdleFetchedData(): Unit = synchronized {\n+    val timestamp = System.currentTimeMillis()\n+    val maxAllowedIdleTimestamp = timestamp - minEvictableIdleTimeMillis\n+    cache.values.foreach { p: CachedFetchedDataList =>\n+      val idles = p.filter(q => !q.inUse && q.lastReleasedTimestamp < maxAllowedIdleTimestamp)\n+      val lstSize = p.size\n+      idles.foreach(idle => p -= idle)\n+      numTotalElements.add(-1 * (lstSize - p.size))\n+    }\n+  }\n+}\n+\n+private[kafka010] object FetchedDataPool {\n+  private[kafka010] case class CachedFetchedData(fetchedData: FetchedData) {\n+    var lastReleasedTimestamp: Long = Long.MaxValue\n+    var lastAcquiredTimestamp: Long = Long.MinValue\n+    var inUse: Boolean = false\n+\n+    def getObject: FetchedData = fetchedData\n+  }\n+\n+  private object CachedFetchedData {\n+    def empty(): CachedFetchedData = {\n+      val emptyData = FetchedData(\n+        ju.Collections.emptyListIterator[ConsumerRecord[Array[Byte], Array[Byte]]],\n+        UNKNOWN_OFFSET,\n+        UNKNOWN_OFFSET)\n+\n+      CachedFetchedData(emptyData)\n+    }\n+  }\n+\n+  private[kafka010] type CachedFetchedDataList = mutable.ListBuffer[CachedFetchedData]\n+\n+  val CONFIG_NAME_PREFIX = \"spark.sql.kafkaFetchedDataCache.\"\n+  val CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS = CONFIG_NAME_PREFIX +\n+    \"minEvictableIdleTimeMillis\"\n+  val CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS = CONFIG_NAME_PREFIX +\n+    \"evictorThreadRunIntervalMillis\"\n+\n+  val DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS = 10 * 60 * 1000 // 10 minutes\n+  val DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS = 5 * 60 * 1000 // 3 minutes\n+\n+  def build: FetchedDataPool = new FetchedDataPool()"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "It's for providing consistency about the way to create instance between `InternalKafkaConsumerPool` and `FetchedDataPool`, so both could be created via `<object>.build`. If you prefer to remove it please let me know so I can remove it.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-26T20:01:59Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+      DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+\n+    val evictorThreadInterval = conf.getLong(\n+      CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+      DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private val executorService = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\n+    \"kafka-fetched-data-cache-evictor\")\n+\n+  private def startEvictorThread(): ScheduledFuture[_] = {\n+    executorService.scheduleAtFixedRate(new Runnable {\n+      override def run(): Unit = {\n+        try {\n+          removeIdleFetchedData()\n+        } catch {\n+          case NonFatal(e) =>\n+            logWarning(\"Exception occurred while removing idle fetched data.\", e)\n+        }\n+      }\n+    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\n+  }\n+\n+  private var scheduled = startEvictorThread()\n+\n+  private val numCreatedFetchedData = new LongAdder()\n+  private val numTotalElements = new LongAdder()\n+\n+  def getNumCreated: Long = numCreatedFetchedData.sum()\n+  def getNumTotal: Long = numTotalElements.sum()\n+\n+  def acquire(key: CacheKey, desiredStartOffset: Long): FetchedData = synchronized {\n+    val fetchedDataList = cache.getOrElseUpdate(key, new CachedFetchedDataList())\n+\n+    val cachedFetchedDataOption = fetchedDataList.find { p =>\n+      !p.inUse && p.getObject.nextOffsetInFetchedData == desiredStartOffset\n+    }\n+\n+    var cachedFetchedData: CachedFetchedData = null\n+    if (cachedFetchedDataOption.isDefined) {\n+      cachedFetchedData = cachedFetchedDataOption.get\n+    } else {\n+      cachedFetchedData = CachedFetchedData.empty()\n+      fetchedDataList += cachedFetchedData\n+\n+      numCreatedFetchedData.increment()\n+      numTotalElements.increment()\n+    }\n+\n+    cachedFetchedData.lastAcquiredTimestamp = System.currentTimeMillis()\n+    cachedFetchedData.inUse = true\n+\n+    cachedFetchedData.getObject\n+  }\n+\n+  def invalidate(key: CacheKey): Unit = synchronized {\n+    cache.remove(key) match {\n+      case Some(lst) => numTotalElements.add(-1 * lst.size)\n+      case None =>\n+    }\n+  }\n+\n+  def release(key: CacheKey, fetchedData: FetchedData): Unit = synchronized {\n+    cache.get(key) match {\n+      case Some(fetchedDataList) =>\n+        val cachedFetchedDataOption = fetchedDataList.find { p =>\n+          p.inUse && p.getObject == fetchedData\n+        }\n+\n+        if (cachedFetchedDataOption.isDefined) {\n+          val cachedFetchedData = cachedFetchedDataOption.get\n+          cachedFetchedData.inUse = false\n+          cachedFetchedData.lastReleasedTimestamp = System.currentTimeMillis()\n+        }\n+\n+      case None => logWarning(s\"No matching data in pool for $fetchedData in key $key. \" +\n+        \"It might be released before, or it was not a part of pool.\")\n+    }\n+  }\n+\n+  def shutdown(): Unit = {\n+    executorService.shutdownNow()\n+  }\n+\n+  def reset(): Unit = synchronized {\n+    scheduled.cancel(true)\n+\n+    cache.clear()\n+    numTotalElements.reset()\n+    numCreatedFetchedData.reset()\n+\n+    scheduled = startEvictorThread()\n+  }\n+\n+  private def removeIdleFetchedData(): Unit = synchronized {\n+    val timestamp = System.currentTimeMillis()\n+    val maxAllowedIdleTimestamp = timestamp - minEvictableIdleTimeMillis\n+    cache.values.foreach { p: CachedFetchedDataList =>\n+      val idles = p.filter(q => !q.inUse && q.lastReleasedTimestamp < maxAllowedIdleTimestamp)\n+      val lstSize = p.size\n+      idles.foreach(idle => p -= idle)\n+      numTotalElements.add(-1 * (lstSize - p.size))\n+    }\n+  }\n+}\n+\n+private[kafka010] object FetchedDataPool {\n+  private[kafka010] case class CachedFetchedData(fetchedData: FetchedData) {\n+    var lastReleasedTimestamp: Long = Long.MaxValue\n+    var lastAcquiredTimestamp: Long = Long.MinValue\n+    var inUse: Boolean = false\n+\n+    def getObject: FetchedData = fetchedData\n+  }\n+\n+  private object CachedFetchedData {\n+    def empty(): CachedFetchedData = {\n+      val emptyData = FetchedData(\n+        ju.Collections.emptyListIterator[ConsumerRecord[Array[Byte], Array[Byte]]],\n+        UNKNOWN_OFFSET,\n+        UNKNOWN_OFFSET)\n+\n+      CachedFetchedData(emptyData)\n+    }\n+  }\n+\n+  private[kafka010] type CachedFetchedDataList = mutable.ListBuffer[CachedFetchedData]\n+\n+  val CONFIG_NAME_PREFIX = \"spark.sql.kafkaFetchedDataCache.\"\n+  val CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS = CONFIG_NAME_PREFIX +\n+    \"minEvictableIdleTimeMillis\"\n+  val CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS = CONFIG_NAME_PREFIX +\n+    \"evictorThreadRunIntervalMillis\"\n+\n+  val DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS = 10 * 60 * 1000 // 10 minutes\n+  val DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS = 5 * 60 * 1000 // 3 minutes\n+\n+  def build: FetchedDataPool = new FetchedDataPool()"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "So this wasn't clear from your PR description. Why are you using commons-pool for the consumer cache, but not here?",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-26T18:25:38Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "I guess javadoc explains it, but I'll add to PR description as well if it's ideal. Please let me know if javadoc also needs to be elaborated here as well.\r\n\r\n> Along with CacheKey, it receives desired start offset to find cached FetchedData which may be stored from previous batch.\r\n\r\nSo there're two keys, CacheKey as primary key, and desired offset as second key. Unlike CacheKey, desired offset is changing in FetchedData once it's borrowed and being read from task. Normally pool assumes key is not changing, so has to go through custom logic of cache to handle exceptional case.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-26T19:35:37Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Updated both javadoc and PR description.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-27T05:35:40Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+import scala.util.control.NonFatal\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.ThreadUtils\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData.\n+ */\n+private[kafka010] class FetchedDataPool extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "I think this can be simplified:\r\n```\r\n  private def startEvictorThread(): ScheduledFuture[_] = {\r\n    executorService.scheduleAtFixedRate(() => {\r\n      Utils.tryLogNonFatalError(removeIdleFetchedData())\r\n    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\r\n  }\r\n```\r\n",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-27T12:52:38Z",
    "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledExecutorService, ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.{Clock, SystemClock, ThreadUtils, Utils}\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData. As \"desired start offset\" plays as second level of key which can be\n+ * modified in same instance, this class cannot be replaced with general pool implementations\n+ * including Apache Commons Pool which pools KafkaConsumer.\n+ */\n+private[kafka010] class FetchedDataPool(\n+    executorService: ScheduledExecutorService,\n+    clock: Clock) extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.get(CONSUMER_CACHE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+    val evictorThreadInterval = conf.get(CONSUMER_CACHE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private def startEvictorThread(): ScheduledFuture[_] = {\n+    executorService.scheduleAtFixedRate(new Runnable {"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "The extra block just so you can avoid calling `SparkEnv.get.conf` twice seems pretty unnecessary.\r\n\r\nIn fact if you pass `conf` as a constructor argument, you could even get rid of `withSparkConf` in your test code.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-27T19:58:50Z",
    "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledExecutorService, ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.{Clock, SystemClock, ThreadUtils, Utils}\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData. As \"desired start offset\" plays as second level of key which can be\n+ * modified in same instance, this class cannot be replaced with general pool implementations\n+ * including Apache Commons Pool which pools KafkaConsumer.\n+ */\n+private[kafka010] class FetchedDataPool(\n+    executorService: ScheduledExecutorService,\n+    clock: Clock) extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Yeah agreed. Will retrieve SparkConf in constructor.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-27T23:45:23Z",
    "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledExecutorService, ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.{Clock, SystemClock, ThreadUtils, Utils}\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData. As \"desired start offset\" plays as second level of key which can be\n+ * modified in same instance, this class cannot be replaced with general pool implementations\n+ * including Apache Commons Pool which pools KafkaConsumer.\n+ */\n+private[kafka010] class FetchedDataPool(\n+    executorService: ScheduledExecutorService,\n+    clock: Clock) extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`currentTimeInMillis`?",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-27T20:04:20Z",
    "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledExecutorService, ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.{Clock, SystemClock, ThreadUtils, Utils}\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData. As \"desired start offset\" plays as second level of key which can be\n+ * modified in same instance, this class cannot be replaced with general pool implementations\n+ * including Apache Commons Pool which pools KafkaConsumer.\n+ */\n+private[kafka010] class FetchedDataPool(\n+    executorService: ScheduledExecutorService,\n+    clock: Clock) extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.get(CONSUMER_CACHE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+    val evictorThreadInterval = conf.get(CONSUMER_CACHE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private def startEvictorThread(): ScheduledFuture[_] = {\n+    executorService.scheduleAtFixedRate(new Runnable {\n+      override def run(): Unit = {\n+        Utils.tryLogNonFatalError(removeIdleFetchedData())\n+      }\n+    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\n+  }\n+\n+  private var scheduled = startEvictorThread()\n+\n+  private val numCreatedFetchedData = new LongAdder()\n+  private val numTotalElements = new LongAdder()\n+\n+  def numCreated: Long = numCreatedFetchedData.sum()\n+  def numTotal: Long = numTotalElements.sum()\n+\n+  def acquire(key: CacheKey, desiredStartOffset: Long): FetchedData = synchronized {\n+    val fetchedDataList = cache.getOrElseUpdate(key, new CachedFetchedDataList())\n+\n+    val cachedFetchedDataOption = fetchedDataList.find { p =>\n+      !p.inUse && p.getObject.nextOffsetInFetchedData == desiredStartOffset\n+    }\n+\n+    var cachedFetchedData: CachedFetchedData = null\n+    if (cachedFetchedDataOption.isDefined) {\n+      cachedFetchedData = cachedFetchedDataOption.get\n+    } else {\n+      cachedFetchedData = CachedFetchedData.empty()\n+      fetchedDataList += cachedFetchedData\n+\n+      numCreatedFetchedData.increment()\n+      numTotalElements.increment()\n+    }\n+\n+    cachedFetchedData.lastAcquiredTimestamp = clock.getTimeMillis()\n+    cachedFetchedData.inUse = true\n+\n+    cachedFetchedData.getObject\n+  }\n+\n+  def invalidate(key: CacheKey): Unit = synchronized {\n+    cache.remove(key) match {\n+      case Some(lst) => numTotalElements.add(-1 * lst.size)\n+      case None =>\n+    }\n+  }\n+\n+  def release(key: CacheKey, fetchedData: FetchedData): Unit = synchronized {\n+    def warnReleasedDataNotInPool(key: CacheKey, fetchedData: FetchedData): Unit = {\n+      logWarning(s\"No matching data in pool for $fetchedData in key $key. \" +\n+        \"It might be released before, or it was not a part of pool.\")\n+    }\n+\n+    cache.get(key) match {\n+      case Some(fetchedDataList) =>\n+        val cachedFetchedDataOption = fetchedDataList.find { p =>\n+          p.inUse && p.getObject == fetchedData\n+        }\n+\n+        if (cachedFetchedDataOption.isEmpty) {\n+          warnReleasedDataNotInPool(key, fetchedData)\n+        } else {\n+          val cachedFetchedData = cachedFetchedDataOption.get\n+          cachedFetchedData.inUse = false\n+          cachedFetchedData.lastReleasedTimestamp = clock.getTimeMillis()\n+        }\n+\n+      case None =>\n+        warnReleasedDataNotInPool(key, fetchedData)\n+    }\n+  }\n+\n+  def shutdown(): Unit = {\n+    ThreadUtils.shutdown(executorService)\n+  }\n+\n+  def reset(): Unit = synchronized {\n+    scheduled.cancel(true)\n+\n+    cache.clear()\n+    numTotalElements.reset()\n+    numCreatedFetchedData.reset()\n+\n+    scheduled = startEvictorThread()\n+  }\n+\n+  private def removeIdleFetchedData(): Unit = synchronized {\n+    val timestamp = clock.getTimeMillis()\n+    val maxAllowedIdleTimestamp = timestamp - minEvictableIdleTimeMillis\n+    cache.values.foreach { p: CachedFetchedDataList =>\n+      val expired = p.filter { q =>\n+        // also check timestamp is bigger than last released timestamp to avoid\n+        // the case System.currentTimeInMillis looking as \"move back\""
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "As we no longer need this I'll just remove the comment.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-27T23:14:03Z",
    "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledExecutorService, ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.{Clock, SystemClock, ThreadUtils, Utils}\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData. As \"desired start offset\" plays as second level of key which can be\n+ * modified in same instance, this class cannot be replaced with general pool implementations\n+ * including Apache Commons Pool which pools KafkaConsumer.\n+ */\n+private[kafka010] class FetchedDataPool(\n+    executorService: ScheduledExecutorService,\n+    clock: Clock) extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.get(CONSUMER_CACHE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+    val evictorThreadInterval = conf.get(CONSUMER_CACHE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private def startEvictorThread(): ScheduledFuture[_] = {\n+    executorService.scheduleAtFixedRate(new Runnable {\n+      override def run(): Unit = {\n+        Utils.tryLogNonFatalError(removeIdleFetchedData())\n+      }\n+    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\n+  }\n+\n+  private var scheduled = startEvictorThread()\n+\n+  private val numCreatedFetchedData = new LongAdder()\n+  private val numTotalElements = new LongAdder()\n+\n+  def numCreated: Long = numCreatedFetchedData.sum()\n+  def numTotal: Long = numTotalElements.sum()\n+\n+  def acquire(key: CacheKey, desiredStartOffset: Long): FetchedData = synchronized {\n+    val fetchedDataList = cache.getOrElseUpdate(key, new CachedFetchedDataList())\n+\n+    val cachedFetchedDataOption = fetchedDataList.find { p =>\n+      !p.inUse && p.getObject.nextOffsetInFetchedData == desiredStartOffset\n+    }\n+\n+    var cachedFetchedData: CachedFetchedData = null\n+    if (cachedFetchedDataOption.isDefined) {\n+      cachedFetchedData = cachedFetchedDataOption.get\n+    } else {\n+      cachedFetchedData = CachedFetchedData.empty()\n+      fetchedDataList += cachedFetchedData\n+\n+      numCreatedFetchedData.increment()\n+      numTotalElements.increment()\n+    }\n+\n+    cachedFetchedData.lastAcquiredTimestamp = clock.getTimeMillis()\n+    cachedFetchedData.inUse = true\n+\n+    cachedFetchedData.getObject\n+  }\n+\n+  def invalidate(key: CacheKey): Unit = synchronized {\n+    cache.remove(key) match {\n+      case Some(lst) => numTotalElements.add(-1 * lst.size)\n+      case None =>\n+    }\n+  }\n+\n+  def release(key: CacheKey, fetchedData: FetchedData): Unit = synchronized {\n+    def warnReleasedDataNotInPool(key: CacheKey, fetchedData: FetchedData): Unit = {\n+      logWarning(s\"No matching data in pool for $fetchedData in key $key. \" +\n+        \"It might be released before, or it was not a part of pool.\")\n+    }\n+\n+    cache.get(key) match {\n+      case Some(fetchedDataList) =>\n+        val cachedFetchedDataOption = fetchedDataList.find { p =>\n+          p.inUse && p.getObject == fetchedData\n+        }\n+\n+        if (cachedFetchedDataOption.isEmpty) {\n+          warnReleasedDataNotInPool(key, fetchedData)\n+        } else {\n+          val cachedFetchedData = cachedFetchedDataOption.get\n+          cachedFetchedData.inUse = false\n+          cachedFetchedData.lastReleasedTimestamp = clock.getTimeMillis()\n+        }\n+\n+      case None =>\n+        warnReleasedDataNotInPool(key, fetchedData)\n+    }\n+  }\n+\n+  def shutdown(): Unit = {\n+    ThreadUtils.shutdown(executorService)\n+  }\n+\n+  def reset(): Unit = synchronized {\n+    scheduled.cancel(true)\n+\n+    cache.clear()\n+    numTotalElements.reset()\n+    numCreatedFetchedData.reset()\n+\n+    scheduled = startEvictorThread()\n+  }\n+\n+  private def removeIdleFetchedData(): Unit = synchronized {\n+    val timestamp = clock.getTimeMillis()\n+    val maxAllowedIdleTimestamp = timestamp - minEvictableIdleTimeMillis\n+    cache.values.foreach { p: CachedFetchedDataList =>\n+      val expired = p.filter { q =>\n+        // also check timestamp is bigger than last released timestamp to avoid\n+        // the case System.currentTimeInMillis looking as \"move back\""
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "s/timestamp/now",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-27T20:11:13Z",
    "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledExecutorService, ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.{Clock, SystemClock, ThreadUtils, Utils}\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData. As \"desired start offset\" plays as second level of key which can be\n+ * modified in same instance, this class cannot be replaced with general pool implementations\n+ * including Apache Commons Pool which pools KafkaConsumer.\n+ */\n+private[kafka010] class FetchedDataPool(\n+    executorService: ScheduledExecutorService,\n+    clock: Clock) extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.get(CONSUMER_CACHE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+    val evictorThreadInterval = conf.get(CONSUMER_CACHE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private def startEvictorThread(): ScheduledFuture[_] = {\n+    executorService.scheduleAtFixedRate(new Runnable {\n+      override def run(): Unit = {\n+        Utils.tryLogNonFatalError(removeIdleFetchedData())\n+      }\n+    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\n+  }\n+\n+  private var scheduled = startEvictorThread()\n+\n+  private val numCreatedFetchedData = new LongAdder()\n+  private val numTotalElements = new LongAdder()\n+\n+  def numCreated: Long = numCreatedFetchedData.sum()\n+  def numTotal: Long = numTotalElements.sum()\n+\n+  def acquire(key: CacheKey, desiredStartOffset: Long): FetchedData = synchronized {\n+    val fetchedDataList = cache.getOrElseUpdate(key, new CachedFetchedDataList())\n+\n+    val cachedFetchedDataOption = fetchedDataList.find { p =>\n+      !p.inUse && p.getObject.nextOffsetInFetchedData == desiredStartOffset\n+    }\n+\n+    var cachedFetchedData: CachedFetchedData = null\n+    if (cachedFetchedDataOption.isDefined) {\n+      cachedFetchedData = cachedFetchedDataOption.get\n+    } else {\n+      cachedFetchedData = CachedFetchedData.empty()\n+      fetchedDataList += cachedFetchedData\n+\n+      numCreatedFetchedData.increment()\n+      numTotalElements.increment()\n+    }\n+\n+    cachedFetchedData.lastAcquiredTimestamp = clock.getTimeMillis()\n+    cachedFetchedData.inUse = true\n+\n+    cachedFetchedData.getObject\n+  }\n+\n+  def invalidate(key: CacheKey): Unit = synchronized {\n+    cache.remove(key) match {\n+      case Some(lst) => numTotalElements.add(-1 * lst.size)\n+      case None =>\n+    }\n+  }\n+\n+  def release(key: CacheKey, fetchedData: FetchedData): Unit = synchronized {\n+    def warnReleasedDataNotInPool(key: CacheKey, fetchedData: FetchedData): Unit = {\n+      logWarning(s\"No matching data in pool for $fetchedData in key $key. \" +\n+        \"It might be released before, or it was not a part of pool.\")\n+    }\n+\n+    cache.get(key) match {\n+      case Some(fetchedDataList) =>\n+        val cachedFetchedDataOption = fetchedDataList.find { p =>\n+          p.inUse && p.getObject == fetchedData\n+        }\n+\n+        if (cachedFetchedDataOption.isEmpty) {\n+          warnReleasedDataNotInPool(key, fetchedData)\n+        } else {\n+          val cachedFetchedData = cachedFetchedDataOption.get\n+          cachedFetchedData.inUse = false\n+          cachedFetchedData.lastReleasedTimestamp = clock.getTimeMillis()\n+        }\n+\n+      case None =>\n+        warnReleasedDataNotInPool(key, fetchedData)\n+    }\n+  }\n+\n+  def shutdown(): Unit = {\n+    ThreadUtils.shutdown(executorService)\n+  }\n+\n+  def reset(): Unit = synchronized {\n+    scheduled.cancel(true)\n+\n+    cache.clear()\n+    numTotalElements.reset()\n+    numCreatedFetchedData.reset()\n+\n+    scheduled = startEvictorThread()\n+  }\n+\n+  private def removeIdleFetchedData(): Unit = synchronized {\n+    val timestamp = clock.getTimeMillis()"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "maxAllowedReleasedTimestamp",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-27T20:13:33Z",
    "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledExecutorService, ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.{Clock, SystemClock, ThreadUtils, Utils}\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData. As \"desired start offset\" plays as second level of key which can be\n+ * modified in same instance, this class cannot be replaced with general pool implementations\n+ * including Apache Commons Pool which pools KafkaConsumer.\n+ */\n+private[kafka010] class FetchedDataPool(\n+    executorService: ScheduledExecutorService,\n+    clock: Clock) extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.get(CONSUMER_CACHE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+    val evictorThreadInterval = conf.get(CONSUMER_CACHE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private def startEvictorThread(): ScheduledFuture[_] = {\n+    executorService.scheduleAtFixedRate(new Runnable {\n+      override def run(): Unit = {\n+        Utils.tryLogNonFatalError(removeIdleFetchedData())\n+      }\n+    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\n+  }\n+\n+  private var scheduled = startEvictorThread()\n+\n+  private val numCreatedFetchedData = new LongAdder()\n+  private val numTotalElements = new LongAdder()\n+\n+  def numCreated: Long = numCreatedFetchedData.sum()\n+  def numTotal: Long = numTotalElements.sum()\n+\n+  def acquire(key: CacheKey, desiredStartOffset: Long): FetchedData = synchronized {\n+    val fetchedDataList = cache.getOrElseUpdate(key, new CachedFetchedDataList())\n+\n+    val cachedFetchedDataOption = fetchedDataList.find { p =>\n+      !p.inUse && p.getObject.nextOffsetInFetchedData == desiredStartOffset\n+    }\n+\n+    var cachedFetchedData: CachedFetchedData = null\n+    if (cachedFetchedDataOption.isDefined) {\n+      cachedFetchedData = cachedFetchedDataOption.get\n+    } else {\n+      cachedFetchedData = CachedFetchedData.empty()\n+      fetchedDataList += cachedFetchedData\n+\n+      numCreatedFetchedData.increment()\n+      numTotalElements.increment()\n+    }\n+\n+    cachedFetchedData.lastAcquiredTimestamp = clock.getTimeMillis()\n+    cachedFetchedData.inUse = true\n+\n+    cachedFetchedData.getObject\n+  }\n+\n+  def invalidate(key: CacheKey): Unit = synchronized {\n+    cache.remove(key) match {\n+      case Some(lst) => numTotalElements.add(-1 * lst.size)\n+      case None =>\n+    }\n+  }\n+\n+  def release(key: CacheKey, fetchedData: FetchedData): Unit = synchronized {\n+    def warnReleasedDataNotInPool(key: CacheKey, fetchedData: FetchedData): Unit = {\n+      logWarning(s\"No matching data in pool for $fetchedData in key $key. \" +\n+        \"It might be released before, or it was not a part of pool.\")\n+    }\n+\n+    cache.get(key) match {\n+      case Some(fetchedDataList) =>\n+        val cachedFetchedDataOption = fetchedDataList.find { p =>\n+          p.inUse && p.getObject == fetchedData\n+        }\n+\n+        if (cachedFetchedDataOption.isEmpty) {\n+          warnReleasedDataNotInPool(key, fetchedData)\n+        } else {\n+          val cachedFetchedData = cachedFetchedDataOption.get\n+          cachedFetchedData.inUse = false\n+          cachedFetchedData.lastReleasedTimestamp = clock.getTimeMillis()\n+        }\n+\n+      case None =>\n+        warnReleasedDataNotInPool(key, fetchedData)\n+    }\n+  }\n+\n+  def shutdown(): Unit = {\n+    ThreadUtils.shutdown(executorService)\n+  }\n+\n+  def reset(): Unit = synchronized {\n+    scheduled.cancel(true)\n+\n+    cache.clear()\n+    numTotalElements.reset()\n+    numCreatedFetchedData.reset()\n+\n+    scheduled = startEvictorThread()\n+  }\n+\n+  private def removeIdleFetchedData(): Unit = synchronized {\n+    val timestamp = clock.getTimeMillis()\n+    val maxAllowedIdleTimestamp = timestamp - minEvictableIdleTimeMillis"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Everything in one line, or move `idle =>` to previous line.\r\n\r\nOr, even better: `p --= expired`.\r\n",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-27T20:16:20Z",
    "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledExecutorService, ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.{Clock, SystemClock, ThreadUtils, Utils}\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData. As \"desired start offset\" plays as second level of key which can be\n+ * modified in same instance, this class cannot be replaced with general pool implementations\n+ * including Apache Commons Pool which pools KafkaConsumer.\n+ */\n+private[kafka010] class FetchedDataPool(\n+    executorService: ScheduledExecutorService,\n+    clock: Clock) extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.get(CONSUMER_CACHE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+    val evictorThreadInterval = conf.get(CONSUMER_CACHE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private def startEvictorThread(): ScheduledFuture[_] = {\n+    executorService.scheduleAtFixedRate(new Runnable {\n+      override def run(): Unit = {\n+        Utils.tryLogNonFatalError(removeIdleFetchedData())\n+      }\n+    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\n+  }\n+\n+  private var scheduled = startEvictorThread()\n+\n+  private val numCreatedFetchedData = new LongAdder()\n+  private val numTotalElements = new LongAdder()\n+\n+  def numCreated: Long = numCreatedFetchedData.sum()\n+  def numTotal: Long = numTotalElements.sum()\n+\n+  def acquire(key: CacheKey, desiredStartOffset: Long): FetchedData = synchronized {\n+    val fetchedDataList = cache.getOrElseUpdate(key, new CachedFetchedDataList())\n+\n+    val cachedFetchedDataOption = fetchedDataList.find { p =>\n+      !p.inUse && p.getObject.nextOffsetInFetchedData == desiredStartOffset\n+    }\n+\n+    var cachedFetchedData: CachedFetchedData = null\n+    if (cachedFetchedDataOption.isDefined) {\n+      cachedFetchedData = cachedFetchedDataOption.get\n+    } else {\n+      cachedFetchedData = CachedFetchedData.empty()\n+      fetchedDataList += cachedFetchedData\n+\n+      numCreatedFetchedData.increment()\n+      numTotalElements.increment()\n+    }\n+\n+    cachedFetchedData.lastAcquiredTimestamp = clock.getTimeMillis()\n+    cachedFetchedData.inUse = true\n+\n+    cachedFetchedData.getObject\n+  }\n+\n+  def invalidate(key: CacheKey): Unit = synchronized {\n+    cache.remove(key) match {\n+      case Some(lst) => numTotalElements.add(-1 * lst.size)\n+      case None =>\n+    }\n+  }\n+\n+  def release(key: CacheKey, fetchedData: FetchedData): Unit = synchronized {\n+    def warnReleasedDataNotInPool(key: CacheKey, fetchedData: FetchedData): Unit = {\n+      logWarning(s\"No matching data in pool for $fetchedData in key $key. \" +\n+        \"It might be released before, or it was not a part of pool.\")\n+    }\n+\n+    cache.get(key) match {\n+      case Some(fetchedDataList) =>\n+        val cachedFetchedDataOption = fetchedDataList.find { p =>\n+          p.inUse && p.getObject == fetchedData\n+        }\n+\n+        if (cachedFetchedDataOption.isEmpty) {\n+          warnReleasedDataNotInPool(key, fetchedData)\n+        } else {\n+          val cachedFetchedData = cachedFetchedDataOption.get\n+          cachedFetchedData.inUse = false\n+          cachedFetchedData.lastReleasedTimestamp = clock.getTimeMillis()\n+        }\n+\n+      case None =>\n+        warnReleasedDataNotInPool(key, fetchedData)\n+    }\n+  }\n+\n+  def shutdown(): Unit = {\n+    ThreadUtils.shutdown(executorService)\n+  }\n+\n+  def reset(): Unit = synchronized {\n+    scheduled.cancel(true)\n+\n+    cache.clear()\n+    numTotalElements.reset()\n+    numCreatedFetchedData.reset()\n+\n+    scheduled = startEvictorThread()\n+  }\n+\n+  private def removeIdleFetchedData(): Unit = synchronized {\n+    val timestamp = clock.getTimeMillis()\n+    val maxAllowedIdleTimestamp = timestamp - minEvictableIdleTimeMillis\n+    cache.values.foreach { p: CachedFetchedDataList =>\n+      val expired = p.filter { q =>\n+        // also check timestamp is bigger than last released timestamp to avoid\n+        // the case System.currentTimeInMillis looking as \"move back\"\n+        !q.inUse && timestamp > q.lastReleasedTimestamp &&\n+          q.lastReleasedTimestamp < maxAllowedIdleTimestamp\n+      }\n+      expired.foreach {\n+        idle => p -= idle"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`timestamp > q.lastReleasedTimestamp` feels redundant; `maxAllowedIdleTimestamp` should be strictly smaller than `timestamp`, so if `q.lastReleasedTimestamp < maxAllowedIdleTimestamp` then it by association is also less than the current timestamp.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-27T20:21:03Z",
    "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledExecutorService, ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.{Clock, SystemClock, ThreadUtils, Utils}\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData. As \"desired start offset\" plays as second level of key which can be\n+ * modified in same instance, this class cannot be replaced with general pool implementations\n+ * including Apache Commons Pool which pools KafkaConsumer.\n+ */\n+private[kafka010] class FetchedDataPool(\n+    executorService: ScheduledExecutorService,\n+    clock: Clock) extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.get(CONSUMER_CACHE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+    val evictorThreadInterval = conf.get(CONSUMER_CACHE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private def startEvictorThread(): ScheduledFuture[_] = {\n+    executorService.scheduleAtFixedRate(new Runnable {\n+      override def run(): Unit = {\n+        Utils.tryLogNonFatalError(removeIdleFetchedData())\n+      }\n+    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\n+  }\n+\n+  private var scheduled = startEvictorThread()\n+\n+  private val numCreatedFetchedData = new LongAdder()\n+  private val numTotalElements = new LongAdder()\n+\n+  def numCreated: Long = numCreatedFetchedData.sum()\n+  def numTotal: Long = numTotalElements.sum()\n+\n+  def acquire(key: CacheKey, desiredStartOffset: Long): FetchedData = synchronized {\n+    val fetchedDataList = cache.getOrElseUpdate(key, new CachedFetchedDataList())\n+\n+    val cachedFetchedDataOption = fetchedDataList.find { p =>\n+      !p.inUse && p.getObject.nextOffsetInFetchedData == desiredStartOffset\n+    }\n+\n+    var cachedFetchedData: CachedFetchedData = null\n+    if (cachedFetchedDataOption.isDefined) {\n+      cachedFetchedData = cachedFetchedDataOption.get\n+    } else {\n+      cachedFetchedData = CachedFetchedData.empty()\n+      fetchedDataList += cachedFetchedData\n+\n+      numCreatedFetchedData.increment()\n+      numTotalElements.increment()\n+    }\n+\n+    cachedFetchedData.lastAcquiredTimestamp = clock.getTimeMillis()\n+    cachedFetchedData.inUse = true\n+\n+    cachedFetchedData.getObject\n+  }\n+\n+  def invalidate(key: CacheKey): Unit = synchronized {\n+    cache.remove(key) match {\n+      case Some(lst) => numTotalElements.add(-1 * lst.size)\n+      case None =>\n+    }\n+  }\n+\n+  def release(key: CacheKey, fetchedData: FetchedData): Unit = synchronized {\n+    def warnReleasedDataNotInPool(key: CacheKey, fetchedData: FetchedData): Unit = {\n+      logWarning(s\"No matching data in pool for $fetchedData in key $key. \" +\n+        \"It might be released before, or it was not a part of pool.\")\n+    }\n+\n+    cache.get(key) match {\n+      case Some(fetchedDataList) =>\n+        val cachedFetchedDataOption = fetchedDataList.find { p =>\n+          p.inUse && p.getObject == fetchedData\n+        }\n+\n+        if (cachedFetchedDataOption.isEmpty) {\n+          warnReleasedDataNotInPool(key, fetchedData)\n+        } else {\n+          val cachedFetchedData = cachedFetchedDataOption.get\n+          cachedFetchedData.inUse = false\n+          cachedFetchedData.lastReleasedTimestamp = clock.getTimeMillis()\n+        }\n+\n+      case None =>\n+        warnReleasedDataNotInPool(key, fetchedData)\n+    }\n+  }\n+\n+  def shutdown(): Unit = {\n+    ThreadUtils.shutdown(executorService)\n+  }\n+\n+  def reset(): Unit = synchronized {\n+    scheduled.cancel(true)\n+\n+    cache.clear()\n+    numTotalElements.reset()\n+    numCreatedFetchedData.reset()\n+\n+    scheduled = startEvictorThread()\n+  }\n+\n+  private def removeIdleFetchedData(): Unit = synchronized {\n+    val timestamp = clock.getTimeMillis()\n+    val maxAllowedIdleTimestamp = timestamp - minEvictableIdleTimeMillis\n+    cache.values.foreach { p: CachedFetchedDataList =>\n+      val expired = p.filter { q =>\n+        // also check timestamp is bigger than last released timestamp to avoid\n+        // the case System.currentTimeInMillis looking as \"move back\"\n+        !q.inUse && timestamp > q.lastReleasedTimestamp &&"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "My bad. You're right. I was wrong about worrying the case release and cleanup occur concurrently (System.currentTimeInMillis may not be in sync in different threads AFAIK) so intended to add guard, but they're even synchronized so it'll never happen. And your math is also correct so also redundant. Nice finding. Will remove.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-27T23:13:22Z",
    "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledExecutorService, ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.{Clock, SystemClock, ThreadUtils, Utils}\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData. As \"desired start offset\" plays as second level of key which can be\n+ * modified in same instance, this class cannot be replaced with general pool implementations\n+ * including Apache Commons Pool which pools KafkaConsumer.\n+ */\n+private[kafka010] class FetchedDataPool(\n+    executorService: ScheduledExecutorService,\n+    clock: Clock) extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.get(CONSUMER_CACHE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+    val evictorThreadInterval = conf.get(CONSUMER_CACHE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private def startEvictorThread(): ScheduledFuture[_] = {\n+    executorService.scheduleAtFixedRate(new Runnable {\n+      override def run(): Unit = {\n+        Utils.tryLogNonFatalError(removeIdleFetchedData())\n+      }\n+    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\n+  }\n+\n+  private var scheduled = startEvictorThread()\n+\n+  private val numCreatedFetchedData = new LongAdder()\n+  private val numTotalElements = new LongAdder()\n+\n+  def numCreated: Long = numCreatedFetchedData.sum()\n+  def numTotal: Long = numTotalElements.sum()\n+\n+  def acquire(key: CacheKey, desiredStartOffset: Long): FetchedData = synchronized {\n+    val fetchedDataList = cache.getOrElseUpdate(key, new CachedFetchedDataList())\n+\n+    val cachedFetchedDataOption = fetchedDataList.find { p =>\n+      !p.inUse && p.getObject.nextOffsetInFetchedData == desiredStartOffset\n+    }\n+\n+    var cachedFetchedData: CachedFetchedData = null\n+    if (cachedFetchedDataOption.isDefined) {\n+      cachedFetchedData = cachedFetchedDataOption.get\n+    } else {\n+      cachedFetchedData = CachedFetchedData.empty()\n+      fetchedDataList += cachedFetchedData\n+\n+      numCreatedFetchedData.increment()\n+      numTotalElements.increment()\n+    }\n+\n+    cachedFetchedData.lastAcquiredTimestamp = clock.getTimeMillis()\n+    cachedFetchedData.inUse = true\n+\n+    cachedFetchedData.getObject\n+  }\n+\n+  def invalidate(key: CacheKey): Unit = synchronized {\n+    cache.remove(key) match {\n+      case Some(lst) => numTotalElements.add(-1 * lst.size)\n+      case None =>\n+    }\n+  }\n+\n+  def release(key: CacheKey, fetchedData: FetchedData): Unit = synchronized {\n+    def warnReleasedDataNotInPool(key: CacheKey, fetchedData: FetchedData): Unit = {\n+      logWarning(s\"No matching data in pool for $fetchedData in key $key. \" +\n+        \"It might be released before, or it was not a part of pool.\")\n+    }\n+\n+    cache.get(key) match {\n+      case Some(fetchedDataList) =>\n+        val cachedFetchedDataOption = fetchedDataList.find { p =>\n+          p.inUse && p.getObject == fetchedData\n+        }\n+\n+        if (cachedFetchedDataOption.isEmpty) {\n+          warnReleasedDataNotInPool(key, fetchedData)\n+        } else {\n+          val cachedFetchedData = cachedFetchedDataOption.get\n+          cachedFetchedData.inUse = false\n+          cachedFetchedData.lastReleasedTimestamp = clock.getTimeMillis()\n+        }\n+\n+      case None =>\n+        warnReleasedDataNotInPool(key, fetchedData)\n+    }\n+  }\n+\n+  def shutdown(): Unit = {\n+    ThreadUtils.shutdown(executorService)\n+  }\n+\n+  def reset(): Unit = synchronized {\n+    scheduled.cancel(true)\n+\n+    cache.clear()\n+    numTotalElements.reset()\n+    numCreatedFetchedData.reset()\n+\n+    scheduled = startEvictorThread()\n+  }\n+\n+  private def removeIdleFetchedData(): Unit = synchronized {\n+    val timestamp = clock.getTimeMillis()\n+    val maxAllowedIdleTimestamp = timestamp - minEvictableIdleTimeMillis\n+    cache.values.foreach { p: CachedFetchedDataList =>\n+      val expired = p.filter { q =>\n+        // also check timestamp is bigger than last released timestamp to avoid\n+        // the case System.currentTimeInMillis looking as \"move back\"\n+        !q.inUse && timestamp > q.lastReleasedTimestamp &&"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "I actually prefer these as constructors in `FetchedDataPool` instead of factory methods.\r\n\r\nAn alternative would be `apply` methods (so you could say `FetchedDataPool()` instead of `new FetchedDataPool()`), but I really prefer the constructor approach better.\r\n\r\n(This also applies to the other pool where you have these `build` methods in something that is not a builder.)",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-27T20:24:57Z",
    "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledExecutorService, ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.{Clock, SystemClock, ThreadUtils, Utils}\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData. As \"desired start offset\" plays as second level of key which can be\n+ * modified in same instance, this class cannot be replaced with general pool implementations\n+ * including Apache Commons Pool which pools KafkaConsumer.\n+ */\n+private[kafka010] class FetchedDataPool(\n+    executorService: ScheduledExecutorService,\n+    clock: Clock) extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.get(CONSUMER_CACHE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+    val evictorThreadInterval = conf.get(CONSUMER_CACHE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private def startEvictorThread(): ScheduledFuture[_] = {\n+    executorService.scheduleAtFixedRate(new Runnable {\n+      override def run(): Unit = {\n+        Utils.tryLogNonFatalError(removeIdleFetchedData())\n+      }\n+    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\n+  }\n+\n+  private var scheduled = startEvictorThread()\n+\n+  private val numCreatedFetchedData = new LongAdder()\n+  private val numTotalElements = new LongAdder()\n+\n+  def numCreated: Long = numCreatedFetchedData.sum()\n+  def numTotal: Long = numTotalElements.sum()\n+\n+  def acquire(key: CacheKey, desiredStartOffset: Long): FetchedData = synchronized {\n+    val fetchedDataList = cache.getOrElseUpdate(key, new CachedFetchedDataList())\n+\n+    val cachedFetchedDataOption = fetchedDataList.find { p =>\n+      !p.inUse && p.getObject.nextOffsetInFetchedData == desiredStartOffset\n+    }\n+\n+    var cachedFetchedData: CachedFetchedData = null\n+    if (cachedFetchedDataOption.isDefined) {\n+      cachedFetchedData = cachedFetchedDataOption.get\n+    } else {\n+      cachedFetchedData = CachedFetchedData.empty()\n+      fetchedDataList += cachedFetchedData\n+\n+      numCreatedFetchedData.increment()\n+      numTotalElements.increment()\n+    }\n+\n+    cachedFetchedData.lastAcquiredTimestamp = clock.getTimeMillis()\n+    cachedFetchedData.inUse = true\n+\n+    cachedFetchedData.getObject\n+  }\n+\n+  def invalidate(key: CacheKey): Unit = synchronized {\n+    cache.remove(key) match {\n+      case Some(lst) => numTotalElements.add(-1 * lst.size)\n+      case None =>\n+    }\n+  }\n+\n+  def release(key: CacheKey, fetchedData: FetchedData): Unit = synchronized {\n+    def warnReleasedDataNotInPool(key: CacheKey, fetchedData: FetchedData): Unit = {\n+      logWarning(s\"No matching data in pool for $fetchedData in key $key. \" +\n+        \"It might be released before, or it was not a part of pool.\")\n+    }\n+\n+    cache.get(key) match {\n+      case Some(fetchedDataList) =>\n+        val cachedFetchedDataOption = fetchedDataList.find { p =>\n+          p.inUse && p.getObject == fetchedData\n+        }\n+\n+        if (cachedFetchedDataOption.isEmpty) {\n+          warnReleasedDataNotInPool(key, fetchedData)\n+        } else {\n+          val cachedFetchedData = cachedFetchedDataOption.get\n+          cachedFetchedData.inUse = false\n+          cachedFetchedData.lastReleasedTimestamp = clock.getTimeMillis()\n+        }\n+\n+      case None =>\n+        warnReleasedDataNotInPool(key, fetchedData)\n+    }\n+  }\n+\n+  def shutdown(): Unit = {\n+    ThreadUtils.shutdown(executorService)\n+  }\n+\n+  def reset(): Unit = synchronized {\n+    scheduled.cancel(true)\n+\n+    cache.clear()\n+    numTotalElements.reset()\n+    numCreatedFetchedData.reset()\n+\n+    scheduled = startEvictorThread()\n+  }\n+\n+  private def removeIdleFetchedData(): Unit = synchronized {\n+    val timestamp = clock.getTimeMillis()\n+    val maxAllowedIdleTimestamp = timestamp - minEvictableIdleTimeMillis\n+    cache.values.foreach { p: CachedFetchedDataList =>\n+      val expired = p.filter { q =>\n+        // also check timestamp is bigger than last released timestamp to avoid\n+        // the case System.currentTimeInMillis looking as \"move back\"\n+        !q.inUse && timestamp > q.lastReleasedTimestamp &&\n+          q.lastReleasedTimestamp < maxAllowedIdleTimestamp\n+      }\n+      expired.foreach {\n+        idle => p -= idle\n+      }\n+      numTotalElements.add(-1 * expired.size)\n+    }\n+  }\n+}\n+\n+private[kafka010] object FetchedDataPool {\n+  private[kafka010] case class CachedFetchedData(fetchedData: FetchedData) {\n+    var lastReleasedTimestamp: Long = Long.MaxValue\n+    var lastAcquiredTimestamp: Long = Long.MinValue\n+    var inUse: Boolean = false\n+\n+    def getObject: FetchedData = fetchedData\n+  }\n+\n+  private object CachedFetchedData {\n+    def empty(): CachedFetchedData = {\n+      val emptyData = FetchedData(\n+        ju.Collections.emptyListIterator[ConsumerRecord[Array[Byte], Array[Byte]]],\n+        UNKNOWN_OFFSET,\n+        UNKNOWN_OFFSET)\n+\n+      CachedFetchedData(emptyData)\n+    }\n+  }\n+\n+  private[kafka010] type CachedFetchedDataList = mutable.ListBuffer[CachedFetchedData]\n+\n+  def build(executorService: ScheduledExecutorService, clock: Clock): FetchedDataPool = {"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "No strong opinion on this. `apply` sounds better than `build` but as I don't have strong opinion on this, I'll change them as constructors.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-27T23:17:03Z",
    "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledExecutorService, ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.{Clock, SystemClock, ThreadUtils, Utils}\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData. As \"desired start offset\" plays as second level of key which can be\n+ * modified in same instance, this class cannot be replaced with general pool implementations\n+ * including Apache Commons Pool which pools KafkaConsumer.\n+ */\n+private[kafka010] class FetchedDataPool(\n+    executorService: ScheduledExecutorService,\n+    clock: Clock) extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.get(CONSUMER_CACHE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+    val evictorThreadInterval = conf.get(CONSUMER_CACHE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private def startEvictorThread(): ScheduledFuture[_] = {\n+    executorService.scheduleAtFixedRate(new Runnable {\n+      override def run(): Unit = {\n+        Utils.tryLogNonFatalError(removeIdleFetchedData())\n+      }\n+    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\n+  }\n+\n+  private var scheduled = startEvictorThread()\n+\n+  private val numCreatedFetchedData = new LongAdder()\n+  private val numTotalElements = new LongAdder()\n+\n+  def numCreated: Long = numCreatedFetchedData.sum()\n+  def numTotal: Long = numTotalElements.sum()\n+\n+  def acquire(key: CacheKey, desiredStartOffset: Long): FetchedData = synchronized {\n+    val fetchedDataList = cache.getOrElseUpdate(key, new CachedFetchedDataList())\n+\n+    val cachedFetchedDataOption = fetchedDataList.find { p =>\n+      !p.inUse && p.getObject.nextOffsetInFetchedData == desiredStartOffset\n+    }\n+\n+    var cachedFetchedData: CachedFetchedData = null\n+    if (cachedFetchedDataOption.isDefined) {\n+      cachedFetchedData = cachedFetchedDataOption.get\n+    } else {\n+      cachedFetchedData = CachedFetchedData.empty()\n+      fetchedDataList += cachedFetchedData\n+\n+      numCreatedFetchedData.increment()\n+      numTotalElements.increment()\n+    }\n+\n+    cachedFetchedData.lastAcquiredTimestamp = clock.getTimeMillis()\n+    cachedFetchedData.inUse = true\n+\n+    cachedFetchedData.getObject\n+  }\n+\n+  def invalidate(key: CacheKey): Unit = synchronized {\n+    cache.remove(key) match {\n+      case Some(lst) => numTotalElements.add(-1 * lst.size)\n+      case None =>\n+    }\n+  }\n+\n+  def release(key: CacheKey, fetchedData: FetchedData): Unit = synchronized {\n+    def warnReleasedDataNotInPool(key: CacheKey, fetchedData: FetchedData): Unit = {\n+      logWarning(s\"No matching data in pool for $fetchedData in key $key. \" +\n+        \"It might be released before, or it was not a part of pool.\")\n+    }\n+\n+    cache.get(key) match {\n+      case Some(fetchedDataList) =>\n+        val cachedFetchedDataOption = fetchedDataList.find { p =>\n+          p.inUse && p.getObject == fetchedData\n+        }\n+\n+        if (cachedFetchedDataOption.isEmpty) {\n+          warnReleasedDataNotInPool(key, fetchedData)\n+        } else {\n+          val cachedFetchedData = cachedFetchedDataOption.get\n+          cachedFetchedData.inUse = false\n+          cachedFetchedData.lastReleasedTimestamp = clock.getTimeMillis()\n+        }\n+\n+      case None =>\n+        warnReleasedDataNotInPool(key, fetchedData)\n+    }\n+  }\n+\n+  def shutdown(): Unit = {\n+    ThreadUtils.shutdown(executorService)\n+  }\n+\n+  def reset(): Unit = synchronized {\n+    scheduled.cancel(true)\n+\n+    cache.clear()\n+    numTotalElements.reset()\n+    numCreatedFetchedData.reset()\n+\n+    scheduled = startEvictorThread()\n+  }\n+\n+  private def removeIdleFetchedData(): Unit = synchronized {\n+    val timestamp = clock.getTimeMillis()\n+    val maxAllowedIdleTimestamp = timestamp - minEvictableIdleTimeMillis\n+    cache.values.foreach { p: CachedFetchedDataList =>\n+      val expired = p.filter { q =>\n+        // also check timestamp is bigger than last released timestamp to avoid\n+        // the case System.currentTimeInMillis looking as \"move back\"\n+        !q.inUse && timestamp > q.lastReleasedTimestamp &&\n+          q.lastReleasedTimestamp < maxAllowedIdleTimestamp\n+      }\n+      expired.foreach {\n+        idle => p -= idle\n+      }\n+      numTotalElements.add(-1 * expired.size)\n+    }\n+  }\n+}\n+\n+private[kafka010] object FetchedDataPool {\n+  private[kafka010] case class CachedFetchedData(fetchedData: FetchedData) {\n+    var lastReleasedTimestamp: Long = Long.MaxValue\n+    var lastAcquiredTimestamp: Long = Long.MinValue\n+    var inUse: Boolean = false\n+\n+    def getObject: FetchedData = fetchedData\n+  }\n+\n+  private object CachedFetchedData {\n+    def empty(): CachedFetchedData = {\n+      val emptyData = FetchedData(\n+        ju.Collections.emptyListIterator[ConsumerRecord[Array[Byte], Array[Byte]]],\n+        UNKNOWN_OFFSET,\n+        UNKNOWN_OFFSET)\n+\n+      CachedFetchedData(emptyData)\n+    }\n+  }\n+\n+  private[kafka010] type CachedFetchedDataList = mutable.ListBuffer[CachedFetchedData]\n+\n+  def build(executorService: ScheduledExecutorService, clock: Clock): FetchedDataPool = {"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Regardless of the above, methods that do things (such as start a new thread pool!) should have `()`.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-27T20:26:15Z",
    "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledExecutorService, ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.{Clock, SystemClock, ThreadUtils, Utils}\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData. As \"desired start offset\" plays as second level of key which can be\n+ * modified in same instance, this class cannot be replaced with general pool implementations\n+ * including Apache Commons Pool which pools KafkaConsumer.\n+ */\n+private[kafka010] class FetchedDataPool(\n+    executorService: ScheduledExecutorService,\n+    clock: Clock) extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.get(CONSUMER_CACHE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+    val evictorThreadInterval = conf.get(CONSUMER_CACHE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private def startEvictorThread(): ScheduledFuture[_] = {\n+    executorService.scheduleAtFixedRate(new Runnable {\n+      override def run(): Unit = {\n+        Utils.tryLogNonFatalError(removeIdleFetchedData())\n+      }\n+    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\n+  }\n+\n+  private var scheduled = startEvictorThread()\n+\n+  private val numCreatedFetchedData = new LongAdder()\n+  private val numTotalElements = new LongAdder()\n+\n+  def numCreated: Long = numCreatedFetchedData.sum()\n+  def numTotal: Long = numTotalElements.sum()\n+\n+  def acquire(key: CacheKey, desiredStartOffset: Long): FetchedData = synchronized {\n+    val fetchedDataList = cache.getOrElseUpdate(key, new CachedFetchedDataList())\n+\n+    val cachedFetchedDataOption = fetchedDataList.find { p =>\n+      !p.inUse && p.getObject.nextOffsetInFetchedData == desiredStartOffset\n+    }\n+\n+    var cachedFetchedData: CachedFetchedData = null\n+    if (cachedFetchedDataOption.isDefined) {\n+      cachedFetchedData = cachedFetchedDataOption.get\n+    } else {\n+      cachedFetchedData = CachedFetchedData.empty()\n+      fetchedDataList += cachedFetchedData\n+\n+      numCreatedFetchedData.increment()\n+      numTotalElements.increment()\n+    }\n+\n+    cachedFetchedData.lastAcquiredTimestamp = clock.getTimeMillis()\n+    cachedFetchedData.inUse = true\n+\n+    cachedFetchedData.getObject\n+  }\n+\n+  def invalidate(key: CacheKey): Unit = synchronized {\n+    cache.remove(key) match {\n+      case Some(lst) => numTotalElements.add(-1 * lst.size)\n+      case None =>\n+    }\n+  }\n+\n+  def release(key: CacheKey, fetchedData: FetchedData): Unit = synchronized {\n+    def warnReleasedDataNotInPool(key: CacheKey, fetchedData: FetchedData): Unit = {\n+      logWarning(s\"No matching data in pool for $fetchedData in key $key. \" +\n+        \"It might be released before, or it was not a part of pool.\")\n+    }\n+\n+    cache.get(key) match {\n+      case Some(fetchedDataList) =>\n+        val cachedFetchedDataOption = fetchedDataList.find { p =>\n+          p.inUse && p.getObject == fetchedData\n+        }\n+\n+        if (cachedFetchedDataOption.isEmpty) {\n+          warnReleasedDataNotInPool(key, fetchedData)\n+        } else {\n+          val cachedFetchedData = cachedFetchedDataOption.get\n+          cachedFetchedData.inUse = false\n+          cachedFetchedData.lastReleasedTimestamp = clock.getTimeMillis()\n+        }\n+\n+      case None =>\n+        warnReleasedDataNotInPool(key, fetchedData)\n+    }\n+  }\n+\n+  def shutdown(): Unit = {\n+    ThreadUtils.shutdown(executorService)\n+  }\n+\n+  def reset(): Unit = synchronized {\n+    scheduled.cancel(true)\n+\n+    cache.clear()\n+    numTotalElements.reset()\n+    numCreatedFetchedData.reset()\n+\n+    scheduled = startEvictorThread()\n+  }\n+\n+  private def removeIdleFetchedData(): Unit = synchronized {\n+    val timestamp = clock.getTimeMillis()\n+    val maxAllowedIdleTimestamp = timestamp - minEvictableIdleTimeMillis\n+    cache.values.foreach { p: CachedFetchedDataList =>\n+      val expired = p.filter { q =>\n+        // also check timestamp is bigger than last released timestamp to avoid\n+        // the case System.currentTimeInMillis looking as \"move back\"\n+        !q.inUse && timestamp > q.lastReleasedTimestamp &&\n+          q.lastReleasedTimestamp < maxAllowedIdleTimestamp\n+      }\n+      expired.foreach {\n+        idle => p -= idle\n+      }\n+      numTotalElements.add(-1 * expired.size)\n+    }\n+  }\n+}\n+\n+private[kafka010] object FetchedDataPool {\n+  private[kafka010] case class CachedFetchedData(fetchedData: FetchedData) {\n+    var lastReleasedTimestamp: Long = Long.MaxValue\n+    var lastAcquiredTimestamp: Long = Long.MinValue\n+    var inUse: Boolean = false\n+\n+    def getObject: FetchedData = fetchedData\n+  }\n+\n+  private object CachedFetchedData {\n+    def empty(): CachedFetchedData = {\n+      val emptyData = FetchedData(\n+        ju.Collections.emptyListIterator[ConsumerRecord[Array[Byte], Array[Byte]]],\n+        UNKNOWN_OFFSET,\n+        UNKNOWN_OFFSET)\n+\n+      CachedFetchedData(emptyData)\n+    }\n+  }\n+\n+  private[kafka010] type CachedFetchedDataList = mutable.ListBuffer[CachedFetchedData]\n+\n+  def build(executorService: ScheduledExecutorService, clock: Clock): FetchedDataPool = {\n+    new FetchedDataPool(executorService, clock)\n+  }\n+\n+  def build: FetchedDataPool = {"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Yeah right. Myself 1yr before was not that familiar with Scala idiomatic :) Will fix.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-27T23:19:34Z",
    "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{ScheduledExecutorService, ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.LongAdder\n+\n+import scala.collection.mutable\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.{CacheKey, UNKNOWN_OFFSET}\n+import org.apache.spark.util.{Clock, SystemClock, ThreadUtils, Utils}\n+\n+/**\n+ * Provides object pool for [[FetchedData]] which is grouped by [[CacheKey]].\n+ *\n+ * Along with CacheKey, it receives desired start offset to find cached FetchedData which\n+ * may be stored from previous batch. If it can't find one to match, it will create\n+ * a new FetchedData. As \"desired start offset\" plays as second level of key which can be\n+ * modified in same instance, this class cannot be replaced with general pool implementations\n+ * including Apache Commons Pool which pools KafkaConsumer.\n+ */\n+private[kafka010] class FetchedDataPool(\n+    executorService: ScheduledExecutorService,\n+    clock: Clock) extends Logging {\n+  import FetchedDataPool._\n+\n+  private val cache: mutable.Map[CacheKey, CachedFetchedDataList] = mutable.HashMap.empty\n+\n+  private val (minEvictableIdleTimeMillis, evictorThreadRunIntervalMillis): (Long, Long) = {\n+    val conf = SparkEnv.get.conf\n+\n+    val minEvictIdleTime = conf.get(CONSUMER_CACHE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+    val evictorThreadInterval = conf.get(CONSUMER_CACHE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+    (minEvictIdleTime, evictorThreadInterval)\n+  }\n+\n+  private def startEvictorThread(): ScheduledFuture[_] = {\n+    executorService.scheduleAtFixedRate(new Runnable {\n+      override def run(): Unit = {\n+        Utils.tryLogNonFatalError(removeIdleFetchedData())\n+      }\n+    }, 0, evictorThreadRunIntervalMillis, TimeUnit.MILLISECONDS)\n+  }\n+\n+  private var scheduled = startEvictorThread()\n+\n+  private val numCreatedFetchedData = new LongAdder()\n+  private val numTotalElements = new LongAdder()\n+\n+  def numCreated: Long = numCreatedFetchedData.sum()\n+  def numTotal: Long = numTotalElements.sum()\n+\n+  def acquire(key: CacheKey, desiredStartOffset: Long): FetchedData = synchronized {\n+    val fetchedDataList = cache.getOrElseUpdate(key, new CachedFetchedDataList())\n+\n+    val cachedFetchedDataOption = fetchedDataList.find { p =>\n+      !p.inUse && p.getObject.nextOffsetInFetchedData == desiredStartOffset\n+    }\n+\n+    var cachedFetchedData: CachedFetchedData = null\n+    if (cachedFetchedDataOption.isDefined) {\n+      cachedFetchedData = cachedFetchedDataOption.get\n+    } else {\n+      cachedFetchedData = CachedFetchedData.empty()\n+      fetchedDataList += cachedFetchedData\n+\n+      numCreatedFetchedData.increment()\n+      numTotalElements.increment()\n+    }\n+\n+    cachedFetchedData.lastAcquiredTimestamp = clock.getTimeMillis()\n+    cachedFetchedData.inUse = true\n+\n+    cachedFetchedData.getObject\n+  }\n+\n+  def invalidate(key: CacheKey): Unit = synchronized {\n+    cache.remove(key) match {\n+      case Some(lst) => numTotalElements.add(-1 * lst.size)\n+      case None =>\n+    }\n+  }\n+\n+  def release(key: CacheKey, fetchedData: FetchedData): Unit = synchronized {\n+    def warnReleasedDataNotInPool(key: CacheKey, fetchedData: FetchedData): Unit = {\n+      logWarning(s\"No matching data in pool for $fetchedData in key $key. \" +\n+        \"It might be released before, or it was not a part of pool.\")\n+    }\n+\n+    cache.get(key) match {\n+      case Some(fetchedDataList) =>\n+        val cachedFetchedDataOption = fetchedDataList.find { p =>\n+          p.inUse && p.getObject == fetchedData\n+        }\n+\n+        if (cachedFetchedDataOption.isEmpty) {\n+          warnReleasedDataNotInPool(key, fetchedData)\n+        } else {\n+          val cachedFetchedData = cachedFetchedDataOption.get\n+          cachedFetchedData.inUse = false\n+          cachedFetchedData.lastReleasedTimestamp = clock.getTimeMillis()\n+        }\n+\n+      case None =>\n+        warnReleasedDataNotInPool(key, fetchedData)\n+    }\n+  }\n+\n+  def shutdown(): Unit = {\n+    ThreadUtils.shutdown(executorService)\n+  }\n+\n+  def reset(): Unit = synchronized {\n+    scheduled.cancel(true)\n+\n+    cache.clear()\n+    numTotalElements.reset()\n+    numCreatedFetchedData.reset()\n+\n+    scheduled = startEvictorThread()\n+  }\n+\n+  private def removeIdleFetchedData(): Unit = synchronized {\n+    val timestamp = clock.getTimeMillis()\n+    val maxAllowedIdleTimestamp = timestamp - minEvictableIdleTimeMillis\n+    cache.values.foreach { p: CachedFetchedDataList =>\n+      val expired = p.filter { q =>\n+        // also check timestamp is bigger than last released timestamp to avoid\n+        // the case System.currentTimeInMillis looking as \"move back\"\n+        !q.inUse && timestamp > q.lastReleasedTimestamp &&\n+          q.lastReleasedTimestamp < maxAllowedIdleTimestamp\n+      }\n+      expired.foreach {\n+        idle => p -= idle\n+      }\n+      numTotalElements.add(-1 * expired.size)\n+    }\n+  }\n+}\n+\n+private[kafka010] object FetchedDataPool {\n+  private[kafka010] case class CachedFetchedData(fetchedData: FetchedData) {\n+    var lastReleasedTimestamp: Long = Long.MaxValue\n+    var lastAcquiredTimestamp: Long = Long.MinValue\n+    var inUse: Boolean = false\n+\n+    def getObject: FetchedData = fetchedData\n+  }\n+\n+  private object CachedFetchedData {\n+    def empty(): CachedFetchedData = {\n+      val emptyData = FetchedData(\n+        ju.Collections.emptyListIterator[ConsumerRecord[Array[Byte], Array[Byte]]],\n+        UNKNOWN_OFFSET,\n+        UNKNOWN_OFFSET)\n+\n+      CachedFetchedData(emptyData)\n+    }\n+  }\n+\n+  private[kafka010] type CachedFetchedDataList = mutable.ListBuffer[CachedFetchedData]\n+\n+  def build(executorService: ScheduledExecutorService, clock: Clock): FetchedDataPool = {\n+    new FetchedDataPool(executorService, clock)\n+  }\n+\n+  def build: FetchedDataPool = {"
  }],
  "prId": 22138
}]