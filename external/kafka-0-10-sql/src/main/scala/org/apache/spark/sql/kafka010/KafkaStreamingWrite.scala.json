[{
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "I think this null check has significant meaning. Let's consider the following example:\r\n* Task aquires a producer\r\n* `dataWriter.write` called\r\n* `dataWriter.commit` called which decrements `inUseCount`\r\n* Random exception thrown\r\n* `dataWriter.abort` called which decrements `inUseCount` again\r\n\r\nHaving null check and setting null after released would solve double release.\r\nThe same applies to `KafkaWriteTask.scala`.\r\n",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-19T13:36:09Z",
    "diffHunk": "@@ -103,20 +103,18 @@ class KafkaStreamDataWriter(\n     // Send is asynchronous, but we can't commit until all rows are actually in Kafka.\n     // This requires flushing and then checking that no callbacks produced errors.\n     // We also check for errors before to fail as soon as possible - the check is cheap.\n-    checkForErrors()\n-    producer.flush()\n-    checkForErrors()\n+    close()\n     KafkaWriterCommitMessage\n   }\n \n-  def abort(): Unit = {}\n+  def abort(): Unit = {\n+    close()\n+  }\n \n-  def close(): Unit = {\n+  private def close(): Unit = {\n+    checkForErrors()\n+    producer.flush()\n     checkForErrors()\n-    if (producer != null) {"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "This doesn't seem to be addressed yet. Given we are now dealing reference count, bugs on double release would break the logic. Could you please comment if you think it is not needed, or address it if you think it's valid?",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-30T02:47:10Z",
    "diffHunk": "@@ -103,20 +103,18 @@ class KafkaStreamDataWriter(\n     // Send is asynchronous, but we can't commit until all rows are actually in Kafka.\n     // This requires flushing and then checking that no callbacks produced errors.\n     // We also check for errors before to fail as soon as possible - the check is cheap.\n-    checkForErrors()\n-    producer.flush()\n-    checkForErrors()\n+    close()\n     KafkaWriterCommitMessage\n   }\n \n-  def abort(): Unit = {}\n+  def abort(): Unit = {\n+    close()\n+  }\n \n-  def close(): Unit = {\n+  private def close(): Unit = {\n+    checkForErrors()\n+    producer.flush()\n     checkForErrors()\n-    if (producer != null) {"
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "In the later comments @gaborgsomogyi, has himself clarified that it is not required, because now we can safely call double release on a producer.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-03T09:43:02Z",
    "diffHunk": "@@ -103,20 +103,18 @@ class KafkaStreamDataWriter(\n     // Send is asynchronous, but we can't commit until all rows are actually in Kafka.\n     // This requires flushing and then checking that no callbacks produced errors.\n     // We also check for errors before to fail as soon as possible - the check is cheap.\n-    checkForErrors()\n-    producer.flush()\n-    checkForErrors()\n+    close()\n     KafkaWriterCommitMessage\n   }\n \n-  def abort(): Unit = {}\n+  def abort(): Unit = {\n+    close()\n+  }\n \n-  def close(): Unit = {\n+  private def close(): Unit = {\n+    checkForErrors()\n+    producer.flush()\n     checkForErrors()\n-    if (producer != null) {"
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "ScrapCodes"
    },
    "body": "@zsxwing and @gaborgsomogyi, Doing a flush during a commit can have an impact on the overall performance of the KafkaSink. I have not changed it, from what it was, but it is worth mentioning. ",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-27T06:27:24Z",
    "diffHunk": "@@ -103,20 +103,21 @@ class KafkaStreamDataWriter(\n     // Send is asynchronous, but we can't commit until all rows are actually in Kafka.\n     // This requires flushing and then checking that no callbacks produced errors.\n     // We also check for errors before to fail as soon as possible - the check is cheap.\n-    checkForErrors()\n-    producer.flush()\n-    checkForErrors()\n+    close()"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "If my understanding is correct, this is to prevent data loss where Spark marks the offset as committed but writing to Kafka finally failed asynchronously: I think performance doesn't matter related to data loss issue.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-27T06:38:47Z",
    "diffHunk": "@@ -103,20 +103,21 @@ class KafkaStreamDataWriter(\n     // Send is asynchronous, but we can't commit until all rows are actually in Kafka.\n     // This requires flushing and then checking that no callbacks produced errors.\n     // We also check for errors before to fail as soon as possible - the check is cheap.\n-    checkForErrors()\n-    producer.flush()\n-    checkForErrors()\n+    close()"
  }],
  "prId": 19096
}]