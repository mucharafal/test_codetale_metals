[{
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Nit: no need for interpolation",
    "commit": "73647d69668a19a0542f129287fcae2cf5ba8777",
    "createdAt": "2019-08-26T14:09:37Z",
    "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.commons.io.IOUtils\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.execution.streaming.{HDFSMetadataLog, SerializedOffset}\n+\n+/** A version of [[HDFSMetadataLog]] specialized for saving the initial offsets. */\n+private[kafka010] class KafkaSourceInitialOffsetWriter(\n+    sparkSession: SparkSession,\n+    metadataPath: String)\n+  extends HDFSMetadataLog[KafkaSourceOffset](sparkSession, metadataPath) {\n+\n+  val VERSION = 1\n+\n+  override def serialize(metadata: KafkaSourceOffset, out: OutputStream): Unit = {\n+    out.write(0) // A zero byte is written to support Spark 2.1.0 (SPARK-19517)\n+    val writer = new BufferedWriter(new OutputStreamWriter(out, StandardCharsets.UTF_8))\n+    writer.write(\"v\" + VERSION + \"\\n\")\n+    writer.write(metadata.json)\n+    writer.flush\n+  }\n+\n+  override def deserialize(in: InputStream): KafkaSourceOffset = {\n+    in.read() // A zero byte is read to support Spark 2.1.0 (SPARK-19517)\n+    val content = IOUtils.toString(new InputStreamReader(in, StandardCharsets.UTF_8))\n+    // HDFSMetadataLog guarantees that it never creates a partial file.\n+    assert(content.length != 0)\n+    if (content(0) == 'v') {\n+      val indexOfNewLine = content.indexOf(\"\\n\")\n+      if (indexOfNewLine > 0) {\n+        validateVersion(content.substring(0, indexOfNewLine), VERSION)\n+        KafkaSourceOffset(SerializedOffset(content.substring(indexOfNewLine + 1)))\n+      } else {\n+        throw new IllegalStateException(\n+          s\"Log file was malformed: failed to detect the log file version line.\")"
  }],
  "prId": 25583
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Could be .nonEmpty, but I'm not even sure we want asserts here",
    "commit": "73647d69668a19a0542f129287fcae2cf5ba8777",
    "createdAt": "2019-08-26T14:10:34Z",
    "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.commons.io.IOUtils\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.execution.streaming.{HDFSMetadataLog, SerializedOffset}\n+\n+/** A version of [[HDFSMetadataLog]] specialized for saving the initial offsets. */\n+private[kafka010] class KafkaSourceInitialOffsetWriter(\n+    sparkSession: SparkSession,\n+    metadataPath: String)\n+  extends HDFSMetadataLog[KafkaSourceOffset](sparkSession, metadataPath) {\n+\n+  val VERSION = 1\n+\n+  override def serialize(metadata: KafkaSourceOffset, out: OutputStream): Unit = {\n+    out.write(0) // A zero byte is written to support Spark 2.1.0 (SPARK-19517)\n+    val writer = new BufferedWriter(new OutputStreamWriter(out, StandardCharsets.UTF_8))\n+    writer.write(\"v\" + VERSION + \"\\n\")\n+    writer.write(metadata.json)\n+    writer.flush\n+  }\n+\n+  override def deserialize(in: InputStream): KafkaSourceOffset = {\n+    in.read() // A zero byte is read to support Spark 2.1.0 (SPARK-19517)\n+    val content = IOUtils.toString(new InputStreamReader(in, StandardCharsets.UTF_8))\n+    // HDFSMetadataLog guarantees that it never creates a partial file.\n+    assert(content.length != 0)"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "We want to assert but `require` would be better as `assert` may be just ignored under JVM option. I've just changed to `require(content.nonEmpty)`.",
    "commit": "73647d69668a19a0542f129287fcae2cf5ba8777",
    "createdAt": "2019-08-26T14:59:45Z",
    "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.commons.io.IOUtils\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.execution.streaming.{HDFSMetadataLog, SerializedOffset}\n+\n+/** A version of [[HDFSMetadataLog]] specialized for saving the initial offsets. */\n+private[kafka010] class KafkaSourceInitialOffsetWriter(\n+    sparkSession: SparkSession,\n+    metadataPath: String)\n+  extends HDFSMetadataLog[KafkaSourceOffset](sparkSession, metadataPath) {\n+\n+  val VERSION = 1\n+\n+  override def serialize(metadata: KafkaSourceOffset, out: OutputStream): Unit = {\n+    out.write(0) // A zero byte is written to support Spark 2.1.0 (SPARK-19517)\n+    val writer = new BufferedWriter(new OutputStreamWriter(out, StandardCharsets.UTF_8))\n+    writer.write(\"v\" + VERSION + \"\\n\")\n+    writer.write(metadata.json)\n+    writer.flush\n+  }\n+\n+  override def deserialize(in: InputStream): KafkaSourceOffset = {\n+    in.read() // A zero byte is read to support Spark 2.1.0 (SPARK-19517)\n+    val content = IOUtils.toString(new InputStreamReader(in, StandardCharsets.UTF_8))\n+    // HDFSMetadataLog guarantees that it never creates a partial file.\n+    assert(content.length != 0)"
  }],
  "prId": 25583
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "And while we're here you could use interpolation here",
    "commit": "73647d69668a19a0542f129287fcae2cf5ba8777",
    "createdAt": "2019-08-26T14:10:44Z",
    "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.io._\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.commons.io.IOUtils\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.execution.streaming.{HDFSMetadataLog, SerializedOffset}\n+\n+/** A version of [[HDFSMetadataLog]] specialized for saving the initial offsets. */\n+private[kafka010] class KafkaSourceInitialOffsetWriter(\n+    sparkSession: SparkSession,\n+    metadataPath: String)\n+  extends HDFSMetadataLog[KafkaSourceOffset](sparkSession, metadataPath) {\n+\n+  val VERSION = 1\n+\n+  override def serialize(metadata: KafkaSourceOffset, out: OutputStream): Unit = {\n+    out.write(0) // A zero byte is written to support Spark 2.1.0 (SPARK-19517)\n+    val writer = new BufferedWriter(new OutputStreamWriter(out, StandardCharsets.UTF_8))\n+    writer.write(\"v\" + VERSION + \"\\n\")"
  }],
  "prId": 25583
}]