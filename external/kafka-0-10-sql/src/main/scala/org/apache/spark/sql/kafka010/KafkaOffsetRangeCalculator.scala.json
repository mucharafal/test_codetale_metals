[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "add docs.",
    "commit": "602ab36490a692080682867f98a8a5d8f7b2390d",
    "createdAt": "2018-03-01T01:31:57Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.sql.sources.v2.DataSourceOptions\n+\n+\n+private[kafka010] class KafkaOffsetRangeCalculator(val minPartitions: Int) {"
  }],
  "prId": 20698
}, {
  "comments": [{
    "author": {
      "login": "jose-torres"
    },
    "body": "nit: /s/numPartitions/minPartitions",
    "commit": "602ab36490a692080682867f98a8a5d8f7b2390d",
    "createdAt": "2018-03-01T20:31:21Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.sql.sources.v2.DataSourceOptions\n+\n+\n+/**\n+ * Class to calculate offset ranges to process based on the the from and until offsets, and\n+ * the configured `minPartitions`.\n+ */\n+private[kafka010] class KafkaOffsetRangeCalculator(val minPartitions: Int) {\n+  require(minPartitions >= 0)\n+\n+  import KafkaOffsetRangeCalculator._\n+  /**\n+   * Calculate the offset ranges that we are going to process this batch. If `numPartitions`"
  }],
  "prId": 20698
}, {
  "comments": [{
    "author": {
      "login": "jose-torres"
    },
    "body": "super-nit: this isn't really a default, 0 isn't a valid number of min partitions",
    "commit": "602ab36490a692080682867f98a8a5d8f7b2390d",
    "createdAt": "2018-03-01T20:32:08Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.sql.sources.v2.DataSourceOptions\n+\n+\n+/**\n+ * Class to calculate offset ranges to process based on the the from and until offsets, and\n+ * the configured `minPartitions`.\n+ */\n+private[kafka010] class KafkaOffsetRangeCalculator(val minPartitions: Int) {\n+  require(minPartitions >= 0)\n+\n+  import KafkaOffsetRangeCalculator._\n+  /**\n+   * Calculate the offset ranges that we are going to process this batch. If `numPartitions`\n+   * is not set or is set less than or equal the number of `topicPartitions` that we're going to\n+   * consume, then we fall back to a 1-1 mapping of Spark tasks to Kafka partitions. If\n+   * `numPartitions` is set higher than the number of our `topicPartitions`, then we will split up\n+   * the read tasks of the skewed partitions to multiple Spark tasks.\n+   * The number of Spark tasks will be *approximately* `numPartitions`. It can be less or more\n+   * depending on rounding errors or Kafka partitions that didn't receive any new data.\n+   */\n+  def getRanges(\n+      fromOffsets: PartitionOffsetMap,\n+      untilOffsets: PartitionOffsetMap,\n+      executorLocations: Seq[String] = Seq.empty): Seq[KafkaOffsetRange] = {\n+    val partitionsToRead = untilOffsets.keySet.intersect(fromOffsets.keySet)\n+\n+    val offsetRanges = partitionsToRead.toSeq.map { tp =>\n+      KafkaOffsetRange(tp, fromOffsets(tp), untilOffsets(tp), preferredLoc = None)\n+    }\n+\n+    // If minPartitions not set or there are enough partitions to satisfy minPartitions\n+    if (minPartitions == DEFAULT_MIN_PARTITIONS || offsetRanges.size > minPartitions) {\n+      // Assign preferred executor locations to each range such that the same topic-partition is\n+      // always read from the same executor and the KafkaConsumer can be reused\n+      offsetRanges.map { range =>\n+        range.copy(preferredLoc = getLocation(range.topicPartition, executorLocations))\n+      }\n+    } else {\n+\n+      // Splits offset ranges with relatively large amount of data to smaller ones.\n+      val totalSize = offsetRanges.map(o => o.untilOffset - o.fromOffset).sum\n+      offsetRanges.flatMap { offsetRange =>\n+        val tp = offsetRange.topicPartition\n+        val size = offsetRange.untilOffset - offsetRange.fromOffset\n+        // number of partitions to divvy up this topic partition to\n+        val parts = math.max(math.round(size * 1.0 / totalSize * minPartitions), 1).toInt\n+        var remaining = size\n+        var startOffset = offsetRange.fromOffset\n+        (0 until parts).map { part =>\n+          // Fine to do integer division. Last partition will consume all the round off errors\n+          val thisPartition = remaining / (parts - part)\n+          remaining -= thisPartition\n+          val endOffset = startOffset + thisPartition\n+          val offsetRange = KafkaOffsetRange(tp, startOffset, endOffset, preferredLoc = None)\n+          startOffset = endOffset\n+          offsetRange\n+        }\n+      }\n+    }\n+  }\n+\n+  private def getLocation(tp: TopicPartition, executorLocations: Seq[String]): Option[String] = {\n+    def floorMod(a: Long, b: Int): Int = ((a % b).toInt + b) % b\n+\n+    val numExecutors = executorLocations.length\n+    if (numExecutors > 0) {\n+      // This allows cached KafkaConsumers in the executors to be re-used to read the same\n+      // partition in every batch.\n+      Some(executorLocations(floorMod(tp.hashCode, numExecutors)))\n+    } else None\n+  }\n+}\n+\n+private[kafka010] object KafkaOffsetRangeCalculator {\n+\n+  private val DEFAULT_MIN_PARTITIONS = 0"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Ideally, we shouldn't be using default values like this. Rather I want to use Options. However, DataSourceOptions does not give me a way to get back an Option[Int], thus forces me to specify some default value. Let me see what I can do about it. I dont want to reason about 0 in the subsequent conditions and math calculations either.",
    "commit": "602ab36490a692080682867f98a8a5d8f7b2390d",
    "createdAt": "2018-03-02T01:05:32Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.sql.sources.v2.DataSourceOptions\n+\n+\n+/**\n+ * Class to calculate offset ranges to process based on the the from and until offsets, and\n+ * the configured `minPartitions`.\n+ */\n+private[kafka010] class KafkaOffsetRangeCalculator(val minPartitions: Int) {\n+  require(minPartitions >= 0)\n+\n+  import KafkaOffsetRangeCalculator._\n+  /**\n+   * Calculate the offset ranges that we are going to process this batch. If `numPartitions`\n+   * is not set or is set less than or equal the number of `topicPartitions` that we're going to\n+   * consume, then we fall back to a 1-1 mapping of Spark tasks to Kafka partitions. If\n+   * `numPartitions` is set higher than the number of our `topicPartitions`, then we will split up\n+   * the read tasks of the skewed partitions to multiple Spark tasks.\n+   * The number of Spark tasks will be *approximately* `numPartitions`. It can be less or more\n+   * depending on rounding errors or Kafka partitions that didn't receive any new data.\n+   */\n+  def getRanges(\n+      fromOffsets: PartitionOffsetMap,\n+      untilOffsets: PartitionOffsetMap,\n+      executorLocations: Seq[String] = Seq.empty): Seq[KafkaOffsetRange] = {\n+    val partitionsToRead = untilOffsets.keySet.intersect(fromOffsets.keySet)\n+\n+    val offsetRanges = partitionsToRead.toSeq.map { tp =>\n+      KafkaOffsetRange(tp, fromOffsets(tp), untilOffsets(tp), preferredLoc = None)\n+    }\n+\n+    // If minPartitions not set or there are enough partitions to satisfy minPartitions\n+    if (minPartitions == DEFAULT_MIN_PARTITIONS || offsetRanges.size > minPartitions) {\n+      // Assign preferred executor locations to each range such that the same topic-partition is\n+      // always read from the same executor and the KafkaConsumer can be reused\n+      offsetRanges.map { range =>\n+        range.copy(preferredLoc = getLocation(range.topicPartition, executorLocations))\n+      }\n+    } else {\n+\n+      // Splits offset ranges with relatively large amount of data to smaller ones.\n+      val totalSize = offsetRanges.map(o => o.untilOffset - o.fromOffset).sum\n+      offsetRanges.flatMap { offsetRange =>\n+        val tp = offsetRange.topicPartition\n+        val size = offsetRange.untilOffset - offsetRange.fromOffset\n+        // number of partitions to divvy up this topic partition to\n+        val parts = math.max(math.round(size * 1.0 / totalSize * minPartitions), 1).toInt\n+        var remaining = size\n+        var startOffset = offsetRange.fromOffset\n+        (0 until parts).map { part =>\n+          // Fine to do integer division. Last partition will consume all the round off errors\n+          val thisPartition = remaining / (parts - part)\n+          remaining -= thisPartition\n+          val endOffset = startOffset + thisPartition\n+          val offsetRange = KafkaOffsetRange(tp, startOffset, endOffset, preferredLoc = None)\n+          startOffset = endOffset\n+          offsetRange\n+        }\n+      }\n+    }\n+  }\n+\n+  private def getLocation(tp: TopicPartition, executorLocations: Seq[String]): Option[String] = {\n+    def floorMod(a: Long, b: Int): Int = ((a % b).toInt + b) % b\n+\n+    val numExecutors = executorLocations.length\n+    if (numExecutors > 0) {\n+      // This allows cached KafkaConsumers in the executors to be re-used to read the same\n+      // partition in every batch.\n+      Some(executorLocations(floorMod(tp.hashCode, numExecutors)))\n+    } else None\n+  }\n+}\n+\n+private[kafka010] object KafkaOffsetRangeCalculator {\n+\n+  private val DEFAULT_MIN_PARTITIONS = 0"
  }],
  "prId": 20698
}, {
  "comments": [{
    "author": {
      "login": "jose-torres"
    },
    "body": "I worry that \"always\" is misleading here. It's not guaranteed that the same executor will run the partition or that the KafkaConsumer can be reused.",
    "commit": "602ab36490a692080682867f98a8a5d8f7b2390d",
    "createdAt": "2018-03-01T20:33:47Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.sql.sources.v2.DataSourceOptions\n+\n+\n+/**\n+ * Class to calculate offset ranges to process based on the the from and until offsets, and\n+ * the configured `minPartitions`.\n+ */\n+private[kafka010] class KafkaOffsetRangeCalculator(val minPartitions: Int) {\n+  require(minPartitions >= 0)\n+\n+  import KafkaOffsetRangeCalculator._\n+  /**\n+   * Calculate the offset ranges that we are going to process this batch. If `numPartitions`\n+   * is not set or is set less than or equal the number of `topicPartitions` that we're going to\n+   * consume, then we fall back to a 1-1 mapping of Spark tasks to Kafka partitions. If\n+   * `numPartitions` is set higher than the number of our `topicPartitions`, then we will split up\n+   * the read tasks of the skewed partitions to multiple Spark tasks.\n+   * The number of Spark tasks will be *approximately* `numPartitions`. It can be less or more\n+   * depending on rounding errors or Kafka partitions that didn't receive any new data.\n+   */\n+  def getRanges(\n+      fromOffsets: PartitionOffsetMap,\n+      untilOffsets: PartitionOffsetMap,\n+      executorLocations: Seq[String] = Seq.empty): Seq[KafkaOffsetRange] = {\n+    val partitionsToRead = untilOffsets.keySet.intersect(fromOffsets.keySet)\n+\n+    val offsetRanges = partitionsToRead.toSeq.map { tp =>\n+      KafkaOffsetRange(tp, fromOffsets(tp), untilOffsets(tp), preferredLoc = None)\n+    }\n+\n+    // If minPartitions not set or there are enough partitions to satisfy minPartitions\n+    if (minPartitions == DEFAULT_MIN_PARTITIONS || offsetRanges.size > minPartitions) {\n+      // Assign preferred executor locations to each range such that the same topic-partition is\n+      // always read from the same executor and the KafkaConsumer can be reused"
  }],
  "prId": 20698
}, {
  "comments": [{
    "author": {
      "login": "jose-torres"
    },
    "body": "It's hard to understand why this number is being calculated as it is. I think it's correct, but a comment explaining why this is the right number to divvy would help.",
    "commit": "602ab36490a692080682867f98a8a5d8f7b2390d",
    "createdAt": "2018-03-01T21:27:33Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.sql.sources.v2.DataSourceOptions\n+\n+\n+/**\n+ * Class to calculate offset ranges to process based on the the from and until offsets, and\n+ * the configured `minPartitions`.\n+ */\n+private[kafka010] class KafkaOffsetRangeCalculator(val minPartitions: Int) {\n+  require(minPartitions >= 0)\n+\n+  import KafkaOffsetRangeCalculator._\n+  /**\n+   * Calculate the offset ranges that we are going to process this batch. If `numPartitions`\n+   * is not set or is set less than or equal the number of `topicPartitions` that we're going to\n+   * consume, then we fall back to a 1-1 mapping of Spark tasks to Kafka partitions. If\n+   * `numPartitions` is set higher than the number of our `topicPartitions`, then we will split up\n+   * the read tasks of the skewed partitions to multiple Spark tasks.\n+   * The number of Spark tasks will be *approximately* `numPartitions`. It can be less or more\n+   * depending on rounding errors or Kafka partitions that didn't receive any new data.\n+   */\n+  def getRanges(\n+      fromOffsets: PartitionOffsetMap,\n+      untilOffsets: PartitionOffsetMap,\n+      executorLocations: Seq[String] = Seq.empty): Seq[KafkaOffsetRange] = {\n+    val partitionsToRead = untilOffsets.keySet.intersect(fromOffsets.keySet)\n+\n+    val offsetRanges = partitionsToRead.toSeq.map { tp =>\n+      KafkaOffsetRange(tp, fromOffsets(tp), untilOffsets(tp), preferredLoc = None)\n+    }\n+\n+    // If minPartitions not set or there are enough partitions to satisfy minPartitions\n+    if (minPartitions == DEFAULT_MIN_PARTITIONS || offsetRanges.size > minPartitions) {\n+      // Assign preferred executor locations to each range such that the same topic-partition is\n+      // always read from the same executor and the KafkaConsumer can be reused\n+      offsetRanges.map { range =>\n+        range.copy(preferredLoc = getLocation(range.topicPartition, executorLocations))\n+      }\n+    } else {\n+\n+      // Splits offset ranges with relatively large amount of data to smaller ones.\n+      val totalSize = offsetRanges.map(o => o.untilOffset - o.fromOffset).sum\n+      offsetRanges.flatMap { offsetRange =>\n+        val tp = offsetRange.topicPartition\n+        val size = offsetRange.untilOffset - offsetRange.fromOffset\n+        // number of partitions to divvy up this topic partition to\n+        val parts = math.max(math.round(size * 1.0 / totalSize * minPartitions), 1).toInt"
  }, {
    "author": {
      "login": "brkyvz"
    },
    "body": "yeah, a comment about how this is calculating the `weight` of partitions to assign to this topic would help. In addition, the sum of `parts` after this calculation will be `>= minPartitions`",
    "commit": "602ab36490a692080682867f98a8a5d8f7b2390d",
    "createdAt": "2018-03-02T00:09:45Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.sql.sources.v2.DataSourceOptions\n+\n+\n+/**\n+ * Class to calculate offset ranges to process based on the the from and until offsets, and\n+ * the configured `minPartitions`.\n+ */\n+private[kafka010] class KafkaOffsetRangeCalculator(val minPartitions: Int) {\n+  require(minPartitions >= 0)\n+\n+  import KafkaOffsetRangeCalculator._\n+  /**\n+   * Calculate the offset ranges that we are going to process this batch. If `numPartitions`\n+   * is not set or is set less than or equal the number of `topicPartitions` that we're going to\n+   * consume, then we fall back to a 1-1 mapping of Spark tasks to Kafka partitions. If\n+   * `numPartitions` is set higher than the number of our `topicPartitions`, then we will split up\n+   * the read tasks of the skewed partitions to multiple Spark tasks.\n+   * The number of Spark tasks will be *approximately* `numPartitions`. It can be less or more\n+   * depending on rounding errors or Kafka partitions that didn't receive any new data.\n+   */\n+  def getRanges(\n+      fromOffsets: PartitionOffsetMap,\n+      untilOffsets: PartitionOffsetMap,\n+      executorLocations: Seq[String] = Seq.empty): Seq[KafkaOffsetRange] = {\n+    val partitionsToRead = untilOffsets.keySet.intersect(fromOffsets.keySet)\n+\n+    val offsetRanges = partitionsToRead.toSeq.map { tp =>\n+      KafkaOffsetRange(tp, fromOffsets(tp), untilOffsets(tp), preferredLoc = None)\n+    }\n+\n+    // If minPartitions not set or there are enough partitions to satisfy minPartitions\n+    if (minPartitions == DEFAULT_MIN_PARTITIONS || offsetRanges.size > minPartitions) {\n+      // Assign preferred executor locations to each range such that the same topic-partition is\n+      // always read from the same executor and the KafkaConsumer can be reused\n+      offsetRanges.map { range =>\n+        range.copy(preferredLoc = getLocation(range.topicPartition, executorLocations))\n+      }\n+    } else {\n+\n+      // Splits offset ranges with relatively large amount of data to smaller ones.\n+      val totalSize = offsetRanges.map(o => o.untilOffset - o.fromOffset).sum\n+      offsetRanges.flatMap { offsetRange =>\n+        val tp = offsetRange.topicPartition\n+        val size = offsetRange.untilOffset - offsetRange.fromOffset\n+        // number of partitions to divvy up this topic partition to\n+        val parts = math.max(math.round(size * 1.0 / totalSize * minPartitions), 1).toInt"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "I rewrote this completely using the code used by from `sparkContext.parallelize` to make splits. \r\nhttps://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala#L123",
    "commit": "602ab36490a692080682867f98a8a5d8f7b2390d",
    "createdAt": "2018-03-02T02:17:15Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.sql.sources.v2.DataSourceOptions\n+\n+\n+/**\n+ * Class to calculate offset ranges to process based on the the from and until offsets, and\n+ * the configured `minPartitions`.\n+ */\n+private[kafka010] class KafkaOffsetRangeCalculator(val minPartitions: Int) {\n+  require(minPartitions >= 0)\n+\n+  import KafkaOffsetRangeCalculator._\n+  /**\n+   * Calculate the offset ranges that we are going to process this batch. If `numPartitions`\n+   * is not set or is set less than or equal the number of `topicPartitions` that we're going to\n+   * consume, then we fall back to a 1-1 mapping of Spark tasks to Kafka partitions. If\n+   * `numPartitions` is set higher than the number of our `topicPartitions`, then we will split up\n+   * the read tasks of the skewed partitions to multiple Spark tasks.\n+   * The number of Spark tasks will be *approximately* `numPartitions`. It can be less or more\n+   * depending on rounding errors or Kafka partitions that didn't receive any new data.\n+   */\n+  def getRanges(\n+      fromOffsets: PartitionOffsetMap,\n+      untilOffsets: PartitionOffsetMap,\n+      executorLocations: Seq[String] = Seq.empty): Seq[KafkaOffsetRange] = {\n+    val partitionsToRead = untilOffsets.keySet.intersect(fromOffsets.keySet)\n+\n+    val offsetRanges = partitionsToRead.toSeq.map { tp =>\n+      KafkaOffsetRange(tp, fromOffsets(tp), untilOffsets(tp), preferredLoc = None)\n+    }\n+\n+    // If minPartitions not set or there are enough partitions to satisfy minPartitions\n+    if (minPartitions == DEFAULT_MIN_PARTITIONS || offsetRanges.size > minPartitions) {\n+      // Assign preferred executor locations to each range such that the same topic-partition is\n+      // always read from the same executor and the KafkaConsumer can be reused\n+      offsetRanges.map { range =>\n+        range.copy(preferredLoc = getLocation(range.topicPartition, executorLocations))\n+      }\n+    } else {\n+\n+      // Splits offset ranges with relatively large amount of data to smaller ones.\n+      val totalSize = offsetRanges.map(o => o.untilOffset - o.fromOffset).sum\n+      offsetRanges.flatMap { offsetRange =>\n+        val tp = offsetRange.topicPartition\n+        val size = offsetRange.untilOffset - offsetRange.fromOffset\n+        // number of partitions to divvy up this topic partition to\n+        val parts = math.max(math.round(size * 1.0 / totalSize * minPartitions), 1).toInt"
  }],
  "prId": 20698
}, {
  "comments": [{
    "author": {
      "login": "brkyvz"
    },
    "body": "I don't think we need the first check. `offsetRanges.size` should be greater than 0 right? Otherwise we wouldn't have called into this.",
    "commit": "602ab36490a692080682867f98a8a5d8f7b2390d",
    "createdAt": "2018-03-02T00:06:54Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.sql.sources.v2.DataSourceOptions\n+\n+\n+/**\n+ * Class to calculate offset ranges to process based on the the from and until offsets, and\n+ * the configured `minPartitions`.\n+ */\n+private[kafka010] class KafkaOffsetRangeCalculator(val minPartitions: Int) {\n+  require(minPartitions >= 0)\n+\n+  import KafkaOffsetRangeCalculator._\n+  /**\n+   * Calculate the offset ranges that we are going to process this batch. If `numPartitions`\n+   * is not set or is set less than or equal the number of `topicPartitions` that we're going to\n+   * consume, then we fall back to a 1-1 mapping of Spark tasks to Kafka partitions. If\n+   * `numPartitions` is set higher than the number of our `topicPartitions`, then we will split up\n+   * the read tasks of the skewed partitions to multiple Spark tasks.\n+   * The number of Spark tasks will be *approximately* `numPartitions`. It can be less or more\n+   * depending on rounding errors or Kafka partitions that didn't receive any new data.\n+   */\n+  def getRanges(\n+      fromOffsets: PartitionOffsetMap,\n+      untilOffsets: PartitionOffsetMap,\n+      executorLocations: Seq[String] = Seq.empty): Seq[KafkaOffsetRange] = {\n+    val partitionsToRead = untilOffsets.keySet.intersect(fromOffsets.keySet)\n+\n+    val offsetRanges = partitionsToRead.toSeq.map { tp =>\n+      KafkaOffsetRange(tp, fromOffsets(tp), untilOffsets(tp), preferredLoc = None)\n+    }\n+\n+    // If minPartitions not set or there are enough partitions to satisfy minPartitions\n+    if (minPartitions == DEFAULT_MIN_PARTITIONS || offsetRanges.size > minPartitions) {"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Rewritten. I dont want to rely on this default value of 0, as @jose-torres expressed concern earlier. So i rewrote this to explicitly check whether minPartitions have been set or not.",
    "commit": "602ab36490a692080682867f98a8a5d8f7b2390d",
    "createdAt": "2018-03-02T02:14:16Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.sql.sources.v2.DataSourceOptions\n+\n+\n+/**\n+ * Class to calculate offset ranges to process based on the the from and until offsets, and\n+ * the configured `minPartitions`.\n+ */\n+private[kafka010] class KafkaOffsetRangeCalculator(val minPartitions: Int) {\n+  require(minPartitions >= 0)\n+\n+  import KafkaOffsetRangeCalculator._\n+  /**\n+   * Calculate the offset ranges that we are going to process this batch. If `numPartitions`\n+   * is not set or is set less than or equal the number of `topicPartitions` that we're going to\n+   * consume, then we fall back to a 1-1 mapping of Spark tasks to Kafka partitions. If\n+   * `numPartitions` is set higher than the number of our `topicPartitions`, then we will split up\n+   * the read tasks of the skewed partitions to multiple Spark tasks.\n+   * The number of Spark tasks will be *approximately* `numPartitions`. It can be less or more\n+   * depending on rounding errors or Kafka partitions that didn't receive any new data.\n+   */\n+  def getRanges(\n+      fromOffsets: PartitionOffsetMap,\n+      untilOffsets: PartitionOffsetMap,\n+      executorLocations: Seq[String] = Seq.empty): Seq[KafkaOffsetRange] = {\n+    val partitionsToRead = untilOffsets.keySet.intersect(fromOffsets.keySet)\n+\n+    val offsetRanges = partitionsToRead.toSeq.map { tp =>\n+      KafkaOffsetRange(tp, fromOffsets(tp), untilOffsets(tp), preferredLoc = None)\n+    }\n+\n+    // If minPartitions not set or there are enough partitions to satisfy minPartitions\n+    if (minPartitions == DEFAULT_MIN_PARTITIONS || offsetRanges.size > minPartitions) {"
  }],
  "prId": 20698
}, {
  "comments": [{
    "author": {
      "login": "brkyvz"
    },
    "body": "was this check here before? What if there are new topic partitions? Are we missing those, because they may not exist in fromOffsets?",
    "commit": "602ab36490a692080682867f98a8a5d8f7b2390d",
    "createdAt": "2018-03-02T00:07:45Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.sql.sources.v2.DataSourceOptions\n+\n+\n+/**\n+ * Class to calculate offset ranges to process based on the the from and until offsets, and\n+ * the configured `minPartitions`.\n+ */\n+private[kafka010] class KafkaOffsetRangeCalculator(val minPartitions: Int) {\n+  require(minPartitions >= 0)\n+\n+  import KafkaOffsetRangeCalculator._\n+  /**\n+   * Calculate the offset ranges that we are going to process this batch. If `numPartitions`\n+   * is not set or is set less than or equal the number of `topicPartitions` that we're going to\n+   * consume, then we fall back to a 1-1 mapping of Spark tasks to Kafka partitions. If\n+   * `numPartitions` is set higher than the number of our `topicPartitions`, then we will split up\n+   * the read tasks of the skewed partitions to multiple Spark tasks.\n+   * The number of Spark tasks will be *approximately* `numPartitions`. It can be less or more\n+   * depending on rounding errors or Kafka partitions that didn't receive any new data.\n+   */\n+  def getRanges(\n+      fromOffsets: PartitionOffsetMap,\n+      untilOffsets: PartitionOffsetMap,\n+      executorLocations: Seq[String] = Seq.empty): Seq[KafkaOffsetRange] = {\n+    val partitionsToRead = untilOffsets.keySet.intersect(fromOffsets.keySet)",
    "line": 46
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "`fromOffsets` here will contain the initial offsets of new partitions. See the how fromOffsets is set with `startOffsets + newPartitionInitialOffsets`.",
    "commit": "602ab36490a692080682867f98a8a5d8f7b2390d",
    "createdAt": "2018-03-02T02:15:40Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.sql.sources.v2.DataSourceOptions\n+\n+\n+/**\n+ * Class to calculate offset ranges to process based on the the from and until offsets, and\n+ * the configured `minPartitions`.\n+ */\n+private[kafka010] class KafkaOffsetRangeCalculator(val minPartitions: Int) {\n+  require(minPartitions >= 0)\n+\n+  import KafkaOffsetRangeCalculator._\n+  /**\n+   * Calculate the offset ranges that we are going to process this batch. If `numPartitions`\n+   * is not set or is set less than or equal the number of `topicPartitions` that we're going to\n+   * consume, then we fall back to a 1-1 mapping of Spark tasks to Kafka partitions. If\n+   * `numPartitions` is set higher than the number of our `topicPartitions`, then we will split up\n+   * the read tasks of the skewed partitions to multiple Spark tasks.\n+   * The number of Spark tasks will be *approximately* `numPartitions`. It can be less or more\n+   * depending on rounding errors or Kafka partitions that didn't receive any new data.\n+   */\n+  def getRanges(\n+      fromOffsets: PartitionOffsetMap,\n+      untilOffsets: PartitionOffsetMap,\n+      executorLocations: Seq[String] = Seq.empty): Seq[KafkaOffsetRange] = {\n+    val partitionsToRead = untilOffsets.keySet.intersect(fromOffsets.keySet)",
    "line": 46
  }],
  "prId": 20698
}, {
  "comments": [{
    "author": {
      "login": "brkyvz"
    },
    "body": "nit: extra line",
    "commit": "602ab36490a692080682867f98a8a5d8f7b2390d",
    "createdAt": "2018-03-02T22:48:40Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.sql.sources.v2.DataSourceOptions\n+\n+\n+/**\n+ * Class to calculate offset ranges to process based on the the from and until offsets, and\n+ * the configured `minPartitions`.\n+ */\n+private[kafka010] class KafkaOffsetRangeCalculator(val minPartitions: Option[Int]) {\n+  require(minPartitions.isEmpty || minPartitions.get > 0)\n+\n+  import KafkaOffsetRangeCalculator._\n+  /**\n+   * Calculate the offset ranges that we are going to process this batch. If `minPartitions`\n+   * is not set or is set less than or equal the number of `topicPartitions` that we're going to\n+   * consume, then we fall back to a 1-1 mapping of Spark tasks to Kafka partitions. If\n+   * `numPartitions` is set higher than the number of our `topicPartitions`, then we will split up\n+   * the read tasks of the skewed partitions to multiple Spark tasks.\n+   * The number of Spark tasks will be *approximately* `numPartitions`. It can be less or more\n+   * depending on rounding errors or Kafka partitions that didn't receive any new data.\n+   */\n+  def getRanges(\n+      fromOffsets: PartitionOffsetMap,\n+      untilOffsets: PartitionOffsetMap,\n+      executorLocations: Seq[String] = Seq.empty): Seq[KafkaOffsetRange] = {\n+    val partitionsToRead = untilOffsets.keySet.intersect(fromOffsets.keySet)\n+\n+    val offsetRanges = partitionsToRead.toSeq.map { tp =>\n+      KafkaOffsetRange(tp, fromOffsets(tp), untilOffsets(tp), preferredLoc = None)\n+    }\n+\n+    // If minPartitions not set or there are enough partitions to satisfy minPartitions\n+    if (minPartitions.isEmpty || offsetRanges.size > minPartitions.get) {\n+      // Assign preferred executor locations to each range such that the same topic-partition is\n+      // preferentially read from the same executor and the KafkaConsumer can be reused.\n+      offsetRanges.map { range =>\n+        range.copy(preferredLoc = getLocation(range.topicPartition, executorLocations))\n+      }\n+    } else {\n+\n+      // Splits offset ranges with relatively large amount of data to smaller ones.\n+      val totalSize = offsetRanges.map(o => o.untilOffset - o.fromOffset).sum\n+      val idealRangeSize = totalSize.toDouble / minPartitions.get\n+\n+      offsetRanges.flatMap { range =>\n+        // Split the current range into subranges as close to the ideal range size\n+        val rangeSize = range.untilOffset - range.fromOffset\n+        val numSplitsInRange = math.round(rangeSize.toDouble / idealRangeSize).toInt\n+\n+        (0 until numSplitsInRange).map { i =>\n+          val splitStart = range.fromOffset + rangeSize * (i.toDouble / numSplitsInRange)\n+          val splitEnd = range.fromOffset + rangeSize * ((i.toDouble + 1) / numSplitsInRange)\n+          KafkaOffsetRange(\n+            range.topicPartition, splitStart.toLong, splitEnd.toLong, preferredLoc = None)\n+        }\n+"
  }],
  "prId": 20698
}, {
  "comments": [{
    "author": {
      "login": "brkyvz"
    },
    "body": "nit: `map(_.size).sum`",
    "commit": "602ab36490a692080682867f98a8a5d8f7b2390d",
    "createdAt": "2018-03-02T23:20:11Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.sql.sources.v2.DataSourceOptions\n+\n+\n+/**\n+ * Class to calculate offset ranges to process based on the the from and until offsets, and\n+ * the configured `minPartitions`.\n+ */\n+private[kafka010] class KafkaOffsetRangeCalculator(val minPartitions: Option[Int]) {\n+  require(minPartitions.isEmpty || minPartitions.get > 0)\n+\n+  import KafkaOffsetRangeCalculator._\n+  /**\n+   * Calculate the offset ranges that we are going to process this batch. If `minPartitions`\n+   * is not set or is set less than or equal the number of `topicPartitions` that we're going to\n+   * consume, then we fall back to a 1-1 mapping of Spark tasks to Kafka partitions. If\n+   * `numPartitions` is set higher than the number of our `topicPartitions`, then we will split up\n+   * the read tasks of the skewed partitions to multiple Spark tasks.\n+   * The number of Spark tasks will be *approximately* `numPartitions`. It can be less or more\n+   * depending on rounding errors or Kafka partitions that didn't receive any new data.\n+   */\n+  def getRanges(\n+      fromOffsets: PartitionOffsetMap,\n+      untilOffsets: PartitionOffsetMap,\n+      executorLocations: Seq[String] = Seq.empty): Seq[KafkaOffsetRange] = {\n+    val partitionsToRead = untilOffsets.keySet.intersect(fromOffsets.keySet)\n+\n+    val offsetRanges = partitionsToRead.toSeq.map { tp =>\n+      KafkaOffsetRange(tp, fromOffsets(tp), untilOffsets(tp), preferredLoc = None)\n+    }.filter(_.size > 0)\n+\n+    // If minPartitions not set or there are enough partitions to satisfy minPartitions\n+    if (minPartitions.isEmpty || offsetRanges.size > minPartitions.get) {\n+      // Assign preferred executor locations to each range such that the same topic-partition is\n+      // preferentially read from the same executor and the KafkaConsumer can be reused.\n+      offsetRanges.map { range =>\n+        range.copy(preferredLoc = getLocation(range.topicPartition, executorLocations))\n+      }\n+    } else {\n+\n+      // Splits offset ranges with relatively large amount of data to smaller ones.\n+      val totalSize = offsetRanges.map(o => o.untilOffset - o.fromOffset).sum"
  }],
  "prId": 20698
}, {
  "comments": [{
    "author": {
      "login": "brkyvz"
    },
    "body": "nit: `range.size`, you may remove `rangeSize` above",
    "commit": "602ab36490a692080682867f98a8a5d8f7b2390d",
    "createdAt": "2018-03-02T23:21:01Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.sql.sources.v2.DataSourceOptions\n+\n+\n+/**\n+ * Class to calculate offset ranges to process based on the the from and until offsets, and\n+ * the configured `minPartitions`.\n+ */\n+private[kafka010] class KafkaOffsetRangeCalculator(val minPartitions: Option[Int]) {\n+  require(minPartitions.isEmpty || minPartitions.get > 0)\n+\n+  import KafkaOffsetRangeCalculator._\n+  /**\n+   * Calculate the offset ranges that we are going to process this batch. If `minPartitions`\n+   * is not set or is set less than or equal the number of `topicPartitions` that we're going to\n+   * consume, then we fall back to a 1-1 mapping of Spark tasks to Kafka partitions. If\n+   * `numPartitions` is set higher than the number of our `topicPartitions`, then we will split up\n+   * the read tasks of the skewed partitions to multiple Spark tasks.\n+   * The number of Spark tasks will be *approximately* `numPartitions`. It can be less or more\n+   * depending on rounding errors or Kafka partitions that didn't receive any new data.\n+   */\n+  def getRanges(\n+      fromOffsets: PartitionOffsetMap,\n+      untilOffsets: PartitionOffsetMap,\n+      executorLocations: Seq[String] = Seq.empty): Seq[KafkaOffsetRange] = {\n+    val partitionsToRead = untilOffsets.keySet.intersect(fromOffsets.keySet)\n+\n+    val offsetRanges = partitionsToRead.toSeq.map { tp =>\n+      KafkaOffsetRange(tp, fromOffsets(tp), untilOffsets(tp), preferredLoc = None)\n+    }.filter(_.size > 0)\n+\n+    // If minPartitions not set or there are enough partitions to satisfy minPartitions\n+    if (minPartitions.isEmpty || offsetRanges.size > minPartitions.get) {\n+      // Assign preferred executor locations to each range such that the same topic-partition is\n+      // preferentially read from the same executor and the KafkaConsumer can be reused.\n+      offsetRanges.map { range =>\n+        range.copy(preferredLoc = getLocation(range.topicPartition, executorLocations))\n+      }\n+    } else {\n+\n+      // Splits offset ranges with relatively large amount of data to smaller ones.\n+      val totalSize = offsetRanges.map(o => o.untilOffset - o.fromOffset).sum\n+      val idealRangeSize = totalSize.toDouble / minPartitions.get\n+\n+      offsetRanges.flatMap { range =>\n+        // Split the current range into subranges as close to the ideal range size\n+        val rangeSize = range.untilOffset - range.fromOffset\n+        val numSplitsInRange = math.round(rangeSize.toDouble / idealRangeSize).toInt"
  }],
  "prId": 20698
}, {
  "comments": [{
    "author": {
      "login": "brkyvz"
    },
    "body": "nit: `.orNull` instead of `.orElse(null)`. Why don't you actually do:\r\n```\r\noptions.get(\"minPartitions\").map(_.toInt)\r\n```",
    "commit": "602ab36490a692080682867f98a8a5d8f7b2390d",
    "createdAt": "2018-03-02T23:22:15Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.sql.sources.v2.DataSourceOptions\n+\n+\n+/**\n+ * Class to calculate offset ranges to process based on the the from and until offsets, and\n+ * the configured `minPartitions`.\n+ */\n+private[kafka010] class KafkaOffsetRangeCalculator(val minPartitions: Option[Int]) {\n+  require(minPartitions.isEmpty || minPartitions.get > 0)\n+\n+  import KafkaOffsetRangeCalculator._\n+  /**\n+   * Calculate the offset ranges that we are going to process this batch. If `minPartitions`\n+   * is not set or is set less than or equal the number of `topicPartitions` that we're going to\n+   * consume, then we fall back to a 1-1 mapping of Spark tasks to Kafka partitions. If\n+   * `numPartitions` is set higher than the number of our `topicPartitions`, then we will split up\n+   * the read tasks of the skewed partitions to multiple Spark tasks.\n+   * The number of Spark tasks will be *approximately* `numPartitions`. It can be less or more\n+   * depending on rounding errors or Kafka partitions that didn't receive any new data.\n+   */\n+  def getRanges(\n+      fromOffsets: PartitionOffsetMap,\n+      untilOffsets: PartitionOffsetMap,\n+      executorLocations: Seq[String] = Seq.empty): Seq[KafkaOffsetRange] = {\n+    val partitionsToRead = untilOffsets.keySet.intersect(fromOffsets.keySet)\n+\n+    val offsetRanges = partitionsToRead.toSeq.map { tp =>\n+      KafkaOffsetRange(tp, fromOffsets(tp), untilOffsets(tp), preferredLoc = None)\n+    }.filter(_.size > 0)\n+\n+    // If minPartitions not set or there are enough partitions to satisfy minPartitions\n+    if (minPartitions.isEmpty || offsetRanges.size > minPartitions.get) {\n+      // Assign preferred executor locations to each range such that the same topic-partition is\n+      // preferentially read from the same executor and the KafkaConsumer can be reused.\n+      offsetRanges.map { range =>\n+        range.copy(preferredLoc = getLocation(range.topicPartition, executorLocations))\n+      }\n+    } else {\n+\n+      // Splits offset ranges with relatively large amount of data to smaller ones.\n+      val totalSize = offsetRanges.map(o => o.untilOffset - o.fromOffset).sum\n+      val idealRangeSize = totalSize.toDouble / minPartitions.get\n+\n+      offsetRanges.flatMap { range =>\n+        // Split the current range into subranges as close to the ideal range size\n+        val rangeSize = range.untilOffset - range.fromOffset\n+        val numSplitsInRange = math.round(rangeSize.toDouble / idealRangeSize).toInt\n+\n+        (0 until numSplitsInRange).map { i =>\n+          val splitStart = range.fromOffset + rangeSize * (i.toDouble / numSplitsInRange)\n+          val splitEnd = range.fromOffset + rangeSize * ((i.toDouble + 1) / numSplitsInRange)\n+          KafkaOffsetRange(\n+            range.topicPartition, splitStart.toLong, splitEnd.toLong, preferredLoc = None)\n+        }\n+      }\n+    }\n+  }\n+\n+  private def getLocation(tp: TopicPartition, executorLocations: Seq[String]): Option[String] = {\n+    def floorMod(a: Long, b: Int): Int = ((a % b).toInt + b) % b\n+\n+    val numExecutors = executorLocations.length\n+    if (numExecutors > 0) {\n+      // This allows cached KafkaConsumers in the executors to be re-used to read the same\n+      // partition in every batch.\n+      Some(executorLocations(floorMod(tp.hashCode, numExecutors)))\n+    } else None\n+  }\n+}\n+\n+private[kafka010] object KafkaOffsetRangeCalculator {\n+\n+  def apply(options: DataSourceOptions): KafkaOffsetRangeCalculator = {\n+    val optionalValue = Option(options.get(\"minPartitions\").orElse(null)).map(_.toInt)",
    "line": 94
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Because it returns java Optional and not scala Option. ",
    "commit": "602ab36490a692080682867f98a8a5d8f7b2390d",
    "createdAt": "2018-03-02T23:34:51Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.sql.sources.v2.DataSourceOptions\n+\n+\n+/**\n+ * Class to calculate offset ranges to process based on the the from and until offsets, and\n+ * the configured `minPartitions`.\n+ */\n+private[kafka010] class KafkaOffsetRangeCalculator(val minPartitions: Option[Int]) {\n+  require(minPartitions.isEmpty || minPartitions.get > 0)\n+\n+  import KafkaOffsetRangeCalculator._\n+  /**\n+   * Calculate the offset ranges that we are going to process this batch. If `minPartitions`\n+   * is not set or is set less than or equal the number of `topicPartitions` that we're going to\n+   * consume, then we fall back to a 1-1 mapping of Spark tasks to Kafka partitions. If\n+   * `numPartitions` is set higher than the number of our `topicPartitions`, then we will split up\n+   * the read tasks of the skewed partitions to multiple Spark tasks.\n+   * The number of Spark tasks will be *approximately* `numPartitions`. It can be less or more\n+   * depending on rounding errors or Kafka partitions that didn't receive any new data.\n+   */\n+  def getRanges(\n+      fromOffsets: PartitionOffsetMap,\n+      untilOffsets: PartitionOffsetMap,\n+      executorLocations: Seq[String] = Seq.empty): Seq[KafkaOffsetRange] = {\n+    val partitionsToRead = untilOffsets.keySet.intersect(fromOffsets.keySet)\n+\n+    val offsetRanges = partitionsToRead.toSeq.map { tp =>\n+      KafkaOffsetRange(tp, fromOffsets(tp), untilOffsets(tp), preferredLoc = None)\n+    }.filter(_.size > 0)\n+\n+    // If minPartitions not set or there are enough partitions to satisfy minPartitions\n+    if (minPartitions.isEmpty || offsetRanges.size > minPartitions.get) {\n+      // Assign preferred executor locations to each range such that the same topic-partition is\n+      // preferentially read from the same executor and the KafkaConsumer can be reused.\n+      offsetRanges.map { range =>\n+        range.copy(preferredLoc = getLocation(range.topicPartition, executorLocations))\n+      }\n+    } else {\n+\n+      // Splits offset ranges with relatively large amount of data to smaller ones.\n+      val totalSize = offsetRanges.map(o => o.untilOffset - o.fromOffset).sum\n+      val idealRangeSize = totalSize.toDouble / minPartitions.get\n+\n+      offsetRanges.flatMap { range =>\n+        // Split the current range into subranges as close to the ideal range size\n+        val rangeSize = range.untilOffset - range.fromOffset\n+        val numSplitsInRange = math.round(rangeSize.toDouble / idealRangeSize).toInt\n+\n+        (0 until numSplitsInRange).map { i =>\n+          val splitStart = range.fromOffset + rangeSize * (i.toDouble / numSplitsInRange)\n+          val splitEnd = range.fromOffset + rangeSize * ((i.toDouble + 1) / numSplitsInRange)\n+          KafkaOffsetRange(\n+            range.topicPartition, splitStart.toLong, splitEnd.toLong, preferredLoc = None)\n+        }\n+      }\n+    }\n+  }\n+\n+  private def getLocation(tp: TopicPartition, executorLocations: Seq[String]): Option[String] = {\n+    def floorMod(a: Long, b: Int): Int = ((a % b).toInt + b) % b\n+\n+    val numExecutors = executorLocations.length\n+    if (numExecutors > 0) {\n+      // This allows cached KafkaConsumers in the executors to be re-used to read the same\n+      // partition in every batch.\n+      Some(executorLocations(floorMod(tp.hashCode, numExecutors)))\n+    } else None\n+  }\n+}\n+\n+private[kafka010] object KafkaOffsetRangeCalculator {\n+\n+  def apply(options: DataSourceOptions): KafkaOffsetRangeCalculator = {\n+    val optionalValue = Option(options.get(\"minPartitions\").orElse(null)).map(_.toInt)",
    "line": 94
  }],
  "prId": 20698
}, {
  "comments": [{
    "author": {
      "login": "brkyvz"
    },
    "body": "nite: maybe make this a `lazy val` so that it'll be calculated once",
    "commit": "602ab36490a692080682867f98a8a5d8f7b2390d",
    "createdAt": "2018-03-02T23:22:45Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.sql.sources.v2.DataSourceOptions\n+\n+\n+/**\n+ * Class to calculate offset ranges to process based on the the from and until offsets, and\n+ * the configured `minPartitions`.\n+ */\n+private[kafka010] class KafkaOffsetRangeCalculator(val minPartitions: Option[Int]) {\n+  require(minPartitions.isEmpty || minPartitions.get > 0)\n+\n+  import KafkaOffsetRangeCalculator._\n+  /**\n+   * Calculate the offset ranges that we are going to process this batch. If `minPartitions`\n+   * is not set or is set less than or equal the number of `topicPartitions` that we're going to\n+   * consume, then we fall back to a 1-1 mapping of Spark tasks to Kafka partitions. If\n+   * `numPartitions` is set higher than the number of our `topicPartitions`, then we will split up\n+   * the read tasks of the skewed partitions to multiple Spark tasks.\n+   * The number of Spark tasks will be *approximately* `numPartitions`. It can be less or more\n+   * depending on rounding errors or Kafka partitions that didn't receive any new data.\n+   */\n+  def getRanges(\n+      fromOffsets: PartitionOffsetMap,\n+      untilOffsets: PartitionOffsetMap,\n+      executorLocations: Seq[String] = Seq.empty): Seq[KafkaOffsetRange] = {\n+    val partitionsToRead = untilOffsets.keySet.intersect(fromOffsets.keySet)\n+\n+    val offsetRanges = partitionsToRead.toSeq.map { tp =>\n+      KafkaOffsetRange(tp, fromOffsets(tp), untilOffsets(tp), preferredLoc = None)\n+    }.filter(_.size > 0)\n+\n+    // If minPartitions not set or there are enough partitions to satisfy minPartitions\n+    if (minPartitions.isEmpty || offsetRanges.size > minPartitions.get) {\n+      // Assign preferred executor locations to each range such that the same topic-partition is\n+      // preferentially read from the same executor and the KafkaConsumer can be reused.\n+      offsetRanges.map { range =>\n+        range.copy(preferredLoc = getLocation(range.topicPartition, executorLocations))\n+      }\n+    } else {\n+\n+      // Splits offset ranges with relatively large amount of data to smaller ones.\n+      val totalSize = offsetRanges.map(o => o.untilOffset - o.fromOffset).sum\n+      val idealRangeSize = totalSize.toDouble / minPartitions.get\n+\n+      offsetRanges.flatMap { range =>\n+        // Split the current range into subranges as close to the ideal range size\n+        val rangeSize = range.untilOffset - range.fromOffset\n+        val numSplitsInRange = math.round(rangeSize.toDouble / idealRangeSize).toInt\n+\n+        (0 until numSplitsInRange).map { i =>\n+          val splitStart = range.fromOffset + rangeSize * (i.toDouble / numSplitsInRange)\n+          val splitEnd = range.fromOffset + rangeSize * ((i.toDouble + 1) / numSplitsInRange)\n+          KafkaOffsetRange(\n+            range.topicPartition, splitStart.toLong, splitEnd.toLong, preferredLoc = None)\n+        }\n+      }\n+    }\n+  }\n+\n+  private def getLocation(tp: TopicPartition, executorLocations: Seq[String]): Option[String] = {\n+    def floorMod(a: Long, b: Int): Int = ((a % b).toInt + b) % b\n+\n+    val numExecutors = executorLocations.length\n+    if (numExecutors > 0) {\n+      // This allows cached KafkaConsumers in the executors to be re-used to read the same\n+      // partition in every batch.\n+      Some(executorLocations(floorMod(tp.hashCode, numExecutors)))\n+    } else None\n+  }\n+}\n+\n+private[kafka010] object KafkaOffsetRangeCalculator {\n+\n+  def apply(options: DataSourceOptions): KafkaOffsetRangeCalculator = {\n+    val optionalValue = Option(options.get(\"minPartitions\").orElse(null)).map(_.toInt)\n+    new KafkaOffsetRangeCalculator(optionalValue)\n+  }\n+}\n+\n+private[kafka010] case class KafkaOffsetRange(\n+    topicPartition: TopicPartition,\n+    fromOffset: Long,\n+    untilOffset: Long,\n+    preferredLoc: Option[String]) {\n+  def size: Long = untilOffset - fromOffset"
  }],
  "prId": 20698
}]