[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "can we keep it same as before?\r\n```\r\nimport org.apache.spark.sql.sources.v2.reader.streaming.{Offset => OffsetV2, PartitionOffset}\r\n```",
    "commit": "1d90cf11710e452c5b2adf819da8003c8f96c6e1",
    "createdAt": "2018-01-31T03:00:59Z",
    "diffHunk": "@@ -20,14 +20,16 @@ package org.apache.spark.sql.kafka010\n import org.apache.kafka.common.TopicPartition\n \n import org.apache.spark.sql.execution.streaming.{Offset, SerializedOffset}\n-import org.apache.spark.sql.sources.v2.streaming.reader.{Offset => OffsetV2, PartitionOffset}\n+import org.apache.spark.sql.sources.v2.reader.streaming\n+import org.apache.spark.sql.sources.v2.reader.streaming.PartitionOffset"
  }],
  "prId": 20435
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "Should this `Offset` be `streaming.Offset`?",
    "commit": "1d90cf11710e452c5b2adf819da8003c8f96c6e1",
    "createdAt": "2018-01-31T04:58:05Z",
    "diffHunk": "@@ -20,14 +20,16 @@ package org.apache.spark.sql.kafka010\n import org.apache.kafka.common.TopicPartition\n \n import org.apache.spark.sql.execution.streaming.{Offset, SerializedOffset}\n-import org.apache.spark.sql.sources.v2.streaming.reader.{Offset => OffsetV2, PartitionOffset}\n+import org.apache.spark.sql.sources.v2.reader.streaming\n+import org.apache.spark.sql.sources.v2.reader.streaming.PartitionOffset\n \n /**\n  * An [[Offset]] for the [[KafkaSource]]. This one tracks all partitions of subscribed topics and"
  }],
  "prId": 20435
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "unnecessary change?",
    "commit": "1d90cf11710e452c5b2adf819da8003c8f96c6e1",
    "createdAt": "2018-01-31T13:16:51Z",
    "diffHunk": "@@ -20,14 +20,15 @@ package org.apache.spark.sql.kafka010\n import org.apache.kafka.common.TopicPartition\n \n import org.apache.spark.sql.execution.streaming.{Offset, SerializedOffset}\n-import org.apache.spark.sql.sources.v2.streaming.reader.{Offset => OffsetV2, PartitionOffset}\n+import org.apache.spark.sql.sources.v2.reader.streaming.{Offset => OffsetV2, PartitionOffset}\n \n /**\n  * An [[Offset]] for the [[KafkaSource]]. This one tracks all partitions of subscribed topics and\n  * their offsets.\n  */\n private[kafka010]\n-case class KafkaSourceOffset(partitionToOffsets: Map[TopicPartition, Long]) extends OffsetV2 {\n+case class KafkaSourceOffset(partitionToOffsets: Map[TopicPartition, Long])\n+  extends OffsetV2 {"
  }],
  "prId": 20435
}]