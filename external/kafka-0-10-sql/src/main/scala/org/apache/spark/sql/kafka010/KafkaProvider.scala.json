[{
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "nit: could you move these classes and `deserClassName` to `object KafkaProvider`?",
    "commit": "3bc7c4cb8cb93eef2275a517ce72f0bcb8919cfe",
    "createdAt": "2017-01-24T19:25:40Z",
    "diffHunk": "@@ -28,19 +28,27 @@ import org.apache.kafka.common.serialization.ByteArrayDeserializer\n import org.apache.spark.internal.Logging\n import org.apache.spark.sql.SQLContext\n import org.apache.spark.sql.execution.streaming.Source\n-import org.apache.spark.sql.kafka010.KafkaSource._\n-import org.apache.spark.sql.sources.{DataSourceRegister, StreamSourceProvider}\n+import org.apache.spark.sql.kafka010.KafkaOffsetReader.{AssignStrategy, SubscribePatternStrategy, SubscribeStrategy}\n+import org.apache.spark.sql.sources._\n import org.apache.spark.sql.types.StructType\n \n /**\n  * The provider class for the [[KafkaSource]]. This provider is designed such that it throws\n  * IllegalArgumentException when the Kafka Dataset is created, so that it can catch\n  * missing options even before the query is started.\n  */\n-private[kafka010] class KafkaSourceProvider extends StreamSourceProvider\n-  with DataSourceRegister with Logging {\n+private[kafka010] class KafkaProvider extends DataSourceRegister with StreamSourceProvider\n+  with RelationProvider with Logging {\n+  import KafkaProvider._\n \n-  import KafkaSourceProvider._\n+  // Used to check parameters for different source modes\n+  private sealed trait Mode"
  }],
  "prId": 16686
}]