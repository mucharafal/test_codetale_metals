[{
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "Why kafkaParams is passed as the second argument? \r\nAs I see CacheKey itself is constructed from kafkaParams so would not be better to store kafkaParam in a val within CacheKey? \r\n\r\nThen objectFactory.keyToKafkaParams would be deleted along with updateKafkaParamForKey. Is not it?",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-08-29T09:54:42Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer._\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value, and\n+ * the pool will have reasonable default value if the value is not provided.\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(objectFactory: ObjectFactory,\n+                                                  poolConfig: PoolConfig) {\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the capacity, pool will try\n+   * to clear some of idle objects. If it doesn't help getting empty space to create new object,\n+   * it will throw [[NoSuchElementException]] immediately.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {",
    "line": 76
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "It is to reduce unnecessary computation along with comparing map while accessing with pool. You can see CacheKey keeps as it is, and I guess CacheKey was designed for same reason.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-08-29T23:35:39Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer._\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value, and\n+ * the pool will have reasonable default value if the value is not provided.\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(objectFactory: ObjectFactory,\n+                                                  poolConfig: PoolConfig) {\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the capacity, pool will try\n+   * to clear some of idle objects. If it doesn't help getting empty space to create new object,\n+   * it will throw [[NoSuchElementException]] immediately.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {",
    "line": 76
  }, {
    "author": {
      "login": "attilapiros"
    },
    "body": "Oh I see. What about changing CacheKey to not a case class but to a class where kafkaParams is a member but its equals and hashCode methods does not use kafkaParams?\r\nAs this values are goes together I have the feeling encapsulating them is better then keeping their relation in a separate map (keyToKafkaParams). It is just an idea.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-08-29T23:59:32Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer._\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value, and\n+ * the pool will have reasonable default value if the value is not provided.\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(objectFactory: ObjectFactory,\n+                                                  poolConfig: PoolConfig) {\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the capacity, pool will try\n+   * to clear some of idle objects. If it doesn't help getting empty space to create new object,\n+   * it will throw [[NoSuchElementException]] immediately.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {",
    "line": 76
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "That sounds good, but let's wait for voices on committers since CacheKey is designed before, not introduced in this patch.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-08-30T00:08:06Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer._\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value, and\n+ * the pool will have reasonable default value if the value is not provided.\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(objectFactory: ObjectFactory,\n+                                                  poolConfig: PoolConfig) {\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the capacity, pool will try\n+   * to clear some of idle objects. If it doesn't help getting empty space to create new object,\n+   * it will throw [[NoSuchElementException]] immediately.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {",
    "line": 76
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "Is this call (getNumActive) really necessary?",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-08-29T09:56:12Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer._\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value, and\n+ * the pool will have reasonable default value if the value is not provided.\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(objectFactory: ObjectFactory,\n+                                                  poolConfig: PoolConfig) {\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the capacity, pool will try\n+   * to clear some of idle objects. If it doesn't help getting empty space to create new object,\n+   * it will throw [[NoSuchElementException]] immediately.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(intConsumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(intConsumer), intConsumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(intConsumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(intConsumer), intConsumer)\n+  }\n+\n+  /**\n+   * Invalidates current idle and active (borrowed) objects for the key. It ensure no invalidated\n+   * object will be provided again via borrowObject.\n+   *\n+   * It doesn't mean the key will not be available: valid objects will be available via calling\n+   * borrowObject afterwards.\n+   */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    // invalidate all idle consumers for the key\n+    pool.clear(key)\n+\n+    pool.getNumActive()"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "My bad. Will remove.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-08-29T23:36:18Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer._\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value, and\n+ * the pool will have reasonable default value if the value is not provided.\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(objectFactory: ObjectFactory,\n+                                                  poolConfig: PoolConfig) {\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the capacity, pool will try\n+   * to clear some of idle objects. If it doesn't help getting empty space to create new object,\n+   * it will throw [[NoSuchElementException]] immediately.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(intConsumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(intConsumer), intConsumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(intConsumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(intConsumer), intConsumer)\n+  }\n+\n+  /**\n+   * Invalidates current idle and active (borrowed) objects for the key. It ensure no invalidated\n+   * object will be provided again via borrowObject.\n+   *\n+   * It doesn't mean the key will not be available: valid objects will be available via calling\n+   * borrowObject afterwards.\n+   */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    // invalidate all idle consumers for the key\n+    pool.clear(key)\n+\n+    pool.getNumActive()"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "I have not found the referenced CachedInternalKafkaConsumerPool. I guess you mean InternalKafkaConsumerPool.  Isn't it?",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-08-29T10:18:01Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer._\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value, and\n+ * the pool will have reasonable default value if the value is not provided.\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(objectFactory: ObjectFactory,\n+                                                  poolConfig: PoolConfig) {\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the capacity, pool will try\n+   * to clear some of idle objects. If it doesn't help getting empty space to create new object,\n+   * it will throw [[NoSuchElementException]] immediately.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(intConsumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(intConsumer), intConsumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(intConsumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(intConsumer), intConsumer)\n+  }\n+\n+  /**\n+   * Invalidates current idle and active (borrowed) objects for the key. It ensure no invalidated\n+   * object will be provided again via borrowObject.\n+   *\n+   * It doesn't mean the key will not be available: valid objects will be available via calling\n+   * borrowObject afterwards.\n+   */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    // invalidate all idle consumers for the key\n+    pool.clear(key)\n+\n+    pool.getNumActive()\n+    // set invalidate timestamp to let active objects being destroyed when returning to pool\n+    objectFactory.keyToLastInvalidatedTimestamp.put(key, System.currentTimeMillis())\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def getNumIdle: Int = pool.getNumIdle\n+\n+  def getNumIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def getNumActive: Int = pool.getNumActive\n+\n+  def getNumActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def getTotal: Int = getNumIdle + getNumActive\n+\n+  def getTotal(key: CacheKey): Int = getNumIdle(key) + getNumActive(key)\n+\n+  private def updateKafkaParamForKey(key: CacheKey, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+  }\n+\n+  private def extractCacheKey(intConsumer: InternalKafkaConsumer): CacheKey = {\n+    new CacheKey(intConsumer.topicPartition, intConsumer.kafkaParams)\n+  }\n+}\n+\n+private[kafka010] object InternalKafkaConsumerPool {\n+\n+  /**\n+   * Builds the pool for [[InternalKafkaConsumer]]. The pool instance is created per each call.\n+   */\n+  def build: InternalKafkaConsumerPool = {\n+    val objFactory = new ObjectFactory\n+    val poolConfig = new PoolConfig\n+    new InternalKafkaConsumerPool(objFactory, poolConfig)\n+  }\n+\n+  case class PooledObjectInvalidated(key: CacheKey, lastInvalidatedTimestamp: Long,\n+                                     lastBorrowedTime: Long) extends RuntimeException\n+\n+  object CustomSwallowedExceptionListener extends SwallowedExceptionListener with Logging {\n+    override def onSwallowException(e: Exception): Unit = e match {\n+      case e1: PooledObjectInvalidated =>\n+        logDebug(\"Pool for key was invalidated after cached object was borrowed. \" +\n+          s\"Invalidating cached object - key: ${e1.key} / borrowed timestamp: \" +\n+          s\"${e1.lastBorrowedTime} / invalidated timestamp for key: ${e1.lastInvalidatedTimestamp}\")\n+\n+      case _ => logError(s\"Error closing Kafka consumer\", e)\n+    }\n+  }\n+\n+  class PoolConfig extends GenericKeyedObjectPoolConfig[InternalKafkaConsumer] {\n+    init()\n+\n+    def init(): Unit = {\n+      import PoolConfig._\n+\n+      val conf = SparkEnv.get.conf\n+      val capacity = conf.getInt(CONFIG_NAME_CAPACITY, DEFAULT_VALUE_CAPACITY)\n+      val jmxEnabled = conf.getBoolean(CONFIG_NAME_JMX_ENABLED,\n+        defaultValue = DEFAULT_VALUE_JMX_ENABLED)\n+      val minEvictableIdleTimeMillis = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+        DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+      val evictorThreadRunIntervalMillis = conf.getLong(\n+        CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+        DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+      // NOTE: Below lines define the behavior of CachedInternalKafkaConsumerPool, so do not modify"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Yeah I changed the class name and missed to replace it with new name. Will fix.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-08-29T23:37:03Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer._\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value, and\n+ * the pool will have reasonable default value if the value is not provided.\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(objectFactory: ObjectFactory,\n+                                                  poolConfig: PoolConfig) {\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the capacity, pool will try\n+   * to clear some of idle objects. If it doesn't help getting empty space to create new object,\n+   * it will throw [[NoSuchElementException]] immediately.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(intConsumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(intConsumer), intConsumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(intConsumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(intConsumer), intConsumer)\n+  }\n+\n+  /**\n+   * Invalidates current idle and active (borrowed) objects for the key. It ensure no invalidated\n+   * object will be provided again via borrowObject.\n+   *\n+   * It doesn't mean the key will not be available: valid objects will be available via calling\n+   * borrowObject afterwards.\n+   */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    // invalidate all idle consumers for the key\n+    pool.clear(key)\n+\n+    pool.getNumActive()\n+    // set invalidate timestamp to let active objects being destroyed when returning to pool\n+    objectFactory.keyToLastInvalidatedTimestamp.put(key, System.currentTimeMillis())\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def getNumIdle: Int = pool.getNumIdle\n+\n+  def getNumIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def getNumActive: Int = pool.getNumActive\n+\n+  def getNumActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def getTotal: Int = getNumIdle + getNumActive\n+\n+  def getTotal(key: CacheKey): Int = getNumIdle(key) + getNumActive(key)\n+\n+  private def updateKafkaParamForKey(key: CacheKey, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+  }\n+\n+  private def extractCacheKey(intConsumer: InternalKafkaConsumer): CacheKey = {\n+    new CacheKey(intConsumer.topicPartition, intConsumer.kafkaParams)\n+  }\n+}\n+\n+private[kafka010] object InternalKafkaConsumerPool {\n+\n+  /**\n+   * Builds the pool for [[InternalKafkaConsumer]]. The pool instance is created per each call.\n+   */\n+  def build: InternalKafkaConsumerPool = {\n+    val objFactory = new ObjectFactory\n+    val poolConfig = new PoolConfig\n+    new InternalKafkaConsumerPool(objFactory, poolConfig)\n+  }\n+\n+  case class PooledObjectInvalidated(key: CacheKey, lastInvalidatedTimestamp: Long,\n+                                     lastBorrowedTime: Long) extends RuntimeException\n+\n+  object CustomSwallowedExceptionListener extends SwallowedExceptionListener with Logging {\n+    override def onSwallowException(e: Exception): Unit = e match {\n+      case e1: PooledObjectInvalidated =>\n+        logDebug(\"Pool for key was invalidated after cached object was borrowed. \" +\n+          s\"Invalidating cached object - key: ${e1.key} / borrowed timestamp: \" +\n+          s\"${e1.lastBorrowedTime} / invalidated timestamp for key: ${e1.lastInvalidatedTimestamp}\")\n+\n+      case _ => logError(s\"Error closing Kafka consumer\", e)\n+    }\n+  }\n+\n+  class PoolConfig extends GenericKeyedObjectPoolConfig[InternalKafkaConsumer] {\n+    init()\n+\n+    def init(): Unit = {\n+      import PoolConfig._\n+\n+      val conf = SparkEnv.get.conf\n+      val capacity = conf.getInt(CONFIG_NAME_CAPACITY, DEFAULT_VALUE_CAPACITY)\n+      val jmxEnabled = conf.getBoolean(CONFIG_NAME_JMX_ENABLED,\n+        defaultValue = DEFAULT_VALUE_JMX_ENABLED)\n+      val minEvictableIdleTimeMillis = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+        DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+      val evictorThreadRunIntervalMillis = conf.getLong(\n+        CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+        DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+      // NOTE: Below lines define the behavior of CachedInternalKafkaConsumerPool, so do not modify"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "Is there a common place where we should document these configurations?",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-08-29T11:28:40Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer._\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value, and\n+ * the pool will have reasonable default value if the value is not provided.\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(objectFactory: ObjectFactory,\n+                                                  poolConfig: PoolConfig) {\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the capacity, pool will try\n+   * to clear some of idle objects. If it doesn't help getting empty space to create new object,\n+   * it will throw [[NoSuchElementException]] immediately.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(intConsumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(intConsumer), intConsumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(intConsumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(intConsumer), intConsumer)\n+  }\n+\n+  /**\n+   * Invalidates current idle and active (borrowed) objects for the key. It ensure no invalidated\n+   * object will be provided again via borrowObject.\n+   *\n+   * It doesn't mean the key will not be available: valid objects will be available via calling\n+   * borrowObject afterwards.\n+   */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    // invalidate all idle consumers for the key\n+    pool.clear(key)\n+\n+    pool.getNumActive()\n+    // set invalidate timestamp to let active objects being destroyed when returning to pool\n+    objectFactory.keyToLastInvalidatedTimestamp.put(key, System.currentTimeMillis())\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def getNumIdle: Int = pool.getNumIdle\n+\n+  def getNumIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def getNumActive: Int = pool.getNumActive\n+\n+  def getNumActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def getTotal: Int = getNumIdle + getNumActive\n+\n+  def getTotal(key: CacheKey): Int = getNumIdle(key) + getNumActive(key)\n+\n+  private def updateKafkaParamForKey(key: CacheKey, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+  }\n+\n+  private def extractCacheKey(intConsumer: InternalKafkaConsumer): CacheKey = {\n+    new CacheKey(intConsumer.topicPartition, intConsumer.kafkaParams)\n+  }\n+}\n+\n+private[kafka010] object InternalKafkaConsumerPool {\n+\n+  /**\n+   * Builds the pool for [[InternalKafkaConsumer]]. The pool instance is created per each call.\n+   */\n+  def build: InternalKafkaConsumerPool = {\n+    val objFactory = new ObjectFactory\n+    val poolConfig = new PoolConfig\n+    new InternalKafkaConsumerPool(objFactory, poolConfig)\n+  }\n+\n+  case class PooledObjectInvalidated(key: CacheKey, lastInvalidatedTimestamp: Long,\n+                                     lastBorrowedTime: Long) extends RuntimeException\n+\n+  object CustomSwallowedExceptionListener extends SwallowedExceptionListener with Logging {\n+    override def onSwallowException(e: Exception): Unit = e match {\n+      case e1: PooledObjectInvalidated =>\n+        logDebug(\"Pool for key was invalidated after cached object was borrowed. \" +\n+          s\"Invalidating cached object - key: ${e1.key} / borrowed timestamp: \" +\n+          s\"${e1.lastBorrowedTime} / invalidated timestamp for key: ${e1.lastInvalidatedTimestamp}\")\n+\n+      case _ => logError(s\"Error closing Kafka consumer\", e)\n+    }\n+  }\n+\n+  class PoolConfig extends GenericKeyedObjectPoolConfig[InternalKafkaConsumer] {\n+    init()\n+\n+    def init(): Unit = {\n+      import PoolConfig._\n+\n+      val conf = SparkEnv.get.conf\n+      val capacity = conf.getInt(CONFIG_NAME_CAPACITY, DEFAULT_VALUE_CAPACITY)"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Hmm... I think `spark.sql.kafkaConsumerCache.capacity` wasn't documented somewhere, but it may be the thing to fix. The patch will provide more configurations to let end users can tune, so maybe worth to add them to some docs? Not 100% sure where to.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-08-29T23:39:37Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer._\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value, and\n+ * the pool will have reasonable default value if the value is not provided.\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(objectFactory: ObjectFactory,\n+                                                  poolConfig: PoolConfig) {\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the capacity, pool will try\n+   * to clear some of idle objects. If it doesn't help getting empty space to create new object,\n+   * it will throw [[NoSuchElementException]] immediately.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(intConsumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(intConsumer), intConsumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(intConsumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(intConsumer), intConsumer)\n+  }\n+\n+  /**\n+   * Invalidates current idle and active (borrowed) objects for the key. It ensure no invalidated\n+   * object will be provided again via borrowObject.\n+   *\n+   * It doesn't mean the key will not be available: valid objects will be available via calling\n+   * borrowObject afterwards.\n+   */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    // invalidate all idle consumers for the key\n+    pool.clear(key)\n+\n+    pool.getNumActive()\n+    // set invalidate timestamp to let active objects being destroyed when returning to pool\n+    objectFactory.keyToLastInvalidatedTimestamp.put(key, System.currentTimeMillis())\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def getNumIdle: Int = pool.getNumIdle\n+\n+  def getNumIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def getNumActive: Int = pool.getNumActive\n+\n+  def getNumActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def getTotal: Int = getNumIdle + getNumActive\n+\n+  def getTotal(key: CacheKey): Int = getNumIdle(key) + getNumActive(key)\n+\n+  private def updateKafkaParamForKey(key: CacheKey, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+  }\n+\n+  private def extractCacheKey(intConsumer: InternalKafkaConsumer): CacheKey = {\n+    new CacheKey(intConsumer.topicPartition, intConsumer.kafkaParams)\n+  }\n+}\n+\n+private[kafka010] object InternalKafkaConsumerPool {\n+\n+  /**\n+   * Builds the pool for [[InternalKafkaConsumer]]. The pool instance is created per each call.\n+   */\n+  def build: InternalKafkaConsumerPool = {\n+    val objFactory = new ObjectFactory\n+    val poolConfig = new PoolConfig\n+    new InternalKafkaConsumerPool(objFactory, poolConfig)\n+  }\n+\n+  case class PooledObjectInvalidated(key: CacheKey, lastInvalidatedTimestamp: Long,\n+                                     lastBorrowedTime: Long) extends RuntimeException\n+\n+  object CustomSwallowedExceptionListener extends SwallowedExceptionListener with Logging {\n+    override def onSwallowException(e: Exception): Unit = e match {\n+      case e1: PooledObjectInvalidated =>\n+        logDebug(\"Pool for key was invalidated after cached object was borrowed. \" +\n+          s\"Invalidating cached object - key: ${e1.key} / borrowed timestamp: \" +\n+          s\"${e1.lastBorrowedTime} / invalidated timestamp for key: ${e1.lastInvalidatedTimestamp}\")\n+\n+      case _ => logError(s\"Error closing Kafka consumer\", e)\n+    }\n+  }\n+\n+  class PoolConfig extends GenericKeyedObjectPoolConfig[InternalKafkaConsumer] {\n+    init()\n+\n+    def init(): Unit = {\n+      import PoolConfig._\n+\n+      val conf = SparkEnv.get.conf\n+      val capacity = conf.getInt(CONFIG_NAME_CAPACITY, DEFAULT_VALUE_CAPACITY)"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "This has to be bounded somehow with a configuration. I can't imagine it will be efficient if for instance more than 16 tasks read the same topicPartitions.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-03T14:24:49Z",
    "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(\n+    objectFactory: ObjectFactory,\n+    poolConfig: PoolConfig) {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (getTotal == poolConfig.getSoftMaxTotal()) {\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates all idle consumers for the key */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def getNumIdle: Int = pool.getNumIdle\n+\n+  def getNumIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def getNumActive: Int = pool.getNumActive\n+\n+  def getNumActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def getTotal: Int = getNumIdle + getNumActive\n+\n+  def getTotal(key: CacheKey): Int = getNumIdle(key) + getNumActive(key)\n+\n+  private def updateKafkaParamForKey(key: CacheKey, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+  }\n+\n+  private def extractCacheKey(consumer: InternalKafkaConsumer): CacheKey = {\n+    new CacheKey(consumer.topicPartition, consumer.kafkaParams)\n+  }\n+}\n+\n+private[kafka010] object InternalKafkaConsumerPool {\n+\n+  /**\n+   * Builds the pool for [[InternalKafkaConsumer]]. The pool instance is created per each call.\n+   */\n+  def build: InternalKafkaConsumerPool = {\n+    val objFactory = new ObjectFactory\n+    val poolConfig = new PoolConfig\n+    new InternalKafkaConsumerPool(objFactory, poolConfig)\n+  }\n+\n+  case class PooledObjectInvalidated(key: CacheKey, lastInvalidatedTimestamp: Long,\n+                                     lastBorrowedTime: Long) extends RuntimeException\n+\n+  object CustomSwallowedExceptionListener extends SwallowedExceptionListener with Logging {\n+    override def onSwallowException(e: Exception): Unit = {\n+      logError(s\"Error closing Kafka consumer\", e)\n+    }\n+  }\n+\n+  class PoolConfig extends GenericKeyedObjectPoolConfig[InternalKafkaConsumer] {\n+    private var softMaxTotal = Int.MaxValue\n+\n+    def getSoftMaxTotal(): Int = softMaxTotal\n+\n+    init()\n+\n+    def init(): Unit = {\n+      import PoolConfig._\n+\n+      val conf = SparkEnv.get.conf\n+\n+      softMaxTotal = conf.getInt(CONFIG_NAME_CAPACITY, DEFAULT_VALUE_CAPACITY)\n+\n+      val jmxEnabled = conf.getBoolean(CONFIG_NAME_JMX_ENABLED,\n+        defaultValue = DEFAULT_VALUE_JMX_ENABLED)\n+      val minEvictableIdleTimeMillis = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+        DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+      val evictorThreadRunIntervalMillis = conf.getLong(\n+        CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+        DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+      // NOTE: Below lines define the behavior, so do not modify unless you know what you are\n+      // doing, and update the class doc accordingly if necessary when you modify.\n+\n+      // 1. Set min idle objects per key to 0 to avoid creating unnecessary object.\n+      // 2. Set max idle objects per key to 3 but set total objects per key to infinite\n+      // which ensures borrowing per key is not restricted.\n+      // 3. Set max total objects to infinite which ensures all objects are managed in this pool.\n+      setMinIdlePerKey(0)\n+      setMaxIdlePerKey(3)\n+      setMaxTotalPerKey(-1)",
    "line": 181
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "This is intended to be unbounded, because existing behavior doesn't block borrowing consumers. It just used non-cached consumers instead, and it always succeeded so we should ensure this behavior too. \r\nIMHO min idle/max idle is the key to maintain optimal pool, so it may be good for experts to enable configuration on these values for tuning pool.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-04T13:13:36Z",
    "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(\n+    objectFactory: ObjectFactory,\n+    poolConfig: PoolConfig) {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (getTotal == poolConfig.getSoftMaxTotal()) {\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates all idle consumers for the key */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def getNumIdle: Int = pool.getNumIdle\n+\n+  def getNumIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def getNumActive: Int = pool.getNumActive\n+\n+  def getNumActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def getTotal: Int = getNumIdle + getNumActive\n+\n+  def getTotal(key: CacheKey): Int = getNumIdle(key) + getNumActive(key)\n+\n+  private def updateKafkaParamForKey(key: CacheKey, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+  }\n+\n+  private def extractCacheKey(consumer: InternalKafkaConsumer): CacheKey = {\n+    new CacheKey(consumer.topicPartition, consumer.kafkaParams)\n+  }\n+}\n+\n+private[kafka010] object InternalKafkaConsumerPool {\n+\n+  /**\n+   * Builds the pool for [[InternalKafkaConsumer]]. The pool instance is created per each call.\n+   */\n+  def build: InternalKafkaConsumerPool = {\n+    val objFactory = new ObjectFactory\n+    val poolConfig = new PoolConfig\n+    new InternalKafkaConsumerPool(objFactory, poolConfig)\n+  }\n+\n+  case class PooledObjectInvalidated(key: CacheKey, lastInvalidatedTimestamp: Long,\n+                                     lastBorrowedTime: Long) extends RuntimeException\n+\n+  object CustomSwallowedExceptionListener extends SwallowedExceptionListener with Logging {\n+    override def onSwallowException(e: Exception): Unit = {\n+      logError(s\"Error closing Kafka consumer\", e)\n+    }\n+  }\n+\n+  class PoolConfig extends GenericKeyedObjectPoolConfig[InternalKafkaConsumer] {\n+    private var softMaxTotal = Int.MaxValue\n+\n+    def getSoftMaxTotal(): Int = softMaxTotal\n+\n+    init()\n+\n+    def init(): Unit = {\n+      import PoolConfig._\n+\n+      val conf = SparkEnv.get.conf\n+\n+      softMaxTotal = conf.getInt(CONFIG_NAME_CAPACITY, DEFAULT_VALUE_CAPACITY)\n+\n+      val jmxEnabled = conf.getBoolean(CONFIG_NAME_JMX_ENABLED,\n+        defaultValue = DEFAULT_VALUE_JMX_ENABLED)\n+      val minEvictableIdleTimeMillis = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+        DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+      val evictorThreadRunIntervalMillis = conf.getLong(\n+        CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+        DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+      // NOTE: Below lines define the behavior, so do not modify unless you know what you are\n+      // doing, and update the class doc accordingly if necessary when you modify.\n+\n+      // 1. Set min idle objects per key to 0 to avoid creating unnecessary object.\n+      // 2. Set max idle objects per key to 3 but set total objects per key to infinite\n+      // which ensures borrowing per key is not restricted.\n+      // 3. Set max total objects to infinite which ensures all objects are managed in this pool.\n+      setMinIdlePerKey(0)\n+      setMaxIdlePerKey(3)\n+      setMaxTotalPerKey(-1)",
    "line": 181
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "I know it's the same in the original implementation but this is vulnerable to short time resource allocation flood which can't be handled with idle time.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-05T13:34:41Z",
    "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(\n+    objectFactory: ObjectFactory,\n+    poolConfig: PoolConfig) {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (getTotal == poolConfig.getSoftMaxTotal()) {\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates all idle consumers for the key */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def getNumIdle: Int = pool.getNumIdle\n+\n+  def getNumIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def getNumActive: Int = pool.getNumActive\n+\n+  def getNumActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def getTotal: Int = getNumIdle + getNumActive\n+\n+  def getTotal(key: CacheKey): Int = getNumIdle(key) + getNumActive(key)\n+\n+  private def updateKafkaParamForKey(key: CacheKey, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+  }\n+\n+  private def extractCacheKey(consumer: InternalKafkaConsumer): CacheKey = {\n+    new CacheKey(consumer.topicPartition, consumer.kafkaParams)\n+  }\n+}\n+\n+private[kafka010] object InternalKafkaConsumerPool {\n+\n+  /**\n+   * Builds the pool for [[InternalKafkaConsumer]]. The pool instance is created per each call.\n+   */\n+  def build: InternalKafkaConsumerPool = {\n+    val objFactory = new ObjectFactory\n+    val poolConfig = new PoolConfig\n+    new InternalKafkaConsumerPool(objFactory, poolConfig)\n+  }\n+\n+  case class PooledObjectInvalidated(key: CacheKey, lastInvalidatedTimestamp: Long,\n+                                     lastBorrowedTime: Long) extends RuntimeException\n+\n+  object CustomSwallowedExceptionListener extends SwallowedExceptionListener with Logging {\n+    override def onSwallowException(e: Exception): Unit = {\n+      logError(s\"Error closing Kafka consumer\", e)\n+    }\n+  }\n+\n+  class PoolConfig extends GenericKeyedObjectPoolConfig[InternalKafkaConsumer] {\n+    private var softMaxTotal = Int.MaxValue\n+\n+    def getSoftMaxTotal(): Int = softMaxTotal\n+\n+    init()\n+\n+    def init(): Unit = {\n+      import PoolConfig._\n+\n+      val conf = SparkEnv.get.conf\n+\n+      softMaxTotal = conf.getInt(CONFIG_NAME_CAPACITY, DEFAULT_VALUE_CAPACITY)\n+\n+      val jmxEnabled = conf.getBoolean(CONFIG_NAME_JMX_ENABLED,\n+        defaultValue = DEFAULT_VALUE_JMX_ENABLED)\n+      val minEvictableIdleTimeMillis = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+        DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+      val evictorThreadRunIntervalMillis = conf.getLong(\n+        CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+        DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+      // NOTE: Below lines define the behavior, so do not modify unless you know what you are\n+      // doing, and update the class doc accordingly if necessary when you modify.\n+\n+      // 1. Set min idle objects per key to 0 to avoid creating unnecessary object.\n+      // 2. Set max idle objects per key to 3 but set total objects per key to infinite\n+      // which ensures borrowing per key is not restricted.\n+      // 3. Set max total objects to infinite which ensures all objects are managed in this pool.\n+      setMinIdlePerKey(0)\n+      setMaxIdlePerKey(3)\n+      setMaxTotalPerKey(-1)",
    "line": 181
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Hmm... yeah it would make sense, though it still needs to provide consumers whenever tasks request. \r\n\r\nTwo approaches come in my mind: 1. bounded & provide non-cached consumer with some delays when borrow fails 2. unbounded & apply random delay when borrowing. Former requires me to reintroduce handling of cached vs non-cached so would discard simplicity on my patch, whereas latter just requires me to add random delay (jitter) but the chance to avoid resource allocation flood would be completely based on probability. (max ~50 ms might be enough?).\r\n\r\nI'd like to lean on 2 but also open to 1 if we don't feel safe with 2.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-05T15:04:59Z",
    "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(\n+    objectFactory: ObjectFactory,\n+    poolConfig: PoolConfig) {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (getTotal == poolConfig.getSoftMaxTotal()) {\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates all idle consumers for the key */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def getNumIdle: Int = pool.getNumIdle\n+\n+  def getNumIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def getNumActive: Int = pool.getNumActive\n+\n+  def getNumActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def getTotal: Int = getNumIdle + getNumActive\n+\n+  def getTotal(key: CacheKey): Int = getNumIdle(key) + getNumActive(key)\n+\n+  private def updateKafkaParamForKey(key: CacheKey, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+  }\n+\n+  private def extractCacheKey(consumer: InternalKafkaConsumer): CacheKey = {\n+    new CacheKey(consumer.topicPartition, consumer.kafkaParams)\n+  }\n+}\n+\n+private[kafka010] object InternalKafkaConsumerPool {\n+\n+  /**\n+   * Builds the pool for [[InternalKafkaConsumer]]. The pool instance is created per each call.\n+   */\n+  def build: InternalKafkaConsumerPool = {\n+    val objFactory = new ObjectFactory\n+    val poolConfig = new PoolConfig\n+    new InternalKafkaConsumerPool(objFactory, poolConfig)\n+  }\n+\n+  case class PooledObjectInvalidated(key: CacheKey, lastInvalidatedTimestamp: Long,\n+                                     lastBorrowedTime: Long) extends RuntimeException\n+\n+  object CustomSwallowedExceptionListener extends SwallowedExceptionListener with Logging {\n+    override def onSwallowException(e: Exception): Unit = {\n+      logError(s\"Error closing Kafka consumer\", e)\n+    }\n+  }\n+\n+  class PoolConfig extends GenericKeyedObjectPoolConfig[InternalKafkaConsumer] {\n+    private var softMaxTotal = Int.MaxValue\n+\n+    def getSoftMaxTotal(): Int = softMaxTotal\n+\n+    init()\n+\n+    def init(): Unit = {\n+      import PoolConfig._\n+\n+      val conf = SparkEnv.get.conf\n+\n+      softMaxTotal = conf.getInt(CONFIG_NAME_CAPACITY, DEFAULT_VALUE_CAPACITY)\n+\n+      val jmxEnabled = conf.getBoolean(CONFIG_NAME_JMX_ENABLED,\n+        defaultValue = DEFAULT_VALUE_JMX_ENABLED)\n+      val minEvictableIdleTimeMillis = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+        DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+      val evictorThreadRunIntervalMillis = conf.getLong(\n+        CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+        DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+      // NOTE: Below lines define the behavior, so do not modify unless you know what you are\n+      // doing, and update the class doc accordingly if necessary when you modify.\n+\n+      // 1. Set min idle objects per key to 0 to avoid creating unnecessary object.\n+      // 2. Set max idle objects per key to 3 but set total objects per key to infinite\n+      // which ensures borrowing per key is not restricted.\n+      // 3. Set max total objects to infinite which ensures all objects are managed in this pool.\n+      setMinIdlePerKey(0)\n+      setMaxIdlePerKey(3)\n+      setMaxTotalPerKey(-1)",
    "line": 181
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Yeah, the first approach is heavy and doesn't solve the problem itself. The second case I'm not sure how this can help (maybe not understood properly). Additionally it could introduce fluctuation in the performance. Maybe we can leave the soft boundary (as it's like that in the original implementation and I don't see jiras with it) and just put warning on each new allocation after a threshold. This way admins/devs can see this in the log and don't have to monitor the cache constantly.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-06T11:32:00Z",
    "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(\n+    objectFactory: ObjectFactory,\n+    poolConfig: PoolConfig) {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (getTotal == poolConfig.getSoftMaxTotal()) {\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates all idle consumers for the key */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def getNumIdle: Int = pool.getNumIdle\n+\n+  def getNumIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def getNumActive: Int = pool.getNumActive\n+\n+  def getNumActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def getTotal: Int = getNumIdle + getNumActive\n+\n+  def getTotal(key: CacheKey): Int = getNumIdle(key) + getNumActive(key)\n+\n+  private def updateKafkaParamForKey(key: CacheKey, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+  }\n+\n+  private def extractCacheKey(consumer: InternalKafkaConsumer): CacheKey = {\n+    new CacheKey(consumer.topicPartition, consumer.kafkaParams)\n+  }\n+}\n+\n+private[kafka010] object InternalKafkaConsumerPool {\n+\n+  /**\n+   * Builds the pool for [[InternalKafkaConsumer]]. The pool instance is created per each call.\n+   */\n+  def build: InternalKafkaConsumerPool = {\n+    val objFactory = new ObjectFactory\n+    val poolConfig = new PoolConfig\n+    new InternalKafkaConsumerPool(objFactory, poolConfig)\n+  }\n+\n+  case class PooledObjectInvalidated(key: CacheKey, lastInvalidatedTimestamp: Long,\n+                                     lastBorrowedTime: Long) extends RuntimeException\n+\n+  object CustomSwallowedExceptionListener extends SwallowedExceptionListener with Logging {\n+    override def onSwallowException(e: Exception): Unit = {\n+      logError(s\"Error closing Kafka consumer\", e)\n+    }\n+  }\n+\n+  class PoolConfig extends GenericKeyedObjectPoolConfig[InternalKafkaConsumer] {\n+    private var softMaxTotal = Int.MaxValue\n+\n+    def getSoftMaxTotal(): Int = softMaxTotal\n+\n+    init()\n+\n+    def init(): Unit = {\n+      import PoolConfig._\n+\n+      val conf = SparkEnv.get.conf\n+\n+      softMaxTotal = conf.getInt(CONFIG_NAME_CAPACITY, DEFAULT_VALUE_CAPACITY)\n+\n+      val jmxEnabled = conf.getBoolean(CONFIG_NAME_JMX_ENABLED,\n+        defaultValue = DEFAULT_VALUE_JMX_ENABLED)\n+      val minEvictableIdleTimeMillis = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+        DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+      val evictorThreadRunIntervalMillis = conf.getLong(\n+        CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+        DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+      // NOTE: Below lines define the behavior, so do not modify unless you know what you are\n+      // doing, and update the class doc accordingly if necessary when you modify.\n+\n+      // 1. Set min idle objects per key to 0 to avoid creating unnecessary object.\n+      // 2. Set max idle objects per key to 3 but set total objects per key to infinite\n+      // which ensures borrowing per key is not restricted.\n+      // 3. Set max total objects to infinite which ensures all objects are managed in this pool.\n+      setMinIdlePerKey(0)\n+      setMaxIdlePerKey(3)\n+      setMaxTotalPerKey(-1)",
    "line": 181
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Actually soft boundary with existing cache is more like total keys in new cache (and new cache implements it), while you're concerning about it is regarding same key having too many tasks hence too many consumers.\r\n\r\nIn new cache we have more metrics: the number of idle/active consumers for given key, the number of total idle/active consumers. We can check the metrics and log when it exceeds the threshold or meet condition. Even simpler, we can just leave log message whenever cache **creates** consumer instead of returning idle consumer, if it would help end users to tune pool as well as tasks.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-06T13:56:44Z",
    "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(\n+    objectFactory: ObjectFactory,\n+    poolConfig: PoolConfig) {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (getTotal == poolConfig.getSoftMaxTotal()) {\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates all idle consumers for the key */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def getNumIdle: Int = pool.getNumIdle\n+\n+  def getNumIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def getNumActive: Int = pool.getNumActive\n+\n+  def getNumActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def getTotal: Int = getNumIdle + getNumActive\n+\n+  def getTotal(key: CacheKey): Int = getNumIdle(key) + getNumActive(key)\n+\n+  private def updateKafkaParamForKey(key: CacheKey, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+  }\n+\n+  private def extractCacheKey(consumer: InternalKafkaConsumer): CacheKey = {\n+    new CacheKey(consumer.topicPartition, consumer.kafkaParams)\n+  }\n+}\n+\n+private[kafka010] object InternalKafkaConsumerPool {\n+\n+  /**\n+   * Builds the pool for [[InternalKafkaConsumer]]. The pool instance is created per each call.\n+   */\n+  def build: InternalKafkaConsumerPool = {\n+    val objFactory = new ObjectFactory\n+    val poolConfig = new PoolConfig\n+    new InternalKafkaConsumerPool(objFactory, poolConfig)\n+  }\n+\n+  case class PooledObjectInvalidated(key: CacheKey, lastInvalidatedTimestamp: Long,\n+                                     lastBorrowedTime: Long) extends RuntimeException\n+\n+  object CustomSwallowedExceptionListener extends SwallowedExceptionListener with Logging {\n+    override def onSwallowException(e: Exception): Unit = {\n+      logError(s\"Error closing Kafka consumer\", e)\n+    }\n+  }\n+\n+  class PoolConfig extends GenericKeyedObjectPoolConfig[InternalKafkaConsumer] {\n+    private var softMaxTotal = Int.MaxValue\n+\n+    def getSoftMaxTotal(): Int = softMaxTotal\n+\n+    init()\n+\n+    def init(): Unit = {\n+      import PoolConfig._\n+\n+      val conf = SparkEnv.get.conf\n+\n+      softMaxTotal = conf.getInt(CONFIG_NAME_CAPACITY, DEFAULT_VALUE_CAPACITY)\n+\n+      val jmxEnabled = conf.getBoolean(CONFIG_NAME_JMX_ENABLED,\n+        defaultValue = DEFAULT_VALUE_JMX_ENABLED)\n+      val minEvictableIdleTimeMillis = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+        DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+      val evictorThreadRunIntervalMillis = conf.getLong(\n+        CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+        DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+      // NOTE: Below lines define the behavior, so do not modify unless you know what you are\n+      // doing, and update the class doc accordingly if necessary when you modify.\n+\n+      // 1. Set min idle objects per key to 0 to avoid creating unnecessary object.\n+      // 2. Set max idle objects per key to 3 but set total objects per key to infinite\n+      // which ensures borrowing per key is not restricted.\n+      // 3. Set max total objects to infinite which ensures all objects are managed in this pool.\n+      setMinIdlePerKey(0)\n+      setMaxIdlePerKey(3)\n+      setMaxTotalPerKey(-1)",
    "line": 181
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "In my view the mentioned simple debug log message is fine. This can be seen even without constant monitoring.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-19T09:08:06Z",
    "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(\n+    objectFactory: ObjectFactory,\n+    poolConfig: PoolConfig) {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (getTotal == poolConfig.getSoftMaxTotal()) {\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates all idle consumers for the key */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def getNumIdle: Int = pool.getNumIdle\n+\n+  def getNumIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def getNumActive: Int = pool.getNumActive\n+\n+  def getNumActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def getTotal: Int = getNumIdle + getNumActive\n+\n+  def getTotal(key: CacheKey): Int = getNumIdle(key) + getNumActive(key)\n+\n+  private def updateKafkaParamForKey(key: CacheKey, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+  }\n+\n+  private def extractCacheKey(consumer: InternalKafkaConsumer): CacheKey = {\n+    new CacheKey(consumer.topicPartition, consumer.kafkaParams)\n+  }\n+}\n+\n+private[kafka010] object InternalKafkaConsumerPool {\n+\n+  /**\n+   * Builds the pool for [[InternalKafkaConsumer]]. The pool instance is created per each call.\n+   */\n+  def build: InternalKafkaConsumerPool = {\n+    val objFactory = new ObjectFactory\n+    val poolConfig = new PoolConfig\n+    new InternalKafkaConsumerPool(objFactory, poolConfig)\n+  }\n+\n+  case class PooledObjectInvalidated(key: CacheKey, lastInvalidatedTimestamp: Long,\n+                                     lastBorrowedTime: Long) extends RuntimeException\n+\n+  object CustomSwallowedExceptionListener extends SwallowedExceptionListener with Logging {\n+    override def onSwallowException(e: Exception): Unit = {\n+      logError(s\"Error closing Kafka consumer\", e)\n+    }\n+  }\n+\n+  class PoolConfig extends GenericKeyedObjectPoolConfig[InternalKafkaConsumer] {\n+    private var softMaxTotal = Int.MaxValue\n+\n+    def getSoftMaxTotal(): Int = softMaxTotal\n+\n+    init()\n+\n+    def init(): Unit = {\n+      import PoolConfig._\n+\n+      val conf = SparkEnv.get.conf\n+\n+      softMaxTotal = conf.getInt(CONFIG_NAME_CAPACITY, DEFAULT_VALUE_CAPACITY)\n+\n+      val jmxEnabled = conf.getBoolean(CONFIG_NAME_JMX_ENABLED,\n+        defaultValue = DEFAULT_VALUE_JMX_ENABLED)\n+      val minEvictableIdleTimeMillis = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+        DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+      val evictorThreadRunIntervalMillis = conf.getLong(\n+        CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+        DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+      // NOTE: Below lines define the behavior, so do not modify unless you know what you are\n+      // doing, and update the class doc accordingly if necessary when you modify.\n+\n+      // 1. Set min idle objects per key to 0 to avoid creating unnecessary object.\n+      // 2. Set max idle objects per key to 3 but set total objects per key to infinite\n+      // which ensures borrowing per key is not restricted.\n+      // 3. Set max total objects to infinite which ensures all objects are managed in this pool.\n+      setMinIdlePerKey(0)\n+      setMaxIdlePerKey(3)\n+      setMaxTotalPerKey(-1)",
    "line": 181
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Let's see how committers think about it. We can wait and react.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-19T20:39:23Z",
    "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(\n+    objectFactory: ObjectFactory,\n+    poolConfig: PoolConfig) {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (getTotal == poolConfig.getSoftMaxTotal()) {\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates all idle consumers for the key */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def getNumIdle: Int = pool.getNumIdle\n+\n+  def getNumIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def getNumActive: Int = pool.getNumActive\n+\n+  def getNumActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def getTotal: Int = getNumIdle + getNumActive\n+\n+  def getTotal(key: CacheKey): Int = getNumIdle(key) + getNumActive(key)\n+\n+  private def updateKafkaParamForKey(key: CacheKey, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+  }\n+\n+  private def extractCacheKey(consumer: InternalKafkaConsumer): CacheKey = {\n+    new CacheKey(consumer.topicPartition, consumer.kafkaParams)\n+  }\n+}\n+\n+private[kafka010] object InternalKafkaConsumerPool {\n+\n+  /**\n+   * Builds the pool for [[InternalKafkaConsumer]]. The pool instance is created per each call.\n+   */\n+  def build: InternalKafkaConsumerPool = {\n+    val objFactory = new ObjectFactory\n+    val poolConfig = new PoolConfig\n+    new InternalKafkaConsumerPool(objFactory, poolConfig)\n+  }\n+\n+  case class PooledObjectInvalidated(key: CacheKey, lastInvalidatedTimestamp: Long,\n+                                     lastBorrowedTime: Long) extends RuntimeException\n+\n+  object CustomSwallowedExceptionListener extends SwallowedExceptionListener with Logging {\n+    override def onSwallowException(e: Exception): Unit = {\n+      logError(s\"Error closing Kafka consumer\", e)\n+    }\n+  }\n+\n+  class PoolConfig extends GenericKeyedObjectPoolConfig[InternalKafkaConsumer] {\n+    private var softMaxTotal = Int.MaxValue\n+\n+    def getSoftMaxTotal(): Int = softMaxTotal\n+\n+    init()\n+\n+    def init(): Unit = {\n+      import PoolConfig._\n+\n+      val conf = SparkEnv.get.conf\n+\n+      softMaxTotal = conf.getInt(CONFIG_NAME_CAPACITY, DEFAULT_VALUE_CAPACITY)\n+\n+      val jmxEnabled = conf.getBoolean(CONFIG_NAME_JMX_ENABLED,\n+        defaultValue = DEFAULT_VALUE_JMX_ENABLED)\n+      val minEvictableIdleTimeMillis = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+        DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+      val evictorThreadRunIntervalMillis = conf.getLong(\n+        CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+        DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+      // NOTE: Below lines define the behavior, so do not modify unless you know what you are\n+      // doing, and update the class doc accordingly if necessary when you modify.\n+\n+      // 1. Set min idle objects per key to 0 to avoid creating unnecessary object.\n+      // 2. Set max idle objects per key to 3 but set total objects per key to infinite\n+      // which ensures borrowing per key is not restricted.\n+      // 3. Set max total objects to infinite which ensures all objects are managed in this pool.\n+      setMinIdlePerKey(0)\n+      setMaxIdlePerKey(3)\n+      setMaxTotalPerKey(-1)",
    "line": 181
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "OK.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-21T08:23:19Z",
    "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(\n+    objectFactory: ObjectFactory,\n+    poolConfig: PoolConfig) {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (getTotal == poolConfig.getSoftMaxTotal()) {\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates all idle consumers for the key */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def getNumIdle: Int = pool.getNumIdle\n+\n+  def getNumIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def getNumActive: Int = pool.getNumActive\n+\n+  def getNumActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def getTotal: Int = getNumIdle + getNumActive\n+\n+  def getTotal(key: CacheKey): Int = getNumIdle(key) + getNumActive(key)\n+\n+  private def updateKafkaParamForKey(key: CacheKey, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+  }\n+\n+  private def extractCacheKey(consumer: InternalKafkaConsumer): CacheKey = {\n+    new CacheKey(consumer.topicPartition, consumer.kafkaParams)\n+  }\n+}\n+\n+private[kafka010] object InternalKafkaConsumerPool {\n+\n+  /**\n+   * Builds the pool for [[InternalKafkaConsumer]]. The pool instance is created per each call.\n+   */\n+  def build: InternalKafkaConsumerPool = {\n+    val objFactory = new ObjectFactory\n+    val poolConfig = new PoolConfig\n+    new InternalKafkaConsumerPool(objFactory, poolConfig)\n+  }\n+\n+  case class PooledObjectInvalidated(key: CacheKey, lastInvalidatedTimestamp: Long,\n+                                     lastBorrowedTime: Long) extends RuntimeException\n+\n+  object CustomSwallowedExceptionListener extends SwallowedExceptionListener with Logging {\n+    override def onSwallowException(e: Exception): Unit = {\n+      logError(s\"Error closing Kafka consumer\", e)\n+    }\n+  }\n+\n+  class PoolConfig extends GenericKeyedObjectPoolConfig[InternalKafkaConsumer] {\n+    private var softMaxTotal = Int.MaxValue\n+\n+    def getSoftMaxTotal(): Int = softMaxTotal\n+\n+    init()\n+\n+    def init(): Unit = {\n+      import PoolConfig._\n+\n+      val conf = SparkEnv.get.conf\n+\n+      softMaxTotal = conf.getInt(CONFIG_NAME_CAPACITY, DEFAULT_VALUE_CAPACITY)\n+\n+      val jmxEnabled = conf.getBoolean(CONFIG_NAME_JMX_ENABLED,\n+        defaultValue = DEFAULT_VALUE_JMX_ENABLED)\n+      val minEvictableIdleTimeMillis = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+        DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+      val evictorThreadRunIntervalMillis = conf.getLong(\n+        CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+        DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+      // NOTE: Below lines define the behavior, so do not modify unless you know what you are\n+      // doing, and update the class doc accordingly if necessary when you modify.\n+\n+      // 1. Set min idle objects per key to 0 to avoid creating unnecessary object.\n+      // 2. Set max idle objects per key to 3 but set total objects per key to infinite\n+      // which ensures borrowing per key is not restricted.\n+      // 3. Set max total objects to infinite which ensures all objects are managed in this pool.\n+      setMinIdlePerKey(0)\n+      setMaxIdlePerKey(3)\n+      setMaxTotalPerKey(-1)",
    "line": 181
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Why not using option?",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-04T06:40:13Z",
    "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(\n+    objectFactory: ObjectFactory,\n+    poolConfig: PoolConfig) {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (getTotal == poolConfig.getSoftMaxTotal()) {\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates all idle consumers for the key */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def getNumIdle: Int = pool.getNumIdle\n+\n+  def getNumIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def getNumActive: Int = pool.getNumActive\n+\n+  def getNumActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def getTotal: Int = getNumIdle + getNumActive\n+\n+  def getTotal(key: CacheKey): Int = getNumIdle(key) + getNumActive(key)\n+\n+  private def updateKafkaParamForKey(key: CacheKey, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+  }\n+\n+  private def extractCacheKey(consumer: InternalKafkaConsumer): CacheKey = {\n+    new CacheKey(consumer.topicPartition, consumer.kafkaParams)\n+  }\n+}\n+\n+private[kafka010] object InternalKafkaConsumerPool {\n+\n+  /**\n+   * Builds the pool for [[InternalKafkaConsumer]]. The pool instance is created per each call.\n+   */\n+  def build: InternalKafkaConsumerPool = {\n+    val objFactory = new ObjectFactory\n+    val poolConfig = new PoolConfig\n+    new InternalKafkaConsumerPool(objFactory, poolConfig)\n+  }\n+\n+  case class PooledObjectInvalidated(key: CacheKey, lastInvalidatedTimestamp: Long,\n+                                     lastBorrowedTime: Long) extends RuntimeException\n+\n+  object CustomSwallowedExceptionListener extends SwallowedExceptionListener with Logging {\n+    override def onSwallowException(e: Exception): Unit = {\n+      logError(s\"Error closing Kafka consumer\", e)\n+    }\n+  }\n+\n+  class PoolConfig extends GenericKeyedObjectPoolConfig[InternalKafkaConsumer] {\n+    private var softMaxTotal = Int.MaxValue\n+\n+    def getSoftMaxTotal(): Int = softMaxTotal\n+\n+    init()\n+\n+    def init(): Unit = {\n+      import PoolConfig._\n+\n+      val conf = SparkEnv.get.conf\n+\n+      softMaxTotal = conf.getInt(CONFIG_NAME_CAPACITY, DEFAULT_VALUE_CAPACITY)\n+\n+      val jmxEnabled = conf.getBoolean(CONFIG_NAME_JMX_ENABLED,\n+        defaultValue = DEFAULT_VALUE_JMX_ENABLED)\n+      val minEvictableIdleTimeMillis = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+        DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+      val evictorThreadRunIntervalMillis = conf.getLong(\n+        CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+        DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+      // NOTE: Below lines define the behavior, so do not modify unless you know what you are\n+      // doing, and update the class doc accordingly if necessary when you modify.\n+\n+      // 1. Set min idle objects per key to 0 to avoid creating unnecessary object.\n+      // 2. Set max idle objects per key to 3 but set total objects per key to infinite\n+      // which ensures borrowing per key is not restricted.\n+      // 3. Set max total objects to infinite which ensures all objects are managed in this pool.\n+      setMinIdlePerKey(0)\n+      setMaxIdlePerKey(3)\n+      setMaxTotalPerKey(-1)\n+      setMaxTotal(-1)\n+\n+      // Set minimum evictable idle time which will be referred from evictor thread\n+      setMinEvictableIdleTimeMillis(minEvictableIdleTimeMillis)\n+      setSoftMinEvictableIdleTimeMillis(-1)\n+\n+      // evictor thread will run test with ten idle objects\n+      setTimeBetweenEvictionRunsMillis(evictorThreadRunIntervalMillis)\n+      setNumTestsPerEvictionRun(10)\n+      setEvictionPolicy(new DefaultEvictionPolicy[InternalKafkaConsumer]())\n+\n+      // Immediately fail on exhausted pool while borrowing\n+      setBlockWhenExhausted(false)\n+\n+      setJmxEnabled(jmxEnabled)\n+      setJmxNamePrefix(\"kafka010-cached-simple-kafka-consumer-pool\")\n+    }\n+  }\n+\n+  object PoolConfig {\n+    val CONFIG_NAME_PREFIX = \"spark.sql.kafkaConsumerCache.\"\n+    val CONFIG_NAME_CAPACITY = CONFIG_NAME_PREFIX + \"capacity\"\n+    val CONFIG_NAME_JMX_ENABLED = CONFIG_NAME_PREFIX + \"jmx.enable\"\n+    val CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS = CONFIG_NAME_PREFIX +\n+      \"minEvictableIdleTimeMillis\"\n+    val CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS = CONFIG_NAME_PREFIX +\n+      \"evictorThreadRunIntervalMillis\"\n+\n+    val DEFAULT_VALUE_CAPACITY = 64\n+    val DEFAULT_VALUE_JMX_ENABLED = false\n+    val DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS = 5 * 60 * 1000 // 5 minutes\n+    val DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS = 3 * 60 * 1000 // 3 minutes\n+  }\n+\n+  class ObjectFactory extends BaseKeyedPooledObjectFactory[CacheKey, InternalKafkaConsumer]\n+    with Logging {\n+\n+    val keyToKafkaParams: ConcurrentHashMap[CacheKey, ju.Map[String, Object]] =\n+      new ConcurrentHashMap[CacheKey, ju.Map[String, Object]]()\n+\n+    override def create(key: CacheKey): InternalKafkaConsumer = {\n+      val kafkaParams = keyToKafkaParams.get(key)\n+      if (kafkaParams == null) {"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "No strong opinion on this. Looks like Spark allows using `null` and doesn't enforce guarding with `Option` so I just leveraged what `ConcurrentHashMap.get` provides. Did you intend `.getOrElse(throw ...)`?",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-04T13:28:50Z",
    "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(\n+    objectFactory: ObjectFactory,\n+    poolConfig: PoolConfig) {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (getTotal == poolConfig.getSoftMaxTotal()) {\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates all idle consumers for the key */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def getNumIdle: Int = pool.getNumIdle\n+\n+  def getNumIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def getNumActive: Int = pool.getNumActive\n+\n+  def getNumActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def getTotal: Int = getNumIdle + getNumActive\n+\n+  def getTotal(key: CacheKey): Int = getNumIdle(key) + getNumActive(key)\n+\n+  private def updateKafkaParamForKey(key: CacheKey, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+  }\n+\n+  private def extractCacheKey(consumer: InternalKafkaConsumer): CacheKey = {\n+    new CacheKey(consumer.topicPartition, consumer.kafkaParams)\n+  }\n+}\n+\n+private[kafka010] object InternalKafkaConsumerPool {\n+\n+  /**\n+   * Builds the pool for [[InternalKafkaConsumer]]. The pool instance is created per each call.\n+   */\n+  def build: InternalKafkaConsumerPool = {\n+    val objFactory = new ObjectFactory\n+    val poolConfig = new PoolConfig\n+    new InternalKafkaConsumerPool(objFactory, poolConfig)\n+  }\n+\n+  case class PooledObjectInvalidated(key: CacheKey, lastInvalidatedTimestamp: Long,\n+                                     lastBorrowedTime: Long) extends RuntimeException\n+\n+  object CustomSwallowedExceptionListener extends SwallowedExceptionListener with Logging {\n+    override def onSwallowException(e: Exception): Unit = {\n+      logError(s\"Error closing Kafka consumer\", e)\n+    }\n+  }\n+\n+  class PoolConfig extends GenericKeyedObjectPoolConfig[InternalKafkaConsumer] {\n+    private var softMaxTotal = Int.MaxValue\n+\n+    def getSoftMaxTotal(): Int = softMaxTotal\n+\n+    init()\n+\n+    def init(): Unit = {\n+      import PoolConfig._\n+\n+      val conf = SparkEnv.get.conf\n+\n+      softMaxTotal = conf.getInt(CONFIG_NAME_CAPACITY, DEFAULT_VALUE_CAPACITY)\n+\n+      val jmxEnabled = conf.getBoolean(CONFIG_NAME_JMX_ENABLED,\n+        defaultValue = DEFAULT_VALUE_JMX_ENABLED)\n+      val minEvictableIdleTimeMillis = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+        DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+      val evictorThreadRunIntervalMillis = conf.getLong(\n+        CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+        DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+      // NOTE: Below lines define the behavior, so do not modify unless you know what you are\n+      // doing, and update the class doc accordingly if necessary when you modify.\n+\n+      // 1. Set min idle objects per key to 0 to avoid creating unnecessary object.\n+      // 2. Set max idle objects per key to 3 but set total objects per key to infinite\n+      // which ensures borrowing per key is not restricted.\n+      // 3. Set max total objects to infinite which ensures all objects are managed in this pool.\n+      setMinIdlePerKey(0)\n+      setMaxIdlePerKey(3)\n+      setMaxTotalPerKey(-1)\n+      setMaxTotal(-1)\n+\n+      // Set minimum evictable idle time which will be referred from evictor thread\n+      setMinEvictableIdleTimeMillis(minEvictableIdleTimeMillis)\n+      setSoftMinEvictableIdleTimeMillis(-1)\n+\n+      // evictor thread will run test with ten idle objects\n+      setTimeBetweenEvictionRunsMillis(evictorThreadRunIntervalMillis)\n+      setNumTestsPerEvictionRun(10)\n+      setEvictionPolicy(new DefaultEvictionPolicy[InternalKafkaConsumer]())\n+\n+      // Immediately fail on exhausted pool while borrowing\n+      setBlockWhenExhausted(false)\n+\n+      setJmxEnabled(jmxEnabled)\n+      setJmxNamePrefix(\"kafka010-cached-simple-kafka-consumer-pool\")\n+    }\n+  }\n+\n+  object PoolConfig {\n+    val CONFIG_NAME_PREFIX = \"spark.sql.kafkaConsumerCache.\"\n+    val CONFIG_NAME_CAPACITY = CONFIG_NAME_PREFIX + \"capacity\"\n+    val CONFIG_NAME_JMX_ENABLED = CONFIG_NAME_PREFIX + \"jmx.enable\"\n+    val CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS = CONFIG_NAME_PREFIX +\n+      \"minEvictableIdleTimeMillis\"\n+    val CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS = CONFIG_NAME_PREFIX +\n+      \"evictorThreadRunIntervalMillis\"\n+\n+    val DEFAULT_VALUE_CAPACITY = 64\n+    val DEFAULT_VALUE_JMX_ENABLED = false\n+    val DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS = 5 * 60 * 1000 // 5 minutes\n+    val DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS = 3 * 60 * 1000 // 3 minutes\n+  }\n+\n+  class ObjectFactory extends BaseKeyedPooledObjectFactory[CacheKey, InternalKafkaConsumer]\n+    with Logging {\n+\n+    val keyToKafkaParams: ConcurrentHashMap[CacheKey, ju.Map[String, Object]] =\n+      new ConcurrentHashMap[CacheKey, ju.Map[String, Object]]()\n+\n+    override def create(key: CacheKey): InternalKafkaConsumer = {\n+      val kafkaParams = keyToKafkaParams.get(key)\n+      if (kafkaParams == null) {"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "It's more a beautify thing then an issue. Using `Option` is more scala idiomatic then checking null.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-05T13:42:46Z",
    "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(\n+    objectFactory: ObjectFactory,\n+    poolConfig: PoolConfig) {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (getTotal == poolConfig.getSoftMaxTotal()) {\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates all idle consumers for the key */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def getNumIdle: Int = pool.getNumIdle\n+\n+  def getNumIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def getNumActive: Int = pool.getNumActive\n+\n+  def getNumActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def getTotal: Int = getNumIdle + getNumActive\n+\n+  def getTotal(key: CacheKey): Int = getNumIdle(key) + getNumActive(key)\n+\n+  private def updateKafkaParamForKey(key: CacheKey, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+  }\n+\n+  private def extractCacheKey(consumer: InternalKafkaConsumer): CacheKey = {\n+    new CacheKey(consumer.topicPartition, consumer.kafkaParams)\n+  }\n+}\n+\n+private[kafka010] object InternalKafkaConsumerPool {\n+\n+  /**\n+   * Builds the pool for [[InternalKafkaConsumer]]. The pool instance is created per each call.\n+   */\n+  def build: InternalKafkaConsumerPool = {\n+    val objFactory = new ObjectFactory\n+    val poolConfig = new PoolConfig\n+    new InternalKafkaConsumerPool(objFactory, poolConfig)\n+  }\n+\n+  case class PooledObjectInvalidated(key: CacheKey, lastInvalidatedTimestamp: Long,\n+                                     lastBorrowedTime: Long) extends RuntimeException\n+\n+  object CustomSwallowedExceptionListener extends SwallowedExceptionListener with Logging {\n+    override def onSwallowException(e: Exception): Unit = {\n+      logError(s\"Error closing Kafka consumer\", e)\n+    }\n+  }\n+\n+  class PoolConfig extends GenericKeyedObjectPoolConfig[InternalKafkaConsumer] {\n+    private var softMaxTotal = Int.MaxValue\n+\n+    def getSoftMaxTotal(): Int = softMaxTotal\n+\n+    init()\n+\n+    def init(): Unit = {\n+      import PoolConfig._\n+\n+      val conf = SparkEnv.get.conf\n+\n+      softMaxTotal = conf.getInt(CONFIG_NAME_CAPACITY, DEFAULT_VALUE_CAPACITY)\n+\n+      val jmxEnabled = conf.getBoolean(CONFIG_NAME_JMX_ENABLED,\n+        defaultValue = DEFAULT_VALUE_JMX_ENABLED)\n+      val minEvictableIdleTimeMillis = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+        DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+      val evictorThreadRunIntervalMillis = conf.getLong(\n+        CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+        DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+      // NOTE: Below lines define the behavior, so do not modify unless you know what you are\n+      // doing, and update the class doc accordingly if necessary when you modify.\n+\n+      // 1. Set min idle objects per key to 0 to avoid creating unnecessary object.\n+      // 2. Set max idle objects per key to 3 but set total objects per key to infinite\n+      // which ensures borrowing per key is not restricted.\n+      // 3. Set max total objects to infinite which ensures all objects are managed in this pool.\n+      setMinIdlePerKey(0)\n+      setMaxIdlePerKey(3)\n+      setMaxTotalPerKey(-1)\n+      setMaxTotal(-1)\n+\n+      // Set minimum evictable idle time which will be referred from evictor thread\n+      setMinEvictableIdleTimeMillis(minEvictableIdleTimeMillis)\n+      setSoftMinEvictableIdleTimeMillis(-1)\n+\n+      // evictor thread will run test with ten idle objects\n+      setTimeBetweenEvictionRunsMillis(evictorThreadRunIntervalMillis)\n+      setNumTestsPerEvictionRun(10)\n+      setEvictionPolicy(new DefaultEvictionPolicy[InternalKafkaConsumer]())\n+\n+      // Immediately fail on exhausted pool while borrowing\n+      setBlockWhenExhausted(false)\n+\n+      setJmxEnabled(jmxEnabled)\n+      setJmxNamePrefix(\"kafka010-cached-simple-kafka-consumer-pool\")\n+    }\n+  }\n+\n+  object PoolConfig {\n+    val CONFIG_NAME_PREFIX = \"spark.sql.kafkaConsumerCache.\"\n+    val CONFIG_NAME_CAPACITY = CONFIG_NAME_PREFIX + \"capacity\"\n+    val CONFIG_NAME_JMX_ENABLED = CONFIG_NAME_PREFIX + \"jmx.enable\"\n+    val CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS = CONFIG_NAME_PREFIX +\n+      \"minEvictableIdleTimeMillis\"\n+    val CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS = CONFIG_NAME_PREFIX +\n+      \"evictorThreadRunIntervalMillis\"\n+\n+    val DEFAULT_VALUE_CAPACITY = 64\n+    val DEFAULT_VALUE_JMX_ENABLED = false\n+    val DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS = 5 * 60 * 1000 // 5 minutes\n+    val DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS = 3 * 60 * 1000 // 3 minutes\n+  }\n+\n+  class ObjectFactory extends BaseKeyedPooledObjectFactory[CacheKey, InternalKafkaConsumer]\n+    with Logging {\n+\n+    val keyToKafkaParams: ConcurrentHashMap[CacheKey, ju.Map[String, Object]] =\n+      new ConcurrentHashMap[CacheKey, ju.Map[String, Object]]()\n+\n+    override def create(key: CacheKey): InternalKafkaConsumer = {\n+      val kafkaParams = keyToKafkaParams.get(key)\n+      if (kafkaParams == null) {"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "I agree it is scala idiomatic. Let me see whether it is critical path, and apply `Option` if it's not.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-05T15:17:49Z",
    "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(\n+    objectFactory: ObjectFactory,\n+    poolConfig: PoolConfig) {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (getTotal == poolConfig.getSoftMaxTotal()) {\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates all idle consumers for the key */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def getNumIdle: Int = pool.getNumIdle\n+\n+  def getNumIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def getNumActive: Int = pool.getNumActive\n+\n+  def getNumActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def getTotal: Int = getNumIdle + getNumActive\n+\n+  def getTotal(key: CacheKey): Int = getNumIdle(key) + getNumActive(key)\n+\n+  private def updateKafkaParamForKey(key: CacheKey, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+  }\n+\n+  private def extractCacheKey(consumer: InternalKafkaConsumer): CacheKey = {\n+    new CacheKey(consumer.topicPartition, consumer.kafkaParams)\n+  }\n+}\n+\n+private[kafka010] object InternalKafkaConsumerPool {\n+\n+  /**\n+   * Builds the pool for [[InternalKafkaConsumer]]. The pool instance is created per each call.\n+   */\n+  def build: InternalKafkaConsumerPool = {\n+    val objFactory = new ObjectFactory\n+    val poolConfig = new PoolConfig\n+    new InternalKafkaConsumerPool(objFactory, poolConfig)\n+  }\n+\n+  case class PooledObjectInvalidated(key: CacheKey, lastInvalidatedTimestamp: Long,\n+                                     lastBorrowedTime: Long) extends RuntimeException\n+\n+  object CustomSwallowedExceptionListener extends SwallowedExceptionListener with Logging {\n+    override def onSwallowException(e: Exception): Unit = {\n+      logError(s\"Error closing Kafka consumer\", e)\n+    }\n+  }\n+\n+  class PoolConfig extends GenericKeyedObjectPoolConfig[InternalKafkaConsumer] {\n+    private var softMaxTotal = Int.MaxValue\n+\n+    def getSoftMaxTotal(): Int = softMaxTotal\n+\n+    init()\n+\n+    def init(): Unit = {\n+      import PoolConfig._\n+\n+      val conf = SparkEnv.get.conf\n+\n+      softMaxTotal = conf.getInt(CONFIG_NAME_CAPACITY, DEFAULT_VALUE_CAPACITY)\n+\n+      val jmxEnabled = conf.getBoolean(CONFIG_NAME_JMX_ENABLED,\n+        defaultValue = DEFAULT_VALUE_JMX_ENABLED)\n+      val minEvictableIdleTimeMillis = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+        DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+      val evictorThreadRunIntervalMillis = conf.getLong(\n+        CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+        DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+      // NOTE: Below lines define the behavior, so do not modify unless you know what you are\n+      // doing, and update the class doc accordingly if necessary when you modify.\n+\n+      // 1. Set min idle objects per key to 0 to avoid creating unnecessary object.\n+      // 2. Set max idle objects per key to 3 but set total objects per key to infinite\n+      // which ensures borrowing per key is not restricted.\n+      // 3. Set max total objects to infinite which ensures all objects are managed in this pool.\n+      setMinIdlePerKey(0)\n+      setMaxIdlePerKey(3)\n+      setMaxTotalPerKey(-1)\n+      setMaxTotal(-1)\n+\n+      // Set minimum evictable idle time which will be referred from evictor thread\n+      setMinEvictableIdleTimeMillis(minEvictableIdleTimeMillis)\n+      setSoftMinEvictableIdleTimeMillis(-1)\n+\n+      // evictor thread will run test with ten idle objects\n+      setTimeBetweenEvictionRunsMillis(evictorThreadRunIntervalMillis)\n+      setNumTestsPerEvictionRun(10)\n+      setEvictionPolicy(new DefaultEvictionPolicy[InternalKafkaConsumer]())\n+\n+      // Immediately fail on exhausted pool while borrowing\n+      setBlockWhenExhausted(false)\n+\n+      setJmxEnabled(jmxEnabled)\n+      setJmxNamePrefix(\"kafka010-cached-simple-kafka-consumer-pool\")\n+    }\n+  }\n+\n+  object PoolConfig {\n+    val CONFIG_NAME_PREFIX = \"spark.sql.kafkaConsumerCache.\"\n+    val CONFIG_NAME_CAPACITY = CONFIG_NAME_PREFIX + \"capacity\"\n+    val CONFIG_NAME_JMX_ENABLED = CONFIG_NAME_PREFIX + \"jmx.enable\"\n+    val CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS = CONFIG_NAME_PREFIX +\n+      \"minEvictableIdleTimeMillis\"\n+    val CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS = CONFIG_NAME_PREFIX +\n+      \"evictorThreadRunIntervalMillis\"\n+\n+    val DEFAULT_VALUE_CAPACITY = 64\n+    val DEFAULT_VALUE_JMX_ENABLED = false\n+    val DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS = 5 * 60 * 1000 // 5 minutes\n+    val DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS = 3 * 60 * 1000 // 3 minutes\n+  }\n+\n+  class ObjectFactory extends BaseKeyedPooledObjectFactory[CacheKey, InternalKafkaConsumer]\n+    with Logging {\n+\n+    val keyToKafkaParams: ConcurrentHashMap[CacheKey, ju.Map[String, Object]] =\n+      new ConcurrentHashMap[CacheKey, ju.Map[String, Object]]()\n+\n+    override def create(key: CacheKey): InternalKafkaConsumer = {\n+      val kafkaParams = keyToKafkaParams.get(key)\n+      if (kafkaParams == null) {"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Then why not enforce it?",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-04T07:39:05Z",
    "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(\n+    objectFactory: ObjectFactory,\n+    poolConfig: PoolConfig) {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (getTotal == poolConfig.getSoftMaxTotal()) {\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates all idle consumers for the key */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def getNumIdle: Int = pool.getNumIdle\n+\n+  def getNumIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def getNumActive: Int = pool.getNumActive\n+\n+  def getNumActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def getTotal: Int = getNumIdle + getNumActive\n+\n+  def getTotal(key: CacheKey): Int = getNumIdle(key) + getNumActive(key)\n+\n+  private def updateKafkaParamForKey(key: CacheKey, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,",
    "line": 137
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "This is actually one of optimization kept in existing cache: if `kafkaParams` is cheap enough to be added to CacheKey I would do it. Would we be better to add `require` to do assertion?",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-04T13:16:13Z",
    "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(\n+    objectFactory: ObjectFactory,\n+    poolConfig: PoolConfig) {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (getTotal == poolConfig.getSoftMaxTotal()) {\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates all idle consumers for the key */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def getNumIdle: Int = pool.getNumIdle\n+\n+  def getNumIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def getNumActive: Int = pool.getNumActive\n+\n+  def getNumActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def getTotal: Int = getNumIdle + getNumActive\n+\n+  def getTotal(key: CacheKey): Int = getNumIdle(key) + getNumActive(key)\n+\n+  private def updateKafkaParamForKey(key: CacheKey, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,",
    "line": 137
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "I meant some kind of assertion. When all goes well it should never fire but if somebody touches this codepart later and doesn't read this, can be notified easily.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-05T13:46:05Z",
    "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(\n+    objectFactory: ObjectFactory,\n+    poolConfig: PoolConfig) {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (getTotal == poolConfig.getSoftMaxTotal()) {\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates all idle consumers for the key */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def getNumIdle: Int = pool.getNumIdle\n+\n+  def getNumIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def getNumActive: Int = pool.getNumActive\n+\n+  def getNumActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def getTotal: Int = getNumIdle + getNumActive\n+\n+  def getTotal(key: CacheKey): Int = getNumIdle(key) + getNumActive(key)\n+\n+  private def updateKafkaParamForKey(key: CacheKey, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,",
    "line": 137
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "OK makes sense. Will add assertion.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-05T15:10:27Z",
    "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(\n+    objectFactory: ObjectFactory,\n+    poolConfig: PoolConfig) {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (getTotal == poolConfig.getSoftMaxTotal()) {\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates all idle consumers for the key */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def getNumIdle: Int = pool.getNumIdle\n+\n+  def getNumIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def getNumActive: Int = pool.getNumActive\n+\n+  def getNumActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def getTotal: Int = getNumIdle + getNumActive\n+\n+  def getTotal(key: CacheKey): Int = getNumIdle(key) + getNumActive(key)\n+\n+  private def updateKafkaParamForKey(key: CacheKey, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,",
    "line": 137
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "I don't see where it's used.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-19T08:21:39Z",
    "diffHunk": "@@ -0,0 +1,243 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(\n+    objectFactory: ObjectFactory,\n+    poolConfig: PoolConfig) {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (getTotal == poolConfig.getSoftMaxTotal()) {\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates all idle consumers for the key */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def getNumIdle: Int = pool.getNumIdle\n+\n+  def getNumIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def getNumActive: Int = pool.getNumActive\n+\n+  def getNumActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def getTotal: Int = getNumIdle + getNumActive\n+\n+  def getTotal(key: CacheKey): Int = getNumIdle(key) + getNumActive(key)\n+\n+  private def updateKafkaParamForKey(key: CacheKey, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    val oldKafkaParams = objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+    require(oldKafkaParams == null || kafkaParams == oldKafkaParams, \"Kafka parameters for same \" +\n+      s\"cache key should be equal. old parameters: $oldKafkaParams new parameters: $kafkaParams\")\n+  }\n+\n+  private def extractCacheKey(consumer: InternalKafkaConsumer): CacheKey = {\n+    new CacheKey(consumer.topicPartition, consumer.kafkaParams)\n+  }\n+}\n+\n+private[kafka010] object InternalKafkaConsumerPool {\n+\n+  /**\n+   * Builds the pool for [[InternalKafkaConsumer]]. The pool instance is created per each call.\n+   */\n+  def build: InternalKafkaConsumerPool = {\n+    val objFactory = new ObjectFactory\n+    val poolConfig = new PoolConfig\n+    new InternalKafkaConsumerPool(objFactory, poolConfig)\n+  }\n+\n+  case class PooledObjectInvalidated(key: CacheKey, lastInvalidatedTimestamp: Long,"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "That was used from previous approach. Thanks for finding. Will remove.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2018-09-19T12:08:51Z",
    "diffHunk": "@@ -0,0 +1,243 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(\n+    objectFactory: ObjectFactory,\n+    poolConfig: PoolConfig) {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (getTotal == poolConfig.getSoftMaxTotal()) {\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates all idle consumers for the key */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def getNumIdle: Int = pool.getNumIdle\n+\n+  def getNumIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def getNumActive: Int = pool.getNumActive\n+\n+  def getNumActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def getTotal: Int = getNumIdle + getNumActive\n+\n+  def getTotal(key: CacheKey): Int = getNumIdle(key) + getNumActive(key)\n+\n+  private def updateKafkaParamForKey(key: CacheKey, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    val oldKafkaParams = objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+    require(oldKafkaParams == null || kafkaParams == oldKafkaParams, \"Kafka parameters for same \" +\n+      s\"cache key should be equal. old parameters: $oldKafkaParams new parameters: $kafkaParams\")\n+  }\n+\n+  private def extractCacheKey(consumer: InternalKafkaConsumer): CacheKey = {\n+    new CacheKey(consumer.topicPartition, consumer.kafkaParams)\n+  }\n+}\n+\n+private[kafka010] object InternalKafkaConsumerPool {\n+\n+  /**\n+   * Builds the pool for [[InternalKafkaConsumer]]. The pool instance is created per each call.\n+   */\n+  def build: InternalKafkaConsumerPool = {\n+    val objFactory = new ObjectFactory\n+    val poolConfig = new PoolConfig\n+    new InternalKafkaConsumerPool(objFactory, poolConfig)\n+  }\n+\n+  case class PooledObjectInvalidated(key: CacheKey, lastInvalidatedTimestamp: Long,"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Any reason why these are not config constants?",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-07-10T22:52:12Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(\n+    objectFactory: ObjectFactory,\n+    poolConfig: PoolConfig) {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (getTotal == poolConfig.getSoftMaxTotal()) {\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates all idle consumers for the key */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def reset(): Unit = {\n+    // this is the best-effort of clearing up. otherwise we should close the pool and create again\n+    // but we don't want to make it \"var\" only because of tests.\n+    pool.clear()\n+  }\n+\n+  def getNumIdle: Int = pool.getNumIdle\n+\n+  def getNumIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def getNumActive: Int = pool.getNumActive\n+\n+  def getNumActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def getTotal: Int = getNumIdle + getNumActive\n+\n+  def getTotal(key: CacheKey): Int = getNumIdle(key) + getNumActive(key)\n+\n+  private def updateKafkaParamForKey(key: CacheKey, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    val oldKafkaParams = objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+    require(oldKafkaParams == null || kafkaParams == oldKafkaParams, \"Kafka parameters for same \" +\n+      s\"cache key should be equal. old parameters: $oldKafkaParams new parameters: $kafkaParams\")\n+  }\n+\n+  private def extractCacheKey(consumer: InternalKafkaConsumer): CacheKey = {\n+    new CacheKey(consumer.topicPartition, consumer.kafkaParams)\n+  }\n+}\n+\n+private[kafka010] object InternalKafkaConsumerPool {\n+\n+  /**\n+   * Builds the pool for [[InternalKafkaConsumer]]. The pool instance is created per each call.\n+   */\n+  def build: InternalKafkaConsumerPool = {\n+    val objFactory = new ObjectFactory\n+    val poolConfig = new PoolConfig\n+    new InternalKafkaConsumerPool(objFactory, poolConfig)\n+  }\n+\n+  object CustomSwallowedExceptionListener extends SwallowedExceptionListener with Logging {\n+    override def onSwallowException(e: Exception): Unit = {\n+      logError(s\"Error closing Kafka consumer\", e)\n+    }\n+  }\n+\n+  class PoolConfig extends GenericKeyedObjectPoolConfig[InternalKafkaConsumer] {\n+    private var softMaxTotal = Int.MaxValue\n+\n+    def getSoftMaxTotal(): Int = softMaxTotal\n+\n+    init()\n+\n+    def init(): Unit = {\n+      import PoolConfig._\n+\n+      val conf = SparkEnv.get.conf\n+\n+      softMaxTotal = conf.getInt(CONFIG_NAME_CAPACITY, DEFAULT_VALUE_CAPACITY)\n+\n+      val jmxEnabled = conf.getBoolean(CONFIG_NAME_JMX_ENABLED,\n+        defaultValue = DEFAULT_VALUE_JMX_ENABLED)\n+      val minEvictableIdleTimeMillis = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+        DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+      val evictorThreadRunIntervalMillis = conf.getLong(\n+        CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+        DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+      // NOTE: Below lines define the behavior, so do not modify unless you know what you are\n+      // doing, and update the class doc accordingly if necessary when you modify.\n+\n+      // 1. Set min idle objects per key to 0 to avoid creating unnecessary object.\n+      // 2. Set max idle objects per key to 3 but set total objects per key to infinite\n+      // which ensures borrowing per key is not restricted.\n+      // 3. Set max total objects to infinite which ensures all objects are managed in this pool.\n+      setMinIdlePerKey(0)\n+      setMaxIdlePerKey(3)\n+      setMaxTotalPerKey(-1)\n+      setMaxTotal(-1)\n+\n+      // Set minimum evictable idle time which will be referred from evictor thread\n+      setMinEvictableIdleTimeMillis(minEvictableIdleTimeMillis)\n+      setSoftMinEvictableIdleTimeMillis(-1)\n+\n+      // evictor thread will run test with ten idle objects\n+      setTimeBetweenEvictionRunsMillis(evictorThreadRunIntervalMillis)\n+      setNumTestsPerEvictionRun(10)\n+      setEvictionPolicy(new DefaultEvictionPolicy[InternalKafkaConsumer]())\n+\n+      // Immediately fail on exhausted pool while borrowing\n+      setBlockWhenExhausted(false)\n+\n+      setJmxEnabled(jmxEnabled)\n+      setJmxNamePrefix(\"kafka010-cached-simple-kafka-consumer-pool\")\n+    }\n+  }\n+\n+  object PoolConfig {\n+    val CONFIG_NAME_PREFIX = \"spark.sql.kafkaConsumerCache.\"\n+    val CONFIG_NAME_CAPACITY = CONFIG_NAME_PREFIX + \"capacity\""
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "I guess I was following the style of existing code, but since we have put efforts on moving string config to config constants on Spark 3.0, we could apply it. Will make a change.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-07-13T00:41:55Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(\n+    objectFactory: ObjectFactory,\n+    poolConfig: PoolConfig) {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (getTotal == poolConfig.getSoftMaxTotal()) {\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates all idle consumers for the key */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def reset(): Unit = {\n+    // this is the best-effort of clearing up. otherwise we should close the pool and create again\n+    // but we don't want to make it \"var\" only because of tests.\n+    pool.clear()\n+  }\n+\n+  def getNumIdle: Int = pool.getNumIdle\n+\n+  def getNumIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def getNumActive: Int = pool.getNumActive\n+\n+  def getNumActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def getTotal: Int = getNumIdle + getNumActive\n+\n+  def getTotal(key: CacheKey): Int = getNumIdle(key) + getNumActive(key)\n+\n+  private def updateKafkaParamForKey(key: CacheKey, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    val oldKafkaParams = objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+    require(oldKafkaParams == null || kafkaParams == oldKafkaParams, \"Kafka parameters for same \" +\n+      s\"cache key should be equal. old parameters: $oldKafkaParams new parameters: $kafkaParams\")\n+  }\n+\n+  private def extractCacheKey(consumer: InternalKafkaConsumer): CacheKey = {\n+    new CacheKey(consumer.topicPartition, consumer.kafkaParams)\n+  }\n+}\n+\n+private[kafka010] object InternalKafkaConsumerPool {\n+\n+  /**\n+   * Builds the pool for [[InternalKafkaConsumer]]. The pool instance is created per each call.\n+   */\n+  def build: InternalKafkaConsumerPool = {\n+    val objFactory = new ObjectFactory\n+    val poolConfig = new PoolConfig\n+    new InternalKafkaConsumerPool(objFactory, poolConfig)\n+  }\n+\n+  object CustomSwallowedExceptionListener extends SwallowedExceptionListener with Logging {\n+    override def onSwallowException(e: Exception): Unit = {\n+      logError(s\"Error closing Kafka consumer\", e)\n+    }\n+  }\n+\n+  class PoolConfig extends GenericKeyedObjectPoolConfig[InternalKafkaConsumer] {\n+    private var softMaxTotal = Int.MaxValue\n+\n+    def getSoftMaxTotal(): Int = softMaxTotal\n+\n+    init()\n+\n+    def init(): Unit = {\n+      import PoolConfig._\n+\n+      val conf = SparkEnv.get.conf\n+\n+      softMaxTotal = conf.getInt(CONFIG_NAME_CAPACITY, DEFAULT_VALUE_CAPACITY)\n+\n+      val jmxEnabled = conf.getBoolean(CONFIG_NAME_JMX_ENABLED,\n+        defaultValue = DEFAULT_VALUE_JMX_ENABLED)\n+      val minEvictableIdleTimeMillis = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+        DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+      val evictorThreadRunIntervalMillis = conf.getLong(\n+        CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+        DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+      // NOTE: Below lines define the behavior, so do not modify unless you know what you are\n+      // doing, and update the class doc accordingly if necessary when you modify.\n+\n+      // 1. Set min idle objects per key to 0 to avoid creating unnecessary object.\n+      // 2. Set max idle objects per key to 3 but set total objects per key to infinite\n+      // which ensures borrowing per key is not restricted.\n+      // 3. Set max total objects to infinite which ensures all objects are managed in this pool.\n+      setMinIdlePerKey(0)\n+      setMaxIdlePerKey(3)\n+      setMaxTotalPerKey(-1)\n+      setMaxTotal(-1)\n+\n+      // Set minimum evictable idle time which will be referred from evictor thread\n+      setMinEvictableIdleTimeMillis(minEvictableIdleTimeMillis)\n+      setSoftMinEvictableIdleTimeMillis(-1)\n+\n+      // evictor thread will run test with ten idle objects\n+      setTimeBetweenEvictionRunsMillis(evictorThreadRunIntervalMillis)\n+      setNumTestsPerEvictionRun(10)\n+      setEvictionPolicy(new DefaultEvictionPolicy[InternalKafkaConsumer]())\n+\n+      // Immediately fail on exhausted pool while borrowing\n+      setBlockWhenExhausted(false)\n+\n+      setJmxEnabled(jmxEnabled)\n+      setJmxNamePrefix(\"kafka010-cached-simple-kafka-consumer-pool\")\n+    }\n+  }\n+\n+  object PoolConfig {\n+    val CONFIG_NAME_PREFIX = \"spark.sql.kafkaConsumerCache.\"\n+    val CONFIG_NAME_CAPACITY = CONFIG_NAME_PREFIX + \"capacity\""
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Same prefix stuff here.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-07-12T13:10:29Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(\n+    objectFactory: ObjectFactory,\n+    poolConfig: PoolConfig) {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (getTotal == poolConfig.getSoftMaxTotal()) {\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates all idle consumers for the key */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def reset(): Unit = {\n+    // this is the best-effort of clearing up. otherwise we should close the pool and create again\n+    // but we don't want to make it \"var\" only because of tests.\n+    pool.clear()\n+  }\n+\n+  def getNumIdle: Int = pool.getNumIdle\n+\n+  def getNumIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def getNumActive: Int = pool.getNumActive\n+\n+  def getNumActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def getTotal: Int = getNumIdle + getNumActive\n+\n+  def getTotal(key: CacheKey): Int = getNumIdle(key) + getNumActive(key)\n+\n+  private def updateKafkaParamForKey(key: CacheKey, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    val oldKafkaParams = objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+    require(oldKafkaParams == null || kafkaParams == oldKafkaParams, \"Kafka parameters for same \" +\n+      s\"cache key should be equal. old parameters: $oldKafkaParams new parameters: $kafkaParams\")\n+  }\n+\n+  private def extractCacheKey(consumer: InternalKafkaConsumer): CacheKey = {\n+    new CacheKey(consumer.topicPartition, consumer.kafkaParams)\n+  }\n+}\n+\n+private[kafka010] object InternalKafkaConsumerPool {\n+\n+  /**\n+   * Builds the pool for [[InternalKafkaConsumer]]. The pool instance is created per each call.\n+   */\n+  def build: InternalKafkaConsumerPool = {\n+    val objFactory = new ObjectFactory\n+    val poolConfig = new PoolConfig\n+    new InternalKafkaConsumerPool(objFactory, poolConfig)\n+  }\n+\n+  object CustomSwallowedExceptionListener extends SwallowedExceptionListener with Logging {\n+    override def onSwallowException(e: Exception): Unit = {\n+      logError(s\"Error closing Kafka consumer\", e)\n+    }\n+  }\n+\n+  class PoolConfig extends GenericKeyedObjectPoolConfig[InternalKafkaConsumer] {\n+    private var softMaxTotal = Int.MaxValue\n+\n+    def getSoftMaxTotal(): Int = softMaxTotal\n+\n+    init()\n+\n+    def init(): Unit = {\n+      import PoolConfig._\n+\n+      val conf = SparkEnv.get.conf\n+\n+      softMaxTotal = conf.getInt(CONFIG_NAME_CAPACITY, DEFAULT_VALUE_CAPACITY)\n+\n+      val jmxEnabled = conf.getBoolean(CONFIG_NAME_JMX_ENABLED,\n+        defaultValue = DEFAULT_VALUE_JMX_ENABLED)\n+      val minEvictableIdleTimeMillis = conf.getLong(CONFIG_NAME_MIN_EVICTABLE_IDLE_TIME_MILLIS,\n+        DEFAULT_VALUE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+      val evictorThreadRunIntervalMillis = conf.getLong(\n+        CONFIG_NAME_EVICTOR_THREAD_RUN_INTERVAL_MILLIS,\n+        DEFAULT_VALUE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+      // NOTE: Below lines define the behavior, so do not modify unless you know what you are\n+      // doing, and update the class doc accordingly if necessary when you modify.\n+\n+      // 1. Set min idle objects per key to 0 to avoid creating unnecessary object.\n+      // 2. Set max idle objects per key to 3 but set total objects per key to infinite\n+      // which ensures borrowing per key is not restricted.\n+      // 3. Set max total objects to infinite which ensures all objects are managed in this pool.\n+      setMinIdlePerKey(0)\n+      setMaxIdlePerKey(3)\n+      setMaxTotalPerKey(-1)\n+      setMaxTotal(-1)\n+\n+      // Set minimum evictable idle time which will be referred from evictor thread\n+      setMinEvictableIdleTimeMillis(minEvictableIdleTimeMillis)\n+      setSoftMinEvictableIdleTimeMillis(-1)\n+\n+      // evictor thread will run test with ten idle objects\n+      setTimeBetweenEvictionRunsMillis(evictorThreadRunIntervalMillis)\n+      setNumTestsPerEvictionRun(10)\n+      setEvictionPolicy(new DefaultEvictionPolicy[InternalKafkaConsumer]())\n+\n+      // Immediately fail on exhausted pool while borrowing\n+      setBlockWhenExhausted(false)\n+\n+      setJmxEnabled(jmxEnabled)\n+      setJmxNamePrefix(\"kafka010-cached-simple-kafka-consumer-pool\")\n+    }\n+  }\n+\n+  object PoolConfig {\n+    val CONFIG_NAME_PREFIX = \"spark.sql.kafkaConsumerCache.\""
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Any reason why this needs to be lazy?",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-26T18:28:05Z",
    "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(\n+    objectFactory: ObjectFactory,\n+    poolConfig: PoolConfig) {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private lazy val pool = {"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Just wanted to delay spending cost to when it's actually used, but now I think it's not a big deal as it's meant to be used fairly soon. Will remove lazy.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-26T20:05:06Z",
    "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(\n+    objectFactory: ObjectFactory,\n+    poolConfig: PoolConfig) {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private lazy val pool = {"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Same as previous comment about avoiding `get` in Scala.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-26T18:30:36Z",
    "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(\n+    objectFactory: ObjectFactory,\n+    poolConfig: PoolConfig) {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (getTotal == poolConfig.getSoftMaxTotal()) {\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates all idle consumers for the key */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def reset(): Unit = {\n+    // this is the best-effort of clearing up. otherwise we should close the pool and create again\n+    // but we don't want to make it \"var\" only because of tests.\n+    pool.clear()\n+  }\n+\n+  def getNumIdle: Int = pool.getNumIdle"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "\"count\" or \"size\" instead of \"total\"?",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-26T18:31:03Z",
    "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(\n+    objectFactory: ObjectFactory,\n+    poolConfig: PoolConfig) {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (getTotal == poolConfig.getSoftMaxTotal()) {\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates all idle consumers for the key */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def reset(): Unit = {\n+    // this is the best-effort of clearing up. otherwise we should close the pool and create again\n+    // but we don't want to make it \"var\" only because of tests.\n+    pool.clear()\n+  }\n+\n+  def getNumIdle: Int = pool.getNumIdle\n+\n+  def getNumIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def getNumActive: Int = pool.getNumActive\n+\n+  def getNumActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def getTotal: Int = getNumIdle + getNumActive"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Same comment re: get. e.g. here use `def softMaxTotal`, and use an underscore in the `var` where the value is stored.\r\n\r\nAnd also same re: using \"count\" or \"size\".",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-26T18:35:42Z",
    "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(\n+    objectFactory: ObjectFactory,\n+    poolConfig: PoolConfig) {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (getTotal == poolConfig.getSoftMaxTotal()) {\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates all idle consumers for the key */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def reset(): Unit = {\n+    // this is the best-effort of clearing up. otherwise we should close the pool and create again\n+    // but we don't want to make it \"var\" only because of tests.\n+    pool.clear()\n+  }\n+\n+  def getNumIdle: Int = pool.getNumIdle\n+\n+  def getNumIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def getNumActive: Int = pool.getNumActive\n+\n+  def getNumActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def getTotal: Int = getNumIdle + getNumActive\n+\n+  def getTotal(key: CacheKey): Int = getNumIdle(key) + getNumActive(key)\n+\n+  private def updateKafkaParamForKey(key: CacheKey, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    val oldKafkaParams = objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+    require(oldKafkaParams == null || kafkaParams == oldKafkaParams, \"Kafka parameters for same \" +\n+      s\"cache key should be equal. old parameters: $oldKafkaParams new parameters: $kafkaParams\")\n+  }\n+\n+  private def extractCacheKey(consumer: InternalKafkaConsumer): CacheKey = {\n+    new CacheKey(consumer.topicPartition, consumer.kafkaParams)\n+  }\n+}\n+\n+private[kafka010] object InternalKafkaConsumerPool {\n+\n+  /**\n+   * Builds the pool for [[InternalKafkaConsumer]]. The pool instance is created per each call.\n+   */\n+  def build: InternalKafkaConsumerPool = {\n+    val objFactory = new ObjectFactory\n+    val poolConfig = new PoolConfig\n+    new InternalKafkaConsumerPool(objFactory, poolConfig)\n+  }\n+\n+  object CustomSwallowedExceptionListener extends SwallowedExceptionListener with Logging {\n+    override def onSwallowException(e: Exception): Unit = {\n+      logError(s\"Error closing Kafka consumer\", e)\n+    }\n+  }\n+\n+  class PoolConfig extends GenericKeyedObjectPoolConfig[InternalKafkaConsumer] {\n+    private var softMaxTotal = Int.MaxValue\n+\n+    def getSoftMaxTotal(): Int = softMaxTotal"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "The type declaration is redundant here.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-26T18:38:33Z",
    "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkEnv\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"spark.sql.kafkaConsumerCache.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(\n+    objectFactory: ObjectFactory,\n+    poolConfig: PoolConfig) {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private lazy val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (getTotal == poolConfig.getSoftMaxTotal()) {\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates all idle consumers for the key */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def reset(): Unit = {\n+    // this is the best-effort of clearing up. otherwise we should close the pool and create again\n+    // but we don't want to make it \"var\" only because of tests.\n+    pool.clear()\n+  }\n+\n+  def getNumIdle: Int = pool.getNumIdle\n+\n+  def getNumIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def getNumActive: Int = pool.getNumActive\n+\n+  def getNumActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def getTotal: Int = getNumIdle + getNumActive\n+\n+  def getTotal(key: CacheKey): Int = getNumIdle(key) + getNumActive(key)\n+\n+  private def updateKafkaParamForKey(key: CacheKey, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    val oldKafkaParams = objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+    require(oldKafkaParams == null || kafkaParams == oldKafkaParams, \"Kafka parameters for same \" +\n+      s\"cache key should be equal. old parameters: $oldKafkaParams new parameters: $kafkaParams\")\n+  }\n+\n+  private def extractCacheKey(consumer: InternalKafkaConsumer): CacheKey = {\n+    new CacheKey(consumer.topicPartition, consumer.kafkaParams)\n+  }\n+}\n+\n+private[kafka010] object InternalKafkaConsumerPool {\n+\n+  /**\n+   * Builds the pool for [[InternalKafkaConsumer]]. The pool instance is created per each call.\n+   */\n+  def build: InternalKafkaConsumerPool = {\n+    val objFactory = new ObjectFactory\n+    val poolConfig = new PoolConfig\n+    new InternalKafkaConsumerPool(objFactory, poolConfig)\n+  }\n+\n+  object CustomSwallowedExceptionListener extends SwallowedExceptionListener with Logging {\n+    override def onSwallowException(e: Exception): Unit = {\n+      logError(s\"Error closing Kafka consumer\", e)\n+    }\n+  }\n+\n+  class PoolConfig extends GenericKeyedObjectPoolConfig[InternalKafkaConsumer] {\n+    private var softMaxTotal = Int.MaxValue\n+\n+    def getSoftMaxTotal(): Int = softMaxTotal\n+\n+    init()\n+\n+    def init(): Unit = {\n+      val conf = SparkEnv.get.conf\n+\n+      softMaxTotal = conf.get(CONSUMER_CACHE_CAPACITY)\n+\n+      val jmxEnabled = conf.get(CONSUMER_CACHE_JMX_ENABLED)\n+      val minEvictableIdleTimeMillis = conf.get(CONSUMER_CACHE_MIN_EVICTABLE_IDLE_TIME_MILLIS)\n+      val evictorThreadRunIntervalMillis = conf.get(\n+        CONSUMER_CACHE_EVICTOR_THREAD_RUN_INTERVAL_MILLIS)\n+\n+      // NOTE: Below lines define the behavior, so do not modify unless you know what you are\n+      // doing, and update the class doc accordingly if necessary when you modify.\n+\n+      // 1. Set min idle objects per key to 0 to avoid creating unnecessary object.\n+      // 2. Set max idle objects per key to 3 but set total objects per key to infinite\n+      // which ensures borrowing per key is not restricted.\n+      // 3. Set max total objects to infinite which ensures all objects are managed in this pool.\n+      setMinIdlePerKey(0)\n+      setMaxIdlePerKey(3)\n+      setMaxTotalPerKey(-1)\n+      setMaxTotal(-1)\n+\n+      // Set minimum evictable idle time which will be referred from evictor thread\n+      setMinEvictableIdleTimeMillis(minEvictableIdleTimeMillis)\n+      setSoftMinEvictableIdleTimeMillis(-1)\n+\n+      // evictor thread will run test with ten idle objects\n+      setTimeBetweenEvictionRunsMillis(evictorThreadRunIntervalMillis)\n+      setNumTestsPerEvictionRun(10)\n+      setEvictionPolicy(new DefaultEvictionPolicy[InternalKafkaConsumer]())\n+\n+      // Immediately fail on exhausted pool while borrowing\n+      setBlockWhenExhausted(false)\n+\n+      setJmxEnabled(jmxEnabled)\n+      setJmxNamePrefix(\"kafka010-cached-simple-kafka-consumer-pool\")\n+    }\n+  }\n+\n+  class ObjectFactory extends BaseKeyedPooledObjectFactory[CacheKey, InternalKafkaConsumer]\n+    with Logging {\n+\n+    val keyToKafkaParams: ConcurrentHashMap[CacheKey, ju.Map[String, Object]] ="
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Since this is just returning a value, we don't generally add the `()`.\r\n\r\n- use `def method():` when the method does stuff (e.g. initializes things, has to to computation to return something)\r\n- use `def method:` when you're just returning something that has already been computed (e.g. a private field or an entry in a map)\r\n",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-28T19:22:46Z",
    "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"spark.kafka.consumer.cache.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(\n+    objectFactory: ObjectFactory,\n+    poolConfig: PoolConfig) {\n+\n+  def this(conf: SparkConf) = {\n+    this(new ObjectFactory, new PoolConfig(conf))\n+  }\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (size == poolConfig.softMaxSize()) {\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates all idle consumers for the key */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def reset(): Unit = {\n+    // this is the best-effort of clearing up. otherwise we should close the pool and create again\n+    // but we don't want to make it \"var\" only because of tests.\n+    pool.clear()\n+  }\n+\n+  def numIdle: Int = pool.getNumIdle\n+\n+  def numIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def numActive: Int = pool.getNumActive\n+\n+  def numActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def size: Int = numIdle + numActive\n+\n+  def size(key: CacheKey): Int = numIdle(key) + numActive(key)\n+\n+  private def updateKafkaParamForKey(key: CacheKey, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    val oldKafkaParams = objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+    require(oldKafkaParams == null || kafkaParams == oldKafkaParams, \"Kafka parameters for same \" +\n+      s\"cache key should be equal. old parameters: $oldKafkaParams new parameters: $kafkaParams\")\n+  }\n+\n+  private def extractCacheKey(consumer: InternalKafkaConsumer): CacheKey = {\n+    new CacheKey(consumer.topicPartition, consumer.kafkaParams)\n+  }\n+}\n+\n+private[kafka010] object InternalKafkaConsumerPool {\n+  object CustomSwallowedExceptionListener extends SwallowedExceptionListener with Logging {\n+    override def onSwallowException(e: Exception): Unit = {\n+      logError(s\"Error closing Kafka consumer\", e)\n+    }\n+  }\n+\n+  class PoolConfig(conf: SparkConf) extends GenericKeyedObjectPoolConfig[InternalKafkaConsumer] {\n+    private var _softMaxSize = Int.MaxValue\n+\n+    def softMaxSize(): Int = _softMaxSize"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This isn't logging anything.",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-28T19:23:35Z",
    "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"spark.kafka.consumer.cache.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(\n+    objectFactory: ObjectFactory,\n+    poolConfig: PoolConfig) {\n+\n+  def this(conf: SparkConf) = {\n+    this(new ObjectFactory, new PoolConfig(conf))\n+  }\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (size == poolConfig.softMaxSize()) {\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates all idle consumers for the key */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def reset(): Unit = {\n+    // this is the best-effort of clearing up. otherwise we should close the pool and create again\n+    // but we don't want to make it \"var\" only because of tests.\n+    pool.clear()\n+  }\n+\n+  def numIdle: Int = pool.getNumIdle\n+\n+  def numIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def numActive: Int = pool.getNumActive\n+\n+  def numActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def size: Int = numIdle + numActive\n+\n+  def size(key: CacheKey): Int = numIdle(key) + numActive(key)\n+\n+  private def updateKafkaParamForKey(key: CacheKey, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    val oldKafkaParams = objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+    require(oldKafkaParams == null || kafkaParams == oldKafkaParams, \"Kafka parameters for same \" +\n+      s\"cache key should be equal. old parameters: $oldKafkaParams new parameters: $kafkaParams\")\n+  }\n+\n+  private def extractCacheKey(consumer: InternalKafkaConsumer): CacheKey = {\n+    new CacheKey(consumer.topicPartition, consumer.kafkaParams)\n+  }\n+}\n+\n+private[kafka010] object InternalKafkaConsumerPool {\n+  object CustomSwallowedExceptionListener extends SwallowedExceptionListener with Logging {\n+    override def onSwallowException(e: Exception): Unit = {\n+      logError(s\"Error closing Kafka consumer\", e)\n+    }\n+  }\n+\n+  class PoolConfig(conf: SparkConf) extends GenericKeyedObjectPoolConfig[InternalKafkaConsumer] {\n+    private var _softMaxSize = Int.MaxValue\n+\n+    def softMaxSize(): Int = _softMaxSize\n+\n+    init()\n+\n+    def init(): Unit = {\n+      _softMaxSize = conf.get(CONSUMER_CACHE_CAPACITY)\n+\n+      val jmxEnabled = conf.get(CONSUMER_CACHE_JMX_ENABLED)\n+      val minEvictableIdleTimeMillis = conf.get(CONSUMER_CACHE_TIMEOUT)\n+      val evictorThreadRunIntervalMillis = conf.get(\n+        CONSUMER_CACHE_EVICTOR_THREAD_RUN_INTERVAL)\n+\n+      // NOTE: Below lines define the behavior, so do not modify unless you know what you are\n+      // doing, and update the class doc accordingly if necessary when you modify.\n+\n+      // 1. Set min idle objects per key to 0 to avoid creating unnecessary object.\n+      // 2. Set max idle objects per key to 3 but set total objects per key to infinite\n+      // which ensures borrowing per key is not restricted.\n+      // 3. Set max total objects to infinite which ensures all objects are managed in this pool.\n+      setMinIdlePerKey(0)\n+      setMaxIdlePerKey(3)\n+      setMaxTotalPerKey(-1)\n+      setMaxTotal(-1)\n+\n+      // Set minimum evictable idle time which will be referred from evictor thread\n+      setMinEvictableIdleTimeMillis(minEvictableIdleTimeMillis)\n+      setSoftMinEvictableIdleTimeMillis(-1)\n+\n+      // evictor thread will run test with ten idle objects\n+      setTimeBetweenEvictionRunsMillis(evictorThreadRunIntervalMillis)\n+      setNumTestsPerEvictionRun(10)\n+      setEvictionPolicy(new DefaultEvictionPolicy[InternalKafkaConsumer]())\n+\n+      // Immediately fail on exhausted pool while borrowing\n+      setBlockWhenExhausted(false)\n+\n+      setJmxEnabled(jmxEnabled)\n+      setJmxNamePrefix(\"kafka010-cached-simple-kafka-consumer-pool\")\n+    }\n+  }\n+\n+  class ObjectFactory extends BaseKeyedPooledObjectFactory[CacheKey, InternalKafkaConsumer]\n+    with Logging {"
  }],
  "prId": 22138
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Its good point to revisit later. Here `(groupId, topicPartition)` is the key, on producer side `kafkaParams`. Just a side note I haven't seen any performance issues even if the key is relatively huge on the producer side. I've the same understanding to leave it like this for now and revisit when we have some proof which one is better.\r\n\r\nI have the not so strong opinion that using `(groupId, topicPartition)` is the less exact solution because the cache may use a consumer which differs in configuration parameters. Without deep understanding and tests executed I have the feeling this may cause some exotic bugs.\r\n",
    "commit": "68af3d56710e21b4b8a9a1640ededb3eb7d3117b",
    "createdAt": "2019-08-30T12:49:16Z",
    "diffHunk": "@@ -0,0 +1,221 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.kafka010.InternalKafkaConsumerPool._\n+import org.apache.spark.sql.kafka010.KafkaDataConsumer.CacheKey\n+\n+/**\n+ * Provides object pool for [[InternalKafkaConsumer]] which is grouped by [[CacheKey]].\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"spark.kafka.consumer.cache.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe [[InternalKafkaConsumer]]\n+ * unless caller shares the object to multiple threads.\n+ */\n+private[kafka010] class InternalKafkaConsumerPool(\n+    objectFactory: ObjectFactory,\n+    poolConfig: PoolConfig) extends Logging {\n+\n+  def this(conf: SparkConf) = {\n+    this(new ObjectFactory, new PoolConfig(conf))\n+  }\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private val pool = {\n+    val internalPool = new GenericKeyedObjectPool[CacheKey, InternalKafkaConsumer](\n+      objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(CustomSwallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows [[InternalKafkaConsumer]] object from the pool. If there's no idle object for the key,\n+   * the pool will create the [[InternalKafkaConsumer]] object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: CacheKey, kafkaParams: ju.Map[String, Object]): InternalKafkaConsumer = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (size >= poolConfig.softMaxSize) {\n+      logWarning(\"Pool exceeds its soft max size, cleaning up idle objects...\")\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.returnObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(consumer: InternalKafkaConsumer): Unit = {\n+    pool.invalidateObject(extractCacheKey(consumer), consumer)\n+  }\n+\n+  /** Invalidates all idle consumers for the key */\n+  def invalidateKey(key: CacheKey): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def reset(): Unit = {\n+    // this is the best-effort of clearing up. otherwise we should close the pool and create again\n+    // but we don't want to make it \"var\" only because of tests.\n+    pool.clear()\n+  }\n+\n+  def numIdle: Int = pool.getNumIdle\n+\n+  def numIdle(key: CacheKey): Int = pool.getNumIdle(key)\n+\n+  def numActive: Int = pool.getNumActive\n+\n+  def numActive(key: CacheKey): Int = pool.getNumActive(key)\n+\n+  def size: Int = numIdle + numActive\n+\n+  def size(key: CacheKey): Int = numIdle(key) + numActive(key)\n+\n+  // TODO: revisit the relation between CacheKey and kafkaParams - for now it looks a bit weird",
    "line": 131
  }],
  "prId": 22138
}]