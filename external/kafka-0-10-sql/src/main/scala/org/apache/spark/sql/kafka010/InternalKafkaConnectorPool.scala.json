[{
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "For other reviewers: this is pretty same as previous InternalKafkaConsumerPool, except\r\n\r\n1. It brings some abstract method to deal with extracting key from object.\r\n2. It replaces \"consumer\" related words in javadoc/code to common one to apply this to both consumer and producer.",
    "commit": "ebe9708c96e8127b9ef5c43fc26cff2811a79d82",
    "createdAt": "2019-09-21T03:08:13Z",
    "diffHunk": "@@ -0,0 +1,203 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.internal.Logging\n+\n+/**\n+ * Provides object pool for objects which is grouped by a key.\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"poolConfig.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe objects unless caller\n+ * shares the object to multiple threads.\n+ */\n+private[kafka010] abstract class InternalKafkaConnectorPool[K, V]("
  }],
  "prId": 25853
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "For other reviewers: this is pretty same as previous InternalKafkaConsumerPool.PoolConfig, except it brings some abstract methods to enable reading values from different configuration keys.",
    "commit": "ebe9708c96e8127b9ef5c43fc26cff2811a79d82",
    "createdAt": "2019-09-21T03:11:25Z",
    "diffHunk": "@@ -0,0 +1,203 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.internal.Logging\n+\n+/**\n+ * Provides object pool for objects which is grouped by a key.\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"poolConfig.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe objects unless caller\n+ * shares the object to multiple threads.\n+ */\n+private[kafka010] abstract class InternalKafkaConnectorPool[K, V](\n+    objectFactory: ObjectFactory[K, V],\n+    poolConfig: PoolConfig[V],\n+    swallowedExceptionListener: SwallowedExceptionListener) extends Logging {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private val pool = {\n+    val internalPool = new GenericKeyedObjectPool[K, V](objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(swallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows object from the pool. If there's no idle object for the key,\n+   * the pool will create the object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: K, kafkaParams: ju.Map[String, Object]): V = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (size >= poolConfig.softMaxSize) {\n+      logWarning(\"Pool exceeds its soft max size, cleaning up idle objects...\")\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(connector: V): Unit = {\n+    pool.returnObject(createKey(connector), connector)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(connector: V): Unit = {\n+    pool.invalidateObject(createKey(connector), connector)\n+  }\n+\n+  /** Invalidates all idle values for the key */\n+  def invalidateKey(key: K): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def reset(): Unit = {\n+    // this is the best-effort of clearing up. otherwise we should close the pool and create again\n+    // but we don't want to make it \"var\" only because of tests.\n+    pool.clear()\n+  }\n+\n+  def numIdle: Int = pool.getNumIdle\n+\n+  def numIdle(key: K): Int = pool.getNumIdle(key)\n+\n+  def numActive: Int = pool.getNumActive\n+\n+  def numActive(key: K): Int = pool.getNumActive(key)\n+\n+  def size: Int = numIdle + numActive\n+\n+  def size(key: K): Int = numIdle(key) + numActive(key)\n+\n+  private def updateKafkaParamForKey(key: K, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    val oldKafkaParams = objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+    require(oldKafkaParams == null || kafkaParams == oldKafkaParams, \"Kafka parameters for same \" +\n+      s\"cache key should be equal. old parameters: $oldKafkaParams new parameters: $kafkaParams\")\n+  }\n+\n+  protected def createKey(connector: V): K\n+}\n+\n+private[kafka010] abstract class PoolConfig[V] extends GenericKeyedObjectPoolConfig[V] {",
    "line": 137
  }],
  "prId": 25853
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Could you check that we could restrict V to be subtype of `Closeable` (should be reflected to all above classes), and retain `destroyObject` only here?",
    "commit": "ebe9708c96e8127b9ef5c43fc26cff2811a79d82",
    "createdAt": "2019-09-21T03:16:36Z",
    "diffHunk": "@@ -0,0 +1,203 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.internal.Logging\n+\n+/**\n+ * Provides object pool for objects which is grouped by a key.\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"poolConfig.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe objects unless caller\n+ * shares the object to multiple threads.\n+ */\n+private[kafka010] abstract class InternalKafkaConnectorPool[K, V](\n+    objectFactory: ObjectFactory[K, V],\n+    poolConfig: PoolConfig[V],\n+    swallowedExceptionListener: SwallowedExceptionListener) extends Logging {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private val pool = {\n+    val internalPool = new GenericKeyedObjectPool[K, V](objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(swallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows object from the pool. If there's no idle object for the key,\n+   * the pool will create the object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: K, kafkaParams: ju.Map[String, Object]): V = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (size >= poolConfig.softMaxSize) {\n+      logWarning(\"Pool exceeds its soft max size, cleaning up idle objects...\")\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(connector: V): Unit = {\n+    pool.returnObject(createKey(connector), connector)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(connector: V): Unit = {\n+    pool.invalidateObject(createKey(connector), connector)\n+  }\n+\n+  /** Invalidates all idle values for the key */\n+  def invalidateKey(key: K): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def reset(): Unit = {\n+    // this is the best-effort of clearing up. otherwise we should close the pool and create again\n+    // but we don't want to make it \"var\" only because of tests.\n+    pool.clear()\n+  }\n+\n+  def numIdle: Int = pool.getNumIdle\n+\n+  def numIdle(key: K): Int = pool.getNumIdle(key)\n+\n+  def numActive: Int = pool.getNumActive\n+\n+  def numActive(key: K): Int = pool.getNumActive(key)\n+\n+  def size: Int = numIdle + numActive\n+\n+  def size(key: K): Int = numIdle(key) + numActive(key)\n+\n+  private def updateKafkaParamForKey(key: K, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    val oldKafkaParams = objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+    require(oldKafkaParams == null || kafkaParams == oldKafkaParams, \"Kafka parameters for same \" +\n+      s\"cache key should be equal. old parameters: $oldKafkaParams new parameters: $kafkaParams\")\n+  }\n+\n+  protected def createKey(connector: V): K\n+}\n+\n+private[kafka010] abstract class PoolConfig[V] extends GenericKeyedObjectPoolConfig[V] {\n+\n+  init()\n+\n+  def softMaxSize: Int\n+\n+  def jmxEnabled: Boolean\n+\n+  def minEvictableIdleTimeMillis: Long\n+\n+  def evictorThreadRunIntervalMillis: Long\n+\n+  def jmxNamePrefix: String\n+\n+  def init(): Unit = {\n+    // NOTE: Below lines define the behavior, so do not modify unless you know what you are\n+    // doing, and update the class doc accordingly if necessary when you modify.\n+\n+    // 1. Set min idle objects per key to 0 to avoid creating unnecessary object.\n+    // 2. Set max idle objects per key to 3 but set total objects per key to infinite\n+    // which ensures borrowing per key is not restricted.\n+    // 3. Set max total objects to infinite which ensures all objects are managed in this pool.\n+    setMinIdlePerKey(0)\n+    setMaxIdlePerKey(3)\n+    setMaxTotalPerKey(-1)\n+    setMaxTotal(-1)\n+\n+    // Set minimum evictable idle time which will be referred from evictor thread\n+    setMinEvictableIdleTimeMillis(minEvictableIdleTimeMillis)\n+    setSoftMinEvictableIdleTimeMillis(-1)\n+\n+    // evictor thread will run test with ten idle objects\n+    setTimeBetweenEvictionRunsMillis(evictorThreadRunIntervalMillis)\n+    setNumTestsPerEvictionRun(10)\n+    setEvictionPolicy(new DefaultEvictionPolicy[V]())\n+\n+    // Immediately fail on exhausted pool while borrowing\n+    setBlockWhenExhausted(false)\n+\n+    setJmxEnabled(jmxEnabled)\n+    setJmxNamePrefix(jmxNamePrefix)\n+  }\n+}\n+\n+private[kafka010] abstract class ObjectFactory[K, V] extends BaseKeyedPooledObjectFactory[K, V] {"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Good idea, checking...",
    "commit": "ebe9708c96e8127b9ef5c43fc26cff2811a79d82",
    "createdAt": "2019-09-24T07:54:08Z",
    "diffHunk": "@@ -0,0 +1,203 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.internal.Logging\n+\n+/**\n+ * Provides object pool for objects which is grouped by a key.\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"poolConfig.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe objects unless caller\n+ * shares the object to multiple threads.\n+ */\n+private[kafka010] abstract class InternalKafkaConnectorPool[K, V](\n+    objectFactory: ObjectFactory[K, V],\n+    poolConfig: PoolConfig[V],\n+    swallowedExceptionListener: SwallowedExceptionListener) extends Logging {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private val pool = {\n+    val internalPool = new GenericKeyedObjectPool[K, V](objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(swallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows object from the pool. If there's no idle object for the key,\n+   * the pool will create the object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: K, kafkaParams: ju.Map[String, Object]): V = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (size >= poolConfig.softMaxSize) {\n+      logWarning(\"Pool exceeds its soft max size, cleaning up idle objects...\")\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(connector: V): Unit = {\n+    pool.returnObject(createKey(connector), connector)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(connector: V): Unit = {\n+    pool.invalidateObject(createKey(connector), connector)\n+  }\n+\n+  /** Invalidates all idle values for the key */\n+  def invalidateKey(key: K): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def reset(): Unit = {\n+    // this is the best-effort of clearing up. otherwise we should close the pool and create again\n+    // but we don't want to make it \"var\" only because of tests.\n+    pool.clear()\n+  }\n+\n+  def numIdle: Int = pool.getNumIdle\n+\n+  def numIdle(key: K): Int = pool.getNumIdle(key)\n+\n+  def numActive: Int = pool.getNumActive\n+\n+  def numActive(key: K): Int = pool.getNumActive(key)\n+\n+  def size: Int = numIdle + numActive\n+\n+  def size(key: K): Int = numIdle(key) + numActive(key)\n+\n+  private def updateKafkaParamForKey(key: K, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    val oldKafkaParams = objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+    require(oldKafkaParams == null || kafkaParams == oldKafkaParams, \"Kafka parameters for same \" +\n+      s\"cache key should be equal. old parameters: $oldKafkaParams new parameters: $kafkaParams\")\n+  }\n+\n+  protected def createKey(connector: V): K\n+}\n+\n+private[kafka010] abstract class PoolConfig[V] extends GenericKeyedObjectPoolConfig[V] {\n+\n+  init()\n+\n+  def softMaxSize: Int\n+\n+  def jmxEnabled: Boolean\n+\n+  def minEvictableIdleTimeMillis: Long\n+\n+  def evictorThreadRunIntervalMillis: Long\n+\n+  def jmxNamePrefix: String\n+\n+  def init(): Unit = {\n+    // NOTE: Below lines define the behavior, so do not modify unless you know what you are\n+    // doing, and update the class doc accordingly if necessary when you modify.\n+\n+    // 1. Set min idle objects per key to 0 to avoid creating unnecessary object.\n+    // 2. Set max idle objects per key to 3 but set total objects per key to infinite\n+    // which ensures borrowing per key is not restricted.\n+    // 3. Set max total objects to infinite which ensures all objects are managed in this pool.\n+    setMinIdlePerKey(0)\n+    setMaxIdlePerKey(3)\n+    setMaxTotalPerKey(-1)\n+    setMaxTotal(-1)\n+\n+    // Set minimum evictable idle time which will be referred from evictor thread\n+    setMinEvictableIdleTimeMillis(minEvictableIdleTimeMillis)\n+    setSoftMinEvictableIdleTimeMillis(-1)\n+\n+    // evictor thread will run test with ten idle objects\n+    setTimeBetweenEvictionRunsMillis(evictorThreadRunIntervalMillis)\n+    setNumTestsPerEvictionRun(10)\n+    setEvictionPolicy(new DefaultEvictionPolicy[V]())\n+\n+    // Immediately fail on exhausted pool while borrowing\n+    setBlockWhenExhausted(false)\n+\n+    setJmxEnabled(jmxEnabled)\n+    setJmxNamePrefix(jmxNamePrefix)\n+  }\n+}\n+\n+private[kafka010] abstract class ObjectFactory[K, V] extends BaseKeyedPooledObjectFactory[K, V] {"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Changed.",
    "commit": "ebe9708c96e8127b9ef5c43fc26cff2811a79d82",
    "createdAt": "2019-09-24T11:03:41Z",
    "diffHunk": "@@ -0,0 +1,203 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.internal.Logging\n+\n+/**\n+ * Provides object pool for objects which is grouped by a key.\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"poolConfig.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe objects unless caller\n+ * shares the object to multiple threads.\n+ */\n+private[kafka010] abstract class InternalKafkaConnectorPool[K, V](\n+    objectFactory: ObjectFactory[K, V],\n+    poolConfig: PoolConfig[V],\n+    swallowedExceptionListener: SwallowedExceptionListener) extends Logging {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private val pool = {\n+    val internalPool = new GenericKeyedObjectPool[K, V](objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(swallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows object from the pool. If there's no idle object for the key,\n+   * the pool will create the object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: K, kafkaParams: ju.Map[String, Object]): V = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (size >= poolConfig.softMaxSize) {\n+      logWarning(\"Pool exceeds its soft max size, cleaning up idle objects...\")\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(connector: V): Unit = {\n+    pool.returnObject(createKey(connector), connector)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(connector: V): Unit = {\n+    pool.invalidateObject(createKey(connector), connector)\n+  }\n+\n+  /** Invalidates all idle values for the key */\n+  def invalidateKey(key: K): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def reset(): Unit = {\n+    // this is the best-effort of clearing up. otherwise we should close the pool and create again\n+    // but we don't want to make it \"var\" only because of tests.\n+    pool.clear()\n+  }\n+\n+  def numIdle: Int = pool.getNumIdle\n+\n+  def numIdle(key: K): Int = pool.getNumIdle(key)\n+\n+  def numActive: Int = pool.getNumActive\n+\n+  def numActive(key: K): Int = pool.getNumActive(key)\n+\n+  def size: Int = numIdle + numActive\n+\n+  def size(key: K): Int = numIdle(key) + numActive(key)\n+\n+  private def updateKafkaParamForKey(key: K, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    val oldKafkaParams = objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+    require(oldKafkaParams == null || kafkaParams == oldKafkaParams, \"Kafka parameters for same \" +\n+      s\"cache key should be equal. old parameters: $oldKafkaParams new parameters: $kafkaParams\")\n+  }\n+\n+  protected def createKey(connector: V): K\n+}\n+\n+private[kafka010] abstract class PoolConfig[V] extends GenericKeyedObjectPoolConfig[V] {\n+\n+  init()\n+\n+  def softMaxSize: Int\n+\n+  def jmxEnabled: Boolean\n+\n+  def minEvictableIdleTimeMillis: Long\n+\n+  def evictorThreadRunIntervalMillis: Long\n+\n+  def jmxNamePrefix: String\n+\n+  def init(): Unit = {\n+    // NOTE: Below lines define the behavior, so do not modify unless you know what you are\n+    // doing, and update the class doc accordingly if necessary when you modify.\n+\n+    // 1. Set min idle objects per key to 0 to avoid creating unnecessary object.\n+    // 2. Set max idle objects per key to 3 but set total objects per key to infinite\n+    // which ensures borrowing per key is not restricted.\n+    // 3. Set max total objects to infinite which ensures all objects are managed in this pool.\n+    setMinIdlePerKey(0)\n+    setMaxIdlePerKey(3)\n+    setMaxTotalPerKey(-1)\n+    setMaxTotal(-1)\n+\n+    // Set minimum evictable idle time which will be referred from evictor thread\n+    setMinEvictableIdleTimeMillis(minEvictableIdleTimeMillis)\n+    setSoftMinEvictableIdleTimeMillis(-1)\n+\n+    // evictor thread will run test with ten idle objects\n+    setTimeBetweenEvictionRunsMillis(evictorThreadRunIntervalMillis)\n+    setNumTestsPerEvictionRun(10)\n+    setEvictionPolicy(new DefaultEvictionPolicy[V]())\n+\n+    // Immediately fail on exhausted pool while borrowing\n+    setBlockWhenExhausted(false)\n+\n+    setJmxEnabled(jmxEnabled)\n+    setJmxNamePrefix(jmxNamePrefix)\n+  }\n+}\n+\n+private[kafka010] abstract class ObjectFactory[K, V] extends BaseKeyedPooledObjectFactory[K, V] {"
  }],
  "prId": 25853
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "For other reviewers: this is pretty same as previous InternalKafkaConsumerPool.ObjectFactory, except it brings some abstract methods to create/destroy objects.",
    "commit": "ebe9708c96e8127b9ef5c43fc26cff2811a79d82",
    "createdAt": "2019-09-21T03:18:32Z",
    "diffHunk": "@@ -0,0 +1,203 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.internal.Logging\n+\n+/**\n+ * Provides object pool for objects which is grouped by a key.\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"poolConfig.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe objects unless caller\n+ * shares the object to multiple threads.\n+ */\n+private[kafka010] abstract class InternalKafkaConnectorPool[K, V](\n+    objectFactory: ObjectFactory[K, V],\n+    poolConfig: PoolConfig[V],\n+    swallowedExceptionListener: SwallowedExceptionListener) extends Logging {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private val pool = {\n+    val internalPool = new GenericKeyedObjectPool[K, V](objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(swallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows object from the pool. If there's no idle object for the key,\n+   * the pool will create the object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: K, kafkaParams: ju.Map[String, Object]): V = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (size >= poolConfig.softMaxSize) {\n+      logWarning(\"Pool exceeds its soft max size, cleaning up idle objects...\")\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(connector: V): Unit = {\n+    pool.returnObject(createKey(connector), connector)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(connector: V): Unit = {\n+    pool.invalidateObject(createKey(connector), connector)\n+  }\n+\n+  /** Invalidates all idle values for the key */\n+  def invalidateKey(key: K): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def reset(): Unit = {\n+    // this is the best-effort of clearing up. otherwise we should close the pool and create again\n+    // but we don't want to make it \"var\" only because of tests.\n+    pool.clear()\n+  }\n+\n+  def numIdle: Int = pool.getNumIdle\n+\n+  def numIdle(key: K): Int = pool.getNumIdle(key)\n+\n+  def numActive: Int = pool.getNumActive\n+\n+  def numActive(key: K): Int = pool.getNumActive(key)\n+\n+  def size: Int = numIdle + numActive\n+\n+  def size(key: K): Int = numIdle(key) + numActive(key)\n+\n+  private def updateKafkaParamForKey(key: K, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    val oldKafkaParams = objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+    require(oldKafkaParams == null || kafkaParams == oldKafkaParams, \"Kafka parameters for same \" +\n+      s\"cache key should be equal. old parameters: $oldKafkaParams new parameters: $kafkaParams\")\n+  }\n+\n+  protected def createKey(connector: V): K\n+}\n+\n+private[kafka010] abstract class PoolConfig[V] extends GenericKeyedObjectPoolConfig[V] {\n+\n+  init()\n+\n+  def softMaxSize: Int\n+\n+  def jmxEnabled: Boolean\n+\n+  def minEvictableIdleTimeMillis: Long\n+\n+  def evictorThreadRunIntervalMillis: Long\n+\n+  def jmxNamePrefix: String\n+\n+  def init(): Unit = {\n+    // NOTE: Below lines define the behavior, so do not modify unless you know what you are\n+    // doing, and update the class doc accordingly if necessary when you modify.\n+\n+    // 1. Set min idle objects per key to 0 to avoid creating unnecessary object.\n+    // 2. Set max idle objects per key to 3 but set total objects per key to infinite\n+    // which ensures borrowing per key is not restricted.\n+    // 3. Set max total objects to infinite which ensures all objects are managed in this pool.\n+    setMinIdlePerKey(0)\n+    setMaxIdlePerKey(3)\n+    setMaxTotalPerKey(-1)\n+    setMaxTotal(-1)\n+\n+    // Set minimum evictable idle time which will be referred from evictor thread\n+    setMinEvictableIdleTimeMillis(minEvictableIdleTimeMillis)\n+    setSoftMinEvictableIdleTimeMillis(-1)\n+\n+    // evictor thread will run test with ten idle objects\n+    setTimeBetweenEvictionRunsMillis(evictorThreadRunIntervalMillis)\n+    setNumTestsPerEvictionRun(10)\n+    setEvictionPolicy(new DefaultEvictionPolicy[V]())\n+\n+    // Immediately fail on exhausted pool while borrowing\n+    setBlockWhenExhausted(false)\n+\n+    setJmxEnabled(jmxEnabled)\n+    setJmxNamePrefix(jmxNamePrefix)\n+  }\n+}\n+\n+private[kafka010] abstract class ObjectFactory[K, V] extends BaseKeyedPooledObjectFactory[K, V] {"
  }],
  "prId": 25853
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "I just noticed that there's no call site for this in your change; and also there's no code calling this in the repo, that I can find.\r\n\r\nIs something missing or can this method go away?\r\n\r\n(e.g. `KafkaDataWriter` calls `checkForErrors` which throws an exception if an error exists; that sounds like it should be calling this instead of just using the default `finally` block and calling `releaseProducer`?)",
    "commit": "ebe9708c96e8127b9ef5c43fc26cff2811a79d82",
    "createdAt": "2019-10-28T21:54:58Z",
    "diffHunk": "@@ -0,0 +1,209 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.io.Closeable\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.internal.Logging\n+\n+/**\n+ * Provides object pool for objects which is grouped by a key.\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"poolConfig.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe objects unless caller\n+ * shares the object to multiple threads.\n+ */\n+private[kafka010] abstract class InternalKafkaConnectorPool[K, V <: Closeable](\n+    objectFactory: ObjectFactory[K, V],\n+    poolConfig: PoolConfig[V],\n+    swallowedExceptionListener: SwallowedExceptionListener) extends Logging {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private val pool = {\n+    val internalPool = new GenericKeyedObjectPool[K, V](objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(swallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows object from the pool. If there's no idle object for the key,\n+   * the pool will create the object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: K, kafkaParams: ju.Map[String, Object]): V = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (size >= poolConfig.softMaxSize) {\n+      logWarning(\"Pool exceeds its soft max size, cleaning up idle objects...\")\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(connector: V): Unit = {\n+    pool.returnObject(createKey(connector), connector)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(connector: V): Unit = {\n+    pool.invalidateObject(createKey(connector), connector)\n+  }\n+\n+  /** Invalidates all idle values for the key */\n+  def invalidateKey(key: K): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def reset(): Unit = {\n+    // this is the best-effort of clearing up. otherwise we should close the pool and create again\n+    // but we don't want to make it \"var\" only because of tests.\n+    pool.clear()\n+  }\n+\n+  def numIdle: Int = pool.getNumIdle\n+\n+  def numIdle(key: K): Int = pool.getNumIdle(key)\n+\n+  def numActive: Int = pool.getNumActive\n+\n+  def numActive(key: K): Int = pool.getNumActive(key)\n+\n+  def size: Int = numIdle + numActive\n+\n+  def size(key: K): Int = numIdle(key) + numActive(key)\n+\n+  private def updateKafkaParamForKey(key: K, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    val oldKafkaParams = objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+    require(oldKafkaParams == null || kafkaParams == oldKafkaParams, \"Kafka parameters for same \" +\n+      s\"cache key should be equal. old parameters: $oldKafkaParams new parameters: $kafkaParams\")\n+  }\n+\n+  protected def createKey(connector: V): K\n+}\n+\n+private[kafka010] abstract class PoolConfig[V] extends GenericKeyedObjectPoolConfig[V] {\n+\n+  init()\n+\n+  def softMaxSize: Int\n+\n+  def jmxEnabled: Boolean\n+\n+  def minEvictableIdleTimeMillis: Long\n+\n+  def evictorThreadRunIntervalMillis: Long\n+\n+  def jmxNamePrefix: String\n+\n+  def init(): Unit = {\n+    // NOTE: Below lines define the behavior, so do not modify unless you know what you are\n+    // doing, and update the class doc accordingly if necessary when you modify.\n+\n+    // 1. Set min idle objects per key to 0 to avoid creating unnecessary object.\n+    // 2. Set max idle objects per key to 3 but set total objects per key to infinite\n+    // which ensures borrowing per key is not restricted.\n+    // 3. Set max total objects to infinite which ensures all objects are managed in this pool.\n+    setMinIdlePerKey(0)\n+    setMaxIdlePerKey(3)\n+    setMaxTotalPerKey(-1)\n+    setMaxTotal(-1)\n+\n+    // Set minimum evictable idle time which will be referred from evictor thread\n+    setMinEvictableIdleTimeMillis(minEvictableIdleTimeMillis)\n+    setSoftMinEvictableIdleTimeMillis(-1)\n+\n+    // evictor thread will run test with ten idle objects\n+    setTimeBetweenEvictionRunsMillis(evictorThreadRunIntervalMillis)\n+    setNumTestsPerEvictionRun(10)\n+    setEvictionPolicy(new DefaultEvictionPolicy[V]())\n+\n+    // Immediately fail on exhausted pool while borrowing\n+    setBlockWhenExhausted(false)\n+\n+    setJmxEnabled(jmxEnabled)\n+    setJmxNamePrefix(jmxNamePrefix)\n+  }\n+}\n+\n+private[kafka010] abstract class ObjectFactory[K, V <: Closeable]\n+  extends BaseKeyedPooledObjectFactory[K, V] {\n+  val keyToKafkaParams = new ConcurrentHashMap[K, ju.Map[String, Object]]()\n+\n+  override def create(key: K): V = {\n+    Option(keyToKafkaParams.get(key)) match {\n+      case Some(kafkaParams) => createValue(key, kafkaParams)\n+      case None => throw new IllegalStateException(\"Kafka params should be set before \" +\n+        \"borrowing object.\")\n+    }\n+  }\n+\n+  override def wrap(value: V): PooledObject[V] = {\n+    new DefaultPooledObject[V](value)\n+  }\n+\n+  override def destroyObject(key: K, p: PooledObject[V]): Unit = {",
    "line": 197
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "You've touched an important point here and I have a plan for this.\r\nIn the consumer area we've already done a similar solution what I plan to add here. Namely when a task realizes any exception it just returns the object into the pool (not returned object will stay in cache infinitely). In the next round when Spark realizes that it's a re-attempt it will invalidate the cache key and creates new instances. Please see the example [here](https://github.com/apache/spark/blob/44a27bdccdc39d5394ee95d935455eb7ff4b84c2/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaDataConsumer.scala#L628-L637). I've already opened SPARK-27042 to add this functionality but only after if this merged.\r\n\r\n`destroyObject` is needed when an item is not used till its timeout and the pool initiates the eviction.\r\n",
    "commit": "ebe9708c96e8127b9ef5c43fc26cff2811a79d82",
    "createdAt": "2019-10-30T14:01:31Z",
    "diffHunk": "@@ -0,0 +1,209 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.io.Closeable\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.internal.Logging\n+\n+/**\n+ * Provides object pool for objects which is grouped by a key.\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"poolConfig.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe objects unless caller\n+ * shares the object to multiple threads.\n+ */\n+private[kafka010] abstract class InternalKafkaConnectorPool[K, V <: Closeable](\n+    objectFactory: ObjectFactory[K, V],\n+    poolConfig: PoolConfig[V],\n+    swallowedExceptionListener: SwallowedExceptionListener) extends Logging {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private val pool = {\n+    val internalPool = new GenericKeyedObjectPool[K, V](objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(swallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows object from the pool. If there's no idle object for the key,\n+   * the pool will create the object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: K, kafkaParams: ju.Map[String, Object]): V = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (size >= poolConfig.softMaxSize) {\n+      logWarning(\"Pool exceeds its soft max size, cleaning up idle objects...\")\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(connector: V): Unit = {\n+    pool.returnObject(createKey(connector), connector)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(connector: V): Unit = {\n+    pool.invalidateObject(createKey(connector), connector)\n+  }\n+\n+  /** Invalidates all idle values for the key */\n+  def invalidateKey(key: K): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def reset(): Unit = {\n+    // this is the best-effort of clearing up. otherwise we should close the pool and create again\n+    // but we don't want to make it \"var\" only because of tests.\n+    pool.clear()\n+  }\n+\n+  def numIdle: Int = pool.getNumIdle\n+\n+  def numIdle(key: K): Int = pool.getNumIdle(key)\n+\n+  def numActive: Int = pool.getNumActive\n+\n+  def numActive(key: K): Int = pool.getNumActive(key)\n+\n+  def size: Int = numIdle + numActive\n+\n+  def size(key: K): Int = numIdle(key) + numActive(key)\n+\n+  private def updateKafkaParamForKey(key: K, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    val oldKafkaParams = objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+    require(oldKafkaParams == null || kafkaParams == oldKafkaParams, \"Kafka parameters for same \" +\n+      s\"cache key should be equal. old parameters: $oldKafkaParams new parameters: $kafkaParams\")\n+  }\n+\n+  protected def createKey(connector: V): K\n+}\n+\n+private[kafka010] abstract class PoolConfig[V] extends GenericKeyedObjectPoolConfig[V] {\n+\n+  init()\n+\n+  def softMaxSize: Int\n+\n+  def jmxEnabled: Boolean\n+\n+  def minEvictableIdleTimeMillis: Long\n+\n+  def evictorThreadRunIntervalMillis: Long\n+\n+  def jmxNamePrefix: String\n+\n+  def init(): Unit = {\n+    // NOTE: Below lines define the behavior, so do not modify unless you know what you are\n+    // doing, and update the class doc accordingly if necessary when you modify.\n+\n+    // 1. Set min idle objects per key to 0 to avoid creating unnecessary object.\n+    // 2. Set max idle objects per key to 3 but set total objects per key to infinite\n+    // which ensures borrowing per key is not restricted.\n+    // 3. Set max total objects to infinite which ensures all objects are managed in this pool.\n+    setMinIdlePerKey(0)\n+    setMaxIdlePerKey(3)\n+    setMaxTotalPerKey(-1)\n+    setMaxTotal(-1)\n+\n+    // Set minimum evictable idle time which will be referred from evictor thread\n+    setMinEvictableIdleTimeMillis(minEvictableIdleTimeMillis)\n+    setSoftMinEvictableIdleTimeMillis(-1)\n+\n+    // evictor thread will run test with ten idle objects\n+    setTimeBetweenEvictionRunsMillis(evictorThreadRunIntervalMillis)\n+    setNumTestsPerEvictionRun(10)\n+    setEvictionPolicy(new DefaultEvictionPolicy[V]())\n+\n+    // Immediately fail on exhausted pool while borrowing\n+    setBlockWhenExhausted(false)\n+\n+    setJmxEnabled(jmxEnabled)\n+    setJmxNamePrefix(jmxNamePrefix)\n+  }\n+}\n+\n+private[kafka010] abstract class ObjectFactory[K, V <: Closeable]\n+  extends BaseKeyedPooledObjectFactory[K, V] {\n+  val keyToKafkaParams = new ConcurrentHashMap[K, ju.Map[String, Object]]()\n+\n+  override def create(key: K): V = {\n+    Option(keyToKafkaParams.get(key)) match {\n+      case Some(kafkaParams) => createValue(key, kafkaParams)\n+      case None => throw new IllegalStateException(\"Kafka params should be set before \" +\n+        \"borrowing object.\")\n+    }\n+  }\n+\n+  override def wrap(value: V): PooledObject[V] = {\n+    new DefaultPooledObject[V](value)\n+  }\n+\n+  override def destroyObject(key: K, p: PooledObject[V]): Unit = {",
    "line": 197
  }],
  "prId": 25853
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Add an empty line to separate from class declaration.",
    "commit": "ebe9708c96e8127b9ef5c43fc26cff2811a79d82",
    "createdAt": "2019-10-28T21:55:16Z",
    "diffHunk": "@@ -0,0 +1,209 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.io.Closeable\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.internal.Logging\n+\n+/**\n+ * Provides object pool for objects which is grouped by a key.\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"poolConfig.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe objects unless caller\n+ * shares the object to multiple threads.\n+ */\n+private[kafka010] abstract class InternalKafkaConnectorPool[K, V <: Closeable](\n+    objectFactory: ObjectFactory[K, V],\n+    poolConfig: PoolConfig[V],\n+    swallowedExceptionListener: SwallowedExceptionListener) extends Logging {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private val pool = {\n+    val internalPool = new GenericKeyedObjectPool[K, V](objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(swallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows object from the pool. If there's no idle object for the key,\n+   * the pool will create the object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: K, kafkaParams: ju.Map[String, Object]): V = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (size >= poolConfig.softMaxSize) {\n+      logWarning(\"Pool exceeds its soft max size, cleaning up idle objects...\")\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(connector: V): Unit = {\n+    pool.returnObject(createKey(connector), connector)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(connector: V): Unit = {\n+    pool.invalidateObject(createKey(connector), connector)\n+  }\n+\n+  /** Invalidates all idle values for the key */\n+  def invalidateKey(key: K): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def reset(): Unit = {\n+    // this is the best-effort of clearing up. otherwise we should close the pool and create again\n+    // but we don't want to make it \"var\" only because of tests.\n+    pool.clear()\n+  }\n+\n+  def numIdle: Int = pool.getNumIdle\n+\n+  def numIdle(key: K): Int = pool.getNumIdle(key)\n+\n+  def numActive: Int = pool.getNumActive\n+\n+  def numActive(key: K): Int = pool.getNumActive(key)\n+\n+  def size: Int = numIdle + numActive\n+\n+  def size(key: K): Int = numIdle(key) + numActive(key)\n+\n+  private def updateKafkaParamForKey(key: K, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    val oldKafkaParams = objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+    require(oldKafkaParams == null || kafkaParams == oldKafkaParams, \"Kafka parameters for same \" +\n+      s\"cache key should be equal. old parameters: $oldKafkaParams new parameters: $kafkaParams\")\n+  }\n+\n+  protected def createKey(connector: V): K\n+}\n+\n+private[kafka010] abstract class PoolConfig[V] extends GenericKeyedObjectPoolConfig[V] {\n+\n+  init()\n+\n+  def softMaxSize: Int\n+\n+  def jmxEnabled: Boolean\n+\n+  def minEvictableIdleTimeMillis: Long\n+\n+  def evictorThreadRunIntervalMillis: Long\n+\n+  def jmxNamePrefix: String\n+\n+  def init(): Unit = {\n+    // NOTE: Below lines define the behavior, so do not modify unless you know what you are\n+    // doing, and update the class doc accordingly if necessary when you modify.\n+\n+    // 1. Set min idle objects per key to 0 to avoid creating unnecessary object.\n+    // 2. Set max idle objects per key to 3 but set total objects per key to infinite\n+    // which ensures borrowing per key is not restricted.\n+    // 3. Set max total objects to infinite which ensures all objects are managed in this pool.\n+    setMinIdlePerKey(0)\n+    setMaxIdlePerKey(3)\n+    setMaxTotalPerKey(-1)\n+    setMaxTotal(-1)\n+\n+    // Set minimum evictable idle time which will be referred from evictor thread\n+    setMinEvictableIdleTimeMillis(minEvictableIdleTimeMillis)\n+    setSoftMinEvictableIdleTimeMillis(-1)\n+\n+    // evictor thread will run test with ten idle objects\n+    setTimeBetweenEvictionRunsMillis(evictorThreadRunIntervalMillis)\n+    setNumTestsPerEvictionRun(10)\n+    setEvictionPolicy(new DefaultEvictionPolicy[V]())\n+\n+    // Immediately fail on exhausted pool while borrowing\n+    setBlockWhenExhausted(false)\n+\n+    setJmxEnabled(jmxEnabled)\n+    setJmxNamePrefix(jmxNamePrefix)\n+  }\n+}\n+\n+private[kafka010] abstract class ObjectFactory[K, V <: Closeable]\n+  extends BaseKeyedPooledObjectFactory[K, V] {\n+  val keyToKafkaParams = new ConcurrentHashMap[K, ju.Map[String, Object]]()\n+\n+  override def create(key: K): V = {\n+    Option(keyToKafkaParams.get(key)) match {\n+      case Some(kafkaParams) => createValue(key, kafkaParams)\n+      case None => throw new IllegalStateException(\"Kafka params should be set before \" +\n+        \"borrowing object.\")\n+    }\n+  }\n+\n+  override def wrap(value: V): PooledObject[V] = {\n+    new DefaultPooledObject[V](value)\n+  }\n+\n+  override def destroyObject(key: K, p: PooledObject[V]): Unit = {\n+    p.getObject.close()\n+  }\n+\n+  protected def createValue(key: K, kafkaParams: ju.Map[String, Object]): V\n+}\n+\n+private[kafka010] class CustomSwallowedExceptionListener(connectorType: String)\n+  extends SwallowedExceptionListener with Logging {\n+  override def onSwallowException(e: Exception): Unit = {",
    "line": 207
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Added.",
    "commit": "ebe9708c96e8127b9ef5c43fc26cff2811a79d82",
    "createdAt": "2019-10-30T14:02:07Z",
    "diffHunk": "@@ -0,0 +1,209 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.io.Closeable\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.internal.Logging\n+\n+/**\n+ * Provides object pool for objects which is grouped by a key.\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"poolConfig.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe objects unless caller\n+ * shares the object to multiple threads.\n+ */\n+private[kafka010] abstract class InternalKafkaConnectorPool[K, V <: Closeable](\n+    objectFactory: ObjectFactory[K, V],\n+    poolConfig: PoolConfig[V],\n+    swallowedExceptionListener: SwallowedExceptionListener) extends Logging {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private val pool = {\n+    val internalPool = new GenericKeyedObjectPool[K, V](objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(swallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows object from the pool. If there's no idle object for the key,\n+   * the pool will create the object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: K, kafkaParams: ju.Map[String, Object]): V = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (size >= poolConfig.softMaxSize) {\n+      logWarning(\"Pool exceeds its soft max size, cleaning up idle objects...\")\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(connector: V): Unit = {\n+    pool.returnObject(createKey(connector), connector)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(connector: V): Unit = {\n+    pool.invalidateObject(createKey(connector), connector)\n+  }\n+\n+  /** Invalidates all idle values for the key */\n+  def invalidateKey(key: K): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def reset(): Unit = {\n+    // this is the best-effort of clearing up. otherwise we should close the pool and create again\n+    // but we don't want to make it \"var\" only because of tests.\n+    pool.clear()\n+  }\n+\n+  def numIdle: Int = pool.getNumIdle\n+\n+  def numIdle(key: K): Int = pool.getNumIdle(key)\n+\n+  def numActive: Int = pool.getNumActive\n+\n+  def numActive(key: K): Int = pool.getNumActive(key)\n+\n+  def size: Int = numIdle + numActive\n+\n+  def size(key: K): Int = numIdle(key) + numActive(key)\n+\n+  private def updateKafkaParamForKey(key: K, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    val oldKafkaParams = objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+    require(oldKafkaParams == null || kafkaParams == oldKafkaParams, \"Kafka parameters for same \" +\n+      s\"cache key should be equal. old parameters: $oldKafkaParams new parameters: $kafkaParams\")\n+  }\n+\n+  protected def createKey(connector: V): K\n+}\n+\n+private[kafka010] abstract class PoolConfig[V] extends GenericKeyedObjectPoolConfig[V] {\n+\n+  init()\n+\n+  def softMaxSize: Int\n+\n+  def jmxEnabled: Boolean\n+\n+  def minEvictableIdleTimeMillis: Long\n+\n+  def evictorThreadRunIntervalMillis: Long\n+\n+  def jmxNamePrefix: String\n+\n+  def init(): Unit = {\n+    // NOTE: Below lines define the behavior, so do not modify unless you know what you are\n+    // doing, and update the class doc accordingly if necessary when you modify.\n+\n+    // 1. Set min idle objects per key to 0 to avoid creating unnecessary object.\n+    // 2. Set max idle objects per key to 3 but set total objects per key to infinite\n+    // which ensures borrowing per key is not restricted.\n+    // 3. Set max total objects to infinite which ensures all objects are managed in this pool.\n+    setMinIdlePerKey(0)\n+    setMaxIdlePerKey(3)\n+    setMaxTotalPerKey(-1)\n+    setMaxTotal(-1)\n+\n+    // Set minimum evictable idle time which will be referred from evictor thread\n+    setMinEvictableIdleTimeMillis(minEvictableIdleTimeMillis)\n+    setSoftMinEvictableIdleTimeMillis(-1)\n+\n+    // evictor thread will run test with ten idle objects\n+    setTimeBetweenEvictionRunsMillis(evictorThreadRunIntervalMillis)\n+    setNumTestsPerEvictionRun(10)\n+    setEvictionPolicy(new DefaultEvictionPolicy[V]())\n+\n+    // Immediately fail on exhausted pool while borrowing\n+    setBlockWhenExhausted(false)\n+\n+    setJmxEnabled(jmxEnabled)\n+    setJmxNamePrefix(jmxNamePrefix)\n+  }\n+}\n+\n+private[kafka010] abstract class ObjectFactory[K, V <: Closeable]\n+  extends BaseKeyedPooledObjectFactory[K, V] {\n+  val keyToKafkaParams = new ConcurrentHashMap[K, ju.Map[String, Object]]()\n+\n+  override def create(key: K): V = {\n+    Option(keyToKafkaParams.get(key)) match {\n+      case Some(kafkaParams) => createValue(key, kafkaParams)\n+      case None => throw new IllegalStateException(\"Kafka params should be set before \" +\n+        \"borrowing object.\")\n+    }\n+  }\n+\n+  override def wrap(value: V): PooledObject[V] = {\n+    new DefaultPooledObject[V](value)\n+  }\n+\n+  override def destroyObject(key: K, p: PooledObject[V]): Unit = {\n+    p.getObject.close()\n+  }\n+\n+  protected def createValue(key: K, kafkaParams: ju.Map[String, Object]): V\n+}\n+\n+private[kafka010] class CustomSwallowedExceptionListener(connectorType: String)\n+  extends SwallowedExceptionListener with Logging {\n+  override def onSwallowException(e: Exception): Unit = {",
    "line": 207
  }],
  "prId": 25853
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "\"logError\" when closing something sounds a little excessive.",
    "commit": "ebe9708c96e8127b9ef5c43fc26cff2811a79d82",
    "createdAt": "2019-11-04T23:48:17Z",
    "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.io.Closeable\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.internal.Logging\n+\n+/**\n+ * Provides object pool for objects which is grouped by a key.\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"poolConfig.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe objects unless caller\n+ * shares the object to multiple threads.\n+ */\n+private[kafka010] abstract class InternalKafkaConnectorPool[K, V <: Closeable](\n+    objectFactory: ObjectFactory[K, V],\n+    poolConfig: PoolConfig[V],\n+    swallowedExceptionListener: SwallowedExceptionListener) extends Logging {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private val pool = {\n+    val internalPool = new GenericKeyedObjectPool[K, V](objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(swallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows object from the pool. If there's no idle object for the key,\n+   * the pool will create the object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: K, kafkaParams: ju.Map[String, Object]): V = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (size >= poolConfig.softMaxSize) {\n+      logWarning(\"Pool exceeds its soft max size, cleaning up idle objects...\")\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(connector: V): Unit = {\n+    pool.returnObject(createKey(connector), connector)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(connector: V): Unit = {\n+    pool.invalidateObject(createKey(connector), connector)\n+  }\n+\n+  /** Invalidates all idle values for the key */\n+  def invalidateKey(key: K): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def reset(): Unit = {\n+    // this is the best-effort of clearing up. otherwise we should close the pool and create again\n+    // but we don't want to make it \"var\" only because of tests.\n+    pool.clear()\n+  }\n+\n+  def numIdle: Int = pool.getNumIdle\n+\n+  def numIdle(key: K): Int = pool.getNumIdle(key)\n+\n+  def numActive: Int = pool.getNumActive\n+\n+  def numActive(key: K): Int = pool.getNumActive(key)\n+\n+  def size: Int = numIdle + numActive\n+\n+  def size(key: K): Int = numIdle(key) + numActive(key)\n+\n+  private def updateKafkaParamForKey(key: K, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    val oldKafkaParams = objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+    require(oldKafkaParams == null || kafkaParams == oldKafkaParams, \"Kafka parameters for same \" +\n+      s\"cache key should be equal. old parameters: $oldKafkaParams new parameters: $kafkaParams\")\n+  }\n+\n+  protected def createKey(connector: V): K\n+}\n+\n+private[kafka010] abstract class PoolConfig[V] extends GenericKeyedObjectPoolConfig[V] {\n+\n+  init()\n+\n+  def softMaxSize: Int\n+\n+  def jmxEnabled: Boolean\n+\n+  def minEvictableIdleTimeMillis: Long\n+\n+  def evictorThreadRunIntervalMillis: Long\n+\n+  def jmxNamePrefix: String\n+\n+  def init(): Unit = {\n+    // NOTE: Below lines define the behavior, so do not modify unless you know what you are\n+    // doing, and update the class doc accordingly if necessary when you modify.\n+\n+    // 1. Set min idle objects per key to 0 to avoid creating unnecessary object.\n+    // 2. Set max idle objects per key to 3 but set total objects per key to infinite\n+    // which ensures borrowing per key is not restricted.\n+    // 3. Set max total objects to infinite which ensures all objects are managed in this pool.\n+    setMinIdlePerKey(0)\n+    setMaxIdlePerKey(3)\n+    setMaxTotalPerKey(-1)\n+    setMaxTotal(-1)\n+\n+    // Set minimum evictable idle time which will be referred from evictor thread\n+    setMinEvictableIdleTimeMillis(minEvictableIdleTimeMillis)\n+    setSoftMinEvictableIdleTimeMillis(-1)\n+\n+    // evictor thread will run test with ten idle objects\n+    setTimeBetweenEvictionRunsMillis(evictorThreadRunIntervalMillis)\n+    setNumTestsPerEvictionRun(10)\n+    setEvictionPolicy(new DefaultEvictionPolicy[V]())\n+\n+    // Immediately fail on exhausted pool while borrowing\n+    setBlockWhenExhausted(false)\n+\n+    setJmxEnabled(jmxEnabled)\n+    setJmxNamePrefix(jmxNamePrefix)\n+  }\n+}\n+\n+private[kafka010] abstract class ObjectFactory[K, V <: Closeable]\n+  extends BaseKeyedPooledObjectFactory[K, V] {\n+  val keyToKafkaParams = new ConcurrentHashMap[K, ju.Map[String, Object]]()\n+\n+  override def create(key: K): V = {\n+    Option(keyToKafkaParams.get(key)) match {\n+      case Some(kafkaParams) => createValue(key, kafkaParams)\n+      case None => throw new IllegalStateException(\"Kafka params should be set before \" +\n+        \"borrowing object.\")\n+    }\n+  }\n+\n+  override def wrap(value: V): PooledObject[V] = {\n+    new DefaultPooledObject[V](value)\n+  }\n+\n+  override def destroyObject(key: K, p: PooledObject[V]): Unit = {\n+    p.getObject.close()\n+  }\n+\n+  protected def createValue(key: K, kafkaParams: ju.Map[String, Object]): V\n+}\n+\n+private[kafka010] class CustomSwallowedExceptionListener(connectorType: String)\n+  extends SwallowedExceptionListener with Logging {\n+\n+  override def onSwallowException(e: Exception): Unit = {\n+    logError(s\"Error closing Kafka $connectorType\", e)"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Since it's a recoverable problem I agree, fixed.",
    "commit": "ebe9708c96e8127b9ef5c43fc26cff2811a79d82",
    "createdAt": "2019-11-05T09:30:09Z",
    "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.io.Closeable\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.commons.pool2.{BaseKeyedPooledObjectFactory, PooledObject, SwallowedExceptionListener}\n+import org.apache.commons.pool2.impl.{DefaultEvictionPolicy, DefaultPooledObject, GenericKeyedObjectPool, GenericKeyedObjectPoolConfig}\n+\n+import org.apache.spark.internal.Logging\n+\n+/**\n+ * Provides object pool for objects which is grouped by a key.\n+ *\n+ * This class leverages [[GenericKeyedObjectPool]] internally, hence providing methods based on\n+ * the class, and same contract applies: after using the borrowed object, you must either call\n+ * returnObject() if the object is healthy to return to pool, or invalidateObject() if the object\n+ * should be destroyed.\n+ *\n+ * The soft capacity of pool is determined by \"poolConfig.capacity\" config value,\n+ * and the pool will have reasonable default value if the value is not provided.\n+ * (The instance will do its best effort to respect soft capacity but it can exceed when there's\n+ * a borrowing request and there's neither free space nor idle object to clear.)\n+ *\n+ * This class guarantees that no caller will get pooled object once the object is borrowed and\n+ * not yet returned, hence provide thread-safety usage of non-thread-safe objects unless caller\n+ * shares the object to multiple threads.\n+ */\n+private[kafka010] abstract class InternalKafkaConnectorPool[K, V <: Closeable](\n+    objectFactory: ObjectFactory[K, V],\n+    poolConfig: PoolConfig[V],\n+    swallowedExceptionListener: SwallowedExceptionListener) extends Logging {\n+\n+  // the class is intended to have only soft capacity\n+  assert(poolConfig.getMaxTotal < 0)\n+\n+  private val pool = {\n+    val internalPool = new GenericKeyedObjectPool[K, V](objectFactory, poolConfig)\n+    internalPool.setSwallowedExceptionListener(swallowedExceptionListener)\n+    internalPool\n+  }\n+\n+  /**\n+   * Borrows object from the pool. If there's no idle object for the key,\n+   * the pool will create the object.\n+   *\n+   * If the pool doesn't have idle object for the key and also exceeds the soft capacity,\n+   * pool will try to clear some of idle objects.\n+   *\n+   * Borrowed object must be returned by either calling returnObject or invalidateObject, otherwise\n+   * the object will be kept in pool as active object.\n+   */\n+  def borrowObject(key: K, kafkaParams: ju.Map[String, Object]): V = {\n+    updateKafkaParamForKey(key, kafkaParams)\n+\n+    if (size >= poolConfig.softMaxSize) {\n+      logWarning(\"Pool exceeds its soft max size, cleaning up idle objects...\")\n+      pool.clearOldest()\n+    }\n+\n+    pool.borrowObject(key)\n+  }\n+\n+  /** Returns borrowed object to the pool. */\n+  def returnObject(connector: V): Unit = {\n+    pool.returnObject(createKey(connector), connector)\n+  }\n+\n+  /** Invalidates (destroy) borrowed object to the pool. */\n+  def invalidateObject(connector: V): Unit = {\n+    pool.invalidateObject(createKey(connector), connector)\n+  }\n+\n+  /** Invalidates all idle values for the key */\n+  def invalidateKey(key: K): Unit = {\n+    pool.clear(key)\n+  }\n+\n+  /**\n+   * Closes the keyed object pool. Once the pool is closed,\n+   * borrowObject will fail with [[IllegalStateException]], but returnObject and invalidateObject\n+   * will continue to work, with returned objects destroyed on return.\n+   *\n+   * Also destroys idle instances in the pool.\n+   */\n+  def close(): Unit = {\n+    pool.close()\n+  }\n+\n+  def reset(): Unit = {\n+    // this is the best-effort of clearing up. otherwise we should close the pool and create again\n+    // but we don't want to make it \"var\" only because of tests.\n+    pool.clear()\n+  }\n+\n+  def numIdle: Int = pool.getNumIdle\n+\n+  def numIdle(key: K): Int = pool.getNumIdle(key)\n+\n+  def numActive: Int = pool.getNumActive\n+\n+  def numActive(key: K): Int = pool.getNumActive(key)\n+\n+  def size: Int = numIdle + numActive\n+\n+  def size(key: K): Int = numIdle(key) + numActive(key)\n+\n+  private def updateKafkaParamForKey(key: K, kafkaParams: ju.Map[String, Object]): Unit = {\n+    // We can assume that kafkaParam should not be different for same cache key,\n+    // otherwise we can't reuse the cached object and cache key should contain kafkaParam.\n+    // So it should be safe to put the key/value pair only when the key doesn't exist.\n+    val oldKafkaParams = objectFactory.keyToKafkaParams.putIfAbsent(key, kafkaParams)\n+    require(oldKafkaParams == null || kafkaParams == oldKafkaParams, \"Kafka parameters for same \" +\n+      s\"cache key should be equal. old parameters: $oldKafkaParams new parameters: $kafkaParams\")\n+  }\n+\n+  protected def createKey(connector: V): K\n+}\n+\n+private[kafka010] abstract class PoolConfig[V] extends GenericKeyedObjectPoolConfig[V] {\n+\n+  init()\n+\n+  def softMaxSize: Int\n+\n+  def jmxEnabled: Boolean\n+\n+  def minEvictableIdleTimeMillis: Long\n+\n+  def evictorThreadRunIntervalMillis: Long\n+\n+  def jmxNamePrefix: String\n+\n+  def init(): Unit = {\n+    // NOTE: Below lines define the behavior, so do not modify unless you know what you are\n+    // doing, and update the class doc accordingly if necessary when you modify.\n+\n+    // 1. Set min idle objects per key to 0 to avoid creating unnecessary object.\n+    // 2. Set max idle objects per key to 3 but set total objects per key to infinite\n+    // which ensures borrowing per key is not restricted.\n+    // 3. Set max total objects to infinite which ensures all objects are managed in this pool.\n+    setMinIdlePerKey(0)\n+    setMaxIdlePerKey(3)\n+    setMaxTotalPerKey(-1)\n+    setMaxTotal(-1)\n+\n+    // Set minimum evictable idle time which will be referred from evictor thread\n+    setMinEvictableIdleTimeMillis(minEvictableIdleTimeMillis)\n+    setSoftMinEvictableIdleTimeMillis(-1)\n+\n+    // evictor thread will run test with ten idle objects\n+    setTimeBetweenEvictionRunsMillis(evictorThreadRunIntervalMillis)\n+    setNumTestsPerEvictionRun(10)\n+    setEvictionPolicy(new DefaultEvictionPolicy[V]())\n+\n+    // Immediately fail on exhausted pool while borrowing\n+    setBlockWhenExhausted(false)\n+\n+    setJmxEnabled(jmxEnabled)\n+    setJmxNamePrefix(jmxNamePrefix)\n+  }\n+}\n+\n+private[kafka010] abstract class ObjectFactory[K, V <: Closeable]\n+  extends BaseKeyedPooledObjectFactory[K, V] {\n+  val keyToKafkaParams = new ConcurrentHashMap[K, ju.Map[String, Object]]()\n+\n+  override def create(key: K): V = {\n+    Option(keyToKafkaParams.get(key)) match {\n+      case Some(kafkaParams) => createValue(key, kafkaParams)\n+      case None => throw new IllegalStateException(\"Kafka params should be set before \" +\n+        \"borrowing object.\")\n+    }\n+  }\n+\n+  override def wrap(value: V): PooledObject[V] = {\n+    new DefaultPooledObject[V](value)\n+  }\n+\n+  override def destroyObject(key: K, p: PooledObject[V]): Unit = {\n+    p.getObject.close()\n+  }\n+\n+  protected def createValue(key: K, kafkaParams: ju.Map[String, Object]): V\n+}\n+\n+private[kafka010] class CustomSwallowedExceptionListener(connectorType: String)\n+  extends SwallowedExceptionListener with Logging {\n+\n+  override def onSwallowException(e: Exception): Unit = {\n+    logError(s\"Error closing Kafka $connectorType\", e)"
  }],
  "prId": 25853
}]