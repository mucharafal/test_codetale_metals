[{
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "maximum number?",
    "commit": "38bc96dcdbba7961ac68f09cbdb74dd719568f79",
    "createdAt": "2019-05-13T14:33:06Z",
    "diffHunk": "@@ -33,8 +33,8 @@ package object kafka010 {   // scalastyle:ignore\n       .createWithDefaultString(\"10m\")\n \n   private[kafka010] val CONSUMER_CACHE_CAPACITY =\n-    ConfigBuilder(\"spark.sql.kafkaConsumerCache.capacity\")\n-      .doc(\"The size of consumers cached.\")\n+    ConfigBuilder(\"spark.kafka.consumer.cache.capacity\")\n+      .doc(\"The number of consumers cached.\")"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Fixed and additionally added a note about the parameter soft limit nature.",
    "commit": "38bc96dcdbba7961ac68f09cbdb74dd719568f79",
    "createdAt": "2019-05-14T06:21:04Z",
    "diffHunk": "@@ -33,8 +33,8 @@ package object kafka010 {   // scalastyle:ignore\n       .createWithDefaultString(\"10m\")\n \n   private[kafka010] val CONSUMER_CACHE_CAPACITY =\n-    ConfigBuilder(\"spark.sql.kafkaConsumerCache.capacity\")\n-      .doc(\"The size of consumers cached.\")\n+    ConfigBuilder(\"spark.kafka.consumer.cache.capacity\")\n+      .doc(\"The number of consumers cached.\")"
  }],
  "prId": 24590
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Thank you for new doc, @gaborgsomogyi .\r\nSince this is used since Spark 2.0.x, shall we add a little comment on the migration doc?",
    "commit": "38bc96dcdbba7961ac68f09cbdb74dd719568f79",
    "createdAt": "2019-05-13T16:24:36Z",
    "diffHunk": "@@ -33,8 +33,8 @@ package object kafka010 {   // scalastyle:ignore\n       .createWithDefaultString(\"10m\")\n \n   private[kafka010] val CONSUMER_CACHE_CAPACITY =",
    "line": 3
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Originally I've taken a look at the related migration guide (sql-migration-guide-upgrade) but there I haven't seen any parameter rename entry. Thought it's the way because of the following:\r\n* The parameter just deprecated and will work properly in 3.0 as well\r\n* The Spark config system will give a warning message which tells what is the new parameter\r\n\r\nIf my thoughts are not correct please correct it and I'll comment on the migration doc.\r\n",
    "commit": "38bc96dcdbba7961ac68f09cbdb74dd719568f79",
    "createdAt": "2019-05-14T06:26:16Z",
    "diffHunk": "@@ -33,8 +33,8 @@ package object kafka010 {   // scalastyle:ignore\n       .createWithDefaultString(\"10m\")\n \n   private[kafka010] val CONSUMER_CACHE_CAPACITY =",
    "line": 3
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "Yeah this one doesn't require migration per se. We'd definitely mention it if the old config went away. Could mention it now too. My only hesitation is overloading the already huge 3.0 migration guide with something that doesn't actually require action now and will be flagged to the user.",
    "commit": "38bc96dcdbba7961ac68f09cbdb74dd719568f79",
    "createdAt": "2019-05-14T13:48:15Z",
    "diffHunk": "@@ -33,8 +33,8 @@ package object kafka010 {   // scalastyle:ignore\n       .createWithDefaultString(\"10m\")\n \n   private[kafka010] val CONSUMER_CACHE_CAPACITY =",
    "line": 3
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Same understanding.",
    "commit": "38bc96dcdbba7961ac68f09cbdb74dd719568f79",
    "createdAt": "2019-05-14T15:17:10Z",
    "diffHunk": "@@ -33,8 +33,8 @@ package object kafka010 {   // scalastyle:ignore\n       .createWithDefaultString(\"10m\")\n \n   private[kafka010] val CONSUMER_CACHE_CAPACITY =",
    "line": 3
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Got it. Thanks!",
    "commit": "38bc96dcdbba7961ac68f09cbdb74dd719568f79",
    "createdAt": "2019-05-15T17:37:58Z",
    "diffHunk": "@@ -33,8 +33,8 @@ package object kafka010 {   // scalastyle:ignore\n       .createWithDefaultString(\"10m\")\n \n   private[kafka010] val CONSUMER_CACHE_CAPACITY =",
    "line": 3
  }],
  "prId": 24590
}]