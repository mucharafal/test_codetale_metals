[{
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "I think we might need to retain the previous constructor's signature for compatibility?\r\nBut could this just be an option in sourceOptions? that sort of seems to be what it's for. I haven't checked, but, it might avoid plumbing through this boolean all over the place.",
    "commit": "de02de411aa00cbacc94be5b746dc48be0fe77a3",
    "createdAt": "2019-06-18T18:55:03Z",
    "diffHunk": "@@ -38,19 +38,21 @@ import org.apache.spark.sql.sources.v2.reader.streaming._\n  *                      read by per-task consumers generated later.\n  * @param kafkaParams   String params for per-task Kafka consumers.\n  * @param sourceOptions Params which are not Kafka consumer params.\n- * @param metadataPath Path to a directory this reader can use for writing metadata.\n+ * @param metadataPath  Path to a directory this reader can use for writing metadata.\n  * @param initialOffsets The Kafka offsets to start reading data at.\n  * @param failOnDataLoss Flag indicating whether reading should fail in data loss\n  *                       scenarios, where some offsets after the specified initial ones can't be\n  *                       properly read.\n+ * @param includeHeaders Flag indicating whether to include Kafka records' headers.\n  */\n class KafkaContinuousStream(\n     offsetReader: KafkaOffsetReader,\n     kafkaParams: ju.Map[String, Object],\n     sourceOptions: Map[String, String],\n     metadataPath: String,\n     initialOffsets: KafkaOffsetRangeLimit,\n-    failOnDataLoss: Boolean)\n+    failOnDataLoss: Boolean,\n+    includeHeaders: Boolean)"
  }, {
    "author": {
      "login": "dongjinleekr"
    },
    "body": "You are right, `includeHeaders` can be retrieved from `sourceOptions` like `pollTimeoutMs` does. It also applies to the case of `KafkaSource` and `KafkaMicroBatchStream`. So, reverted into the previous constructors.",
    "commit": "de02de411aa00cbacc94be5b746dc48be0fe77a3",
    "createdAt": "2019-06-19T00:14:13Z",
    "diffHunk": "@@ -38,19 +38,21 @@ import org.apache.spark.sql.sources.v2.reader.streaming._\n  *                      read by per-task consumers generated later.\n  * @param kafkaParams   String params for per-task Kafka consumers.\n  * @param sourceOptions Params which are not Kafka consumer params.\n- * @param metadataPath Path to a directory this reader can use for writing metadata.\n+ * @param metadataPath  Path to a directory this reader can use for writing metadata.\n  * @param initialOffsets The Kafka offsets to start reading data at.\n  * @param failOnDataLoss Flag indicating whether reading should fail in data loss\n  *                       scenarios, where some offsets after the specified initial ones can't be\n  *                       properly read.\n+ * @param includeHeaders Flag indicating whether to include Kafka records' headers.\n  */\n class KafkaContinuousStream(\n     offsetReader: KafkaOffsetReader,\n     kafkaParams: ju.Map[String, Object],\n     sourceOptions: Map[String, String],\n     metadataPath: String,\n     initialOffsets: KafkaOffsetRangeLimit,\n-    failOnDataLoss: Boolean)\n+    failOnDataLoss: Boolean,\n+    includeHeaders: Boolean)"
  }],
  "prId": 22282
}, {
  "comments": [{
    "author": {
      "login": "dongjinleekr"
    },
    "body": "@HeartSaVioR I found why [the previous tests were failed](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/106646/testReport/), but the reason is a little bit wierd. It seems like the keys of `sourceOptions` are all lowercased, so the `includeHeaders` option is not detected without lowercased",
    "commit": "de02de411aa00cbacc94be5b746dc48be0fe77a3",
    "createdAt": "2019-06-19T17:51:50Z",
    "diffHunk": "@@ -54,7 +55,8 @@ class KafkaContinuousStream(\n   extends ContinuousStream with Logging {\n \n   private val pollTimeoutMs = sourceOptions.getOrElse(CONSUMER_POLL_TIMEOUT, \"512\").toLong\n-  private val includeHeaders = sourceOptions.getOrElse(INCLUDE_HEADERS, \"false\").toBoolean\n+  private val includeHeaders ="
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "@dongjinleekr \r\nYeah I also found missing spot on current code, raised #24942 . It would be nice to rebase and apply here as well if #24942 got merged earlier than this.",
    "commit": "de02de411aa00cbacc94be5b746dc48be0fe77a3",
    "createdAt": "2019-06-24T00:32:26Z",
    "diffHunk": "@@ -54,7 +55,8 @@ class KafkaContinuousStream(\n   extends ContinuousStream with Logging {\n \n   private val pollTimeoutMs = sourceOptions.getOrElse(CONSUMER_POLL_TIMEOUT, \"512\").toLong\n-  private val includeHeaders = sourceOptions.getOrElse(INCLUDE_HEADERS, \"false\").toBoolean\n+  private val includeHeaders ="
  }],
  "prId": 22282
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "I'd have `KafkaRecordToRowProjector` (either class or object, but object would be fine) instead and move every projectors newly added in KafkaOffsetReader to there.",
    "commit": "de02de411aa00cbacc94be5b746dc48be0fe77a3",
    "createdAt": "2019-07-25T00:27:17Z",
    "diffHunk": "@@ -184,9 +188,10 @@ class KafkaContinuousPartitionReader(\n     startOffset: Long,\n     kafkaParams: ju.Map[String, Object],\n     pollTimeoutMs: Long,\n-    failOnDataLoss: Boolean) extends ContinuousPartitionReader[InternalRow] {\n+    failOnDataLoss: Boolean,\n+    includeHeaders: Boolean) extends ContinuousPartitionReader[InternalRow] {\n   private val consumer = KafkaDataConsumer.acquire(topicPartition, kafkaParams, useCache = false)\n-  private val converter = new KafkaRecordToUnsafeRowConverter",
    "line": 72
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "So +1 on your proposal. The proposed name is just 2 cents, and I'm not sure which name fits best. ",
    "commit": "de02de411aa00cbacc94be5b746dc48be0fe77a3",
    "createdAt": "2019-07-25T00:46:27Z",
    "diffHunk": "@@ -184,9 +188,10 @@ class KafkaContinuousPartitionReader(\n     startOffset: Long,\n     kafkaParams: ju.Map[String, Object],\n     pollTimeoutMs: Long,\n-    failOnDataLoss: Boolean) extends ContinuousPartitionReader[InternalRow] {\n+    failOnDataLoss: Boolean,\n+    includeHeaders: Boolean) extends ContinuousPartitionReader[InternalRow] {\n   private val consumer = KafkaDataConsumer.acquire(topicPartition, kafkaParams, useCache = false)\n-  private val converter = new KafkaRecordToUnsafeRowConverter",
    "line": 72
  }, {
    "author": {
      "login": "dongjinleekr"
    },
    "body": "@HeartSaVioR Great. Let's continue on discussing which name would be the best.",
    "commit": "de02de411aa00cbacc94be5b746dc48be0fe77a3",
    "createdAt": "2019-07-25T01:40:48Z",
    "diffHunk": "@@ -184,9 +188,10 @@ class KafkaContinuousPartitionReader(\n     startOffset: Long,\n     kafkaParams: ju.Map[String, Object],\n     pollTimeoutMs: Long,\n-    failOnDataLoss: Boolean) extends ContinuousPartitionReader[InternalRow] {\n+    failOnDataLoss: Boolean,\n+    includeHeaders: Boolean) extends ContinuousPartitionReader[InternalRow] {\n   private val consumer = KafkaDataConsumer.acquire(topicPartition, kafkaParams, useCache = false)\n-  private val converter = new KafkaRecordToUnsafeRowConverter",
    "line": 72
  }],
  "prId": 22282
}]