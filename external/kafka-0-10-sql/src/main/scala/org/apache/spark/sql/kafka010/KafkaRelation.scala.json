[{
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Same here.",
    "commit": "de02de411aa00cbacc94be5b746dc48be0fe77a3",
    "createdAt": "2018-09-03T09:12:52Z",
    "diffHunk": "@@ -115,7 +116,12 @@ private[kafka010] class KafkaRelation(\n         cr.partition,\n         cr.offset,\n         DateTimeUtils.fromJavaTimestamp(new java.sql.Timestamp(cr.timestamp)),\n-        cr.timestampType.id)\n+        cr.timestampType.id,\n+        UnsafeMapData.of(\n+          UnsafeArrayData.fromStringArray(cr.headers().toArray.map(_.key())),"
  }],
  "prId": 22282
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "I noticed that there are several unnecessary memory copies here. Could you just go through the iterator provided by `cr.headers()` and build a `UnsafeMapData` directly?",
    "commit": "de02de411aa00cbacc94be5b746dc48be0fe77a3",
    "createdAt": "2019-03-18T18:58:53Z",
    "diffHunk": "@@ -107,14 +108,20 @@ private[kafka010] class KafkaRelation(\n     val rdd = new KafkaSourceRDD(\n       sqlContext.sparkContext, executorKafkaParams, offsetRanges,\n       pollTimeoutMs, failOnDataLoss, reuseKafkaConsumer = false).map { cr =>\n+      val headers = cr.headers().toArray\n       InternalRow(\n         cr.key,\n         cr.value,\n         UTF8String.fromString(cr.topic),\n         cr.partition,\n         cr.offset,\n         DateTimeUtils.fromJavaTimestamp(new java.sql.Timestamp(cr.timestamp)),\n-        cr.timestampType.id)\n+        cr.timestampType.id,\n+        UnsafeMapData.of("
  }],
  "prId": 22282
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Same here. Rather than duplicate all this, construct a Seq of the values and optionally add the last one if headers are included. Then make an Internalrow from it.",
    "commit": "de02de411aa00cbacc94be5b746dc48be0fe77a3",
    "createdAt": "2019-06-18T18:57:41Z",
    "diffHunk": "@@ -102,17 +103,35 @@ private[kafka010] class KafkaRelation(\n     // Create an RDD that reads from Kafka and get the (key, value) pair as byte arrays.\n     val executorKafkaParams =\n       KafkaSourceProvider.kafkaParamsForExecutors(specifiedKafkaParams, uniqueGroupId)\n-    val rdd = new KafkaSourceRDD(\n-      sqlContext.sparkContext, executorKafkaParams, offsetRanges,\n-      pollTimeoutMs, failOnDataLoss, reuseKafkaConsumer = false).map { cr =>\n-      InternalRow(\n-        cr.key,\n-        cr.value,\n-        UTF8String.fromString(cr.topic),\n-        cr.partition,\n-        cr.offset,\n-        DateTimeUtils.fromJavaTimestamp(new java.sql.Timestamp(cr.timestamp)),\n-        cr.timestampType.id)\n+    val rdd = if (includeHeaders) {\n+      new KafkaSourceRDD(\n+        sqlContext.sparkContext, executorKafkaParams, offsetRanges,\n+        pollTimeoutMs, failOnDataLoss, reuseKafkaConsumer = false).map { cr =>\n+          InternalRow("
  }, {
    "author": {
      "login": "dongjinleekr"
    },
    "body": "I first took that approach, but it results in a serialization error, dislike to `KafkaOffsetReader` above.",
    "commit": "de02de411aa00cbacc94be5b746dc48be0fe77a3",
    "createdAt": "2019-06-19T00:14:25Z",
    "diffHunk": "@@ -102,17 +103,35 @@ private[kafka010] class KafkaRelation(\n     // Create an RDD that reads from Kafka and get the (key, value) pair as byte arrays.\n     val executorKafkaParams =\n       KafkaSourceProvider.kafkaParamsForExecutors(specifiedKafkaParams, uniqueGroupId)\n-    val rdd = new KafkaSourceRDD(\n-      sqlContext.sparkContext, executorKafkaParams, offsetRanges,\n-      pollTimeoutMs, failOnDataLoss, reuseKafkaConsumer = false).map { cr =>\n-      InternalRow(\n-        cr.key,\n-        cr.value,\n-        UTF8String.fromString(cr.topic),\n-        cr.partition,\n-        cr.offset,\n-        DateTimeUtils.fromJavaTimestamp(new java.sql.Timestamp(cr.timestamp)),\n-        cr.timestampType.id)\n+    val rdd = if (includeHeaders) {\n+      new KafkaSourceRDD(\n+        sqlContext.sparkContext, executorKafkaParams, offsetRanges,\n+        pollTimeoutMs, failOnDataLoss, reuseKafkaConsumer = false).map { cr =>\n+          InternalRow("
  }],
  "prId": 22282
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Instead of duplicating this block, can you have one method that's called with an additional boolean param?",
    "commit": "de02de411aa00cbacc94be5b746dc48be0fe77a3",
    "createdAt": "2019-08-03T13:58:26Z",
    "diffHunk": "@@ -100,17 +98,16 @@ private[kafka010] class KafkaRelation(\n     // Create an RDD that reads from Kafka and get the (key, value) pair as byte arrays.\n     val executorKafkaParams =\n       KafkaSourceProvider.kafkaParamsForExecutors(specifiedKafkaParams, uniqueGroupId)\n-    val rdd = new KafkaSourceRDD(\n-      sqlContext.sparkContext, executorKafkaParams, offsetRanges,\n-      pollTimeoutMs, failOnDataLoss, reuseKafkaConsumer = false).map { cr =>\n-      InternalRow(\n-        cr.key,\n-        cr.value,\n-        UTF8String.fromString(cr.topic),\n-        cr.partition,\n-        cr.offset,\n-        DateTimeUtils.fromJavaTimestamp(new java.sql.Timestamp(cr.timestamp)),\n-        cr.timestampType.id)\n+    val rdd = if (includeHeaders) {\n+      new KafkaSourceRDD(\n+        sqlContext.sparkContext, executorKafkaParams, offsetRanges,\n+        pollTimeoutMs, failOnDataLoss, reuseKafkaConsumer = false)\n+        .map(KafkaOffsetReader.toInternalRowWithHeaders(_))"
  }],
  "prId": 22282
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "`.map(toInternalRow)`?",
    "commit": "de02de411aa00cbacc94be5b746dc48be0fe77a3",
    "createdAt": "2019-09-10T14:04:33Z",
    "diffHunk": "@@ -100,18 +102,14 @@ private[kafka010] class KafkaRelation(\n     // Create an RDD that reads from Kafka and get the (key, value) pair as byte arrays.\n     val executorKafkaParams =\n       KafkaSourceProvider.kafkaParamsForExecutors(specifiedKafkaParams, uniqueGroupId)\n+    val toInternalRow = if (includeHeaders) {\n+      converter.toInternalRowWithHeaders\n+    } else {\n+      converter.toInternalRowWithoutHeaders\n+    }\n     val rdd = new KafkaSourceRDD(\n       sqlContext.sparkContext, executorKafkaParams, offsetRanges,\n-      pollTimeoutMs, failOnDataLoss).map { cr =>\n-      InternalRow(\n-        cr.key,\n-        cr.value,\n-        UTF8String.fromString(cr.topic),\n-        cr.partition,\n-        cr.offset,\n-        DateTimeUtils.fromJavaTimestamp(new java.sql.Timestamp(cr.timestamp)),\n-        cr.timestampType.id)\n-    }\n+      pollTimeoutMs, failOnDataLoss).map(toInternalRow(_))"
  }],
  "prId": 22282
}]