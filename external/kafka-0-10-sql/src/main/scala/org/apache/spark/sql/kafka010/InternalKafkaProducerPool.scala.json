[{
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Let's use `InternalKafkaProducerPool.toCacheKey` to unify the way to create CacheKey.",
    "commit": "ebe9708c96e8127b9ef5c43fc26cff2811a79d82",
    "createdAt": "2019-09-21T03:24:07Z",
    "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.commons.pool2.PooledObject\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.sql.kafka010.InternalKafkaProducerPool.CacheKey\n+\n+private[kafka010] class InternalKafkaProducerPool(\n+    objectFactory: ProducerObjectFactory,\n+    poolConfig: ProducerPoolConfig)\n+  extends InternalKafkaConnectorPool[CacheKey, CachedKafkaProducer](\n+      objectFactory,\n+      poolConfig,\n+      new CustomSwallowedExceptionListener(\"producer\")) {\n+\n+  def this(conf: SparkConf) = {\n+    this(new ProducerObjectFactory, new ProducerPoolConfig(conf))\n+  }\n+\n+  protected def createKey(producer: CachedKafkaProducer): CacheKey = {\n+    producer.kafkaParams.asScala.toSeq.sortBy(x => x._1)"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Changed.",
    "commit": "ebe9708c96e8127b9ef5c43fc26cff2811a79d82",
    "createdAt": "2019-09-24T08:14:09Z",
    "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.commons.pool2.PooledObject\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.sql.kafka010.InternalKafkaProducerPool.CacheKey\n+\n+private[kafka010] class InternalKafkaProducerPool(\n+    objectFactory: ProducerObjectFactory,\n+    poolConfig: ProducerPoolConfig)\n+  extends InternalKafkaConnectorPool[CacheKey, CachedKafkaProducer](\n+      objectFactory,\n+      poolConfig,\n+      new CustomSwallowedExceptionListener(\"producer\")) {\n+\n+  def this(conf: SparkConf) = {\n+    this(new ProducerObjectFactory, new ProducerPoolConfig(conf))\n+  }\n+\n+  protected def createKey(producer: CachedKafkaProducer): CacheKey = {\n+    producer.kafkaParams.asScala.toSeq.sortBy(x => x._1)"
  }],
  "prId": 25853
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "I'm a little confused, what exactly is this achieving? Do you somehow have different `Map` implementations being used by the code, and they somehow don't have consistent `hashCode` / `equals` behavior?",
    "commit": "ebe9708c96e8127b9ef5c43fc26cff2811a79d82",
    "createdAt": "2019-10-28T21:47:58Z",
    "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.commons.pool2.PooledObject\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.sql.kafka010.InternalKafkaProducerPool.CacheKey\n+\n+private[kafka010] class InternalKafkaProducerPool(\n+    objectFactory: ProducerObjectFactory,\n+    poolConfig: ProducerPoolConfig)\n+  extends InternalKafkaConnectorPool[CacheKey, CachedKafkaProducer](\n+      objectFactory,\n+      poolConfig,\n+      new CustomSwallowedExceptionListener(\"producer\")) {\n+\n+  def this(conf: SparkConf) = {\n+    this(new ProducerObjectFactory, new ProducerPoolConfig(conf))\n+  }\n+\n+  protected def createKey(producer: CachedKafkaProducer): CacheKey = {\n+    InternalKafkaProducerPool.toCacheKey(producer.kafkaParams)\n+  }\n+}\n+\n+private class ProducerPoolConfig(conf: SparkConf) extends PoolConfig[CachedKafkaProducer] {\n+  def softMaxSize: Int = conf.get(PRODUCER_CACHE_CAPACITY)\n+  def jmxEnabled: Boolean = conf.get(PRODUCER_CACHE_JMX_ENABLED)\n+  def minEvictableIdleTimeMillis: Long = conf.get(PRODUCER_CACHE_TIMEOUT)\n+  def evictorThreadRunIntervalMillis: Long = conf.get(PRODUCER_CACHE_EVICTOR_THREAD_RUN_INTERVAL)\n+  def jmxNamePrefix: String = \"kafka010-cached-simple-kafka-producer-pool\"\n+}\n+\n+private class ProducerObjectFactory extends ObjectFactory[CacheKey, CachedKafkaProducer] {\n+  protected def createValue(\n+      key: CacheKey,\n+      kafkaParams: ju.Map[String, Object]): CachedKafkaProducer = {\n+    new CachedKafkaProducer(kafkaParams)\n+  }\n+}\n+\n+private[kafka010] object InternalKafkaProducerPool {\n+  type CacheKey = Seq[(String, Object)]\n+\n+  def toCacheKey(params: ju.Map[String, Object]): CacheKey = {",
    "line": 65
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Sorted `Seq` as key is coming from the original implementation. I understand the same way, namely different `Map` implementations or different number of buckets (this is dynamically changing when `loadFactor` reached) may end-up in different order which trigger a cache miss. The sorted `Seq` approach is one way to overcame this situation.\r\n\r\nI was thinking about to use `Map` as key but didn't change because:\r\n* To use the same `Map` implementation is not enforced on API level and I think it would be an overkill to introduce it (depending on implementation detail on class API would make the code less flexible)\r\n* Depending on implementations which provide insertion order is just error prone because multiple code places must match in terms of order and I can hardly believe devs will aware of this constraint\r\n* The only option what I see is a `Map` implementation which makes ordering in insertion time for example `TreeMap` (but considering the first point I'm not really a big fan of that).\r\n",
    "commit": "ebe9708c96e8127b9ef5c43fc26cff2811a79d82",
    "createdAt": "2019-10-30T13:44:15Z",
    "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.commons.pool2.PooledObject\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.sql.kafka010.InternalKafkaProducerPool.CacheKey\n+\n+private[kafka010] class InternalKafkaProducerPool(\n+    objectFactory: ProducerObjectFactory,\n+    poolConfig: ProducerPoolConfig)\n+  extends InternalKafkaConnectorPool[CacheKey, CachedKafkaProducer](\n+      objectFactory,\n+      poolConfig,\n+      new CustomSwallowedExceptionListener(\"producer\")) {\n+\n+  def this(conf: SparkConf) = {\n+    this(new ProducerObjectFactory, new ProducerPoolConfig(conf))\n+  }\n+\n+  protected def createKey(producer: CachedKafkaProducer): CacheKey = {\n+    InternalKafkaProducerPool.toCacheKey(producer.kafkaParams)\n+  }\n+}\n+\n+private class ProducerPoolConfig(conf: SparkConf) extends PoolConfig[CachedKafkaProducer] {\n+  def softMaxSize: Int = conf.get(PRODUCER_CACHE_CAPACITY)\n+  def jmxEnabled: Boolean = conf.get(PRODUCER_CACHE_JMX_ENABLED)\n+  def minEvictableIdleTimeMillis: Long = conf.get(PRODUCER_CACHE_TIMEOUT)\n+  def evictorThreadRunIntervalMillis: Long = conf.get(PRODUCER_CACHE_EVICTOR_THREAD_RUN_INTERVAL)\n+  def jmxNamePrefix: String = \"kafka010-cached-simple-kafka-producer-pool\"\n+}\n+\n+private class ProducerObjectFactory extends ObjectFactory[CacheKey, CachedKafkaProducer] {\n+  protected def createValue(\n+      key: CacheKey,\n+      kafkaParams: ju.Map[String, Object]): CachedKafkaProducer = {\n+    new CachedKafkaProducer(kafkaParams)\n+  }\n+}\n+\n+private[kafka010] object InternalKafkaProducerPool {\n+  type CacheKey = Seq[(String, Object)]\n+\n+  def toCacheKey(params: ju.Map[String, Object]): CacheKey = {",
    "line": 65
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Not sure I follow you, because consistency in the behavior of `equals` and `hashCode` is defined in the `java.util.Map` interface itself. So I was wondering if you actually ran into any problem that required this, but if it comes from previous code...\r\n\r\nYou can try it yourself.\r\n\r\n```\r\nscala> val hm = new java.util.HashMap[String, String]()\r\nhm: java.util.HashMap[String,String] = {}\r\n\r\nscala> val tm = new java.util.TreeMap[String, String]()\r\ntm: java.util.TreeMap[String,String] = {}\r\n\r\nscala> hm == tm\r\nres0: Boolean = true\r\n\r\nscala> hm.put(\"1\", \"1\")\r\nres1: String = null\r\n\r\nscala> tm.put(\"1\", \"1\")\r\nres2: String = null\r\n\r\nscala> hm == tm\r\nres3: Boolean = true\r\n\r\nscala> hm.hashCode()\r\nres4: Int = 0\r\n\r\nscala> tm.hashCode()\r\nres5: Int = 0\r\n```",
    "commit": "ebe9708c96e8127b9ef5c43fc26cff2811a79d82",
    "createdAt": "2019-11-05T17:22:10Z",
    "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.commons.pool2.PooledObject\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.sql.kafka010.InternalKafkaProducerPool.CacheKey\n+\n+private[kafka010] class InternalKafkaProducerPool(\n+    objectFactory: ProducerObjectFactory,\n+    poolConfig: ProducerPoolConfig)\n+  extends InternalKafkaConnectorPool[CacheKey, CachedKafkaProducer](\n+      objectFactory,\n+      poolConfig,\n+      new CustomSwallowedExceptionListener(\"producer\")) {\n+\n+  def this(conf: SparkConf) = {\n+    this(new ProducerObjectFactory, new ProducerPoolConfig(conf))\n+  }\n+\n+  protected def createKey(producer: CachedKafkaProducer): CacheKey = {\n+    InternalKafkaProducerPool.toCacheKey(producer.kafkaParams)\n+  }\n+}\n+\n+private class ProducerPoolConfig(conf: SparkConf) extends PoolConfig[CachedKafkaProducer] {\n+  def softMaxSize: Int = conf.get(PRODUCER_CACHE_CAPACITY)\n+  def jmxEnabled: Boolean = conf.get(PRODUCER_CACHE_JMX_ENABLED)\n+  def minEvictableIdleTimeMillis: Long = conf.get(PRODUCER_CACHE_TIMEOUT)\n+  def evictorThreadRunIntervalMillis: Long = conf.get(PRODUCER_CACHE_EVICTOR_THREAD_RUN_INTERVAL)\n+  def jmxNamePrefix: String = \"kafka010-cached-simple-kafka-producer-pool\"\n+}\n+\n+private class ProducerObjectFactory extends ObjectFactory[CacheKey, CachedKafkaProducer] {\n+  protected def createValue(\n+      key: CacheKey,\n+      kafkaParams: ju.Map[String, Object]): CachedKafkaProducer = {\n+    new CachedKafkaProducer(kafkaParams)\n+  }\n+}\n+\n+private[kafka010] object InternalKafkaProducerPool {\n+  type CacheKey = Seq[(String, Object)]\n+\n+  def toCacheKey(params: ju.Map[String, Object]): CacheKey = {",
    "line": 65
  }],
  "prId": 25853
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`override`",
    "commit": "ebe9708c96e8127b9ef5c43fc26cff2811a79d82",
    "createdAt": "2019-11-06T19:04:48Z",
    "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.commons.pool2.PooledObject\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.sql.kafka010.InternalKafkaProducerPool.CacheKey\n+\n+private[kafka010] class InternalKafkaProducerPool(\n+    objectFactory: ProducerObjectFactory,\n+    poolConfig: ProducerPoolConfig)\n+  extends InternalKafkaConnectorPool[CacheKey, CachedKafkaProducer](\n+      objectFactory,\n+      poolConfig,\n+      new CustomSwallowedExceptionListener(\"producer\")) {\n+\n+  def this(conf: SparkConf) = {\n+    this(new ProducerObjectFactory, new ProducerPoolConfig(conf))\n+  }\n+\n+  protected def createKey(producer: CachedKafkaProducer): CacheKey = {\n+    InternalKafkaProducerPool.toCacheKey(producer.kafkaParams)\n+  }\n+}\n+\n+private class ProducerPoolConfig(conf: SparkConf) extends PoolConfig[CachedKafkaProducer] {\n+  def softMaxSize: Int = conf.get(PRODUCER_CACHE_CAPACITY)\n+  def jmxEnabled: Boolean = conf.get(PRODUCER_CACHE_JMX_ENABLED)\n+  def minEvictableIdleTimeMillis: Long = conf.get(PRODUCER_CACHE_TIMEOUT)\n+  def evictorThreadRunIntervalMillis: Long = conf.get(PRODUCER_CACHE_EVICTOR_THREAD_RUN_INTERVAL)\n+  def jmxNamePrefix: String = \"kafka010-cached-simple-kafka-producer-pool\"\n+}\n+\n+private class ProducerObjectFactory extends ObjectFactory[CacheKey, CachedKafkaProducer] {\n+  protected def createValue("
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Fixed.",
    "commit": "ebe9708c96e8127b9ef5c43fc26cff2811a79d82",
    "createdAt": "2019-11-07T13:51:52Z",
    "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.commons.pool2.PooledObject\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.sql.kafka010.InternalKafkaProducerPool.CacheKey\n+\n+private[kafka010] class InternalKafkaProducerPool(\n+    objectFactory: ProducerObjectFactory,\n+    poolConfig: ProducerPoolConfig)\n+  extends InternalKafkaConnectorPool[CacheKey, CachedKafkaProducer](\n+      objectFactory,\n+      poolConfig,\n+      new CustomSwallowedExceptionListener(\"producer\")) {\n+\n+  def this(conf: SparkConf) = {\n+    this(new ProducerObjectFactory, new ProducerPoolConfig(conf))\n+  }\n+\n+  protected def createKey(producer: CachedKafkaProducer): CacheKey = {\n+    InternalKafkaProducerPool.toCacheKey(producer.kafkaParams)\n+  }\n+}\n+\n+private class ProducerPoolConfig(conf: SparkConf) extends PoolConfig[CachedKafkaProducer] {\n+  def softMaxSize: Int = conf.get(PRODUCER_CACHE_CAPACITY)\n+  def jmxEnabled: Boolean = conf.get(PRODUCER_CACHE_JMX_ENABLED)\n+  def minEvictableIdleTimeMillis: Long = conf.get(PRODUCER_CACHE_TIMEOUT)\n+  def evictorThreadRunIntervalMillis: Long = conf.get(PRODUCER_CACHE_EVICTOR_THREAD_RUN_INTERVAL)\n+  def jmxNamePrefix: String = \"kafka010-cached-simple-kafka-producer-pool\"\n+}\n+\n+private class ProducerObjectFactory extends ObjectFactory[CacheKey, CachedKafkaProducer] {\n+  protected def createValue("
  }],
  "prId": 25853
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`override`",
    "commit": "ebe9708c96e8127b9ef5c43fc26cff2811a79d82",
    "createdAt": "2019-11-06T19:06:53Z",
    "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.commons.pool2.PooledObject\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.sql.kafka010.InternalKafkaProducerPool.CacheKey\n+\n+private[kafka010] class InternalKafkaProducerPool(\n+    objectFactory: ProducerObjectFactory,\n+    poolConfig: ProducerPoolConfig)\n+  extends InternalKafkaConnectorPool[CacheKey, CachedKafkaProducer](\n+      objectFactory,\n+      poolConfig,\n+      new CustomSwallowedExceptionListener(\"producer\")) {\n+\n+  def this(conf: SparkConf) = {\n+    this(new ProducerObjectFactory, new ProducerPoolConfig(conf))\n+  }\n+\n+  protected def createKey(producer: CachedKafkaProducer): CacheKey = {"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Fixed.",
    "commit": "ebe9708c96e8127b9ef5c43fc26cff2811a79d82",
    "createdAt": "2019-11-07T13:52:07Z",
    "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.commons.pool2.PooledObject\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.sql.kafka010.InternalKafkaProducerPool.CacheKey\n+\n+private[kafka010] class InternalKafkaProducerPool(\n+    objectFactory: ProducerObjectFactory,\n+    poolConfig: ProducerPoolConfig)\n+  extends InternalKafkaConnectorPool[CacheKey, CachedKafkaProducer](\n+      objectFactory,\n+      poolConfig,\n+      new CustomSwallowedExceptionListener(\"producer\")) {\n+\n+  def this(conf: SparkConf) = {\n+    this(new ProducerObjectFactory, new ProducerPoolConfig(conf))\n+  }\n+\n+  protected def createKey(producer: CachedKafkaProducer): CacheKey = {"
  }],
  "prId": 25853
}]