[{
  "comments": [{
    "author": {
      "login": "arunmahadevan"
    },
    "body": "Its kind of odd that the writer commit message includes source offset. IMO, better to define a `KafkaSinkOffset` or if it can be common, something like `KafkaOffsets`.",
    "commit": "c812effdf54f3ce371fde561c26e2485ce996b64",
    "createdAt": "2018-08-20T17:03:28Z",
    "diffHunk": "@@ -19,18 +19,23 @@ package org.apache.spark.sql.kafka010\n \n import scala.collection.JavaConverters._\n \n+import org.json4s.JsonDSL._\n+import org.json4s.jackson.JsonMethods._\n+\n import org.apache.spark.sql.catalyst.InternalRow\n import org.apache.spark.sql.catalyst.expressions.Attribute\n import org.apache.spark.sql.kafka010.KafkaWriter.validateQuery\n+import org.apache.spark.sql.sources.v2.CustomMetrics\n import org.apache.spark.sql.sources.v2.writer._\n-import org.apache.spark.sql.sources.v2.writer.streaming.StreamWriter\n+import org.apache.spark.sql.sources.v2.writer.streaming.{StreamWriter, SupportsCustomWriterMetrics}\n import org.apache.spark.sql.types.StructType\n \n /**\n  * Dummy commit message. The DataSourceV2 framework requires a commit message implementation but we\n  * don't need to really send one.\n  */\n-case object KafkaWriterCommitMessage extends WriterCommitMessage\n+case class KafkaWriterCommitMessage(minOffset: KafkaSourceOffset, maxOffset: KafkaSourceOffset)",
    "line": 21
  }, {
    "author": {
      "login": "vackosar"
    },
    "body": "I would have to rename the class itself to not add additional duplicate class. I would love to do that, it is just that I am not sure if it would be accepted.",
    "commit": "c812effdf54f3ce371fde561c26e2485ce996b64",
    "createdAt": "2018-08-20T19:25:58Z",
    "diffHunk": "@@ -19,18 +19,23 @@ package org.apache.spark.sql.kafka010\n \n import scala.collection.JavaConverters._\n \n+import org.json4s.JsonDSL._\n+import org.json4s.jackson.JsonMethods._\n+\n import org.apache.spark.sql.catalyst.InternalRow\n import org.apache.spark.sql.catalyst.expressions.Attribute\n import org.apache.spark.sql.kafka010.KafkaWriter.validateQuery\n+import org.apache.spark.sql.sources.v2.CustomMetrics\n import org.apache.spark.sql.sources.v2.writer._\n-import org.apache.spark.sql.sources.v2.writer.streaming.StreamWriter\n+import org.apache.spark.sql.sources.v2.writer.streaming.{StreamWriter, SupportsCustomWriterMetrics}\n import org.apache.spark.sql.types.StructType\n \n /**\n  * Dummy commit message. The DataSourceV2 framework requires a commit message implementation but we\n  * don't need to really send one.\n  */\n-case object KafkaWriterCommitMessage extends WriterCommitMessage\n+case class KafkaWriterCommitMessage(minOffset: KafkaSourceOffset, maxOffset: KafkaSourceOffset)",
    "line": 21
  }],
  "prId": 22143
}, {
  "comments": [{
    "author": {
      "login": "arunmahadevan"
    },
    "body": "good to leave some comment on what this does. It seems to be computing the min/max offset per partition? If so choosing an apt name for that function would make it clearer.",
    "commit": "c812effdf54f3ce371fde561c26e2485ce996b64",
    "createdAt": "2018-08-20T17:05:37Z",
    "diffHunk": "@@ -116,3 +133,66 @@ class KafkaStreamDataWriter(\n     }\n   }\n }\n+\n+private[kafka010] case class KafkaWriterCustomMetrics(\n+    minOffset: KafkaSourceOffset,\n+    maxOffset: KafkaSourceOffset) extends CustomMetrics {\n+  override def json(): String = {\n+    val jsonVal = (\"minOffset\" -> parse(minOffset.json)) ~\n+      (\"maxOffset\" -> parse(maxOffset.json))\n+    compact(render(jsonVal))\n+  }\n+\n+  override def toString: String = json()\n+}\n+\n+private[kafka010] object KafkaWriterCustomMetrics {\n+\n+  import Math.{min, max}\n+\n+  def apply(messages: Array[WriterCommitMessage]): KafkaWriterCustomMetrics = {\n+    val minMax = collate(messages)\n+    KafkaWriterCustomMetrics(minMax._1, minMax._2)\n+  }\n+\n+  private def collate(messages: Array[WriterCommitMessage]):",
    "line": 91
  }, {
    "author": {
      "login": "vackosar"
    },
    "body": "Thanks, I will rename to something with minMax.",
    "commit": "c812effdf54f3ce371fde561c26e2485ce996b64",
    "createdAt": "2018-08-20T19:34:04Z",
    "diffHunk": "@@ -116,3 +133,66 @@ class KafkaStreamDataWriter(\n     }\n   }\n }\n+\n+private[kafka010] case class KafkaWriterCustomMetrics(\n+    minOffset: KafkaSourceOffset,\n+    maxOffset: KafkaSourceOffset) extends CustomMetrics {\n+  override def json(): String = {\n+    val jsonVal = (\"minOffset\" -> parse(minOffset.json)) ~\n+      (\"maxOffset\" -> parse(maxOffset.json))\n+    compact(render(jsonVal))\n+  }\n+\n+  override def toString: String = json()\n+}\n+\n+private[kafka010] object KafkaWriterCustomMetrics {\n+\n+  import Math.{min, max}\n+\n+  def apply(messages: Array[WriterCommitMessage]): KafkaWriterCustomMetrics = {\n+    val minMax = collate(messages)\n+    KafkaWriterCustomMetrics(minMax._1, minMax._2)\n+  }\n+\n+  private def collate(messages: Array[WriterCommitMessage]):",
    "line": 91
  }],
  "prId": 22143
}]