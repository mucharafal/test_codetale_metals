[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: what the long name? why not simply KafkaOffsetReader as it was before?",
    "commit": "3bc7c4cb8cb93eef2275a517ce72f0bcb8919cfe",
    "createdAt": "2017-02-03T12:33:52Z",
    "diffHunk": "@@ -0,0 +1,300 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import scala.collection.JavaConverters._\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.concurrent.duration.Duration\n+import scala.util.control.NonFatal\n+\n+import org.apache.kafka.clients.consumer.{Consumer, ConsumerConfig, KafkaConsumer}\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.{ThreadUtils, UninterruptibleThread}\n+\n+/**\n+ * This class uses Kafka's own [[KafkaConsumer]] API to read data offsets from Kafka.\n+ * The [[ConsumerStrategy]] class defines which Kafka topics and partitions should be read\n+ * by this source. These strategies directly correspond to the different consumption options\n+ * in. This class is designed to return a configured [[KafkaConsumer]] that is used by the\n+ * [[KafkaSource]] to query for the offsets. See the docs on\n+ * [[org.apache.spark.sql.kafka010.ConsumerStrategy]]\n+ * for more details.\n+ *\n+ * Note: This class is not ThreadSafe\n+ */\n+private[kafka010] class KafkaTopicPartitionOffsetReader("
  }, {
    "author": {
      "login": "tcondie"
    },
    "body": "I'll rollback. My thought was to also indicate that it's being used to read TopicPartition(s).",
    "commit": "3bc7c4cb8cb93eef2275a517ce72f0bcb8919cfe",
    "createdAt": "2017-02-03T17:24:46Z",
    "diffHunk": "@@ -0,0 +1,300 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import scala.collection.JavaConverters._\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.concurrent.duration.Duration\n+import scala.util.control.NonFatal\n+\n+import org.apache.kafka.clients.consumer.{Consumer, ConsumerConfig, KafkaConsumer}\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.{ThreadUtils, UninterruptibleThread}\n+\n+/**\n+ * This class uses Kafka's own [[KafkaConsumer]] API to read data offsets from Kafka.\n+ * The [[ConsumerStrategy]] class defines which Kafka topics and partitions should be read\n+ * by this source. These strategies directly correspond to the different consumption options\n+ * in. This class is designed to return a configured [[KafkaConsumer]] that is used by the\n+ * [[KafkaSource]] to query for the offsets. See the docs on\n+ * [[org.apache.spark.sql.kafka010.ConsumerStrategy]]\n+ * for more details.\n+ *\n+ * Note: This class is not ThreadSafe\n+ */\n+private[kafka010] class KafkaTopicPartitionOffsetReader("
  }],
  "prId": 16686
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "can you specify which partitions? maybe \"offsets of all partitions to be consumed according the consumer strategy\"\r\n\r\nsame for docs of other methods that do not take a specific list of partitions.",
    "commit": "3bc7c4cb8cb93eef2275a517ce72f0bcb8919cfe",
    "createdAt": "2017-02-03T12:56:38Z",
    "diffHunk": "@@ -0,0 +1,300 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import scala.collection.JavaConverters._\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.concurrent.duration.Duration\n+import scala.util.control.NonFatal\n+\n+import org.apache.kafka.clients.consumer.{Consumer, ConsumerConfig, KafkaConsumer}\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.{ThreadUtils, UninterruptibleThread}\n+\n+/**\n+ * This class uses Kafka's own [[KafkaConsumer]] API to read data offsets from Kafka.\n+ * The [[ConsumerStrategy]] class defines which Kafka topics and partitions should be read\n+ * by this source. These strategies directly correspond to the different consumption options\n+ * in. This class is designed to return a configured [[KafkaConsumer]] that is used by the\n+ * [[KafkaSource]] to query for the offsets. See the docs on\n+ * [[org.apache.spark.sql.kafka010.ConsumerStrategy]]\n+ * for more details.\n+ *\n+ * Note: This class is not ThreadSafe\n+ */\n+private[kafka010] class KafkaTopicPartitionOffsetReader(\n+    consumerStrategy: ConsumerStrategy,\n+    driverKafkaParams: ju.Map[String, Object],\n+    readerOptions: Map[String, String],\n+    driverGroupIdPrefix: String) extends Logging {\n+  /**\n+   * Used to ensure execute fetch operations execute in an UninterruptibleThread\n+   */\n+  val kafkaReaderThread = Executors.newSingleThreadExecutor(new ThreadFactory {\n+    override def newThread(r: Runnable): Thread = {\n+      val t = new UninterruptibleThread(\"Kafka Offset Reader\") {\n+        override def run(): Unit = {\n+          r.run()\n+        }\n+      }\n+      t.setDaemon(true)\n+      t\n+    }\n+  })\n+  val execContext = ExecutionContext.fromExecutorService(kafkaReaderThread)\n+\n+  /**\n+   * A KafkaConsumer used in the driver to query the latest Kafka offsets. This only queries the\n+   * offsets and never commits them.\n+   */\n+  protected var consumer = createConsumer()\n+\n+  private val maxOffsetFetchAttempts =\n+    readerOptions.getOrElse(\"fetchOffset.numRetries\", \"3\").toInt\n+\n+  private val offsetFetchAttemptIntervalMs =\n+    readerOptions.getOrElse(\"fetchOffset.retryIntervalMs\", \"1000\").toLong\n+\n+  private var groupId: String = null\n+\n+  private var nextId = 0\n+\n+  private def nextGroupId(): String = {\n+    groupId = driverGroupIdPrefix + \"-\" + nextId\n+    nextId += 1\n+    groupId\n+  }\n+\n+  override def toString(): String = consumerStrategy.toString\n+\n+  /**\n+   * Closes the connection to Kafka, and cleans up state.\n+   */\n+  def close(): Unit = {\n+    consumer.close()\n+    kafkaReaderThread.shutdownNow()\n+  }\n+\n+  /**\n+   * @return The Set of TopicPartitions for a given topic\n+   */\n+  def fetchTopicPartitions(): Set[TopicPartition] = runUninterruptibly {\n+    assert(Thread.currentThread().isInstanceOf[UninterruptibleThread])\n+    // Poll to get the latest assigned partitions\n+    consumer.poll(0)\n+    val partitions = consumer.assignment()\n+    consumer.pause(partitions)\n+    partitions.asScala.toSet\n+  }\n+\n+  /**\n+   * Resolves the specific offsets based on Kafka seek positions.\n+   * This method resolves offset value -1 to the latest and -2 to the\n+   * earliest Kafka seek position.\n+   */\n+  def fetchSpecificOffsets(\n+      partitionOffsets: Map[TopicPartition, Long]): Map[TopicPartition, Long] =\n+    runUninterruptibly {\n+      withRetriesWithoutInterrupt {\n+        // Poll to get the latest assigned partitions\n+        consumer.poll(0)\n+        val partitions = consumer.assignment()\n+        consumer.pause(partitions)\n+        assert(partitions.asScala == partitionOffsets.keySet,\n+          \"If startingOffsets contains specific offsets, you must specify all TopicPartitions.\\n\" +\n+            \"Use -1 for latest, -2 for earliest, if you don't care.\\n\" +\n+            s\"Specified: ${partitionOffsets.keySet} Assigned: ${partitions.asScala}\")\n+        logDebug(s\"Partitions assigned to consumer: $partitions. Seeking to $partitionOffsets\")\n+\n+        partitionOffsets.foreach {\n+          case (tp, KafkaOffsets.LATEST) => consumer.seekToEnd(ju.Arrays.asList(tp))\n+          case (tp, KafkaOffsets.EARLIEST) => consumer.seekToBeginning(ju.Arrays.asList(tp))\n+          case (tp, off) => consumer.seek(tp, off)\n+        }\n+        partitionOffsets.map {\n+          case (tp, _) => tp -> consumer.position(tp)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * Fetch the earliest offsets of partitions."
  }],
  "prId": 16686
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: add docs.",
    "commit": "3bc7c4cb8cb93eef2275a517ce72f0bcb8919cfe",
    "createdAt": "2017-02-03T12:59:06Z",
    "diffHunk": "@@ -0,0 +1,300 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import scala.collection.JavaConverters._\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.concurrent.duration.Duration\n+import scala.util.control.NonFatal\n+\n+import org.apache.kafka.clients.consumer.{Consumer, ConsumerConfig, KafkaConsumer}\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.{ThreadUtils, UninterruptibleThread}\n+\n+/**\n+ * This class uses Kafka's own [[KafkaConsumer]] API to read data offsets from Kafka.\n+ * The [[ConsumerStrategy]] class defines which Kafka topics and partitions should be read\n+ * by this source. These strategies directly correspond to the different consumption options\n+ * in. This class is designed to return a configured [[KafkaConsumer]] that is used by the\n+ * [[KafkaSource]] to query for the offsets. See the docs on\n+ * [[org.apache.spark.sql.kafka010.ConsumerStrategy]]\n+ * for more details.\n+ *\n+ * Note: This class is not ThreadSafe\n+ */\n+private[kafka010] class KafkaTopicPartitionOffsetReader(\n+    consumerStrategy: ConsumerStrategy,\n+    driverKafkaParams: ju.Map[String, Object],\n+    readerOptions: Map[String, String],\n+    driverGroupIdPrefix: String) extends Logging {\n+  /**\n+   * Used to ensure execute fetch operations execute in an UninterruptibleThread\n+   */\n+  val kafkaReaderThread = Executors.newSingleThreadExecutor(new ThreadFactory {\n+    override def newThread(r: Runnable): Thread = {\n+      val t = new UninterruptibleThread(\"Kafka Offset Reader\") {\n+        override def run(): Unit = {\n+          r.run()\n+        }\n+      }\n+      t.setDaemon(true)\n+      t\n+    }\n+  })\n+  val execContext = ExecutionContext.fromExecutorService(kafkaReaderThread)\n+\n+  /**\n+   * A KafkaConsumer used in the driver to query the latest Kafka offsets. This only queries the\n+   * offsets and never commits them.\n+   */\n+  protected var consumer = createConsumer()\n+\n+  private val maxOffsetFetchAttempts =\n+    readerOptions.getOrElse(\"fetchOffset.numRetries\", \"3\").toInt\n+\n+  private val offsetFetchAttemptIntervalMs =\n+    readerOptions.getOrElse(\"fetchOffset.retryIntervalMs\", \"1000\").toLong\n+\n+  private var groupId: String = null\n+\n+  private var nextId = 0\n+\n+  private def nextGroupId(): String = {\n+    groupId = driverGroupIdPrefix + \"-\" + nextId\n+    nextId += 1\n+    groupId\n+  }\n+\n+  override def toString(): String = consumerStrategy.toString\n+\n+  /**\n+   * Closes the connection to Kafka, and cleans up state.\n+   */\n+  def close(): Unit = {\n+    consumer.close()\n+    kafkaReaderThread.shutdownNow()\n+  }\n+\n+  /**\n+   * @return The Set of TopicPartitions for a given topic\n+   */\n+  def fetchTopicPartitions(): Set[TopicPartition] = runUninterruptibly {\n+    assert(Thread.currentThread().isInstanceOf[UninterruptibleThread])\n+    // Poll to get the latest assigned partitions\n+    consumer.poll(0)\n+    val partitions = consumer.assignment()\n+    consumer.pause(partitions)\n+    partitions.asScala.toSet\n+  }\n+\n+  /**\n+   * Resolves the specific offsets based on Kafka seek positions.\n+   * This method resolves offset value -1 to the latest and -2 to the\n+   * earliest Kafka seek position.\n+   */\n+  def fetchSpecificOffsets(\n+      partitionOffsets: Map[TopicPartition, Long]): Map[TopicPartition, Long] =\n+    runUninterruptibly {\n+      withRetriesWithoutInterrupt {\n+        // Poll to get the latest assigned partitions\n+        consumer.poll(0)\n+        val partitions = consumer.assignment()\n+        consumer.pause(partitions)\n+        assert(partitions.asScala == partitionOffsets.keySet,\n+          \"If startingOffsets contains specific offsets, you must specify all TopicPartitions.\\n\" +\n+            \"Use -1 for latest, -2 for earliest, if you don't care.\\n\" +\n+            s\"Specified: ${partitionOffsets.keySet} Assigned: ${partitions.asScala}\")\n+        logDebug(s\"Partitions assigned to consumer: $partitions. Seeking to $partitionOffsets\")\n+\n+        partitionOffsets.foreach {\n+          case (tp, KafkaOffsets.LATEST) => consumer.seekToEnd(ju.Arrays.asList(tp))\n+          case (tp, KafkaOffsets.EARLIEST) => consumer.seekToBeginning(ju.Arrays.asList(tp))\n+          case (tp, off) => consumer.seek(tp, off)\n+        }\n+        partitionOffsets.map {\n+          case (tp, _) => tp -> consumer.position(tp)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * Fetch the earliest offsets of partitions.\n+   */\n+  def fetchEarliestOffsets(): Map[TopicPartition, Long] = runUninterruptibly {\n+    withRetriesWithoutInterrupt {\n+      // Poll to get the latest assigned partitions\n+      consumer.poll(0)\n+      val partitions = consumer.assignment()\n+      consumer.pause(partitions)\n+      logDebug(s\"Partitions assigned to consumer: $partitions. Seeking to the beginning\")\n+\n+      consumer.seekToBeginning(partitions)\n+      val partitionOffsets = partitions.asScala.map(p => p -> consumer.position(p)).toMap\n+      logDebug(s\"Got earliest offsets for partition : $partitionOffsets\")\n+      partitionOffsets\n+    }\n+  }\n+\n+  /**\n+   * Fetch the latest offsets of partitions.\n+   */\n+  def fetchLatestOffsets(): Map[TopicPartition, Long] = runUninterruptibly {\n+    withRetriesWithoutInterrupt {\n+      // Poll to get the latest assigned partitions\n+      consumer.poll(0)\n+      val partitions = consumer.assignment()\n+      consumer.pause(partitions)\n+      logDebug(s\"Partitions assigned to consumer: $partitions. Seeking to the end.\")\n+\n+      consumer.seekToEnd(partitions)\n+      val partitionOffsets = partitions.asScala.map(p => p -> consumer.position(p)).toMap\n+      logDebug(s\"Got latest offsets for partition : $partitionOffsets\")\n+      partitionOffsets\n+    }\n+  }\n+\n+  /**\n+   * Fetch the earliest offsets for specific partitions.\n+   * The return result may not contain some partitions if they are deleted.\n+   */\n+  def fetchEarliestOffsets(\n+      newPartitions: Seq[TopicPartition]): Map[TopicPartition, Long] = {\n+    if (newPartitions.isEmpty) {\n+      Map.empty[TopicPartition, Long]\n+    } else {\n+      runUninterruptibly {\n+        withRetriesWithoutInterrupt {\n+          // Poll to get the latest assigned partitions\n+          consumer.poll(0)\n+          val partitions = consumer.assignment()\n+          consumer.pause(partitions)\n+          logDebug(s\"\\tPartitions assigned to consumer: $partitions\")\n+\n+          // Get the earliest offset of each partition\n+          consumer.seekToBeginning(partitions)\n+          val partitionOffsets = newPartitions.filter { p =>\n+            // When deleting topics happen at the same time, some partitions may not be in\n+            // `partitions`. So we need to ignore them\n+            partitions.contains(p)\n+          }.map(p => p -> consumer.position(p)).toMap\n+          logDebug(s\"Got earliest offsets for new partitions: $partitionOffsets\")\n+          partitionOffsets\n+        }\n+      }\n+    }\n+  }\n+\n+  private def runUninterruptibly[T](body: => T): T = {"
  }],
  "prId": 16686
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: on ~the~ a body",
    "commit": "3bc7c4cb8cb93eef2275a517ce72f0bcb8919cfe",
    "createdAt": "2017-02-03T13:01:00Z",
    "diffHunk": "@@ -0,0 +1,300 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import java.{util => ju}\n+import java.util.concurrent.{Executors, ThreadFactory}\n+\n+import scala.collection.JavaConverters._\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.concurrent.duration.Duration\n+import scala.util.control.NonFatal\n+\n+import org.apache.kafka.clients.consumer.{Consumer, ConsumerConfig, KafkaConsumer}\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.{ThreadUtils, UninterruptibleThread}\n+\n+/**\n+ * This class uses Kafka's own [[KafkaConsumer]] API to read data offsets from Kafka.\n+ * The [[ConsumerStrategy]] class defines which Kafka topics and partitions should be read\n+ * by this source. These strategies directly correspond to the different consumption options\n+ * in. This class is designed to return a configured [[KafkaConsumer]] that is used by the\n+ * [[KafkaSource]] to query for the offsets. See the docs on\n+ * [[org.apache.spark.sql.kafka010.ConsumerStrategy]]\n+ * for more details.\n+ *\n+ * Note: This class is not ThreadSafe\n+ */\n+private[kafka010] class KafkaTopicPartitionOffsetReader(\n+    consumerStrategy: ConsumerStrategy,\n+    driverKafkaParams: ju.Map[String, Object],\n+    readerOptions: Map[String, String],\n+    driverGroupIdPrefix: String) extends Logging {\n+  /**\n+   * Used to ensure execute fetch operations execute in an UninterruptibleThread\n+   */\n+  val kafkaReaderThread = Executors.newSingleThreadExecutor(new ThreadFactory {\n+    override def newThread(r: Runnable): Thread = {\n+      val t = new UninterruptibleThread(\"Kafka Offset Reader\") {\n+        override def run(): Unit = {\n+          r.run()\n+        }\n+      }\n+      t.setDaemon(true)\n+      t\n+    }\n+  })\n+  val execContext = ExecutionContext.fromExecutorService(kafkaReaderThread)\n+\n+  /**\n+   * A KafkaConsumer used in the driver to query the latest Kafka offsets. This only queries the\n+   * offsets and never commits them.\n+   */\n+  protected var consumer = createConsumer()\n+\n+  private val maxOffsetFetchAttempts =\n+    readerOptions.getOrElse(\"fetchOffset.numRetries\", \"3\").toInt\n+\n+  private val offsetFetchAttemptIntervalMs =\n+    readerOptions.getOrElse(\"fetchOffset.retryIntervalMs\", \"1000\").toLong\n+\n+  private var groupId: String = null\n+\n+  private var nextId = 0\n+\n+  private def nextGroupId(): String = {\n+    groupId = driverGroupIdPrefix + \"-\" + nextId\n+    nextId += 1\n+    groupId\n+  }\n+\n+  override def toString(): String = consumerStrategy.toString\n+\n+  /**\n+   * Closes the connection to Kafka, and cleans up state.\n+   */\n+  def close(): Unit = {\n+    consumer.close()\n+    kafkaReaderThread.shutdownNow()\n+  }\n+\n+  /**\n+   * @return The Set of TopicPartitions for a given topic\n+   */\n+  def fetchTopicPartitions(): Set[TopicPartition] = runUninterruptibly {\n+    assert(Thread.currentThread().isInstanceOf[UninterruptibleThread])\n+    // Poll to get the latest assigned partitions\n+    consumer.poll(0)\n+    val partitions = consumer.assignment()\n+    consumer.pause(partitions)\n+    partitions.asScala.toSet\n+  }\n+\n+  /**\n+   * Resolves the specific offsets based on Kafka seek positions.\n+   * This method resolves offset value -1 to the latest and -2 to the\n+   * earliest Kafka seek position.\n+   */\n+  def fetchSpecificOffsets(\n+      partitionOffsets: Map[TopicPartition, Long]): Map[TopicPartition, Long] =\n+    runUninterruptibly {\n+      withRetriesWithoutInterrupt {\n+        // Poll to get the latest assigned partitions\n+        consumer.poll(0)\n+        val partitions = consumer.assignment()\n+        consumer.pause(partitions)\n+        assert(partitions.asScala == partitionOffsets.keySet,\n+          \"If startingOffsets contains specific offsets, you must specify all TopicPartitions.\\n\" +\n+            \"Use -1 for latest, -2 for earliest, if you don't care.\\n\" +\n+            s\"Specified: ${partitionOffsets.keySet} Assigned: ${partitions.asScala}\")\n+        logDebug(s\"Partitions assigned to consumer: $partitions. Seeking to $partitionOffsets\")\n+\n+        partitionOffsets.foreach {\n+          case (tp, KafkaOffsets.LATEST) => consumer.seekToEnd(ju.Arrays.asList(tp))\n+          case (tp, KafkaOffsets.EARLIEST) => consumer.seekToBeginning(ju.Arrays.asList(tp))\n+          case (tp, off) => consumer.seek(tp, off)\n+        }\n+        partitionOffsets.map {\n+          case (tp, _) => tp -> consumer.position(tp)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * Fetch the earliest offsets of partitions.\n+   */\n+  def fetchEarliestOffsets(): Map[TopicPartition, Long] = runUninterruptibly {\n+    withRetriesWithoutInterrupt {\n+      // Poll to get the latest assigned partitions\n+      consumer.poll(0)\n+      val partitions = consumer.assignment()\n+      consumer.pause(partitions)\n+      logDebug(s\"Partitions assigned to consumer: $partitions. Seeking to the beginning\")\n+\n+      consumer.seekToBeginning(partitions)\n+      val partitionOffsets = partitions.asScala.map(p => p -> consumer.position(p)).toMap\n+      logDebug(s\"Got earliest offsets for partition : $partitionOffsets\")\n+      partitionOffsets\n+    }\n+  }\n+\n+  /**\n+   * Fetch the latest offsets of partitions.\n+   */\n+  def fetchLatestOffsets(): Map[TopicPartition, Long] = runUninterruptibly {\n+    withRetriesWithoutInterrupt {\n+      // Poll to get the latest assigned partitions\n+      consumer.poll(0)\n+      val partitions = consumer.assignment()\n+      consumer.pause(partitions)\n+      logDebug(s\"Partitions assigned to consumer: $partitions. Seeking to the end.\")\n+\n+      consumer.seekToEnd(partitions)\n+      val partitionOffsets = partitions.asScala.map(p => p -> consumer.position(p)).toMap\n+      logDebug(s\"Got latest offsets for partition : $partitionOffsets\")\n+      partitionOffsets\n+    }\n+  }\n+\n+  /**\n+   * Fetch the earliest offsets for specific partitions.\n+   * The return result may not contain some partitions if they are deleted.\n+   */\n+  def fetchEarliestOffsets(\n+      newPartitions: Seq[TopicPartition]): Map[TopicPartition, Long] = {\n+    if (newPartitions.isEmpty) {\n+      Map.empty[TopicPartition, Long]\n+    } else {\n+      runUninterruptibly {\n+        withRetriesWithoutInterrupt {\n+          // Poll to get the latest assigned partitions\n+          consumer.poll(0)\n+          val partitions = consumer.assignment()\n+          consumer.pause(partitions)\n+          logDebug(s\"\\tPartitions assigned to consumer: $partitions\")\n+\n+          // Get the earliest offset of each partition\n+          consumer.seekToBeginning(partitions)\n+          val partitionOffsets = newPartitions.filter { p =>\n+            // When deleting topics happen at the same time, some partitions may not be in\n+            // `partitions`. So we need to ignore them\n+            partitions.contains(p)\n+          }.map(p => p -> consumer.position(p)).toMap\n+          logDebug(s\"Got earliest offsets for new partitions: $partitionOffsets\")\n+          partitionOffsets\n+        }\n+      }\n+    }\n+  }\n+\n+  private def runUninterruptibly[T](body: => T): T = {\n+    if (!Thread.currentThread.isInstanceOf[UninterruptibleThread]) {\n+      val future = Future { body }(execContext)\n+      ThreadUtils.awaitResult(future, Duration.Inf)\n+    } else {\n+      body\n+    }\n+  }\n+\n+  /**\n+   * Helper function that does multiple retries on the a body of code that returns offsets."
  }],
  "prId": 16686
}]