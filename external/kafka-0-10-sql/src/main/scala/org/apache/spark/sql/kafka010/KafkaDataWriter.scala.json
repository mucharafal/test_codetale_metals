[{
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Spark leverages the fact that Kafka producer is thread-safe. You may need to check whether it's still valid for transactional producer as well. (My instinct says it may not, otherwise you'll deal with 2PC via multiple partitions with same producer id in same executor. Sounds weird.)",
    "commit": "4ce3967db60a807c07ec892e0594ba5e06b1621a",
    "createdAt": "2019-08-29T23:28:42Z",
    "diffHunk": "@@ -18,16 +18,160 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import com.google.common.cache._\n \n import org.apache.spark.sql.catalyst.InternalRow\n import org.apache.spark.sql.catalyst.expressions.Attribute\n import org.apache.spark.sql.sources.v2.writer._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A [[WriterCommitMessage]] for Kafka commit message.\n+ * @param transactionalId Unique transactionalId for each producer.\n+ * @param epoch Transactional epoch.\n+ * @param producerId Transactional producerId for producer, got when init transaction.\n+ */\n+private[kafka010] case class ProducerTransactionMetaData(\n+    transactionalId: String,\n+    epoch: Short,\n+    producerId: Long)\n+  extends WriterCommitMessage\n+\n+/**\n+ * Emtpy commit message for resume transaction.\n+ */\n+private case object EmptyCommitMessage extends WriterCommitMessage\n+\n+private[kafka010] case object ProducerTransactionMetaData {\n+  val VERSION = 1\n+\n+  def toTransactionId(\n+      executorId: String,\n+      taskIndex: String,\n+      transactionalIdSuffix: String): String = {\n+    toTransactionId(toProducerIdentity(executorId, taskIndex), transactionalIdSuffix)\n+  }\n+\n+  def toTransactionId(producerIdentity: String, transactionalIdSuffix: String): String = {\n+    s\"$producerIdentity||$transactionalIdSuffix\"\n+  }\n+\n+  def toTransactionalIdSuffix(transactionalId: String): String = {\n+    transactionalId.split(\"\\\\|\\\\|\", 2)(1)\n+  }\n+\n+  def toProducerIdentity(transactionalId: String): String = {\n+    transactionalId.split(\"\\\\|\\\\|\", 2)(0)\n+  }\n+\n+  def toExecutorId(transactionalId: String): String = {\n+    val producerIdentity = toProducerIdentity(transactionalId)\n+    producerIdentity.split(\"-\", 2)(0)\n+  }\n+\n+  def toTaskIndex(transactionalId: String): String = {\n+    val producerIdentity = toProducerIdentity(transactionalId)\n+    producerIdentity.split(\"-\", 2)(1)\n+  }\n+\n+  def toProducerIdentity(executorId: String, taskIndex: String): String = {\n+    s\"$executorId-$taskIndex\"\n+  }\n+}\n+\n+/**\n+ * A [[DataWriter]] for Kafka transactional writing. One data writer will be created\n+ * in each partition to process incoming rows.\n+ *\n+ * @param targetTopic The topic that this data writer is targeting. If None, topic will be inferred\n+ *                    from a `topic` field in the incoming data.\n+ * @param producerParams Parameters to use for the Kafka producer.\n+ * @param inputSchema The attributes in the input data.\n+ */\n+private[kafka010] class KafkaTransactionDataWriter(\n+    targetTopic: Option[String],\n+    producerParams: ju.Map[String, Object],\n+    inputSchema: Seq[Attribute])\n+  extends KafkaRowWriter(inputSchema, targetTopic) with DataWriter[InternalRow] {\n+\n+  private lazy val producer = {\n+    val kafkaProducer = CachedKafkaProducer.getOrCreate(producerParams)",
    "line": 83
  }, {
    "author": {
      "login": "wenxuanguan"
    },
    "body": "I have considered kafka producer per executor. But there will be data loss to abort transaction when multiple task share one transaction, and some task failed and retry in other executor.\r\nSo to avoid create too many producer, task will reuse the created producer. And the config `producer.create.factor` will limit producer total number in abnormal scene, such as long tail task.",
    "commit": "4ce3967db60a807c07ec892e0594ba5e06b1621a",
    "createdAt": "2019-08-30T01:57:26Z",
    "diffHunk": "@@ -18,16 +18,160 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import com.google.common.cache._\n \n import org.apache.spark.sql.catalyst.InternalRow\n import org.apache.spark.sql.catalyst.expressions.Attribute\n import org.apache.spark.sql.sources.v2.writer._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A [[WriterCommitMessage]] for Kafka commit message.\n+ * @param transactionalId Unique transactionalId for each producer.\n+ * @param epoch Transactional epoch.\n+ * @param producerId Transactional producerId for producer, got when init transaction.\n+ */\n+private[kafka010] case class ProducerTransactionMetaData(\n+    transactionalId: String,\n+    epoch: Short,\n+    producerId: Long)\n+  extends WriterCommitMessage\n+\n+/**\n+ * Emtpy commit message for resume transaction.\n+ */\n+private case object EmptyCommitMessage extends WriterCommitMessage\n+\n+private[kafka010] case object ProducerTransactionMetaData {\n+  val VERSION = 1\n+\n+  def toTransactionId(\n+      executorId: String,\n+      taskIndex: String,\n+      transactionalIdSuffix: String): String = {\n+    toTransactionId(toProducerIdentity(executorId, taskIndex), transactionalIdSuffix)\n+  }\n+\n+  def toTransactionId(producerIdentity: String, transactionalIdSuffix: String): String = {\n+    s\"$producerIdentity||$transactionalIdSuffix\"\n+  }\n+\n+  def toTransactionalIdSuffix(transactionalId: String): String = {\n+    transactionalId.split(\"\\\\|\\\\|\", 2)(1)\n+  }\n+\n+  def toProducerIdentity(transactionalId: String): String = {\n+    transactionalId.split(\"\\\\|\\\\|\", 2)(0)\n+  }\n+\n+  def toExecutorId(transactionalId: String): String = {\n+    val producerIdentity = toProducerIdentity(transactionalId)\n+    producerIdentity.split(\"-\", 2)(0)\n+  }\n+\n+  def toTaskIndex(transactionalId: String): String = {\n+    val producerIdentity = toProducerIdentity(transactionalId)\n+    producerIdentity.split(\"-\", 2)(1)\n+  }\n+\n+  def toProducerIdentity(executorId: String, taskIndex: String): String = {\n+    s\"$executorId-$taskIndex\"\n+  }\n+}\n+\n+/**\n+ * A [[DataWriter]] for Kafka transactional writing. One data writer will be created\n+ * in each partition to process incoming rows.\n+ *\n+ * @param targetTopic The topic that this data writer is targeting. If None, topic will be inferred\n+ *                    from a `topic` field in the incoming data.\n+ * @param producerParams Parameters to use for the Kafka producer.\n+ * @param inputSchema The attributes in the input data.\n+ */\n+private[kafka010] class KafkaTransactionDataWriter(\n+    targetTopic: Option[String],\n+    producerParams: ju.Map[String, Object],\n+    inputSchema: Seq[Attribute])\n+  extends KafkaRowWriter(inputSchema, targetTopic) with DataWriter[InternalRow] {\n+\n+  private lazy val producer = {\n+    val kafkaProducer = CachedKafkaProducer.getOrCreate(producerParams)",
    "line": 83
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "What I meant is Spark shares Kafka producer in multi-threads in executor once `producerParams` is same. So what you considered is exactly what Spark is doing now. (I meant to point out this.) According to your explanation, caching logic should be changed to restrict multi-threads usage.",
    "commit": "4ce3967db60a807c07ec892e0594ba5e06b1621a",
    "createdAt": "2019-08-30T02:08:17Z",
    "diffHunk": "@@ -18,16 +18,160 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import com.google.common.cache._\n \n import org.apache.spark.sql.catalyst.InternalRow\n import org.apache.spark.sql.catalyst.expressions.Attribute\n import org.apache.spark.sql.sources.v2.writer._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A [[WriterCommitMessage]] for Kafka commit message.\n+ * @param transactionalId Unique transactionalId for each producer.\n+ * @param epoch Transactional epoch.\n+ * @param producerId Transactional producerId for producer, got when init transaction.\n+ */\n+private[kafka010] case class ProducerTransactionMetaData(\n+    transactionalId: String,\n+    epoch: Short,\n+    producerId: Long)\n+  extends WriterCommitMessage\n+\n+/**\n+ * Emtpy commit message for resume transaction.\n+ */\n+private case object EmptyCommitMessage extends WriterCommitMessage\n+\n+private[kafka010] case object ProducerTransactionMetaData {\n+  val VERSION = 1\n+\n+  def toTransactionId(\n+      executorId: String,\n+      taskIndex: String,\n+      transactionalIdSuffix: String): String = {\n+    toTransactionId(toProducerIdentity(executorId, taskIndex), transactionalIdSuffix)\n+  }\n+\n+  def toTransactionId(producerIdentity: String, transactionalIdSuffix: String): String = {\n+    s\"$producerIdentity||$transactionalIdSuffix\"\n+  }\n+\n+  def toTransactionalIdSuffix(transactionalId: String): String = {\n+    transactionalId.split(\"\\\\|\\\\|\", 2)(1)\n+  }\n+\n+  def toProducerIdentity(transactionalId: String): String = {\n+    transactionalId.split(\"\\\\|\\\\|\", 2)(0)\n+  }\n+\n+  def toExecutorId(transactionalId: String): String = {\n+    val producerIdentity = toProducerIdentity(transactionalId)\n+    producerIdentity.split(\"-\", 2)(0)\n+  }\n+\n+  def toTaskIndex(transactionalId: String): String = {\n+    val producerIdentity = toProducerIdentity(transactionalId)\n+    producerIdentity.split(\"-\", 2)(1)\n+  }\n+\n+  def toProducerIdentity(executorId: String, taskIndex: String): String = {\n+    s\"$executorId-$taskIndex\"\n+  }\n+}\n+\n+/**\n+ * A [[DataWriter]] for Kafka transactional writing. One data writer will be created\n+ * in each partition to process incoming rows.\n+ *\n+ * @param targetTopic The topic that this data writer is targeting. If None, topic will be inferred\n+ *                    from a `topic` field in the incoming data.\n+ * @param producerParams Parameters to use for the Kafka producer.\n+ * @param inputSchema The attributes in the input data.\n+ */\n+private[kafka010] class KafkaTransactionDataWriter(\n+    targetTopic: Option[String],\n+    producerParams: ju.Map[String, Object],\n+    inputSchema: Seq[Attribute])\n+  extends KafkaRowWriter(inputSchema, targetTopic) with DataWriter[InternalRow] {\n+\n+  private lazy val producer = {\n+    val kafkaProducer = CachedKafkaProducer.getOrCreate(producerParams)",
    "line": 83
  }, {
    "author": {
      "login": "wenxuanguan"
    },
    "body": "I think caching logic is ok and we can control producer creation per task, and also failover with transactional.id in producerParams.\r\nTransaction producer is not thread safe, so what I do is one producer per task in one micro-batch, and in next batch reused the created producer instead of recreate one since transaction is complete in every micro-batch.  With producerParams, transactional.id is different between tasks in one micro-batch, but same in the next micro-batch.\r\nAnd if task number is same for every executor in every micro-batch, no more producer will be created except the first micro-batch. ",
    "commit": "4ce3967db60a807c07ec892e0594ba5e06b1621a",
    "createdAt": "2019-08-30T03:08:24Z",
    "diffHunk": "@@ -18,16 +18,160 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import com.google.common.cache._\n \n import org.apache.spark.sql.catalyst.InternalRow\n import org.apache.spark.sql.catalyst.expressions.Attribute\n import org.apache.spark.sql.sources.v2.writer._\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * A [[WriterCommitMessage]] for Kafka commit message.\n+ * @param transactionalId Unique transactionalId for each producer.\n+ * @param epoch Transactional epoch.\n+ * @param producerId Transactional producerId for producer, got when init transaction.\n+ */\n+private[kafka010] case class ProducerTransactionMetaData(\n+    transactionalId: String,\n+    epoch: Short,\n+    producerId: Long)\n+  extends WriterCommitMessage\n+\n+/**\n+ * Emtpy commit message for resume transaction.\n+ */\n+private case object EmptyCommitMessage extends WriterCommitMessage\n+\n+private[kafka010] case object ProducerTransactionMetaData {\n+  val VERSION = 1\n+\n+  def toTransactionId(\n+      executorId: String,\n+      taskIndex: String,\n+      transactionalIdSuffix: String): String = {\n+    toTransactionId(toProducerIdentity(executorId, taskIndex), transactionalIdSuffix)\n+  }\n+\n+  def toTransactionId(producerIdentity: String, transactionalIdSuffix: String): String = {\n+    s\"$producerIdentity||$transactionalIdSuffix\"\n+  }\n+\n+  def toTransactionalIdSuffix(transactionalId: String): String = {\n+    transactionalId.split(\"\\\\|\\\\|\", 2)(1)\n+  }\n+\n+  def toProducerIdentity(transactionalId: String): String = {\n+    transactionalId.split(\"\\\\|\\\\|\", 2)(0)\n+  }\n+\n+  def toExecutorId(transactionalId: String): String = {\n+    val producerIdentity = toProducerIdentity(transactionalId)\n+    producerIdentity.split(\"-\", 2)(0)\n+  }\n+\n+  def toTaskIndex(transactionalId: String): String = {\n+    val producerIdentity = toProducerIdentity(transactionalId)\n+    producerIdentity.split(\"-\", 2)(1)\n+  }\n+\n+  def toProducerIdentity(executorId: String, taskIndex: String): String = {\n+    s\"$executorId-$taskIndex\"\n+  }\n+}\n+\n+/**\n+ * A [[DataWriter]] for Kafka transactional writing. One data writer will be created\n+ * in each partition to process incoming rows.\n+ *\n+ * @param targetTopic The topic that this data writer is targeting. If None, topic will be inferred\n+ *                    from a `topic` field in the incoming data.\n+ * @param producerParams Parameters to use for the Kafka producer.\n+ * @param inputSchema The attributes in the input data.\n+ */\n+private[kafka010] class KafkaTransactionDataWriter(\n+    targetTopic: Option[String],\n+    producerParams: ju.Map[String, Object],\n+    inputSchema: Seq[Attribute])\n+  extends KafkaRowWriter(inputSchema, targetTopic) with DataWriter[InternalRow] {\n+\n+  private lazy val producer = {\n+    val kafkaProducer = CachedKafkaProducer.getOrCreate(producerParams)",
    "line": 83
  }],
  "prId": 25618
}]