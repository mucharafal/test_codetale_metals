[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "I know that there are docs in the DataSourceV2  API classes, but can you add brief docs to these classes to make it easier to understand how these class relate to each other and how they are used? Otherwise jumping back and forth to the superclases to read the docs is tricky.",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T02:24:32Z",
    "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.clients.producer.{Callback, ProducerRecord, RecordMetadata}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection}\n+import org.apache.spark.sql.kafka010.KafkaSourceProvider.{kafkaParamsForProducer, TOPIC_OPTION_KEY}\n+import org.apache.spark.sql.sources.v2.streaming.writer.ContinuousWriter\n+import org.apache.spark.sql.sources.v2.writer._\n+import org.apache.spark.sql.streaming.OutputMode\n+import org.apache.spark.sql.types.{BinaryType, StringType, StructType}\n+\n+class ContinuousKafkaWriter(\n+    topic: Option[String], producerParams: Map[String, String], schema: StructType)\n+  extends ContinuousWriter with SupportsWriteInternalRow {\n+\n+  override def createInternalRowWriterFactory(): KafkaWriterFactory =\n+    KafkaWriterFactory(topic, producerParams, schema)\n+\n+  override def commit(epochId: Long, messages: Array[WriterCommitMessage]): Unit = {}\n+  override def abort(messages: Array[WriterCommitMessage]): Unit = {}\n+}\n+\n+case class KafkaWriterFactory("
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Also make it KafkaContinuousDataWriterFactory to make it clear it is factory of KafkaContinuousDataWriter and not for any other writer. ",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T19:31:44Z",
    "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.clients.producer.{Callback, ProducerRecord, RecordMetadata}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection}\n+import org.apache.spark.sql.kafka010.KafkaSourceProvider.{kafkaParamsForProducer, TOPIC_OPTION_KEY}\n+import org.apache.spark.sql.sources.v2.streaming.writer.ContinuousWriter\n+import org.apache.spark.sql.sources.v2.writer._\n+import org.apache.spark.sql.streaming.OutputMode\n+import org.apache.spark.sql.types.{BinaryType, StringType, StructType}\n+\n+class ContinuousKafkaWriter(\n+    topic: Option[String], producerParams: Map[String, String], schema: StructType)\n+  extends ContinuousWriter with SupportsWriteInternalRow {\n+\n+  override def createInternalRowWriterFactory(): KafkaWriterFactory =\n+    KafkaWriterFactory(topic, producerParams, schema)\n+\n+  override def commit(epochId: Long, messages: Array[WriterCommitMessage]): Unit = {}\n+  override def abort(messages: Array[WriterCommitMessage]): Unit = {}\n+}\n+\n+case class KafkaWriterFactory("
  }],
  "prId": 20096
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "If there is no param, then this can be a `case object`. And I dont think `{}` is needed.  ",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T18:54:22Z",
    "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.clients.producer.{Callback, ProducerRecord, RecordMetadata}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection}\n+import org.apache.spark.sql.kafka010.KafkaSourceProvider.{kafkaParamsForProducer, TOPIC_OPTION_KEY}\n+import org.apache.spark.sql.sources.v2.streaming.writer.ContinuousWriter\n+import org.apache.spark.sql.sources.v2.writer._\n+import org.apache.spark.sql.streaming.OutputMode\n+import org.apache.spark.sql.types.{BinaryType, StringType, StructType}\n+\n+class ContinuousKafkaWriter(\n+    topic: Option[String], producerParams: Map[String, String], schema: StructType)\n+  extends ContinuousWriter with SupportsWriteInternalRow {\n+\n+  override def createInternalRowWriterFactory(): KafkaWriterFactory =\n+    KafkaWriterFactory(topic, producerParams, schema)\n+\n+  override def commit(epochId: Long, messages: Array[WriterCommitMessage]): Unit = {}\n+  override def abort(messages: Array[WriterCommitMessage]): Unit = {}\n+}\n+\n+case class KafkaWriterFactory(\n+    topic: Option[String], producerParams: Map[String, String], schema: StructType)\n+  extends DataWriterFactory[InternalRow] {\n+\n+  override def createDataWriter(partitionId: Int, attemptNumber: Int): DataWriter[InternalRow] = {\n+    new KafkaDataWriter(topic, producerParams, schema.toAttributes)\n+  }\n+}\n+\n+case class KafkaWriterCommitMessage() extends WriterCommitMessage {}"
  }],
  "prId": 20096
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Rename to KafkaContinuousWriter to make it easier to find.",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T19:19:44Z",
    "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.clients.producer.{Callback, ProducerRecord, RecordMetadata}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection}\n+import org.apache.spark.sql.kafka010.KafkaSourceProvider.{kafkaParamsForProducer, TOPIC_OPTION_KEY}\n+import org.apache.spark.sql.sources.v2.streaming.writer.ContinuousWriter\n+import org.apache.spark.sql.sources.v2.writer._\n+import org.apache.spark.sql.streaming.OutputMode\n+import org.apache.spark.sql.types.{BinaryType, StringType, StructType}\n+\n+class ContinuousKafkaWriter("
  }],
  "prId": 20096
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "I am guessing this class is solely for continuous writing and not for any other kind of writing. If so, better to make it clear in the name ... KafkaContinuousDataWriter.",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T19:33:05Z",
    "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.clients.producer.{Callback, ProducerRecord, RecordMetadata}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection}\n+import org.apache.spark.sql.kafka010.KafkaSourceProvider.{kafkaParamsForProducer, TOPIC_OPTION_KEY}\n+import org.apache.spark.sql.sources.v2.streaming.writer.ContinuousWriter\n+import org.apache.spark.sql.sources.v2.writer._\n+import org.apache.spark.sql.streaming.OutputMode\n+import org.apache.spark.sql.types.{BinaryType, StringType, StructType}\n+\n+class ContinuousKafkaWriter(\n+    topic: Option[String], producerParams: Map[String, String], schema: StructType)\n+  extends ContinuousWriter with SupportsWriteInternalRow {\n+\n+  override def createInternalRowWriterFactory(): KafkaWriterFactory =\n+    KafkaWriterFactory(topic, producerParams, schema)\n+\n+  override def commit(epochId: Long, messages: Array[WriterCommitMessage]): Unit = {}\n+  override def abort(messages: Array[WriterCommitMessage]): Unit = {}\n+}\n+\n+case class KafkaWriterFactory(\n+    topic: Option[String], producerParams: Map[String, String], schema: StructType)\n+  extends DataWriterFactory[InternalRow] {\n+\n+  override def createDataWriter(partitionId: Int, attemptNumber: Int): DataWriter[InternalRow] = {\n+    new KafkaDataWriter(topic, producerParams, schema.toAttributes)\n+  }\n+}\n+\n+case class KafkaWriterCommitMessage() extends WriterCommitMessage {}\n+\n+class KafkaDataWriter("
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "also add docs.",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T19:42:30Z",
    "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.clients.producer.{Callback, ProducerRecord, RecordMetadata}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection}\n+import org.apache.spark.sql.kafka010.KafkaSourceProvider.{kafkaParamsForProducer, TOPIC_OPTION_KEY}\n+import org.apache.spark.sql.sources.v2.streaming.writer.ContinuousWriter\n+import org.apache.spark.sql.sources.v2.writer._\n+import org.apache.spark.sql.streaming.OutputMode\n+import org.apache.spark.sql.types.{BinaryType, StringType, StructType}\n+\n+class ContinuousKafkaWriter(\n+    topic: Option[String], producerParams: Map[String, String], schema: StructType)\n+  extends ContinuousWriter with SupportsWriteInternalRow {\n+\n+  override def createInternalRowWriterFactory(): KafkaWriterFactory =\n+    KafkaWriterFactory(topic, producerParams, schema)\n+\n+  override def commit(epochId: Long, messages: Array[WriterCommitMessage]): Unit = {}\n+  override def abort(messages: Array[WriterCommitMessage]): Unit = {}\n+}\n+\n+case class KafkaWriterFactory(\n+    topic: Option[String], producerParams: Map[String, String], schema: StructType)\n+  extends DataWriterFactory[InternalRow] {\n+\n+  override def createDataWriter(partitionId: Int, attemptNumber: Int): DataWriter[InternalRow] = {\n+    new KafkaDataWriter(topic, producerParams, schema.toAttributes)\n+  }\n+}\n+\n+case class KafkaWriterCommitMessage() extends WriterCommitMessage {}\n+\n+class KafkaDataWriter("
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "A lot of the code in this class is duplicated with KafkaWriter. Is it possible to create a common base class?\r\n  ",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T20:43:10Z",
    "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.clients.producer.{Callback, ProducerRecord, RecordMetadata}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection}\n+import org.apache.spark.sql.kafka010.KafkaSourceProvider.{kafkaParamsForProducer, TOPIC_OPTION_KEY}\n+import org.apache.spark.sql.sources.v2.streaming.writer.ContinuousWriter\n+import org.apache.spark.sql.sources.v2.writer._\n+import org.apache.spark.sql.streaming.OutputMode\n+import org.apache.spark.sql.types.{BinaryType, StringType, StructType}\n+\n+class ContinuousKafkaWriter(\n+    topic: Option[String], producerParams: Map[String, String], schema: StructType)\n+  extends ContinuousWriter with SupportsWriteInternalRow {\n+\n+  override def createInternalRowWriterFactory(): KafkaWriterFactory =\n+    KafkaWriterFactory(topic, producerParams, schema)\n+\n+  override def commit(epochId: Long, messages: Array[WriterCommitMessage]): Unit = {}\n+  override def abort(messages: Array[WriterCommitMessage]): Unit = {}\n+}\n+\n+case class KafkaWriterFactory(\n+    topic: Option[String], producerParams: Map[String, String], schema: StructType)\n+  extends DataWriterFactory[InternalRow] {\n+\n+  override def createDataWriter(partitionId: Int, attemptNumber: Int): DataWriter[InternalRow] = {\n+    new KafkaDataWriter(topic, producerParams, schema.toAttributes)\n+  }\n+}\n+\n+case class KafkaWriterCommitMessage() extends WriterCommitMessage {}\n+\n+class KafkaDataWriter("
  }],
  "prId": 20096
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "insert space",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T19:40:09Z",
    "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.clients.producer.{Callback, ProducerRecord, RecordMetadata}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection}\n+import org.apache.spark.sql.kafka010.KafkaSourceProvider.{kafkaParamsForProducer, TOPIC_OPTION_KEY}\n+import org.apache.spark.sql.sources.v2.streaming.writer.ContinuousWriter\n+import org.apache.spark.sql.sources.v2.writer._\n+import org.apache.spark.sql.streaming.OutputMode\n+import org.apache.spark.sql.types.{BinaryType, StringType, StructType}\n+\n+class ContinuousKafkaWriter(\n+    topic: Option[String], producerParams: Map[String, String], schema: StructType)\n+  extends ContinuousWriter with SupportsWriteInternalRow {\n+\n+  override def createInternalRowWriterFactory(): KafkaWriterFactory =\n+    KafkaWriterFactory(topic, producerParams, schema)\n+\n+  override def commit(epochId: Long, messages: Array[WriterCommitMessage]): Unit = {}\n+  override def abort(messages: Array[WriterCommitMessage]): Unit = {}\n+}\n+\n+case class KafkaWriterFactory(\n+    topic: Option[String], producerParams: Map[String, String], schema: StructType)\n+  extends DataWriterFactory[InternalRow] {\n+\n+  override def createDataWriter(partitionId: Int, attemptNumber: Int): DataWriter[InternalRow] = {\n+    new KafkaDataWriter(topic, producerParams, schema.toAttributes)\n+  }\n+}\n+\n+case class KafkaWriterCommitMessage() extends WriterCommitMessage {}\n+\n+class KafkaDataWriter(\n+    topic: Option[String], producerParams: Map[String, String], inputSchema: Seq[Attribute])\n+  extends DataWriter[InternalRow] {\n+  import scala.collection.JavaConverters._\n+\n+  @volatile private var failedWrite: Exception = _\n+  private val projection = createProjection\n+  private lazy val producer = CachedKafkaProducer.getOrCreate(\n+    new java.util.HashMap[String, Object](producerParams.asJava))\n+\n+  private val callback = new Callback() {\n+    override def onCompletion(recordMetadata: RecordMetadata, e: Exception): Unit = {\n+      if (failedWrite == null && e != null) {\n+        failedWrite = e\n+      }\n+    }\n+  }\n+\n+  def write(row: InternalRow): Unit = {\n+    if (failedWrite != null) return\n+\n+    val projectedRow = projection(row)\n+    val topic = projectedRow.getUTF8String(0)\n+    val key = projectedRow.getBinary(1)\n+    val value = projectedRow.getBinary(2)\n+\n+    if (topic == null) {\n+      throw new NullPointerException(s\"null topic present in the data. Use the \" +\n+        s\"${KafkaSourceProvider.TOPIC_OPTION_KEY} option for setting a default topic.\")\n+    }\n+    val record = new ProducerRecord[Array[Byte], Array[Byte]](topic.toString, key, value)\n+    producer.send(record, callback)\n+  }\n+\n+  def commit(): WriterCommitMessage = KafkaWriterCommitMessage()\n+  def abort(): Unit = {}"
  }],
  "prId": 20096
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "This whole method is duplicate from KafkaWriterTask. I mentioned in another comment, but we can probably dedup it with a common base class.",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T19:42:21Z",
    "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.clients.producer.{Callback, ProducerRecord, RecordMetadata}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection}\n+import org.apache.spark.sql.kafka010.KafkaSourceProvider.{kafkaParamsForProducer, TOPIC_OPTION_KEY}\n+import org.apache.spark.sql.sources.v2.streaming.writer.ContinuousWriter\n+import org.apache.spark.sql.sources.v2.writer._\n+import org.apache.spark.sql.streaming.OutputMode\n+import org.apache.spark.sql.types.{BinaryType, StringType, StructType}\n+\n+class ContinuousKafkaWriter(\n+    topic: Option[String], producerParams: Map[String, String], schema: StructType)\n+  extends ContinuousWriter with SupportsWriteInternalRow {\n+\n+  override def createInternalRowWriterFactory(): KafkaWriterFactory =\n+    KafkaWriterFactory(topic, producerParams, schema)\n+\n+  override def commit(epochId: Long, messages: Array[WriterCommitMessage]): Unit = {}\n+  override def abort(messages: Array[WriterCommitMessage]): Unit = {}\n+}\n+\n+case class KafkaWriterFactory(\n+    topic: Option[String], producerParams: Map[String, String], schema: StructType)\n+  extends DataWriterFactory[InternalRow] {\n+\n+  override def createDataWriter(partitionId: Int, attemptNumber: Int): DataWriter[InternalRow] = {\n+    new KafkaDataWriter(topic, producerParams, schema.toAttributes)\n+  }\n+}\n+\n+case class KafkaWriterCommitMessage() extends WriterCommitMessage {}\n+\n+class KafkaDataWriter(\n+    topic: Option[String], producerParams: Map[String, String], inputSchema: Seq[Attribute])\n+  extends DataWriter[InternalRow] {\n+  import scala.collection.JavaConverters._\n+\n+  @volatile private var failedWrite: Exception = _\n+  private val projection = createProjection\n+  private lazy val producer = CachedKafkaProducer.getOrCreate(\n+    new java.util.HashMap[String, Object](producerParams.asJava))\n+\n+  private val callback = new Callback() {\n+    override def onCompletion(recordMetadata: RecordMetadata, e: Exception): Unit = {\n+      if (failedWrite == null && e != null) {\n+        failedWrite = e\n+      }\n+    }\n+  }\n+\n+  def write(row: InternalRow): Unit = {\n+    if (failedWrite != null) return\n+\n+    val projectedRow = projection(row)\n+    val topic = projectedRow.getUTF8String(0)\n+    val key = projectedRow.getBinary(1)\n+    val value = projectedRow.getBinary(2)\n+\n+    if (topic == null) {\n+      throw new NullPointerException(s\"null topic present in the data. Use the \" +\n+        s\"${KafkaSourceProvider.TOPIC_OPTION_KEY} option for setting a default topic.\")\n+    }\n+    val record = new ProducerRecord[Array[Byte], Array[Byte]](topic.toString, key, value)\n+    producer.send(record, callback)\n+  }\n+\n+  def commit(): WriterCommitMessage = KafkaWriterCommitMessage()\n+  def abort(): Unit = {}\n+\n+  def close(): Unit = {\n+    checkForErrors()\n+    if (producer != null) {\n+      producer.flush()\n+      checkForErrors()\n+    }\n+  }\n+\n+  private def createProjection: UnsafeProjection = {"
  }],
  "prId": 20096
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Maybe rename this file to KafkaContinuousWriter (since there is no class called KafkaSinkV2). \r\n",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T19:53:13Z",
    "diffHunk": "@@ -0,0 +1,142 @@\n+/*"
  }],
  "prId": 20096
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "throw the exception immediately",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T21:22:34Z",
    "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.clients.producer.{Callback, ProducerRecord, RecordMetadata}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection}\n+import org.apache.spark.sql.kafka010.KafkaSourceProvider.{kafkaParamsForProducer, TOPIC_OPTION_KEY}\n+import org.apache.spark.sql.sources.v2.streaming.writer.ContinuousWriter\n+import org.apache.spark.sql.sources.v2.writer._\n+import org.apache.spark.sql.streaming.OutputMode\n+import org.apache.spark.sql.types.{BinaryType, StringType, StructType}\n+\n+class ContinuousKafkaWriter(\n+    topic: Option[String], producerParams: Map[String, String], schema: StructType)\n+  extends ContinuousWriter with SupportsWriteInternalRow {\n+\n+  override def createInternalRowWriterFactory(): KafkaWriterFactory =\n+    KafkaWriterFactory(topic, producerParams, schema)\n+\n+  override def commit(epochId: Long, messages: Array[WriterCommitMessage]): Unit = {}\n+  override def abort(messages: Array[WriterCommitMessage]): Unit = {}\n+}\n+\n+case class KafkaWriterFactory(\n+    topic: Option[String], producerParams: Map[String, String], schema: StructType)\n+  extends DataWriterFactory[InternalRow] {\n+\n+  override def createDataWriter(partitionId: Int, attemptNumber: Int): DataWriter[InternalRow] = {\n+    new KafkaDataWriter(topic, producerParams, schema.toAttributes)\n+  }\n+}\n+\n+case class KafkaWriterCommitMessage() extends WriterCommitMessage {}\n+\n+class KafkaDataWriter(\n+    topic: Option[String], producerParams: Map[String, String], inputSchema: Seq[Attribute])\n+  extends DataWriter[InternalRow] {\n+  import scala.collection.JavaConverters._\n+\n+  @volatile private var failedWrite: Exception = _\n+  private val projection = createProjection\n+  private lazy val producer = CachedKafkaProducer.getOrCreate(\n+    new java.util.HashMap[String, Object](producerParams.asJava))\n+\n+  private val callback = new Callback() {\n+    override def onCompletion(recordMetadata: RecordMetadata, e: Exception): Unit = {\n+      if (failedWrite == null && e != null) {\n+        failedWrite = e\n+      }\n+    }\n+  }\n+\n+  def write(row: InternalRow): Unit = {\n+    if (failedWrite != null) return"
  }],
  "prId": 20096
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "IMPORTANT: commit must flush the producer to ensure all records in the current epoch have been pushed without any error.\r\n  ",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T21:37:59Z",
    "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.clients.producer.{Callback, ProducerRecord, RecordMetadata}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection}\n+import org.apache.spark.sql.kafka010.KafkaSourceProvider.{kafkaParamsForProducer, TOPIC_OPTION_KEY}\n+import org.apache.spark.sql.sources.v2.streaming.writer.ContinuousWriter\n+import org.apache.spark.sql.sources.v2.writer._\n+import org.apache.spark.sql.streaming.OutputMode\n+import org.apache.spark.sql.types.{BinaryType, StringType, StructType}\n+\n+class ContinuousKafkaWriter(\n+    topic: Option[String], producerParams: Map[String, String], schema: StructType)\n+  extends ContinuousWriter with SupportsWriteInternalRow {\n+\n+  override def createInternalRowWriterFactory(): KafkaWriterFactory =\n+    KafkaWriterFactory(topic, producerParams, schema)\n+\n+  override def commit(epochId: Long, messages: Array[WriterCommitMessage]): Unit = {}\n+  override def abort(messages: Array[WriterCommitMessage]): Unit = {}\n+}\n+\n+case class KafkaWriterFactory(\n+    topic: Option[String], producerParams: Map[String, String], schema: StructType)\n+  extends DataWriterFactory[InternalRow] {\n+\n+  override def createDataWriter(partitionId: Int, attemptNumber: Int): DataWriter[InternalRow] = {\n+    new KafkaDataWriter(topic, producerParams, schema.toAttributes)\n+  }\n+}\n+\n+case class KafkaWriterCommitMessage() extends WriterCommitMessage {}\n+\n+class KafkaDataWriter(\n+    topic: Option[String], producerParams: Map[String, String], inputSchema: Seq[Attribute])\n+  extends DataWriter[InternalRow] {\n+  import scala.collection.JavaConverters._\n+\n+  @volatile private var failedWrite: Exception = _\n+  private val projection = createProjection\n+  private lazy val producer = CachedKafkaProducer.getOrCreate(\n+    new java.util.HashMap[String, Object](producerParams.asJava))\n+\n+  private val callback = new Callback() {\n+    override def onCompletion(recordMetadata: RecordMetadata, e: Exception): Unit = {\n+      if (failedWrite == null && e != null) {\n+        failedWrite = e\n+      }\n+    }\n+  }\n+\n+  def write(row: InternalRow): Unit = {\n+    if (failedWrite != null) return\n+\n+    val projectedRow = projection(row)\n+    val topic = projectedRow.getUTF8String(0)\n+    val key = projectedRow.getBinary(1)\n+    val value = projectedRow.getBinary(2)\n+\n+    if (topic == null) {\n+      throw new NullPointerException(s\"null topic present in the data. Use the \" +\n+        s\"${KafkaSourceProvider.TOPIC_OPTION_KEY} option for setting a default topic.\")\n+    }\n+    val record = new ProducerRecord[Array[Byte], Array[Byte]](topic.toString, key, value)\n+    producer.send(record, callback)\n+  }\n+\n+  def commit(): WriterCommitMessage = KafkaWriterCommitMessage()"
  }],
  "prId": 20096
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "you havent closed the producer.",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T21:57:36Z",
    "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.clients.producer.{Callback, ProducerRecord, RecordMetadata}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection}\n+import org.apache.spark.sql.kafka010.KafkaSourceProvider.{kafkaParamsForProducer, TOPIC_OPTION_KEY}\n+import org.apache.spark.sql.sources.v2.streaming.writer.ContinuousWriter\n+import org.apache.spark.sql.sources.v2.writer._\n+import org.apache.spark.sql.streaming.OutputMode\n+import org.apache.spark.sql.types.{BinaryType, StringType, StructType}\n+\n+class ContinuousKafkaWriter(\n+    topic: Option[String], producerParams: Map[String, String], schema: StructType)\n+  extends ContinuousWriter with SupportsWriteInternalRow {\n+\n+  override def createInternalRowWriterFactory(): KafkaWriterFactory =\n+    KafkaWriterFactory(topic, producerParams, schema)\n+\n+  override def commit(epochId: Long, messages: Array[WriterCommitMessage]): Unit = {}\n+  override def abort(messages: Array[WriterCommitMessage]): Unit = {}\n+}\n+\n+case class KafkaWriterFactory(\n+    topic: Option[String], producerParams: Map[String, String], schema: StructType)\n+  extends DataWriterFactory[InternalRow] {\n+\n+  override def createDataWriter(partitionId: Int, attemptNumber: Int): DataWriter[InternalRow] = {\n+    new KafkaDataWriter(topic, producerParams, schema.toAttributes)\n+  }\n+}\n+\n+case class KafkaWriterCommitMessage() extends WriterCommitMessage {}\n+\n+class KafkaDataWriter(\n+    topic: Option[String], producerParams: Map[String, String], inputSchema: Seq[Attribute])\n+  extends DataWriter[InternalRow] {\n+  import scala.collection.JavaConverters._\n+\n+  @volatile private var failedWrite: Exception = _\n+  private val projection = createProjection\n+  private lazy val producer = CachedKafkaProducer.getOrCreate(\n+    new java.util.HashMap[String, Object](producerParams.asJava))\n+\n+  private val callback = new Callback() {\n+    override def onCompletion(recordMetadata: RecordMetadata, e: Exception): Unit = {\n+      if (failedWrite == null && e != null) {\n+        failedWrite = e\n+      }\n+    }\n+  }\n+\n+  def write(row: InternalRow): Unit = {\n+    if (failedWrite != null) return\n+\n+    val projectedRow = projection(row)\n+    val topic = projectedRow.getUTF8String(0)\n+    val key = projectedRow.getBinary(1)\n+    val value = projectedRow.getBinary(2)\n+\n+    if (topic == null) {\n+      throw new NullPointerException(s\"null topic present in the data. Use the \" +\n+        s\"${KafkaSourceProvider.TOPIC_OPTION_KEY} option for setting a default topic.\")\n+    }\n+    val record = new ProducerRecord[Array[Byte], Array[Byte]](topic.toString, key, value)\n+    producer.send(record, callback)\n+  }\n+\n+  def commit(): WriterCommitMessage = KafkaWriterCommitMessage()\n+  def abort(): Unit = {}\n+\n+  def close(): Unit = {\n+    checkForErrors()\n+    if (producer != null) {\n+      producer.flush()\n+      checkForErrors()\n+    }"
  }, {
    "author": {
      "login": "jose-torres"
    },
    "body": "I think CachedKafkaProducer handles closing automatically, but since these are long lived I can do it explicitly too.",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-05T01:42:45Z",
    "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.clients.producer.{Callback, ProducerRecord, RecordMetadata}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection}\n+import org.apache.spark.sql.kafka010.KafkaSourceProvider.{kafkaParamsForProducer, TOPIC_OPTION_KEY}\n+import org.apache.spark.sql.sources.v2.streaming.writer.ContinuousWriter\n+import org.apache.spark.sql.sources.v2.writer._\n+import org.apache.spark.sql.streaming.OutputMode\n+import org.apache.spark.sql.types.{BinaryType, StringType, StructType}\n+\n+class ContinuousKafkaWriter(\n+    topic: Option[String], producerParams: Map[String, String], schema: StructType)\n+  extends ContinuousWriter with SupportsWriteInternalRow {\n+\n+  override def createInternalRowWriterFactory(): KafkaWriterFactory =\n+    KafkaWriterFactory(topic, producerParams, schema)\n+\n+  override def commit(epochId: Long, messages: Array[WriterCommitMessage]): Unit = {}\n+  override def abort(messages: Array[WriterCommitMessage]): Unit = {}\n+}\n+\n+case class KafkaWriterFactory(\n+    topic: Option[String], producerParams: Map[String, String], schema: StructType)\n+  extends DataWriterFactory[InternalRow] {\n+\n+  override def createDataWriter(partitionId: Int, attemptNumber: Int): DataWriter[InternalRow] = {\n+    new KafkaDataWriter(topic, producerParams, schema.toAttributes)\n+  }\n+}\n+\n+case class KafkaWriterCommitMessage() extends WriterCommitMessage {}\n+\n+class KafkaDataWriter(\n+    topic: Option[String], producerParams: Map[String, String], inputSchema: Seq[Attribute])\n+  extends DataWriter[InternalRow] {\n+  import scala.collection.JavaConverters._\n+\n+  @volatile private var failedWrite: Exception = _\n+  private val projection = createProjection\n+  private lazy val producer = CachedKafkaProducer.getOrCreate(\n+    new java.util.HashMap[String, Object](producerParams.asJava))\n+\n+  private val callback = new Callback() {\n+    override def onCompletion(recordMetadata: RecordMetadata, e: Exception): Unit = {\n+      if (failedWrite == null && e != null) {\n+        failedWrite = e\n+      }\n+    }\n+  }\n+\n+  def write(row: InternalRow): Unit = {\n+    if (failedWrite != null) return\n+\n+    val projectedRow = projection(row)\n+    val topic = projectedRow.getUTF8String(0)\n+    val key = projectedRow.getBinary(1)\n+    val value = projectedRow.getBinary(2)\n+\n+    if (topic == null) {\n+      throw new NullPointerException(s\"null topic present in the data. Use the \" +\n+        s\"${KafkaSourceProvider.TOPIC_OPTION_KEY} option for setting a default topic.\")\n+    }\n+    val record = new ProducerRecord[Array[Byte], Array[Byte]](topic.toString, key, value)\n+    producer.send(record, callback)\n+  }\n+\n+  def commit(): WriterCommitMessage = KafkaWriterCommitMessage()\n+  def abort(): Unit = {}\n+\n+  def close(): Unit = {\n+    checkForErrors()\n+    if (producer != null) {\n+      producer.flush()\n+      checkForErrors()\n+    }"
  }],
  "prId": 20096
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "this topic variable overshadows the constructor param topic. i know that this pattern was present in the KafkaWriterTask, but lets not repeat the mistakes of the past. we can fix KafkaWriterTask once we migrate that to v2.",
    "commit": "f94b53e3ab7e37fdcb9f34cf7d1313a4905fa341",
    "createdAt": "2018-01-04T21:58:54Z",
    "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.kafka010\n+\n+import org.apache.kafka.clients.producer.{Callback, ProducerRecord, RecordMetadata}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{Row, SparkSession}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Cast, Literal, UnsafeProjection}\n+import org.apache.spark.sql.kafka010.KafkaSourceProvider.{kafkaParamsForProducer, TOPIC_OPTION_KEY}\n+import org.apache.spark.sql.sources.v2.streaming.writer.ContinuousWriter\n+import org.apache.spark.sql.sources.v2.writer._\n+import org.apache.spark.sql.streaming.OutputMode\n+import org.apache.spark.sql.types.{BinaryType, StringType, StructType}\n+\n+class ContinuousKafkaWriter(\n+    topic: Option[String], producerParams: Map[String, String], schema: StructType)\n+  extends ContinuousWriter with SupportsWriteInternalRow {\n+\n+  override def createInternalRowWriterFactory(): KafkaWriterFactory =\n+    KafkaWriterFactory(topic, producerParams, schema)\n+\n+  override def commit(epochId: Long, messages: Array[WriterCommitMessage]): Unit = {}\n+  override def abort(messages: Array[WriterCommitMessage]): Unit = {}\n+}\n+\n+case class KafkaWriterFactory(\n+    topic: Option[String], producerParams: Map[String, String], schema: StructType)\n+  extends DataWriterFactory[InternalRow] {\n+\n+  override def createDataWriter(partitionId: Int, attemptNumber: Int): DataWriter[InternalRow] = {\n+    new KafkaDataWriter(topic, producerParams, schema.toAttributes)\n+  }\n+}\n+\n+case class KafkaWriterCommitMessage() extends WriterCommitMessage {}\n+\n+class KafkaDataWriter(\n+    topic: Option[String], producerParams: Map[String, String], inputSchema: Seq[Attribute])\n+  extends DataWriter[InternalRow] {\n+  import scala.collection.JavaConverters._\n+\n+  @volatile private var failedWrite: Exception = _\n+  private val projection = createProjection\n+  private lazy val producer = CachedKafkaProducer.getOrCreate(\n+    new java.util.HashMap[String, Object](producerParams.asJava))\n+\n+  private val callback = new Callback() {\n+    override def onCompletion(recordMetadata: RecordMetadata, e: Exception): Unit = {\n+      if (failedWrite == null && e != null) {\n+        failedWrite = e\n+      }\n+    }\n+  }\n+\n+  def write(row: InternalRow): Unit = {\n+    if (failedWrite != null) return\n+\n+    val projectedRow = projection(row)\n+    val topic = projectedRow.getUTF8String(0)"
  }],
  "prId": 20096
}]