[{
  "comments": [{
    "author": {
      "login": "skonto"
    },
    "body": "Could you print the producer UUID here as well for tracking reasons?",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-07T10:43:58Z",
    "diffHunk": "@@ -86,27 +112,38 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n+  /** For explicitly closing kafka producer, will not close an inuse producer until released. */\n   private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n     val paramsSeq = paramsToSeq(kafkaParams)\n     guavaCache.invalidate(paramsSeq)\n   }\n \n-  /** Auto close on cache evict */\n-  private def close(paramsSeq: Seq[(String, Object)], producer: Producer): Unit = {\n+  /** Close this producer and process pending closes. */\n+  private def close(producer: CachedKafkaProducer): Unit = synchronized {\n     try {\n-      logInfo(s\"Closing the KafkaProducer with params: ${paramsSeq.mkString(\"\\n\")}.\")\n       producer.close()\n+      // Check and close any producers evicted, and pending to be closed.\n+      for (p <- closeQueue.iterator().asScala) {\n+        if (p.inUseCount.intValue() <= 0) {\n+          producer.close()"
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "I want to be consistent too, if everyone agrees one way, I can change it. Currently, I have favoured less verbosity.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-08T11:57:45Z",
    "diffHunk": "@@ -86,27 +112,38 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n+  /** For explicitly closing kafka producer, will not close an inuse producer until released. */\n   private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n     val paramsSeq = paramsToSeq(kafkaParams)\n     guavaCache.invalidate(paramsSeq)\n   }\n \n-  /** Auto close on cache evict */\n-  private def close(paramsSeq: Seq[(String, Object)], producer: Producer): Unit = {\n+  /** Close this producer and process pending closes. */\n+  private def close(producer: CachedKafkaProducer): Unit = synchronized {\n     try {\n-      logInfo(s\"Closing the KafkaProducer with params: ${paramsSeq.mkString(\"\\n\")}.\")\n       producer.close()\n+      // Check and close any producers evicted, and pending to be closed.\n+      for (p <- closeQueue.iterator().asScala) {\n+        if (p.inUseCount.intValue() <= 0) {\n+          producer.close()"
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "Sorry, wrote the message at the wrong place. You can ignore it.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-08T15:45:02Z",
    "diffHunk": "@@ -86,27 +112,38 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n+  /** For explicitly closing kafka producer, will not close an inuse producer until released. */\n   private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n     val paramsSeq = paramsToSeq(kafkaParams)\n     guavaCache.invalidate(paramsSeq)\n   }\n \n-  /** Auto close on cache evict */\n-  private def close(paramsSeq: Seq[(String, Object)], producer: Producer): Unit = {\n+  /** Close this producer and process pending closes. */\n+  private def close(producer: CachedKafkaProducer): Unit = synchronized {\n     try {\n-      logInfo(s\"Closing the KafkaProducer with params: ${paramsSeq.mkString(\"\\n\")}.\")\n       producer.close()\n+      // Check and close any producers evicted, and pending to be closed.\n+      for (p <- closeQueue.iterator().asScala) {\n+        if (p.inUseCount.intValue() <= 0) {\n+          producer.close()"
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "skonto"
    },
    "body": "Could you print the producer UUID here as well for tracking reasons?",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-07T10:44:09Z",
    "diffHunk": "@@ -29,51 +30,76 @@ import scala.util.control.NonFatal\n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(id: String, inUseCount: AtomicInteger,\n+    kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]]) extends Logging {\n+  private var closed: Boolean = false\n+  private def close(): Unit = this.synchronized {\n+    if (!closed) {\n+      closed = true\n+      kafkaProducer.close()\n+      logInfo(s\"Closed kafka producer: $kafkaProducer\")\n+    }\n+  }\n+\n+  def flush(): Unit = {\n+    kafkaProducer.flush()\n+  }\n+\n+  private[kafka010] def isClosed: Boolean = closed\n+}\n \n-  private type Producer = KafkaProducer[Array[Byte], Array[Byte]]\n+private[kafka010] object CachedKafkaProducer extends Logging {\n \n   private lazy val cacheExpireTimeout: Long =\n     SparkEnv.get.conf.getTimeAsMs(\"spark.kafka.producer.cache.timeout\", \"10m\")\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(config: Seq[(String, Object)]): CachedKafkaProducer = {\n       val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n       createKafkaProducer(configMap)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}\")\n+      if (producer.inUseCount.intValue() > 0) {\n+        // When a inuse producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+      } else {\n+        close(producer)"
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "UUID is printed on close.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-08T11:56:11Z",
    "diffHunk": "@@ -29,51 +30,76 @@ import scala.util.control.NonFatal\n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(id: String, inUseCount: AtomicInteger,\n+    kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]]) extends Logging {\n+  private var closed: Boolean = false\n+  private def close(): Unit = this.synchronized {\n+    if (!closed) {\n+      closed = true\n+      kafkaProducer.close()\n+      logInfo(s\"Closed kafka producer: $kafkaProducer\")\n+    }\n+  }\n+\n+  def flush(): Unit = {\n+    kafkaProducer.flush()\n+  }\n+\n+  private[kafka010] def isClosed: Boolean = closed\n+}\n \n-  private type Producer = KafkaProducer[Array[Byte], Array[Byte]]\n+private[kafka010] object CachedKafkaProducer extends Logging {\n \n   private lazy val cacheExpireTimeout: Long =\n     SparkEnv.get.conf.getTimeAsMs(\"spark.kafka.producer.cache.timeout\", \"10m\")\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(config: Seq[(String, Object)]): CachedKafkaProducer = {\n       val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n       createKafkaProducer(configMap)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}\")\n+      if (producer.inUseCount.intValue() > 0) {\n+        // When a inuse producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+      } else {\n+        close(producer)"
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "skonto"
    },
    "body": "Could you add a debug log statement here?",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-07T10:44:23Z",
    "diffHunk": "@@ -29,51 +30,76 @@ import scala.util.control.NonFatal\n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(id: String, inUseCount: AtomicInteger,\n+    kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]]) extends Logging {\n+  private var closed: Boolean = false\n+  private def close(): Unit = this.synchronized {\n+    if (!closed) {\n+      closed = true\n+      kafkaProducer.close()\n+      logInfo(s\"Closed kafka producer: $kafkaProducer\")\n+    }\n+  }\n+\n+  def flush(): Unit = {\n+    kafkaProducer.flush()\n+  }\n+\n+  private[kafka010] def isClosed: Boolean = closed\n+}\n \n-  private type Producer = KafkaProducer[Array[Byte], Array[Byte]]\n+private[kafka010] object CachedKafkaProducer extends Logging {\n \n   private lazy val cacheExpireTimeout: Long =\n     SparkEnv.get.conf.getTimeAsMs(\"spark.kafka.producer.cache.timeout\", \"10m\")\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(config: Seq[(String, Object)]): CachedKafkaProducer = {\n       val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n       createKafkaProducer(configMap)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}\")\n+      if (producer.inUseCount.intValue() > 0) {\n+        // When a inuse producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)",
    "line": 113
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "skonto"
    },
    "body": "Is there a guarantee that this will be called? I can imagine a scenario where long running tasks make all producers to be evicted, then at some point ` producer.inUseCount.decrementAndGet()` is called in the write task for each producer and this causes all of them to be closable but the method here is never called (might be wrong).",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-07T11:03:01Z",
    "diffHunk": "@@ -86,27 +112,38 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n+  /** For explicitly closing kafka producer, will not close an inuse producer until released. */\n   private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n     val paramsSeq = paramsToSeq(kafkaParams)\n     guavaCache.invalidate(paramsSeq)\n   }\n \n-  /** Auto close on cache evict */\n-  private def close(paramsSeq: Seq[(String, Object)], producer: Producer): Unit = {\n+  /** Close this producer and process pending closes. */\n+  private def close(producer: CachedKafkaProducer): Unit = synchronized {"
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "We try to close each time, a producer goes not in use, when not in cache.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-11T05:00:39Z",
    "diffHunk": "@@ -86,27 +112,38 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n+  /** For explicitly closing kafka producer, will not close an inuse producer until released. */\n   private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n     val paramsSeq = paramsToSeq(kafkaParams)\n     guavaCache.invalidate(paramsSeq)\n   }\n \n-  /** Auto close on cache evict */\n-  private def close(paramsSeq: Seq[(String, Object)], producer: Producer): Unit = {\n+  /** Close this producer and process pending closes. */\n+  private def close(producer: CachedKafkaProducer): Unit = synchronized {"
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "What is the concept here? It returns a `Boolean` which never used.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-11T10:42:14Z",
    "diffHunk": "@@ -18,103 +18,180 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n \n-  private type Producer = KafkaProducer[Array[Byte], Array[Byte]]\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  lazy val kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]] = {\n+    val producer = new KafkaProducer[Array[Byte], Array[Byte]](configMap)\n+    logDebug(s\"Created a new instance of KafkaProducer for \" +\n+      s\"$kafkaParams with Id: $id\")\n+    closed = false\n+    producer\n+  }\n+  private var isCached: Boolean = true\n+  private var closed: Boolean = true\n+  private def close(): Unit = {\n+    try {\n+      this.synchronized {\n+        if (!closed) {\n+          closed = true\n+          kafkaProducer.close()\n+          logInfo(s\"Closed kafka producer: $kafkaProducer\")\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error while closing kafka producer with params: $kafkaParams\", e)\n+    }\n+  }\n+\n+  private def inUse(): Boolean = {\n+    inUseCount.get() > 0\n+  }\n+  private def unCache(): Unit = {\n+    isCached = false\n+  }\n+  private[kafka010] def getInUseCount: Int = inUseCount.get()\n+\n+  private[kafka010] def getKafkaParams: Seq[(String, Object)] = kafkaParams\n+\n+  private[kafka010] def flush(): Unit = {\n+    kafkaProducer.flush()\n+  }\n+\n+  private[kafka010] def isClosed: Boolean = closed\n+}\n+\n+private[kafka010] object CachedKafkaProducer extends Logging {\n \n   private val defaultCacheExpireTimeout = TimeUnit.MINUTES.toMillis(10)\n \n   private lazy val cacheExpireTimeout: Long =\n     Option(SparkEnv.get).map(_.conf.getTimeAsMs(\n-      \"spark.kafka.producer.cache.timeout\",\n-      s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n+      key = \"spark.kafka.producer.cache.timeout\",\n+      defaultValue = s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n+\n+  private def updatedAuthConfigIfNeeded(kafkaParamsMap: ju.Map[String, Object]) =\n+    KafkaConfigUpdater(\"executor\", kafkaParamsMap.asScala.toMap)\n+      .setAuthenticationConfigIfNeeded()\n+      .build()\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(updatedAuthConfigIfNeeded(kafkaParamsMap))\n     try {\n-      guavaCache.get(paramsSeq)\n+      val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+      val useCount: Int = cachedKafkaProducer.inUseCount.incrementAndGet()\n+      logDebug(s\"Granted producer $cachedKafkaProducer, inuse-count: $useCount\")\n+      cachedKafkaProducer\n     } catch {\n       case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n         if e.getCause != null =>\n         throw e.getCause\n     }\n   }\n \n-  private def paramsToSeq(kafkaParams: ju.Map[String, Object]): Seq[(String, Object)] = {\n-    val paramsSeq: Seq[(String, Object)] = kafkaParams.asScala.toSeq.sortBy(x => x._1)\n+  private def paramsToSeq(kafkaParamsMap: ju.Map[String, Object]): Seq[(String, Object)] = {\n+    val paramsSeq: Seq[(String, Object)] = kafkaParamsMap.asScala.toSeq.sortBy(x => x._1)\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n-  private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n-    val paramsSeq = paramsToSeq(kafkaParams)\n-    guavaCache.invalidate(paramsSeq)\n+  /* Release a kafka producer back to the kafka cache. We simple decrement it's inuse count. */\n+  private[kafka010] def release(producer: CachedKafkaProducer, offending: Boolean): Boolean = {"
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "I kept it for testing, but can be removed. Since it is not used. Thanks for picking it.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-12T05:16:14Z",
    "diffHunk": "@@ -18,103 +18,180 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n \n-  private type Producer = KafkaProducer[Array[Byte], Array[Byte]]\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  lazy val kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]] = {\n+    val producer = new KafkaProducer[Array[Byte], Array[Byte]](configMap)\n+    logDebug(s\"Created a new instance of KafkaProducer for \" +\n+      s\"$kafkaParams with Id: $id\")\n+    closed = false\n+    producer\n+  }\n+  private var isCached: Boolean = true\n+  private var closed: Boolean = true\n+  private def close(): Unit = {\n+    try {\n+      this.synchronized {\n+        if (!closed) {\n+          closed = true\n+          kafkaProducer.close()\n+          logInfo(s\"Closed kafka producer: $kafkaProducer\")\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error while closing kafka producer with params: $kafkaParams\", e)\n+    }\n+  }\n+\n+  private def inUse(): Boolean = {\n+    inUseCount.get() > 0\n+  }\n+  private def unCache(): Unit = {\n+    isCached = false\n+  }\n+  private[kafka010] def getInUseCount: Int = inUseCount.get()\n+\n+  private[kafka010] def getKafkaParams: Seq[(String, Object)] = kafkaParams\n+\n+  private[kafka010] def flush(): Unit = {\n+    kafkaProducer.flush()\n+  }\n+\n+  private[kafka010] def isClosed: Boolean = closed\n+}\n+\n+private[kafka010] object CachedKafkaProducer extends Logging {\n \n   private val defaultCacheExpireTimeout = TimeUnit.MINUTES.toMillis(10)\n \n   private lazy val cacheExpireTimeout: Long =\n     Option(SparkEnv.get).map(_.conf.getTimeAsMs(\n-      \"spark.kafka.producer.cache.timeout\",\n-      s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n+      key = \"spark.kafka.producer.cache.timeout\",\n+      defaultValue = s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n+\n+  private def updatedAuthConfigIfNeeded(kafkaParamsMap: ju.Map[String, Object]) =\n+    KafkaConfigUpdater(\"executor\", kafkaParamsMap.asScala.toMap)\n+      .setAuthenticationConfigIfNeeded()\n+      .build()\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(updatedAuthConfigIfNeeded(kafkaParamsMap))\n     try {\n-      guavaCache.get(paramsSeq)\n+      val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+      val useCount: Int = cachedKafkaProducer.inUseCount.incrementAndGet()\n+      logDebug(s\"Granted producer $cachedKafkaProducer, inuse-count: $useCount\")\n+      cachedKafkaProducer\n     } catch {\n       case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n         if e.getCause != null =>\n         throw e.getCause\n     }\n   }\n \n-  private def paramsToSeq(kafkaParams: ju.Map[String, Object]): Seq[(String, Object)] = {\n-    val paramsSeq: Seq[(String, Object)] = kafkaParams.asScala.toSeq.sortBy(x => x._1)\n+  private def paramsToSeq(kafkaParamsMap: ju.Map[String, Object]): Seq[(String, Object)] = {\n+    val paramsSeq: Seq[(String, Object)] = kafkaParamsMap.asScala.toSeq.sortBy(x => x._1)\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n-  private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n-    val paramsSeq = paramsToSeq(kafkaParams)\n-    guavaCache.invalidate(paramsSeq)\n+  /* Release a kafka producer back to the kafka cache. We simple decrement it's inuse count. */\n+  private[kafka010] def release(producer: CachedKafkaProducer, offending: Boolean): Boolean = {"
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Why a producer can be null?",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-11T10:45:16Z",
    "diffHunk": "@@ -18,103 +18,180 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n \n-  private type Producer = KafkaProducer[Array[Byte], Array[Byte]]\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  lazy val kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]] = {\n+    val producer = new KafkaProducer[Array[Byte], Array[Byte]](configMap)\n+    logDebug(s\"Created a new instance of KafkaProducer for \" +\n+      s\"$kafkaParams with Id: $id\")\n+    closed = false\n+    producer\n+  }\n+  private var isCached: Boolean = true\n+  private var closed: Boolean = true\n+  private def close(): Unit = {\n+    try {\n+      this.synchronized {\n+        if (!closed) {\n+          closed = true\n+          kafkaProducer.close()\n+          logInfo(s\"Closed kafka producer: $kafkaProducer\")\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error while closing kafka producer with params: $kafkaParams\", e)\n+    }\n+  }\n+\n+  private def inUse(): Boolean = {\n+    inUseCount.get() > 0\n+  }\n+  private def unCache(): Unit = {\n+    isCached = false\n+  }\n+  private[kafka010] def getInUseCount: Int = inUseCount.get()\n+\n+  private[kafka010] def getKafkaParams: Seq[(String, Object)] = kafkaParams\n+\n+  private[kafka010] def flush(): Unit = {\n+    kafkaProducer.flush()\n+  }\n+\n+  private[kafka010] def isClosed: Boolean = closed\n+}\n+\n+private[kafka010] object CachedKafkaProducer extends Logging {\n \n   private val defaultCacheExpireTimeout = TimeUnit.MINUTES.toMillis(10)\n \n   private lazy val cacheExpireTimeout: Long =\n     Option(SparkEnv.get).map(_.conf.getTimeAsMs(\n-      \"spark.kafka.producer.cache.timeout\",\n-      s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n+      key = \"spark.kafka.producer.cache.timeout\",\n+      defaultValue = s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n+\n+  private def updatedAuthConfigIfNeeded(kafkaParamsMap: ju.Map[String, Object]) =\n+    KafkaConfigUpdater(\"executor\", kafkaParamsMap.asScala.toMap)\n+      .setAuthenticationConfigIfNeeded()\n+      .build()\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(updatedAuthConfigIfNeeded(kafkaParamsMap))\n     try {\n-      guavaCache.get(paramsSeq)\n+      val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+      val useCount: Int = cachedKafkaProducer.inUseCount.incrementAndGet()\n+      logDebug(s\"Granted producer $cachedKafkaProducer, inuse-count: $useCount\")\n+      cachedKafkaProducer\n     } catch {\n       case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n         if e.getCause != null =>\n         throw e.getCause\n     }\n   }\n \n-  private def paramsToSeq(kafkaParams: ju.Map[String, Object]): Seq[(String, Object)] = {\n-    val paramsSeq: Seq[(String, Object)] = kafkaParams.asScala.toSeq.sortBy(x => x._1)\n+  private def paramsToSeq(kafkaParamsMap: ju.Map[String, Object]): Seq[(String, Object)] = {\n+    val paramsSeq: Seq[(String, Object)] = kafkaParamsMap.asScala.toSeq.sortBy(x => x._1)\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n-  private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n-    val paramsSeq = paramsToSeq(kafkaParams)\n-    guavaCache.invalidate(paramsSeq)\n+  /* Release a kafka producer back to the kafka cache. We simple decrement it's inuse count. */\n+  private[kafka010] def release(producer: CachedKafkaProducer, offending: Boolean): Boolean = {\n+    if (producer != null) {"
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Why looking into the cache with delegation token? If delegation token changes it will end-up in missing producer. Let's take the following example:\r\n* App starts with token X\r\n* Cached consumer created with kafka params including token X\r\n* Spark renews the token to Y and sends it across the cluster\r\n* A task comes and looks for the producer with kafka params including token Y\r\n",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-11T10:49:23Z",
    "diffHunk": "@@ -18,103 +18,180 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n \n-  private type Producer = KafkaProducer[Array[Byte], Array[Byte]]\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  lazy val kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]] = {\n+    val producer = new KafkaProducer[Array[Byte], Array[Byte]](configMap)\n+    logDebug(s\"Created a new instance of KafkaProducer for \" +\n+      s\"$kafkaParams with Id: $id\")\n+    closed = false\n+    producer\n+  }\n+  private var isCached: Boolean = true\n+  private var closed: Boolean = true\n+  private def close(): Unit = {\n+    try {\n+      this.synchronized {\n+        if (!closed) {\n+          closed = true\n+          kafkaProducer.close()\n+          logInfo(s\"Closed kafka producer: $kafkaProducer\")\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error while closing kafka producer with params: $kafkaParams\", e)\n+    }\n+  }\n+\n+  private def inUse(): Boolean = {\n+    inUseCount.get() > 0\n+  }\n+  private def unCache(): Unit = {\n+    isCached = false\n+  }\n+  private[kafka010] def getInUseCount: Int = inUseCount.get()\n+\n+  private[kafka010] def getKafkaParams: Seq[(String, Object)] = kafkaParams\n+\n+  private[kafka010] def flush(): Unit = {\n+    kafkaProducer.flush()\n+  }\n+\n+  private[kafka010] def isClosed: Boolean = closed\n+}\n+\n+private[kafka010] object CachedKafkaProducer extends Logging {\n \n   private val defaultCacheExpireTimeout = TimeUnit.MINUTES.toMillis(10)\n \n   private lazy val cacheExpireTimeout: Long =\n     Option(SparkEnv.get).map(_.conf.getTimeAsMs(\n-      \"spark.kafka.producer.cache.timeout\",\n-      s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n+      key = \"spark.kafka.producer.cache.timeout\",\n+      defaultValue = s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n+\n+  private def updatedAuthConfigIfNeeded(kafkaParamsMap: ju.Map[String, Object]) =\n+    KafkaConfigUpdater(\"executor\", kafkaParamsMap.asScala.toMap)\n+      .setAuthenticationConfigIfNeeded()\n+      .build()\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(updatedAuthConfigIfNeeded(kafkaParamsMap))",
    "line": 148
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "So you mean, a producer instance need not be renewed, if delegation token is changed. Does kafka pick up the update token automatically, i.e. without recreating the producer or updating it in any way? If that is the case, then this approach has to change a bit.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-12T05:30:37Z",
    "diffHunk": "@@ -18,103 +18,180 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n \n-  private type Producer = KafkaProducer[Array[Byte], Array[Byte]]\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  lazy val kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]] = {\n+    val producer = new KafkaProducer[Array[Byte], Array[Byte]](configMap)\n+    logDebug(s\"Created a new instance of KafkaProducer for \" +\n+      s\"$kafkaParams with Id: $id\")\n+    closed = false\n+    producer\n+  }\n+  private var isCached: Boolean = true\n+  private var closed: Boolean = true\n+  private def close(): Unit = {\n+    try {\n+      this.synchronized {\n+        if (!closed) {\n+          closed = true\n+          kafkaProducer.close()\n+          logInfo(s\"Closed kafka producer: $kafkaProducer\")\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error while closing kafka producer with params: $kafkaParams\", e)\n+    }\n+  }\n+\n+  private def inUse(): Boolean = {\n+    inUseCount.get() > 0\n+  }\n+  private def unCache(): Unit = {\n+    isCached = false\n+  }\n+  private[kafka010] def getInUseCount: Int = inUseCount.get()\n+\n+  private[kafka010] def getKafkaParams: Seq[(String, Object)] = kafkaParams\n+\n+  private[kafka010] def flush(): Unit = {\n+    kafkaProducer.flush()\n+  }\n+\n+  private[kafka010] def isClosed: Boolean = closed\n+}\n+\n+private[kafka010] object CachedKafkaProducer extends Logging {\n \n   private val defaultCacheExpireTimeout = TimeUnit.MINUTES.toMillis(10)\n \n   private lazy val cacheExpireTimeout: Long =\n     Option(SparkEnv.get).map(_.conf.getTimeAsMs(\n-      \"spark.kafka.producer.cache.timeout\",\n-      s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n+      key = \"spark.kafka.producer.cache.timeout\",\n+      defaultValue = s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n+\n+  private def updatedAuthConfigIfNeeded(kafkaParamsMap: ju.Map[String, Object]) =\n+    KafkaConfigUpdater(\"executor\", kafkaParamsMap.asScala.toMap)\n+      .setAuthenticationConfigIfNeeded()\n+      .build()\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(updatedAuthConfigIfNeeded(kafkaParamsMap))",
    "line": 148
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "If delegation token expired then new instance has to be created. I've just created a jira to add dynamic configuration possibility [here](https://issues.apache.org/jira/browse/KAFKA-8128) but that's another question.\r\n\r\nThinking about this a bit more the approach what you've added here looks good and would end-up in less task retry (having token in key). The cache eviction would close the not used and old producer instances. As I've just checked this change is not in the latest change set.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-19T14:48:50Z",
    "diffHunk": "@@ -18,103 +18,180 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n \n-  private type Producer = KafkaProducer[Array[Byte], Array[Byte]]\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  lazy val kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]] = {\n+    val producer = new KafkaProducer[Array[Byte], Array[Byte]](configMap)\n+    logDebug(s\"Created a new instance of KafkaProducer for \" +\n+      s\"$kafkaParams with Id: $id\")\n+    closed = false\n+    producer\n+  }\n+  private var isCached: Boolean = true\n+  private var closed: Boolean = true\n+  private def close(): Unit = {\n+    try {\n+      this.synchronized {\n+        if (!closed) {\n+          closed = true\n+          kafkaProducer.close()\n+          logInfo(s\"Closed kafka producer: $kafkaProducer\")\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error while closing kafka producer with params: $kafkaParams\", e)\n+    }\n+  }\n+\n+  private def inUse(): Boolean = {\n+    inUseCount.get() > 0\n+  }\n+  private def unCache(): Unit = {\n+    isCached = false\n+  }\n+  private[kafka010] def getInUseCount: Int = inUseCount.get()\n+\n+  private[kafka010] def getKafkaParams: Seq[(String, Object)] = kafkaParams\n+\n+  private[kafka010] def flush(): Unit = {\n+    kafkaProducer.flush()\n+  }\n+\n+  private[kafka010] def isClosed: Boolean = closed\n+}\n+\n+private[kafka010] object CachedKafkaProducer extends Logging {\n \n   private val defaultCacheExpireTimeout = TimeUnit.MINUTES.toMillis(10)\n \n   private lazy val cacheExpireTimeout: Long =\n     Option(SparkEnv.get).map(_.conf.getTimeAsMs(\n-      \"spark.kafka.producer.cache.timeout\",\n-      s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n+      key = \"spark.kafka.producer.cache.timeout\",\n+      defaultValue = s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n+\n+  private def updatedAuthConfigIfNeeded(kafkaParamsMap: ju.Map[String, Object]) =\n+    KafkaConfigUpdater(\"executor\", kafkaParamsMap.asScala.toMap)\n+      .setAuthenticationConfigIfNeeded()\n+      .build()\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(updatedAuthConfigIfNeeded(kafkaParamsMap))",
    "line": 148
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "As a general comment I would write out the kafka params all the time which would make it easier to follow the lifecycle of a producer. (Maybe instance is also useful but the kafka params needed from my perspective).",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-11T10:56:12Z",
    "diffHunk": "@@ -18,103 +18,180 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n \n-  private type Producer = KafkaProducer[Array[Byte], Array[Byte]]\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  lazy val kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]] = {\n+    val producer = new KafkaProducer[Array[Byte], Array[Byte]](configMap)\n+    logDebug(s\"Created a new instance of KafkaProducer for \" +\n+      s\"$kafkaParams with Id: $id\")\n+    closed = false\n+    producer\n+  }\n+  private var isCached: Boolean = true\n+  private var closed: Boolean = true\n+  private def close(): Unit = {\n+    try {\n+      this.synchronized {\n+        if (!closed) {\n+          closed = true\n+          kafkaProducer.close()\n+          logInfo(s\"Closed kafka producer: $kafkaProducer\")"
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "you are right, my bad.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-12T05:10:56Z",
    "diffHunk": "@@ -18,103 +18,180 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n \n-  private type Producer = KafkaProducer[Array[Byte], Array[Byte]]\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  lazy val kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]] = {\n+    val producer = new KafkaProducer[Array[Byte], Array[Byte]](configMap)\n+    logDebug(s\"Created a new instance of KafkaProducer for \" +\n+      s\"$kafkaParams with Id: $id\")\n+    closed = false\n+    producer\n+  }\n+  private var isCached: Boolean = true\n+  private var closed: Boolean = true\n+  private def close(): Unit = {\n+    try {\n+      this.synchronized {\n+        if (!closed) {\n+          closed = true\n+          kafkaProducer.close()\n+          logInfo(s\"Closed kafka producer: $kafkaProducer\")"
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "What is the concept with this `id` field?",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-11T10:57:34Z",
    "diffHunk": "@@ -18,103 +18,180 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,",
    "line": 24
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "`Id`, makes it easy to track the lifecycle of a producer in the large log output. ",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-12T05:10:20Z",
    "diffHunk": "@@ -18,103 +18,180 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,",
    "line": 24
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "I see, it makes sense on the other hand it can be useful when all the places printed out where something happens with the producer.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-18T13:24:58Z",
    "diffHunk": "@@ -18,103 +18,180 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,",
    "line": 24
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "I think there is a race here. Let's take a simple example:\r\n* Cache has one producer\r\n* Task tries to release it\r\n* Execution in thread1 stands in `release` function right before `close`\r\n* thread2 is trying to acquire a producer and stands after `guavaCache.get(paramsSeq)`\r\n* thread1 gets processor time and closes the producer\r\n* thread 2 increments the counter and gives back a closed producer\r\n\r\nNit: Is the type needed here?\r\n",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-11T11:02:33Z",
    "diffHunk": "@@ -18,103 +18,180 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n \n-  private type Producer = KafkaProducer[Array[Byte], Array[Byte]]\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  lazy val kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]] = {\n+    val producer = new KafkaProducer[Array[Byte], Array[Byte]](configMap)\n+    logDebug(s\"Created a new instance of KafkaProducer for \" +\n+      s\"$kafkaParams with Id: $id\")\n+    closed = false\n+    producer\n+  }\n+  private var isCached: Boolean = true\n+  private var closed: Boolean = true\n+  private def close(): Unit = {\n+    try {\n+      this.synchronized {\n+        if (!closed) {\n+          closed = true\n+          kafkaProducer.close()\n+          logInfo(s\"Closed kafka producer: $kafkaProducer\")\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error while closing kafka producer with params: $kafkaParams\", e)\n+    }\n+  }\n+\n+  private def inUse(): Boolean = {\n+    inUseCount.get() > 0\n+  }\n+  private def unCache(): Unit = {\n+    isCached = false\n+  }\n+  private[kafka010] def getInUseCount: Int = inUseCount.get()\n+\n+  private[kafka010] def getKafkaParams: Seq[(String, Object)] = kafkaParams\n+\n+  private[kafka010] def flush(): Unit = {\n+    kafkaProducer.flush()\n+  }\n+\n+  private[kafka010] def isClosed: Boolean = closed\n+}\n+\n+private[kafka010] object CachedKafkaProducer extends Logging {\n \n   private val defaultCacheExpireTimeout = TimeUnit.MINUTES.toMillis(10)\n \n   private lazy val cacheExpireTimeout: Long =\n     Option(SparkEnv.get).map(_.conf.getTimeAsMs(\n-      \"spark.kafka.producer.cache.timeout\",\n-      s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n+      key = \"spark.kafka.producer.cache.timeout\",\n+      defaultValue = s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n+\n+  private def updatedAuthConfigIfNeeded(kafkaParamsMap: ju.Map[String, Object]) =\n+    KafkaConfigUpdater(\"executor\", kafkaParamsMap.asScala.toMap)\n+      .setAuthenticationConfigIfNeeded()\n+      .build()\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(updatedAuthConfigIfNeeded(kafkaParamsMap))\n     try {\n-      guavaCache.get(paramsSeq)\n+      val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+      val useCount: Int = cachedKafkaProducer.inUseCount.incrementAndGet()"
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "This race condition is no longer valid. Because we close a producer, only if it is not cached. If a producer is not cached, then new task threads cannot acquire it.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-20T07:56:56Z",
    "diffHunk": "@@ -18,103 +18,180 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n \n-  private type Producer = KafkaProducer[Array[Byte], Array[Byte]]\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  lazy val kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]] = {\n+    val producer = new KafkaProducer[Array[Byte], Array[Byte]](configMap)\n+    logDebug(s\"Created a new instance of KafkaProducer for \" +\n+      s\"$kafkaParams with Id: $id\")\n+    closed = false\n+    producer\n+  }\n+  private var isCached: Boolean = true\n+  private var closed: Boolean = true\n+  private def close(): Unit = {\n+    try {\n+      this.synchronized {\n+        if (!closed) {\n+          closed = true\n+          kafkaProducer.close()\n+          logInfo(s\"Closed kafka producer: $kafkaProducer\")\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error while closing kafka producer with params: $kafkaParams\", e)\n+    }\n+  }\n+\n+  private def inUse(): Boolean = {\n+    inUseCount.get() > 0\n+  }\n+  private def unCache(): Unit = {\n+    isCached = false\n+  }\n+  private[kafka010] def getInUseCount: Int = inUseCount.get()\n+\n+  private[kafka010] def getKafkaParams: Seq[(String, Object)] = kafkaParams\n+\n+  private[kafka010] def flush(): Unit = {\n+    kafkaProducer.flush()\n+  }\n+\n+  private[kafka010] def isClosed: Boolean = closed\n+}\n+\n+private[kafka010] object CachedKafkaProducer extends Logging {\n \n   private val defaultCacheExpireTimeout = TimeUnit.MINUTES.toMillis(10)\n \n   private lazy val cacheExpireTimeout: Long =\n     Option(SparkEnv.get).map(_.conf.getTimeAsMs(\n-      \"spark.kafka.producer.cache.timeout\",\n-      s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n+      key = \"spark.kafka.producer.cache.timeout\",\n+      defaultValue = s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n+\n+  private def updatedAuthConfigIfNeeded(kafkaParamsMap: ju.Map[String, Object]) =\n+    KafkaConfigUpdater(\"executor\", kafkaParamsMap.asScala.toMap)\n+      .setAuthenticationConfigIfNeeded()\n+      .build()\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(updatedAuthConfigIfNeeded(kafkaParamsMap))\n     try {\n-      guavaCache.get(paramsSeq)\n+      val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+      val useCount: Int = cachedKafkaProducer.inUseCount.incrementAndGet()"
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Not yet see why these changes needed.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-11T11:22:00Z",
    "diffHunk": "@@ -18,103 +18,180 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n \n-  private type Producer = KafkaProducer[Array[Byte], Array[Byte]]\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  lazy val kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]] = {\n+    val producer = new KafkaProducer[Array[Byte], Array[Byte]](configMap)\n+    logDebug(s\"Created a new instance of KafkaProducer for \" +\n+      s\"$kafkaParams with Id: $id\")\n+    closed = false\n+    producer\n+  }\n+  private var isCached: Boolean = true\n+  private var closed: Boolean = true\n+  private def close(): Unit = {\n+    try {\n+      this.synchronized {\n+        if (!closed) {\n+          closed = true\n+          kafkaProducer.close()\n+          logInfo(s\"Closed kafka producer: $kafkaProducer\")\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error while closing kafka producer with params: $kafkaParams\", e)\n+    }\n+  }\n+\n+  private def inUse(): Boolean = {\n+    inUseCount.get() > 0\n+  }\n+  private def unCache(): Unit = {\n+    isCached = false\n+  }\n+  private[kafka010] def getInUseCount: Int = inUseCount.get()\n+\n+  private[kafka010] def getKafkaParams: Seq[(String, Object)] = kafkaParams\n+\n+  private[kafka010] def flush(): Unit = {\n+    kafkaProducer.flush()\n+  }\n+\n+  private[kafka010] def isClosed: Boolean = closed\n+}\n+\n+private[kafka010] object CachedKafkaProducer extends Logging {\n \n   private val defaultCacheExpireTimeout = TimeUnit.MINUTES.toMillis(10)\n \n   private lazy val cacheExpireTimeout: Long =\n     Option(SparkEnv.get).map(_.conf.getTimeAsMs(\n-      \"spark.kafka.producer.cache.timeout\",\n-      s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n+      key = \"spark.kafka.producer.cache.timeout\","
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "Sorry about this, but I thought it is harmless, Intellij highlighted and I did it.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-12T05:12:51Z",
    "diffHunk": "@@ -18,103 +18,180 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n \n-  private type Producer = KafkaProducer[Array[Byte], Array[Byte]]\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  lazy val kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]] = {\n+    val producer = new KafkaProducer[Array[Byte], Array[Byte]](configMap)\n+    logDebug(s\"Created a new instance of KafkaProducer for \" +\n+      s\"$kafkaParams with Id: $id\")\n+    closed = false\n+    producer\n+  }\n+  private var isCached: Boolean = true\n+  private var closed: Boolean = true\n+  private def close(): Unit = {\n+    try {\n+      this.synchronized {\n+        if (!closed) {\n+          closed = true\n+          kafkaProducer.close()\n+          logInfo(s\"Closed kafka producer: $kafkaProducer\")\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error while closing kafka producer with params: $kafkaParams\", e)\n+    }\n+  }\n+\n+  private def inUse(): Boolean = {\n+    inUseCount.get() > 0\n+  }\n+  private def unCache(): Unit = {\n+    isCached = false\n+  }\n+  private[kafka010] def getInUseCount: Int = inUseCount.get()\n+\n+  private[kafka010] def getKafkaParams: Seq[(String, Object)] = kafkaParams\n+\n+  private[kafka010] def flush(): Unit = {\n+    kafkaProducer.flush()\n+  }\n+\n+  private[kafka010] def isClosed: Boolean = closed\n+}\n+\n+private[kafka010] object CachedKafkaProducer extends Logging {\n \n   private val defaultCacheExpireTimeout = TimeUnit.MINUTES.toMillis(10)\n \n   private lazy val cacheExpireTimeout: Long =\n     Option(SparkEnv.get).map(_.conf.getTimeAsMs(\n-      \"spark.kafka.producer.cache.timeout\",\n-      s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n+      key = \"spark.kafka.producer.cache.timeout\","
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Don't see the point to extract this since only one place used.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-11T11:24:39Z",
    "diffHunk": "@@ -18,103 +18,180 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n \n-  private type Producer = KafkaProducer[Array[Byte], Array[Byte]]\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  lazy val kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]] = {\n+    val producer = new KafkaProducer[Array[Byte], Array[Byte]](configMap)\n+    logDebug(s\"Created a new instance of KafkaProducer for \" +\n+      s\"$kafkaParams with Id: $id\")\n+    closed = false\n+    producer\n+  }\n+  private var isCached: Boolean = true\n+  private var closed: Boolean = true\n+  private def close(): Unit = {\n+    try {\n+      this.synchronized {\n+        if (!closed) {\n+          closed = true\n+          kafkaProducer.close()\n+          logInfo(s\"Closed kafka producer: $kafkaProducer\")\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error while closing kafka producer with params: $kafkaParams\", e)\n+    }\n+  }\n+\n+  private def inUse(): Boolean = {\n+    inUseCount.get() > 0\n+  }\n+  private def unCache(): Unit = {\n+    isCached = false\n+  }\n+  private[kafka010] def getInUseCount: Int = inUseCount.get()\n+\n+  private[kafka010] def getKafkaParams: Seq[(String, Object)] = kafkaParams\n+\n+  private[kafka010] def flush(): Unit = {\n+    kafkaProducer.flush()\n+  }\n+\n+  private[kafka010] def isClosed: Boolean = closed\n+}\n+\n+private[kafka010] object CachedKafkaProducer extends Logging {\n \n   private val defaultCacheExpireTimeout = TimeUnit.MINUTES.toMillis(10)\n \n   private lazy val cacheExpireTimeout: Long =\n     Option(SparkEnv.get).map(_.conf.getTimeAsMs(\n-      \"spark.kafka.producer.cache.timeout\",\n-      s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n+      key = \"spark.kafka.producer.cache.timeout\",\n+      defaultValue = s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n+\n+  private def updatedAuthConfigIfNeeded(kafkaParamsMap: ju.Map[String, Object]) ="
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Not sure it's required since `producer.inUseCount.decrementAndGet()` gives back the value.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-11T11:34:39Z",
    "diffHunk": "@@ -18,103 +18,180 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n \n-  private type Producer = KafkaProducer[Array[Byte], Array[Byte]]\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  lazy val kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]] = {\n+    val producer = new KafkaProducer[Array[Byte], Array[Byte]](configMap)\n+    logDebug(s\"Created a new instance of KafkaProducer for \" +\n+      s\"$kafkaParams with Id: $id\")\n+    closed = false\n+    producer\n+  }\n+  private var isCached: Boolean = true\n+  private var closed: Boolean = true\n+  private def close(): Unit = {\n+    try {\n+      this.synchronized {\n+        if (!closed) {\n+          closed = true\n+          kafkaProducer.close()\n+          logInfo(s\"Closed kafka producer: $kafkaProducer\")\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error while closing kafka producer with params: $kafkaParams\", e)\n+    }\n+  }\n+\n+  private def inUse(): Boolean = {\n+    inUseCount.get() > 0\n+  }\n+  private def unCache(): Unit = {\n+    isCached = false\n+  }\n+  private[kafka010] def getInUseCount: Int = inUseCount.get()",
    "line": 61
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "I did not want to expose inuse count outside of CachedKafkaProducer.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-12T05:11:32Z",
    "diffHunk": "@@ -18,103 +18,180 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n \n-  private type Producer = KafkaProducer[Array[Byte], Array[Byte]]\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  lazy val kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]] = {\n+    val producer = new KafkaProducer[Array[Byte], Array[Byte]](configMap)\n+    logDebug(s\"Created a new instance of KafkaProducer for \" +\n+      s\"$kafkaParams with Id: $id\")\n+    closed = false\n+    producer\n+  }\n+  private var isCached: Boolean = true\n+  private var closed: Boolean = true\n+  private def close(): Unit = {\n+    try {\n+      this.synchronized {\n+        if (!closed) {\n+          closed = true\n+          kafkaProducer.close()\n+          logInfo(s\"Closed kafka producer: $kafkaProducer\")\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error while closing kafka producer with params: $kafkaParams\", e)\n+    }\n+  }\n+\n+  private def inUse(): Boolean = {\n+    inUseCount.get() > 0\n+  }\n+  private def unCache(): Unit = {\n+    isCached = false\n+  }\n+  private[kafka010] def getInUseCount: Int = inUseCount.get()",
    "line": 61
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Ah, you mean make it read-only, make sense.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-19T15:01:08Z",
    "diffHunk": "@@ -18,103 +18,180 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n \n-  private type Producer = KafkaProducer[Array[Byte], Array[Byte]]\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  lazy val kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]] = {\n+    val producer = new KafkaProducer[Array[Byte], Array[Byte]](configMap)\n+    logDebug(s\"Created a new instance of KafkaProducer for \" +\n+      s\"$kafkaParams with Id: $id\")\n+    closed = false\n+    producer\n+  }\n+  private var isCached: Boolean = true\n+  private var closed: Boolean = true\n+  private def close(): Unit = {\n+    try {\n+      this.synchronized {\n+        if (!closed) {\n+          closed = true\n+          kafkaProducer.close()\n+          logInfo(s\"Closed kafka producer: $kafkaProducer\")\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error while closing kafka producer with params: $kafkaParams\", e)\n+    }\n+  }\n+\n+  private def inUse(): Boolean = {\n+    inUseCount.get() > 0\n+  }\n+  private def unCache(): Unit = {\n+    isCached = false\n+  }\n+  private[kafka010] def getInUseCount: Int = inUseCount.get()",
    "line": 61
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Shouldn't this be volatile?",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-11T11:41:09Z",
    "diffHunk": "@@ -18,103 +18,180 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n \n-  private type Producer = KafkaProducer[Array[Byte], Array[Byte]]\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  lazy val kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]] = {\n+    val producer = new KafkaProducer[Array[Byte], Array[Byte]](configMap)\n+    logDebug(s\"Created a new instance of KafkaProducer for \" +\n+      s\"$kafkaParams with Id: $id\")\n+    closed = false\n+    producer\n+  }\n+  private var isCached: Boolean = true"
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "Yes, better to keep it volatile.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-12T05:14:30Z",
    "diffHunk": "@@ -18,103 +18,180 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n \n-  private type Producer = KafkaProducer[Array[Byte], Array[Byte]]\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  lazy val kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]] = {\n+    val producer = new KafkaProducer[Array[Byte], Array[Byte]](configMap)\n+    logDebug(s\"Created a new instance of KafkaProducer for \" +\n+      s\"$kafkaParams with Id: $id\")\n+    closed = false\n+    producer\n+  }\n+  private var isCached: Boolean = true"
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "Before invalidating the producer, it's better to also check if the cached one in guavaCache is the same `CachedKafkaProducer`. If not, we should not invalidate it. Then it will be safe to call `release` multiple times.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-19T22:16:37Z",
    "diffHunk": "@@ -40,81 +94,103 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n       \"spark.kafka.producer.cache.timeout\",\n       s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}.\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n-    try {\n-      guavaCache.get(paramsSeq)\n-    } catch {\n-      case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n-        if e.getCause != null =>\n-        throw e.getCause\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer =\n+    this.synchronized {\n+      val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParamsMap)\n+      try {\n+        val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+        val useCount = cachedKafkaProducer.inUseCount.incrementAndGet()\n+        logDebug(s\"Granted producer $cachedKafkaProducer, inuse-count: $useCount\")\n+        cachedKafkaProducer\n+      } catch {\n+        case e@(_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n+          if e.getCause != null =>\n+          throw e.getCause\n+      }\n     }\n-  }\n \n-  private def paramsToSeq(kafkaParams: ju.Map[String, Object]): Seq[(String, Object)] = {\n-    val paramsSeq: Seq[(String, Object)] = kafkaParams.asScala.toSeq.sortBy(x => x._1)\n+  private def paramsToSeq(kafkaParamsMap: ju.Map[String, Object]): Seq[(String, Object)] = {\n+    val paramsSeq: Seq[(String, Object)] = kafkaParamsMap.asScala.toSeq.sortBy(x => x._1)\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n-  private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n-    val paramsSeq = paramsToSeq(kafkaParams)\n-    guavaCache.invalidate(paramsSeq)\n+  /* Release a kafka producer back to the kafka cache. We simple decrement it's inuse count. */\n+  private[kafka010] def release(producer: CachedKafkaProducer, failing: Boolean): Unit = {\n+    this.synchronized {\n+      val inUseCount = producer.inUseCount.decrementAndGet()\n+      logDebug(s\"Released producer $producer, updated inuse count: $inUseCount\")\n+      if (failing) {\n+        // If this producer is failing to write, we remove it from cache.\n+        // So that it is re-created, eventually.\n+        logDebug(s\"Invalidated a failing producer: $producer.\")\n+        guavaCache.invalidate(producer.kafkaParams)"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Good idea, additionally makes the code more simple compared to my suggestion [here](https://github.com/apache/spark/pull/19096#discussion_r266888173). My one can be resolved because this covers it.\r\n@ScrapCodes it would be good to cover this functionality with a unit test. Something like:\r\n```\r\n  test(\"Should not close already closed producer.\") {\r\n  ...\r\n  }\r\n```\r\n",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-20T09:56:24Z",
    "diffHunk": "@@ -40,81 +94,103 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n       \"spark.kafka.producer.cache.timeout\",\n       s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}.\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n-    try {\n-      guavaCache.get(paramsSeq)\n-    } catch {\n-      case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n-        if e.getCause != null =>\n-        throw e.getCause\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer =\n+    this.synchronized {\n+      val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParamsMap)\n+      try {\n+        val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+        val useCount = cachedKafkaProducer.inUseCount.incrementAndGet()\n+        logDebug(s\"Granted producer $cachedKafkaProducer, inuse-count: $useCount\")\n+        cachedKafkaProducer\n+      } catch {\n+        case e@(_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n+          if e.getCause != null =>\n+          throw e.getCause\n+      }\n     }\n-  }\n \n-  private def paramsToSeq(kafkaParams: ju.Map[String, Object]): Seq[(String, Object)] = {\n-    val paramsSeq: Seq[(String, Object)] = kafkaParams.asScala.toSeq.sortBy(x => x._1)\n+  private def paramsToSeq(kafkaParamsMap: ju.Map[String, Object]): Seq[(String, Object)] = {\n+    val paramsSeq: Seq[(String, Object)] = kafkaParamsMap.asScala.toSeq.sortBy(x => x._1)\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n-  private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n-    val paramsSeq = paramsToSeq(kafkaParams)\n-    guavaCache.invalidate(paramsSeq)\n+  /* Release a kafka producer back to the kafka cache. We simple decrement it's inuse count. */\n+  private[kafka010] def release(producer: CachedKafkaProducer, failing: Boolean): Unit = {\n+    this.synchronized {\n+      val inUseCount = producer.inUseCount.decrementAndGet()\n+      logDebug(s\"Released producer $producer, updated inuse count: $inUseCount\")\n+      if (failing) {\n+        // If this producer is failing to write, we remove it from cache.\n+        // So that it is re-created, eventually.\n+        logDebug(s\"Invalidated a failing producer: $producer.\")\n+        guavaCache.invalidate(producer.kafkaParams)"
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "Could you move this codes out of the lock? Closing producers takes time as it will block until all pending requests are sent. We can avoid blocking other threads that are acquiring producers.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-19T23:08:31Z",
    "diffHunk": "@@ -40,81 +94,103 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n       \"spark.kafka.producer.cache.timeout\",\n       s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}.\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n-    try {\n-      guavaCache.get(paramsSeq)\n-    } catch {\n-      case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n-        if e.getCause != null =>\n-        throw e.getCause\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer =\n+    this.synchronized {\n+      val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParamsMap)\n+      try {\n+        val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+        val useCount = cachedKafkaProducer.inUseCount.incrementAndGet()\n+        logDebug(s\"Granted producer $cachedKafkaProducer, inuse-count: $useCount\")\n+        cachedKafkaProducer\n+      } catch {\n+        case e@(_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n+          if e.getCause != null =>\n+          throw e.getCause\n+      }\n     }\n-  }\n \n-  private def paramsToSeq(kafkaParams: ju.Map[String, Object]): Seq[(String, Object)] = {\n-    val paramsSeq: Seq[(String, Object)] = kafkaParams.asScala.toSeq.sortBy(x => x._1)\n+  private def paramsToSeq(kafkaParamsMap: ju.Map[String, Object]): Seq[(String, Object)] = {\n+    val paramsSeq: Seq[(String, Object)] = kafkaParamsMap.asScala.toSeq.sortBy(x => x._1)\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n-  private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n-    val paramsSeq = paramsToSeq(kafkaParams)\n-    guavaCache.invalidate(paramsSeq)\n+  /* Release a kafka producer back to the kafka cache. We simple decrement it's inuse count. */\n+  private[kafka010] def release(producer: CachedKafkaProducer, failing: Boolean): Unit = {\n+    this.synchronized {\n+      val inUseCount = producer.inUseCount.decrementAndGet()\n+      logDebug(s\"Released producer $producer, updated inuse count: $inUseCount\")\n+      if (failing) {\n+        // If this producer is failing to write, we remove it from cache.\n+        // So that it is re-created, eventually.\n+        logDebug(s\"Invalidated a failing producer: $producer.\")\n+        guavaCache.invalidate(producer.kafkaParams)\n+      }\n+      if (!producer.inUse() && !producer.isCached) {\n+        // it will take care of removing it from close queue as well.\n+        close(producer)\n+      }\n+    }\n   }\n \n-  /** Auto close on cache evict */\n-  private def close(paramsSeq: Seq[(String, Object)], producer: Producer): Unit = {\n-    try {\n-      logInfo(s\"Closing the KafkaProducer with params: ${paramsSeq.mkString(\"\\n\")}.\")\n-      producer.close()\n-    } catch {\n-      case NonFatal(e) => logWarning(\"Error while closing kafka producer.\", e)\n+  /** Close this producer and process pending closes. */\n+  private def close(producer: CachedKafkaProducer): Unit = {\n+    producer.close()\n+    // Check and close any other producers previously evicted, but pending to be closed.\n+    for (p <- closeQueue.iterator().asScala) {",
    "line": 210
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "nit: you can just lock the following two lines:\r\n```\r\nval cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\r\n        val useCount = cachedKafkaProducer.inUseCount.incrementAndGet()\r\n```",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-19T23:21:26Z",
    "diffHunk": "@@ -40,81 +94,103 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n       \"spark.kafka.producer.cache.timeout\",\n       s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}.\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n-    try {\n-      guavaCache.get(paramsSeq)\n-    } catch {\n-      case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n-        if e.getCause != null =>\n-        throw e.getCause\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer =\n+    this.synchronized {"
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "I am wondering, if there is a case, when not locking this piece of code causes any incorrect results.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-20T08:05:07Z",
    "diffHunk": "@@ -40,81 +94,103 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n       \"spark.kafka.producer.cache.timeout\",\n       s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}.\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n-    try {\n-      guavaCache.get(paramsSeq)\n-    } catch {\n-      case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n-        if e.getCause != null =>\n-        throw e.getCause\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer =\n+    this.synchronized {"
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Nit: inUse, unCache and flush can be inlined (`private def inUse(): Boolean = inUseCount.get() > 0`).",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-20T10:34:00Z",
    "diffHunk": "@@ -18,20 +18,74 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n+\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  private def updatedAuthConfigIfNeeded(kafkaParamsMap: ju.Map[String, Object]) =\n+    KafkaConfigUpdater(\"executor\", kafkaParamsMap.asScala.toMap)\n+      .setAuthenticationConfigIfNeeded()\n+      .build()\n+\n+  lazy val kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]] = {\n+    val producer = new KafkaProducer[Array[Byte], Array[Byte]](updatedAuthConfigIfNeeded(configMap))\n+    logDebug(s\"Created a new instance of KafkaProducer for \" +\n+      s\"$kafkaParams with Id: $id\")\n+    closed = false\n+    producer\n+  }\n+  @volatile\n+  private var isCached: Boolean = true\n+  private var closed: Boolean = true\n+  private def close(): Unit = {\n+    try {\n+      this.synchronized {\n+        if (!closed) {\n+          closed = true\n+          kafkaProducer.close()\n+          logInfo(s\"Closed kafka producer: $this\")\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error while closing kafka producer with params: $kafkaParams\", e)\n+    }\n+  }\n+\n+  private def inUse(): Boolean = {"
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Just want to mention Kafka allows double close so `closed` flag is not needed but because its not provided as a guarantee its fine to have it.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-27T10:06:11Z",
    "diffHunk": "@@ -18,20 +18,70 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n+\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  private def updatedAuthConfigIfNeeded(kafkaParamsMap: ju.Map[String, Object]) =\n+    KafkaConfigUpdater(\"executor\", kafkaParamsMap.asScala.toMap)\n+      .setAuthenticationConfigIfNeeded()\n+      .build()\n+\n+  lazy val kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]] = {\n+    val producer = new KafkaProducer[Array[Byte], Array[Byte]](updatedAuthConfigIfNeeded(configMap))\n+    logDebug(s\"Created a new instance of KafkaProducer for \" +\n+      s\"$kafkaParams with Id: $id\")\n+    closed = false\n+    producer\n+  }\n+  @volatile\n+  private var isCached: Boolean = true\n+  private var closed: Boolean = true\n+  private def close(): Unit = {\n+    try {\n+      this.synchronized {\n+        if (!closed) {",
    "line": 44
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Once we guard `closed` as synchronization block, accessing it should be ideally guarded as well. For `closed` it's only be accessed from synchronization block (assuming lazy val is also guarded when lazily computed) but this enables to access it without synchronization.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-30T00:52:02Z",
    "diffHunk": "@@ -18,20 +18,70 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n+\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  private def updatedAuthConfigIfNeeded(kafkaParamsMap: ju.Map[String, Object]) =\n+    KafkaConfigUpdater(\"executor\", kafkaParamsMap.asScala.toMap)\n+      .setAuthenticationConfigIfNeeded()\n+      .build()\n+\n+  lazy val kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]] = {\n+    val producer = new KafkaProducer[Array[Byte], Array[Byte]](updatedAuthConfigIfNeeded(configMap))\n+    logDebug(s\"Created a new instance of KafkaProducer for \" +\n+      s\"$kafkaParams with Id: $id\")\n+    closed = false\n+    producer\n+  }\n+  @volatile\n+  private var isCached: Boolean = true\n+  private var closed: Boolean = true\n+  private def close(): Unit = {\n+    try {\n+      this.synchronized {\n+        if (!closed) {\n+          closed = true\n+          kafkaProducer.close()\n+          logInfo(s\"Closed kafka producer: $this\")\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error while closing kafka producer with params: $kafkaParams\", e)\n+    }\n+  }\n+\n+  private def inUse(): Boolean = inUseCount.get() > 0\n+\n+  private def unCache(): Unit = isCached = false\n+\n+  private[kafka010] def getInUseCount: Int = inUseCount.get()\n \n-  private type Producer = KafkaProducer[Array[Byte], Array[Byte]]\n+  private[kafka010] def getKafkaParams: Seq[(String, Object)] = kafkaParams\n+\n+  private[kafka010] def flush(): Unit = kafkaProducer.flush()\n+\n+  private[kafka010] def isClosed: Boolean = closed",
    "line": 68
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "closed is used only in one synchronized block, in non test code. Why do you think it needs to be guarded? ",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-03T09:40:18Z",
    "diffHunk": "@@ -18,20 +18,70 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n+\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  private def updatedAuthConfigIfNeeded(kafkaParamsMap: ju.Map[String, Object]) =\n+    KafkaConfigUpdater(\"executor\", kafkaParamsMap.asScala.toMap)\n+      .setAuthenticationConfigIfNeeded()\n+      .build()\n+\n+  lazy val kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]] = {\n+    val producer = new KafkaProducer[Array[Byte], Array[Byte]](updatedAuthConfigIfNeeded(configMap))\n+    logDebug(s\"Created a new instance of KafkaProducer for \" +\n+      s\"$kafkaParams with Id: $id\")\n+    closed = false\n+    producer\n+  }\n+  @volatile\n+  private var isCached: Boolean = true\n+  private var closed: Boolean = true\n+  private def close(): Unit = {\n+    try {\n+      this.synchronized {\n+        if (!closed) {\n+          closed = true\n+          kafkaProducer.close()\n+          logInfo(s\"Closed kafka producer: $this\")\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error while closing kafka producer with params: $kafkaParams\", e)\n+    }\n+  }\n+\n+  private def inUse(): Boolean = inUseCount.get() > 0\n+\n+  private def unCache(): Unit = isCached = false\n+\n+  private[kafka010] def getInUseCount: Int = inUseCount.get()\n \n-  private type Producer = KafkaProducer[Array[Byte], Array[Byte]]\n+  private[kafka010] def getKafkaParams: Seq[(String, Object)] = kafkaParams\n+\n+  private[kafka010] def flush(): Unit = kafkaProducer.flush()\n+\n+  private[kafka010] def isClosed: Boolean = closed",
    "line": 68
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "\"in non test code\" sounds like you tend to agree it's not properly guarded if we include test code, and it's being used as an assertion in multiple places in test code - which might lead intermittent test failure.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-05T06:47:01Z",
    "diffHunk": "@@ -18,20 +18,70 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n+\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  private def updatedAuthConfigIfNeeded(kafkaParamsMap: ju.Map[String, Object]) =\n+    KafkaConfigUpdater(\"executor\", kafkaParamsMap.asScala.toMap)\n+      .setAuthenticationConfigIfNeeded()\n+      .build()\n+\n+  lazy val kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]] = {\n+    val producer = new KafkaProducer[Array[Byte], Array[Byte]](updatedAuthConfigIfNeeded(configMap))\n+    logDebug(s\"Created a new instance of KafkaProducer for \" +\n+      s\"$kafkaParams with Id: $id\")\n+    closed = false\n+    producer\n+  }\n+  @volatile\n+  private var isCached: Boolean = true\n+  private var closed: Boolean = true\n+  private def close(): Unit = {\n+    try {\n+      this.synchronized {\n+        if (!closed) {\n+          closed = true\n+          kafkaProducer.close()\n+          logInfo(s\"Closed kafka producer: $this\")\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error while closing kafka producer with params: $kafkaParams\", e)\n+    }\n+  }\n+\n+  private def inUse(): Boolean = inUseCount.get() > 0\n+\n+  private def unCache(): Unit = isCached = false\n+\n+  private[kafka010] def getInUseCount: Int = inUseCount.get()\n \n-  private type Producer = KafkaProducer[Array[Byte], Array[Byte]]\n+  private[kafka010] def getKafkaParams: Seq[(String, Object)] = kafkaParams\n+\n+  private[kafka010] def flush(): Unit = kafkaProducer.flush()\n+\n+  private[kafka010] def isClosed: Boolean = closed",
    "line": 68
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "@HeartSaVioR nice catch and I agree there is a problem here. JVM doesn't guarantee what's the content of a variable (processor caching) and/or may reorganize bytecode around until not:\r\n* codeblocks synchronized all the time consistently\r\n* variable is protected with volatile\r\n",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-05T11:22:22Z",
    "diffHunk": "@@ -18,20 +18,70 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n+\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  private def updatedAuthConfigIfNeeded(kafkaParamsMap: ju.Map[String, Object]) =\n+    KafkaConfigUpdater(\"executor\", kafkaParamsMap.asScala.toMap)\n+      .setAuthenticationConfigIfNeeded()\n+      .build()\n+\n+  lazy val kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]] = {\n+    val producer = new KafkaProducer[Array[Byte], Array[Byte]](updatedAuthConfigIfNeeded(configMap))\n+    logDebug(s\"Created a new instance of KafkaProducer for \" +\n+      s\"$kafkaParams with Id: $id\")\n+    closed = false\n+    producer\n+  }\n+  @volatile\n+  private var isCached: Boolean = true\n+  private var closed: Boolean = true\n+  private def close(): Unit = {\n+    try {\n+      this.synchronized {\n+        if (!closed) {\n+          closed = true\n+          kafkaProducer.close()\n+          logInfo(s\"Closed kafka producer: $this\")\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error while closing kafka producer with params: $kafkaParams\", e)\n+    }\n+  }\n+\n+  private def inUse(): Boolean = inUseCount.get() > 0\n+\n+  private def unCache(): Unit = isCached = false\n+\n+  private[kafka010] def getInUseCount: Int = inUseCount.get()\n \n-  private type Producer = KafkaProducer[Array[Byte], Array[Byte]]\n+  private[kafka010] def getKafkaParams: Seq[(String, Object)] = kafkaParams\n+\n+  private[kafka010] def flush(): Unit = kafkaProducer.flush()\n+\n+  private[kafka010] def isClosed: Boolean = closed",
    "line": 68
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "closed is now Guarded, because it is used by tests, and if multiple tests run in different threads, it can lead to some kind of unpredictable output, at least in theory. ",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-10T07:02:26Z",
    "diffHunk": "@@ -18,20 +18,70 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n+\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  private def updatedAuthConfigIfNeeded(kafkaParamsMap: ju.Map[String, Object]) =\n+    KafkaConfigUpdater(\"executor\", kafkaParamsMap.asScala.toMap)\n+      .setAuthenticationConfigIfNeeded()\n+      .build()\n+\n+  lazy val kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]] = {\n+    val producer = new KafkaProducer[Array[Byte], Array[Byte]](updatedAuthConfigIfNeeded(configMap))\n+    logDebug(s\"Created a new instance of KafkaProducer for \" +\n+      s\"$kafkaParams with Id: $id\")\n+    closed = false\n+    producer\n+  }\n+  @volatile\n+  private var isCached: Boolean = true\n+  private var closed: Boolean = true\n+  private def close(): Unit = {\n+    try {\n+      this.synchronized {\n+        if (!closed) {\n+          closed = true\n+          kafkaProducer.close()\n+          logInfo(s\"Closed kafka producer: $this\")\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error while closing kafka producer with params: $kafkaParams\", e)\n+    }\n+  }\n+\n+  private def inUse(): Boolean = inUseCount.get() > 0\n+\n+  private def unCache(): Unit = isCached = false\n+\n+  private[kafka010] def getInUseCount: Int = inUseCount.get()\n \n-  private type Producer = KafkaProducer[Array[Byte], Array[Byte]]\n+  private[kafka010] def getKafkaParams: Seq[(String, Object)] = kafkaParams\n+\n+  private[kafka010] def flush(): Unit = kafkaProducer.flush()\n+\n+  private[kafka010] def isClosed: Boolean = closed",
    "line": 68
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Just to check on my understanding, if I understand correctly, this represents in-use count being messed up due to some bugs, and the producer can still be closed before even some thread actually uses it. Do I understand correctly?",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-30T01:15:00Z",
    "diffHunk": "@@ -40,81 +90,114 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n       \"spark.kafka.producer.cache.timeout\",\n       s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}.\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParamsMap)\n     try {\n-      guavaCache.get(paramsSeq)\n+      val producer = this.synchronized {\n+        val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+        cachedKafkaProducer.inUseCount.incrementAndGet()\n+        logDebug(s\"Granted producer $cachedKafkaProducer\")\n+        cachedKafkaProducer\n+      }\n+      producer\n     } catch {\n-      case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n+      case e@(_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n         if e.getCause != null =>\n         throw e.getCause\n     }\n   }\n \n-  private def paramsToSeq(kafkaParams: ju.Map[String, Object]): Seq[(String, Object)] = {\n-    val paramsSeq: Seq[(String, Object)] = kafkaParams.asScala.toSeq.sortBy(x => x._1)\n+  private def paramsToSeq(kafkaParamsMap: ju.Map[String, Object]): Seq[(String, Object)] = {\n+    val paramsSeq: Seq[(String, Object)] = kafkaParamsMap.asScala.toSeq.sortBy(x => x._1)\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n-  private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n-    val paramsSeq = paramsToSeq(kafkaParams)\n-    guavaCache.invalidate(paramsSeq)\n+  /* Release a kafka producer back to the kafka cache. We simply decrement it's inuse count. */\n+  private[kafka010] def release(producer: CachedKafkaProducer, failing: Boolean): Unit = {\n+    this.synchronized {\n+      // It should be ok to call release multiple times on the same producer object.\n+      if (producer.inUse()) {\n+        // So that we do not end up with -ve in-use counts.\n+        producer.inUseCount.decrementAndGet()\n+        logDebug(s\"Released producer $producer.\")\n+      } else {\n+        logWarning(s\"Tried to release a not in use producer, $producer.\")",
    "line": 185
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "I have left this warning, for testing.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-03T09:26:32Z",
    "diffHunk": "@@ -40,81 +90,114 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n       \"spark.kafka.producer.cache.timeout\",\n       s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}.\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParamsMap)\n     try {\n-      guavaCache.get(paramsSeq)\n+      val producer = this.synchronized {\n+        val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+        cachedKafkaProducer.inUseCount.incrementAndGet()\n+        logDebug(s\"Granted producer $cachedKafkaProducer\")\n+        cachedKafkaProducer\n+      }\n+      producer\n     } catch {\n-      case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n+      case e@(_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n         if e.getCause != null =>\n         throw e.getCause\n     }\n   }\n \n-  private def paramsToSeq(kafkaParams: ju.Map[String, Object]): Seq[(String, Object)] = {\n-    val paramsSeq: Seq[(String, Object)] = kafkaParams.asScala.toSeq.sortBy(x => x._1)\n+  private def paramsToSeq(kafkaParamsMap: ju.Map[String, Object]): Seq[(String, Object)] = {\n+    val paramsSeq: Seq[(String, Object)] = kafkaParamsMap.asScala.toSeq.sortBy(x => x._1)\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n-  private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n-    val paramsSeq = paramsToSeq(kafkaParams)\n-    guavaCache.invalidate(paramsSeq)\n+  /* Release a kafka producer back to the kafka cache. We simply decrement it's inuse count. */\n+  private[kafka010] def release(producer: CachedKafkaProducer, failing: Boolean): Unit = {\n+    this.synchronized {\n+      // It should be ok to call release multiple times on the same producer object.\n+      if (producer.inUse()) {\n+        // So that we do not end up with -ve in-use counts.\n+        producer.inUseCount.decrementAndGet()\n+        logDebug(s\"Released producer $producer.\")\n+      } else {\n+        logWarning(s\"Tried to release a not in use producer, $producer.\")",
    "line": 185
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "I meant this would indicate a bug in logic if my understanding is right (so should not happen):  then worth to log as error or even throw IllegalStateException.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-05T06:55:21Z",
    "diffHunk": "@@ -40,81 +90,114 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n       \"spark.kafka.producer.cache.timeout\",\n       s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}.\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParamsMap)\n     try {\n-      guavaCache.get(paramsSeq)\n+      val producer = this.synchronized {\n+        val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+        cachedKafkaProducer.inUseCount.incrementAndGet()\n+        logDebug(s\"Granted producer $cachedKafkaProducer\")\n+        cachedKafkaProducer\n+      }\n+      producer\n     } catch {\n-      case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n+      case e@(_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n         if e.getCause != null =>\n         throw e.getCause\n     }\n   }\n \n-  private def paramsToSeq(kafkaParams: ju.Map[String, Object]): Seq[(String, Object)] = {\n-    val paramsSeq: Seq[(String, Object)] = kafkaParams.asScala.toSeq.sortBy(x => x._1)\n+  private def paramsToSeq(kafkaParamsMap: ju.Map[String, Object]): Seq[(String, Object)] = {\n+    val paramsSeq: Seq[(String, Object)] = kafkaParamsMap.asScala.toSeq.sortBy(x => x._1)\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n-  private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n-    val paramsSeq = paramsToSeq(kafkaParams)\n-    guavaCache.invalidate(paramsSeq)\n+  /* Release a kafka producer back to the kafka cache. We simply decrement it's inuse count. */\n+  private[kafka010] def release(producer: CachedKafkaProducer, failing: Boolean): Unit = {\n+    this.synchronized {\n+      // It should be ok to call release multiple times on the same producer object.\n+      if (producer.inUse()) {\n+        // So that we do not end up with -ve in-use counts.\n+        producer.inUseCount.decrementAndGet()\n+        logDebug(s\"Released producer $producer.\")\n+      } else {\n+        logWarning(s\"Tried to release a not in use producer, $producer.\")",
    "line": 185
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Such case exists in consumer side where I've found some minor problems. I've fixed this when I've adapted the related PR to DStreams but this minor issue maybe still exists in structured area. Just like these 2 cases I think the same here, namely if such case happens it's a problem but this shouldn't stop the processing with an exception. In order to conform with consumer side it's good to write this with a warning.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-05T11:07:44Z",
    "diffHunk": "@@ -40,81 +90,114 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n       \"spark.kafka.producer.cache.timeout\",\n       s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}.\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParamsMap)\n     try {\n-      guavaCache.get(paramsSeq)\n+      val producer = this.synchronized {\n+        val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+        cachedKafkaProducer.inUseCount.incrementAndGet()\n+        logDebug(s\"Granted producer $cachedKafkaProducer\")\n+        cachedKafkaProducer\n+      }\n+      producer\n     } catch {\n-      case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n+      case e@(_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n         if e.getCause != null =>\n         throw e.getCause\n     }\n   }\n \n-  private def paramsToSeq(kafkaParams: ju.Map[String, Object]): Seq[(String, Object)] = {\n-    val paramsSeq: Seq[(String, Object)] = kafkaParams.asScala.toSeq.sortBy(x => x._1)\n+  private def paramsToSeq(kafkaParamsMap: ju.Map[String, Object]): Seq[(String, Object)] = {\n+    val paramsSeq: Seq[(String, Object)] = kafkaParamsMap.asScala.toSeq.sortBy(x => x._1)\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n-  private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n-    val paramsSeq = paramsToSeq(kafkaParams)\n-    guavaCache.invalidate(paramsSeq)\n+  /* Release a kafka producer back to the kafka cache. We simply decrement it's inuse count. */\n+  private[kafka010] def release(producer: CachedKafkaProducer, failing: Boolean): Unit = {\n+    this.synchronized {\n+      // It should be ok to call release multiple times on the same producer object.\n+      if (producer.inUse()) {\n+        // So that we do not end up with -ve in-use counts.\n+        producer.inUseCount.decrementAndGet()\n+        logDebug(s\"Released producer $producer.\")\n+      } else {\n+        logWarning(s\"Tried to release a not in use producer, $producer.\")",
    "line": 185
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Here and removalListener are not guarded with same lock, so some kinds of race condition can happen. One of case would be:\r\n\r\n- producer going to be evicted somehow: removalListener called\r\n  - in removalListener, `inUse() = true`, so it adds producer to closeQueue, and context-switch happens\r\n- that producer is going to be released from other thread: release called\r\n  - in release, decrementing inUse count makes `inUse = false`, and forward to `if (!producer.inUse() && !producer.isCached)`, and skip this line because of `isCached = true` (though it's in closeQueue)\r\n- removalListener takes control - producer.unCache() called but the producer cannot be cleaned up from usage threads\r\n\r\nI guess iterating closeQueue and closing them would make leaked producers be eventually closed, but this if statement is not always true (it might not occur frequently) so don't know when they can be closed.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-30T01:58:07Z",
    "diffHunk": "@@ -40,81 +90,114 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n       \"spark.kafka.producer.cache.timeout\",\n       s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}.\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParamsMap)\n     try {\n-      guavaCache.get(paramsSeq)\n+      val producer = this.synchronized {\n+        val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+        cachedKafkaProducer.inUseCount.incrementAndGet()\n+        logDebug(s\"Granted producer $cachedKafkaProducer\")\n+        cachedKafkaProducer\n+      }\n+      producer\n     } catch {\n-      case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n+      case e@(_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n         if e.getCause != null =>\n         throw e.getCause\n     }\n   }\n \n-  private def paramsToSeq(kafkaParams: ju.Map[String, Object]): Seq[(String, Object)] = {\n-    val paramsSeq: Seq[(String, Object)] = kafkaParams.asScala.toSeq.sortBy(x => x._1)\n+  private def paramsToSeq(kafkaParamsMap: ju.Map[String, Object]): Seq[(String, Object)] = {\n+    val paramsSeq: Seq[(String, Object)] = kafkaParamsMap.asScala.toSeq.sortBy(x => x._1)\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n-  private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n-    val paramsSeq = paramsToSeq(kafkaParams)\n-    guavaCache.invalidate(paramsSeq)\n+  /* Release a kafka producer back to the kafka cache. We simply decrement it's inuse count. */\n+  private[kafka010] def release(producer: CachedKafkaProducer, failing: Boolean): Unit = {\n+    this.synchronized {\n+      // It should be ok to call release multiple times on the same producer object.\n+      if (producer.inUse()) {\n+        // So that we do not end up with -ve in-use counts.\n+        producer.inUseCount.decrementAndGet()\n+        logDebug(s\"Released producer $producer.\")\n+      } else {\n+        logWarning(s\"Tried to release a not in use producer, $producer.\")\n+      }\n+      if (failing) {\n+        // If this producer is failing to write, we remove it from cache.\n+        // So that it is re-created, eventually.\n+        val cachedProducer = guavaCache.getIfPresent(producer.kafkaParams)\n+        if (cachedProducer != null && cachedProducer.id == producer.id) {\n+          logDebug(s\"Invalidating a failing producer: $producer.\")\n+          guavaCache.invalidate(producer.kafkaParams)\n+        }\n+      }\n+    }\n+    if (!producer.inUse() && !producer.isCached) {"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "And I feel complicated that there're two different places which close producer. \r\n\r\nEvery producers are cached, and all the producers will be go through removal listener to be removed in cache and be closed immediately or added to close queue to be closed eventually (when it fulfills the condition).\r\n\r\nThen from outer removal listener we just need to concern about how to handle close queue. The if statement here makes us to determine when to handle close queue immediately, but there's race condition I described before so it's not exactly correct. \r\n(Actually if it's exactly thread-safe we even don't need a close queue. Just release it once if conditions are met.)\r\n\r\nOther approach would be injecting cleanup of close queue to the end of `release` method with some probability ([Guava cache seems to evict like this way](https://github.com/google/guava/wiki/CachesExplained#when-does-cleanup-happen)), or even periodically in background thread. Handling close queue doesn't need to be synchronized with others (if we avoid concurrent modification exception) - only thing would be `inUse` in producer, but once we revisit the producer eventually (not too late) no need to be exact.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-30T02:28:13Z",
    "diffHunk": "@@ -40,81 +90,114 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n       \"spark.kafka.producer.cache.timeout\",\n       s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}.\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParamsMap)\n     try {\n-      guavaCache.get(paramsSeq)\n+      val producer = this.synchronized {\n+        val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+        cachedKafkaProducer.inUseCount.incrementAndGet()\n+        logDebug(s\"Granted producer $cachedKafkaProducer\")\n+        cachedKafkaProducer\n+      }\n+      producer\n     } catch {\n-      case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n+      case e@(_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n         if e.getCause != null =>\n         throw e.getCause\n     }\n   }\n \n-  private def paramsToSeq(kafkaParams: ju.Map[String, Object]): Seq[(String, Object)] = {\n-    val paramsSeq: Seq[(String, Object)] = kafkaParams.asScala.toSeq.sortBy(x => x._1)\n+  private def paramsToSeq(kafkaParamsMap: ju.Map[String, Object]): Seq[(String, Object)] = {\n+    val paramsSeq: Seq[(String, Object)] = kafkaParamsMap.asScala.toSeq.sortBy(x => x._1)\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n-  private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n-    val paramsSeq = paramsToSeq(kafkaParams)\n-    guavaCache.invalidate(paramsSeq)\n+  /* Release a kafka producer back to the kafka cache. We simply decrement it's inuse count. */\n+  private[kafka010] def release(producer: CachedKafkaProducer, failing: Boolean): Unit = {\n+    this.synchronized {\n+      // It should be ok to call release multiple times on the same producer object.\n+      if (producer.inUse()) {\n+        // So that we do not end up with -ve in-use counts.\n+        producer.inUseCount.decrementAndGet()\n+        logDebug(s\"Released producer $producer.\")\n+      } else {\n+        logWarning(s\"Tried to release a not in use producer, $producer.\")\n+      }\n+      if (failing) {\n+        // If this producer is failing to write, we remove it from cache.\n+        // So that it is re-created, eventually.\n+        val cachedProducer = guavaCache.getIfPresent(producer.kafkaParams)\n+        if (cachedProducer != null && cachedProducer.id == producer.id) {\n+          logDebug(s\"Invalidating a failing producer: $producer.\")\n+          guavaCache.invalidate(producer.kafkaParams)\n+        }\n+      }\n+    }\n+    if (!producer.inUse() && !producer.isCached) {"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "I realized `close` which removal listener calls is same as `close` here so the possibility is pretty higher than I thought. (I feel it might be too frequent though.)\r\n\r\nThen I think if statement here can be just removed and it wouldn't hurt.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-30T04:50:08Z",
    "diffHunk": "@@ -40,81 +90,114 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n       \"spark.kafka.producer.cache.timeout\",\n       s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}.\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParamsMap)\n     try {\n-      guavaCache.get(paramsSeq)\n+      val producer = this.synchronized {\n+        val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+        cachedKafkaProducer.inUseCount.incrementAndGet()\n+        logDebug(s\"Granted producer $cachedKafkaProducer\")\n+        cachedKafkaProducer\n+      }\n+      producer\n     } catch {\n-      case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n+      case e@(_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n         if e.getCause != null =>\n         throw e.getCause\n     }\n   }\n \n-  private def paramsToSeq(kafkaParams: ju.Map[String, Object]): Seq[(String, Object)] = {\n-    val paramsSeq: Seq[(String, Object)] = kafkaParams.asScala.toSeq.sortBy(x => x._1)\n+  private def paramsToSeq(kafkaParamsMap: ju.Map[String, Object]): Seq[(String, Object)] = {\n+    val paramsSeq: Seq[(String, Object)] = kafkaParamsMap.asScala.toSeq.sortBy(x => x._1)\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n-  private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n-    val paramsSeq = paramsToSeq(kafkaParams)\n-    guavaCache.invalidate(paramsSeq)\n+  /* Release a kafka producer back to the kafka cache. We simply decrement it's inuse count. */\n+  private[kafka010] def release(producer: CachedKafkaProducer, failing: Boolean): Unit = {\n+    this.synchronized {\n+      // It should be ok to call release multiple times on the same producer object.\n+      if (producer.inUse()) {\n+        // So that we do not end up with -ve in-use counts.\n+        producer.inUseCount.decrementAndGet()\n+        logDebug(s\"Released producer $producer.\")\n+      } else {\n+        logWarning(s\"Tried to release a not in use producer, $producer.\")\n+      }\n+      if (failing) {\n+        // If this producer is failing to write, we remove it from cache.\n+        // So that it is re-created, eventually.\n+        val cachedProducer = guavaCache.getIfPresent(producer.kafkaParams)\n+        if (cachedProducer != null && cachedProducer.id == producer.id) {\n+          logDebug(s\"Invalidating a failing producer: $producer.\")\n+          guavaCache.invalidate(producer.kafkaParams)\n+        }\n+      }\n+    }\n+    if (!producer.inUse() && !producer.isCached) {"
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "The race condition is not clear to me, you have concluded in the comment that there is no problem, since eventually producer will be closed. \r\neither a producer is closed immediately, because it is not in use and evicted by guava, or it is in use and should be closed once it is finally released by all the threads using it.\r\n\r\nCan you give a test, to reproduce the problem you are stating?",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-03T09:32:55Z",
    "diffHunk": "@@ -40,81 +90,114 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n       \"spark.kafka.producer.cache.timeout\",\n       s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}.\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParamsMap)\n     try {\n-      guavaCache.get(paramsSeq)\n+      val producer = this.synchronized {\n+        val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+        cachedKafkaProducer.inUseCount.incrementAndGet()\n+        logDebug(s\"Granted producer $cachedKafkaProducer\")\n+        cachedKafkaProducer\n+      }\n+      producer\n     } catch {\n-      case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n+      case e@(_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n         if e.getCause != null =>\n         throw e.getCause\n     }\n   }\n \n-  private def paramsToSeq(kafkaParams: ju.Map[String, Object]): Seq[(String, Object)] = {\n-    val paramsSeq: Seq[(String, Object)] = kafkaParams.asScala.toSeq.sortBy(x => x._1)\n+  private def paramsToSeq(kafkaParamsMap: ju.Map[String, Object]): Seq[(String, Object)] = {\n+    val paramsSeq: Seq[(String, Object)] = kafkaParamsMap.asScala.toSeq.sortBy(x => x._1)\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n-  private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n-    val paramsSeq = paramsToSeq(kafkaParams)\n-    guavaCache.invalidate(paramsSeq)\n+  /* Release a kafka producer back to the kafka cache. We simply decrement it's inuse count. */\n+  private[kafka010] def release(producer: CachedKafkaProducer, failing: Boolean): Unit = {\n+    this.synchronized {\n+      // It should be ok to call release multiple times on the same producer object.\n+      if (producer.inUse()) {\n+        // So that we do not end up with -ve in-use counts.\n+        producer.inUseCount.decrementAndGet()\n+        logDebug(s\"Released producer $producer.\")\n+      } else {\n+        logWarning(s\"Tried to release a not in use producer, $producer.\")\n+      }\n+      if (failing) {\n+        // If this producer is failing to write, we remove it from cache.\n+        // So that it is re-created, eventually.\n+        val cachedProducer = guavaCache.getIfPresent(producer.kafkaParams)\n+        if (cachedProducer != null && cachedProducer.id == producer.id) {\n+          logDebug(s\"Invalidating a failing producer: $producer.\")\n+          guavaCache.invalidate(producer.kafkaParams)\n+        }\n+      }\n+    }\n+    if (!producer.inUse() && !producer.isCached) {"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Could you point out which you don't understand my explanation of race condition? Race condition is not a thing a test can reproduce consistently.\r\n\r\nIn short, `Adding producer to closeQueue` and `marking producer as uncached` are not being done atomically given there's no synchronization in `removalListener.onRemoval`. There's a case producer is added to closeQueue but isCached is still `true` - race condition happens here.\r\n\r\nIn other words, suppose `inCount` in producer is exactly correct and thread-safe: then why we need close queue? There should be \"exactly only once\" when inUse = false, either `onRemoval` or `release`, then closeQueue doesn't be needed and just closing the producer should work. `closeQueue` is just for mitigation when synchronization of `inCount` is off, and looping closeQueue every time when other producer is closing is not free.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-05T07:20:03Z",
    "diffHunk": "@@ -40,81 +90,114 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n       \"spark.kafka.producer.cache.timeout\",\n       s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}.\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParamsMap)\n     try {\n-      guavaCache.get(paramsSeq)\n+      val producer = this.synchronized {\n+        val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+        cachedKafkaProducer.inUseCount.incrementAndGet()\n+        logDebug(s\"Granted producer $cachedKafkaProducer\")\n+        cachedKafkaProducer\n+      }\n+      producer\n     } catch {\n-      case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n+      case e@(_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n         if e.getCause != null =>\n         throw e.getCause\n     }\n   }\n \n-  private def paramsToSeq(kafkaParams: ju.Map[String, Object]): Seq[(String, Object)] = {\n-    val paramsSeq: Seq[(String, Object)] = kafkaParams.asScala.toSeq.sortBy(x => x._1)\n+  private def paramsToSeq(kafkaParamsMap: ju.Map[String, Object]): Seq[(String, Object)] = {\n+    val paramsSeq: Seq[(String, Object)] = kafkaParamsMap.asScala.toSeq.sortBy(x => x._1)\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n-  private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n-    val paramsSeq = paramsToSeq(kafkaParams)\n-    guavaCache.invalidate(paramsSeq)\n+  /* Release a kafka producer back to the kafka cache. We simply decrement it's inuse count. */\n+  private[kafka010] def release(producer: CachedKafkaProducer, failing: Boolean): Unit = {\n+    this.synchronized {\n+      // It should be ok to call release multiple times on the same producer object.\n+      if (producer.inUse()) {\n+        // So that we do not end up with -ve in-use counts.\n+        producer.inUseCount.decrementAndGet()\n+        logDebug(s\"Released producer $producer.\")\n+      } else {\n+        logWarning(s\"Tried to release a not in use producer, $producer.\")\n+      }\n+      if (failing) {\n+        // If this producer is failing to write, we remove it from cache.\n+        // So that it is re-created, eventually.\n+        val cachedProducer = guavaCache.getIfPresent(producer.kafkaParams)\n+        if (cachedProducer != null && cachedProducer.id == producer.id) {\n+          logDebug(s\"Invalidating a failing producer: $producer.\")\n+          guavaCache.invalidate(producer.kafkaParams)\n+        }\n+      }\n+    }\n+    if (!producer.inUse() && !producer.isCached) {"
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "The purpose of closeQueue is, when an inuse producer is evicted by guava cache, due to a long running task. Then in such rare cases, closeQueue will hold the producer until it is finally released by all the tasks. Two important properties to note, 1) As soon as the producer enters in the closeQueue, it cannot be acquired by any new tasks.(Guarantee by guava) 2) Once all the tasks have released it, then it is closed and removed from the closeQueue. \r\n\r\nCurrently, the answers to the following question is not clear. Please feel free to help me, anyone.\r\n\r\nHow closeQueue is just for mitigation of inCount goes out of sync?\r\n\r\nHow does race condition happens when isCached is true and object is not added to closeQueue Or vice-versa?",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-05T11:07:12Z",
    "diffHunk": "@@ -40,81 +90,114 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n       \"spark.kafka.producer.cache.timeout\",\n       s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}.\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParamsMap)\n     try {\n-      guavaCache.get(paramsSeq)\n+      val producer = this.synchronized {\n+        val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+        cachedKafkaProducer.inUseCount.incrementAndGet()\n+        logDebug(s\"Granted producer $cachedKafkaProducer\")\n+        cachedKafkaProducer\n+      }\n+      producer\n     } catch {\n-      case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n+      case e@(_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n         if e.getCause != null =>\n         throw e.getCause\n     }\n   }\n \n-  private def paramsToSeq(kafkaParams: ju.Map[String, Object]): Seq[(String, Object)] = {\n-    val paramsSeq: Seq[(String, Object)] = kafkaParams.asScala.toSeq.sortBy(x => x._1)\n+  private def paramsToSeq(kafkaParamsMap: ju.Map[String, Object]): Seq[(String, Object)] = {\n+    val paramsSeq: Seq[(String, Object)] = kafkaParamsMap.asScala.toSeq.sortBy(x => x._1)\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n-  private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n-    val paramsSeq = paramsToSeq(kafkaParams)\n-    guavaCache.invalidate(paramsSeq)\n+  /* Release a kafka producer back to the kafka cache. We simply decrement it's inuse count. */\n+  private[kafka010] def release(producer: CachedKafkaProducer, failing: Boolean): Unit = {\n+    this.synchronized {\n+      // It should be ok to call release multiple times on the same producer object.\n+      if (producer.inUse()) {\n+        // So that we do not end up with -ve in-use counts.\n+        producer.inUseCount.decrementAndGet()\n+        logDebug(s\"Released producer $producer.\")\n+      } else {\n+        logWarning(s\"Tried to release a not in use producer, $producer.\")\n+      }\n+      if (failing) {\n+        // If this producer is failing to write, we remove it from cache.\n+        // So that it is re-created, eventually.\n+        val cachedProducer = guavaCache.getIfPresent(producer.kafkaParams)\n+        if (cachedProducer != null && cachedProducer.id == producer.id) {\n+          logDebug(s\"Invalidating a failing producer: $producer.\")\n+          guavaCache.invalidate(producer.kafkaParams)\n+        }\n+      }\n+    }\n+    if (!producer.inUse() && !producer.isCached) {"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "@HeartSaVioR nice catch again. More eyes more opportunities.\r\n\r\n@ScrapCodes To oversimplify this problem: because of threading producers sitting in `closeQueue` may wait really long to be closed (until another `release` called on another producer no race condition happens again). Since its race condition don't think meaningful test can be given and even test is written maybe the code hits it rarely. On the other hand such race in streaming is nasty enough to spend weeks to hunt this down.\r\n\r\nI personally like the way how guava is doing cleanup. Simple and effective.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-05T11:54:08Z",
    "diffHunk": "@@ -40,81 +90,114 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n       \"spark.kafka.producer.cache.timeout\",\n       s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}.\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParamsMap)\n     try {\n-      guavaCache.get(paramsSeq)\n+      val producer = this.synchronized {\n+        val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+        cachedKafkaProducer.inUseCount.incrementAndGet()\n+        logDebug(s\"Granted producer $cachedKafkaProducer\")\n+        cachedKafkaProducer\n+      }\n+      producer\n     } catch {\n-      case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n+      case e@(_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n         if e.getCause != null =>\n         throw e.getCause\n     }\n   }\n \n-  private def paramsToSeq(kafkaParams: ju.Map[String, Object]): Seq[(String, Object)] = {\n-    val paramsSeq: Seq[(String, Object)] = kafkaParams.asScala.toSeq.sortBy(x => x._1)\n+  private def paramsToSeq(kafkaParamsMap: ju.Map[String, Object]): Seq[(String, Object)] = {\n+    val paramsSeq: Seq[(String, Object)] = kafkaParamsMap.asScala.toSeq.sortBy(x => x._1)\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n-  private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n-    val paramsSeq = paramsToSeq(kafkaParams)\n-    guavaCache.invalidate(paramsSeq)\n+  /* Release a kafka producer back to the kafka cache. We simply decrement it's inuse count. */\n+  private[kafka010] def release(producer: CachedKafkaProducer, failing: Boolean): Unit = {\n+    this.synchronized {\n+      // It should be ok to call release multiple times on the same producer object.\n+      if (producer.inUse()) {\n+        // So that we do not end up with -ve in-use counts.\n+        producer.inUseCount.decrementAndGet()\n+        logDebug(s\"Released producer $producer.\")\n+      } else {\n+        logWarning(s\"Tried to release a not in use producer, $producer.\")\n+      }\n+      if (failing) {\n+        // If this producer is failing to write, we remove it from cache.\n+        // So that it is re-created, eventually.\n+        val cachedProducer = guavaCache.getIfPresent(producer.kafkaParams)\n+        if (cachedProducer != null && cachedProducer.id == producer.id) {\n+          logDebug(s\"Invalidating a failing producer: $producer.\")\n+          guavaCache.invalidate(producer.kafkaParams)\n+        }\n+      }\n+    }\n+    if (!producer.inUse() && !producer.isCached) {"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "@ScrapCodes I can't really give better explanation than @HeartSaVioR given [here](https://github.com/apache/spark/pull/19096#discussion_r270608975). `removalListener` runs in separate thread than `release` for instance.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-05T13:20:29Z",
    "diffHunk": "@@ -40,81 +90,114 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n       \"spark.kafka.producer.cache.timeout\",\n       s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}.\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParamsMap)\n     try {\n-      guavaCache.get(paramsSeq)\n+      val producer = this.synchronized {\n+        val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+        cachedKafkaProducer.inUseCount.incrementAndGet()\n+        logDebug(s\"Granted producer $cachedKafkaProducer\")\n+        cachedKafkaProducer\n+      }\n+      producer\n     } catch {\n-      case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n+      case e@(_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n         if e.getCause != null =>\n         throw e.getCause\n     }\n   }\n \n-  private def paramsToSeq(kafkaParams: ju.Map[String, Object]): Seq[(String, Object)] = {\n-    val paramsSeq: Seq[(String, Object)] = kafkaParams.asScala.toSeq.sortBy(x => x._1)\n+  private def paramsToSeq(kafkaParamsMap: ju.Map[String, Object]): Seq[(String, Object)] = {\n+    val paramsSeq: Seq[(String, Object)] = kafkaParamsMap.asScala.toSeq.sortBy(x => x._1)\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n-  private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n-    val paramsSeq = paramsToSeq(kafkaParams)\n-    guavaCache.invalidate(paramsSeq)\n+  /* Release a kafka producer back to the kafka cache. We simply decrement it's inuse count. */\n+  private[kafka010] def release(producer: CachedKafkaProducer, failing: Boolean): Unit = {\n+    this.synchronized {\n+      // It should be ok to call release multiple times on the same producer object.\n+      if (producer.inUse()) {\n+        // So that we do not end up with -ve in-use counts.\n+        producer.inUseCount.decrementAndGet()\n+        logDebug(s\"Released producer $producer.\")\n+      } else {\n+        logWarning(s\"Tried to release a not in use producer, $producer.\")\n+      }\n+      if (failing) {\n+        // If this producer is failing to write, we remove it from cache.\n+        // So that it is re-created, eventually.\n+        val cachedProducer = guavaCache.getIfPresent(producer.kafkaParams)\n+        if (cachedProducer != null && cachedProducer.id == producer.id) {\n+          logDebug(s\"Invalidating a failing producer: $producer.\")\n+          guavaCache.invalidate(producer.kafkaParams)\n+        }\n+      }\n+    }\n+    if (!producer.inUse() && !producer.isCached) {"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "@ScrapCodes \r\n\r\n> Then in such rare cases, closeQueue will hold the producer until it is finally released by all the tasks.\r\n\r\nIf things pretty work fine, closeQueue is not needed: each task will hold the producer when it uses the producer even closeQueue doesn't exist, and eventually one of task will close it when decreasing inCount makes inCount being zero.\r\n\r\n> 1) As soon as the producer enters in the closeQueue, it cannot be acquired by any new tasks.(Guarantee by guava) \r\n\r\nI think it should be guaranteed by Guava when onRemoval is called with such producer. No one can get the producer afterwards. Same without closeQueue, right?\r\n\r\n> 2) Once all the tasks have released it, then it is closed and removed from the closeQueue.\r\n\r\ncloseQueue is not needed as I explained earlier.\r\n\r\n> How closeQueue is just for mitigation of inCount goes out of sync?\r\n\r\nAs I explained race condition can happen, let's assume inCount can be out of sync. Once producer is being added to closeQueue, even race condition I've specified happens, the producer is added to closeQueue (though it will not be closed at that time) and it can be closed once another producer is being closed. So getting another chance to be closed: I meant this as \"mitigation\".\r\n\r\n> How does race condition happens when isCached is true and object is not added to closeQueue Or vice-versa?\r\n\r\nI've already explained two times and @gaborgsomogyi can also see the case. Please take a deep look at my comment: I understand it's not easy for author to see defect in his/her own code.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-06T07:14:23Z",
    "diffHunk": "@@ -40,81 +90,114 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n       \"spark.kafka.producer.cache.timeout\",\n       s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}.\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParamsMap)\n     try {\n-      guavaCache.get(paramsSeq)\n+      val producer = this.synchronized {\n+        val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+        cachedKafkaProducer.inUseCount.incrementAndGet()\n+        logDebug(s\"Granted producer $cachedKafkaProducer\")\n+        cachedKafkaProducer\n+      }\n+      producer\n     } catch {\n-      case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n+      case e@(_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n         if e.getCause != null =>\n         throw e.getCause\n     }\n   }\n \n-  private def paramsToSeq(kafkaParams: ju.Map[String, Object]): Seq[(String, Object)] = {\n-    val paramsSeq: Seq[(String, Object)] = kafkaParams.asScala.toSeq.sortBy(x => x._1)\n+  private def paramsToSeq(kafkaParamsMap: ju.Map[String, Object]): Seq[(String, Object)] = {\n+    val paramsSeq: Seq[(String, Object)] = kafkaParamsMap.asScala.toSeq.sortBy(x => x._1)\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n-  private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n-    val paramsSeq = paramsToSeq(kafkaParams)\n-    guavaCache.invalidate(paramsSeq)\n+  /* Release a kafka producer back to the kafka cache. We simply decrement it's inuse count. */\n+  private[kafka010] def release(producer: CachedKafkaProducer, failing: Boolean): Unit = {\n+    this.synchronized {\n+      // It should be ok to call release multiple times on the same producer object.\n+      if (producer.inUse()) {\n+        // So that we do not end up with -ve in-use counts.\n+        producer.inUseCount.decrementAndGet()\n+        logDebug(s\"Released producer $producer.\")\n+      } else {\n+        logWarning(s\"Tried to release a not in use producer, $producer.\")\n+      }\n+      if (failing) {\n+        // If this producer is failing to write, we remove it from cache.\n+        // So that it is re-created, eventually.\n+        val cachedProducer = guavaCache.getIfPresent(producer.kafkaParams)\n+        if (cachedProducer != null && cachedProducer.id == producer.id) {\n+          logDebug(s\"Invalidating a failing producer: $producer.\")\n+          guavaCache.invalidate(producer.kafkaParams)\n+        }\n+      }\n+    }\n+    if (!producer.inUse() && !producer.isCached) {"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Maybe just removing closeQueue might resolve the race condition. (Though I guess we still need to synchronize between `release` and `removal listener` to ensure safety) The issue happens because there're two steps to uncache the producer, and even each step is thread-safe, doing two steps together is NOT thread-safe.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-06T07:18:29Z",
    "diffHunk": "@@ -40,81 +90,114 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n       \"spark.kafka.producer.cache.timeout\",\n       s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}.\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParamsMap)\n     try {\n-      guavaCache.get(paramsSeq)\n+      val producer = this.synchronized {\n+        val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+        cachedKafkaProducer.inUseCount.incrementAndGet()\n+        logDebug(s\"Granted producer $cachedKafkaProducer\")\n+        cachedKafkaProducer\n+      }\n+      producer\n     } catch {\n-      case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n+      case e@(_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n         if e.getCause != null =>\n         throw e.getCause\n     }\n   }\n \n-  private def paramsToSeq(kafkaParams: ju.Map[String, Object]): Seq[(String, Object)] = {\n-    val paramsSeq: Seq[(String, Object)] = kafkaParams.asScala.toSeq.sortBy(x => x._1)\n+  private def paramsToSeq(kafkaParamsMap: ju.Map[String, Object]): Seq[(String, Object)] = {\n+    val paramsSeq: Seq[(String, Object)] = kafkaParamsMap.asScala.toSeq.sortBy(x => x._1)\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n-  private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n-    val paramsSeq = paramsToSeq(kafkaParams)\n-    guavaCache.invalidate(paramsSeq)\n+  /* Release a kafka producer back to the kafka cache. We simply decrement it's inuse count. */\n+  private[kafka010] def release(producer: CachedKafkaProducer, failing: Boolean): Unit = {\n+    this.synchronized {\n+      // It should be ok to call release multiple times on the same producer object.\n+      if (producer.inUse()) {\n+        // So that we do not end up with -ve in-use counts.\n+        producer.inUseCount.decrementAndGet()\n+        logDebug(s\"Released producer $producer.\")\n+      } else {\n+        logWarning(s\"Tried to release a not in use producer, $producer.\")\n+      }\n+      if (failing) {\n+        // If this producer is failing to write, we remove it from cache.\n+        // So that it is re-created, eventually.\n+        val cachedProducer = guavaCache.getIfPresent(producer.kafkaParams)\n+        if (cachedProducer != null && cachedProducer.id == producer.id) {\n+          logDebug(s\"Invalidating a failing producer: $producer.\")\n+          guavaCache.invalidate(producer.kafkaParams)\n+        }\n+      }\n+    }\n+    if (!producer.inUse() && !producer.isCached) {"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "> Maybe just removing closeQueue might resolve the race condition.\r\n\r\n@HeartSaVioR At the first glance don't really see how to solve it without `closeQueue`. If the task failed failed for instance (maybe producer issue maybe not) several other users not yet finished so cannot be closed immediately. Yeah, there is a reference to this consumer on the user side but since there is no guarantee they will call release with that specific producer I think it's better to have this queue.\r\n\r\nI see the solution in the proper synchronization. As an optional change I like how guava does the clean-up and I would move this code:\r\n```\r\n    for (p <- closeQueue.iterator().asScala) {\r\n      if (!p.inUse()) {\r\n        closeQueue.remove(p)\r\n        p.close()\r\n      }\r\n    }\r\n```\r\nat the end of `release` without any condition. If its empty doesn't consume much ticks and more safe to deal with producers in the queue.\r\n",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-08T07:43:50Z",
    "diffHunk": "@@ -40,81 +90,114 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n       \"spark.kafka.producer.cache.timeout\",\n       s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}.\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParamsMap)\n     try {\n-      guavaCache.get(paramsSeq)\n+      val producer = this.synchronized {\n+        val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+        cachedKafkaProducer.inUseCount.incrementAndGet()\n+        logDebug(s\"Granted producer $cachedKafkaProducer\")\n+        cachedKafkaProducer\n+      }\n+      producer\n     } catch {\n-      case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n+      case e@(_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n         if e.getCause != null =>\n         throw e.getCause\n     }\n   }\n \n-  private def paramsToSeq(kafkaParams: ju.Map[String, Object]): Seq[(String, Object)] = {\n-    val paramsSeq: Seq[(String, Object)] = kafkaParams.asScala.toSeq.sortBy(x => x._1)\n+  private def paramsToSeq(kafkaParamsMap: ju.Map[String, Object]): Seq[(String, Object)] = {\n+    val paramsSeq: Seq[(String, Object)] = kafkaParamsMap.asScala.toSeq.sortBy(x => x._1)\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n-  private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n-    val paramsSeq = paramsToSeq(kafkaParams)\n-    guavaCache.invalidate(paramsSeq)\n+  /* Release a kafka producer back to the kafka cache. We simply decrement it's inuse count. */\n+  private[kafka010] def release(producer: CachedKafkaProducer, failing: Boolean): Unit = {\n+    this.synchronized {\n+      // It should be ok to call release multiple times on the same producer object.\n+      if (producer.inUse()) {\n+        // So that we do not end up with -ve in-use counts.\n+        producer.inUseCount.decrementAndGet()\n+        logDebug(s\"Released producer $producer.\")\n+      } else {\n+        logWarning(s\"Tried to release a not in use producer, $producer.\")\n+      }\n+      if (failing) {\n+        // If this producer is failing to write, we remove it from cache.\n+        // So that it is re-created, eventually.\n+        val cachedProducer = guavaCache.getIfPresent(producer.kafkaParams)\n+        if (cachedProducer != null && cachedProducer.id == producer.id) {\n+          logDebug(s\"Invalidating a failing producer: $producer.\")\n+          guavaCache.invalidate(producer.kafkaParams)\n+        }\n+      }\n+    }\n+    if (!producer.inUse() && !producer.isCached) {"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "@gaborgsomogyi \r\n\r\n> Yeah, there is a reference to this consumer on the user side but since there is no guarantee they will call release with that specific producer I think it's better to have this queue.\r\n\r\nThe situation is same even having closeQueue. If we suppose the case any borrower doesn't call release (say, leak on releasing resource), producer never reaches inCount = 0 and producer wouldn't be closed in closeQueue.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-08T08:25:19Z",
    "diffHunk": "@@ -40,81 +90,114 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n       \"spark.kafka.producer.cache.timeout\",\n       s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}.\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParamsMap)\n     try {\n-      guavaCache.get(paramsSeq)\n+      val producer = this.synchronized {\n+        val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+        cachedKafkaProducer.inUseCount.incrementAndGet()\n+        logDebug(s\"Granted producer $cachedKafkaProducer\")\n+        cachedKafkaProducer\n+      }\n+      producer\n     } catch {\n-      case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n+      case e@(_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n         if e.getCause != null =>\n         throw e.getCause\n     }\n   }\n \n-  private def paramsToSeq(kafkaParams: ju.Map[String, Object]): Seq[(String, Object)] = {\n-    val paramsSeq: Seq[(String, Object)] = kafkaParams.asScala.toSeq.sortBy(x => x._1)\n+  private def paramsToSeq(kafkaParamsMap: ju.Map[String, Object]): Seq[(String, Object)] = {\n+    val paramsSeq: Seq[(String, Object)] = kafkaParamsMap.asScala.toSeq.sortBy(x => x._1)\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n-  private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n-    val paramsSeq = paramsToSeq(kafkaParams)\n-    guavaCache.invalidate(paramsSeq)\n+  /* Release a kafka producer back to the kafka cache. We simply decrement it's inuse count. */\n+  private[kafka010] def release(producer: CachedKafkaProducer, failing: Boolean): Unit = {\n+    this.synchronized {\n+      // It should be ok to call release multiple times on the same producer object.\n+      if (producer.inUse()) {\n+        // So that we do not end up with -ve in-use counts.\n+        producer.inUseCount.decrementAndGet()\n+        logDebug(s\"Released producer $producer.\")\n+      } else {\n+        logWarning(s\"Tried to release a not in use producer, $producer.\")\n+      }\n+      if (failing) {\n+        // If this producer is failing to write, we remove it from cache.\n+        // So that it is re-created, eventually.\n+        val cachedProducer = guavaCache.getIfPresent(producer.kafkaParams)\n+        if (cachedProducer != null && cachedProducer.id == producer.id) {\n+          logDebug(s\"Invalidating a failing producer: $producer.\")\n+          guavaCache.invalidate(producer.kafkaParams)\n+        }\n+      }\n+    }\n+    if (!producer.inUse() && !producer.isCached) {"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "I see the point now and makes sense. This case both way is fine for me.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-08T08:33:18Z",
    "diffHunk": "@@ -40,81 +90,114 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n       \"spark.kafka.producer.cache.timeout\",\n       s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}.\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParamsMap)\n     try {\n-      guavaCache.get(paramsSeq)\n+      val producer = this.synchronized {\n+        val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+        cachedKafkaProducer.inUseCount.incrementAndGet()\n+        logDebug(s\"Granted producer $cachedKafkaProducer\")\n+        cachedKafkaProducer\n+      }\n+      producer\n     } catch {\n-      case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n+      case e@(_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n         if e.getCause != null =>\n         throw e.getCause\n     }\n   }\n \n-  private def paramsToSeq(kafkaParams: ju.Map[String, Object]): Seq[(String, Object)] = {\n-    val paramsSeq: Seq[(String, Object)] = kafkaParams.asScala.toSeq.sortBy(x => x._1)\n+  private def paramsToSeq(kafkaParamsMap: ju.Map[String, Object]): Seq[(String, Object)] = {\n+    val paramsSeq: Seq[(String, Object)] = kafkaParamsMap.asScala.toSeq.sortBy(x => x._1)\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n-  private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n-    val paramsSeq = paramsToSeq(kafkaParams)\n-    guavaCache.invalidate(paramsSeq)\n+  /* Release a kafka producer back to the kafka cache. We simply decrement it's inuse count. */\n+  private[kafka010] def release(producer: CachedKafkaProducer, failing: Boolean): Unit = {\n+    this.synchronized {\n+      // It should be ok to call release multiple times on the same producer object.\n+      if (producer.inUse()) {\n+        // So that we do not end up with -ve in-use counts.\n+        producer.inUseCount.decrementAndGet()\n+        logDebug(s\"Released producer $producer.\")\n+      } else {\n+        logWarning(s\"Tried to release a not in use producer, $producer.\")\n+      }\n+      if (failing) {\n+        // If this producer is failing to write, we remove it from cache.\n+        // So that it is re-created, eventually.\n+        val cachedProducer = guavaCache.getIfPresent(producer.kafkaParams)\n+        if (cachedProducer != null && cachedProducer.id == producer.id) {\n+          logDebug(s\"Invalidating a failing producer: $producer.\")\n+          guavaCache.invalidate(producer.kafkaParams)\n+        }\n+      }\n+    }\n+    if (!producer.inUse() && !producer.isCached) {"
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "Maybe close queue is helpful, as we can delegate a close to be done outside the synchronized block.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-10T06:59:04Z",
    "diffHunk": "@@ -40,81 +90,114 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n       \"spark.kafka.producer.cache.timeout\",\n       s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}.\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParamsMap)\n     try {\n-      guavaCache.get(paramsSeq)\n+      val producer = this.synchronized {\n+        val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+        cachedKafkaProducer.inUseCount.incrementAndGet()\n+        logDebug(s\"Granted producer $cachedKafkaProducer\")\n+        cachedKafkaProducer\n+      }\n+      producer\n     } catch {\n-      case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n+      case e@(_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n         if e.getCause != null =>\n         throw e.getCause\n     }\n   }\n \n-  private def paramsToSeq(kafkaParams: ju.Map[String, Object]): Seq[(String, Object)] = {\n-    val paramsSeq: Seq[(String, Object)] = kafkaParams.asScala.toSeq.sortBy(x => x._1)\n+  private def paramsToSeq(kafkaParamsMap: ju.Map[String, Object]): Seq[(String, Object)] = {\n+    val paramsSeq: Seq[(String, Object)] = kafkaParamsMap.asScala.toSeq.sortBy(x => x._1)\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n-  private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n-    val paramsSeq = paramsToSeq(kafkaParams)\n-    guavaCache.invalidate(paramsSeq)\n+  /* Release a kafka producer back to the kafka cache. We simply decrement it's inuse count. */\n+  private[kafka010] def release(producer: CachedKafkaProducer, failing: Boolean): Unit = {\n+    this.synchronized {\n+      // It should be ok to call release multiple times on the same producer object.\n+      if (producer.inUse()) {\n+        // So that we do not end up with -ve in-use counts.\n+        producer.inUseCount.decrementAndGet()\n+        logDebug(s\"Released producer $producer.\")\n+      } else {\n+        logWarning(s\"Tried to release a not in use producer, $producer.\")\n+      }\n+      if (failing) {\n+        // If this producer is failing to write, we remove it from cache.\n+        // So that it is re-created, eventually.\n+        val cachedProducer = guavaCache.getIfPresent(producer.kafkaParams)\n+        if (cachedProducer != null && cachedProducer.id == producer.id) {\n+          logDebug(s\"Invalidating a failing producer: $producer.\")\n+          guavaCache.invalidate(producer.kafkaParams)\n+        }\n+      }\n+    }\n+    if (!producer.inUse() && !producer.isCached) {"
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "May be better to have each comment for each method - Intended for testing purpose only.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-30T02:35:11Z",
    "diffHunk": "@@ -40,81 +90,114 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n       \"spark.kafka.producer.cache.timeout\",\n       s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}.\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParamsMap)\n     try {\n-      guavaCache.get(paramsSeq)\n+      val producer = this.synchronized {\n+        val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+        cachedKafkaProducer.inUseCount.incrementAndGet()\n+        logDebug(s\"Granted producer $cachedKafkaProducer\")\n+        cachedKafkaProducer\n+      }\n+      producer\n     } catch {\n-      case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n+      case e@(_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n         if e.getCause != null =>\n         throw e.getCause\n     }\n   }\n \n-  private def paramsToSeq(kafkaParams: ju.Map[String, Object]): Seq[(String, Object)] = {\n-    val paramsSeq: Seq[(String, Object)] = kafkaParams.asScala.toSeq.sortBy(x => x._1)\n+  private def paramsToSeq(kafkaParamsMap: ju.Map[String, Object]): Seq[(String, Object)] = {\n+    val paramsSeq: Seq[(String, Object)] = kafkaParamsMap.asScala.toSeq.sortBy(x => x._1)\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n-  private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n-    val paramsSeq = paramsToSeq(kafkaParams)\n-    guavaCache.invalidate(paramsSeq)\n+  /* Release a kafka producer back to the kafka cache. We simply decrement it's inuse count. */\n+  private[kafka010] def release(producer: CachedKafkaProducer, failing: Boolean): Unit = {\n+    this.synchronized {\n+      // It should be ok to call release multiple times on the same producer object.\n+      if (producer.inUse()) {\n+        // So that we do not end up with -ve in-use counts.\n+        producer.inUseCount.decrementAndGet()\n+        logDebug(s\"Released producer $producer.\")\n+      } else {\n+        logWarning(s\"Tried to release a not in use producer, $producer.\")\n+      }\n+      if (failing) {\n+        // If this producer is failing to write, we remove it from cache.\n+        // So that it is re-created, eventually.\n+        val cachedProducer = guavaCache.getIfPresent(producer.kafkaParams)\n+        if (cachedProducer != null && cachedProducer.id == producer.id) {\n+          logDebug(s\"Invalidating a failing producer: $producer.\")\n+          guavaCache.invalidate(producer.kafkaParams)\n+        }\n+      }\n+    }\n+    if (!producer.inUse() && !producer.isCached) {\n+      // it will take care of removing it from close queue as well.\n+      close(producer)\n+    }\n   }\n \n-  /** Auto close on cache evict */\n-  private def close(paramsSeq: Seq[(String, Object)], producer: Producer): Unit = {\n-    try {\n-      logInfo(s\"Closing the KafkaProducer with params: ${paramsSeq.mkString(\"\\n\")}.\")\n-      producer.close()\n-    } catch {\n-      case NonFatal(e) => logWarning(\"Error while closing kafka producer.\", e)\n+  /** Close this producer and process pending closes. */\n+  private def close(producer: CachedKafkaProducer): Unit = {\n+    producer.close()\n+    // Check and close any other producers previously evicted, but pending to be closed.\n+    for (p <- closeQueue.iterator().asScala) {\n+      if (!p.inUse()) {\n+        closeQueue.remove(p)\n+        p.close()\n+      }\n     }\n   }\n \n+  // Intended for testing purpose only.\n   private[kafka010] def clear(): Unit = {\n-    logInfo(\"Cleaning up guava cache.\")\n+    logInfo(\"Cleaning up guava cache and force closing all kafka producers.\")\n     guavaCache.invalidateAll()\n+    for (p <- closeQueue.iterator().asScala) {\n+      p.close()\n+    }\n+    closeQueue.clear()\n   }\n \n-  // Intended for testing purpose only.\n-  private def getAsMap: ConcurrentMap[Seq[(String, Object)], Producer] = guavaCache.asMap()\n+  private[kafka010] def evict(params: Seq[(String, Object)]): Unit = {",
    "line": 233
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "done",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-10T07:03:17Z",
    "diffHunk": "@@ -40,81 +90,114 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n       \"spark.kafka.producer.cache.timeout\",\n       s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}.\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParamsMap)\n     try {\n-      guavaCache.get(paramsSeq)\n+      val producer = this.synchronized {\n+        val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+        cachedKafkaProducer.inUseCount.incrementAndGet()\n+        logDebug(s\"Granted producer $cachedKafkaProducer\")\n+        cachedKafkaProducer\n+      }\n+      producer\n     } catch {\n-      case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n+      case e@(_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n         if e.getCause != null =>\n         throw e.getCause\n     }\n   }\n \n-  private def paramsToSeq(kafkaParams: ju.Map[String, Object]): Seq[(String, Object)] = {\n-    val paramsSeq: Seq[(String, Object)] = kafkaParams.asScala.toSeq.sortBy(x => x._1)\n+  private def paramsToSeq(kafkaParamsMap: ju.Map[String, Object]): Seq[(String, Object)] = {\n+    val paramsSeq: Seq[(String, Object)] = kafkaParamsMap.asScala.toSeq.sortBy(x => x._1)\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n-  private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n-    val paramsSeq = paramsToSeq(kafkaParams)\n-    guavaCache.invalidate(paramsSeq)\n+  /* Release a kafka producer back to the kafka cache. We simply decrement it's inuse count. */\n+  private[kafka010] def release(producer: CachedKafkaProducer, failing: Boolean): Unit = {\n+    this.synchronized {\n+      // It should be ok to call release multiple times on the same producer object.\n+      if (producer.inUse()) {\n+        // So that we do not end up with -ve in-use counts.\n+        producer.inUseCount.decrementAndGet()\n+        logDebug(s\"Released producer $producer.\")\n+      } else {\n+        logWarning(s\"Tried to release a not in use producer, $producer.\")\n+      }\n+      if (failing) {\n+        // If this producer is failing to write, we remove it from cache.\n+        // So that it is re-created, eventually.\n+        val cachedProducer = guavaCache.getIfPresent(producer.kafkaParams)\n+        if (cachedProducer != null && cachedProducer.id == producer.id) {\n+          logDebug(s\"Invalidating a failing producer: $producer.\")\n+          guavaCache.invalidate(producer.kafkaParams)\n+        }\n+      }\n+    }\n+    if (!producer.inUse() && !producer.isCached) {\n+      // it will take care of removing it from close queue as well.\n+      close(producer)\n+    }\n   }\n \n-  /** Auto close on cache evict */\n-  private def close(paramsSeq: Seq[(String, Object)], producer: Producer): Unit = {\n-    try {\n-      logInfo(s\"Closing the KafkaProducer with params: ${paramsSeq.mkString(\"\\n\")}.\")\n-      producer.close()\n-    } catch {\n-      case NonFatal(e) => logWarning(\"Error while closing kafka producer.\", e)\n+  /** Close this producer and process pending closes. */\n+  private def close(producer: CachedKafkaProducer): Unit = {\n+    producer.close()\n+    // Check and close any other producers previously evicted, but pending to be closed.\n+    for (p <- closeQueue.iterator().asScala) {\n+      if (!p.inUse()) {\n+        closeQueue.remove(p)\n+        p.close()\n+      }\n     }\n   }\n \n+  // Intended for testing purpose only.\n   private[kafka010] def clear(): Unit = {\n-    logInfo(\"Cleaning up guava cache.\")\n+    logInfo(\"Cleaning up guava cache and force closing all kafka producers.\")\n     guavaCache.invalidateAll()\n+    for (p <- closeQueue.iterator().asScala) {\n+      p.close()\n+    }\n+    closeQueue.clear()\n   }\n \n-  // Intended for testing purpose only.\n-  private def getAsMap: ConcurrentMap[Seq[(String, Object)], Producer] = guavaCache.asMap()\n+  private[kafka010] def evict(params: Seq[(String, Object)]): Unit = {",
    "line": 233
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Same here.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-03-30T02:35:18Z",
    "diffHunk": "@@ -40,81 +90,114 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n       \"spark.kafka.producer.cache.timeout\",\n       s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}.\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParamsMap)\n     try {\n-      guavaCache.get(paramsSeq)\n+      val producer = this.synchronized {\n+        val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+        cachedKafkaProducer.inUseCount.incrementAndGet()\n+        logDebug(s\"Granted producer $cachedKafkaProducer\")\n+        cachedKafkaProducer\n+      }\n+      producer\n     } catch {\n-      case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n+      case e@(_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n         if e.getCause != null =>\n         throw e.getCause\n     }\n   }\n \n-  private def paramsToSeq(kafkaParams: ju.Map[String, Object]): Seq[(String, Object)] = {\n-    val paramsSeq: Seq[(String, Object)] = kafkaParams.asScala.toSeq.sortBy(x => x._1)\n+  private def paramsToSeq(kafkaParamsMap: ju.Map[String, Object]): Seq[(String, Object)] = {\n+    val paramsSeq: Seq[(String, Object)] = kafkaParamsMap.asScala.toSeq.sortBy(x => x._1)\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n-  private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n-    val paramsSeq = paramsToSeq(kafkaParams)\n-    guavaCache.invalidate(paramsSeq)\n+  /* Release a kafka producer back to the kafka cache. We simply decrement it's inuse count. */\n+  private[kafka010] def release(producer: CachedKafkaProducer, failing: Boolean): Unit = {\n+    this.synchronized {\n+      // It should be ok to call release multiple times on the same producer object.\n+      if (producer.inUse()) {\n+        // So that we do not end up with -ve in-use counts.\n+        producer.inUseCount.decrementAndGet()\n+        logDebug(s\"Released producer $producer.\")\n+      } else {\n+        logWarning(s\"Tried to release a not in use producer, $producer.\")\n+      }\n+      if (failing) {\n+        // If this producer is failing to write, we remove it from cache.\n+        // So that it is re-created, eventually.\n+        val cachedProducer = guavaCache.getIfPresent(producer.kafkaParams)\n+        if (cachedProducer != null && cachedProducer.id == producer.id) {\n+          logDebug(s\"Invalidating a failing producer: $producer.\")\n+          guavaCache.invalidate(producer.kafkaParams)\n+        }\n+      }\n+    }\n+    if (!producer.inUse() && !producer.isCached) {\n+      // it will take care of removing it from close queue as well.\n+      close(producer)\n+    }\n   }\n \n-  /** Auto close on cache evict */\n-  private def close(paramsSeq: Seq[(String, Object)], producer: Producer): Unit = {\n-    try {\n-      logInfo(s\"Closing the KafkaProducer with params: ${paramsSeq.mkString(\"\\n\")}.\")\n-      producer.close()\n-    } catch {\n-      case NonFatal(e) => logWarning(\"Error while closing kafka producer.\", e)\n+  /** Close this producer and process pending closes. */\n+  private def close(producer: CachedKafkaProducer): Unit = {\n+    producer.close()\n+    // Check and close any other producers previously evicted, but pending to be closed.\n+    for (p <- closeQueue.iterator().asScala) {\n+      if (!p.inUse()) {\n+        closeQueue.remove(p)\n+        p.close()\n+      }\n     }\n   }\n \n+  // Intended for testing purpose only.\n   private[kafka010] def clear(): Unit = {\n-    logInfo(\"Cleaning up guava cache.\")\n+    logInfo(\"Cleaning up guava cache and force closing all kafka producers.\")\n     guavaCache.invalidateAll()\n+    for (p <- closeQueue.iterator().asScala) {\n+      p.close()\n+    }\n+    closeQueue.clear()\n   }\n \n-  // Intended for testing purpose only.\n-  private def getAsMap: ConcurrentMap[Seq[(String, Object)], Producer] = guavaCache.asMap()\n+  private[kafka010] def evict(params: Seq[(String, Object)]): Unit = {\n+    guavaCache.invalidate(params)\n+  }\n+\n+  private[kafka010] def getAsMap: ConcurrentMap[Seq[(String, Object)], CachedKafkaProducer] =",
    "line": 240
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "done",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-10T07:03:10Z",
    "diffHunk": "@@ -40,81 +90,114 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n       \"spark.kafka.producer.cache.timeout\",\n       s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      if (producer.inUse()) {\n+        logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}.\")\n+        // When `inuse` producer is evicted we wait for it to be released before finally closing it.\n+        closeQueue.add(producer)\n+        producer.unCache()\n+      } else {\n+        close(producer)\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParamsMap)\n     try {\n-      guavaCache.get(paramsSeq)\n+      val producer = this.synchronized {\n+        val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+        cachedKafkaProducer.inUseCount.incrementAndGet()\n+        logDebug(s\"Granted producer $cachedKafkaProducer\")\n+        cachedKafkaProducer\n+      }\n+      producer\n     } catch {\n-      case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n+      case e@(_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n         if e.getCause != null =>\n         throw e.getCause\n     }\n   }\n \n-  private def paramsToSeq(kafkaParams: ju.Map[String, Object]): Seq[(String, Object)] = {\n-    val paramsSeq: Seq[(String, Object)] = kafkaParams.asScala.toSeq.sortBy(x => x._1)\n+  private def paramsToSeq(kafkaParamsMap: ju.Map[String, Object]): Seq[(String, Object)] = {\n+    val paramsSeq: Seq[(String, Object)] = kafkaParamsMap.asScala.toSeq.sortBy(x => x._1)\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n-  private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n-    val paramsSeq = paramsToSeq(kafkaParams)\n-    guavaCache.invalidate(paramsSeq)\n+  /* Release a kafka producer back to the kafka cache. We simply decrement it's inuse count. */\n+  private[kafka010] def release(producer: CachedKafkaProducer, failing: Boolean): Unit = {\n+    this.synchronized {\n+      // It should be ok to call release multiple times on the same producer object.\n+      if (producer.inUse()) {\n+        // So that we do not end up with -ve in-use counts.\n+        producer.inUseCount.decrementAndGet()\n+        logDebug(s\"Released producer $producer.\")\n+      } else {\n+        logWarning(s\"Tried to release a not in use producer, $producer.\")\n+      }\n+      if (failing) {\n+        // If this producer is failing to write, we remove it from cache.\n+        // So that it is re-created, eventually.\n+        val cachedProducer = guavaCache.getIfPresent(producer.kafkaParams)\n+        if (cachedProducer != null && cachedProducer.id == producer.id) {\n+          logDebug(s\"Invalidating a failing producer: $producer.\")\n+          guavaCache.invalidate(producer.kafkaParams)\n+        }\n+      }\n+    }\n+    if (!producer.inUse() && !producer.isCached) {\n+      // it will take care of removing it from close queue as well.\n+      close(producer)\n+    }\n   }\n \n-  /** Auto close on cache evict */\n-  private def close(paramsSeq: Seq[(String, Object)], producer: Producer): Unit = {\n-    try {\n-      logInfo(s\"Closing the KafkaProducer with params: ${paramsSeq.mkString(\"\\n\")}.\")\n-      producer.close()\n-    } catch {\n-      case NonFatal(e) => logWarning(\"Error while closing kafka producer.\", e)\n+  /** Close this producer and process pending closes. */\n+  private def close(producer: CachedKafkaProducer): Unit = {\n+    producer.close()\n+    // Check and close any other producers previously evicted, but pending to be closed.\n+    for (p <- closeQueue.iterator().asScala) {\n+      if (!p.inUse()) {\n+        closeQueue.remove(p)\n+        p.close()\n+      }\n     }\n   }\n \n+  // Intended for testing purpose only.\n   private[kafka010] def clear(): Unit = {\n-    logInfo(\"Cleaning up guava cache.\")\n+    logInfo(\"Cleaning up guava cache and force closing all kafka producers.\")\n     guavaCache.invalidateAll()\n+    for (p <- closeQueue.iterator().asScala) {\n+      p.close()\n+    }\n+    closeQueue.clear()\n   }\n \n-  // Intended for testing purpose only.\n-  private def getAsMap: ConcurrentMap[Seq[(String, Object)], Producer] = guavaCache.asMap()\n+  private[kafka010] def evict(params: Seq[(String, Object)]): Unit = {\n+    guavaCache.invalidate(params)\n+  }\n+\n+  private[kafka010] def getAsMap: ConcurrentMap[Seq[(String, Object)], CachedKafkaProducer] =",
    "line": 240
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Previously we've discussed where to put config update and you've convinced me to put delegation token inside the producer key (time eviction takes care of old and not used producers + this way tokens can be used without task retry). As I've just taken a look at the code despite of this agreement it's not done somehow. As I've just filed multi-cluster DT support PR you can see what I mean [here](https://github.com/apache/spark/pull/24305/files#diff-ac8844e8d791a75aaee3d0d10bfc1f2aR78).",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-08T07:33:18Z",
    "diffHunk": "@@ -18,20 +18,70 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n+\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  private def updatedAuthConfigIfNeeded(kafkaParamsMap: ju.Map[String, Object]) =\n+    KafkaConfigUpdater(\"executor\", kafkaParamsMap.asScala.toMap)\n+      .setAuthenticationConfigIfNeeded()\n+      .build()\n+\n+  lazy val kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]] = {\n+    val producer = new KafkaProducer[Array[Byte], Array[Byte]](updatedAuthConfigIfNeeded(configMap))"
  }, {
    "author": {
      "login": "ScrapCodes"
    },
    "body": "I will go through those changes soon, thanks for letting me know.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-10T06:59:52Z",
    "diffHunk": "@@ -18,20 +18,70 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n+\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  private def updatedAuthConfigIfNeeded(kafkaParamsMap: ju.Map[String, Object]) =\n+    KafkaConfigUpdater(\"executor\", kafkaParamsMap.asScala.toMap)\n+      .setAuthenticationConfigIfNeeded()\n+      .build()\n+\n+  lazy val kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]] = {\n+    val producer = new KafkaProducer[Array[Byte], Array[Byte]](updatedAuthConfigIfNeeded(configMap))"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Still don't see the change in the last version.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-10T12:14:37Z",
    "diffHunk": "@@ -18,20 +18,70 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n+\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  private def updatedAuthConfigIfNeeded(kafkaParamsMap: ju.Map[String, Object]) =\n+    KafkaConfigUpdater(\"executor\", kafkaParamsMap.asScala.toMap)\n+      .setAuthenticationConfigIfNeeded()\n+      .build()\n+\n+  lazy val kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]] = {\n+    val producer = new KafkaProducer[Array[Byte], Array[Byte]](updatedAuthConfigIfNeeded(configMap))"
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Since creation is debug close can be the same.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-10T12:18:46Z",
    "diffHunk": "@@ -18,20 +18,68 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+import javax.annotation.concurrent.GuardedBy\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n+\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  private def updatedAuthConfigIfNeeded(kafkaParamsMap: ju.Map[String, Object]) =\n+    KafkaConfigUpdater(\"executor\", kafkaParamsMap.asScala.toMap)\n+      .setAuthenticationConfigIfNeeded()\n+      .build()\n+\n+  lazy val kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]] = {\n+    val producer = new KafkaProducer[Array[Byte], Array[Byte]](updatedAuthConfigIfNeeded(configMap))\n+    logDebug(s\"Created a new instance of KafkaProducer for \" +\n+      s\"$kafkaParams with Id: $id\")\n+    closed = false\n+    producer\n+  }\n+  @GuardedBy(\"this\")\n+  private var closed: Boolean = true\n+  private def close(): Unit = {\n+    try {\n+      this.synchronized {\n+        if (!closed) {\n+          closed = true\n+          kafkaProducer.close()\n+          logInfo(s\"Closed kafka producer: $this\")"
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Nit: Not sure if I understand this comment. Maybe negative?",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-10T12:30:52Z",
    "diffHunk": "@@ -40,81 +88,113 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n       \"spark.kafka.producer.cache.timeout\",\n       s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}.\")\n+      if (producer.inUse()) {\n+        // When `inuse` producer is evicted we wait for it to be released by all the tasks,\n+        // before finally closing it.\n+        closeQueue.add(producer)\n+      } else {\n+        producer.close()\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParamsMap)\n     try {\n-      guavaCache.get(paramsSeq)\n+      val producer = this.synchronized {\n+        val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+        cachedKafkaProducer.inUseCount.incrementAndGet()\n+        logDebug(s\"Granted producer $cachedKafkaProducer\")\n+        cachedKafkaProducer\n+      }\n+      producer\n     } catch {\n-      case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n+      case e@(_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n         if e.getCause != null =>\n         throw e.getCause\n     }\n   }\n \n-  private def paramsToSeq(kafkaParams: ju.Map[String, Object]): Seq[(String, Object)] = {\n-    val paramsSeq: Seq[(String, Object)] = kafkaParams.asScala.toSeq.sortBy(x => x._1)\n+  private def paramsToSeq(kafkaParamsMap: ju.Map[String, Object]): Seq[(String, Object)] = {\n+    val paramsSeq: Seq[(String, Object)] = kafkaParamsMap.asScala.toSeq.sortBy(x => x._1)\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n-  private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n-    val paramsSeq = paramsToSeq(kafkaParams)\n-    guavaCache.invalidate(paramsSeq)\n+  /* Release a kafka producer back to the kafka cache. We simply decrement it's inuse count. */\n+  private[kafka010] def release(producer: CachedKafkaProducer, failing: Boolean): Unit = {\n+    this.synchronized {\n+      // It should be ok to call release multiple times on the same producer object.\n+      if (producer.inUse()) {\n+        // So that we do not end up with -ve in-use counts."
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Asked once but ask again because you've resolved the conversation. As you've convinced me to include delegation token in key don't you plan to modify the code accordingly?",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-10T13:11:27Z",
    "diffHunk": "@@ -18,20 +18,68 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+import javax.annotation.concurrent.GuardedBy\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n+\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  private def updatedAuthConfigIfNeeded(kafkaParamsMap: ju.Map[String, Object]) ="
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Nit: Unused parameter.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-04-10T13:13:51Z",
    "diffHunk": "@@ -40,81 +88,113 @@ private[kafka010] object CachedKafkaProducer extends Logging {\n       \"spark.kafka.producer.cache.timeout\",\n       s\"${defaultCacheExpireTimeout}ms\")).getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      val configMap = config.map(x => x._1 -> x._2).toMap.asJava\n-      createKafkaProducer(configMap)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n-      logDebug(\n-        s\"Evicting kafka producer $producer params: $paramsSeq, due to ${notification.getCause}\")\n-      close(paramsSeq, producer)\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n+      logDebug(s\"Evicting kafka producer $producer, due to ${notification.getCause}.\")\n+      if (producer.inUse()) {\n+        // When `inuse` producer is evicted we wait for it to be released by all the tasks,\n+        // before finally closing it.\n+        closeQueue.add(producer)\n+      } else {\n+        producer.close()\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(producerConfiguration: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", producerConfiguration.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val kafkaProducer: Producer = new Producer(updatedKafkaProducerConfiguration)\n-    logDebug(s\"Created a new instance of KafkaProducer for $updatedKafkaProducerConfiguration.\")\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParams)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(kafkaParamsMap)\n     try {\n-      guavaCache.get(paramsSeq)\n+      val producer = this.synchronized {\n+        val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+        cachedKafkaProducer.inUseCount.incrementAndGet()\n+        logDebug(s\"Granted producer $cachedKafkaProducer\")\n+        cachedKafkaProducer\n+      }\n+      producer\n     } catch {\n-      case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n+      case e@(_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n         if e.getCause != null =>\n         throw e.getCause\n     }\n   }\n \n-  private def paramsToSeq(kafkaParams: ju.Map[String, Object]): Seq[(String, Object)] = {\n-    val paramsSeq: Seq[(String, Object)] = kafkaParams.asScala.toSeq.sortBy(x => x._1)\n+  private def paramsToSeq(kafkaParamsMap: ju.Map[String, Object]): Seq[(String, Object)] = {\n+    val paramsSeq: Seq[(String, Object)] = kafkaParamsMap.asScala.toSeq.sortBy(x => x._1)\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n-  private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n-    val paramsSeq = paramsToSeq(kafkaParams)\n-    guavaCache.invalidate(paramsSeq)\n+  /* Release a kafka producer back to the kafka cache. We simply decrement it's inuse count. */\n+  private[kafka010] def release(producer: CachedKafkaProducer, failing: Boolean): Unit = {\n+    this.synchronized {\n+      // It should be ok to call release multiple times on the same producer object.\n+      if (producer.inUse()) {\n+        // So that we do not end up with -ve in-use counts.\n+        producer.inUseCount.decrementAndGet()\n+        logDebug(s\"Released producer $producer.\")\n+      } else {\n+        logWarning(s\"Tried to release a not in use producer, $producer.\")\n+      }\n+      if (failing) {\n+        // If this producer is failing to write, we remove it from cache.\n+        // So that it is re-created, eventually.\n+        val cachedProducer = guavaCache.getIfPresent(producer.kafkaParams)\n+        if (cachedProducer != null && cachedProducer.id == producer.id) {\n+          logDebug(s\"Invalidating a failing producer: $producer.\")\n+          guavaCache.invalidate(producer.kafkaParams)\n+        }\n+      }\n+    }\n+    // We need a close queue, so that we can close the producer(s) outside of a synchronized block.\n+    processPendingClose(producer)\n   }\n \n-  /** Auto close on cache evict */\n-  private def close(paramsSeq: Seq[(String, Object)], producer: Producer): Unit = {\n-    try {\n-      logInfo(s\"Closing the KafkaProducer with params: ${paramsSeq.mkString(\"\\n\")}.\")\n-      producer.close()\n-    } catch {\n-      case NonFatal(e) => logWarning(\"Error while closing kafka producer.\", e)\n+  /** Process pending closes. */\n+  private def processPendingClose(producer: CachedKafkaProducer): Unit = {"
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "jose-torres"
    },
    "body": "Is this required? It's risky to add new global locks to things.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-08-19T16:38:44Z",
    "diffHunk": "@@ -18,111 +18,191 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+import javax.annotation.concurrent.GuardedBy\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n import org.apache.spark.kafka010.{KafkaConfigUpdater, KafkaRedactionUtil}\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n+\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  lazy val kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]] = {\n+    val producer = new KafkaProducer[Array[Byte], Array[Byte]](configMap)\n+    if (log.isDebugEnabled()) {\n+      val redactedParamsSeq = KafkaRedactionUtil.redactParams(kafkaParams)\n+      logDebug(s\"Created a new instance of KafkaProducer for $redactedParamsSeq, with Id: $id.\")\n+    }\n+    closed = false\n+    producer\n+  }\n+  @GuardedBy(\"this\")\n+  private var closed: Boolean = true\n+  private def close(): Unit = {\n+    try {\n+      this.synchronized {\n+        if (!closed) {\n+          closed = true\n+          if (log.isInfoEnabled()) {\n+            val redactedParamsSeq = KafkaRedactionUtil.redactParams(kafkaParams)\n+            logInfo(s\"Closing the KafkaProducer with params: ${redactedParamsSeq.mkString(\"\\n\")}.\")\n+          }\n+          kafkaProducer.close()\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error while closing kafka producer with params: $kafkaParams\", e)\n+    }\n+  }\n+\n+  private def inUse(): Boolean = inUseCount.get() > 0\n+\n+  private[kafka010] def getInUseCount: Int = inUseCount.get()\n \n-  private type Producer = KafkaProducer[Array[Byte], Array[Byte]]\n+  private[kafka010] def getKafkaParams: Seq[(String, Object)] = kafkaParams\n+\n+  private[kafka010] def flush(): Unit = kafkaProducer.flush()\n+\n+  private[kafka010] def isClosed: Boolean = closed\n+}\n+\n+private[kafka010] object CachedKafkaProducer extends Logging {\n \n   private val defaultCacheExpireTimeout = TimeUnit.MINUTES.toMillis(10)\n \n   private lazy val cacheExpireTimeout: Long = Option(SparkEnv.get)\n     .map(_.conf.get(PRODUCER_CACHE_TIMEOUT))\n     .getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      createKafkaProducer(config)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private def updatedAuthConfigIfNeeded(kafkaParamsMap: ju.Map[String, Object]) =\n+    KafkaConfigUpdater(\"executor\", kafkaParamsMap.asScala.toMap)\n+      .setAuthenticationConfigIfNeeded()\n+      .build()\n+\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n       if (log.isDebugEnabled()) {\n-        val redactedParamsSeq = KafkaRedactionUtil.redactParams(paramsSeq)\n+        val redactedParamsSeq = KafkaRedactionUtil.redactParams(producer.kafkaParams)\n         logDebug(s\"Evicting kafka producer $producer params: $redactedParamsSeq, \" +\n           s\"due to ${notification.getCause}\")\n       }\n-      close(paramsSeq, producer)\n+      if (producer.inUse()) {\n+        // When `inuse` producer is evicted we wait for it to be released by all the tasks,\n+        // before finally closing it.\n+        closeQueue.add(producer)\n+      } else {\n+        producer.close()\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(paramsSeq: Seq[(String, Object)]): Producer = {\n-    val kafkaProducer: Producer = new Producer(paramsSeq.toMap.asJava)\n-    if (log.isDebugEnabled()) {\n-      val redactedParamsSeq = KafkaRedactionUtil.redactParams(paramsSeq)\n-      logDebug(s\"Created a new instance of KafkaProducer for $redactedParamsSeq.\")\n-    }\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", kafkaParams.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(updatedKafkaProducerConfiguration)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(updatedAuthConfigIfNeeded(kafkaParamsMap))\n     try {\n-      guavaCache.get(paramsSeq)\n+      val producer = this.synchronized {",
    "line": 151
  }],
  "prId": 19096
}, {
  "comments": [{
    "author": {
      "login": "jose-torres"
    },
    "body": "But it's not really okay, right? If task A calls release multiple times, the producer might have its inUseCount decremented to 0 even though task B is using it.",
    "commit": "8a0906e11eab36cb92c159915915b77334d34312",
    "createdAt": "2019-08-19T16:40:31Z",
    "diffHunk": "@@ -18,111 +18,191 @@\n package org.apache.spark.sql.kafka010\n \n import java.{util => ju}\n-import java.util.concurrent.{ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.{ConcurrentLinkedQueue, ConcurrentMap, ExecutionException, TimeUnit}\n+import java.util.concurrent.atomic.AtomicInteger\n+import javax.annotation.concurrent.GuardedBy\n+\n+import scala.collection.JavaConverters._\n+import scala.util.control.NonFatal\n \n import com.google.common.cache._\n import com.google.common.util.concurrent.{ExecutionError, UncheckedExecutionException}\n import org.apache.kafka.clients.producer.KafkaProducer\n-import scala.collection.JavaConverters._\n-import scala.util.control.NonFatal\n \n import org.apache.spark.SparkEnv\n import org.apache.spark.internal.Logging\n import org.apache.spark.kafka010.{KafkaConfigUpdater, KafkaRedactionUtil}\n \n-private[kafka010] object CachedKafkaProducer extends Logging {\n+private[kafka010] case class CachedKafkaProducer(\n+    private val id: String = ju.UUID.randomUUID().toString,\n+    private val inUseCount: AtomicInteger = new AtomicInteger(0),\n+    private val kafkaParams: Seq[(String, Object)]) extends Logging {\n+\n+  private val configMap = kafkaParams.map(x => x._1 -> x._2).toMap.asJava\n+\n+  lazy val kafkaProducer: KafkaProducer[Array[Byte], Array[Byte]] = {\n+    val producer = new KafkaProducer[Array[Byte], Array[Byte]](configMap)\n+    if (log.isDebugEnabled()) {\n+      val redactedParamsSeq = KafkaRedactionUtil.redactParams(kafkaParams)\n+      logDebug(s\"Created a new instance of KafkaProducer for $redactedParamsSeq, with Id: $id.\")\n+    }\n+    closed = false\n+    producer\n+  }\n+  @GuardedBy(\"this\")\n+  private var closed: Boolean = true\n+  private def close(): Unit = {\n+    try {\n+      this.synchronized {\n+        if (!closed) {\n+          closed = true\n+          if (log.isInfoEnabled()) {\n+            val redactedParamsSeq = KafkaRedactionUtil.redactParams(kafkaParams)\n+            logInfo(s\"Closing the KafkaProducer with params: ${redactedParamsSeq.mkString(\"\\n\")}.\")\n+          }\n+          kafkaProducer.close()\n+        }\n+      }\n+    } catch {\n+      case NonFatal(e) =>\n+        logWarning(s\"Error while closing kafka producer with params: $kafkaParams\", e)\n+    }\n+  }\n+\n+  private def inUse(): Boolean = inUseCount.get() > 0\n+\n+  private[kafka010] def getInUseCount: Int = inUseCount.get()\n \n-  private type Producer = KafkaProducer[Array[Byte], Array[Byte]]\n+  private[kafka010] def getKafkaParams: Seq[(String, Object)] = kafkaParams\n+\n+  private[kafka010] def flush(): Unit = kafkaProducer.flush()\n+\n+  private[kafka010] def isClosed: Boolean = closed\n+}\n+\n+private[kafka010] object CachedKafkaProducer extends Logging {\n \n   private val defaultCacheExpireTimeout = TimeUnit.MINUTES.toMillis(10)\n \n   private lazy val cacheExpireTimeout: Long = Option(SparkEnv.get)\n     .map(_.conf.get(PRODUCER_CACHE_TIMEOUT))\n     .getOrElse(defaultCacheExpireTimeout)\n \n-  private val cacheLoader = new CacheLoader[Seq[(String, Object)], Producer] {\n-    override def load(config: Seq[(String, Object)]): Producer = {\n-      createKafkaProducer(config)\n+  private val cacheLoader = new CacheLoader[Seq[(String, Object)], CachedKafkaProducer] {\n+    override def load(params: Seq[(String, Object)]): CachedKafkaProducer = {\n+      CachedKafkaProducer(kafkaParams = params)\n     }\n   }\n \n-  private val removalListener = new RemovalListener[Seq[(String, Object)], Producer]() {\n+  private def updatedAuthConfigIfNeeded(kafkaParamsMap: ju.Map[String, Object]) =\n+    KafkaConfigUpdater(\"executor\", kafkaParamsMap.asScala.toMap)\n+      .setAuthenticationConfigIfNeeded()\n+      .build()\n+\n+  private val closeQueue = new ConcurrentLinkedQueue[CachedKafkaProducer]()\n+\n+  private val removalListener = new RemovalListener[Seq[(String, Object)], CachedKafkaProducer]() {\n     override def onRemoval(\n-        notification: RemovalNotification[Seq[(String, Object)], Producer]): Unit = {\n-      val paramsSeq: Seq[(String, Object)] = notification.getKey\n-      val producer: Producer = notification.getValue\n+        notification: RemovalNotification[Seq[(String, Object)], CachedKafkaProducer]): Unit = {\n+      val producer: CachedKafkaProducer = notification.getValue\n       if (log.isDebugEnabled()) {\n-        val redactedParamsSeq = KafkaRedactionUtil.redactParams(paramsSeq)\n+        val redactedParamsSeq = KafkaRedactionUtil.redactParams(producer.kafkaParams)\n         logDebug(s\"Evicting kafka producer $producer params: $redactedParamsSeq, \" +\n           s\"due to ${notification.getCause}\")\n       }\n-      close(paramsSeq, producer)\n+      if (producer.inUse()) {\n+        // When `inuse` producer is evicted we wait for it to be released by all the tasks,\n+        // before finally closing it.\n+        closeQueue.add(producer)\n+      } else {\n+        producer.close()\n+      }\n     }\n   }\n \n-  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], Producer] =\n+  private lazy val guavaCache: LoadingCache[Seq[(String, Object)], CachedKafkaProducer] =\n     CacheBuilder.newBuilder().expireAfterAccess(cacheExpireTimeout, TimeUnit.MILLISECONDS)\n       .removalListener(removalListener)\n-      .build[Seq[(String, Object)], Producer](cacheLoader)\n-\n-  private def createKafkaProducer(paramsSeq: Seq[(String, Object)]): Producer = {\n-    val kafkaProducer: Producer = new Producer(paramsSeq.toMap.asJava)\n-    if (log.isDebugEnabled()) {\n-      val redactedParamsSeq = KafkaRedactionUtil.redactParams(paramsSeq)\n-      logDebug(s\"Created a new instance of KafkaProducer for $redactedParamsSeq.\")\n-    }\n-    kafkaProducer\n-  }\n+      .build[Seq[(String, Object)], CachedKafkaProducer](cacheLoader)\n \n   /**\n    * Get a cached KafkaProducer for a given configuration. If matching KafkaProducer doesn't\n    * exist, a new KafkaProducer will be created. KafkaProducer is thread safe, it is best to keep\n    * one instance per specified kafkaParams.\n    */\n-  private[kafka010] def getOrCreate(kafkaParams: ju.Map[String, Object]): Producer = {\n-    val updatedKafkaProducerConfiguration =\n-      KafkaConfigUpdater(\"executor\", kafkaParams.asScala.toMap)\n-        .setAuthenticationConfigIfNeeded()\n-        .build()\n-    val paramsSeq: Seq[(String, Object)] = paramsToSeq(updatedKafkaProducerConfiguration)\n+  private[kafka010] def acquire(kafkaParamsMap: ju.Map[String, Object]): CachedKafkaProducer = {\n+    val paramsSeq: Seq[(String, Object)] = paramsToSeq(updatedAuthConfigIfNeeded(kafkaParamsMap))\n     try {\n-      guavaCache.get(paramsSeq)\n+      val producer = this.synchronized {\n+        val cachedKafkaProducer: CachedKafkaProducer = guavaCache.get(paramsSeq)\n+        cachedKafkaProducer.inUseCount.incrementAndGet()\n+        logDebug(s\"Granted producer $cachedKafkaProducer\")\n+        cachedKafkaProducer\n+      }\n+      producer\n     } catch {\n-      case e @ (_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n+      case e@(_: ExecutionException | _: UncheckedExecutionException | _: ExecutionError)\n         if e.getCause != null =>\n         throw e.getCause\n     }\n   }\n \n-  private def paramsToSeq(kafkaParams: ju.Map[String, Object]): Seq[(String, Object)] = {\n-    val paramsSeq: Seq[(String, Object)] = kafkaParams.asScala.toSeq.sortBy(x => x._1)\n+  private def paramsToSeq(kafkaParamsMap: ju.Map[String, Object]): Seq[(String, Object)] = {\n+    val paramsSeq: Seq[(String, Object)] = kafkaParamsMap.asScala.toSeq.sortBy(x => x._1)\n     paramsSeq\n   }\n \n-  /** For explicitly closing kafka producer */\n-  private[kafka010] def close(kafkaParams: ju.Map[String, Object]): Unit = {\n-    val paramsSeq = paramsToSeq(kafkaParams)\n-    guavaCache.invalidate(paramsSeq)\n+  /* Release a kafka producer back to the kafka cache. We simply decrement it's inuse count. */\n+  private[kafka010] def release(producer: CachedKafkaProducer, failing: Boolean): Unit = {\n+    this.synchronized {\n+      // It should be ok to call release multiple times on the same producer object.",
    "line": 180
  }],
  "prId": 19096
}]