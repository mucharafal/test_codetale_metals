[{
  "comments": [{
    "author": {
      "login": "jerryshao"
    },
    "body": "Hey Hari, ASF header should be at the top of file :).\n",
    "commit": "e3110b9b551fb0019d6095a16504e8b25d70f962",
    "createdAt": "2014-08-15T05:06:06Z",
    "diffHunk": "@@ -0,0 +1,208 @@\n+package org.apache.spark.streaming.flume.sink\n+\n+import java.net.InetSocketAddress\n+import java.util.concurrent.atomic.AtomicInteger\n+import java.util.concurrent.{CountDownLatch, Executors}\n+\n+import scala.collection.JavaConversions._\n+import scala.concurrent.{Promise, Future}\n+import scala.util.{Failure, Success, Try}\n+\n+import com.google.common.util.concurrent.ThreadFactoryBuilder\n+import org.apache.avro.ipc.NettyTransceiver\n+import org.apache.avro.ipc.specific.SpecificRequestor\n+import org.apache.flume.Context\n+import org.apache.flume.channel.MemoryChannel\n+import org.apache.flume.event.EventBuilder\n+import org.apache.spark.streaming.TestSuiteBase\n+import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory\n+\n+\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more"
  }, {
    "author": {
      "login": "harishreedharan"
    },
    "body": "Thanks! Done.\n",
    "commit": "e3110b9b551fb0019d6095a16504e8b25d70f962",
    "createdAt": "2014-08-15T06:23:41Z",
    "diffHunk": "@@ -0,0 +1,208 @@\n+package org.apache.spark.streaming.flume.sink\n+\n+import java.net.InetSocketAddress\n+import java.util.concurrent.atomic.AtomicInteger\n+import java.util.concurrent.{CountDownLatch, Executors}\n+\n+import scala.collection.JavaConversions._\n+import scala.concurrent.{Promise, Future}\n+import scala.util.{Failure, Success, Try}\n+\n+import com.google.common.util.concurrent.ThreadFactoryBuilder\n+import org.apache.avro.ipc.NettyTransceiver\n+import org.apache.avro.ipc.specific.SpecificRequestor\n+import org.apache.flume.Context\n+import org.apache.flume.channel.MemoryChannel\n+import org.apache.flume.event.EventBuilder\n+import org.apache.spark.streaming.TestSuiteBase\n+import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory\n+\n+\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more"
  }],
  "prId": 1958
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Incorrect indentation. This line should have 2 space indent.\n",
    "commit": "e3110b9b551fb0019d6095a16504e8b25d70f962",
    "createdAt": "2014-08-18T21:46:36Z",
    "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume.sink\n+\n+import java.net.InetSocketAddress\n+import java.util.concurrent.atomic.AtomicInteger\n+import java.util.concurrent.{CountDownLatch, Executors}\n+\n+import scala.collection.JavaConversions._\n+import scala.concurrent.{Promise, Future}\n+import scala.util.{Failure, Success, Try}\n+\n+import com.google.common.util.concurrent.ThreadFactoryBuilder\n+import org.apache.avro.ipc.NettyTransceiver\n+import org.apache.avro.ipc.specific.SpecificRequestor\n+import org.apache.flume.Context\n+import org.apache.flume.channel.MemoryChannel\n+import org.apache.flume.event.EventBuilder\n+import org.apache.spark.streaming.TestSuiteBase\n+import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory\n+\n+class SparkSinkSuite extends TestSuiteBase {\n+  val eventsPerBatch = 1000\n+  val channelCapacity = 5000\n+\n+  test(\"Success test\") {\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+\n+    putEvents(channel, eventsPerBatch)\n+\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    client.ack(events.getSequenceNumber)\n+    assert(events.getEvents.size() === 1000)\n+    assertChannelIsEmpty(channel)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Nack\") {\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+    putEvents(channel, eventsPerBatch)\n+\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    assert(events.getEvents.size() === 1000)\n+    client.nack(events.getSequenceNumber)\n+    assert(availableChannelSlots(channel) === 4000)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Timeout\") {\n+    val (channel, sink) = initializeChannelAndSink(Option(Map(SparkSinkConfig\n+      .CONF_TRANSACTION_TIMEOUT -> 1.toString)))\n+    channel.start()\n+    sink.start()\n+    putEvents(channel, eventsPerBatch)\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    assert(events.getEvents.size() === 1000)\n+    Thread.sleep(1000)\n+    assert(availableChannelSlots(channel) === 4000)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Multiple consumers\") {\n+    multipleClients(failSome = false)\n+  }\n+\n+  test(\"Multiple consumers With Some Failures\") {\n+    multipleClients(failSome = true)\n+  }\n+\n+  def multipleClients(failSome: Boolean): Unit = {\n+    import scala.concurrent.ExecutionContext.Implicits.global\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+    (1 to 5).map(_ => putEvents(channel, eventsPerBatch))\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val transAndClient = getTransceiverAndClient(address, 5)\n+    val batchCounter = new CountDownLatch(5)\n+    val counter = new AtomicInteger(0)\n+    transAndClient.foreach(x => {\n+      val promise = Promise[EventBatch]()\n+      val future = promise.future\n+      Future {\n+        val client = x._2\n+        var events: EventBatch = null\n+        Try {\n+          events = client.getEventBatch(1000)\n+          if(!failSome || counter.getAndIncrement() % 2 == 0) {\n+            client.ack(events.getSequenceNumber)\n+          } else {\n+            client.nack(events.getSequenceNumber)\n+          }\n+        }.map(_ => promise.success(events)).recover({\n+          case e => promise.failure(e)\n+        })\n+      }\n+      future.onComplete {\n+        case Success(events) => assert(events.getEvents.size() === 1000)\n+          batchCounter.countDown()\n+        case Failure(t) => batchCounter.countDown()\n+          throw t\n+      }\n+    })\n+    batchCounter.await()\n+    if(failSome) {\n+      assert(availableChannelSlots(channel) === 3000)\n+    } else {\n+      assertChannelIsEmpty(channel)\n+    }\n+    sink.stop()\n+    channel.stop()\n+    transAndClient.foreach(x => x._1.close())\n+  }\n+\n+  def initializeChannelAndSink(overrides: Option[Map[String, String]]):\n+  (MemoryChannel, SparkSink) = {\n+    val channel = new MemoryChannel()\n+    val channelContext = new Context()\n+\n+    channelContext.put(\"capacity\", channelCapacity.toString)\n+    channelContext.put(\"transactionCapacity\", 1000.toString)\n+    channelContext.put(\"keep-alive\", 0.toString)\n+    overrides.foreach(channelContext.putAll(_))\n+    channel.configure(channelContext)\n+\n+    val sink = new SparkSink()\n+    val sinkContext = new Context()\n+    sinkContext.put(SparkSinkConfig.CONF_HOSTNAME, \"0.0.0.0\")\n+    sinkContext.put(SparkSinkConfig.CONF_PORT, 0.toString)\n+    sink.configure(sinkContext)\n+    sink.setChannel(channel)\n+    (channel, sink)\n+  }\n+\n+  private def putEvents(ch: MemoryChannel, count: Int): Unit = {\n+    val tx = ch.getTransaction\n+    tx.begin()\n+    (1 to count).map(x => ch.put(EventBuilder.withBody(x.toString.getBytes)))\n+    tx.commit()\n+    tx.close()\n+  }\n+\n+  private def getTransceiverAndClient(address: InetSocketAddress, count: Int):\n+  Seq[(NettyTransceiver, SparkFlumeProtocol.Callback)] = {"
  }],
  "prId": 1958
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Wrong Indentation \n",
    "commit": "e3110b9b551fb0019d6095a16504e8b25d70f962",
    "createdAt": "2014-08-18T21:58:59Z",
    "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume.sink\n+\n+import java.net.InetSocketAddress\n+import java.util.concurrent.atomic.AtomicInteger\n+import java.util.concurrent.{CountDownLatch, Executors}\n+\n+import scala.collection.JavaConversions._\n+import scala.concurrent.{Promise, Future}\n+import scala.util.{Failure, Success, Try}\n+\n+import com.google.common.util.concurrent.ThreadFactoryBuilder\n+import org.apache.avro.ipc.NettyTransceiver\n+import org.apache.avro.ipc.specific.SpecificRequestor\n+import org.apache.flume.Context\n+import org.apache.flume.channel.MemoryChannel\n+import org.apache.flume.event.EventBuilder\n+import org.apache.spark.streaming.TestSuiteBase\n+import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory\n+\n+class SparkSinkSuite extends TestSuiteBase {\n+  val eventsPerBatch = 1000\n+  val channelCapacity = 5000\n+\n+  test(\"Success test\") {\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+\n+    putEvents(channel, eventsPerBatch)\n+\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    client.ack(events.getSequenceNumber)\n+    assert(events.getEvents.size() === 1000)\n+    assertChannelIsEmpty(channel)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Nack\") {\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+    putEvents(channel, eventsPerBatch)\n+\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    assert(events.getEvents.size() === 1000)\n+    client.nack(events.getSequenceNumber)\n+    assert(availableChannelSlots(channel) === 4000)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Timeout\") {\n+    val (channel, sink) = initializeChannelAndSink(Option(Map(SparkSinkConfig\n+      .CONF_TRANSACTION_TIMEOUT -> 1.toString)))\n+    channel.start()\n+    sink.start()\n+    putEvents(channel, eventsPerBatch)\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    assert(events.getEvents.size() === 1000)\n+    Thread.sleep(1000)\n+    assert(availableChannelSlots(channel) === 4000)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Multiple consumers\") {\n+    multipleClients(failSome = false)\n+  }\n+\n+  test(\"Multiple consumers With Some Failures\") {\n+    multipleClients(failSome = true)\n+  }\n+\n+  def multipleClients(failSome: Boolean): Unit = {\n+    import scala.concurrent.ExecutionContext.Implicits.global\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+    (1 to 5).map(_ => putEvents(channel, eventsPerBatch))\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val transAndClient = getTransceiverAndClient(address, 5)\n+    val batchCounter = new CountDownLatch(5)\n+    val counter = new AtomicInteger(0)\n+    transAndClient.foreach(x => {\n+      val promise = Promise[EventBatch]()\n+      val future = promise.future\n+      Future {\n+        val client = x._2\n+        var events: EventBatch = null\n+        Try {\n+          events = client.getEventBatch(1000)\n+          if(!failSome || counter.getAndIncrement() % 2 == 0) {\n+            client.ack(events.getSequenceNumber)\n+          } else {\n+            client.nack(events.getSequenceNumber)\n+          }\n+        }.map(_ => promise.success(events)).recover({\n+          case e => promise.failure(e)\n+        })\n+      }\n+      future.onComplete {\n+        case Success(events) => assert(events.getEvents.size() === 1000)\n+          batchCounter.countDown()\n+        case Failure(t) => batchCounter.countDown()\n+          throw t\n+      }\n+    })\n+    batchCounter.await()\n+    if(failSome) {\n+      assert(availableChannelSlots(channel) === 3000)\n+    } else {\n+      assertChannelIsEmpty(channel)\n+    }\n+    sink.stop()\n+    channel.stop()\n+    transAndClient.foreach(x => x._1.close())\n+  }\n+\n+  def initializeChannelAndSink(overrides: Option[Map[String, String]]):\n+  (MemoryChannel, SparkSink) = {"
  }],
  "prId": 1958
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "unnecessary space.\n",
    "commit": "e3110b9b551fb0019d6095a16504e8b25d70f962",
    "createdAt": "2014-08-18T22:00:28Z",
    "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume.sink\n+\n+import java.net.InetSocketAddress\n+import java.util.concurrent.atomic.AtomicInteger\n+import java.util.concurrent.{CountDownLatch, Executors}\n+\n+import scala.collection.JavaConversions._\n+import scala.concurrent.{Promise, Future}\n+import scala.util.{Failure, Success, Try}\n+\n+import com.google.common.util.concurrent.ThreadFactoryBuilder\n+import org.apache.avro.ipc.NettyTransceiver\n+import org.apache.avro.ipc.specific.SpecificRequestor\n+import org.apache.flume.Context\n+import org.apache.flume.channel.MemoryChannel\n+import org.apache.flume.event.EventBuilder\n+import org.apache.spark.streaming.TestSuiteBase\n+import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory\n+\n+class SparkSinkSuite extends TestSuiteBase {\n+  val eventsPerBatch = 1000\n+  val channelCapacity = 5000\n+\n+  test(\"Success test\") {\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+\n+    putEvents(channel, eventsPerBatch)\n+\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    client.ack(events.getSequenceNumber)\n+    assert(events.getEvents.size() === 1000)\n+    assertChannelIsEmpty(channel)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Nack\") {\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+    putEvents(channel, eventsPerBatch)\n+\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    assert(events.getEvents.size() === 1000)\n+    client.nack(events.getSequenceNumber)\n+    assert(availableChannelSlots(channel) === 4000)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Timeout\") {\n+    val (channel, sink) = initializeChannelAndSink(Option(Map(SparkSinkConfig\n+      .CONF_TRANSACTION_TIMEOUT -> 1.toString)))\n+    channel.start()\n+    sink.start()\n+    putEvents(channel, eventsPerBatch)\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    assert(events.getEvents.size() === 1000)\n+    Thread.sleep(1000)\n+    assert(availableChannelSlots(channel) === 4000)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Multiple consumers\") {\n+    multipleClients(failSome = false)\n+  }\n+\n+  test(\"Multiple consumers With Some Failures\") {\n+    multipleClients(failSome = true)\n+  }\n+\n+  def multipleClients(failSome: Boolean): Unit = {\n+    import scala.concurrent.ExecutionContext.Implicits.global\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+    (1 to 5).map(_ => putEvents(channel, eventsPerBatch))\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val transAndClient = getTransceiverAndClient(address, 5)\n+    val batchCounter = new CountDownLatch(5)\n+    val counter = new AtomicInteger(0)\n+    transAndClient.foreach(x => {\n+      val promise = Promise[EventBatch]()\n+      val future = promise.future\n+      Future {\n+        val client = x._2\n+        var events: EventBatch = null\n+        Try {\n+          events = client.getEventBatch(1000)\n+          if(!failSome || counter.getAndIncrement() % 2 == 0) {\n+            client.ack(events.getSequenceNumber)\n+          } else {\n+            client.nack(events.getSequenceNumber)\n+          }\n+        }.map(_ => promise.success(events)).recover({\n+          case e => promise.failure(e)\n+        })\n+      }\n+      future.onComplete {\n+        case Success(events) => assert(events.getEvents.size() === 1000)\n+          batchCounter.countDown()\n+        case Failure(t) => batchCounter.countDown()\n+          throw t\n+      }\n+    })\n+    batchCounter.await()\n+    if(failSome) {\n+      assert(availableChannelSlots(channel) === 3000)\n+    } else {\n+      assertChannelIsEmpty(channel)\n+    }\n+    sink.stop()\n+    channel.stop()\n+    transAndClient.foreach(x => x._1.close())\n+  }\n+\n+  def initializeChannelAndSink(overrides: Option[Map[String, String]]):\n+  (MemoryChannel, SparkSink) = {\n+    val channel = new MemoryChannel()\n+    val channelContext = new Context()\n+\n+    channelContext.put(\"capacity\", channelCapacity.toString)\n+    channelContext.put(\"transactionCapacity\", 1000.toString)\n+    channelContext.put(\"keep-alive\", 0.toString)\n+    overrides.foreach(channelContext.putAll(_))\n+    channel.configure(channelContext)\n+\n+    val sink = new SparkSink()\n+    val sinkContext = new Context()\n+    sinkContext.put(SparkSinkConfig.CONF_HOSTNAME, \"0.0.0.0\")\n+    sinkContext.put(SparkSinkConfig.CONF_PORT, 0.toString)\n+    sink.configure(sinkContext)\n+    sink.setChannel(channel)\n+    (channel, sink)\n+  }\n+\n+  private def putEvents(ch: MemoryChannel, count: Int): Unit = {\n+    val tx = ch.getTransaction\n+    tx.begin()\n+    (1 to count).map(x => ch.put(EventBuilder.withBody(x.toString.getBytes)))\n+    tx.commit()\n+    tx.close()\n+  }\n+\n+  private def getTransceiverAndClient(address: InetSocketAddress, count: Int):\n+  Seq[(NettyTransceiver, SparkFlumeProtocol.Callback)] = {\n+\n+    (1 to count).map(_ => {\n+      lazy val channelFactoryExecutor =\n+        Executors.newCachedThreadPool(new ThreadFactoryBuilder().setDaemon(true).\n+          setNameFormat(\"Flume Receiver Channel Thread - %d\").build())\n+      lazy val channelFactory =\n+        new NioClientSocketChannelFactory(channelFactoryExecutor, channelFactoryExecutor)\n+      val transceiver = new NettyTransceiver(address, channelFactory)\n+      val client = SpecificRequestor.getClient(classOf[SparkFlumeProtocol.Callback], transceiver)\n+      (transceiver, client)\n+    })\n+  }\n+\n+  private def assertChannelIsEmpty(channel: MemoryChannel) = {\n+    assert(availableChannelSlots(channel) === 5000)\n+  }\n+\n+  private def availableChannelSlots(channel: MemoryChannel): Int = {\n+    val queueRemaining = channel.getClass.getDeclaredField(\"queueRemaining\")\n+    queueRemaining.setAccessible(true)\n+    val m = queueRemaining.get(channel).getClass.getDeclaredMethod(\"availablePermits\")\n+    m.invoke(queueRemaining.get(channel)).asInstanceOf[Int]\n+  }\n+"
  }],
  "prId": 1958
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Maybe keep the naming consistent. If this is called \"Success Test\", then name the next ones as \"Failure Test\" and \"Timeout Test\". You can even remove the \"Test\"\n",
    "commit": "e3110b9b551fb0019d6095a16504e8b25d70f962",
    "createdAt": "2014-08-18T22:04:57Z",
    "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume.sink\n+\n+import java.net.InetSocketAddress\n+import java.util.concurrent.atomic.AtomicInteger\n+import java.util.concurrent.{CountDownLatch, Executors}\n+\n+import scala.collection.JavaConversions._\n+import scala.concurrent.{Promise, Future}\n+import scala.util.{Failure, Success, Try}\n+\n+import com.google.common.util.concurrent.ThreadFactoryBuilder\n+import org.apache.avro.ipc.NettyTransceiver\n+import org.apache.avro.ipc.specific.SpecificRequestor\n+import org.apache.flume.Context\n+import org.apache.flume.channel.MemoryChannel\n+import org.apache.flume.event.EventBuilder\n+import org.apache.spark.streaming.TestSuiteBase\n+import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory\n+\n+class SparkSinkSuite extends TestSuiteBase {\n+  val eventsPerBatch = 1000\n+  val channelCapacity = 5000\n+\n+  test(\"Success test\") {"
  }],
  "prId": 1958
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: This is a little weird. Option is not necessary (add None and Some unnecessarily increases verbosity), as it can simply be `def initializeChannelAndSink(overrides: Map[String, String] = Map.empty)` \n",
    "commit": "e3110b9b551fb0019d6095a16504e8b25d70f962",
    "createdAt": "2014-08-19T07:19:57Z",
    "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume.sink\n+\n+import java.net.InetSocketAddress\n+import java.util.concurrent.atomic.AtomicInteger\n+import java.util.concurrent.{CountDownLatch, Executors}\n+\n+import scala.collection.JavaConversions._\n+import scala.concurrent.{Promise, Future}\n+import scala.util.{Failure, Success, Try}\n+\n+import com.google.common.util.concurrent.ThreadFactoryBuilder\n+import org.apache.avro.ipc.NettyTransceiver\n+import org.apache.avro.ipc.specific.SpecificRequestor\n+import org.apache.flume.Context\n+import org.apache.flume.channel.MemoryChannel\n+import org.apache.flume.event.EventBuilder\n+import org.apache.spark.streaming.TestSuiteBase\n+import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory\n+\n+class SparkSinkSuite extends TestSuiteBase {\n+  val eventsPerBatch = 1000\n+  val channelCapacity = 5000\n+\n+  test(\"Success\") {\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+\n+    putEvents(channel, eventsPerBatch)\n+\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    client.ack(events.getSequenceNumber)\n+    assert(events.getEvents.size() === 1000)\n+    assertChannelIsEmpty(channel)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Nack\") {\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+    putEvents(channel, eventsPerBatch)\n+\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    assert(events.getEvents.size() === 1000)\n+    client.nack(events.getSequenceNumber)\n+    assert(availableChannelSlots(channel) === 4000)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Timeout\") {\n+    val (channel, sink) = initializeChannelAndSink(Option(Map(SparkSinkConfig\n+      .CONF_TRANSACTION_TIMEOUT -> 1.toString)))\n+    channel.start()\n+    sink.start()\n+    putEvents(channel, eventsPerBatch)\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    assert(events.getEvents.size() === 1000)\n+    Thread.sleep(1000)\n+    assert(availableChannelSlots(channel) === 4000)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Multiple consumers\") {\n+    multipleClients(failSome = false)\n+  }\n+\n+  test(\"Multiple consumers With Some Failures\") {\n+    multipleClients(failSome = true)\n+  }\n+\n+  def multipleClients(failSome: Boolean): Unit = {\n+    import scala.concurrent.ExecutionContext.Implicits.global\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+    (1 to 5).map(_ => putEvents(channel, eventsPerBatch))\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val transAndClient = getTransceiverAndClient(address, 5)\n+    val batchCounter = new CountDownLatch(5)\n+    val counter = new AtomicInteger(0)\n+    transAndClient.foreach(x => {\n+      val promise = Promise[EventBatch]()\n+      val future = promise.future\n+      Future {\n+        val client = x._2\n+        var events: EventBatch = null\n+        Try {\n+          events = client.getEventBatch(1000)\n+          if(!failSome || counter.getAndIncrement() % 2 == 0) {\n+            client.ack(events.getSequenceNumber)\n+          } else {\n+            client.nack(events.getSequenceNumber)\n+          }\n+        }.map(_ => promise.success(events)).recover({\n+          case e => promise.failure(e)\n+        })\n+      }\n+      future.onComplete {\n+        case Success(events) => assert(events.getEvents.size() === 1000)\n+          batchCounter.countDown()\n+        case Failure(t) => batchCounter.countDown()\n+          throw t\n+      }\n+    })\n+    batchCounter.await()\n+    if(failSome) {\n+      assert(availableChannelSlots(channel) === 3000)\n+    } else {\n+      assertChannelIsEmpty(channel)\n+    }\n+    sink.stop()\n+    channel.stop()\n+    transAndClient.foreach(x => x._1.close())\n+  }\n+\n+  def initializeChannelAndSink(overrides: Option[Map[String, String]]): (MemoryChannel,"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Also, add `private` to keep it consistent to other methods.\n",
    "commit": "e3110b9b551fb0019d6095a16504e8b25d70f962",
    "createdAt": "2014-08-19T07:57:48Z",
    "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume.sink\n+\n+import java.net.InetSocketAddress\n+import java.util.concurrent.atomic.AtomicInteger\n+import java.util.concurrent.{CountDownLatch, Executors}\n+\n+import scala.collection.JavaConversions._\n+import scala.concurrent.{Promise, Future}\n+import scala.util.{Failure, Success, Try}\n+\n+import com.google.common.util.concurrent.ThreadFactoryBuilder\n+import org.apache.avro.ipc.NettyTransceiver\n+import org.apache.avro.ipc.specific.SpecificRequestor\n+import org.apache.flume.Context\n+import org.apache.flume.channel.MemoryChannel\n+import org.apache.flume.event.EventBuilder\n+import org.apache.spark.streaming.TestSuiteBase\n+import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory\n+\n+class SparkSinkSuite extends TestSuiteBase {\n+  val eventsPerBatch = 1000\n+  val channelCapacity = 5000\n+\n+  test(\"Success\") {\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+\n+    putEvents(channel, eventsPerBatch)\n+\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    client.ack(events.getSequenceNumber)\n+    assert(events.getEvents.size() === 1000)\n+    assertChannelIsEmpty(channel)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Nack\") {\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+    putEvents(channel, eventsPerBatch)\n+\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    assert(events.getEvents.size() === 1000)\n+    client.nack(events.getSequenceNumber)\n+    assert(availableChannelSlots(channel) === 4000)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Timeout\") {\n+    val (channel, sink) = initializeChannelAndSink(Option(Map(SparkSinkConfig\n+      .CONF_TRANSACTION_TIMEOUT -> 1.toString)))\n+    channel.start()\n+    sink.start()\n+    putEvents(channel, eventsPerBatch)\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    assert(events.getEvents.size() === 1000)\n+    Thread.sleep(1000)\n+    assert(availableChannelSlots(channel) === 4000)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Multiple consumers\") {\n+    multipleClients(failSome = false)\n+  }\n+\n+  test(\"Multiple consumers With Some Failures\") {\n+    multipleClients(failSome = true)\n+  }\n+\n+  def multipleClients(failSome: Boolean): Unit = {\n+    import scala.concurrent.ExecutionContext.Implicits.global\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+    (1 to 5).map(_ => putEvents(channel, eventsPerBatch))\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val transAndClient = getTransceiverAndClient(address, 5)\n+    val batchCounter = new CountDownLatch(5)\n+    val counter = new AtomicInteger(0)\n+    transAndClient.foreach(x => {\n+      val promise = Promise[EventBatch]()\n+      val future = promise.future\n+      Future {\n+        val client = x._2\n+        var events: EventBatch = null\n+        Try {\n+          events = client.getEventBatch(1000)\n+          if(!failSome || counter.getAndIncrement() % 2 == 0) {\n+            client.ack(events.getSequenceNumber)\n+          } else {\n+            client.nack(events.getSequenceNumber)\n+          }\n+        }.map(_ => promise.success(events)).recover({\n+          case e => promise.failure(e)\n+        })\n+      }\n+      future.onComplete {\n+        case Success(events) => assert(events.getEvents.size() === 1000)\n+          batchCounter.countDown()\n+        case Failure(t) => batchCounter.countDown()\n+          throw t\n+      }\n+    })\n+    batchCounter.await()\n+    if(failSome) {\n+      assert(availableChannelSlots(channel) === 3000)\n+    } else {\n+      assertChannelIsEmpty(channel)\n+    }\n+    sink.stop()\n+    channel.stop()\n+    transAndClient.foreach(x => x._1.close())\n+  }\n+\n+  def initializeChannelAndSink(overrides: Option[Map[String, String]]): (MemoryChannel,"
  }],
  "prId": 1958
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: this should be named \"testMultipleClients\"\n",
    "commit": "e3110b9b551fb0019d6095a16504e8b25d70f962",
    "createdAt": "2014-08-19T07:39:28Z",
    "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume.sink\n+\n+import java.net.InetSocketAddress\n+import java.util.concurrent.atomic.AtomicInteger\n+import java.util.concurrent.{CountDownLatch, Executors}\n+\n+import scala.collection.JavaConversions._\n+import scala.concurrent.{Promise, Future}\n+import scala.util.{Failure, Success, Try}\n+\n+import com.google.common.util.concurrent.ThreadFactoryBuilder\n+import org.apache.avro.ipc.NettyTransceiver\n+import org.apache.avro.ipc.specific.SpecificRequestor\n+import org.apache.flume.Context\n+import org.apache.flume.channel.MemoryChannel\n+import org.apache.flume.event.EventBuilder\n+import org.apache.spark.streaming.TestSuiteBase\n+import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory\n+\n+class SparkSinkSuite extends TestSuiteBase {\n+  val eventsPerBatch = 1000\n+  val channelCapacity = 5000\n+\n+  test(\"Success\") {\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+\n+    putEvents(channel, eventsPerBatch)\n+\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    client.ack(events.getSequenceNumber)\n+    assert(events.getEvents.size() === 1000)\n+    assertChannelIsEmpty(channel)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Nack\") {\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+    putEvents(channel, eventsPerBatch)\n+\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    assert(events.getEvents.size() === 1000)\n+    client.nack(events.getSequenceNumber)\n+    assert(availableChannelSlots(channel) === 4000)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Timeout\") {\n+    val (channel, sink) = initializeChannelAndSink(Option(Map(SparkSinkConfig\n+      .CONF_TRANSACTION_TIMEOUT -> 1.toString)))\n+    channel.start()\n+    sink.start()\n+    putEvents(channel, eventsPerBatch)\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    assert(events.getEvents.size() === 1000)\n+    Thread.sleep(1000)\n+    assert(availableChannelSlots(channel) === 4000)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Multiple consumers\") {\n+    multipleClients(failSome = false)\n+  }\n+\n+  test(\"Multiple consumers With Some Failures\") {\n+    multipleClients(failSome = true)\n+  }\n+\n+  def multipleClients(failSome: Boolean): Unit = {"
  }],
  "prId": 1958
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: inconsistent capitalization, lets rather have \"Multple consumers with some failures\"\n",
    "commit": "e3110b9b551fb0019d6095a16504e8b25d70f962",
    "createdAt": "2014-08-19T07:40:12Z",
    "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume.sink\n+\n+import java.net.InetSocketAddress\n+import java.util.concurrent.atomic.AtomicInteger\n+import java.util.concurrent.{CountDownLatch, Executors}\n+\n+import scala.collection.JavaConversions._\n+import scala.concurrent.{Promise, Future}\n+import scala.util.{Failure, Success, Try}\n+\n+import com.google.common.util.concurrent.ThreadFactoryBuilder\n+import org.apache.avro.ipc.NettyTransceiver\n+import org.apache.avro.ipc.specific.SpecificRequestor\n+import org.apache.flume.Context\n+import org.apache.flume.channel.MemoryChannel\n+import org.apache.flume.event.EventBuilder\n+import org.apache.spark.streaming.TestSuiteBase\n+import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory\n+\n+class SparkSinkSuite extends TestSuiteBase {\n+  val eventsPerBatch = 1000\n+  val channelCapacity = 5000\n+\n+  test(\"Success\") {\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+\n+    putEvents(channel, eventsPerBatch)\n+\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    client.ack(events.getSequenceNumber)\n+    assert(events.getEvents.size() === 1000)\n+    assertChannelIsEmpty(channel)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Nack\") {\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+    putEvents(channel, eventsPerBatch)\n+\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    assert(events.getEvents.size() === 1000)\n+    client.nack(events.getSequenceNumber)\n+    assert(availableChannelSlots(channel) === 4000)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Timeout\") {\n+    val (channel, sink) = initializeChannelAndSink(Option(Map(SparkSinkConfig\n+      .CONF_TRANSACTION_TIMEOUT -> 1.toString)))\n+    channel.start()\n+    sink.start()\n+    putEvents(channel, eventsPerBatch)\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    assert(events.getEvents.size() === 1000)\n+    Thread.sleep(1000)\n+    assert(availableChannelSlots(channel) === 4000)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Multiple consumers\") {\n+    multipleClients(failSome = false)\n+  }\n+\n+  test(\"Multiple consumers With Some Failures\") {"
  }],
  "prId": 1958
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: transAndClients\n",
    "commit": "e3110b9b551fb0019d6095a16504e8b25d70f962",
    "createdAt": "2014-08-19T07:43:19Z",
    "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume.sink\n+\n+import java.net.InetSocketAddress\n+import java.util.concurrent.atomic.AtomicInteger\n+import java.util.concurrent.{CountDownLatch, Executors}\n+\n+import scala.collection.JavaConversions._\n+import scala.concurrent.{Promise, Future}\n+import scala.util.{Failure, Success, Try}\n+\n+import com.google.common.util.concurrent.ThreadFactoryBuilder\n+import org.apache.avro.ipc.NettyTransceiver\n+import org.apache.avro.ipc.specific.SpecificRequestor\n+import org.apache.flume.Context\n+import org.apache.flume.channel.MemoryChannel\n+import org.apache.flume.event.EventBuilder\n+import org.apache.spark.streaming.TestSuiteBase\n+import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory\n+\n+class SparkSinkSuite extends TestSuiteBase {\n+  val eventsPerBatch = 1000\n+  val channelCapacity = 5000\n+\n+  test(\"Success\") {\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+\n+    putEvents(channel, eventsPerBatch)\n+\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    client.ack(events.getSequenceNumber)\n+    assert(events.getEvents.size() === 1000)\n+    assertChannelIsEmpty(channel)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Nack\") {\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+    putEvents(channel, eventsPerBatch)\n+\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    assert(events.getEvents.size() === 1000)\n+    client.nack(events.getSequenceNumber)\n+    assert(availableChannelSlots(channel) === 4000)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Timeout\") {\n+    val (channel, sink) = initializeChannelAndSink(Option(Map(SparkSinkConfig\n+      .CONF_TRANSACTION_TIMEOUT -> 1.toString)))\n+    channel.start()\n+    sink.start()\n+    putEvents(channel, eventsPerBatch)\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    assert(events.getEvents.size() === 1000)\n+    Thread.sleep(1000)\n+    assert(availableChannelSlots(channel) === 4000)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Multiple consumers\") {\n+    multipleClients(failSome = false)\n+  }\n+\n+  test(\"Multiple consumers With Some Failures\") {\n+    multipleClients(failSome = true)\n+  }\n+\n+  def multipleClients(failSome: Boolean): Unit = {\n+    import scala.concurrent.ExecutionContext.Implicits.global\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+    (1 to 5).map(_ => putEvents(channel, eventsPerBatch))\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val transAndClient = getTransceiverAndClient(address, 5)"
  }],
  "prId": 1958
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "I think it should be `recover { case e => promise.failure(e) }` \nIsnt it easier to have the `promise.success(events)` within the Try, and just do `recover { case e => promise.failure (e) }`\n",
    "commit": "e3110b9b551fb0019d6095a16504e8b25d70f962",
    "createdAt": "2014-08-19T07:54:01Z",
    "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume.sink\n+\n+import java.net.InetSocketAddress\n+import java.util.concurrent.atomic.AtomicInteger\n+import java.util.concurrent.{CountDownLatch, Executors}\n+\n+import scala.collection.JavaConversions._\n+import scala.concurrent.{Promise, Future}\n+import scala.util.{Failure, Success, Try}\n+\n+import com.google.common.util.concurrent.ThreadFactoryBuilder\n+import org.apache.avro.ipc.NettyTransceiver\n+import org.apache.avro.ipc.specific.SpecificRequestor\n+import org.apache.flume.Context\n+import org.apache.flume.channel.MemoryChannel\n+import org.apache.flume.event.EventBuilder\n+import org.apache.spark.streaming.TestSuiteBase\n+import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory\n+\n+class SparkSinkSuite extends TestSuiteBase {\n+  val eventsPerBatch = 1000\n+  val channelCapacity = 5000\n+\n+  test(\"Success\") {\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+\n+    putEvents(channel, eventsPerBatch)\n+\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    client.ack(events.getSequenceNumber)\n+    assert(events.getEvents.size() === 1000)\n+    assertChannelIsEmpty(channel)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Nack\") {\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+    putEvents(channel, eventsPerBatch)\n+\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    assert(events.getEvents.size() === 1000)\n+    client.nack(events.getSequenceNumber)\n+    assert(availableChannelSlots(channel) === 4000)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Timeout\") {\n+    val (channel, sink) = initializeChannelAndSink(Option(Map(SparkSinkConfig\n+      .CONF_TRANSACTION_TIMEOUT -> 1.toString)))\n+    channel.start()\n+    sink.start()\n+    putEvents(channel, eventsPerBatch)\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    assert(events.getEvents.size() === 1000)\n+    Thread.sleep(1000)\n+    assert(availableChannelSlots(channel) === 4000)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Multiple consumers\") {\n+    multipleClients(failSome = false)\n+  }\n+\n+  test(\"Multiple consumers With Some Failures\") {\n+    multipleClients(failSome = true)\n+  }\n+\n+  def multipleClients(failSome: Boolean): Unit = {\n+    import scala.concurrent.ExecutionContext.Implicits.global\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+    (1 to 5).map(_ => putEvents(channel, eventsPerBatch))\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val transAndClient = getTransceiverAndClient(address, 5)\n+    val batchCounter = new CountDownLatch(5)\n+    val counter = new AtomicInteger(0)\n+    transAndClient.foreach(x => {\n+      val promise = Promise[EventBatch]()\n+      val future = promise.future\n+      Future {\n+        val client = x._2\n+        var events: EventBatch = null\n+        Try {\n+          events = client.getEventBatch(1000)\n+          if(!failSome || counter.getAndIncrement() % 2 == 0) {\n+            client.ack(events.getSequenceNumber)\n+          } else {\n+            client.nack(events.getSequenceNumber)\n+          }\n+        }.map(_ => promise.success(events)).recover({"
  }],
  "prId": 1958
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "for multiple lines, its started on the next line.\n\n```\ncase Success(events) => \n    assert (.....\n    batchCounter.countDown()\ncase Failure(t) => \n     batchCounter.countDown()\n     throw t\n```\n",
    "commit": "e3110b9b551fb0019d6095a16504e8b25d70f962",
    "createdAt": "2014-08-19T07:55:29Z",
    "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume.sink\n+\n+import java.net.InetSocketAddress\n+import java.util.concurrent.atomic.AtomicInteger\n+import java.util.concurrent.{CountDownLatch, Executors}\n+\n+import scala.collection.JavaConversions._\n+import scala.concurrent.{Promise, Future}\n+import scala.util.{Failure, Success, Try}\n+\n+import com.google.common.util.concurrent.ThreadFactoryBuilder\n+import org.apache.avro.ipc.NettyTransceiver\n+import org.apache.avro.ipc.specific.SpecificRequestor\n+import org.apache.flume.Context\n+import org.apache.flume.channel.MemoryChannel\n+import org.apache.flume.event.EventBuilder\n+import org.apache.spark.streaming.TestSuiteBase\n+import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory\n+\n+class SparkSinkSuite extends TestSuiteBase {\n+  val eventsPerBatch = 1000\n+  val channelCapacity = 5000\n+\n+  test(\"Success\") {\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+\n+    putEvents(channel, eventsPerBatch)\n+\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    client.ack(events.getSequenceNumber)\n+    assert(events.getEvents.size() === 1000)\n+    assertChannelIsEmpty(channel)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Nack\") {\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+    putEvents(channel, eventsPerBatch)\n+\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    assert(events.getEvents.size() === 1000)\n+    client.nack(events.getSequenceNumber)\n+    assert(availableChannelSlots(channel) === 4000)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Timeout\") {\n+    val (channel, sink) = initializeChannelAndSink(Option(Map(SparkSinkConfig\n+      .CONF_TRANSACTION_TIMEOUT -> 1.toString)))\n+    channel.start()\n+    sink.start()\n+    putEvents(channel, eventsPerBatch)\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    assert(events.getEvents.size() === 1000)\n+    Thread.sleep(1000)\n+    assert(availableChannelSlots(channel) === 4000)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Multiple consumers\") {\n+    multipleClients(failSome = false)\n+  }\n+\n+  test(\"Multiple consumers With Some Failures\") {\n+    multipleClients(failSome = true)\n+  }\n+\n+  def multipleClients(failSome: Boolean): Unit = {\n+    import scala.concurrent.ExecutionContext.Implicits.global\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+    (1 to 5).map(_ => putEvents(channel, eventsPerBatch))\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val transAndClient = getTransceiverAndClient(address, 5)\n+    val batchCounter = new CountDownLatch(5)\n+    val counter = new AtomicInteger(0)\n+    transAndClient.foreach(x => {\n+      val promise = Promise[EventBatch]()\n+      val future = promise.future\n+      Future {\n+        val client = x._2\n+        var events: EventBatch = null\n+        Try {\n+          events = client.getEventBatch(1000)\n+          if(!failSome || counter.getAndIncrement() % 2 == 0) {\n+            client.ack(events.getSequenceNumber)\n+          } else {\n+            client.nack(events.getSequenceNumber)\n+          }\n+        }.map(_ => promise.success(events)).recover({\n+          case e => promise.failure(e)\n+        })\n+      }\n+      future.onComplete {\n+        case Success(events) => assert(events.getEvents.size() === 1000)"
  }],
  "prId": 1958
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "This should be equaled to \"channelCapacity\"\n",
    "commit": "e3110b9b551fb0019d6095a16504e8b25d70f962",
    "createdAt": "2014-08-19T07:58:32Z",
    "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume.sink\n+\n+import java.net.InetSocketAddress\n+import java.util.concurrent.atomic.AtomicInteger\n+import java.util.concurrent.{CountDownLatch, Executors}\n+\n+import scala.collection.JavaConversions._\n+import scala.concurrent.{Promise, Future}\n+import scala.util.{Failure, Success, Try}\n+\n+import com.google.common.util.concurrent.ThreadFactoryBuilder\n+import org.apache.avro.ipc.NettyTransceiver\n+import org.apache.avro.ipc.specific.SpecificRequestor\n+import org.apache.flume.Context\n+import org.apache.flume.channel.MemoryChannel\n+import org.apache.flume.event.EventBuilder\n+import org.apache.spark.streaming.TestSuiteBase\n+import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory\n+\n+class SparkSinkSuite extends TestSuiteBase {\n+  val eventsPerBatch = 1000\n+  val channelCapacity = 5000\n+\n+  test(\"Success\") {\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+\n+    putEvents(channel, eventsPerBatch)\n+\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    client.ack(events.getSequenceNumber)\n+    assert(events.getEvents.size() === 1000)\n+    assertChannelIsEmpty(channel)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Nack\") {\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+    putEvents(channel, eventsPerBatch)\n+\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    assert(events.getEvents.size() === 1000)\n+    client.nack(events.getSequenceNumber)\n+    assert(availableChannelSlots(channel) === 4000)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Timeout\") {\n+    val (channel, sink) = initializeChannelAndSink(Option(Map(SparkSinkConfig\n+      .CONF_TRANSACTION_TIMEOUT -> 1.toString)))\n+    channel.start()\n+    sink.start()\n+    putEvents(channel, eventsPerBatch)\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    assert(events.getEvents.size() === 1000)\n+    Thread.sleep(1000)\n+    assert(availableChannelSlots(channel) === 4000)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Multiple consumers\") {\n+    multipleClients(failSome = false)\n+  }\n+\n+  test(\"Multiple consumers With Some Failures\") {\n+    multipleClients(failSome = true)\n+  }\n+\n+  def multipleClients(failSome: Boolean): Unit = {\n+    import scala.concurrent.ExecutionContext.Implicits.global\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+    (1 to 5).map(_ => putEvents(channel, eventsPerBatch))\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val transAndClient = getTransceiverAndClient(address, 5)\n+    val batchCounter = new CountDownLatch(5)\n+    val counter = new AtomicInteger(0)\n+    transAndClient.foreach(x => {\n+      val promise = Promise[EventBatch]()\n+      val future = promise.future\n+      Future {\n+        val client = x._2\n+        var events: EventBatch = null\n+        Try {\n+          events = client.getEventBatch(1000)\n+          if(!failSome || counter.getAndIncrement() % 2 == 0) {\n+            client.ack(events.getSequenceNumber)\n+          } else {\n+            client.nack(events.getSequenceNumber)\n+          }\n+        }.map(_ => promise.success(events)).recover({\n+          case e => promise.failure(e)\n+        })\n+      }\n+      future.onComplete {\n+        case Success(events) => assert(events.getEvents.size() === 1000)\n+          batchCounter.countDown()\n+        case Failure(t) => batchCounter.countDown()\n+          throw t\n+      }\n+    })\n+    batchCounter.await()\n+    if(failSome) {\n+      assert(availableChannelSlots(channel) === 3000)\n+    } else {\n+      assertChannelIsEmpty(channel)\n+    }\n+    sink.stop()\n+    channel.stop()\n+    transAndClient.foreach(x => x._1.close())\n+  }\n+\n+  def initializeChannelAndSink(overrides: Option[Map[String, String]]): (MemoryChannel,\n+    SparkSink) = {\n+    val channel = new MemoryChannel()\n+    val channelContext = new Context()\n+\n+    channelContext.put(\"capacity\", channelCapacity.toString)\n+    channelContext.put(\"transactionCapacity\", 1000.toString)\n+    channelContext.put(\"keep-alive\", 0.toString)\n+    overrides.foreach(channelContext.putAll(_))\n+    channel.configure(channelContext)\n+\n+    val sink = new SparkSink()\n+    val sinkContext = new Context()\n+    sinkContext.put(SparkSinkConfig.CONF_HOSTNAME, \"0.0.0.0\")\n+    sinkContext.put(SparkSinkConfig.CONF_PORT, 0.toString)\n+    sink.configure(sinkContext)\n+    sink.setChannel(channel)\n+    (channel, sink)\n+  }\n+\n+  private def putEvents(ch: MemoryChannel, count: Int): Unit = {\n+    val tx = ch.getTransaction\n+    tx.begin()\n+    (1 to count).map(x => ch.put(EventBuilder.withBody(x.toString.getBytes)))\n+    tx.commit()\n+    tx.close()\n+  }\n+\n+  private def getTransceiverAndClient(address: InetSocketAddress,\n+    count: Int): Seq[(NettyTransceiver, SparkFlumeProtocol.Callback)] = {\n+\n+    (1 to count).map(_ => {\n+      lazy val channelFactoryExecutor =\n+        Executors.newCachedThreadPool(new ThreadFactoryBuilder().setDaemon(true).\n+          setNameFormat(\"Flume Receiver Channel Thread - %d\").build())\n+      lazy val channelFactory =\n+        new NioClientSocketChannelFactory(channelFactoryExecutor, channelFactoryExecutor)\n+      val transceiver = new NettyTransceiver(address, channelFactory)\n+      val client = SpecificRequestor.getClient(classOf[SparkFlumeProtocol.Callback], transceiver)\n+      (transceiver, client)\n+    })\n+  }\n+\n+  private def assertChannelIsEmpty(channel: MemoryChannel) = {\n+    assert(availableChannelSlots(channel) === 5000)"
  }],
  "prId": 1958
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Can you make this as \"Unit = {\"\n",
    "commit": "e3110b9b551fb0019d6095a16504e8b25d70f962",
    "createdAt": "2014-08-19T08:01:13Z",
    "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume.sink\n+\n+import java.net.InetSocketAddress\n+import java.util.concurrent.atomic.AtomicInteger\n+import java.util.concurrent.{CountDownLatch, Executors}\n+\n+import scala.collection.JavaConversions._\n+import scala.concurrent.{Promise, Future}\n+import scala.util.{Failure, Success, Try}\n+\n+import com.google.common.util.concurrent.ThreadFactoryBuilder\n+import org.apache.avro.ipc.NettyTransceiver\n+import org.apache.avro.ipc.specific.SpecificRequestor\n+import org.apache.flume.Context\n+import org.apache.flume.channel.MemoryChannel\n+import org.apache.flume.event.EventBuilder\n+import org.apache.spark.streaming.TestSuiteBase\n+import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory\n+\n+class SparkSinkSuite extends TestSuiteBase {\n+  val eventsPerBatch = 1000\n+  val channelCapacity = 5000\n+\n+  test(\"Success\") {\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+\n+    putEvents(channel, eventsPerBatch)\n+\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    client.ack(events.getSequenceNumber)\n+    assert(events.getEvents.size() === 1000)\n+    assertChannelIsEmpty(channel)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Nack\") {\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+    putEvents(channel, eventsPerBatch)\n+\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    assert(events.getEvents.size() === 1000)\n+    client.nack(events.getSequenceNumber)\n+    assert(availableChannelSlots(channel) === 4000)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Timeout\") {\n+    val (channel, sink) = initializeChannelAndSink(Option(Map(SparkSinkConfig\n+      .CONF_TRANSACTION_TIMEOUT -> 1.toString)))\n+    channel.start()\n+    sink.start()\n+    putEvents(channel, eventsPerBatch)\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    assert(events.getEvents.size() === 1000)\n+    Thread.sleep(1000)\n+    assert(availableChannelSlots(channel) === 4000)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Multiple consumers\") {\n+    multipleClients(failSome = false)\n+  }\n+\n+  test(\"Multiple consumers With Some Failures\") {\n+    multipleClients(failSome = true)\n+  }\n+\n+  def multipleClients(failSome: Boolean): Unit = {\n+    import scala.concurrent.ExecutionContext.Implicits.global\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+    (1 to 5).map(_ => putEvents(channel, eventsPerBatch))\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val transAndClient = getTransceiverAndClient(address, 5)\n+    val batchCounter = new CountDownLatch(5)\n+    val counter = new AtomicInteger(0)\n+    transAndClient.foreach(x => {\n+      val promise = Promise[EventBatch]()\n+      val future = promise.future\n+      Future {\n+        val client = x._2\n+        var events: EventBatch = null\n+        Try {\n+          events = client.getEventBatch(1000)\n+          if(!failSome || counter.getAndIncrement() % 2 == 0) {\n+            client.ack(events.getSequenceNumber)\n+          } else {\n+            client.nack(events.getSequenceNumber)\n+          }\n+        }.map(_ => promise.success(events)).recover({\n+          case e => promise.failure(e)\n+        })\n+      }\n+      future.onComplete {\n+        case Success(events) => assert(events.getEvents.size() === 1000)\n+          batchCounter.countDown()\n+        case Failure(t) => batchCounter.countDown()\n+          throw t\n+      }\n+    })\n+    batchCounter.await()\n+    if(failSome) {\n+      assert(availableChannelSlots(channel) === 3000)\n+    } else {\n+      assertChannelIsEmpty(channel)\n+    }\n+    sink.stop()\n+    channel.stop()\n+    transAndClient.foreach(x => x._1.close())\n+  }\n+\n+  def initializeChannelAndSink(overrides: Option[Map[String, String]]): (MemoryChannel,\n+    SparkSink) = {\n+    val channel = new MemoryChannel()\n+    val channelContext = new Context()\n+\n+    channelContext.put(\"capacity\", channelCapacity.toString)\n+    channelContext.put(\"transactionCapacity\", 1000.toString)\n+    channelContext.put(\"keep-alive\", 0.toString)\n+    overrides.foreach(channelContext.putAll(_))\n+    channel.configure(channelContext)\n+\n+    val sink = new SparkSink()\n+    val sinkContext = new Context()\n+    sinkContext.put(SparkSinkConfig.CONF_HOSTNAME, \"0.0.0.0\")\n+    sinkContext.put(SparkSinkConfig.CONF_PORT, 0.toString)\n+    sink.configure(sinkContext)\n+    sink.setChannel(channel)\n+    (channel, sink)\n+  }\n+\n+  private def putEvents(ch: MemoryChannel, count: Int): Unit = {\n+    val tx = ch.getTransaction\n+    tx.begin()\n+    (1 to count).map(x => ch.put(EventBuilder.withBody(x.toString.getBytes)))\n+    tx.commit()\n+    tx.close()\n+  }\n+\n+  private def getTransceiverAndClient(address: InetSocketAddress,\n+    count: Int): Seq[(NettyTransceiver, SparkFlumeProtocol.Callback)] = {",
    "line": 180
  }],
  "prId": 1958
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Can you make this as \"Unit = {\"\n",
    "commit": "e3110b9b551fb0019d6095a16504e8b25d70f962",
    "createdAt": "2014-08-19T08:01:18Z",
    "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume.sink\n+\n+import java.net.InetSocketAddress\n+import java.util.concurrent.atomic.AtomicInteger\n+import java.util.concurrent.{CountDownLatch, Executors}\n+\n+import scala.collection.JavaConversions._\n+import scala.concurrent.{Promise, Future}\n+import scala.util.{Failure, Success, Try}\n+\n+import com.google.common.util.concurrent.ThreadFactoryBuilder\n+import org.apache.avro.ipc.NettyTransceiver\n+import org.apache.avro.ipc.specific.SpecificRequestor\n+import org.apache.flume.Context\n+import org.apache.flume.channel.MemoryChannel\n+import org.apache.flume.event.EventBuilder\n+import org.apache.spark.streaming.TestSuiteBase\n+import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory\n+\n+class SparkSinkSuite extends TestSuiteBase {\n+  val eventsPerBatch = 1000\n+  val channelCapacity = 5000\n+\n+  test(\"Success\") {\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+\n+    putEvents(channel, eventsPerBatch)\n+\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    client.ack(events.getSequenceNumber)\n+    assert(events.getEvents.size() === 1000)\n+    assertChannelIsEmpty(channel)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Nack\") {\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+    putEvents(channel, eventsPerBatch)\n+\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    assert(events.getEvents.size() === 1000)\n+    client.nack(events.getSequenceNumber)\n+    assert(availableChannelSlots(channel) === 4000)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Timeout\") {\n+    val (channel, sink) = initializeChannelAndSink(Option(Map(SparkSinkConfig\n+      .CONF_TRANSACTION_TIMEOUT -> 1.toString)))\n+    channel.start()\n+    sink.start()\n+    putEvents(channel, eventsPerBatch)\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    assert(events.getEvents.size() === 1000)\n+    Thread.sleep(1000)\n+    assert(availableChannelSlots(channel) === 4000)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Multiple consumers\") {\n+    multipleClients(failSome = false)\n+  }\n+\n+  test(\"Multiple consumers With Some Failures\") {\n+    multipleClients(failSome = true)\n+  }\n+\n+  def multipleClients(failSome: Boolean): Unit = {\n+    import scala.concurrent.ExecutionContext.Implicits.global\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+    (1 to 5).map(_ => putEvents(channel, eventsPerBatch))\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val transAndClient = getTransceiverAndClient(address, 5)\n+    val batchCounter = new CountDownLatch(5)\n+    val counter = new AtomicInteger(0)\n+    transAndClient.foreach(x => {\n+      val promise = Promise[EventBatch]()\n+      val future = promise.future\n+      Future {\n+        val client = x._2\n+        var events: EventBatch = null\n+        Try {\n+          events = client.getEventBatch(1000)\n+          if(!failSome || counter.getAndIncrement() % 2 == 0) {\n+            client.ack(events.getSequenceNumber)\n+          } else {\n+            client.nack(events.getSequenceNumber)\n+          }\n+        }.map(_ => promise.success(events)).recover({\n+          case e => promise.failure(e)\n+        })\n+      }\n+      future.onComplete {\n+        case Success(events) => assert(events.getEvents.size() === 1000)\n+          batchCounter.countDown()\n+        case Failure(t) => batchCounter.countDown()\n+          throw t\n+      }\n+    })\n+    batchCounter.await()\n+    if(failSome) {\n+      assert(availableChannelSlots(channel) === 3000)\n+    } else {\n+      assertChannelIsEmpty(channel)\n+    }\n+    sink.stop()\n+    channel.stop()\n+    transAndClient.foreach(x => x._1.close())\n+  }\n+\n+  def initializeChannelAndSink(overrides: Option[Map[String, String]]): (MemoryChannel,\n+    SparkSink) = {\n+    val channel = new MemoryChannel()\n+    val channelContext = new Context()\n+\n+    channelContext.put(\"capacity\", channelCapacity.toString)\n+    channelContext.put(\"transactionCapacity\", 1000.toString)\n+    channelContext.put(\"keep-alive\", 0.toString)\n+    overrides.foreach(channelContext.putAll(_))\n+    channel.configure(channelContext)\n+\n+    val sink = new SparkSink()\n+    val sinkContext = new Context()\n+    sinkContext.put(SparkSinkConfig.CONF_HOSTNAME, \"0.0.0.0\")\n+    sinkContext.put(SparkSinkConfig.CONF_PORT, 0.toString)\n+    sink.configure(sinkContext)\n+    sink.setChannel(channel)\n+    (channel, sink)\n+  }\n+\n+  private def putEvents(ch: MemoryChannel, count: Int): Unit = {\n+    val tx = ch.getTransaction\n+    tx.begin()\n+    (1 to count).map(x => ch.put(EventBuilder.withBody(x.toString.getBytes)))\n+    tx.commit()\n+    tx.close()\n+  }\n+\n+  private def getTransceiverAndClient(address: InetSocketAddress,\n+    count: Int): Seq[(NettyTransceiver, SparkFlumeProtocol.Callback)] = {\n+\n+    (1 to count).map(_ => {\n+      lazy val channelFactoryExecutor =\n+        Executors.newCachedThreadPool(new ThreadFactoryBuilder().setDaemon(true).\n+          setNameFormat(\"Flume Receiver Channel Thread - %d\").build())\n+      lazy val channelFactory =\n+        new NioClientSocketChannelFactory(channelFactoryExecutor, channelFactoryExecutor)\n+      val transceiver = new NettyTransceiver(address, channelFactory)\n+      val client = SpecificRequestor.getClient(classOf[SparkFlumeProtocol.Callback], transceiver)\n+      (transceiver, client)\n+    })\n+  }\n+\n+  private def assertChannelIsEmpty(channel: MemoryChannel) = {"
  }],
  "prId": 1958
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Why make one Future, than another Promise + Future combination? Just the single Future is sufficient. \n\n```\nFuture {\n   val events = client.getEventBatch(1000)\n   if(!failSome || counter.getAndIncrement() % 2 == 0) {\n       client.ack(events.getSequenceNumber)\n   } else {\n       client.nack(events.getSequenceNumber)\n       throw new Exception(\"intentional exception\")\n   }\n   events\n}.onComplete { case _ => \n  batchCounter.countDown()\n}.onSuccess { case events => \n  assert(events.getEvents.size() === 1000)  \n}\n```\n\nAlso, I question the use of ExecutionContext, we dont know the # of threads in that context, so not sure what parallelism we achieve here. Its best to create an ExecutionContext from a Executors.newFixedThreadPool().\n",
    "commit": "e3110b9b551fb0019d6095a16504e8b25d70f962",
    "createdAt": "2014-08-19T08:09:10Z",
    "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume.sink\n+\n+import java.net.InetSocketAddress\n+import java.util.concurrent.atomic.AtomicInteger\n+import java.util.concurrent.{CountDownLatch, Executors}\n+\n+import scala.collection.JavaConversions._\n+import scala.concurrent.{Promise, Future}\n+import scala.util.{Failure, Success, Try}\n+\n+import com.google.common.util.concurrent.ThreadFactoryBuilder\n+import org.apache.avro.ipc.NettyTransceiver\n+import org.apache.avro.ipc.specific.SpecificRequestor\n+import org.apache.flume.Context\n+import org.apache.flume.channel.MemoryChannel\n+import org.apache.flume.event.EventBuilder\n+import org.apache.spark.streaming.TestSuiteBase\n+import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory\n+\n+class SparkSinkSuite extends TestSuiteBase {\n+  val eventsPerBatch = 1000\n+  val channelCapacity = 5000\n+\n+  test(\"Success\") {\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+\n+    putEvents(channel, eventsPerBatch)\n+\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    client.ack(events.getSequenceNumber)\n+    assert(events.getEvents.size() === 1000)\n+    assertChannelIsEmpty(channel)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Nack\") {\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+    putEvents(channel, eventsPerBatch)\n+\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    assert(events.getEvents.size() === 1000)\n+    client.nack(events.getSequenceNumber)\n+    assert(availableChannelSlots(channel) === 4000)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Timeout\") {\n+    val (channel, sink) = initializeChannelAndSink(Option(Map(SparkSinkConfig\n+      .CONF_TRANSACTION_TIMEOUT -> 1.toString)))\n+    channel.start()\n+    sink.start()\n+    putEvents(channel, eventsPerBatch)\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    assert(events.getEvents.size() === 1000)\n+    Thread.sleep(1000)\n+    assert(availableChannelSlots(channel) === 4000)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Multiple consumers\") {\n+    multipleClients(failSome = false)\n+  }\n+\n+  test(\"Multiple consumers With Some Failures\") {\n+    multipleClients(failSome = true)\n+  }\n+\n+  def multipleClients(failSome: Boolean): Unit = {\n+    import scala.concurrent.ExecutionContext.Implicits.global\n+    val (channel, sink) = initializeChannelAndSink(None)\n+    channel.start()\n+    sink.start()\n+    (1 to 5).map(_ => putEvents(channel, eventsPerBatch))\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val transAndClient = getTransceiverAndClient(address, 5)\n+    val batchCounter = new CountDownLatch(5)\n+    val counter = new AtomicInteger(0)\n+    transAndClient.foreach(x => {\n+      val promise = Promise[EventBatch]()\n+      val future = promise.future\n+      Future {"
  }],
  "prId": 1958
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "super nit, no change necessary. A simpler implementation is possible. No latch necessary.\n\n```\nval futures = transceiversAndClients.map( x => {\n  Future {\n      ....\n  }\n)\nAwait.result(Future.sequence(futures), timeout)    // Future.sequence converst seq of Future to a single Future of seq.\n```\n",
    "commit": "e3110b9b551fb0019d6095a16504e8b25d70f962",
    "createdAt": "2014-08-19T22:26:57Z",
    "diffHunk": "@@ -0,0 +1,204 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume.sink\n+\n+import java.net.InetSocketAddress\n+import java.util.concurrent.atomic.AtomicInteger\n+import java.util.concurrent.{TimeUnit, CountDownLatch, Executors}\n+\n+import scala.collection.JavaConversions._\n+import scala.concurrent.{ExecutionContext, Future}\n+import scala.util.{Failure, Success}\n+\n+import com.google.common.util.concurrent.ThreadFactoryBuilder\n+import org.apache.avro.ipc.NettyTransceiver\n+import org.apache.avro.ipc.specific.SpecificRequestor\n+import org.apache.flume.Context\n+import org.apache.flume.channel.MemoryChannel\n+import org.apache.flume.event.EventBuilder\n+import org.apache.spark.streaming.TestSuiteBase\n+import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory\n+\n+class SparkSinkSuite extends TestSuiteBase {\n+  val eventsPerBatch = 1000\n+  val channelCapacity = 5000\n+\n+  test(\"Success\") {\n+    val (channel, sink) = initializeChannelAndSink()\n+    channel.start()\n+    sink.start()\n+\n+    putEvents(channel, eventsPerBatch)\n+\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    client.ack(events.getSequenceNumber)\n+    assert(events.getEvents.size() === 1000)\n+    assertChannelIsEmpty(channel)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Nack\") {\n+    val (channel, sink) = initializeChannelAndSink()\n+    channel.start()\n+    sink.start()\n+    putEvents(channel, eventsPerBatch)\n+\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    assert(events.getEvents.size() === 1000)\n+    client.nack(events.getSequenceNumber)\n+    assert(availableChannelSlots(channel) === 4000)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Timeout\") {\n+    val (channel, sink) = initializeChannelAndSink(Map(SparkSinkConfig\n+      .CONF_TRANSACTION_TIMEOUT -> 1.toString))\n+    channel.start()\n+    sink.start()\n+    putEvents(channel, eventsPerBatch)\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+\n+    val (transceiver, client) = getTransceiverAndClient(address, 1)(0)\n+    val events = client.getEventBatch(1000)\n+    assert(events.getEvents.size() === 1000)\n+    Thread.sleep(1000)\n+    assert(availableChannelSlots(channel) === 4000)\n+    sink.stop()\n+    channel.stop()\n+    transceiver.close()\n+  }\n+\n+  test(\"Multiple consumers\") {\n+    testMultipleConsumers(failSome = false)\n+  }\n+\n+  test(\"Multiple consumers with some failures\") {\n+    testMultipleConsumers(failSome = true)\n+  }\n+\n+  def testMultipleConsumers(failSome: Boolean): Unit = {\n+    implicit val executorContext = ExecutionContext\n+      .fromExecutorService(Executors.newFixedThreadPool(5))\n+    val (channel, sink) = initializeChannelAndSink()\n+    channel.start()\n+    sink.start()\n+    (1 to 5).foreach(_ => putEvents(channel, eventsPerBatch))\n+    val port = sink.getPort\n+    val address = new InetSocketAddress(\"0.0.0.0\", port)\n+    val transceiversAndClients = getTransceiverAndClient(address, 5)\n+    val batchCounter = new CountDownLatch(5)\n+    val counter = new AtomicInteger(0)\n+    transceiversAndClients.foreach(x => {\n+      Future {",
    "line": 119
  }],
  "prId": 1958
}]