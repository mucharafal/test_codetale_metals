[{
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "You need to use `JavaStreamingContext` so that you can call it in Python. That's why this tests https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/37098/console failed.\n",
    "commit": "03f3e884d433eaa83c8573720d0b3262fadf36ae",
    "createdAt": "2015-07-15T09:23:40Z",
    "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.mqtt\n+\n+import java.net.{ServerSocket, URI}\n+import java.util.concurrent.{TimeUnit, CountDownLatch}\n+\n+import scala.language.postfixOps\n+\n+import org.apache.activemq.broker.{BrokerService, TransportConnector}\n+import org.apache.commons.lang3.RandomUtils\n+import org.eclipse.paho.client.mqttv3._\n+import org.eclipse.paho.client.mqttv3.persist.MqttDefaultFilePersistence\n+\n+import org.apache.spark.streaming.{StreamingContext, Milliseconds}\n+import org.apache.spark.streaming.scheduler.StreamingListener\n+import org.apache.spark.streaming.scheduler.StreamingListenerReceiverStarted\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SparkConf}\n+\n+/**\n+ * Share codes for Scala and Python unit tests\n+ */\n+private class MQTTTestUtils extends Logging {\n+\n+  private val persistenceDir = Utils.createTempDir()\n+  private val brokerHost = \"localhost\"\n+  private var brokerPort = findFreePort()\n+\n+  private var broker: BrokerService = _\n+  private var connector: TransportConnector = _\n+\n+  def brokerUri: String = {\n+    s\"$brokerHost:$brokerPort\"\n+  }\n+\n+  def setup(): Unit = {\n+    broker = new BrokerService()\n+    broker.setDataDirectoryFile(Utils.createTempDir())\n+    connector = new TransportConnector()\n+    connector.setName(\"mqtt\")\n+    connector.setUri(new URI(\"mqtt://\" + brokerUri))\n+    broker.addConnector(connector)\n+    broker.start()\n+  }\n+\n+  def teardown(): Unit = {\n+    if (broker != null) {\n+      broker.stop()\n+      broker = null\n+    }\n+    if (connector != null) {\n+      connector.stop()\n+      connector = null\n+    }\n+  }\n+\n+  private def findFreePort(): Int = {\n+    val candidatePort = RandomUtils.nextInt(1024, 65536)\n+    Utils.startServiceOnPort(candidatePort, (trialPort: Int) => {\n+      val socket = new ServerSocket(trialPort)\n+      socket.close()\n+      (null, trialPort)\n+    }, new SparkConf())._2\n+  }\n+\n+  def publishData(topic: String, data: String): Unit = {\n+    var client: MqttClient = null\n+    try {\n+      val persistence = new MqttDefaultFilePersistence(persistenceDir.getAbsolutePath)\n+      client = new MqttClient(\"tcp://\" + brokerUri, MqttClient.generateClientId(), persistence)\n+      client.connect()\n+      if (client.isConnected) {\n+        val msgTopic = client.getTopic(topic)\n+        val message = new MqttMessage(data.getBytes(\"utf-8\"))\n+        message.setQos(1)\n+        message.setRetained(true)\n+\n+        for (i <- 0 to 10) {\n+          try {\n+            msgTopic.publish(message)\n+          } catch {\n+            case e: MqttException if e.getReasonCode == MqttException.REASON_CODE_MAX_INFLIGHT =>\n+              // wait for Spark streaming to consume something from the message queue\n+              Thread.sleep(50)\n+          }\n+        }\n+      }\n+    } finally {\n+      client.disconnect()\n+      client.close()\n+      client = null\n+    }\n+  }\n+\n+  /**\n+   * Block until at least one receiver has started or timeout occurs.\n+   */\n+  def waitForReceiverToStart(ssc: StreamingContext) : Unit = {"
  }, {
    "author": {
      "login": "prabeesh"
    },
    "body": "In python we can use wait_for instead of waitForReceiverToStart()\n",
    "commit": "03f3e884d433eaa83c8573720d0b3262fadf36ae",
    "createdAt": "2015-07-20T14:09:03Z",
    "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.mqtt\n+\n+import java.net.{ServerSocket, URI}\n+import java.util.concurrent.{TimeUnit, CountDownLatch}\n+\n+import scala.language.postfixOps\n+\n+import org.apache.activemq.broker.{BrokerService, TransportConnector}\n+import org.apache.commons.lang3.RandomUtils\n+import org.eclipse.paho.client.mqttv3._\n+import org.eclipse.paho.client.mqttv3.persist.MqttDefaultFilePersistence\n+\n+import org.apache.spark.streaming.{StreamingContext, Milliseconds}\n+import org.apache.spark.streaming.scheduler.StreamingListener\n+import org.apache.spark.streaming.scheduler.StreamingListenerReceiverStarted\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SparkConf}\n+\n+/**\n+ * Share codes for Scala and Python unit tests\n+ */\n+private class MQTTTestUtils extends Logging {\n+\n+  private val persistenceDir = Utils.createTempDir()\n+  private val brokerHost = \"localhost\"\n+  private var brokerPort = findFreePort()\n+\n+  private var broker: BrokerService = _\n+  private var connector: TransportConnector = _\n+\n+  def brokerUri: String = {\n+    s\"$brokerHost:$brokerPort\"\n+  }\n+\n+  def setup(): Unit = {\n+    broker = new BrokerService()\n+    broker.setDataDirectoryFile(Utils.createTempDir())\n+    connector = new TransportConnector()\n+    connector.setName(\"mqtt\")\n+    connector.setUri(new URI(\"mqtt://\" + brokerUri))\n+    broker.addConnector(connector)\n+    broker.start()\n+  }\n+\n+  def teardown(): Unit = {\n+    if (broker != null) {\n+      broker.stop()\n+      broker = null\n+    }\n+    if (connector != null) {\n+      connector.stop()\n+      connector = null\n+    }\n+  }\n+\n+  private def findFreePort(): Int = {\n+    val candidatePort = RandomUtils.nextInt(1024, 65536)\n+    Utils.startServiceOnPort(candidatePort, (trialPort: Int) => {\n+      val socket = new ServerSocket(trialPort)\n+      socket.close()\n+      (null, trialPort)\n+    }, new SparkConf())._2\n+  }\n+\n+  def publishData(topic: String, data: String): Unit = {\n+    var client: MqttClient = null\n+    try {\n+      val persistence = new MqttDefaultFilePersistence(persistenceDir.getAbsolutePath)\n+      client = new MqttClient(\"tcp://\" + brokerUri, MqttClient.generateClientId(), persistence)\n+      client.connect()\n+      if (client.isConnected) {\n+        val msgTopic = client.getTopic(topic)\n+        val message = new MqttMessage(data.getBytes(\"utf-8\"))\n+        message.setQos(1)\n+        message.setRetained(true)\n+\n+        for (i <- 0 to 10) {\n+          try {\n+            msgTopic.publish(message)\n+          } catch {\n+            case e: MqttException if e.getReasonCode == MqttException.REASON_CODE_MAX_INFLIGHT =>\n+              // wait for Spark streaming to consume something from the message queue\n+              Thread.sleep(50)\n+          }\n+        }\n+      }\n+    } finally {\n+      client.disconnect()\n+      client.close()\n+      client = null\n+    }\n+  }\n+\n+  /**\n+   * Block until at least one receiver has started or timeout occurs.\n+   */\n+  def waitForReceiverToStart(ssc: StreamingContext) : Unit = {"
  }],
  "prId": 4229
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "This can be `val`. \n",
    "commit": "03f3e884d433eaa83c8573720d0b3262fadf36ae",
    "createdAt": "2015-07-15T10:03:33Z",
    "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.mqtt\n+\n+import java.net.{ServerSocket, URI}\n+import java.util.concurrent.{TimeUnit, CountDownLatch}\n+\n+import scala.language.postfixOps\n+\n+import org.apache.activemq.broker.{BrokerService, TransportConnector}\n+import org.apache.commons.lang3.RandomUtils\n+import org.eclipse.paho.client.mqttv3._\n+import org.eclipse.paho.client.mqttv3.persist.MqttDefaultFilePersistence\n+\n+import org.apache.spark.streaming.{StreamingContext, Milliseconds}\n+import org.apache.spark.streaming.scheduler.StreamingListener\n+import org.apache.spark.streaming.scheduler.StreamingListenerReceiverStarted\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SparkConf}\n+\n+/**\n+ * Share codes for Scala and Python unit tests\n+ */\n+private class MQTTTestUtils extends Logging {\n+\n+  private val persistenceDir = Utils.createTempDir()\n+  private val brokerHost = \"localhost\"\n+  private var brokerPort = findFreePort()"
  }],
  "prId": 4229
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "It's possible that `StreamingListenerReceiverStarted` has been sent before we add the StreamingListener. Could you add the listener before `ssc.start()`? I think you need to spilt this method to two methods.\n",
    "commit": "03f3e884d433eaa83c8573720d0b3262fadf36ae",
    "createdAt": "2015-07-15T10:08:09Z",
    "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.mqtt\n+\n+import java.net.{ServerSocket, URI}\n+import java.util.concurrent.{TimeUnit, CountDownLatch}\n+\n+import scala.language.postfixOps\n+\n+import org.apache.activemq.broker.{BrokerService, TransportConnector}\n+import org.apache.commons.lang3.RandomUtils\n+import org.eclipse.paho.client.mqttv3._\n+import org.eclipse.paho.client.mqttv3.persist.MqttDefaultFilePersistence\n+\n+import org.apache.spark.streaming.{StreamingContext, Milliseconds}\n+import org.apache.spark.streaming.scheduler.StreamingListener\n+import org.apache.spark.streaming.scheduler.StreamingListenerReceiverStarted\n+import org.apache.spark.util.Utils\n+import org.apache.spark.{Logging, SparkConf}\n+\n+/**\n+ * Share codes for Scala and Python unit tests\n+ */\n+private class MQTTTestUtils extends Logging {\n+\n+  private val persistenceDir = Utils.createTempDir()\n+  private val brokerHost = \"localhost\"\n+  private var brokerPort = findFreePort()\n+\n+  private var broker: BrokerService = _\n+  private var connector: TransportConnector = _\n+\n+  def brokerUri: String = {\n+    s\"$brokerHost:$brokerPort\"\n+  }\n+\n+  def setup(): Unit = {\n+    broker = new BrokerService()\n+    broker.setDataDirectoryFile(Utils.createTempDir())\n+    connector = new TransportConnector()\n+    connector.setName(\"mqtt\")\n+    connector.setUri(new URI(\"mqtt://\" + brokerUri))\n+    broker.addConnector(connector)\n+    broker.start()\n+  }\n+\n+  def teardown(): Unit = {\n+    if (broker != null) {\n+      broker.stop()\n+      broker = null\n+    }\n+    if (connector != null) {\n+      connector.stop()\n+      connector = null\n+    }\n+  }\n+\n+  private def findFreePort(): Int = {\n+    val candidatePort = RandomUtils.nextInt(1024, 65536)\n+    Utils.startServiceOnPort(candidatePort, (trialPort: Int) => {\n+      val socket = new ServerSocket(trialPort)\n+      socket.close()\n+      (null, trialPort)\n+    }, new SparkConf())._2\n+  }\n+\n+  def publishData(topic: String, data: String): Unit = {\n+    var client: MqttClient = null\n+    try {\n+      val persistence = new MqttDefaultFilePersistence(persistenceDir.getAbsolutePath)\n+      client = new MqttClient(\"tcp://\" + brokerUri, MqttClient.generateClientId(), persistence)\n+      client.connect()\n+      if (client.isConnected) {\n+        val msgTopic = client.getTopic(topic)\n+        val message = new MqttMessage(data.getBytes(\"utf-8\"))\n+        message.setQos(1)\n+        message.setRetained(true)\n+\n+        for (i <- 0 to 10) {\n+          try {\n+            msgTopic.publish(message)\n+          } catch {\n+            case e: MqttException if e.getReasonCode == MqttException.REASON_CODE_MAX_INFLIGHT =>\n+              // wait for Spark streaming to consume something from the message queue\n+              Thread.sleep(50)\n+          }\n+        }\n+      }\n+    } finally {\n+      client.disconnect()\n+      client.close()\n+      client = null\n+    }\n+  }\n+\n+  /**\n+   * Block until at least one receiver has started or timeout occurs.\n+   */\n+  def waitForReceiverToStart(ssc: StreamingContext) : Unit = {\n+    val latch = new CountDownLatch(1)\n+    ssc.addStreamingListener(new StreamingListener {"
  }],
  "prId": 4229
}]