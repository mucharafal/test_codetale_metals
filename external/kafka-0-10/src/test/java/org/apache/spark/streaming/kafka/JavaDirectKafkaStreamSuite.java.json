[{
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "Sometimes, `nextInt` may return the same value as the previous one. This will make this test flaky. Could you avoid that?\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-28T22:29:03Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka;\n+\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.function.Function;\n+import org.apache.spark.api.java.function.VoidFunction;\n+import org.apache.spark.streaming.Durations;\n+import org.apache.spark.streaming.api.java.JavaDStream;\n+import org.apache.spark.streaming.api.java.JavaInputDStream;\n+import org.apache.spark.streaming.api.java.JavaStreamingContext;\n+\n+public class JavaDirectKafkaStreamSuite implements Serializable {\n+  private transient JavaStreamingContext ssc = null;\n+  private transient KafkaTestUtils kafkaTestUtils = null;\n+\n+  @Before\n+  public void setUp() {\n+    kafkaTestUtils = new KafkaTestUtils();\n+    kafkaTestUtils.setup();\n+    SparkConf sparkConf = new SparkConf()\n+      .setMaster(\"local[4]\").setAppName(this.getClass().getSimpleName());\n+    ssc = new JavaStreamingContext(sparkConf, Durations.milliseconds(200));\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    if (ssc != null) {\n+      ssc.stop();\n+      ssc = null;\n+    }\n+\n+    if (kafkaTestUtils != null) {\n+      kafkaTestUtils.teardown();\n+      kafkaTestUtils = null;\n+    }\n+  }\n+\n+  @Test\n+  public void testKafkaStream() throws InterruptedException {\n+    final String topic1 = \"topic1\";\n+    final String topic2 = \"topic2\";\n+    // hold a reference to the current offset ranges, so it can be used downstream\n+    final AtomicReference<OffsetRange[]> offsetRanges = new AtomicReference<>();\n+\n+    String[] topic1data = createTopicAndSendData(topic1);\n+    String[] topic2data = createTopicAndSendData(topic2);\n+\n+    Set<String> sent = new HashSet<>();\n+    sent.addAll(Arrays.asList(topic1data));\n+    sent.addAll(Arrays.asList(topic2data));\n+\n+    Random random = new Random();\n+\n+    final Map<String, Object> kafkaParams = new HashMap<>();\n+    kafkaParams.put(\"bootstrap.servers\", kafkaTestUtils.brokerAddress());\n+    kafkaParams.put(\"key.deserializer\", StringDeserializer.class);\n+    kafkaParams.put(\"value.deserializer\", StringDeserializer.class);\n+    kafkaParams.put(\"auto.offset.reset\", \"earliest\");\n+    kafkaParams.put(\"group.id\", \"java-test-consumer-\" + random.nextInt());\n+\n+    JavaInputDStream<ConsumerRecord<String, String>> istream1 = KafkaUtils.createDirectStream(\n+        ssc,\n+        PreferConsistent.create(),\n+        Subscribe.<String, String>create(Arrays.asList(topic1), kafkaParams)\n+    );\n+\n+    JavaDStream<String> stream1 = istream1.transform(\n+      // Make sure you can get offset ranges from the rdd\n+      new Function<JavaRDD<ConsumerRecord<String, String>>,\n+        JavaRDD<ConsumerRecord<String, String>>>() {\n+          @Override\n+          public JavaRDD<ConsumerRecord<String, String>> call(\n+            JavaRDD<ConsumerRecord<String, String>> rdd\n+          ) {\n+            OffsetRange[] offsets = ((HasOffsetRanges) rdd.rdd()).offsetRanges();\n+            offsetRanges.set(offsets);\n+            Assert.assertEquals(topic1, offsets[0].topic());\n+            return rdd;\n+          }\n+        }\n+    ).map(\n+        new Function<ConsumerRecord<String, String>, String>() {\n+          @Override\n+          public String call(ConsumerRecord<String, String> r) {\n+            return r.value();\n+          }\n+        }\n+    );\n+\n+    final Map<String, Object> kafkaParams2 = new HashMap<>(kafkaParams);\n+    kafkaParams2.put(\"group.id\", \"java-test-consumer-\" + random.nextInt());"
  }],
  "prId": 11863
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "nit: remove this `println`.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-28T22:31:27Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka;\n+\n+import java.io.Serializable;\n+import java.util.*;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.function.Function;\n+import org.apache.spark.api.java.function.VoidFunction;\n+import org.apache.spark.streaming.Durations;\n+import org.apache.spark.streaming.api.java.JavaDStream;\n+import org.apache.spark.streaming.api.java.JavaInputDStream;\n+import org.apache.spark.streaming.api.java.JavaStreamingContext;\n+\n+public class JavaDirectKafkaStreamSuite implements Serializable {\n+  private transient JavaStreamingContext ssc = null;\n+  private transient KafkaTestUtils kafkaTestUtils = null;\n+\n+  @Before\n+  public void setUp() {\n+    kafkaTestUtils = new KafkaTestUtils();\n+    kafkaTestUtils.setup();\n+    SparkConf sparkConf = new SparkConf()\n+      .setMaster(\"local[4]\").setAppName(this.getClass().getSimpleName());\n+    ssc = new JavaStreamingContext(sparkConf, Durations.milliseconds(200));\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    if (ssc != null) {\n+      ssc.stop();\n+      ssc = null;\n+    }\n+\n+    if (kafkaTestUtils != null) {\n+      kafkaTestUtils.teardown();\n+      kafkaTestUtils = null;\n+    }\n+  }\n+\n+  @Test\n+  public void testKafkaStream() throws InterruptedException {\n+    final String topic1 = \"topic1\";\n+    final String topic2 = \"topic2\";\n+    // hold a reference to the current offset ranges, so it can be used downstream\n+    final AtomicReference<OffsetRange[]> offsetRanges = new AtomicReference<>();\n+\n+    String[] topic1data = createTopicAndSendData(topic1);\n+    String[] topic2data = createTopicAndSendData(topic2);\n+\n+    Set<String> sent = new HashSet<>();\n+    sent.addAll(Arrays.asList(topic1data));\n+    sent.addAll(Arrays.asList(topic2data));\n+\n+    Random random = new Random();\n+\n+    final Map<String, Object> kafkaParams = new HashMap<>();\n+    kafkaParams.put(\"bootstrap.servers\", kafkaTestUtils.brokerAddress());\n+    kafkaParams.put(\"key.deserializer\", StringDeserializer.class);\n+    kafkaParams.put(\"value.deserializer\", StringDeserializer.class);\n+    kafkaParams.put(\"auto.offset.reset\", \"earliest\");\n+    kafkaParams.put(\"group.id\", \"java-test-consumer-\" + random.nextInt());\n+\n+    JavaInputDStream<ConsumerRecord<String, String>> istream1 = KafkaUtils.createDirectStream(\n+        ssc,\n+        PreferConsistent.create(),\n+        Subscribe.<String, String>create(Arrays.asList(topic1), kafkaParams)\n+    );\n+\n+    JavaDStream<String> stream1 = istream1.transform(\n+      // Make sure you can get offset ranges from the rdd\n+      new Function<JavaRDD<ConsumerRecord<String, String>>,\n+        JavaRDD<ConsumerRecord<String, String>>>() {\n+          @Override\n+          public JavaRDD<ConsumerRecord<String, String>> call(\n+            JavaRDD<ConsumerRecord<String, String>> rdd\n+          ) {\n+            OffsetRange[] offsets = ((HasOffsetRanges) rdd.rdd()).offsetRanges();\n+            offsetRanges.set(offsets);\n+            Assert.assertEquals(topic1, offsets[0].topic());\n+            return rdd;\n+          }\n+        }\n+    ).map(\n+        new Function<ConsumerRecord<String, String>, String>() {\n+          @Override\n+          public String call(ConsumerRecord<String, String> r) {\n+            return r.value();\n+          }\n+        }\n+    );\n+\n+    final Map<String, Object> kafkaParams2 = new HashMap<>(kafkaParams);\n+    kafkaParams2.put(\"group.id\", \"java-test-consumer-\" + random.nextInt());\n+\n+    JavaInputDStream<ConsumerRecord<String, String>> istream2 = KafkaUtils.createDirectStream(\n+        ssc,\n+        PreferConsistent.create(),\n+        Subscribe.<String, String>create(Arrays.asList(topic2), kafkaParams2)\n+    );\n+\n+    JavaDStream<String> stream2 = istream2.transform(\n+      // Make sure you can get offset ranges from the rdd\n+      new Function<JavaRDD<ConsumerRecord<String, String>>,\n+        JavaRDD<ConsumerRecord<String, String>>>() {\n+          @Override\n+          public JavaRDD<ConsumerRecord<String, String>> call(\n+            JavaRDD<ConsumerRecord<String, String>> rdd\n+          ) {\n+            OffsetRange[] offsets = ((HasOffsetRanges) rdd.rdd()).offsetRanges();\n+            offsetRanges.set(offsets);\n+            Assert.assertEquals(topic2, offsets[0].topic());\n+            return rdd;\n+          }\n+        }\n+    ).map(\n+        new Function<ConsumerRecord<String, String>, String>() {\n+          @Override\n+          public String call(ConsumerRecord<String, String> r) {\n+            return r.value();\n+          }\n+        }\n+    );\n+\n+    JavaDStream<String> unifiedStream = stream1.union(stream2);\n+\n+    final Set<String> result = Collections.synchronizedSet(new HashSet<String>());\n+    unifiedStream.foreachRDD(new VoidFunction<JavaRDD<String>>() {\n+          @Override\n+          public void call(JavaRDD<String> rdd) {\n+            result.addAll(rdd.collect());\n+            for (OffsetRange o : offsetRanges.get()) {\n+              System.out.println("
  }],
  "prId": 11863
}]