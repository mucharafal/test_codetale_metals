[{
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "style -- just by convetion, ranges are an exception to the usual rule, they are wrapped with parens `(x until y).map`",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-09T20:32:04Z",
    "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.util.concurrent.{Executors, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Random\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig._\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark._\n+\n+class KafkaDataConsumerSuite extends SparkFunSuite with BeforeAndAfterAll {\n+\n+  private var testUtils: KafkaTestUtils = _\n+\n+  override def beforeAll {\n+    super.beforeAll()\n+    testUtils = new KafkaTestUtils\n+    testUtils.setup()\n+  }\n+\n+  override def afterAll {\n+    if (testUtils != null) {\n+      testUtils.teardown()\n+      testUtils = null\n+    }\n+    super.afterAll()\n+  }\n+\n+  test(\"concurrent use of KafkaDataConsumer\") {\n+    KafkaDataConsumer.init(16, 64, 0.75f)\n+\n+    val topic = \"topic\" + Random.nextInt()\n+    val data = (1 to 1000).map(_.toString)\n+    val topicPartition = new TopicPartition(topic, 0)\n+    testUtils.createTopic(topic)\n+    testUtils.sendMessages(topic, data.toArray)\n+\n+    val groupId = \"groupId\"\n+    val kafkaParams = Map[String, Object](\n+      GROUP_ID_CONFIG -> groupId,\n+      BOOTSTRAP_SERVERS_CONFIG -> testUtils.brokerAddress,\n+      KEY_DESERIALIZER_CLASS_CONFIG -> classOf[ByteArrayDeserializer].getName,\n+      VALUE_DESERIALIZER_CLASS_CONFIG -> classOf[ByteArrayDeserializer].getName,\n+      AUTO_OFFSET_RESET_CONFIG -> \"earliest\",\n+      ENABLE_AUTO_COMMIT_CONFIG -> \"false\"\n+    )\n+\n+    val numThreads = 100\n+    val numConsumerUsages = 500\n+\n+    @volatile var error: Throwable = null\n+\n+    def consume(i: Int): Unit = {\n+      val useCache = Random.nextBoolean\n+      val taskContext = if (Random.nextBoolean) {\n+        new TaskContextImpl(0, 0, 0, 0, attemptNumber = Random.nextInt(2), null, null, null)\n+      } else {\n+        null\n+      }\n+      val consumer = KafkaDataConsumer.acquire[Array[Byte], Array[Byte]](\n+        groupId, topicPartition, kafkaParams.asJava, taskContext, useCache)\n+      try {\n+        val rcvd = 0 until data.length map { offset =>"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Changed.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-12T13:07:29Z",
    "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.util.concurrent.{Executors, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Random\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig._\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark._\n+\n+class KafkaDataConsumerSuite extends SparkFunSuite with BeforeAndAfterAll {\n+\n+  private var testUtils: KafkaTestUtils = _\n+\n+  override def beforeAll {\n+    super.beforeAll()\n+    testUtils = new KafkaTestUtils\n+    testUtils.setup()\n+  }\n+\n+  override def afterAll {\n+    if (testUtils != null) {\n+      testUtils.teardown()\n+      testUtils = null\n+    }\n+    super.afterAll()\n+  }\n+\n+  test(\"concurrent use of KafkaDataConsumer\") {\n+    KafkaDataConsumer.init(16, 64, 0.75f)\n+\n+    val topic = \"topic\" + Random.nextInt()\n+    val data = (1 to 1000).map(_.toString)\n+    val topicPartition = new TopicPartition(topic, 0)\n+    testUtils.createTopic(topic)\n+    testUtils.sendMessages(topic, data.toArray)\n+\n+    val groupId = \"groupId\"\n+    val kafkaParams = Map[String, Object](\n+      GROUP_ID_CONFIG -> groupId,\n+      BOOTSTRAP_SERVERS_CONFIG -> testUtils.brokerAddress,\n+      KEY_DESERIALIZER_CLASS_CONFIG -> classOf[ByteArrayDeserializer].getName,\n+      VALUE_DESERIALIZER_CLASS_CONFIG -> classOf[ByteArrayDeserializer].getName,\n+      AUTO_OFFSET_RESET_CONFIG -> \"earliest\",\n+      ENABLE_AUTO_COMMIT_CONFIG -> \"false\"\n+    )\n+\n+    val numThreads = 100\n+    val numConsumerUsages = 500\n+\n+    @volatile var error: Throwable = null\n+\n+    def consume(i: Int): Unit = {\n+      val useCache = Random.nextBoolean\n+      val taskContext = if (Random.nextBoolean) {\n+        new TaskContextImpl(0, 0, 0, 0, attemptNumber = Random.nextInt(2), null, null, null)\n+      } else {\n+        null\n+      }\n+      val consumer = KafkaDataConsumer.acquire[Array[Byte], Array[Byte]](\n+        groupId, topicPartition, kafkaParams.asJava, taskContext, useCache)\n+      try {\n+        val rcvd = 0 until data.length map { offset =>"
  }],
  "prId": 20997
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "this is good, but it would be nice to have a test which checks that cached consumers are re-used when possible.  Eg this could pass just by never caching anything.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-09T20:37:19Z",
    "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.util.concurrent.{Executors, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Random\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig._\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark._\n+\n+class KafkaDataConsumerSuite extends SparkFunSuite with BeforeAndAfterAll {\n+\n+  private var testUtils: KafkaTestUtils = _\n+\n+  override def beforeAll {\n+    super.beforeAll()\n+    testUtils = new KafkaTestUtils\n+    testUtils.setup()\n+  }\n+\n+  override def afterAll {\n+    if (testUtils != null) {\n+      testUtils.teardown()\n+      testUtils = null\n+    }\n+    super.afterAll()\n+  }\n+\n+  test(\"concurrent use of KafkaDataConsumer\") {"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Reuse test will be added.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-12T14:48:13Z",
    "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.util.concurrent.{Executors, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Random\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig._\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark._\n+\n+class KafkaDataConsumerSuite extends SparkFunSuite with BeforeAndAfterAll {\n+\n+  private var testUtils: KafkaTestUtils = _\n+\n+  override def beforeAll {\n+    super.beforeAll()\n+    testUtils = new KafkaTestUtils\n+    testUtils.setup()\n+  }\n+\n+  override def afterAll {\n+    if (testUtils != null) {\n+      testUtils.teardown()\n+      testUtils = null\n+    }\n+    super.afterAll()\n+  }\n+\n+  test(\"concurrent use of KafkaDataConsumer\") {"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Reuse test added.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-13T20:32:09Z",
    "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.util.concurrent.{Executors, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Random\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig._\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark._\n+\n+class KafkaDataConsumerSuite extends SparkFunSuite with BeforeAndAfterAll {\n+\n+  private var testUtils: KafkaTestUtils = _\n+\n+  override def beforeAll {\n+    super.beforeAll()\n+    testUtils = new KafkaTestUtils\n+    testUtils.setup()\n+  }\n+\n+  override def afterAll {\n+    if (testUtils != null) {\n+      testUtils.teardown()\n+      testUtils = null\n+    }\n+    super.afterAll()\n+  }\n+\n+  test(\"concurrent use of KafkaDataConsumer\") {"
  }],
  "prId": 20997
}, {
  "comments": [{
    "author": {
      "login": "koeninger"
    },
    "body": "If this PR is intended to fix a problem with silent reading of incorrect data, can you add a test reproducing that?",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-10T02:13:51Z",
    "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.util.concurrent.{Executors, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Random\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig._\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark._\n+\n+class KafkaDataConsumerSuite extends SparkFunSuite with BeforeAndAfterAll {\n+\n+  private var testUtils: KafkaTestUtils = _\n+\n+  override def beforeAll {\n+    super.beforeAll()\n+    testUtils = new KafkaTestUtils\n+    testUtils.setup()\n+  }\n+\n+  override def afterAll {\n+    if (testUtils != null) {\n+      testUtils.teardown()\n+      testUtils = null\n+    }\n+    super.afterAll()\n+  }\n+\n+  test(\"concurrent use of KafkaDataConsumer\") {\n+    KafkaDataConsumer.init(16, 64, 0.75f)\n+\n+    val topic = \"topic\" + Random.nextInt()\n+    val data = (1 to 1000).map(_.toString)\n+    val topicPartition = new TopicPartition(topic, 0)\n+    testUtils.createTopic(topic)\n+    testUtils.sendMessages(topic, data.toArray)\n+\n+    val groupId = \"groupId\"\n+    val kafkaParams = Map[String, Object](\n+      GROUP_ID_CONFIG -> groupId,\n+      BOOTSTRAP_SERVERS_CONFIG -> testUtils.brokerAddress,\n+      KEY_DESERIALIZER_CLASS_CONFIG -> classOf[ByteArrayDeserializer].getName,\n+      VALUE_DESERIALIZER_CLASS_CONFIG -> classOf[ByteArrayDeserializer].getName,\n+      AUTO_OFFSET_RESET_CONFIG -> \"earliest\",\n+      ENABLE_AUTO_COMMIT_CONFIG -> \"false\"\n+    )\n+\n+    val numThreads = 100\n+    val numConsumerUsages = 500\n+\n+    @volatile var error: Throwable = null\n+\n+    def consume(i: Int): Unit = {\n+      val useCache = Random.nextBoolean\n+      val taskContext = if (Random.nextBoolean) {\n+        new TaskContextImpl(0, 0, 0, 0, attemptNumber = Random.nextInt(2), null, null, null)\n+      } else {\n+        null\n+      }\n+      val consumer = KafkaDataConsumer.acquire[Array[Byte], Array[Byte]](\n+        groupId, topicPartition, kafkaParams.asJava, taskContext, useCache)\n+      try {\n+        val rcvd = 0 until data.length map { offset =>\n+          val bytes = consumer.get(offset, 10000).value()\n+          new String(bytes)\n+        }\n+        assert(rcvd == data)\n+      } catch {\n+        case e: Throwable =>\n+          error = e\n+          throw e\n+      } finally {\n+        consumer.release()\n+      }\n+    }\n+\n+    val threadPool = Executors.newFixedThreadPool(numThreads)\n+    try {\n+      val futures = (1 to numConsumerUsages).map { i =>\n+        threadPool.submit(new Runnable {\n+          override def run(): Unit = { consume(i) }\n+        })\n+      }\n+      futures.foreach(_.get(1, TimeUnit.MINUTES))\n+      assert(error == null)\n+    } finally {\n+      threadPool.shutdown()\n+    }\n+  }\n+}",
    "line": 131
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "That's a baad cut and paste issue. This PR intends to solve ` ConcurrentModificationException`.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-12T13:10:18Z",
    "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.util.concurrent.{Executors, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Random\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig._\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark._\n+\n+class KafkaDataConsumerSuite extends SparkFunSuite with BeforeAndAfterAll {\n+\n+  private var testUtils: KafkaTestUtils = _\n+\n+  override def beforeAll {\n+    super.beforeAll()\n+    testUtils = new KafkaTestUtils\n+    testUtils.setup()\n+  }\n+\n+  override def afterAll {\n+    if (testUtils != null) {\n+      testUtils.teardown()\n+      testUtils = null\n+    }\n+    super.afterAll()\n+  }\n+\n+  test(\"concurrent use of KafkaDataConsumer\") {\n+    KafkaDataConsumer.init(16, 64, 0.75f)\n+\n+    val topic = \"topic\" + Random.nextInt()\n+    val data = (1 to 1000).map(_.toString)\n+    val topicPartition = new TopicPartition(topic, 0)\n+    testUtils.createTopic(topic)\n+    testUtils.sendMessages(topic, data.toArray)\n+\n+    val groupId = \"groupId\"\n+    val kafkaParams = Map[String, Object](\n+      GROUP_ID_CONFIG -> groupId,\n+      BOOTSTRAP_SERVERS_CONFIG -> testUtils.brokerAddress,\n+      KEY_DESERIALIZER_CLASS_CONFIG -> classOf[ByteArrayDeserializer].getName,\n+      VALUE_DESERIALIZER_CLASS_CONFIG -> classOf[ByteArrayDeserializer].getName,\n+      AUTO_OFFSET_RESET_CONFIG -> \"earliest\",\n+      ENABLE_AUTO_COMMIT_CONFIG -> \"false\"\n+    )\n+\n+    val numThreads = 100\n+    val numConsumerUsages = 500\n+\n+    @volatile var error: Throwable = null\n+\n+    def consume(i: Int): Unit = {\n+      val useCache = Random.nextBoolean\n+      val taskContext = if (Random.nextBoolean) {\n+        new TaskContextImpl(0, 0, 0, 0, attemptNumber = Random.nextInt(2), null, null, null)\n+      } else {\n+        null\n+      }\n+      val consumer = KafkaDataConsumer.acquire[Array[Byte], Array[Byte]](\n+        groupId, topicPartition, kafkaParams.asJava, taskContext, useCache)\n+      try {\n+        val rcvd = 0 until data.length map { offset =>\n+          val bytes = consumer.get(offset, 10000).value()\n+          new String(bytes)\n+        }\n+        assert(rcvd == data)\n+      } catch {\n+        case e: Throwable =>\n+          error = e\n+          throw e\n+      } finally {\n+        consumer.release()\n+      }\n+    }\n+\n+    val threadPool = Executors.newFixedThreadPool(numThreads)\n+    try {\n+      val futures = (1 to numConsumerUsages).map { i =>\n+        threadPool.submit(new Runnable {\n+          override def run(): Unit = { consume(i) }\n+        })\n+      }\n+      futures.foreach(_.get(1, TimeUnit.MINUTES))\n+      assert(error == null)\n+    } finally {\n+      threadPool.shutdown()\n+    }\n+  }\n+}",
    "line": 131
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Removed from the PR description.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-12T14:18:55Z",
    "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.util.concurrent.{Executors, TimeUnit}\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Random\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig._\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer\n+import org.scalatest.BeforeAndAfterAll\n+\n+import org.apache.spark._\n+\n+class KafkaDataConsumerSuite extends SparkFunSuite with BeforeAndAfterAll {\n+\n+  private var testUtils: KafkaTestUtils = _\n+\n+  override def beforeAll {\n+    super.beforeAll()\n+    testUtils = new KafkaTestUtils\n+    testUtils.setup()\n+  }\n+\n+  override def afterAll {\n+    if (testUtils != null) {\n+      testUtils.teardown()\n+      testUtils = null\n+    }\n+    super.afterAll()\n+  }\n+\n+  test(\"concurrent use of KafkaDataConsumer\") {\n+    KafkaDataConsumer.init(16, 64, 0.75f)\n+\n+    val topic = \"topic\" + Random.nextInt()\n+    val data = (1 to 1000).map(_.toString)\n+    val topicPartition = new TopicPartition(topic, 0)\n+    testUtils.createTopic(topic)\n+    testUtils.sendMessages(topic, data.toArray)\n+\n+    val groupId = \"groupId\"\n+    val kafkaParams = Map[String, Object](\n+      GROUP_ID_CONFIG -> groupId,\n+      BOOTSTRAP_SERVERS_CONFIG -> testUtils.brokerAddress,\n+      KEY_DESERIALIZER_CLASS_CONFIG -> classOf[ByteArrayDeserializer].getName,\n+      VALUE_DESERIALIZER_CLASS_CONFIG -> classOf[ByteArrayDeserializer].getName,\n+      AUTO_OFFSET_RESET_CONFIG -> \"earliest\",\n+      ENABLE_AUTO_COMMIT_CONFIG -> \"false\"\n+    )\n+\n+    val numThreads = 100\n+    val numConsumerUsages = 500\n+\n+    @volatile var error: Throwable = null\n+\n+    def consume(i: Int): Unit = {\n+      val useCache = Random.nextBoolean\n+      val taskContext = if (Random.nextBoolean) {\n+        new TaskContextImpl(0, 0, 0, 0, attemptNumber = Random.nextInt(2), null, null, null)\n+      } else {\n+        null\n+      }\n+      val consumer = KafkaDataConsumer.acquire[Array[Byte], Array[Byte]](\n+        groupId, topicPartition, kafkaParams.asJava, taskContext, useCache)\n+      try {\n+        val rcvd = 0 until data.length map { offset =>\n+          val bytes = consumer.get(offset, 10000).value()\n+          new String(bytes)\n+        }\n+        assert(rcvd == data)\n+      } catch {\n+        case e: Throwable =>\n+          error = e\n+          throw e\n+      } finally {\n+        consumer.release()\n+      }\n+    }\n+\n+    val threadPool = Executors.newFixedThreadPool(numThreads)\n+    try {\n+      val futures = (1 to numConsumerUsages).map { i =>\n+        threadPool.submit(new Runnable {\n+          override def run(): Unit = { consume(i) }\n+        })\n+      }\n+      futures.foreach(_.get(1, TimeUnit.MINUTES))\n+      assert(error == null)\n+    } finally {\n+      threadPool.shutdown()\n+    }\n+  }\n+}",
    "line": 131
  }],
  "prId": 20997
}]