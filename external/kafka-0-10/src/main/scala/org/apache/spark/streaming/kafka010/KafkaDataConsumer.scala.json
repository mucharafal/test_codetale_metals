[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: follow coding style for multi-line declarations.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-09T17:35:18Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Leftover from old code, fixing.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-11T14:43:56Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,"
  }],
  "prId": 20997
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Given this, is there any advantage in passing the group ID as a parameter?",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-09T17:35:55Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "This is mostly vestigial (there used to be a remove method that took a groupId, but no kafkaParams, so there was symmetry).  \r\n\r\nI don't see a reason it can't be changed to match the SQL version at this point, i.e. assign groupId from the params.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-10T02:38:38Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Changed to assign the groupId from the params.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-11T15:50:44Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),"
  }],
  "prId": 20997
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`val topics = ju.Arrays.asList(topicPartition)`",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-09T17:37:31Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Changed.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-11T15:52:11Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()"
  }],
  "prId": 20997
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: break into multiple lines.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-09T17:38:16Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Leftover from old code, fixing.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-11T15:53:15Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }"
  }],
  "prId": 20997
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This generally means the class should be `private` not `private[blah]`.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-09T17:42:01Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.",
    "line": 81
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Changed.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-11T15:54:51Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.",
    "line": 81
  }],
  "prId": 20997
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Modifier goes in same line as class declaration.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-09T17:42:53Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Leftover from old code, fixing.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-11T15:56:31Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]"
  }],
  "prId": 20997
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Multiple lines or drop the braces.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-09T17:43:29Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Changed.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-11T15:58:23Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }"
  }],
  "prId": 20997
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Same.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-09T17:43:39Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Changed.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-11T15:58:27Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }"
  }],
  "prId": 20997
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Unnecessary comment.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-09T17:44:08Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Removed.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-11T16:05:07Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true"
  }],
  "prId": 20997
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Is the full type needed here again? Almost sure the compiler can figure that out alone.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-09T17:45:59Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]("
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Either here having type or have to cast at `entry.getValue`. Don't see the benefit at the moment.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-12T15:07:11Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]("
  }],
  "prId": 20997
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: indent more.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-09T17:46:34Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]): Boolean = {"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Leftover from old code, fixing.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-12T12:08:00Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]): Boolean = {"
  }],
  "prId": 20997
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`.foreach(_.close())`",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-09T17:46:57Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            try {\n+              entry.getValue.asScala.foreach { _.close() }"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Changed.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-12T12:08:31Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            try {\n+              entry.getValue.asScala.foreach { _.close() }"
  }],
  "prId": 20997
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This goes back to that assert somewhere else in the code. Given the existence of that assert, having this as a separate parameter seems confusing and unnecessary.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-09T17:50:05Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            try {\n+              entry.getValue.asScala.foreach { _.close() }\n+            } catch {\n+              case x: KafkaException =>\n+                logError(\"Error closing oldest Kafka consumer\", x)\n+            }\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   * The returned consumer must be released explicitly using [[KafkaDataConsumer.release()]].\n+   *\n+   * Note: This method guarantees that the consumer returned is not currently in use by anyone\n+   * else. Within this guarantee, this method will make a best effort attempt to re-use consumers by\n+   * caching them and tracking when they are in use.\n+   */\n+  def acquire[K, V](\n+      groupId: String,"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Removed.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-12T12:09:00Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            try {\n+              entry.getValue.asScala.foreach { _.close() }\n+            } catch {\n+              case x: KafkaException =>\n+                logError(\"Error closing oldest Kafka consumer\", x)\n+            }\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   * The returned consumer must be released explicitly using [[KafkaDataConsumer.release()]].\n+   *\n+   * Note: This method guarantees that the consumer returned is not currently in use by anyone\n+   * else. Within this guarantee, this method will make a best effort attempt to re-use consumers by\n+   * caching them and tracking when they are in use.\n+   */\n+  def acquire[K, V](\n+      groupId: String,"
  }],
  "prId": 20997
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "it's",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-09T17:52:07Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            try {\n+              entry.getValue.asScala.foreach { _.close() }\n+            } catch {\n+              case x: KafkaException =>\n+                logError(\"Error closing oldest Kafka consumer\", x)\n+            }\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   * The returned consumer must be released explicitly using [[KafkaDataConsumer.release()]].\n+   *\n+   * Note: This method guarantees that the consumer returned is not currently in use by anyone\n+   * else. Within this guarantee, this method will make a best effort attempt to re-use consumers by\n+   * caching them and tracking when they are in use.\n+   */\n+  def acquire[K, V](\n+      groupId: String,\n+      topicPartition: TopicPartition,\n+      kafkaParams: ju.Map[String, Object],\n+      context: TaskContext,\n+      useCache: Boolean): KafkaDataConsumer[K, V] = synchronized {\n+    val key = new CacheKey(groupId, topicPartition)\n+    val existingInternalConsumers = Option(cache.get(key))\n+      .getOrElse(new ju.LinkedList[InternalKafkaConsumer[_, _]])\n+\n+    cache.putIfAbsent(key, existingInternalConsumers)\n+\n+    lazy val newInternalConsumer = new InternalKafkaConsumer[K, V](\n+      groupId, topicPartition, kafkaParams)\n+\n+    if (context != null && context.attemptNumber >= 1) {\n+      // If this is reattempt at running the task, then invalidate cached consumers if any and\n+      // start with a new one.\n+      logDebug(\"Reattempt detected, invalidating cached consumers\")\n+      val closedExistingInternalConsumers = new ju.LinkedList[InternalKafkaConsumer[_, _]]()\n+      existingInternalConsumers.asScala.foreach { existingInternalConsumer =>\n+        // Consumer exists in cache. If its in use, mark it for closing later, or close it now."
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Fixed.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-12T12:10:26Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            try {\n+              entry.getValue.asScala.foreach { _.close() }\n+            } catch {\n+              case x: KafkaException =>\n+                logError(\"Error closing oldest Kafka consumer\", x)\n+            }\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   * The returned consumer must be released explicitly using [[KafkaDataConsumer.release()]].\n+   *\n+   * Note: This method guarantees that the consumer returned is not currently in use by anyone\n+   * else. Within this guarantee, this method will make a best effort attempt to re-use consumers by\n+   * caching them and tracking when they are in use.\n+   */\n+  def acquire[K, V](\n+      groupId: String,\n+      topicPartition: TopicPartition,\n+      kafkaParams: ju.Map[String, Object],\n+      context: TaskContext,\n+      useCache: Boolean): KafkaDataConsumer[K, V] = synchronized {\n+    val key = new CacheKey(groupId, topicPartition)\n+    val existingInternalConsumers = Option(cache.get(key))\n+      .getOrElse(new ju.LinkedList[InternalKafkaConsumer[_, _]])\n+\n+    cache.putIfAbsent(key, existingInternalConsumers)\n+\n+    lazy val newInternalConsumer = new InternalKafkaConsumer[K, V](\n+      groupId, topicPartition, kafkaParams)\n+\n+    if (context != null && context.attemptNumber >= 1) {\n+      // If this is reattempt at running the task, then invalidate cached consumers if any and\n+      // start with a new one.\n+      logDebug(\"Reattempt detected, invalidating cached consumers\")\n+      val closedExistingInternalConsumers = new ju.LinkedList[InternalKafkaConsumer[_, _]]()\n+      existingInternalConsumers.asScala.foreach { existingInternalConsumer =>\n+        // Consumer exists in cache. If its in use, mark it for closing later, or close it now."
  }],
  "prId": 20997
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "All these blank lines before the closing brace are unnecessary.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-09T17:55:21Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            try {\n+              entry.getValue.asScala.foreach { _.close() }\n+            } catch {\n+              case x: KafkaException =>\n+                logError(\"Error closing oldest Kafka consumer\", x)\n+            }\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   * The returned consumer must be released explicitly using [[KafkaDataConsumer.release()]].\n+   *\n+   * Note: This method guarantees that the consumer returned is not currently in use by anyone\n+   * else. Within this guarantee, this method will make a best effort attempt to re-use consumers by\n+   * caching them and tracking when they are in use.\n+   */\n+  def acquire[K, V](\n+      groupId: String,\n+      topicPartition: TopicPartition,\n+      kafkaParams: ju.Map[String, Object],\n+      context: TaskContext,\n+      useCache: Boolean): KafkaDataConsumer[K, V] = synchronized {\n+    val key = new CacheKey(groupId, topicPartition)\n+    val existingInternalConsumers = Option(cache.get(key))\n+      .getOrElse(new ju.LinkedList[InternalKafkaConsumer[_, _]])\n+\n+    cache.putIfAbsent(key, existingInternalConsumers)\n+\n+    lazy val newInternalConsumer = new InternalKafkaConsumer[K, V](\n+      groupId, topicPartition, kafkaParams)\n+\n+    if (context != null && context.attemptNumber >= 1) {\n+      // If this is reattempt at running the task, then invalidate cached consumers if any and\n+      // start with a new one.\n+      logDebug(\"Reattempt detected, invalidating cached consumers\")\n+      val closedExistingInternalConsumers = new ju.LinkedList[InternalKafkaConsumer[_, _]]()\n+      existingInternalConsumers.asScala.foreach { existingInternalConsumer =>\n+        // Consumer exists in cache. If its in use, mark it for closing later, or close it now.\n+        if (existingInternalConsumer.inUse) {\n+          existingInternalConsumer.markedForClose = true\n+        } else {\n+          existingInternalConsumer.close()\n+          closedExistingInternalConsumers.add(existingInternalConsumer)\n+        }\n+      }\n+      existingInternalConsumers.removeAll(closedExistingInternalConsumers)\n+\n+      logDebug(\"Reattempt detected, new cached consumer will be allocated \" +\n+        s\"$newInternalConsumer\")\n+      newInternalConsumer.inUse = true\n+      existingInternalConsumers.add(newInternalConsumer)\n+      CachedKafkaDataConsumer(newInternalConsumer)\n+"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Removed them.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-12T12:11:18Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            try {\n+              entry.getValue.asScala.foreach { _.close() }\n+            } catch {\n+              case x: KafkaException =>\n+                logError(\"Error closing oldest Kafka consumer\", x)\n+            }\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   * The returned consumer must be released explicitly using [[KafkaDataConsumer.release()]].\n+   *\n+   * Note: This method guarantees that the consumer returned is not currently in use by anyone\n+   * else. Within this guarantee, this method will make a best effort attempt to re-use consumers by\n+   * caching them and tracking when they are in use.\n+   */\n+  def acquire[K, V](\n+      groupId: String,\n+      topicPartition: TopicPartition,\n+      kafkaParams: ju.Map[String, Object],\n+      context: TaskContext,\n+      useCache: Boolean): KafkaDataConsumer[K, V] = synchronized {\n+    val key = new CacheKey(groupId, topicPartition)\n+    val existingInternalConsumers = Option(cache.get(key))\n+      .getOrElse(new ju.LinkedList[InternalKafkaConsumer[_, _]])\n+\n+    cache.putIfAbsent(key, existingInternalConsumers)\n+\n+    lazy val newInternalConsumer = new InternalKafkaConsumer[K, V](\n+      groupId, topicPartition, kafkaParams)\n+\n+    if (context != null && context.attemptNumber >= 1) {\n+      // If this is reattempt at running the task, then invalidate cached consumers if any and\n+      // start with a new one.\n+      logDebug(\"Reattempt detected, invalidating cached consumers\")\n+      val closedExistingInternalConsumers = new ju.LinkedList[InternalKafkaConsumer[_, _]]()\n+      existingInternalConsumers.asScala.foreach { existingInternalConsumer =>\n+        // Consumer exists in cache. If its in use, mark it for closing later, or close it now.\n+        if (existingInternalConsumer.inUse) {\n+          existingInternalConsumer.markedForClose = true\n+        } else {\n+          existingInternalConsumer.close()\n+          closedExistingInternalConsumers.add(existingInternalConsumer)\n+        }\n+      }\n+      existingInternalConsumers.removeAll(closedExistingInternalConsumers)\n+\n+      logDebug(\"Reattempt detected, new cached consumer will be allocated \" +\n+        s\"$newInternalConsumer\")\n+      newInternalConsumer.inUse = true\n+      existingInternalConsumers.add(newInternalConsumer)\n+      CachedKafkaDataConsumer(newInternalConsumer)\n+"
  }],
  "prId": 20997
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "I'm a little confused why this needs special treatment.\r\n\r\nIf this is the first attempt, won't the list just be empty? And then you could execute the same code and it would basically be a no-op?\r\n\r\nOr, from a different angle, why can't you reuse the consumers? Isn't the problem just concurrent use? So if the consumer is not in use, it should be fair game, right?",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-09T17:58:38Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            try {\n+              entry.getValue.asScala.foreach { _.close() }\n+            } catch {\n+              case x: KafkaException =>\n+                logError(\"Error closing oldest Kafka consumer\", x)\n+            }\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   * The returned consumer must be released explicitly using [[KafkaDataConsumer.release()]].\n+   *\n+   * Note: This method guarantees that the consumer returned is not currently in use by anyone\n+   * else. Within this guarantee, this method will make a best effort attempt to re-use consumers by\n+   * caching them and tracking when they are in use.\n+   */\n+  def acquire[K, V](\n+      groupId: String,\n+      topicPartition: TopicPartition,\n+      kafkaParams: ju.Map[String, Object],\n+      context: TaskContext,\n+      useCache: Boolean): KafkaDataConsumer[K, V] = synchronized {\n+    val key = new CacheKey(groupId, topicPartition)\n+    val existingInternalConsumers = Option(cache.get(key))\n+      .getOrElse(new ju.LinkedList[InternalKafkaConsumer[_, _]])\n+\n+    cache.putIfAbsent(key, existingInternalConsumers)\n+\n+    lazy val newInternalConsumer = new InternalKafkaConsumer[K, V](\n+      groupId, topicPartition, kafkaParams)\n+\n+    if (context != null && context.attemptNumber >= 1) {"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "I agree, the comment in the old code was more clear: \"just in case the prior attempt failures were cache related\"",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-09T20:29:56Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            try {\n+              entry.getValue.asScala.foreach { _.close() }\n+            } catch {\n+              case x: KafkaException =>\n+                logError(\"Error closing oldest Kafka consumer\", x)\n+            }\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   * The returned consumer must be released explicitly using [[KafkaDataConsumer.release()]].\n+   *\n+   * Note: This method guarantees that the consumer returned is not currently in use by anyone\n+   * else. Within this guarantee, this method will make a best effort attempt to re-use consumers by\n+   * caching them and tracking when they are in use.\n+   */\n+  def acquire[K, V](\n+      groupId: String,\n+      topicPartition: TopicPartition,\n+      kafkaParams: ju.Map[String, Object],\n+      context: TaskContext,\n+      useCache: Boolean): KafkaDataConsumer[K, V] = synchronized {\n+    val key = new CacheKey(groupId, topicPartition)\n+    val existingInternalConsumers = Option(cache.get(key))\n+      .getOrElse(new ju.LinkedList[InternalKafkaConsumer[_, _]])\n+\n+    cache.putIfAbsent(key, existingInternalConsumers)\n+\n+    lazy val newInternalConsumer = new InternalKafkaConsumer[K, V](\n+      groupId, topicPartition, kafkaParams)\n+\n+    if (context != null && context.attemptNumber >= 1) {"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "If this is the first attempt then the list can contain elements because other tasks can create other consumers. Imran is right the `why` was written in `KafkaRDD.scala` which is missing here. Additional explanation added.\r\n\r\nIn short if the possible problematic consumer is not removed from the list then it could infect tasks run later.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-12T12:32:19Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            try {\n+              entry.getValue.asScala.foreach { _.close() }\n+            } catch {\n+              case x: KafkaException =>\n+                logError(\"Error closing oldest Kafka consumer\", x)\n+            }\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   * The returned consumer must be released explicitly using [[KafkaDataConsumer.release()]].\n+   *\n+   * Note: This method guarantees that the consumer returned is not currently in use by anyone\n+   * else. Within this guarantee, this method will make a best effort attempt to re-use consumers by\n+   * caching them and tracking when they are in use.\n+   */\n+  def acquire[K, V](\n+      groupId: String,\n+      topicPartition: TopicPartition,\n+      kafkaParams: ju.Map[String, Object],\n+      context: TaskContext,\n+      useCache: Boolean): KafkaDataConsumer[K, V] = synchronized {\n+    val key = new CacheKey(groupId, topicPartition)\n+    val existingInternalConsumers = Option(cache.get(key))\n+      .getOrElse(new ju.LinkedList[InternalKafkaConsumer[_, _]])\n+\n+    cache.putIfAbsent(key, existingInternalConsumers)\n+\n+    lazy val newInternalConsumer = new InternalKafkaConsumer[K, V](\n+      groupId, topicPartition, kafkaParams)\n+\n+    if (context != null && context.attemptNumber >= 1) {"
  }],
  "prId": 20997
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Shouldn't you just skip messing with the cache if `useCache = false`?",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-09T18:01:29Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            try {\n+              entry.getValue.asScala.foreach { _.close() }\n+            } catch {\n+              case x: KafkaException =>\n+                logError(\"Error closing oldest Kafka consumer\", x)\n+            }\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   * The returned consumer must be released explicitly using [[KafkaDataConsumer.release()]].\n+   *\n+   * Note: This method guarantees that the consumer returned is not currently in use by anyone\n+   * else. Within this guarantee, this method will make a best effort attempt to re-use consumers by\n+   * caching them and tracking when they are in use.\n+   */\n+  def acquire[K, V](\n+      groupId: String,\n+      topicPartition: TopicPartition,\n+      kafkaParams: ju.Map[String, Object],\n+      context: TaskContext,\n+      useCache: Boolean): KafkaDataConsumer[K, V] = synchronized {\n+    val key = new CacheKey(groupId, topicPartition)\n+    val existingInternalConsumers = Option(cache.get(key))"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "One can spare 3 pieces of O(1) operations on the other side would make the code less readable.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-12T12:40:57Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            try {\n+              entry.getValue.asScala.foreach { _.close() }\n+            } catch {\n+              case x: KafkaException =>\n+                logError(\"Error closing oldest Kafka consumer\", x)\n+            }\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   * The returned consumer must be released explicitly using [[KafkaDataConsumer.release()]].\n+   *\n+   * Note: This method guarantees that the consumer returned is not currently in use by anyone\n+   * else. Within this guarantee, this method will make a best effort attempt to re-use consumers by\n+   * caching them and tracking when they are in use.\n+   */\n+  def acquire[K, V](\n+      groupId: String,\n+      topicPartition: TopicPartition,\n+      kafkaParams: ju.Map[String, Object],\n+      context: TaskContext,\n+      useCache: Boolean): KafkaDataConsumer[K, V] = synchronized {\n+    val key = new CacheKey(groupId, topicPartition)\n+    val existingInternalConsumers = Option(cache.get(key))"
  }],
  "prId": 20997
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Given that all these variables are read / updated inside `synchronized` blocks, `@volatile` is unnecessary and a little misleading.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-09T18:02:58Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "This is an example of the cut & paste I was referring to.\r\n\r\nIn this case, I don't believe consumer is ever reassigned, so it doesn't even need to be a var.\r\n\r\nIt was reassigned in the SQL version of the code.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-10T02:16:58Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Yeah, it's an overkill. Removed volatile and switched to val.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-12T12:43:20Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer"
  }],
  "prId": 20997
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "any particular reason you want to use a `java.util.LinkedList`?  If you instead used a scala collection, eg. `scala.collection.mutable.ArrayBuffer`, you would avoid the `asScala`s.  Or `ListBuffer` if you actually want a `LinkedList`",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-09T20:01:19Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            try {\n+              entry.getValue.asScala.foreach { _.close() }\n+            } catch {\n+              case x: KafkaException =>\n+                logError(\"Error closing oldest Kafka consumer\", x)\n+            }\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   * The returned consumer must be released explicitly using [[KafkaDataConsumer.release()]].\n+   *\n+   * Note: This method guarantees that the consumer returned is not currently in use by anyone\n+   * else. Within this guarantee, this method will make a best effort attempt to re-use consumers by\n+   * caching them and tracking when they are in use.\n+   */\n+  def acquire[K, V](\n+      groupId: String,\n+      topicPartition: TopicPartition,\n+      kafkaParams: ju.Map[String, Object],\n+      context: TaskContext,\n+      useCache: Boolean): KafkaDataConsumer[K, V] = synchronized {\n+    val key = new CacheKey(groupId, topicPartition)\n+    val existingInternalConsumers = Option(cache.get(key))\n+      .getOrElse(new ju.LinkedList[InternalKafkaConsumer[_, _]])"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "Here's another potential problem with using a linked list instead of a pool that has more smarts.\r\n\r\nLet's say I have two consumers on the same topicpartion in the same jvm.  Consumer A last read offset 1000 and pre-fetched the next N messages, consumer B last read offset 2000 and pre-fetched the next N messages.\r\n\r\nIf the client code that was using consumer A last time gets consumer B out of the cache this time, the prefetch is wasted and it will have to seek.  All we're saving by caching at that point is connection time.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-10T03:04:07Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            try {\n+              entry.getValue.asScala.foreach { _.close() }\n+            } catch {\n+              case x: KafkaException =>\n+                logError(\"Error closing oldest Kafka consumer\", x)\n+            }\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   * The returned consumer must be released explicitly using [[KafkaDataConsumer.release()]].\n+   *\n+   * Note: This method guarantees that the consumer returned is not currently in use by anyone\n+   * else. Within this guarantee, this method will make a best effort attempt to re-use consumers by\n+   * caching them and tracking when they are in use.\n+   */\n+  def acquire[K, V](\n+      groupId: String,\n+      topicPartition: TopicPartition,\n+      kafkaParams: ju.Map[String, Object],\n+      context: TaskContext,\n+      useCache: Boolean): KafkaDataConsumer[K, V] = synchronized {\n+    val key = new CacheKey(groupId, topicPartition)\n+    val existingInternalConsumers = Option(cache.get(key))\n+      .getOrElse(new ju.LinkedList[InternalKafkaConsumer[_, _]])"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "In general happy to hear better approach.\r\n\r\n@squito The code started with java collections and didn't want to break that. A couple of conversion was the price.\r\n\r\n@koeninger True. The last info what I've seen in the SQL area is that this approach used and discussion started how to make it better. Is there an outcome which can be applied to both area? If the situation is different please share with me.\r\n\r\nEven if `only` the connection can be enhanced that could be significant in case of SSL/TLS.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-12T13:04:07Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            try {\n+              entry.getValue.asScala.foreach { _.close() }\n+            } catch {\n+              case x: KafkaException =>\n+                logError(\"Error closing oldest Kafka consumer\", x)\n+            }\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   * The returned consumer must be released explicitly using [[KafkaDataConsumer.release()]].\n+   *\n+   * Note: This method guarantees that the consumer returned is not currently in use by anyone\n+   * else. Within this guarantee, this method will make a best effort attempt to re-use consumers by\n+   * caching them and tracking when they are in use.\n+   */\n+  def acquire[K, V](\n+      groupId: String,\n+      topicPartition: TopicPartition,\n+      kafkaParams: ju.Map[String, Object],\n+      context: TaskContext,\n+      useCache: Boolean): KafkaDataConsumer[K, V] = synchronized {\n+    val key = new CacheKey(groupId, topicPartition)\n+    val existingInternalConsumers = Option(cache.get(key))\n+      .getOrElse(new ju.LinkedList[InternalKafkaConsumer[_, _]])"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "The SQL code, at least last time I looked at it, isn't keeping a linked list pool, it's just making a fresh consumer that's marked for close, right?  I actually think that's better as a stopgap solution, because it's less likely to leak in some unforeseen way, and it's consistent across both codebases.\r\n\r\n\r\n\r\n",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-13T20:53:30Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            try {\n+              entry.getValue.asScala.foreach { _.close() }\n+            } catch {\n+              case x: KafkaException =>\n+                logError(\"Error closing oldest Kafka consumer\", x)\n+            }\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   * The returned consumer must be released explicitly using [[KafkaDataConsumer.release()]].\n+   *\n+   * Note: This method guarantees that the consumer returned is not currently in use by anyone\n+   * else. Within this guarantee, this method will make a best effort attempt to re-use consumers by\n+   * caching them and tracking when they are in use.\n+   */\n+  def acquire[K, V](\n+      groupId: String,\n+      topicPartition: TopicPartition,\n+      kafkaParams: ju.Map[String, Object],\n+      context: TaskContext,\n+      useCache: Boolean): KafkaDataConsumer[K, V] = synchronized {\n+    val key = new CacheKey(groupId, topicPartition)\n+    val existingInternalConsumers = Option(cache.get(key))\n+      .getOrElse(new ju.LinkedList[InternalKafkaConsumer[_, _]])"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "That's correct, the SQL part isn't keeping a linked list pool but a single cached consumer. I was considering your suggestion and came to the same conclusion:\r\n\r\n```\r\nCan you clarify why you want to allow only 1 cached consumer per topicpartition, closing any others at task end?\r\n\r\nIt seems like opening and closing consumers would be less efficient than allowing a pool of more than one consumer per topicpartition.\r\n```\r\n\r\nThough limiting the number of cached consumers per groupId/TopicPartition is a must as you've pointed out. On the other side if we go the SQL way it's definitely less risky. Do you think we should switch back to the `one cached consumer` approach?\r\n",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-16T08:24:15Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            try {\n+              entry.getValue.asScala.foreach { _.close() }\n+            } catch {\n+              case x: KafkaException =>\n+                logError(\"Error closing oldest Kafka consumer\", x)\n+            }\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   * The returned consumer must be released explicitly using [[KafkaDataConsumer.release()]].\n+   *\n+   * Note: This method guarantees that the consumer returned is not currently in use by anyone\n+   * else. Within this guarantee, this method will make a best effort attempt to re-use consumers by\n+   * caching them and tracking when they are in use.\n+   */\n+  def acquire[K, V](\n+      groupId: String,\n+      topicPartition: TopicPartition,\n+      kafkaParams: ju.Map[String, Object],\n+      context: TaskContext,\n+      useCache: Boolean): KafkaDataConsumer[K, V] = synchronized {\n+    val key = new CacheKey(groupId, topicPartition)\n+    val existingInternalConsumers = Option(cache.get(key))\n+      .getOrElse(new ju.LinkedList[InternalKafkaConsumer[_, _]])"
  }],
  "prId": 20997
}, {
  "comments": [{
    "author": {
      "login": "koeninger"
    },
    "body": "I'm not convinced this lazy val is clearer than simply constructing a consumer when you need it, but that's the way the SQL code is...",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-10T02:46:32Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            try {\n+              entry.getValue.asScala.foreach { _.close() }\n+            } catch {\n+              case x: KafkaException =>\n+                logError(\"Error closing oldest Kafka consumer\", x)\n+            }\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   * The returned consumer must be released explicitly using [[KafkaDataConsumer.release()]].\n+   *\n+   * Note: This method guarantees that the consumer returned is not currently in use by anyone\n+   * else. Within this guarantee, this method will make a best effort attempt to re-use consumers by\n+   * caching them and tracking when they are in use.\n+   */\n+  def acquire[K, V](\n+      groupId: String,\n+      topicPartition: TopicPartition,\n+      kafkaParams: ju.Map[String, Object],\n+      context: TaskContext,\n+      useCache: Boolean): KafkaDataConsumer[K, V] = synchronized {\n+    val key = new CacheKey(groupId, topicPartition)\n+    val existingInternalConsumers = Option(cache.get(key))\n+      .getOrElse(new ju.LinkedList[InternalKafkaConsumer[_, _]])\n+\n+    cache.putIfAbsent(key, existingInternalConsumers)\n+\n+    lazy val newInternalConsumer = new InternalKafkaConsumer[K, V]("
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Is it better to copy/paste the same thing every occasion?",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-12T13:15:25Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            try {\n+              entry.getValue.asScala.foreach { _.close() }\n+            } catch {\n+              case x: KafkaException =>\n+                logError(\"Error closing oldest Kafka consumer\", x)\n+            }\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   * The returned consumer must be released explicitly using [[KafkaDataConsumer.release()]].\n+   *\n+   * Note: This method guarantees that the consumer returned is not currently in use by anyone\n+   * else. Within this guarantee, this method will make a best effort attempt to re-use consumers by\n+   * caching them and tracking when they are in use.\n+   */\n+  def acquire[K, V](\n+      groupId: String,\n+      topicPartition: TopicPartition,\n+      kafkaParams: ju.Map[String, Object],\n+      context: TaskContext,\n+      useCache: Boolean): KafkaDataConsumer[K, V] = synchronized {\n+    val key = new CacheKey(groupId, topicPartition)\n+    val existingInternalConsumers = Option(cache.get(key))\n+      .getOrElse(new ju.LinkedList[InternalKafkaConsumer[_, _]])\n+\n+    cache.putIfAbsent(key, existingInternalConsumers)\n+\n+    lazy val newInternalConsumer = new InternalKafkaConsumer[K, V]("
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "Copy paste a one line static constructor only in the cases a new consumer actually needs to be constructed?  Yes, I think that's clearer than a lazy val.  But again, probably not worth the difference from the SQL code.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-13T20:50:15Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            try {\n+              entry.getValue.asScala.foreach { _.close() }\n+            } catch {\n+              case x: KafkaException =>\n+                logError(\"Error closing oldest Kafka consumer\", x)\n+            }\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   * The returned consumer must be released explicitly using [[KafkaDataConsumer.release()]].\n+   *\n+   * Note: This method guarantees that the consumer returned is not currently in use by anyone\n+   * else. Within this guarantee, this method will make a best effort attempt to re-use consumers by\n+   * caching them and tracking when they are in use.\n+   */\n+  def acquire[K, V](\n+      groupId: String,\n+      topicPartition: TopicPartition,\n+      kafkaParams: ju.Map[String, Object],\n+      context: TaskContext,\n+      useCache: Boolean): KafkaDataConsumer[K, V] = synchronized {\n+    val key = new CacheKey(groupId, topicPartition)\n+    val existingInternalConsumers = Option(cache.get(key))\n+      .getOrElse(new ju.LinkedList[InternalKafkaConsumer[_, _]])\n+\n+    cache.putIfAbsent(key, existingInternalConsumers)\n+\n+    lazy val newInternalConsumer = new InternalKafkaConsumer[K, V]("
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "OK, leaving it how it is now.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-16T07:42:16Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            try {\n+              entry.getValue.asScala.foreach { _.close() }\n+            } catch {\n+              case x: KafkaException =>\n+                logError(\"Error closing oldest Kafka consumer\", x)\n+            }\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   * The returned consumer must be released explicitly using [[KafkaDataConsumer.release()]].\n+   *\n+   * Note: This method guarantees that the consumer returned is not currently in use by anyone\n+   * else. Within this guarantee, this method will make a best effort attempt to re-use consumers by\n+   * caching them and tracking when they are in use.\n+   */\n+  def acquire[K, V](\n+      groupId: String,\n+      topicPartition: TopicPartition,\n+      kafkaParams: ju.Map[String, Object],\n+      context: TaskContext,\n+      useCache: Boolean): KafkaDataConsumer[K, V] = synchronized {\n+    val key = new CacheKey(groupId, topicPartition)\n+    val existingInternalConsumers = Option(cache.get(key))\n+      .getOrElse(new ju.LinkedList[InternalKafkaConsumer[_, _]])\n+\n+    cache.putIfAbsent(key, existingInternalConsumers)\n+\n+    lazy val newInternalConsumer = new InternalKafkaConsumer[K, V]("
  }],
  "prId": 20997
}, {
  "comments": [{
    "author": {
      "login": "koeninger"
    },
    "body": "For all of these lines that set newInternalConsumer.inUse, is there any way it wouldn't already have been true?",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-10T02:47:48Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            try {\n+              entry.getValue.asScala.foreach { _.close() }\n+            } catch {\n+              case x: KafkaException =>\n+                logError(\"Error closing oldest Kafka consumer\", x)\n+            }\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   * The returned consumer must be released explicitly using [[KafkaDataConsumer.release()]].\n+   *\n+   * Note: This method guarantees that the consumer returned is not currently in use by anyone\n+   * else. Within this guarantee, this method will make a best effort attempt to re-use consumers by\n+   * caching them and tracking when they are in use.\n+   */\n+  def acquire[K, V](\n+      groupId: String,\n+      topicPartition: TopicPartition,\n+      kafkaParams: ju.Map[String, Object],\n+      context: TaskContext,\n+      useCache: Boolean): KafkaDataConsumer[K, V] = synchronized {\n+    val key = new CacheKey(groupId, topicPartition)\n+    val existingInternalConsumers = Option(cache.get(key))\n+      .getOrElse(new ju.LinkedList[InternalKafkaConsumer[_, _]])\n+\n+    cache.putIfAbsent(key, existingInternalConsumers)\n+\n+    lazy val newInternalConsumer = new InternalKafkaConsumer[K, V](\n+      groupId, topicPartition, kafkaParams)\n+\n+    if (context != null && context.attemptNumber >= 1) {\n+      // If this is reattempt at running the task, then invalidate cached consumers if any and\n+      // start with a new one.\n+      logDebug(\"Reattempt detected, invalidating cached consumers\")\n+      val closedExistingInternalConsumers = new ju.LinkedList[InternalKafkaConsumer[_, _]]()\n+      existingInternalConsumers.asScala.foreach { existingInternalConsumer =>\n+        // Consumer exists in cache. If its in use, mark it for closing later, or close it now.\n+        if (existingInternalConsumer.inUse) {\n+          existingInternalConsumer.markedForClose = true\n+        } else {\n+          existingInternalConsumer.close()\n+          closedExistingInternalConsumers.add(existingInternalConsumer)\n+        }\n+      }\n+      existingInternalConsumers.removeAll(closedExistingInternalConsumers)\n+\n+      logDebug(\"Reattempt detected, new cached consumer will be allocated \" +\n+        s\"$newInternalConsumer\")\n+      newInternalConsumer.inUse = true"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Ooooh, nice catch :) Removed them.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-12T13:19:00Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            try {\n+              entry.getValue.asScala.foreach { _.close() }\n+            } catch {\n+              case x: KafkaException =>\n+                logError(\"Error closing oldest Kafka consumer\", x)\n+            }\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   * The returned consumer must be released explicitly using [[KafkaDataConsumer.release()]].\n+   *\n+   * Note: This method guarantees that the consumer returned is not currently in use by anyone\n+   * else. Within this guarantee, this method will make a best effort attempt to re-use consumers by\n+   * caching them and tracking when they are in use.\n+   */\n+  def acquire[K, V](\n+      groupId: String,\n+      topicPartition: TopicPartition,\n+      kafkaParams: ju.Map[String, Object],\n+      context: TaskContext,\n+      useCache: Boolean): KafkaDataConsumer[K, V] = synchronized {\n+    val key = new CacheKey(groupId, topicPartition)\n+    val existingInternalConsumers = Option(cache.get(key))\n+      .getOrElse(new ju.LinkedList[InternalKafkaConsumer[_, _]])\n+\n+    cache.putIfAbsent(key, existingInternalConsumers)\n+\n+    lazy val newInternalConsumer = new InternalKafkaConsumer[K, V](\n+      groupId, topicPartition, kafkaParams)\n+\n+    if (context != null && context.attemptNumber >= 1) {\n+      // If this is reattempt at running the task, then invalidate cached consumers if any and\n+      // start with a new one.\n+      logDebug(\"Reattempt detected, invalidating cached consumers\")\n+      val closedExistingInternalConsumers = new ju.LinkedList[InternalKafkaConsumer[_, _]]()\n+      existingInternalConsumers.asScala.foreach { existingInternalConsumer =>\n+        // Consumer exists in cache. If its in use, mark it for closing later, or close it now.\n+        if (existingInternalConsumer.inUse) {\n+          existingInternalConsumer.markedForClose = true\n+        } else {\n+          existingInternalConsumer.close()\n+          closedExistingInternalConsumers.add(existingInternalConsumer)\n+        }\n+      }\n+      existingInternalConsumers.removeAll(closedExistingInternalConsumers)\n+\n+      logDebug(\"Reattempt detected, new cached consumer will be allocated \" +\n+        s\"$newInternalConsumer\")\n+      newInternalConsumer.inUse = true"
  }],
  "prId": 20997
}, {
  "comments": [{
    "author": {
      "login": "koeninger"
    },
    "body": "Is invalidating all consumers for a topicpartition on a re-attempt unnecessarily pessimistic?  Seems like if you have N consumers for the same topicpartition and they're all involved in a re-attempt, the last one to go through this section is going to invalidate all the fresh consumers the others just made.\r\n\r\nI think all you need is to make sure you get a fresh consumer on a re-attempt.\r\n",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-10T02:56:16Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            try {\n+              entry.getValue.asScala.foreach { _.close() }\n+            } catch {\n+              case x: KafkaException =>\n+                logError(\"Error closing oldest Kafka consumer\", x)\n+            }\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   * The returned consumer must be released explicitly using [[KafkaDataConsumer.release()]].\n+   *\n+   * Note: This method guarantees that the consumer returned is not currently in use by anyone\n+   * else. Within this guarantee, this method will make a best effort attempt to re-use consumers by\n+   * caching them and tracking when they are in use.\n+   */\n+  def acquire[K, V](\n+      groupId: String,\n+      topicPartition: TopicPartition,\n+      kafkaParams: ju.Map[String, Object],\n+      context: TaskContext,\n+      useCache: Boolean): KafkaDataConsumer[K, V] = synchronized {\n+    val key = new CacheKey(groupId, topicPartition)\n+    val existingInternalConsumers = Option(cache.get(key))\n+      .getOrElse(new ju.LinkedList[InternalKafkaConsumer[_, _]])\n+\n+    cache.putIfAbsent(key, existingInternalConsumers)\n+\n+    lazy val newInternalConsumer = new InternalKafkaConsumer[K, V](\n+      groupId, topicPartition, kafkaParams)\n+\n+    if (context != null && context.attemptNumber >= 1) {\n+      // If this is reattempt at running the task, then invalidate cached consumers if any and\n+      // start with a new one.\n+      logDebug(\"Reattempt detected, invalidating cached consumers\")\n+      val closedExistingInternalConsumers = new ju.LinkedList[InternalKafkaConsumer[_, _]]()\n+      existingInternalConsumers.asScala.foreach { existingInternalConsumer =>"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "I was thinking about this and maybe came to the wrong conclusion.\r\n\r\nI was thinking that executors are really long living in streaming. If that's true then a problematic consumer would stay there for long time poisoning all the further tasks running there. Giving back a new consumer purely lead to this situation. What have I missed?",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-12T13:40:06Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            try {\n+              entry.getValue.asScala.foreach { _.close() }\n+            } catch {\n+              case x: KafkaException =>\n+                logError(\"Error closing oldest Kafka consumer\", x)\n+            }\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   * The returned consumer must be released explicitly using [[KafkaDataConsumer.release()]].\n+   *\n+   * Note: This method guarantees that the consumer returned is not currently in use by anyone\n+   * else. Within this guarantee, this method will make a best effort attempt to re-use consumers by\n+   * caching them and tracking when they are in use.\n+   */\n+  def acquire[K, V](\n+      groupId: String,\n+      topicPartition: TopicPartition,\n+      kafkaParams: ju.Map[String, Object],\n+      context: TaskContext,\n+      useCache: Boolean): KafkaDataConsumer[K, V] = synchronized {\n+    val key = new CacheKey(groupId, topicPartition)\n+    val existingInternalConsumers = Option(cache.get(key))\n+      .getOrElse(new ju.LinkedList[InternalKafkaConsumer[_, _]])\n+\n+    cache.putIfAbsent(key, existingInternalConsumers)\n+\n+    lazy val newInternalConsumer = new InternalKafkaConsumer[K, V](\n+      groupId, topicPartition, kafkaParams)\n+\n+    if (context != null && context.attemptNumber >= 1) {\n+      // If this is reattempt at running the task, then invalidate cached consumers if any and\n+      // start with a new one.\n+      logDebug(\"Reattempt detected, invalidating cached consumers\")\n+      val closedExistingInternalConsumers = new ju.LinkedList[InternalKafkaConsumer[_, _]]()\n+      existingInternalConsumers.asScala.foreach { existingInternalConsumer =>"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "The problem isn't that you're invalidating all N consumers, it's potentially invalidating N + 1 + 2 + ... (N -1), right?\r\n\r\nI'm saying I think you can solve this by, if it's a retry, and you would normally grab 1 consumer from the cache, invalidate just that one and make a fresh one.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-13T20:49:01Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            try {\n+              entry.getValue.asScala.foreach { _.close() }\n+            } catch {\n+              case x: KafkaException =>\n+                logError(\"Error closing oldest Kafka consumer\", x)\n+            }\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   * The returned consumer must be released explicitly using [[KafkaDataConsumer.release()]].\n+   *\n+   * Note: This method guarantees that the consumer returned is not currently in use by anyone\n+   * else. Within this guarantee, this method will make a best effort attempt to re-use consumers by\n+   * caching them and tracking when they are in use.\n+   */\n+  def acquire[K, V](\n+      groupId: String,\n+      topicPartition: TopicPartition,\n+      kafkaParams: ju.Map[String, Object],\n+      context: TaskContext,\n+      useCache: Boolean): KafkaDataConsumer[K, V] = synchronized {\n+    val key = new CacheKey(groupId, topicPartition)\n+    val existingInternalConsumers = Option(cache.get(key))\n+      .getOrElse(new ju.LinkedList[InternalKafkaConsumer[_, _]])\n+\n+    cache.putIfAbsent(key, existingInternalConsumers)\n+\n+    lazy val newInternalConsumer = new InternalKafkaConsumer[K, V](\n+      groupId, topicPartition, kafkaParams)\n+\n+    if (context != null && context.attemptNumber >= 1) {\n+      // If this is reattempt at running the task, then invalidate cached consumers if any and\n+      // start with a new one.\n+      logDebug(\"Reattempt detected, invalidating cached consumers\")\n+      val closedExistingInternalConsumers = new ju.LinkedList[InternalKafkaConsumer[_, _]]()\n+      existingInternalConsumers.asScala.foreach { existingInternalConsumer =>"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Yeah, makes sense. That way the problematic consumers can be thrown away but a bit slower. Being so pessimistic how it's actually implemented ends up in slower execution overall.\r\n\r\nI've just seen your comment about having only one consumer in the cache just like in the SQL code. If we go that way this question will not be relevant.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-16T07:51:11Z",
    "diffHunk": "@@ -0,0 +1,381 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  protected def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010]\n+class InternalKafkaConsumer[K, V](\n+  val groupId: String,\n+  val topicPartition: TopicPartition,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  require(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  @volatile private var consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  @volatile var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  @volatile var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010]\n+object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse) // make sure this has been set to true\n+    override def release(): Unit = { KafkaDataConsumer.release(internalConsumer) }\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = { internalConsumer.close() }\n+  }\n+\n+  private case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.Map[CacheKey, ju.List[InternalKafkaConsumer[_, _]]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, ju.List[InternalKafkaConsumer[_, _]]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, ju.List[InternalKafkaConsumer[_, _]]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            try {\n+              entry.getValue.asScala.foreach { _.close() }\n+            } catch {\n+              case x: KafkaException =>\n+                logError(\"Error closing oldest Kafka consumer\", x)\n+            }\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   * The returned consumer must be released explicitly using [[KafkaDataConsumer.release()]].\n+   *\n+   * Note: This method guarantees that the consumer returned is not currently in use by anyone\n+   * else. Within this guarantee, this method will make a best effort attempt to re-use consumers by\n+   * caching them and tracking when they are in use.\n+   */\n+  def acquire[K, V](\n+      groupId: String,\n+      topicPartition: TopicPartition,\n+      kafkaParams: ju.Map[String, Object],\n+      context: TaskContext,\n+      useCache: Boolean): KafkaDataConsumer[K, V] = synchronized {\n+    val key = new CacheKey(groupId, topicPartition)\n+    val existingInternalConsumers = Option(cache.get(key))\n+      .getOrElse(new ju.LinkedList[InternalKafkaConsumer[_, _]])\n+\n+    cache.putIfAbsent(key, existingInternalConsumers)\n+\n+    lazy val newInternalConsumer = new InternalKafkaConsumer[K, V](\n+      groupId, topicPartition, kafkaParams)\n+\n+    if (context != null && context.attemptNumber >= 1) {\n+      // If this is reattempt at running the task, then invalidate cached consumers if any and\n+      // start with a new one.\n+      logDebug(\"Reattempt detected, invalidating cached consumers\")\n+      val closedExistingInternalConsumers = new ju.LinkedList[InternalKafkaConsumer[_, _]]()\n+      existingInternalConsumers.asScala.foreach { existingInternalConsumer =>"
  }],
  "prId": 20997
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: `private[kafka010] ` since class already has that.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-25T20:25:35Z",
    "diffHunk": "@@ -0,0 +1,359 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  private[kafka010] def internalConsumer: InternalKafkaConsumer[K, V]"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Removed.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-05-02T10:19:14Z",
    "diffHunk": "@@ -0,0 +1,359 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  private[kafka010] def internalConsumer: InternalKafkaConsumer[K, V]"
  }],
  "prId": 20997
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: `*/`",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-25T20:46:38Z",
    "diffHunk": "@@ -0,0 +1,359 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  private[kafka010] def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010] class InternalKafkaConsumer[K, V](\n+    val topicPartition: TopicPartition,\n+    val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  private[kafka010] val groupId = kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG)\n+    .asInstanceOf[String]\n+\n+  private val consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val topics = ju.Arrays.asList(topicPartition)\n+    c.assign(topics)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) {\n+      poll(timeout)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010] case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+private[kafka010] object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse)\n+    override def release(): Unit = KafkaDataConsumer.release(internalConsumer)\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = internalConsumer.close()\n+  }\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private[kafka010] var cache: ju.Map[CacheKey, InternalKafkaConsumer[_, _]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Fixed.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-05-02T10:20:04Z",
    "diffHunk": "@@ -0,0 +1,359 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  private[kafka010] def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010] class InternalKafkaConsumer[K, V](\n+    val topicPartition: TopicPartition,\n+    val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  private[kafka010] val groupId = kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG)\n+    .asInstanceOf[String]\n+\n+  private val consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val topics = ju.Arrays.asList(topicPartition)\n+    c.assign(topics)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) {\n+      poll(timeout)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010] case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+private[kafka010] object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse)\n+    override def release(): Unit = KafkaDataConsumer.release(internalConsumer)\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = internalConsumer.close()\n+  }\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private[kafka010] var cache: ju.Map[CacheKey, InternalKafkaConsumer[_, _]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */"
  }],
  "prId": 20997
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: `!entry.getValue.inUse`",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-25T20:47:47Z",
    "diffHunk": "@@ -0,0 +1,359 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  private[kafka010] def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010] class InternalKafkaConsumer[K, V](\n+    val topicPartition: TopicPartition,\n+    val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  private[kafka010] val groupId = kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG)\n+    .asInstanceOf[String]\n+\n+  private val consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val topics = ju.Arrays.asList(topicPartition)\n+    c.assign(topics)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) {\n+      poll(timeout)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010] case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+private[kafka010] object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse)\n+    override def release(): Unit = KafkaDataConsumer.release(internalConsumer)\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = internalConsumer.close()\n+  }\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private[kafka010] var cache: ju.Map[CacheKey, InternalKafkaConsumer[_, _]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, InternalKafkaConsumer[_, _]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+            entry: ju.Map.Entry[CacheKey, InternalKafkaConsumer[_, _]]): Boolean = {\n+\n+          // Try to remove the least-used entry if its currently not in use.\n+          //\n+          // If you cannot remove it, then the cache will keep growing. In the worst case,\n+          // the cache will grow to the max number of concurrent tasks that can run in the executor,\n+          // (that is, number of tasks slots) after which it will never reduce. This is unlikely to\n+          // be a serious problem because an executor with more than 64 (default) tasks slots is\n+          // likely running on a beefy machine that can handle a large number of simultaneously\n+          // active consumers.\n+\n+          if (entry.getValue.inUse == false && this.size > maxCapacity) {",
    "line": 250
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Fixed.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-05-02T10:20:19Z",
    "diffHunk": "@@ -0,0 +1,359 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  private[kafka010] def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010] class InternalKafkaConsumer[K, V](\n+    val topicPartition: TopicPartition,\n+    val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  private[kafka010] val groupId = kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG)\n+    .asInstanceOf[String]\n+\n+  private val consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val topics = ju.Arrays.asList(topicPartition)\n+    c.assign(topics)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) {\n+      poll(timeout)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010] case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+private[kafka010] object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse)\n+    override def release(): Unit = KafkaDataConsumer.release(internalConsumer)\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = internalConsumer.close()\n+  }\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private[kafka010] var cache: ju.Map[CacheKey, InternalKafkaConsumer[_, _]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, InternalKafkaConsumer[_, _]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+            entry: ju.Map.Entry[CacheKey, InternalKafkaConsumer[_, _]]): Boolean = {\n+\n+          // Try to remove the least-used entry if its currently not in use.\n+          //\n+          // If you cannot remove it, then the cache will keep growing. In the worst case,\n+          // the cache will grow to the max number of concurrent tasks that can run in the executor,\n+          // (that is, number of tasks slots) after which it will never reduce. This is unlikely to\n+          // be a serious problem because an executor with more than 64 (default) tasks slots is\n+          // likely running on a beefy machine that can handle a large number of simultaneously\n+          // active consumers.\n+\n+          if (entry.getValue.inUse == false && this.size > maxCapacity) {",
    "line": 250
  }],
  "prId": 20997
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: indent",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-25T20:48:03Z",
    "diffHunk": "@@ -0,0 +1,359 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  private[kafka010] def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010] class InternalKafkaConsumer[K, V](\n+    val topicPartition: TopicPartition,\n+    val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  private[kafka010] val groupId = kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG)\n+    .asInstanceOf[String]\n+\n+  private val consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val topics = ju.Arrays.asList(topicPartition)\n+    c.assign(topics)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) {\n+      poll(timeout)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010] case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+private[kafka010] object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse)\n+    override def release(): Unit = KafkaDataConsumer.release(internalConsumer)\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = internalConsumer.close()\n+  }\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private[kafka010] var cache: ju.Map[CacheKey, InternalKafkaConsumer[_, _]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, InternalKafkaConsumer[_, _]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+            entry: ju.Map.Entry[CacheKey, InternalKafkaConsumer[_, _]]): Boolean = {\n+\n+          // Try to remove the least-used entry if its currently not in use.\n+          //\n+          // If you cannot remove it, then the cache will keep growing. In the worst case,\n+          // the cache will grow to the max number of concurrent tasks that can run in the executor,\n+          // (that is, number of tasks slots) after which it will never reduce. This is unlikely to\n+          // be a serious problem because an executor with more than 64 (default) tasks slots is\n+          // likely running on a beefy machine that can handle a large number of simultaneously\n+          // active consumers.\n+\n+          if (entry.getValue.inUse == false && this.size > maxCapacity) {\n+            logWarning(\n+              s\"KafkaConsumer cache hitting max capacity of $maxCapacity, \" +\n+                s\"removing consumer for ${entry.getKey}\")\n+               try {"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Fixed.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-05-02T10:22:49Z",
    "diffHunk": "@@ -0,0 +1,359 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  private[kafka010] def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010] class InternalKafkaConsumer[K, V](\n+    val topicPartition: TopicPartition,\n+    val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  private[kafka010] val groupId = kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG)\n+    .asInstanceOf[String]\n+\n+  private val consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val topics = ju.Arrays.asList(topicPartition)\n+    c.assign(topics)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) {\n+      poll(timeout)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010] case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+private[kafka010] object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse)\n+    override def release(): Unit = KafkaDataConsumer.release(internalConsumer)\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = internalConsumer.close()\n+  }\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private[kafka010] var cache: ju.Map[CacheKey, InternalKafkaConsumer[_, _]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, InternalKafkaConsumer[_, _]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+            entry: ju.Map.Entry[CacheKey, InternalKafkaConsumer[_, _]]): Boolean = {\n+\n+          // Try to remove the least-used entry if its currently not in use.\n+          //\n+          // If you cannot remove it, then the cache will keep growing. In the worst case,\n+          // the cache will grow to the max number of concurrent tasks that can run in the executor,\n+          // (that is, number of tasks slots) after which it will never reduce. This is unlikely to\n+          // be a serious problem because an executor with more than 64 (default) tasks slots is\n+          // likely running on a beefy machine that can handle a large number of simultaneously\n+          // active consumers.\n+\n+          if (entry.getValue.inUse == false && this.size > maxCapacity) {\n+            logWarning(\n+              s\"KafkaConsumer cache hitting max capacity of $maxCapacity, \" +\n+                s\"removing consumer for ${entry.getKey}\")\n+               try {"
  }],
  "prId": 20997
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "After reading this code and also the `acquire` method, is there a useful difference between the `CachedKafkaDataConsumer` and `NonCachedKafkaDataConsumer` types?\r\n\r\nIt seems like the code doesn't really care about those types, but just about whether the consumer is in the cache?",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-25T20:56:43Z",
    "diffHunk": "@@ -0,0 +1,359 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  private[kafka010] def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010] class InternalKafkaConsumer[K, V](\n+    val topicPartition: TopicPartition,\n+    val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  private[kafka010] val groupId = kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG)\n+    .asInstanceOf[String]\n+\n+  private val consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val topics = ju.Arrays.asList(topicPartition)\n+    c.assign(topics)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) {\n+      poll(timeout)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010] case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+private[kafka010] object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse)\n+    override def release(): Unit = KafkaDataConsumer.release(internalConsumer)\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = internalConsumer.close()\n+  }\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private[kafka010] var cache: ju.Map[CacheKey, InternalKafkaConsumer[_, _]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, InternalKafkaConsumer[_, _]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+            entry: ju.Map.Entry[CacheKey, InternalKafkaConsumer[_, _]]): Boolean = {\n+\n+          // Try to remove the least-used entry if its currently not in use.\n+          //\n+          // If you cannot remove it, then the cache will keep growing. In the worst case,\n+          // the cache will grow to the max number of concurrent tasks that can run in the executor,\n+          // (that is, number of tasks slots) after which it will never reduce. This is unlikely to\n+          // be a serious problem because an executor with more than 64 (default) tasks slots is\n+          // likely running on a beefy machine that can handle a large number of simultaneously\n+          // active consumers.\n+\n+          if (entry.getValue.inUse == false && this.size > maxCapacity) {\n+            logWarning(\n+              s\"KafkaConsumer cache hitting max capacity of $maxCapacity, \" +\n+                s\"removing consumer for ${entry.getKey}\")\n+               try {\n+              entry.getValue.close()\n+            } catch {\n+              case x: KafkaException =>\n+                logError(\"Error closing oldest Kafka consumer\", x)\n+            }\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   * The returned consumer must be released explicitly using [[KafkaDataConsumer.release()]].\n+   *\n+   * Note: This method guarantees that the consumer returned is not currently in use by anyone\n+   * else. Within this guarantee, this method will make a best effort attempt to re-use consumers by\n+   * caching them and tracking when they are in use.\n+   */\n+  def acquire[K, V](\n+      topicPartition: TopicPartition,\n+      kafkaParams: ju.Map[String, Object],\n+      context: TaskContext,\n+      useCache: Boolean): KafkaDataConsumer[K, V] = synchronized {\n+    val groupId = kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG).asInstanceOf[String]\n+    val key = new CacheKey(groupId, topicPartition)\n+    val existingInternalConsumer = cache.get(key)\n+\n+    lazy val newInternalConsumer = new InternalKafkaConsumer[K, V](topicPartition, kafkaParams)\n+\n+    if (context != null && context.attemptNumber >= 1) {\n+      // If this is reattempt at running the task, then invalidate cached consumers if any and\n+      // start with a new one. If prior attempt failures were cache related then this way old\n+      // problematic consumers can be removed.\n+      logDebug(s\"Reattempt detected, invalidating cached consumer $existingInternalConsumer\")\n+      if (existingInternalConsumer != null) {\n+        // Consumer exists in cache. If its in use, mark it for closing later, or close it now.\n+        if (existingInternalConsumer.inUse) {\n+          existingInternalConsumer.markedForClose = true\n+        } else {\n+          existingInternalConsumer.close()\n+          // Remove the consumer from cache only if it's closed.\n+          // Marked for close consumers will be removed in release function.\n+          cache.remove(key)\n+        }\n+      }\n+\n+      logDebug(\"Reattempt detected, new non-cached consumer will be allocated \" +\n+        s\"$newInternalConsumer\")\n+      NonCachedKafkaDataConsumer(newInternalConsumer)\n+    } else if (!useCache) {\n+      // If consumer reuse turned off, then do not use it, return a new consumer\n+      logDebug(\"Cache usage turned off, new non-cached consumer will be allocated \" +\n+        s\"$newInternalConsumer\")\n+      NonCachedKafkaDataConsumer(newInternalConsumer)\n+    } else if (existingInternalConsumer == null) {\n+      // If consumer is not already cached, then put a new in the cache and return it\n+      logDebug(\"No cached consumer, new cached consumer will be allocated \" +\n+        s\"$newInternalConsumer\")\n+      cache.put(key, newInternalConsumer)\n+      CachedKafkaDataConsumer(newInternalConsumer)\n+    } else if (existingInternalConsumer.inUse) {\n+      // If consumer is already cached but is currently in use, then return a new consumer\n+      logDebug(\"Used cached consumer found, new non-cached consumer will be allocated \" +\n+        s\"$newInternalConsumer\")\n+      NonCachedKafkaDataConsumer(newInternalConsumer)\n+    } else {\n+      // If consumer is already cached and is currently not in use, then return that consumer\n+      logDebug(s\"Not used cached consumer found, re-using it $existingInternalConsumer\")\n+      existingInternalConsumer.inUse = true\n+      // Any given TopicPartition should have a consistent key and value type\n+      CachedKafkaDataConsumer(existingInternalConsumer.asInstanceOf[InternalKafkaConsumer[K, V]])\n+    }\n+  }\n+\n+  private def release(internalConsumer: InternalKafkaConsumer[_, _]): Unit = synchronized {",
    "line": 334
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "I think that's a good observation.  But I'm not sure it's worth deviating from the same design being used in the SQL code.",
    "commit": "6cd67c6ac7b948eb791cc4871477ab0b1df4fcad",
    "createdAt": "2018-04-25T21:25:31Z",
    "diffHunk": "@@ -0,0 +1,359 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka010\n+\n+import java.{util => ju}\n+\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord, KafkaConsumer}\n+import org.apache.kafka.common.{KafkaException, TopicPartition}\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.internal.Logging\n+\n+private[kafka010] sealed trait KafkaDataConsumer[K, V] {\n+  /**\n+   * Get the record for the given offset if available.\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def get(offset: Long, pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.get(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   *\n+   * @param offset         the offset to fetch.\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    internalConsumer.compactedStart(offset, pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   *\n+   * @param pollTimeoutMs  timeout in milliseconds to poll data from Kafka.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedNext(pollTimeoutMs)\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   *\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    internalConsumer.compactedPrevious()\n+  }\n+\n+  /**\n+   * Release this consumer from being further used. Depending on its implementation,\n+   * this consumer will be either finalized, or reset for reuse later.\n+   */\n+  def release(): Unit\n+\n+  /** Reference to the internal implementation that this wrapper delegates to */\n+  private[kafka010] def internalConsumer: InternalKafkaConsumer[K, V]\n+}\n+\n+\n+/**\n+ * A wrapper around Kafka's KafkaConsumer.\n+ * This is not for direct use outside this file.\n+ */\n+private[kafka010] class InternalKafkaConsumer[K, V](\n+    val topicPartition: TopicPartition,\n+    val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  private[kafka010] val groupId = kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG)\n+    .asInstanceOf[String]\n+\n+  private val consumer = createConsumer\n+\n+  /** indicates whether this consumer is in use or not */\n+  var inUse = true\n+\n+  /** indicate whether this consumer is going to be stopped in the next release */\n+  var markedForClose = false\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  @volatile private var buffer = ju.Collections.emptyListIterator[ConsumerRecord[K, V]]()\n+  @volatile private var nextOffset = InternalKafkaConsumer.UNKNOWN_OFFSET\n+\n+  override def toString: String = {\n+    \"InternalKafkaConsumer(\" +\n+      s\"hash=${Integer.toHexString(hashCode)}, \" +\n+      s\"groupId=$groupId, \" +\n+      s\"topicPartition=$topicPartition)\"\n+  }\n+\n+  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n+  private def createConsumer: KafkaConsumer[K, V] = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val topics = ju.Arrays.asList(topicPartition)\n+    c.assign(topics)\n+    c\n+  }\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    logDebug(s\"Get $groupId $topicPartition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) {\n+      poll(timeout)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      logInfo(s\"Buffer miss for $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      require(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topicPartition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      require(record.offset == offset,\n+        s\"Got wrong record for $groupId $topicPartition even after seeking to offset $offset \" +\n+          s\"got offset ${record.offset} instead. If this is a compacted topic, consider enabling \" +\n+          \"spark.streaming.kafka.allowNonConsecutiveOffsets\"\n+      )\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Start a batch on a compacted topic\n+   */\n+  def compactedStart(offset: Long, pollTimeoutMs: Long): Unit = {\n+    logDebug(s\"compacted start $groupId $topicPartition starting $offset\")\n+    // This seek may not be necessary, but it's hard to tell due to gaps in compacted topics\n+    if (offset != nextOffset) {\n+      logInfo(s\"Initial fetch for compacted $groupId $topicPartition $offset\")\n+      seek(offset)\n+      poll(pollTimeoutMs)\n+    }\n+  }\n+\n+  /**\n+   * Get the next record in the batch from a compacted topic.\n+   * Assumes compactedStart has been called first, and ignores gaps.\n+   */\n+  def compactedNext(pollTimeoutMs: Long): ConsumerRecord[K, V] = {\n+    if (!buffer.hasNext()) {\n+      poll(pollTimeoutMs)\n+    }\n+    require(buffer.hasNext(),\n+      s\"Failed to get records for compacted $groupId $topicPartition \" +\n+        s\"after polling for $pollTimeoutMs\")\n+    val record = buffer.next()\n+    nextOffset = record.offset + 1\n+    record\n+  }\n+\n+  /**\n+   * Rewind to previous record in the batch from a compacted topic.\n+   * @throws NoSuchElementException if no previous element\n+   */\n+  def compactedPrevious(): ConsumerRecord[K, V] = {\n+    buffer.previous()\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    logDebug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    logDebug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.listIterator\n+  }\n+\n+}\n+\n+private[kafka010] case class CacheKey(groupId: String, topicPartition: TopicPartition)\n+\n+private[kafka010] object KafkaDataConsumer extends Logging {\n+\n+  private case class CachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    assert(internalConsumer.inUse)\n+    override def release(): Unit = KafkaDataConsumer.release(internalConsumer)\n+  }\n+\n+  private case class NonCachedKafkaDataConsumer[K, V](internalConsumer: InternalKafkaConsumer[K, V])\n+    extends KafkaDataConsumer[K, V] {\n+    override def release(): Unit = internalConsumer.close()\n+  }\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private[kafka010] var cache: ju.Map[CacheKey, InternalKafkaConsumer[_, _]] = null\n+\n+  /**\n+   * Must be called before acquire, once per JVM, to configure the cache.\n+   * Further calls are ignored.\n+   * */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = synchronized {\n+    if (null == cache) {\n+      logInfo(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, InternalKafkaConsumer[_, _]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+            entry: ju.Map.Entry[CacheKey, InternalKafkaConsumer[_, _]]): Boolean = {\n+\n+          // Try to remove the least-used entry if its currently not in use.\n+          //\n+          // If you cannot remove it, then the cache will keep growing. In the worst case,\n+          // the cache will grow to the max number of concurrent tasks that can run in the executor,\n+          // (that is, number of tasks slots) after which it will never reduce. This is unlikely to\n+          // be a serious problem because an executor with more than 64 (default) tasks slots is\n+          // likely running on a beefy machine that can handle a large number of simultaneously\n+          // active consumers.\n+\n+          if (entry.getValue.inUse == false && this.size > maxCapacity) {\n+            logWarning(\n+              s\"KafkaConsumer cache hitting max capacity of $maxCapacity, \" +\n+                s\"removing consumer for ${entry.getKey}\")\n+               try {\n+              entry.getValue.close()\n+            } catch {\n+              case x: KafkaException =>\n+                logError(\"Error closing oldest Kafka consumer\", x)\n+            }\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   * The returned consumer must be released explicitly using [[KafkaDataConsumer.release()]].\n+   *\n+   * Note: This method guarantees that the consumer returned is not currently in use by anyone\n+   * else. Within this guarantee, this method will make a best effort attempt to re-use consumers by\n+   * caching them and tracking when they are in use.\n+   */\n+  def acquire[K, V](\n+      topicPartition: TopicPartition,\n+      kafkaParams: ju.Map[String, Object],\n+      context: TaskContext,\n+      useCache: Boolean): KafkaDataConsumer[K, V] = synchronized {\n+    val groupId = kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG).asInstanceOf[String]\n+    val key = new CacheKey(groupId, topicPartition)\n+    val existingInternalConsumer = cache.get(key)\n+\n+    lazy val newInternalConsumer = new InternalKafkaConsumer[K, V](topicPartition, kafkaParams)\n+\n+    if (context != null && context.attemptNumber >= 1) {\n+      // If this is reattempt at running the task, then invalidate cached consumers if any and\n+      // start with a new one. If prior attempt failures were cache related then this way old\n+      // problematic consumers can be removed.\n+      logDebug(s\"Reattempt detected, invalidating cached consumer $existingInternalConsumer\")\n+      if (existingInternalConsumer != null) {\n+        // Consumer exists in cache. If its in use, mark it for closing later, or close it now.\n+        if (existingInternalConsumer.inUse) {\n+          existingInternalConsumer.markedForClose = true\n+        } else {\n+          existingInternalConsumer.close()\n+          // Remove the consumer from cache only if it's closed.\n+          // Marked for close consumers will be removed in release function.\n+          cache.remove(key)\n+        }\n+      }\n+\n+      logDebug(\"Reattempt detected, new non-cached consumer will be allocated \" +\n+        s\"$newInternalConsumer\")\n+      NonCachedKafkaDataConsumer(newInternalConsumer)\n+    } else if (!useCache) {\n+      // If consumer reuse turned off, then do not use it, return a new consumer\n+      logDebug(\"Cache usage turned off, new non-cached consumer will be allocated \" +\n+        s\"$newInternalConsumer\")\n+      NonCachedKafkaDataConsumer(newInternalConsumer)\n+    } else if (existingInternalConsumer == null) {\n+      // If consumer is not already cached, then put a new in the cache and return it\n+      logDebug(\"No cached consumer, new cached consumer will be allocated \" +\n+        s\"$newInternalConsumer\")\n+      cache.put(key, newInternalConsumer)\n+      CachedKafkaDataConsumer(newInternalConsumer)\n+    } else if (existingInternalConsumer.inUse) {\n+      // If consumer is already cached but is currently in use, then return a new consumer\n+      logDebug(\"Used cached consumer found, new non-cached consumer will be allocated \" +\n+        s\"$newInternalConsumer\")\n+      NonCachedKafkaDataConsumer(newInternalConsumer)\n+    } else {\n+      // If consumer is already cached and is currently not in use, then return that consumer\n+      logDebug(s\"Not used cached consumer found, re-using it $existingInternalConsumer\")\n+      existingInternalConsumer.inUse = true\n+      // Any given TopicPartition should have a consistent key and value type\n+      CachedKafkaDataConsumer(existingInternalConsumer.asInstanceOf[InternalKafkaConsumer[K, V]])\n+    }\n+  }\n+\n+  private def release(internalConsumer: InternalKafkaConsumer[_, _]): Unit = synchronized {",
    "line": 334
  }],
  "prId": 20997
}]