[{
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Can we use the existing config in core for this?",
    "commit": "064dab5b796847ca8b203ccb6505c7288632ec56",
    "createdAt": "2019-04-02T13:57:01Z",
    "diffHunk": "@@ -64,16 +64,12 @@ private[spark] class KafkaRDD[K, V](\n       \" must be set to false for executor kafka params, else offsets may commit before processing\")\n \n   // TODO is it necessary to have separate configs for initial poll time vs ongoing poll time?\n-  private val pollTimeout = conf.getLong(\"spark.streaming.kafka.consumer.poll.ms\",\n+  private val pollTimeout = conf.get(CONSUMER_POLL_MS).getOrElse(\n     conf.getTimeAsSeconds(\"spark.network.timeout\", \"120s\") * 1000L)"
  }, {
    "author": {
      "login": "beliefer"
    },
    "body": "> Can we use the existing config in core for this?\r\n\r\nWhat do you mean by replacing `spark.network.timeout` with existing config in core?  I checked the code in core have config `spark.network.timeout` not yet.",
    "commit": "064dab5b796847ca8b203ccb6505c7288632ec56",
    "createdAt": "2019-04-03T01:36:04Z",
    "diffHunk": "@@ -64,16 +64,12 @@ private[spark] class KafkaRDD[K, V](\n       \" must be set to false for executor kafka params, else offsets may commit before processing\")\n \n   // TODO is it necessary to have separate configs for initial poll time vs ongoing poll time?\n-  private val pollTimeout = conf.getLong(\"spark.streaming.kafka.consumer.poll.ms\",\n+  private val pollTimeout = conf.get(CONSUMER_POLL_MS).getOrElse(\n     conf.getTimeAsSeconds(\"spark.network.timeout\", \"120s\") * 1000L)"
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "See `.../internal/config/Network.scala` in core",
    "commit": "064dab5b796847ca8b203ccb6505c7288632ec56",
    "createdAt": "2019-04-03T14:14:19Z",
    "diffHunk": "@@ -64,16 +64,12 @@ private[spark] class KafkaRDD[K, V](\n       \" must be set to false for executor kafka params, else offsets may commit before processing\")\n \n   // TODO is it necessary to have separate configs for initial poll time vs ongoing poll time?\n-  private val pollTimeout = conf.getLong(\"spark.streaming.kafka.consumer.poll.ms\",\n+  private val pollTimeout = conf.get(CONSUMER_POLL_MS).getOrElse(\n     conf.getTimeAsSeconds(\"spark.network.timeout\", \"120s\") * 1000L)"
  }, {
    "author": {
      "login": "beliefer"
    },
    "body": "> See `.../internal/config/Network.scala` in core\r\n\r\nOK. I didn't notice this class.",
    "commit": "064dab5b796847ca8b203ccb6505c7288632ec56",
    "createdAt": "2019-04-04T01:41:30Z",
    "diffHunk": "@@ -64,16 +64,12 @@ private[spark] class KafkaRDD[K, V](\n       \" must be set to false for executor kafka params, else offsets may commit before processing\")\n \n   // TODO is it necessary to have separate configs for initial poll time vs ongoing poll time?\n-  private val pollTimeout = conf.getLong(\"spark.streaming.kafka.consumer.poll.ms\",\n+  private val pollTimeout = conf.get(CONSUMER_POLL_MS).getOrElse(\n     conf.getTimeAsSeconds(\"spark.network.timeout\", \"120s\") * 1000L)"
  }, {
    "author": {
      "login": "beliefer"
    },
    "body": "> See `.../internal/config/Network.scala` in core\r\n\r\nThanks, I have replace this config.",
    "commit": "064dab5b796847ca8b203ccb6505c7288632ec56",
    "createdAt": "2019-04-04T02:21:10Z",
    "diffHunk": "@@ -64,16 +64,12 @@ private[spark] class KafkaRDD[K, V](\n       \" must be set to false for executor kafka params, else offsets may commit before processing\")\n \n   // TODO is it necessary to have separate configs for initial poll time vs ongoing poll time?\n-  private val pollTimeout = conf.getLong(\"spark.streaming.kafka.consumer.poll.ms\",\n+  private val pollTimeout = conf.get(CONSUMER_POLL_MS).getOrElse(\n     conf.getTimeAsSeconds(\"spark.network.timeout\", \"120s\") * 1000L)"
  }],
  "prId": 24267
}]