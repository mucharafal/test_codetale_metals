[{
  "comments": [{
    "author": {
      "login": "lfrancke"
    },
    "body": "Calling .size requires an implicit conversion to SeqLike. `.length` is the native Array method\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-21T08:28:35Z",
    "diffHunk": "@@ -0,0 +1,309 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD\n+ * @param preferredHosts map from TopicPartition to preferred host for processing that partition.\n+ * In most cases, use [[DirectKafkaInputDStream.preferConsistent]]\n+ * Use [[DirectKafkaInputDStream.preferBrokers]] if your executors are on same nodes as brokers.\n+ * @param useConsumerCache whether to use a consumer from a per-jvm cache\n+ * @tparam K type of Kafka message key\n+ * @tparam V type of Kafka message value\n+ */\n+@Experimental\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag] private[spark] (\n+    sc: SparkContext,\n+    val kafkaParams: ju.Map[String, Object],\n+    val offsetRanges: Array[OffsetRange],\n+    val preferredHosts: ju.Map[TopicPartition, String],\n+    useConsumerCache: Boolean\n+) extends RDD[ConsumerRecord[K, V]](sc, Nil) with Logging with HasOffsetRanges {\n+\n+  assert(\"none\" ==\n+    kafkaParams.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).asInstanceOf[String],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to none for executor kafka params, else messages may not match offsetRange\")\n+\n+  assert(false ==\n+    kafkaParams.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG).asInstanceOf[Boolean],\n+    ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG +\n+      \" must be set to false for executor kafka params, else offsets may commit before processing\")\n+\n+  // TODO is it necessary to have separate configs for initial poll time vs ongoing poll time?\n+  private val pollTimeout = conf.getLong(\"spark.streaming.kafka.consumer.poll.ms\", 256)\n+  private val cacheInitialCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.initialCapacity\", 16)\n+  private val cacheMaxCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.maxCapacity\", 64)\n+  private val cacheLoadFactor =\n+    conf.getDouble(\"spark.streaming.kafka.consumer.cache.loadFactor\", 0.75).toFloat\n+\n+  override def persist(newLevel: StorageLevel): this.type = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  override def getPartitions: Array[Partition] = {\n+    offsetRanges.zipWithIndex.map { case (o, i) =>\n+        new KafkaRDDPartition(i, o.topic, o.partition, o.fromOffset, o.untilOffset)\n+    }.toArray\n+  }\n+\n+  override def count(): Long = offsetRanges.map(_.count).sum\n+\n+  override def countApprox(\n+      timeout: Long,\n+      confidence: Double = 0.95\n+  ): PartialResult[BoundedDouble] = {\n+    val c = count\n+    new PartialResult(new BoundedDouble(c, 1.0, c, c), true)\n+  }\n+\n+  override def isEmpty(): Boolean = count == 0L\n+\n+  override def take(num: Int): Array[ConsumerRecord[K, V]] = {\n+    val nonEmptyPartitions = this.partitions\n+      .map(_.asInstanceOf[KafkaRDDPartition])\n+      .filter(_.count > 0)\n+\n+    if (num < 1 || nonEmptyPartitions.size < 1) {"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "Yeah, this is possibly an extra object allocation, but it's not like it's in an inner loop.\n\nAgain, this code is also unchanged from 0.8 version. Maybe a separate PR for any cleanups?\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-21T14:21:09Z",
    "diffHunk": "@@ -0,0 +1,309 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD\n+ * @param preferredHosts map from TopicPartition to preferred host for processing that partition.\n+ * In most cases, use [[DirectKafkaInputDStream.preferConsistent]]\n+ * Use [[DirectKafkaInputDStream.preferBrokers]] if your executors are on same nodes as brokers.\n+ * @param useConsumerCache whether to use a consumer from a per-jvm cache\n+ * @tparam K type of Kafka message key\n+ * @tparam V type of Kafka message value\n+ */\n+@Experimental\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag] private[spark] (\n+    sc: SparkContext,\n+    val kafkaParams: ju.Map[String, Object],\n+    val offsetRanges: Array[OffsetRange],\n+    val preferredHosts: ju.Map[TopicPartition, String],\n+    useConsumerCache: Boolean\n+) extends RDD[ConsumerRecord[K, V]](sc, Nil) with Logging with HasOffsetRanges {\n+\n+  assert(\"none\" ==\n+    kafkaParams.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).asInstanceOf[String],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to none for executor kafka params, else messages may not match offsetRange\")\n+\n+  assert(false ==\n+    kafkaParams.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG).asInstanceOf[Boolean],\n+    ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG +\n+      \" must be set to false for executor kafka params, else offsets may commit before processing\")\n+\n+  // TODO is it necessary to have separate configs for initial poll time vs ongoing poll time?\n+  private val pollTimeout = conf.getLong(\"spark.streaming.kafka.consumer.poll.ms\", 256)\n+  private val cacheInitialCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.initialCapacity\", 16)\n+  private val cacheMaxCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.maxCapacity\", 64)\n+  private val cacheLoadFactor =\n+    conf.getDouble(\"spark.streaming.kafka.consumer.cache.loadFactor\", 0.75).toFloat\n+\n+  override def persist(newLevel: StorageLevel): this.type = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  override def getPartitions: Array[Partition] = {\n+    offsetRanges.zipWithIndex.map { case (o, i) =>\n+        new KafkaRDDPartition(i, o.topic, o.partition, o.fromOffset, o.untilOffset)\n+    }.toArray\n+  }\n+\n+  override def count(): Long = offsetRanges.map(_.count).sum\n+\n+  override def countApprox(\n+      timeout: Long,\n+      confidence: Double = 0.95\n+  ): PartialResult[BoundedDouble] = {\n+    val c = count\n+    new PartialResult(new BoundedDouble(c, 1.0, c, c), true)\n+  }\n+\n+  override def isEmpty(): Boolean = count == 0L\n+\n+  override def take(num: Int): Array[ConsumerRecord[K, V]] = {\n+    val nonEmptyPartitions = this.partitions\n+      .map(_.asInstanceOf[KafkaRDDPartition])\n+      .filter(_.count > 0)\n+\n+    if (num < 1 || nonEmptyPartitions.size < 1) {"
  }],
  "prId": 11863
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: doc style comments. also this is used in only in one location, why make a separate function.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-22T22:44:37Z",
    "diffHunk": "@@ -0,0 +1,309 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD\n+ * @param preferredHosts map from TopicPartition to preferred host for processing that partition.\n+ * In most cases, use [[DirectKafkaInputDStream.preferConsistent]]\n+ * Use [[DirectKafkaInputDStream.preferBrokers]] if your executors are on same nodes as brokers.\n+ * @param useConsumerCache whether to use a consumer from a per-jvm cache\n+ * @tparam K type of Kafka message key\n+ * @tparam V type of Kafka message value\n+ */\n+@Experimental\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag] private[spark] (\n+    sc: SparkContext,\n+    val kafkaParams: ju.Map[String, Object],\n+    val offsetRanges: Array[OffsetRange],\n+    val preferredHosts: ju.Map[TopicPartition, String],\n+    useConsumerCache: Boolean\n+) extends RDD[ConsumerRecord[K, V]](sc, Nil) with Logging with HasOffsetRanges {\n+\n+  assert(\"none\" ==\n+    kafkaParams.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).asInstanceOf[String],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to none for executor kafka params, else messages may not match offsetRange\")\n+\n+  assert(false ==\n+    kafkaParams.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG).asInstanceOf[Boolean],\n+    ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG +\n+      \" must be set to false for executor kafka params, else offsets may commit before processing\")\n+\n+  // TODO is it necessary to have separate configs for initial poll time vs ongoing poll time?\n+  private val pollTimeout = conf.getLong(\"spark.streaming.kafka.consumer.poll.ms\", 256)\n+  private val cacheInitialCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.initialCapacity\", 16)\n+  private val cacheMaxCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.maxCapacity\", 64)\n+  private val cacheLoadFactor =\n+    conf.getDouble(\"spark.streaming.kafka.consumer.cache.loadFactor\", 0.75).toFloat\n+\n+  override def persist(newLevel: StorageLevel): this.type = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  override def getPartitions: Array[Partition] = {\n+    offsetRanges.zipWithIndex.map { case (o, i) =>\n+        new KafkaRDDPartition(i, o.topic, o.partition, o.fromOffset, o.untilOffset)\n+    }.toArray\n+  }\n+\n+  override def count(): Long = offsetRanges.map(_.count).sum\n+\n+  override def countApprox(\n+      timeout: Long,\n+      confidence: Double = 0.95\n+  ): PartialResult[BoundedDouble] = {\n+    val c = count\n+    new PartialResult(new BoundedDouble(c, 1.0, c, c), true)\n+  }\n+\n+  override def isEmpty(): Boolean = count == 0L\n+\n+  override def take(num: Int): Array[ConsumerRecord[K, V]] = {\n+    val nonEmptyPartitions = this.partitions\n+      .map(_.asInstanceOf[KafkaRDDPartition])\n+      .filter(_.count > 0)\n+\n+    if (num < 1 || nonEmptyPartitions.size < 1) {\n+      return new Array[ConsumerRecord[K, V]](0)\n+    }\n+\n+    // Determine in advance how many messages need to be taken from each partition\n+    val parts = nonEmptyPartitions.foldLeft(Map[Int, Int]()) { (result, part) =>\n+      val remain = num - result.values.sum\n+      if (remain > 0) {\n+        val taken = Math.min(remain, part.count)\n+        result + (part.index -> taken.toInt)\n+      } else {\n+        result\n+      }\n+    }\n+\n+    val buf = new ArrayBuffer[ConsumerRecord[K, V]]\n+    val res = context.runJob(\n+      this,\n+      (tc: TaskContext, it: Iterator[ConsumerRecord[K, V]]) =>\n+      it.take(parts(tc.partitionId)).toArray, parts.keys.toArray\n+    )\n+    res.foreach(buf ++= _)\n+    buf.toArray\n+  }\n+\n+  private def executors(): Array[ExecutorCacheTaskLocation] = {\n+    val bm = sparkContext.env.blockManager\n+    bm.master.getPeers(bm.blockManagerId).toArray\n+      .map(x => ExecutorCacheTaskLocation(x.host, x.executorId))\n+      .sortWith((a, b) => a.host > b.host || a.executorId > b.executorId)\n+  }\n+\n+  // non-negative modulus, from java 8 math\n+  private def floorMod(a: Int, b: Int): Int = ((a % b) + b) % b"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "to make it clear what it's doing and why\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-23T00:52:50Z",
    "diffHunk": "@@ -0,0 +1,309 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD\n+ * @param preferredHosts map from TopicPartition to preferred host for processing that partition.\n+ * In most cases, use [[DirectKafkaInputDStream.preferConsistent]]\n+ * Use [[DirectKafkaInputDStream.preferBrokers]] if your executors are on same nodes as brokers.\n+ * @param useConsumerCache whether to use a consumer from a per-jvm cache\n+ * @tparam K type of Kafka message key\n+ * @tparam V type of Kafka message value\n+ */\n+@Experimental\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag] private[spark] (\n+    sc: SparkContext,\n+    val kafkaParams: ju.Map[String, Object],\n+    val offsetRanges: Array[OffsetRange],\n+    val preferredHosts: ju.Map[TopicPartition, String],\n+    useConsumerCache: Boolean\n+) extends RDD[ConsumerRecord[K, V]](sc, Nil) with Logging with HasOffsetRanges {\n+\n+  assert(\"none\" ==\n+    kafkaParams.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).asInstanceOf[String],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to none for executor kafka params, else messages may not match offsetRange\")\n+\n+  assert(false ==\n+    kafkaParams.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG).asInstanceOf[Boolean],\n+    ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG +\n+      \" must be set to false for executor kafka params, else offsets may commit before processing\")\n+\n+  // TODO is it necessary to have separate configs for initial poll time vs ongoing poll time?\n+  private val pollTimeout = conf.getLong(\"spark.streaming.kafka.consumer.poll.ms\", 256)\n+  private val cacheInitialCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.initialCapacity\", 16)\n+  private val cacheMaxCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.maxCapacity\", 64)\n+  private val cacheLoadFactor =\n+    conf.getDouble(\"spark.streaming.kafka.consumer.cache.loadFactor\", 0.75).toFloat\n+\n+  override def persist(newLevel: StorageLevel): this.type = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  override def getPartitions: Array[Partition] = {\n+    offsetRanges.zipWithIndex.map { case (o, i) =>\n+        new KafkaRDDPartition(i, o.topic, o.partition, o.fromOffset, o.untilOffset)\n+    }.toArray\n+  }\n+\n+  override def count(): Long = offsetRanges.map(_.count).sum\n+\n+  override def countApprox(\n+      timeout: Long,\n+      confidence: Double = 0.95\n+  ): PartialResult[BoundedDouble] = {\n+    val c = count\n+    new PartialResult(new BoundedDouble(c, 1.0, c, c), true)\n+  }\n+\n+  override def isEmpty(): Boolean = count == 0L\n+\n+  override def take(num: Int): Array[ConsumerRecord[K, V]] = {\n+    val nonEmptyPartitions = this.partitions\n+      .map(_.asInstanceOf[KafkaRDDPartition])\n+      .filter(_.count > 0)\n+\n+    if (num < 1 || nonEmptyPartitions.size < 1) {\n+      return new Array[ConsumerRecord[K, V]](0)\n+    }\n+\n+    // Determine in advance how many messages need to be taken from each partition\n+    val parts = nonEmptyPartitions.foldLeft(Map[Int, Int]()) { (result, part) =>\n+      val remain = num - result.values.sum\n+      if (remain > 0) {\n+        val taken = Math.min(remain, part.count)\n+        result + (part.index -> taken.toInt)\n+      } else {\n+        result\n+      }\n+    }\n+\n+    val buf = new ArrayBuffer[ConsumerRecord[K, V]]\n+    val res = context.runJob(\n+      this,\n+      (tc: TaskContext, it: Iterator[ConsumerRecord[K, V]]) =>\n+      it.take(parts(tc.partitionId)).toArray, parts.keys.toArray\n+    )\n+    res.foreach(buf ++= _)\n+    buf.toArray\n+  }\n+\n+  private def executors(): Array[ExecutorCacheTaskLocation] = {\n+    val bm = sparkContext.env.blockManager\n+    bm.master.getPeers(bm.blockManagerId).toArray\n+      .map(x => ExecutorCacheTaskLocation(x.host, x.executorId))\n+      .sortWith((a, b) => a.host > b.host || a.executorId > b.executorId)\n+  }\n+\n+  // non-negative modulus, from java 8 math\n+  private def floorMod(a: Int, b: Int): Int = ((a % b) + b) % b"
  }],
  "prId": 11863
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Why can this be a lazy val. Otherwise this is called one per partition.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-22T22:51:54Z",
    "diffHunk": "@@ -0,0 +1,309 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD\n+ * @param preferredHosts map from TopicPartition to preferred host for processing that partition.\n+ * In most cases, use [[DirectKafkaInputDStream.preferConsistent]]\n+ * Use [[DirectKafkaInputDStream.preferBrokers]] if your executors are on same nodes as brokers.\n+ * @param useConsumerCache whether to use a consumer from a per-jvm cache\n+ * @tparam K type of Kafka message key\n+ * @tparam V type of Kafka message value\n+ */\n+@Experimental\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag] private[spark] (\n+    sc: SparkContext,\n+    val kafkaParams: ju.Map[String, Object],\n+    val offsetRanges: Array[OffsetRange],\n+    val preferredHosts: ju.Map[TopicPartition, String],\n+    useConsumerCache: Boolean\n+) extends RDD[ConsumerRecord[K, V]](sc, Nil) with Logging with HasOffsetRanges {\n+\n+  assert(\"none\" ==\n+    kafkaParams.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).asInstanceOf[String],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to none for executor kafka params, else messages may not match offsetRange\")\n+\n+  assert(false ==\n+    kafkaParams.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG).asInstanceOf[Boolean],\n+    ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG +\n+      \" must be set to false for executor kafka params, else offsets may commit before processing\")\n+\n+  // TODO is it necessary to have separate configs for initial poll time vs ongoing poll time?\n+  private val pollTimeout = conf.getLong(\"spark.streaming.kafka.consumer.poll.ms\", 256)\n+  private val cacheInitialCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.initialCapacity\", 16)\n+  private val cacheMaxCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.maxCapacity\", 64)\n+  private val cacheLoadFactor =\n+    conf.getDouble(\"spark.streaming.kafka.consumer.cache.loadFactor\", 0.75).toFloat\n+\n+  override def persist(newLevel: StorageLevel): this.type = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  override def getPartitions: Array[Partition] = {\n+    offsetRanges.zipWithIndex.map { case (o, i) =>\n+        new KafkaRDDPartition(i, o.topic, o.partition, o.fromOffset, o.untilOffset)\n+    }.toArray\n+  }\n+\n+  override def count(): Long = offsetRanges.map(_.count).sum\n+\n+  override def countApprox(\n+      timeout: Long,\n+      confidence: Double = 0.95\n+  ): PartialResult[BoundedDouble] = {\n+    val c = count\n+    new PartialResult(new BoundedDouble(c, 1.0, c, c), true)\n+  }\n+\n+  override def isEmpty(): Boolean = count == 0L\n+\n+  override def take(num: Int): Array[ConsumerRecord[K, V]] = {\n+    val nonEmptyPartitions = this.partitions\n+      .map(_.asInstanceOf[KafkaRDDPartition])\n+      .filter(_.count > 0)\n+\n+    if (num < 1 || nonEmptyPartitions.size < 1) {\n+      return new Array[ConsumerRecord[K, V]](0)\n+    }\n+\n+    // Determine in advance how many messages need to be taken from each partition\n+    val parts = nonEmptyPartitions.foldLeft(Map[Int, Int]()) { (result, part) =>\n+      val remain = num - result.values.sum\n+      if (remain > 0) {\n+        val taken = Math.min(remain, part.count)\n+        result + (part.index -> taken.toInt)\n+      } else {\n+        result\n+      }\n+    }\n+\n+    val buf = new ArrayBuffer[ConsumerRecord[K, V]]\n+    val res = context.runJob(\n+      this,\n+      (tc: TaskContext, it: Iterator[ConsumerRecord[K, V]]) =>\n+      it.take(parts(tc.partitionId)).toArray, parts.keys.toArray\n+    )\n+    res.foreach(buf ++= _)\n+    buf.toArray\n+  }\n+\n+  private def executors(): Array[ExecutorCacheTaskLocation] = {"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "executors can change during execution, right?\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-23T00:53:11Z",
    "diffHunk": "@@ -0,0 +1,309 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD\n+ * @param preferredHosts map from TopicPartition to preferred host for processing that partition.\n+ * In most cases, use [[DirectKafkaInputDStream.preferConsistent]]\n+ * Use [[DirectKafkaInputDStream.preferBrokers]] if your executors are on same nodes as brokers.\n+ * @param useConsumerCache whether to use a consumer from a per-jvm cache\n+ * @tparam K type of Kafka message key\n+ * @tparam V type of Kafka message value\n+ */\n+@Experimental\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag] private[spark] (\n+    sc: SparkContext,\n+    val kafkaParams: ju.Map[String, Object],\n+    val offsetRanges: Array[OffsetRange],\n+    val preferredHosts: ju.Map[TopicPartition, String],\n+    useConsumerCache: Boolean\n+) extends RDD[ConsumerRecord[K, V]](sc, Nil) with Logging with HasOffsetRanges {\n+\n+  assert(\"none\" ==\n+    kafkaParams.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).asInstanceOf[String],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to none for executor kafka params, else messages may not match offsetRange\")\n+\n+  assert(false ==\n+    kafkaParams.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG).asInstanceOf[Boolean],\n+    ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG +\n+      \" must be set to false for executor kafka params, else offsets may commit before processing\")\n+\n+  // TODO is it necessary to have separate configs for initial poll time vs ongoing poll time?\n+  private val pollTimeout = conf.getLong(\"spark.streaming.kafka.consumer.poll.ms\", 256)\n+  private val cacheInitialCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.initialCapacity\", 16)\n+  private val cacheMaxCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.maxCapacity\", 64)\n+  private val cacheLoadFactor =\n+    conf.getDouble(\"spark.streaming.kafka.consumer.cache.loadFactor\", 0.75).toFloat\n+\n+  override def persist(newLevel: StorageLevel): this.type = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  override def getPartitions: Array[Partition] = {\n+    offsetRanges.zipWithIndex.map { case (o, i) =>\n+        new KafkaRDDPartition(i, o.topic, o.partition, o.fromOffset, o.untilOffset)\n+    }.toArray\n+  }\n+\n+  override def count(): Long = offsetRanges.map(_.count).sum\n+\n+  override def countApprox(\n+      timeout: Long,\n+      confidence: Double = 0.95\n+  ): PartialResult[BoundedDouble] = {\n+    val c = count\n+    new PartialResult(new BoundedDouble(c, 1.0, c, c), true)\n+  }\n+\n+  override def isEmpty(): Boolean = count == 0L\n+\n+  override def take(num: Int): Array[ConsumerRecord[K, V]] = {\n+    val nonEmptyPartitions = this.partitions\n+      .map(_.asInstanceOf[KafkaRDDPartition])\n+      .filter(_.count > 0)\n+\n+    if (num < 1 || nonEmptyPartitions.size < 1) {\n+      return new Array[ConsumerRecord[K, V]](0)\n+    }\n+\n+    // Determine in advance how many messages need to be taken from each partition\n+    val parts = nonEmptyPartitions.foldLeft(Map[Int, Int]()) { (result, part) =>\n+      val remain = num - result.values.sum\n+      if (remain > 0) {\n+        val taken = Math.min(remain, part.count)\n+        result + (part.index -> taken.toInt)\n+      } else {\n+        result\n+      }\n+    }\n+\n+    val buf = new ArrayBuffer[ConsumerRecord[K, V]]\n+    val res = context.runJob(\n+      this,\n+      (tc: TaskContext, it: Iterator[ConsumerRecord[K, V]]) =>\n+      it.take(parts(tc.partitionId)).toArray, parts.keys.toArray\n+    )\n+    res.foreach(buf ++= _)\n+    buf.toArray\n+  }\n+\n+  private def executors(): Array[ExecutorCacheTaskLocation] = {"
  }],
  "prId": 11863
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: extra line.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-22T22:55:40Z",
    "diffHunk": "@@ -0,0 +1,309 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD\n+ * @param preferredHosts map from TopicPartition to preferred host for processing that partition.\n+ * In most cases, use [[DirectKafkaInputDStream.preferConsistent]]\n+ * Use [[DirectKafkaInputDStream.preferBrokers]] if your executors are on same nodes as brokers.\n+ * @param useConsumerCache whether to use a consumer from a per-jvm cache\n+ * @tparam K type of Kafka message key\n+ * @tparam V type of Kafka message value\n+ */\n+@Experimental\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag] private[spark] (\n+    sc: SparkContext,\n+    val kafkaParams: ju.Map[String, Object],\n+    val offsetRanges: Array[OffsetRange],\n+    val preferredHosts: ju.Map[TopicPartition, String],\n+    useConsumerCache: Boolean\n+) extends RDD[ConsumerRecord[K, V]](sc, Nil) with Logging with HasOffsetRanges {\n+\n+  assert(\"none\" ==\n+    kafkaParams.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).asInstanceOf[String],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to none for executor kafka params, else messages may not match offsetRange\")\n+\n+  assert(false ==\n+    kafkaParams.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG).asInstanceOf[Boolean],\n+    ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG +\n+      \" must be set to false for executor kafka params, else offsets may commit before processing\")\n+\n+  // TODO is it necessary to have separate configs for initial poll time vs ongoing poll time?\n+  private val pollTimeout = conf.getLong(\"spark.streaming.kafka.consumer.poll.ms\", 256)\n+  private val cacheInitialCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.initialCapacity\", 16)\n+  private val cacheMaxCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.maxCapacity\", 64)\n+  private val cacheLoadFactor =\n+    conf.getDouble(\"spark.streaming.kafka.consumer.cache.loadFactor\", 0.75).toFloat\n+\n+  override def persist(newLevel: StorageLevel): this.type = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  override def getPartitions: Array[Partition] = {\n+    offsetRanges.zipWithIndex.map { case (o, i) =>\n+        new KafkaRDDPartition(i, o.topic, o.partition, o.fromOffset, o.untilOffset)\n+    }.toArray\n+  }\n+\n+  override def count(): Long = offsetRanges.map(_.count).sum\n+\n+  override def countApprox(\n+      timeout: Long,\n+      confidence: Double = 0.95\n+  ): PartialResult[BoundedDouble] = {\n+    val c = count\n+    new PartialResult(new BoundedDouble(c, 1.0, c, c), true)\n+  }\n+\n+  override def isEmpty(): Boolean = count == 0L\n+\n+  override def take(num: Int): Array[ConsumerRecord[K, V]] = {\n+    val nonEmptyPartitions = this.partitions\n+      .map(_.asInstanceOf[KafkaRDDPartition])\n+      .filter(_.count > 0)\n+\n+    if (num < 1 || nonEmptyPartitions.size < 1) {\n+      return new Array[ConsumerRecord[K, V]](0)\n+    }\n+\n+    // Determine in advance how many messages need to be taken from each partition\n+    val parts = nonEmptyPartitions.foldLeft(Map[Int, Int]()) { (result, part) =>\n+      val remain = num - result.values.sum\n+      if (remain > 0) {\n+        val taken = Math.min(remain, part.count)\n+        result + (part.index -> taken.toInt)\n+      } else {\n+        result\n+      }\n+    }\n+\n+    val buf = new ArrayBuffer[ConsumerRecord[K, V]]\n+    val res = context.runJob(\n+      this,\n+      (tc: TaskContext, it: Iterator[ConsumerRecord[K, V]]) =>\n+      it.take(parts(tc.partitionId)).toArray, parts.keys.toArray\n+    )\n+    res.foreach(buf ++= _)\n+    buf.toArray\n+  }\n+\n+  private def executors(): Array[ExecutorCacheTaskLocation] = {\n+    val bm = sparkContext.env.blockManager\n+    bm.master.getPeers(bm.blockManagerId).toArray\n+      .map(x => ExecutorCacheTaskLocation(x.host, x.executorId))\n+      .sortWith((a, b) => a.host > b.host || a.executorId > b.executorId)\n+  }\n+\n+  // non-negative modulus, from java 8 math\n+  private def floorMod(a: Int, b: Int): Int = ((a % b) + b) % b\n+\n+  override def getPreferredLocations(thePart: Partition): Seq[String] = {\n+    // TODO what about hosts specified by ip vs name\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    val allExecs = executors()\n+    val tp = part.topicPartition\n+    val prefHost = preferredHosts.get(tp)\n+    val prefExecs = if (null == prefHost) allExecs else allExecs.filter(_.host == prefHost)\n+    val execs = if (prefExecs.isEmpty) allExecs else prefExecs\n+    if (execs.isEmpty) {\n+      Seq()\n+    } else {\n+      val index = this.floorMod(tp.hashCode, execs.length)\n+      val chosen = execs(index)\n+"
  }],
  "prId": 11863
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: class docs.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-22T22:56:38Z",
    "diffHunk": "@@ -0,0 +1,309 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD\n+ * @param preferredHosts map from TopicPartition to preferred host for processing that partition.\n+ * In most cases, use [[DirectKafkaInputDStream.preferConsistent]]\n+ * Use [[DirectKafkaInputDStream.preferBrokers]] if your executors are on same nodes as brokers.\n+ * @param useConsumerCache whether to use a consumer from a per-jvm cache\n+ * @tparam K type of Kafka message key\n+ * @tparam V type of Kafka message value\n+ */\n+@Experimental\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag] private[spark] (\n+    sc: SparkContext,\n+    val kafkaParams: ju.Map[String, Object],\n+    val offsetRanges: Array[OffsetRange],\n+    val preferredHosts: ju.Map[TopicPartition, String],\n+    useConsumerCache: Boolean\n+) extends RDD[ConsumerRecord[K, V]](sc, Nil) with Logging with HasOffsetRanges {\n+\n+  assert(\"none\" ==\n+    kafkaParams.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).asInstanceOf[String],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to none for executor kafka params, else messages may not match offsetRange\")\n+\n+  assert(false ==\n+    kafkaParams.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG).asInstanceOf[Boolean],\n+    ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG +\n+      \" must be set to false for executor kafka params, else offsets may commit before processing\")\n+\n+  // TODO is it necessary to have separate configs for initial poll time vs ongoing poll time?\n+  private val pollTimeout = conf.getLong(\"spark.streaming.kafka.consumer.poll.ms\", 256)\n+  private val cacheInitialCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.initialCapacity\", 16)\n+  private val cacheMaxCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.maxCapacity\", 64)\n+  private val cacheLoadFactor =\n+    conf.getDouble(\"spark.streaming.kafka.consumer.cache.loadFactor\", 0.75).toFloat\n+\n+  override def persist(newLevel: StorageLevel): this.type = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  override def getPartitions: Array[Partition] = {\n+    offsetRanges.zipWithIndex.map { case (o, i) =>\n+        new KafkaRDDPartition(i, o.topic, o.partition, o.fromOffset, o.untilOffset)\n+    }.toArray\n+  }\n+\n+  override def count(): Long = offsetRanges.map(_.count).sum\n+\n+  override def countApprox(\n+      timeout: Long,\n+      confidence: Double = 0.95\n+  ): PartialResult[BoundedDouble] = {\n+    val c = count\n+    new PartialResult(new BoundedDouble(c, 1.0, c, c), true)\n+  }\n+\n+  override def isEmpty(): Boolean = count == 0L\n+\n+  override def take(num: Int): Array[ConsumerRecord[K, V]] = {\n+    val nonEmptyPartitions = this.partitions\n+      .map(_.asInstanceOf[KafkaRDDPartition])\n+      .filter(_.count > 0)\n+\n+    if (num < 1 || nonEmptyPartitions.size < 1) {\n+      return new Array[ConsumerRecord[K, V]](0)\n+    }\n+\n+    // Determine in advance how many messages need to be taken from each partition\n+    val parts = nonEmptyPartitions.foldLeft(Map[Int, Int]()) { (result, part) =>\n+      val remain = num - result.values.sum\n+      if (remain > 0) {\n+        val taken = Math.min(remain, part.count)\n+        result + (part.index -> taken.toInt)\n+      } else {\n+        result\n+      }\n+    }\n+\n+    val buf = new ArrayBuffer[ConsumerRecord[K, V]]\n+    val res = context.runJob(\n+      this,\n+      (tc: TaskContext, it: Iterator[ConsumerRecord[K, V]]) =>\n+      it.take(parts(tc.partitionId)).toArray, parts.keys.toArray\n+    )\n+    res.foreach(buf ++= _)\n+    buf.toArray\n+  }\n+\n+  private def executors(): Array[ExecutorCacheTaskLocation] = {\n+    val bm = sparkContext.env.blockManager\n+    bm.master.getPeers(bm.blockManagerId).toArray\n+      .map(x => ExecutorCacheTaskLocation(x.host, x.executorId))\n+      .sortWith((a, b) => a.host > b.host || a.executorId > b.executorId)\n+  }\n+\n+  // non-negative modulus, from java 8 math\n+  private def floorMod(a: Int, b: Int): Int = ((a % b) + b) % b\n+\n+  override def getPreferredLocations(thePart: Partition): Seq[String] = {\n+    // TODO what about hosts specified by ip vs name\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    val allExecs = executors()\n+    val tp = part.topicPartition\n+    val prefHost = preferredHosts.get(tp)\n+    val prefExecs = if (null == prefHost) allExecs else allExecs.filter(_.host == prefHost)\n+    val execs = if (prefExecs.isEmpty) allExecs else prefExecs\n+    if (execs.isEmpty) {\n+      Seq()\n+    } else {\n+      val index = this.floorMod(tp.hashCode, execs.length)\n+      val chosen = execs(index)\n+\n+      Seq(chosen.toString)\n+    }\n+  }\n+\n+  private def errBeginAfterEnd(part: KafkaRDDPartition): String =\n+    s\"Beginning offset ${part.fromOffset} is after the ending offset ${part.untilOffset} \" +\n+      s\"for topic ${part.topic} partition ${part.partition}. \" +\n+      \"You either provided an invalid fromOffset, or the Kafka topic has been damaged\"\n+\n+  override def compute(thePart: Partition, context: TaskContext): Iterator[ConsumerRecord[K, V]] = {\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    assert(part.fromOffset <= part.untilOffset, errBeginAfterEnd(part))\n+    if (part.fromOffset == part.untilOffset) {\n+      log.info(s\"Beginning offset ${part.fromOffset} is the same as ending offset \" +\n+        s\"skipping ${part.topic} ${part.partition}\")\n+      Iterator.empty\n+    } else {\n+      new KafkaRDDIterator(part, context)\n+    }\n+  }\n+\n+  private class KafkaRDDIterator("
  }],
  "prId": 11863
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "nit: remove `@Experimental` since it's not public\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-28T21:57:49Z",
    "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD\n+ * @param preferredHosts map from TopicPartition to preferred host for processing that partition.\n+ * In most cases, use [[DirectKafkaInputDStream.preferConsistent]]\n+ * Use [[DirectKafkaInputDStream.preferBrokers]] if your executors are on same nodes as brokers.\n+ * @param useConsumerCache whether to use a consumer from a per-jvm cache\n+ * @tparam K type of Kafka message key\n+ * @tparam V type of Kafka message value\n+ */\n+@Experimental"
  }],
  "prId": 11863
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "This comparator is not correct:\n`f(ExecutorCacheTaskLocation(\"a\", 2), ExecutorCacheTaskLocation(\"b\", 1))` and `f(ExecutorCacheTaskLocation(\"b\", 1), ExecutorCacheTaskLocation(\"a\", 2))` both return true.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-29T19:00:34Z",
    "diffHunk": "@@ -0,0 +1,224 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.{Partition, SparkContext, SparkException, TaskContext}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.scheduler.ExecutorCacheTaskLocation\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * A batch-oriented interface for consuming from Kafka.\n+ * Starting and ending offsets are specified in advance,\n+ * so that you can control exactly-once semantics.\n+ * @param kafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.htmll#newconsumerconfigs\">\n+ * configuration parameters</a>. Requires \"bootstrap.servers\" to be set\n+ * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+ * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD\n+ * @param preferredHosts map from TopicPartition to preferred host for processing that partition.\n+ * In most cases, use [[DirectKafkaInputDStream.preferConsistent]]\n+ * Use [[DirectKafkaInputDStream.preferBrokers]] if your executors are on same nodes as brokers.\n+ * @param useConsumerCache whether to use a consumer from a per-jvm cache\n+ * @tparam K type of Kafka message key\n+ * @tparam V type of Kafka message value\n+ */\n+private[spark] class KafkaRDD[K, V](\n+    sc: SparkContext,\n+    val kafkaParams: ju.Map[String, Object],\n+    val offsetRanges: Array[OffsetRange],\n+    val preferredHosts: ju.Map[TopicPartition, String],\n+    useConsumerCache: Boolean\n+) extends RDD[ConsumerRecord[K, V]](sc, Nil) with Logging with HasOffsetRanges {\n+\n+  assert(\"none\" ==\n+    kafkaParams.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).asInstanceOf[String],\n+    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG +\n+      \" must be set to none for executor kafka params, else messages may not match offsetRange\")\n+\n+  assert(false ==\n+    kafkaParams.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG).asInstanceOf[Boolean],\n+    ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG +\n+      \" must be set to false for executor kafka params, else offsets may commit before processing\")\n+\n+  // TODO is it necessary to have separate configs for initial poll time vs ongoing poll time?\n+  private val pollTimeout = conf.getLong(\"spark.streaming.kafka.consumer.poll.ms\", 256)\n+  private val cacheInitialCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.initialCapacity\", 16)\n+  private val cacheMaxCapacity =\n+    conf.getInt(\"spark.streaming.kafka.consumer.cache.maxCapacity\", 64)\n+  private val cacheLoadFactor =\n+    conf.getDouble(\"spark.streaming.kafka.consumer.cache.loadFactor\", 0.75).toFloat\n+\n+  override def persist(newLevel: StorageLevel): this.type = {\n+    logError(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  override def getPartitions: Array[Partition] = {\n+    offsetRanges.zipWithIndex.map { case (o, i) =>\n+        new KafkaRDDPartition(i, o.topic, o.partition, o.fromOffset, o.untilOffset)\n+    }.toArray\n+  }\n+\n+  override def count(): Long = offsetRanges.map(_.count).sum\n+\n+  override def countApprox(\n+      timeout: Long,\n+      confidence: Double = 0.95\n+  ): PartialResult[BoundedDouble] = {\n+    val c = count\n+    new PartialResult(new BoundedDouble(c, 1.0, c, c), true)\n+  }\n+\n+  override def isEmpty(): Boolean = count == 0L\n+\n+  override def take(num: Int): Array[ConsumerRecord[K, V]] = {\n+    val nonEmptyPartitions = this.partitions\n+      .map(_.asInstanceOf[KafkaRDDPartition])\n+      .filter(_.count > 0)\n+\n+    if (num < 1 || nonEmptyPartitions.isEmpty) {\n+      return new Array[ConsumerRecord[K, V]](0)\n+    }\n+\n+    // Determine in advance how many messages need to be taken from each partition\n+    val parts = nonEmptyPartitions.foldLeft(Map[Int, Int]()) { (result, part) =>\n+      val remain = num - result.values.sum\n+      if (remain > 0) {\n+        val taken = Math.min(remain, part.count)\n+        result + (part.index -> taken.toInt)\n+      } else {\n+        result\n+      }\n+    }\n+\n+    val buf = new ArrayBuffer[ConsumerRecord[K, V]]\n+    val res = context.runJob(\n+      this,\n+      (tc: TaskContext, it: Iterator[ConsumerRecord[K, V]]) =>\n+      it.take(parts(tc.partitionId)).toArray, parts.keys.toArray\n+    )\n+    res.foreach(buf ++= _)\n+    buf.toArray\n+  }\n+\n+  private def executors(): Array[ExecutorCacheTaskLocation] = {\n+    val bm = sparkContext.env.blockManager\n+    bm.master.getPeers(bm.blockManagerId).toArray\n+      .map(x => ExecutorCacheTaskLocation(x.host, x.executorId))\n+      .sortWith((a, b) => a.host > b.host || a.executorId > b.executorId)"
  }],
  "prId": 11863
}]