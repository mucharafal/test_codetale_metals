[{
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "What if two jobs use the same `topic` and `groupId`, and they run at the same time? Then a CachedKafkaConsumer may be used by two threads in the same JVM.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-22T19:38:28Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord, KafkaConsumer }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+/**\n+ * Consumer of single topicpartition, intended for cached reuse.\n+ * Underlying consumer is not threadsafe, so neither is this,\n+ * but processing the same topicpartition and group id in multiple threads is usually bad anyway."
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "Using the same group id for any two jobs you expect to keep separate is already a disaster with either the old or new Kafka high level consumers.\n\nThe underlying consumer does have a lightweight locking mechanism, and will just throw a ConcurrentModificationException if someone does do this.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-22T19:44:08Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord, KafkaConsumer }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+/**\n+ * Consumer of single topicpartition, intended for cached reuse.\n+ * Underlying consumer is not threadsafe, so neither is this,\n+ * but processing the same topicpartition and group id in multiple threads is usually bad anyway."
  }, {
    "author": {
      "login": "zsxwing"
    },
    "body": "This could happen when `spark.streaming.concurrentJobs` is more than 1 and some batches are too slow.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-22T19:55:18Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord, KafkaConsumer }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+/**\n+ * Consumer of single topicpartition, intended for cached reuse.\n+ * Underlying consumer is not threadsafe, so neither is this,\n+ * but processing the same topicpartition and group id in multiple threads is usually bad anyway."
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "concurrentJobs is still undocumented right?  I wouldn't expect the existing direct stream to behave predictably under those conditions either.\n\nThe only time I seeing trying to process different chunks of the same topicpartition at the same time as being unavoidable is during checkpoint recovery, which is why the cache isn't used then.\n\nIf someone has a better idea I'm all ears, but given the prefetching / handshaking implications of the new consumer, I don't see an alternative to caching them.  We could write considerably more complicated caching logic with a checkout / return system where a certain number of consumers for the same topicpartition were allowed at a time... but IMHO multiple consumers of the same topicpartition in the same job in the same jvm is not something we want to encourage - it's at best a waste of resources and at worst a correctness problem.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-22T20:30:34Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord, KafkaConsumer }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+/**\n+ * Consumer of single topicpartition, intended for cached reuse.\n+ * Underlying consumer is not threadsafe, so neither is this,\n+ * but processing the same topicpartition and group id in multiple threads is usually bad anyway."
  }],
  "prId": 11863
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "wrong indentation.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-22T22:32:51Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord, KafkaConsumer }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+/**\n+ * Consumer of single topicpartition, intended for cached reuse.\n+ * Underlying consumer is not threadsafe, so neither is this,\n+ * but processing the same topicpartition and group id in multiple threads is usually bad anyway.\n+ */\n+private[kafka]\n+class CachedKafkaConsumer[K, V] private(\n+  val groupId: String,\n+  val topic: String,\n+  val partition: Int,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  assert(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  val topicPartition = new TopicPartition(topic, partition)\n+\n+  protected val consumer = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  protected var buffer = ju.Collections.emptyList[ConsumerRecord[K, V]]().iterator\n+  protected var nextOffset = -2L\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    log.debug(s\"Get $groupId $topic $partition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      log.info(s\"Initial fetch for $groupId $topic $partition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    assert(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topic $partition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      log.info(s\"Buffer miss for $groupId $topic $partition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      assert(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topic $partition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      assert(record.offset == offset,\n+        s\"Got wrong record for $groupId $topic $partition even after seeking to offset $offset\")\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    log.debug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    log.debug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.iterator\n+  }\n+\n+}\n+\n+private[kafka]\n+object CachedKafkaConsumer extends Logging {\n+\n+  private case class CacheKey(groupId: String, topic: String, partition: Int)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.LinkedHashMap[CacheKey, CachedKafkaConsumer[_, _]] = null\n+\n+  /** Must be called before get, once per JVM, to configure the cache. Further calls are ignored */\n+  def init(\n+    initialCapacity: Int,"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "indentation.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-22T22:46:33Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord, KafkaConsumer }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+/**\n+ * Consumer of single topicpartition, intended for cached reuse.\n+ * Underlying consumer is not threadsafe, so neither is this,\n+ * but processing the same topicpartition and group id in multiple threads is usually bad anyway.\n+ */\n+private[kafka]\n+class CachedKafkaConsumer[K, V] private(\n+  val groupId: String,\n+  val topic: String,\n+  val partition: Int,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  assert(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  val topicPartition = new TopicPartition(topic, partition)\n+\n+  protected val consumer = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  protected var buffer = ju.Collections.emptyList[ConsumerRecord[K, V]]().iterator\n+  protected var nextOffset = -2L\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    log.debug(s\"Get $groupId $topic $partition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      log.info(s\"Initial fetch for $groupId $topic $partition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    assert(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topic $partition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      log.info(s\"Buffer miss for $groupId $topic $partition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      assert(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topic $partition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      assert(record.offset == offset,\n+        s\"Got wrong record for $groupId $topic $partition even after seeking to offset $offset\")\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    log.debug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    log.debug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.iterator\n+  }\n+\n+}\n+\n+private[kafka]\n+object CachedKafkaConsumer extends Logging {\n+\n+  private case class CacheKey(groupId: String, topic: String, partition: Int)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.LinkedHashMap[CacheKey, CachedKafkaConsumer[_, _]] = null\n+\n+  /** Must be called before get, once per JVM, to configure the cache. Further calls are ignored */\n+  def init(\n+    initialCapacity: Int,"
  }],
  "prId": 11863
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "wrong indentation\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-22T22:33:00Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord, KafkaConsumer }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+/**\n+ * Consumer of single topicpartition, intended for cached reuse.\n+ * Underlying consumer is not threadsafe, so neither is this,\n+ * but processing the same topicpartition and group id in multiple threads is usually bad anyway.\n+ */\n+private[kafka]\n+class CachedKafkaConsumer[K, V] private(\n+  val groupId: String,\n+  val topic: String,\n+  val partition: Int,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  assert(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  val topicPartition = new TopicPartition(topic, partition)\n+\n+  protected val consumer = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  protected var buffer = ju.Collections.emptyList[ConsumerRecord[K, V]]().iterator\n+  protected var nextOffset = -2L\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    log.debug(s\"Get $groupId $topic $partition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      log.info(s\"Initial fetch for $groupId $topic $partition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    assert(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topic $partition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      log.info(s\"Buffer miss for $groupId $topic $partition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      assert(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topic $partition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      assert(record.offset == offset,\n+        s\"Got wrong record for $groupId $topic $partition even after seeking to offset $offset\")\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    log.debug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    log.debug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.iterator\n+  }\n+\n+}\n+\n+private[kafka]\n+object CachedKafkaConsumer extends Logging {\n+\n+  private case class CacheKey(groupId: String, topic: String, partition: Int)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.LinkedHashMap[CacheKey, CachedKafkaConsumer[_, _]] = null\n+\n+  /** Must be called before get, once per JVM, to configure the cache. Further calls are ignored */\n+  def init(\n+    initialCapacity: Int,\n+    maxCapacity: Int,\n+    loadFactor: Float): Unit = CachedKafkaConsumer.synchronized {\n+    if (null == cache) {\n+      log.info(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, CachedKafkaConsumer[_, _]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, CachedKafkaConsumer[_, _]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            entry.getValue.consumer.close()\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   */\n+  def get[K, V](\n+    groupId: String,"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "indentation.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-22T22:46:27Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord, KafkaConsumer }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+/**\n+ * Consumer of single topicpartition, intended for cached reuse.\n+ * Underlying consumer is not threadsafe, so neither is this,\n+ * but processing the same topicpartition and group id in multiple threads is usually bad anyway.\n+ */\n+private[kafka]\n+class CachedKafkaConsumer[K, V] private(\n+  val groupId: String,\n+  val topic: String,\n+  val partition: Int,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  assert(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  val topicPartition = new TopicPartition(topic, partition)\n+\n+  protected val consumer = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  protected var buffer = ju.Collections.emptyList[ConsumerRecord[K, V]]().iterator\n+  protected var nextOffset = -2L\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    log.debug(s\"Get $groupId $topic $partition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      log.info(s\"Initial fetch for $groupId $topic $partition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    assert(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topic $partition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      log.info(s\"Buffer miss for $groupId $topic $partition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      assert(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topic $partition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      assert(record.offset == offset,\n+        s\"Got wrong record for $groupId $topic $partition even after seeking to offset $offset\")\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    log.debug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    log.debug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.iterator\n+  }\n+\n+}\n+\n+private[kafka]\n+object CachedKafkaConsumer extends Logging {\n+\n+  private case class CacheKey(groupId: String, topic: String, partition: Int)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.LinkedHashMap[CacheKey, CachedKafkaConsumer[_, _]] = null\n+\n+  /** Must be called before get, once per JVM, to configure the cache. Further calls are ignored */\n+  def init(\n+    initialCapacity: Int,\n+    maxCapacity: Int,\n+    loadFactor: Float): Unit = CachedKafkaConsumer.synchronized {\n+    if (null == cache) {\n+      log.info(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, CachedKafkaConsumer[_, _]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, CachedKafkaConsumer[_, _]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            entry.getValue.consumer.close()\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   */\n+  def get[K, V](\n+    groupId: String,"
  }],
  "prId": 11863
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "wrong indentation. \n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-22T22:33:09Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord, KafkaConsumer }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+/**\n+ * Consumer of single topicpartition, intended for cached reuse.\n+ * Underlying consumer is not threadsafe, so neither is this,\n+ * but processing the same topicpartition and group id in multiple threads is usually bad anyway.\n+ */\n+private[kafka]\n+class CachedKafkaConsumer[K, V] private(\n+  val groupId: String,\n+  val topic: String,\n+  val partition: Int,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  assert(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  val topicPartition = new TopicPartition(topic, partition)\n+\n+  protected val consumer = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  protected var buffer = ju.Collections.emptyList[ConsumerRecord[K, V]]().iterator\n+  protected var nextOffset = -2L\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    log.debug(s\"Get $groupId $topic $partition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      log.info(s\"Initial fetch for $groupId $topic $partition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    assert(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topic $partition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      log.info(s\"Buffer miss for $groupId $topic $partition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      assert(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topic $partition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      assert(record.offset == offset,\n+        s\"Got wrong record for $groupId $topic $partition even after seeking to offset $offset\")\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    log.debug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    log.debug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.iterator\n+  }\n+\n+}\n+\n+private[kafka]\n+object CachedKafkaConsumer extends Logging {\n+\n+  private case class CacheKey(groupId: String, topic: String, partition: Int)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.LinkedHashMap[CacheKey, CachedKafkaConsumer[_, _]] = null\n+\n+  /** Must be called before get, once per JVM, to configure the cache. Further calls are ignored */\n+  def init(\n+    initialCapacity: Int,\n+    maxCapacity: Int,\n+    loadFactor: Float): Unit = CachedKafkaConsumer.synchronized {\n+    if (null == cache) {\n+      log.info(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, CachedKafkaConsumer[_, _]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, CachedKafkaConsumer[_, _]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            entry.getValue.consumer.close()\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   */\n+  def get[K, V](\n+    groupId: String,\n+    topic: String,\n+    partition: Int,\n+    kafkaParams: ju.Map[String, Object]): CachedKafkaConsumer[K, V] =\n+    CachedKafkaConsumer.synchronized {\n+      val k = CacheKey(groupId, topic, partition)\n+      val v = cache.get(k)\n+      if (null == v) {\n+        log.info(s\"Cache miss for $k\")\n+        log.debug(cache.keySet.toString)\n+        val c = new CachedKafkaConsumer[K, V](groupId, topic, partition, kafkaParams)\n+        cache.put(k, c)\n+        c\n+      } else {\n+        // any given topicpartition should have a consistent key and value type\n+        v.asInstanceOf[CachedKafkaConsumer[K, V]]\n+      }\n+    }\n+\n+  /**\n+   * Get a fresh new instance, unassociated with the global cache.\n+   * Caller is responsible for closing\n+   */\n+  def getUncached[K, V](\n+    groupId: String,"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "indentation\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-22T22:46:21Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord, KafkaConsumer }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+/**\n+ * Consumer of single topicpartition, intended for cached reuse.\n+ * Underlying consumer is not threadsafe, so neither is this,\n+ * but processing the same topicpartition and group id in multiple threads is usually bad anyway.\n+ */\n+private[kafka]\n+class CachedKafkaConsumer[K, V] private(\n+  val groupId: String,\n+  val topic: String,\n+  val partition: Int,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  assert(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  val topicPartition = new TopicPartition(topic, partition)\n+\n+  protected val consumer = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  protected var buffer = ju.Collections.emptyList[ConsumerRecord[K, V]]().iterator\n+  protected var nextOffset = -2L\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    log.debug(s\"Get $groupId $topic $partition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      log.info(s\"Initial fetch for $groupId $topic $partition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    assert(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topic $partition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      log.info(s\"Buffer miss for $groupId $topic $partition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      assert(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topic $partition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      assert(record.offset == offset,\n+        s\"Got wrong record for $groupId $topic $partition even after seeking to offset $offset\")\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    log.debug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    log.debug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.iterator\n+  }\n+\n+}\n+\n+private[kafka]\n+object CachedKafkaConsumer extends Logging {\n+\n+  private case class CacheKey(groupId: String, topic: String, partition: Int)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.LinkedHashMap[CacheKey, CachedKafkaConsumer[_, _]] = null\n+\n+  /** Must be called before get, once per JVM, to configure the cache. Further calls are ignored */\n+  def init(\n+    initialCapacity: Int,\n+    maxCapacity: Int,\n+    loadFactor: Float): Unit = CachedKafkaConsumer.synchronized {\n+    if (null == cache) {\n+      log.info(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, CachedKafkaConsumer[_, _]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, CachedKafkaConsumer[_, _]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            entry.getValue.consumer.close()\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   */\n+  def get[K, V](\n+    groupId: String,\n+    topic: String,\n+    partition: Int,\n+    kafkaParams: ju.Map[String, Object]): CachedKafkaConsumer[K, V] =\n+    CachedKafkaConsumer.synchronized {\n+      val k = CacheKey(groupId, topic, partition)\n+      val v = cache.get(k)\n+      if (null == v) {\n+        log.info(s\"Cache miss for $k\")\n+        log.debug(cache.keySet.toString)\n+        val c = new CachedKafkaConsumer[K, V](groupId, topic, partition, kafkaParams)\n+        cache.put(k, c)\n+        c\n+      } else {\n+        // any given topicpartition should have a consistent key and value type\n+        v.asInstanceOf[CachedKafkaConsumer[K, V]]\n+      }\n+    }\n+\n+  /**\n+   * Get a fresh new instance, unassociated with the global cache.\n+   * Caller is responsible for closing\n+   */\n+  def getUncached[K, V](\n+    groupId: String,"
  }],
  "prId": 11863
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "nit: Put `cache.remove(k)` into finally since `close` may throw exception (See https://github.com/apache/kafka/blob/107205a7f17a4b1e235089668cb283bf16fbb88a/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java#L1391)\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-28T21:32:03Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord, KafkaConsumer }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+/**\n+ * Consumer of single topicpartition, intended for cached reuse.\n+ * Underlying consumer is not threadsafe, so neither is this,\n+ * but processing the same topicpartition and group id in multiple threads is usually bad anyway.\n+ */\n+private[kafka]\n+class CachedKafkaConsumer[K, V] private(\n+  val groupId: String,\n+  val topic: String,\n+  val partition: Int,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  assert(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  val topicPartition = new TopicPartition(topic, partition)\n+\n+  protected val consumer = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  protected var buffer = ju.Collections.emptyList[ConsumerRecord[K, V]]().iterator\n+  protected var nextOffset = -2L\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    log.debug(s\"Get $groupId $topic $partition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      log.info(s\"Initial fetch for $groupId $topic $partition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    assert(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topic $partition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      log.info(s\"Buffer miss for $groupId $topic $partition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      assert(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topic $partition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      assert(record.offset == offset,\n+        s\"Got wrong record for $groupId $topic $partition even after seeking to offset $offset\")\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    log.debug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    log.debug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.iterator\n+  }\n+\n+}\n+\n+private[kafka]\n+object CachedKafkaConsumer extends Logging {\n+\n+  private case class CacheKey(groupId: String, topic: String, partition: Int)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.LinkedHashMap[CacheKey, CachedKafkaConsumer[_, _]] = null\n+\n+  /** Must be called before get, once per JVM, to configure the cache. Further calls are ignored */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = CachedKafkaConsumer.synchronized {\n+    if (null == cache) {\n+      log.info(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, CachedKafkaConsumer[_, _]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, CachedKafkaConsumer[_, _]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            entry.getValue.consumer.close()\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   */\n+  def get[K, V](\n+      groupId: String,\n+      topic: String,\n+      partition: Int,\n+      kafkaParams: ju.Map[String, Object]): CachedKafkaConsumer[K, V] =\n+    CachedKafkaConsumer.synchronized {\n+      val k = CacheKey(groupId, topic, partition)\n+      val v = cache.get(k)\n+      if (null == v) {\n+        log.info(s\"Cache miss for $k\")\n+        log.debug(cache.keySet.toString)\n+        val c = new CachedKafkaConsumer[K, V](groupId, topic, partition, kafkaParams)\n+        cache.put(k, c)\n+        c\n+      } else {\n+        // any given topicpartition should have a consistent key and value type\n+        v.asInstanceOf[CachedKafkaConsumer[K, V]]\n+      }\n+    }\n+\n+  /**\n+   * Get a fresh new instance, unassociated with the global cache.\n+   * Caller is responsible for closing\n+   */\n+  def getUncached[K, V](\n+      groupId: String,\n+      topic: String,\n+      partition: Int,\n+      kafkaParams: ju.Map[String, Object]): CachedKafkaConsumer[K, V] =\n+    new CachedKafkaConsumer[K, V](groupId, topic, partition, kafkaParams)\n+\n+  /** remove consumer for given groupId, topic, and partition, if it exists */\n+  def remove(groupId: String, topic: String, partition: Int): Unit =\n+    CachedKafkaConsumer.synchronized {\n+      val k = CacheKey(groupId, topic, partition)\n+      log.info(s\"Removing $k from cache\")\n+      val v = cache.get(k)\n+      if (null != v) {\n+        v.close()\n+        cache.remove(k)"
  }, {
    "author": {
      "login": "zsxwing"
    },
    "body": "Or just move `cache.remove(k)` above `v.close()`\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-28T21:43:44Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord, KafkaConsumer }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+/**\n+ * Consumer of single topicpartition, intended for cached reuse.\n+ * Underlying consumer is not threadsafe, so neither is this,\n+ * but processing the same topicpartition and group id in multiple threads is usually bad anyway.\n+ */\n+private[kafka]\n+class CachedKafkaConsumer[K, V] private(\n+  val groupId: String,\n+  val topic: String,\n+  val partition: Int,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  assert(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  val topicPartition = new TopicPartition(topic, partition)\n+\n+  protected val consumer = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  protected var buffer = ju.Collections.emptyList[ConsumerRecord[K, V]]().iterator\n+  protected var nextOffset = -2L\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    log.debug(s\"Get $groupId $topic $partition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      log.info(s\"Initial fetch for $groupId $topic $partition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    assert(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topic $partition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      log.info(s\"Buffer miss for $groupId $topic $partition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      assert(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topic $partition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      assert(record.offset == offset,\n+        s\"Got wrong record for $groupId $topic $partition even after seeking to offset $offset\")\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    log.debug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    log.debug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.iterator\n+  }\n+\n+}\n+\n+private[kafka]\n+object CachedKafkaConsumer extends Logging {\n+\n+  private case class CacheKey(groupId: String, topic: String, partition: Int)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.LinkedHashMap[CacheKey, CachedKafkaConsumer[_, _]] = null\n+\n+  /** Must be called before get, once per JVM, to configure the cache. Further calls are ignored */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = CachedKafkaConsumer.synchronized {\n+    if (null == cache) {\n+      log.info(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, CachedKafkaConsumer[_, _]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, CachedKafkaConsumer[_, _]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            entry.getValue.consumer.close()\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   */\n+  def get[K, V](\n+      groupId: String,\n+      topic: String,\n+      partition: Int,\n+      kafkaParams: ju.Map[String, Object]): CachedKafkaConsumer[K, V] =\n+    CachedKafkaConsumer.synchronized {\n+      val k = CacheKey(groupId, topic, partition)\n+      val v = cache.get(k)\n+      if (null == v) {\n+        log.info(s\"Cache miss for $k\")\n+        log.debug(cache.keySet.toString)\n+        val c = new CachedKafkaConsumer[K, V](groupId, topic, partition, kafkaParams)\n+        cache.put(k, c)\n+        c\n+      } else {\n+        // any given topicpartition should have a consistent key and value type\n+        v.asInstanceOf[CachedKafkaConsumer[K, V]]\n+      }\n+    }\n+\n+  /**\n+   * Get a fresh new instance, unassociated with the global cache.\n+   * Caller is responsible for closing\n+   */\n+  def getUncached[K, V](\n+      groupId: String,\n+      topic: String,\n+      partition: Int,\n+      kafkaParams: ju.Map[String, Object]): CachedKafkaConsumer[K, V] =\n+    new CachedKafkaConsumer[K, V](groupId, topic, partition, kafkaParams)\n+\n+  /** remove consumer for given groupId, topic, and partition, if it exists */\n+  def remove(groupId: String, topic: String, partition: Int): Unit =\n+    CachedKafkaConsumer.synchronized {\n+      val k = CacheKey(groupId, topic, partition)\n+      log.info(s\"Removing $k from cache\")\n+      val v = cache.get(k)\n+      if (null != v) {\n+        v.close()\n+        cache.remove(k)"
  }, {
    "author": {
      "login": "zsxwing"
    },
    "body": "Actually, you can rewrite it like this for fine-grained locking:\n\n``` Scala\n  def remove(groupId: String, topic: String, partition: Int): Unit = {\n    val k = CacheKey(groupId, topic, partition)\n    log.info(s\"Removing $k from cache\")\n    val v = CachedKafkaConsumer.synchronized {\n      cache.remove(k)\n    }\n    if (null != v) {\n      v.close()\n      log.info(s\"Removed $k from cache\")\n    }\n  }\n```\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-28T21:47:30Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord, KafkaConsumer }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+/**\n+ * Consumer of single topicpartition, intended for cached reuse.\n+ * Underlying consumer is not threadsafe, so neither is this,\n+ * but processing the same topicpartition and group id in multiple threads is usually bad anyway.\n+ */\n+private[kafka]\n+class CachedKafkaConsumer[K, V] private(\n+  val groupId: String,\n+  val topic: String,\n+  val partition: Int,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  assert(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  val topicPartition = new TopicPartition(topic, partition)\n+\n+  protected val consumer = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  protected var buffer = ju.Collections.emptyList[ConsumerRecord[K, V]]().iterator\n+  protected var nextOffset = -2L\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    log.debug(s\"Get $groupId $topic $partition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      log.info(s\"Initial fetch for $groupId $topic $partition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    assert(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topic $partition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      log.info(s\"Buffer miss for $groupId $topic $partition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      assert(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topic $partition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      assert(record.offset == offset,\n+        s\"Got wrong record for $groupId $topic $partition even after seeking to offset $offset\")\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    log.debug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    log.debug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.iterator\n+  }\n+\n+}\n+\n+private[kafka]\n+object CachedKafkaConsumer extends Logging {\n+\n+  private case class CacheKey(groupId: String, topic: String, partition: Int)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.LinkedHashMap[CacheKey, CachedKafkaConsumer[_, _]] = null\n+\n+  /** Must be called before get, once per JVM, to configure the cache. Further calls are ignored */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = CachedKafkaConsumer.synchronized {\n+    if (null == cache) {\n+      log.info(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, CachedKafkaConsumer[_, _]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, CachedKafkaConsumer[_, _]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            entry.getValue.consumer.close()\n+            true\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get a cached consumer for groupId, assigned to topic and partition.\n+   * If matching consumer doesn't already exist, will be created using kafkaParams.\n+   */\n+  def get[K, V](\n+      groupId: String,\n+      topic: String,\n+      partition: Int,\n+      kafkaParams: ju.Map[String, Object]): CachedKafkaConsumer[K, V] =\n+    CachedKafkaConsumer.synchronized {\n+      val k = CacheKey(groupId, topic, partition)\n+      val v = cache.get(k)\n+      if (null == v) {\n+        log.info(s\"Cache miss for $k\")\n+        log.debug(cache.keySet.toString)\n+        val c = new CachedKafkaConsumer[K, V](groupId, topic, partition, kafkaParams)\n+        cache.put(k, c)\n+        c\n+      } else {\n+        // any given topicpartition should have a consistent key and value type\n+        v.asInstanceOf[CachedKafkaConsumer[K, V]]\n+      }\n+    }\n+\n+  /**\n+   * Get a fresh new instance, unassociated with the global cache.\n+   * Caller is responsible for closing\n+   */\n+  def getUncached[K, V](\n+      groupId: String,\n+      topic: String,\n+      partition: Int,\n+      kafkaParams: ju.Map[String, Object]): CachedKafkaConsumer[K, V] =\n+    new CachedKafkaConsumer[K, V](groupId, topic, partition, kafkaParams)\n+\n+  /** remove consumer for given groupId, topic, and partition, if it exists */\n+  def remove(groupId: String, topic: String, partition: Int): Unit =\n+    CachedKafkaConsumer.synchronized {\n+      val k = CacheKey(groupId, topic, partition)\n+      log.info(s\"Removing $k from cache\")\n+      val v = cache.get(k)\n+      if (null != v) {\n+        v.close()\n+        cache.remove(k)"
  }],
  "prId": 11863
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "nit: catch the exception for `close` and return `true` because LinkedHashMap won't remove an entry if `removeEldestEntry` throws exception.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-28T21:34:02Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+\n+import org.apache.kafka.clients.consumer.{ ConsumerConfig, ConsumerRecord, KafkaConsumer }\n+import org.apache.kafka.common.TopicPartition\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.internal.Logging\n+\n+\n+/**\n+ * Consumer of single topicpartition, intended for cached reuse.\n+ * Underlying consumer is not threadsafe, so neither is this,\n+ * but processing the same topicpartition and group id in multiple threads is usually bad anyway.\n+ */\n+private[kafka]\n+class CachedKafkaConsumer[K, V] private(\n+  val groupId: String,\n+  val topic: String,\n+  val partition: Int,\n+  val kafkaParams: ju.Map[String, Object]) extends Logging {\n+\n+  assert(groupId == kafkaParams.get(ConsumerConfig.GROUP_ID_CONFIG),\n+    \"groupId used for cache key must match the groupId in kafkaParams\")\n+\n+  val topicPartition = new TopicPartition(topic, partition)\n+\n+  protected val consumer = {\n+    val c = new KafkaConsumer[K, V](kafkaParams)\n+    val tps = new ju.ArrayList[TopicPartition]()\n+    tps.add(topicPartition)\n+    c.assign(tps)\n+    c\n+  }\n+\n+  // TODO if the buffer was kept around as a random-access structure,\n+  // could possibly optimize re-calculating of an RDD in the same batch\n+  protected var buffer = ju.Collections.emptyList[ConsumerRecord[K, V]]().iterator\n+  protected var nextOffset = -2L\n+\n+  def close(): Unit = consumer.close()\n+\n+  /**\n+   * Get the record for the given offset, waiting up to timeout ms if IO is necessary.\n+   * Sequential forward access will use buffers, but random access will be horribly inefficient.\n+   */\n+  def get(offset: Long, timeout: Long): ConsumerRecord[K, V] = {\n+    log.debug(s\"Get $groupId $topic $partition nextOffset $nextOffset requested $offset\")\n+    if (offset != nextOffset) {\n+      log.info(s\"Initial fetch for $groupId $topic $partition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+    }\n+\n+    if (!buffer.hasNext()) { poll(timeout) }\n+    assert(buffer.hasNext(),\n+      s\"Failed to get records for $groupId $topic $partition $offset after polling for $timeout\")\n+    var record = buffer.next()\n+\n+    if (record.offset != offset) {\n+      log.info(s\"Buffer miss for $groupId $topic $partition $offset\")\n+      seek(offset)\n+      poll(timeout)\n+      assert(buffer.hasNext(),\n+        s\"Failed to get records for $groupId $topic $partition $offset after polling for $timeout\")\n+      record = buffer.next()\n+      assert(record.offset == offset,\n+        s\"Got wrong record for $groupId $topic $partition even after seeking to offset $offset\")\n+    }\n+\n+    nextOffset = offset + 1\n+    record\n+  }\n+\n+  private def seek(offset: Long): Unit = {\n+    log.debug(s\"Seeking to $topicPartition $offset\")\n+    consumer.seek(topicPartition, offset)\n+  }\n+\n+  private def poll(timeout: Long): Unit = {\n+    val p = consumer.poll(timeout)\n+    val r = p.records(topicPartition)\n+    log.debug(s\"Polled ${p.partitions()}  ${r.size}\")\n+    buffer = r.iterator\n+  }\n+\n+}\n+\n+private[kafka]\n+object CachedKafkaConsumer extends Logging {\n+\n+  private case class CacheKey(groupId: String, topic: String, partition: Int)\n+\n+  // Don't want to depend on guava, don't want a cleanup thread, use a simple LinkedHashMap\n+  private var cache: ju.LinkedHashMap[CacheKey, CachedKafkaConsumer[_, _]] = null\n+\n+  /** Must be called before get, once per JVM, to configure the cache. Further calls are ignored */\n+  def init(\n+      initialCapacity: Int,\n+      maxCapacity: Int,\n+      loadFactor: Float): Unit = CachedKafkaConsumer.synchronized {\n+    if (null == cache) {\n+      log.info(s\"Initializing cache $initialCapacity $maxCapacity $loadFactor\")\n+      cache = new ju.LinkedHashMap[CacheKey, CachedKafkaConsumer[_, _]](\n+        initialCapacity, loadFactor, true) {\n+        override def removeEldestEntry(\n+          entry: ju.Map.Entry[CacheKey, CachedKafkaConsumer[_, _]]): Boolean = {\n+          if (this.size > maxCapacity) {\n+            entry.getValue.consumer.close()"
  }],
  "prId": 11863
}]