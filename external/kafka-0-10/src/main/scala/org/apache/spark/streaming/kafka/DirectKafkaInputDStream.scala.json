[{
  "comments": [{
    "author": {
      "login": "lfrancke"
    },
    "body": "In Scaladoc this would be `[[org.apache.spark.streaming.kafka.KafkaRDD]]` instead of `{@link ...` (I think)\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-21T07:44:51Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+import java.util.concurrent.ConcurrentLinkedQueue\n+import java.util.concurrent.atomic.AtomicReference\n+\n+import scala.annotation.tailrec\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.reflect.ClassTag\n+\n+import org.apache.kafka.clients.consumer._\n+import org.apache.kafka.common.{ PartitionInfo, TopicPartition }\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.scheduler.{RateController, StreamInputInfo}\n+import org.apache.spark.streaming.scheduler.rate.RateEstimator\n+\n+/**\n+ *  A stream of {@link org.apache.spark.streaming.kafka.KafkaRDD} where"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "This link style works, and this area of the code hasn't changed from the 0.8 version of the consumer.  Like the other cosmetic comments, I'm reluctant to change things in 0.8 version, just to keep this PR easy to compare.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-21T14:10:23Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+import java.util.concurrent.ConcurrentLinkedQueue\n+import java.util.concurrent.atomic.AtomicReference\n+\n+import scala.annotation.tailrec\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.reflect.ClassTag\n+\n+import org.apache.kafka.clients.consumer._\n+import org.apache.kafka.common.{ PartitionInfo, TopicPartition }\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.scheduler.{RateController, StreamInputInfo}\n+import org.apache.spark.streaming.scheduler.rate.RateEstimator\n+\n+/**\n+ *  A stream of {@link org.apache.spark.streaming.kafka.KafkaRDD} where"
  }, {
    "author": {
      "login": "lfrancke"
    },
    "body": "Makes sense. As I said: Only nitpicks.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-21T14:12:22Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+import java.util.concurrent.ConcurrentLinkedQueue\n+import java.util.concurrent.atomic.AtomicReference\n+\n+import scala.annotation.tailrec\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.reflect.ClassTag\n+\n+import org.apache.kafka.clients.consumer._\n+import org.apache.kafka.common.{ PartitionInfo, TopicPartition }\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.scheduler.{RateController, StreamInputInfo}\n+import org.apache.spark.streaming.scheduler.rate.RateEstimator\n+\n+/**\n+ *  A stream of {@link org.apache.spark.streaming.kafka.KafkaRDD} where"
  }],
  "prId": 11863
}, {
  "comments": [{
    "author": {
      "login": "lfrancke"
    },
    "body": "I don't know why but when I build the Scaladoc (using maven scala:doc) none of these constructor parameters are being shown. And if they were I don't think the links to the companion object would work (preferConsistent and preferBrokers) as Scaladoc currently doesn't support links to the companion object.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-21T07:53:59Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+import java.util.concurrent.ConcurrentLinkedQueue\n+import java.util.concurrent.atomic.AtomicReference\n+\n+import scala.annotation.tailrec\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.reflect.ClassTag\n+\n+import org.apache.kafka.clients.consumer._\n+import org.apache.kafka.common.{ PartitionInfo, TopicPartition }\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.scheduler.{RateController, StreamInputInfo}\n+import org.apache.spark.streaming.scheduler.rate.RateEstimator\n+\n+/**\n+ *  A stream of {@link org.apache.spark.streaming.kafka.KafkaRDD} where\n+ * each given Kafka topic/partition corresponds to an RDD partition.\n+ * The spark configuration spark.streaming.kafka.maxRatePerPartition gives the maximum number\n+ *  of messages\n+ * per second that each '''partition''' will accept.\n+ * @param preferredHosts map from TopicPartition to preferred host for processing that partition."
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "The class constructor is private.  The parameters show up in the companion object methods, and the links to the companion object work fine.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-21T14:11:22Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+import java.util.concurrent.ConcurrentLinkedQueue\n+import java.util.concurrent.atomic.AtomicReference\n+\n+import scala.annotation.tailrec\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.reflect.ClassTag\n+\n+import org.apache.kafka.clients.consumer._\n+import org.apache.kafka.common.{ PartitionInfo, TopicPartition }\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.scheduler.{RateController, StreamInputInfo}\n+import org.apache.spark.streaming.scheduler.rate.RateEstimator\n+\n+/**\n+ *  A stream of {@link org.apache.spark.streaming.kafka.KafkaRDD} where\n+ * each given Kafka topic/partition corresponds to an RDD partition.\n+ * The spark configuration spark.streaming.kafka.maxRatePerPartition gives the maximum number\n+ *  of messages\n+ * per second that each '''partition''' will accept.\n+ * @param preferredHosts map from TopicPartition to preferred host for processing that partition."
  }, {
    "author": {
      "login": "lfrancke"
    },
    "body": "But that's because the comment is duplicated there.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-21T14:12:42Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+import java.util.concurrent.ConcurrentLinkedQueue\n+import java.util.concurrent.atomic.AtomicReference\n+\n+import scala.annotation.tailrec\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.reflect.ClassTag\n+\n+import org.apache.kafka.clients.consumer._\n+import org.apache.kafka.common.{ PartitionInfo, TopicPartition }\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.scheduler.{RateController, StreamInputInfo}\n+import org.apache.spark.streaming.scheduler.rate.RateEstimator\n+\n+/**\n+ *  A stream of {@link org.apache.spark.streaming.kafka.KafkaRDD} where\n+ * each given Kafka topic/partition corresponds to an RDD partition.\n+ * The spark configuration spark.streaming.kafka.maxRatePerPartition gives the maximum number\n+ *  of messages\n+ * per second that each '''partition''' will accept.\n+ * @param preferredHosts map from TopicPartition to preferred host for processing that partition."
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "Right, I think the comments on the private class constructor are still useful for anyone looking at the code, even if they don't show up in scaladoc.  Given that the parameters aren't always the same, it's at least possibly worth the duplication.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-21T14:27:31Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+import java.util.concurrent.ConcurrentLinkedQueue\n+import java.util.concurrent.atomic.AtomicReference\n+\n+import scala.annotation.tailrec\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.reflect.ClassTag\n+\n+import org.apache.kafka.clients.consumer._\n+import org.apache.kafka.common.{ PartitionInfo, TopicPartition }\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.scheduler.{RateController, StreamInputInfo}\n+import org.apache.spark.streaming.scheduler.rate.RateEstimator\n+\n+/**\n+ *  A stream of {@link org.apache.spark.streaming.kafka.KafkaRDD} where\n+ * each given Kafka topic/partition corresponds to an RDD partition.\n+ * The spark configuration spark.streaming.kafka.maxRatePerPartition gives the maximum number\n+ *  of messages\n+ * per second that each '''partition''' will accept.\n+ * @param preferredHosts map from TopicPartition to preferred host for processing that partition."
  }],
  "prId": 11863
}, {
  "comments": [{
    "author": {
      "login": "lfrancke"
    },
    "body": "Needs return type\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-21T07:54:44Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+import java.util.concurrent.ConcurrentLinkedQueue\n+import java.util.concurrent.atomic.AtomicReference\n+\n+import scala.annotation.tailrec\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.reflect.ClassTag\n+\n+import org.apache.kafka.clients.consumer._\n+import org.apache.kafka.common.{ PartitionInfo, TopicPartition }\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.scheduler.{RateController, StreamInputInfo}\n+import org.apache.spark.streaming.scheduler.rate.RateEstimator\n+\n+/**\n+ *  A stream of {@link org.apache.spark.streaming.kafka.KafkaRDD} where\n+ * each given Kafka topic/partition corresponds to an RDD partition.\n+ * The spark configuration spark.streaming.kafka.maxRatePerPartition gives the maximum number\n+ *  of messages\n+ * per second that each '''partition''' will accept.\n+ * @param preferredHosts map from TopicPartition to preferred host for processing that partition.\n+ * In most cases, use [[DirectKafkaInputDStream.preferConsistent]]\n+ * Use [[DirectKafkaInputDStream.preferBrokers]] if your executors are on same nodes as brokers.\n+ * @param executorKafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.html#newconsumerconfigs\">\n+ * configuration parameters</a>.\n+ *   Requires  \"bootstrap.servers\" to be set with Kafka broker(s),\n+ *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+ * @param driverConsumer zero-argument function for you to construct a Kafka Consumer,\n+ *  and subscribe topics or assign partitions.\n+ *  This consumer will be used on the driver to query for offsets only, not messages.\n+ *  See <a href=\"http://kafka.apache.org/documentation.html#newconsumerapi\">Consumer doc</a>\n+ * @tparam K type of Kafka message key\n+ * @tparam V type of Kafka message value\n+ */\n+@Experimental\n+class DirectKafkaInputDStream[K: ClassTag, V: ClassTag] private[spark] (\n+    _ssc: StreamingContext,\n+    preferredHosts: ju.Map[TopicPartition, String],\n+    executorKafkaParams: ju.Map[String, Object],\n+    driverConsumer: () => Consumer[K, V]\n+  ) extends InputDStream[ConsumerRecord[K, V]](_ssc) with Logging {\n+\n+  @transient private var kc: Consumer[K, V] = null\n+  def consumer(): Consumer[K, V] = this.synchronized {\n+    if (null == kc) {\n+      kc = driverConsumer()\n+    }\n+    kc\n+  }\n+  consumer()\n+\n+  override def persist(newLevel: StorageLevel): DStream[ConsumerRecord[K, V]] = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  protected def getBrokers = {\n+    val c = consumer\n+    val result = new ju.HashMap[TopicPartition, String]()\n+    val hosts = new ju.HashMap[TopicPartition, String]()\n+    val assignments = c.assignment().iterator()\n+    while (assignments.hasNext()) {\n+      val tp: TopicPartition = assignments.next()\n+      if (null == hosts.get(tp)) {\n+        val infos = c.partitionsFor(tp.topic).iterator()\n+        while (infos.hasNext()) {\n+          val i = infos.next()\n+          hosts.put(new TopicPartition(i.topic(), i.partition()), i.leader.host())\n+        }\n+      }\n+      result.put(tp, hosts.get(tp))\n+    }\n+    result\n+  }\n+\n+  protected def getPreferredHosts: ju.Map[TopicPartition, String] = {\n+    if (preferredHosts == DirectKafkaInputDStream.preferBrokers) {\n+      getBrokers\n+    } else {\n+      preferredHosts\n+    }\n+  }\n+\n+  // Keep this consistent with how other streams are named (e.g. \"Flume polling stream [2]\")\n+  private[streaming] override def name: String = s\"Kafka 0.10 direct stream [$id]\"\n+\n+  protected[streaming] override val checkpointData =\n+    new DirectKafkaInputDStreamCheckpointData\n+\n+\n+  /**\n+   * Asynchronously maintains & sends new rate limits to the receiver through the receiver tracker.\n+   */\n+  override protected[streaming] val rateController: Option[RateController] = {\n+    if (RateController.isBackPressureEnabled(ssc.conf)) {\n+      Some(new DirectKafkaRateController(id,\n+        RateEstimator.create(ssc.conf, context.graph.batchDuration)))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  private val maxRateLimitPerPartition: Int = context.sparkContext.getConf.getInt(\n+    \"spark.streaming.kafka.maxRatePerPartition\", 0)\n+\n+  protected[streaming] def maxMessagesPerPartition(\n+    offsets: Map[TopicPartition, Long]): Option[Map[TopicPartition, Long]] = {\n+    val estimatedRateLimit = rateController.map(_.getLatestRate().toInt)\n+\n+    // calculate a per-partition rate limit based on current lag\n+    val effectiveRateLimitPerPartition = estimatedRateLimit.filter(_ > 0) match {\n+      case Some(rate) =>\n+        val lagPerPartition = offsets.map { case (tp, offset) =>\n+          tp -> Math.max(offset - currentOffsets(tp), 0)\n+        }\n+        val totalLag = lagPerPartition.values.sum\n+\n+        lagPerPartition.map { case (tp, lag) =>\n+          val backpressureRate = Math.round(lag / totalLag.toFloat * rate)\n+          tp -> (if (maxRateLimitPerPartition > 0) {\n+            Math.min(backpressureRate, maxRateLimitPerPartition)} else backpressureRate)\n+        }\n+      case None => offsets.map { case (tp, offset) => tp -> maxRateLimitPerPartition }\n+    }\n+\n+    if (effectiveRateLimitPerPartition.values.sum > 0) {\n+      val secsPerBatch = context.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some(effectiveRateLimitPerPartition.map {\n+        case (tp, limit) => tp -> (secsPerBatch * limit).toLong\n+      })\n+    } else {\n+      None\n+    }\n+  }\n+\n+  protected var currentOffsets = Map[TopicPartition, Long]()\n+\n+  protected def latestOffsets(): Map[TopicPartition, Long] = {\n+    val c = consumer\n+    c.poll(0)\n+    val parts = c.assignment().asScala\n+\n+    // make sure new partitions are reflected in currentOffsets\n+    val newPartitions = parts.diff(currentOffsets.keySet)\n+    currentOffsets = currentOffsets ++ newPartitions.map(tp => tp -> c.position(tp)).toMap\n+    c.pause(newPartitions.asJava)\n+\n+    c.seekToEnd(currentOffsets.keySet.asJava)\n+    parts.map(tp => tp -> c.position(tp)).toMap\n+  }\n+\n+  // limits the maximum number of messages per partition\n+  protected def clamp(\n+    offsets: Map[TopicPartition, Long]): Map[TopicPartition, Long] = {\n+\n+    maxMessagesPerPartition(offsets).map { mmp =>\n+      mmp.map { case (tp, messages) =>\n+          val uo = offsets(tp)\n+          tp -> Math.min(currentOffsets(tp) + messages, uo)\n+      }\n+    }.getOrElse(offsets)\n+  }\n+\n+  override def compute(validTime: Time): Option[KafkaRDD[K, V]] = {\n+    val untilOffsets = clamp(latestOffsets())\n+    val offsetRanges = untilOffsets.map { case (tp, uo) =>\n+      val fo = currentOffsets(tp)\n+      OffsetRange(tp.topic, tp.partition, fo, uo)\n+    }\n+    val rdd = new KafkaRDD[K, V](\n+      context.sparkContext, executorKafkaParams, offsetRanges.toArray, getPreferredHosts, true)\n+\n+    // Report the record number and metadata of this batch interval to InputInfoTracker.\n+    val description = offsetRanges.filter { offsetRange =>\n+      // Don't display empty ranges.\n+      offsetRange.fromOffset != offsetRange.untilOffset\n+    }.map { offsetRange =>\n+      s\"topic: ${offsetRange.topic}\\tpartition: ${offsetRange.partition}\\t\" +\n+        s\"offsets: ${offsetRange.fromOffset} to ${offsetRange.untilOffset}\"\n+    }.mkString(\"\\n\")\n+    // Copy offsetRanges to immutable.List to prevent from being modified by the user\n+    val metadata = Map(\n+      \"offsets\" -> offsetRanges.toList,\n+      StreamInputInfo.METADATA_KEY_DESCRIPTION -> description)\n+    val inputInfo = StreamInputInfo(id, rdd.count, metadata)\n+    ssc.scheduler.inputInfoTracker.reportInfo(validTime, inputInfo)\n+\n+    currentOffsets = untilOffsets\n+    commitAll()\n+    Some(rdd)\n+  }\n+\n+  override def start(): Unit = {\n+    val c = consumer\n+    c.poll(0)\n+    if (currentOffsets.isEmpty) {\n+      currentOffsets = c.assignment().asScala.map { tp =>\n+        tp -> c.position(tp)\n+      }.toMap\n+    }\n+\n+    // don't actually want to consume any messages, so pause all partitions\n+    c.pause(currentOffsets.keySet.asJava)\n+  }\n+\n+  override def stop(): Unit = this.synchronized {\n+    if (kc != null) {\n+      kc.close()\n+    }\n+  }\n+\n+  protected val commitQueue = new ConcurrentLinkedQueue[OffsetRange]\n+  protected val commitCallback = new AtomicReference[OffsetCommitCallback]\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange]): Unit = {\n+    commitAsync(offsetRanges, null)\n+  }\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   * @param callback Only the most recently provided callback will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange], callback: OffsetCommitCallback): Unit = {\n+    commitCallback.set(callback)\n+    commitQueue.addAll(ju.Arrays.asList(offsetRanges: _*))\n+  }\n+\n+  protected def commitAll(): Unit = {\n+    val m = new ju.HashMap[TopicPartition, OffsetAndMetadata]()\n+    val it = commitQueue.iterator()\n+    while (it.hasNext) {\n+      val osr = it.next\n+      val tp = osr.topicPartition\n+      val x = m.get(tp)\n+      val offset = if (null == x) { osr.untilOffset } else { Math.max(x.offset, osr.untilOffset) }\n+      m.put(tp, new OffsetAndMetadata(offset))\n+    }\n+    if (!m.isEmpty) {\n+      consumer.commitAsync(m, commitCallback.get)\n+    }\n+  }\n+\n+  private[streaming]\n+  class DirectKafkaInputDStreamCheckpointData extends DStreamCheckpointData(this) {\n+    def batchForTime: mutable.HashMap[Time, Array[(String, Int, Long, Long)]] = {\n+      data.asInstanceOf[mutable.HashMap[Time, Array[OffsetRange.OffsetRangeTuple]]]\n+    }\n+\n+    override def update(time: Time) {\n+      batchForTime.clear()\n+      generatedRDDs.foreach { kv =>\n+        val a = kv._2.asInstanceOf[KafkaRDD[K, V]].offsetRanges.map(_.toTuple).toArray\n+        batchForTime += kv._1 -> a\n+      }\n+    }\n+\n+    override def cleanup(time: Time) { }\n+\n+    override def restore() {"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "Again, unchanged from 0.8 version\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-21T14:18:29Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+import java.util.concurrent.ConcurrentLinkedQueue\n+import java.util.concurrent.atomic.AtomicReference\n+\n+import scala.annotation.tailrec\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.reflect.ClassTag\n+\n+import org.apache.kafka.clients.consumer._\n+import org.apache.kafka.common.{ PartitionInfo, TopicPartition }\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.scheduler.{RateController, StreamInputInfo}\n+import org.apache.spark.streaming.scheduler.rate.RateEstimator\n+\n+/**\n+ *  A stream of {@link org.apache.spark.streaming.kafka.KafkaRDD} where\n+ * each given Kafka topic/partition corresponds to an RDD partition.\n+ * The spark configuration spark.streaming.kafka.maxRatePerPartition gives the maximum number\n+ *  of messages\n+ * per second that each '''partition''' will accept.\n+ * @param preferredHosts map from TopicPartition to preferred host for processing that partition.\n+ * In most cases, use [[DirectKafkaInputDStream.preferConsistent]]\n+ * Use [[DirectKafkaInputDStream.preferBrokers]] if your executors are on same nodes as brokers.\n+ * @param executorKafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.html#newconsumerconfigs\">\n+ * configuration parameters</a>.\n+ *   Requires  \"bootstrap.servers\" to be set with Kafka broker(s),\n+ *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+ * @param driverConsumer zero-argument function for you to construct a Kafka Consumer,\n+ *  and subscribe topics or assign partitions.\n+ *  This consumer will be used on the driver to query for offsets only, not messages.\n+ *  See <a href=\"http://kafka.apache.org/documentation.html#newconsumerapi\">Consumer doc</a>\n+ * @tparam K type of Kafka message key\n+ * @tparam V type of Kafka message value\n+ */\n+@Experimental\n+class DirectKafkaInputDStream[K: ClassTag, V: ClassTag] private[spark] (\n+    _ssc: StreamingContext,\n+    preferredHosts: ju.Map[TopicPartition, String],\n+    executorKafkaParams: ju.Map[String, Object],\n+    driverConsumer: () => Consumer[K, V]\n+  ) extends InputDStream[ConsumerRecord[K, V]](_ssc) with Logging {\n+\n+  @transient private var kc: Consumer[K, V] = null\n+  def consumer(): Consumer[K, V] = this.synchronized {\n+    if (null == kc) {\n+      kc = driverConsumer()\n+    }\n+    kc\n+  }\n+  consumer()\n+\n+  override def persist(newLevel: StorageLevel): DStream[ConsumerRecord[K, V]] = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  protected def getBrokers = {\n+    val c = consumer\n+    val result = new ju.HashMap[TopicPartition, String]()\n+    val hosts = new ju.HashMap[TopicPartition, String]()\n+    val assignments = c.assignment().iterator()\n+    while (assignments.hasNext()) {\n+      val tp: TopicPartition = assignments.next()\n+      if (null == hosts.get(tp)) {\n+        val infos = c.partitionsFor(tp.topic).iterator()\n+        while (infos.hasNext()) {\n+          val i = infos.next()\n+          hosts.put(new TopicPartition(i.topic(), i.partition()), i.leader.host())\n+        }\n+      }\n+      result.put(tp, hosts.get(tp))\n+    }\n+    result\n+  }\n+\n+  protected def getPreferredHosts: ju.Map[TopicPartition, String] = {\n+    if (preferredHosts == DirectKafkaInputDStream.preferBrokers) {\n+      getBrokers\n+    } else {\n+      preferredHosts\n+    }\n+  }\n+\n+  // Keep this consistent with how other streams are named (e.g. \"Flume polling stream [2]\")\n+  private[streaming] override def name: String = s\"Kafka 0.10 direct stream [$id]\"\n+\n+  protected[streaming] override val checkpointData =\n+    new DirectKafkaInputDStreamCheckpointData\n+\n+\n+  /**\n+   * Asynchronously maintains & sends new rate limits to the receiver through the receiver tracker.\n+   */\n+  override protected[streaming] val rateController: Option[RateController] = {\n+    if (RateController.isBackPressureEnabled(ssc.conf)) {\n+      Some(new DirectKafkaRateController(id,\n+        RateEstimator.create(ssc.conf, context.graph.batchDuration)))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  private val maxRateLimitPerPartition: Int = context.sparkContext.getConf.getInt(\n+    \"spark.streaming.kafka.maxRatePerPartition\", 0)\n+\n+  protected[streaming] def maxMessagesPerPartition(\n+    offsets: Map[TopicPartition, Long]): Option[Map[TopicPartition, Long]] = {\n+    val estimatedRateLimit = rateController.map(_.getLatestRate().toInt)\n+\n+    // calculate a per-partition rate limit based on current lag\n+    val effectiveRateLimitPerPartition = estimatedRateLimit.filter(_ > 0) match {\n+      case Some(rate) =>\n+        val lagPerPartition = offsets.map { case (tp, offset) =>\n+          tp -> Math.max(offset - currentOffsets(tp), 0)\n+        }\n+        val totalLag = lagPerPartition.values.sum\n+\n+        lagPerPartition.map { case (tp, lag) =>\n+          val backpressureRate = Math.round(lag / totalLag.toFloat * rate)\n+          tp -> (if (maxRateLimitPerPartition > 0) {\n+            Math.min(backpressureRate, maxRateLimitPerPartition)} else backpressureRate)\n+        }\n+      case None => offsets.map { case (tp, offset) => tp -> maxRateLimitPerPartition }\n+    }\n+\n+    if (effectiveRateLimitPerPartition.values.sum > 0) {\n+      val secsPerBatch = context.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some(effectiveRateLimitPerPartition.map {\n+        case (tp, limit) => tp -> (secsPerBatch * limit).toLong\n+      })\n+    } else {\n+      None\n+    }\n+  }\n+\n+  protected var currentOffsets = Map[TopicPartition, Long]()\n+\n+  protected def latestOffsets(): Map[TopicPartition, Long] = {\n+    val c = consumer\n+    c.poll(0)\n+    val parts = c.assignment().asScala\n+\n+    // make sure new partitions are reflected in currentOffsets\n+    val newPartitions = parts.diff(currentOffsets.keySet)\n+    currentOffsets = currentOffsets ++ newPartitions.map(tp => tp -> c.position(tp)).toMap\n+    c.pause(newPartitions.asJava)\n+\n+    c.seekToEnd(currentOffsets.keySet.asJava)\n+    parts.map(tp => tp -> c.position(tp)).toMap\n+  }\n+\n+  // limits the maximum number of messages per partition\n+  protected def clamp(\n+    offsets: Map[TopicPartition, Long]): Map[TopicPartition, Long] = {\n+\n+    maxMessagesPerPartition(offsets).map { mmp =>\n+      mmp.map { case (tp, messages) =>\n+          val uo = offsets(tp)\n+          tp -> Math.min(currentOffsets(tp) + messages, uo)\n+      }\n+    }.getOrElse(offsets)\n+  }\n+\n+  override def compute(validTime: Time): Option[KafkaRDD[K, V]] = {\n+    val untilOffsets = clamp(latestOffsets())\n+    val offsetRanges = untilOffsets.map { case (tp, uo) =>\n+      val fo = currentOffsets(tp)\n+      OffsetRange(tp.topic, tp.partition, fo, uo)\n+    }\n+    val rdd = new KafkaRDD[K, V](\n+      context.sparkContext, executorKafkaParams, offsetRanges.toArray, getPreferredHosts, true)\n+\n+    // Report the record number and metadata of this batch interval to InputInfoTracker.\n+    val description = offsetRanges.filter { offsetRange =>\n+      // Don't display empty ranges.\n+      offsetRange.fromOffset != offsetRange.untilOffset\n+    }.map { offsetRange =>\n+      s\"topic: ${offsetRange.topic}\\tpartition: ${offsetRange.partition}\\t\" +\n+        s\"offsets: ${offsetRange.fromOffset} to ${offsetRange.untilOffset}\"\n+    }.mkString(\"\\n\")\n+    // Copy offsetRanges to immutable.List to prevent from being modified by the user\n+    val metadata = Map(\n+      \"offsets\" -> offsetRanges.toList,\n+      StreamInputInfo.METADATA_KEY_DESCRIPTION -> description)\n+    val inputInfo = StreamInputInfo(id, rdd.count, metadata)\n+    ssc.scheduler.inputInfoTracker.reportInfo(validTime, inputInfo)\n+\n+    currentOffsets = untilOffsets\n+    commitAll()\n+    Some(rdd)\n+  }\n+\n+  override def start(): Unit = {\n+    val c = consumer\n+    c.poll(0)\n+    if (currentOffsets.isEmpty) {\n+      currentOffsets = c.assignment().asScala.map { tp =>\n+        tp -> c.position(tp)\n+      }.toMap\n+    }\n+\n+    // don't actually want to consume any messages, so pause all partitions\n+    c.pause(currentOffsets.keySet.asJava)\n+  }\n+\n+  override def stop(): Unit = this.synchronized {\n+    if (kc != null) {\n+      kc.close()\n+    }\n+  }\n+\n+  protected val commitQueue = new ConcurrentLinkedQueue[OffsetRange]\n+  protected val commitCallback = new AtomicReference[OffsetCommitCallback]\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange]): Unit = {\n+    commitAsync(offsetRanges, null)\n+  }\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   * @param callback Only the most recently provided callback will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange], callback: OffsetCommitCallback): Unit = {\n+    commitCallback.set(callback)\n+    commitQueue.addAll(ju.Arrays.asList(offsetRanges: _*))\n+  }\n+\n+  protected def commitAll(): Unit = {\n+    val m = new ju.HashMap[TopicPartition, OffsetAndMetadata]()\n+    val it = commitQueue.iterator()\n+    while (it.hasNext) {\n+      val osr = it.next\n+      val tp = osr.topicPartition\n+      val x = m.get(tp)\n+      val offset = if (null == x) { osr.untilOffset } else { Math.max(x.offset, osr.untilOffset) }\n+      m.put(tp, new OffsetAndMetadata(offset))\n+    }\n+    if (!m.isEmpty) {\n+      consumer.commitAsync(m, commitCallback.get)\n+    }\n+  }\n+\n+  private[streaming]\n+  class DirectKafkaInputDStreamCheckpointData extends DStreamCheckpointData(this) {\n+    def batchForTime: mutable.HashMap[Time, Array[(String, Int, Long, Long)]] = {\n+      data.asInstanceOf[mutable.HashMap[Time, Array[OffsetRange.OffsetRangeTuple]]]\n+    }\n+\n+    override def update(time: Time) {\n+      batchForTime.clear()\n+      generatedRDDs.foreach { kv =>\n+        val a = kv._2.asInstanceOf[KafkaRDD[K, V]].offsetRanges.map(_.toTuple).toArray\n+        batchForTime += kv._1 -> a\n+      }\n+    }\n+\n+    override def cleanup(time: Time) { }\n+\n+    override def restore() {"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "I dont think we should use that excuse. Mistakes made in the past should not be repeated. Lets fix some of these local changes along the way. We can fix 0.8 copy later. \n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-22T22:28:58Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+import java.util.concurrent.ConcurrentLinkedQueue\n+import java.util.concurrent.atomic.AtomicReference\n+\n+import scala.annotation.tailrec\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.reflect.ClassTag\n+\n+import org.apache.kafka.clients.consumer._\n+import org.apache.kafka.common.{ PartitionInfo, TopicPartition }\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.scheduler.{RateController, StreamInputInfo}\n+import org.apache.spark.streaming.scheduler.rate.RateEstimator\n+\n+/**\n+ *  A stream of {@link org.apache.spark.streaming.kafka.KafkaRDD} where\n+ * each given Kafka topic/partition corresponds to an RDD partition.\n+ * The spark configuration spark.streaming.kafka.maxRatePerPartition gives the maximum number\n+ *  of messages\n+ * per second that each '''partition''' will accept.\n+ * @param preferredHosts map from TopicPartition to preferred host for processing that partition.\n+ * In most cases, use [[DirectKafkaInputDStream.preferConsistent]]\n+ * Use [[DirectKafkaInputDStream.preferBrokers]] if your executors are on same nodes as brokers.\n+ * @param executorKafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.html#newconsumerconfigs\">\n+ * configuration parameters</a>.\n+ *   Requires  \"bootstrap.servers\" to be set with Kafka broker(s),\n+ *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+ * @param driverConsumer zero-argument function for you to construct a Kafka Consumer,\n+ *  and subscribe topics or assign partitions.\n+ *  This consumer will be used on the driver to query for offsets only, not messages.\n+ *  See <a href=\"http://kafka.apache.org/documentation.html#newconsumerapi\">Consumer doc</a>\n+ * @tparam K type of Kafka message key\n+ * @tparam V type of Kafka message value\n+ */\n+@Experimental\n+class DirectKafkaInputDStream[K: ClassTag, V: ClassTag] private[spark] (\n+    _ssc: StreamingContext,\n+    preferredHosts: ju.Map[TopicPartition, String],\n+    executorKafkaParams: ju.Map[String, Object],\n+    driverConsumer: () => Consumer[K, V]\n+  ) extends InputDStream[ConsumerRecord[K, V]](_ssc) with Logging {\n+\n+  @transient private var kc: Consumer[K, V] = null\n+  def consumer(): Consumer[K, V] = this.synchronized {\n+    if (null == kc) {\n+      kc = driverConsumer()\n+    }\n+    kc\n+  }\n+  consumer()\n+\n+  override def persist(newLevel: StorageLevel): DStream[ConsumerRecord[K, V]] = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  protected def getBrokers = {\n+    val c = consumer\n+    val result = new ju.HashMap[TopicPartition, String]()\n+    val hosts = new ju.HashMap[TopicPartition, String]()\n+    val assignments = c.assignment().iterator()\n+    while (assignments.hasNext()) {\n+      val tp: TopicPartition = assignments.next()\n+      if (null == hosts.get(tp)) {\n+        val infos = c.partitionsFor(tp.topic).iterator()\n+        while (infos.hasNext()) {\n+          val i = infos.next()\n+          hosts.put(new TopicPartition(i.topic(), i.partition()), i.leader.host())\n+        }\n+      }\n+      result.put(tp, hosts.get(tp))\n+    }\n+    result\n+  }\n+\n+  protected def getPreferredHosts: ju.Map[TopicPartition, String] = {\n+    if (preferredHosts == DirectKafkaInputDStream.preferBrokers) {\n+      getBrokers\n+    } else {\n+      preferredHosts\n+    }\n+  }\n+\n+  // Keep this consistent with how other streams are named (e.g. \"Flume polling stream [2]\")\n+  private[streaming] override def name: String = s\"Kafka 0.10 direct stream [$id]\"\n+\n+  protected[streaming] override val checkpointData =\n+    new DirectKafkaInputDStreamCheckpointData\n+\n+\n+  /**\n+   * Asynchronously maintains & sends new rate limits to the receiver through the receiver tracker.\n+   */\n+  override protected[streaming] val rateController: Option[RateController] = {\n+    if (RateController.isBackPressureEnabled(ssc.conf)) {\n+      Some(new DirectKafkaRateController(id,\n+        RateEstimator.create(ssc.conf, context.graph.batchDuration)))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  private val maxRateLimitPerPartition: Int = context.sparkContext.getConf.getInt(\n+    \"spark.streaming.kafka.maxRatePerPartition\", 0)\n+\n+  protected[streaming] def maxMessagesPerPartition(\n+    offsets: Map[TopicPartition, Long]): Option[Map[TopicPartition, Long]] = {\n+    val estimatedRateLimit = rateController.map(_.getLatestRate().toInt)\n+\n+    // calculate a per-partition rate limit based on current lag\n+    val effectiveRateLimitPerPartition = estimatedRateLimit.filter(_ > 0) match {\n+      case Some(rate) =>\n+        val lagPerPartition = offsets.map { case (tp, offset) =>\n+          tp -> Math.max(offset - currentOffsets(tp), 0)\n+        }\n+        val totalLag = lagPerPartition.values.sum\n+\n+        lagPerPartition.map { case (tp, lag) =>\n+          val backpressureRate = Math.round(lag / totalLag.toFloat * rate)\n+          tp -> (if (maxRateLimitPerPartition > 0) {\n+            Math.min(backpressureRate, maxRateLimitPerPartition)} else backpressureRate)\n+        }\n+      case None => offsets.map { case (tp, offset) => tp -> maxRateLimitPerPartition }\n+    }\n+\n+    if (effectiveRateLimitPerPartition.values.sum > 0) {\n+      val secsPerBatch = context.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some(effectiveRateLimitPerPartition.map {\n+        case (tp, limit) => tp -> (secsPerBatch * limit).toLong\n+      })\n+    } else {\n+      None\n+    }\n+  }\n+\n+  protected var currentOffsets = Map[TopicPartition, Long]()\n+\n+  protected def latestOffsets(): Map[TopicPartition, Long] = {\n+    val c = consumer\n+    c.poll(0)\n+    val parts = c.assignment().asScala\n+\n+    // make sure new partitions are reflected in currentOffsets\n+    val newPartitions = parts.diff(currentOffsets.keySet)\n+    currentOffsets = currentOffsets ++ newPartitions.map(tp => tp -> c.position(tp)).toMap\n+    c.pause(newPartitions.asJava)\n+\n+    c.seekToEnd(currentOffsets.keySet.asJava)\n+    parts.map(tp => tp -> c.position(tp)).toMap\n+  }\n+\n+  // limits the maximum number of messages per partition\n+  protected def clamp(\n+    offsets: Map[TopicPartition, Long]): Map[TopicPartition, Long] = {\n+\n+    maxMessagesPerPartition(offsets).map { mmp =>\n+      mmp.map { case (tp, messages) =>\n+          val uo = offsets(tp)\n+          tp -> Math.min(currentOffsets(tp) + messages, uo)\n+      }\n+    }.getOrElse(offsets)\n+  }\n+\n+  override def compute(validTime: Time): Option[KafkaRDD[K, V]] = {\n+    val untilOffsets = clamp(latestOffsets())\n+    val offsetRanges = untilOffsets.map { case (tp, uo) =>\n+      val fo = currentOffsets(tp)\n+      OffsetRange(tp.topic, tp.partition, fo, uo)\n+    }\n+    val rdd = new KafkaRDD[K, V](\n+      context.sparkContext, executorKafkaParams, offsetRanges.toArray, getPreferredHosts, true)\n+\n+    // Report the record number and metadata of this batch interval to InputInfoTracker.\n+    val description = offsetRanges.filter { offsetRange =>\n+      // Don't display empty ranges.\n+      offsetRange.fromOffset != offsetRange.untilOffset\n+    }.map { offsetRange =>\n+      s\"topic: ${offsetRange.topic}\\tpartition: ${offsetRange.partition}\\t\" +\n+        s\"offsets: ${offsetRange.fromOffset} to ${offsetRange.untilOffset}\"\n+    }.mkString(\"\\n\")\n+    // Copy offsetRanges to immutable.List to prevent from being modified by the user\n+    val metadata = Map(\n+      \"offsets\" -> offsetRanges.toList,\n+      StreamInputInfo.METADATA_KEY_DESCRIPTION -> description)\n+    val inputInfo = StreamInputInfo(id, rdd.count, metadata)\n+    ssc.scheduler.inputInfoTracker.reportInfo(validTime, inputInfo)\n+\n+    currentOffsets = untilOffsets\n+    commitAll()\n+    Some(rdd)\n+  }\n+\n+  override def start(): Unit = {\n+    val c = consumer\n+    c.poll(0)\n+    if (currentOffsets.isEmpty) {\n+      currentOffsets = c.assignment().asScala.map { tp =>\n+        tp -> c.position(tp)\n+      }.toMap\n+    }\n+\n+    // don't actually want to consume any messages, so pause all partitions\n+    c.pause(currentOffsets.keySet.asJava)\n+  }\n+\n+  override def stop(): Unit = this.synchronized {\n+    if (kc != null) {\n+      kc.close()\n+    }\n+  }\n+\n+  protected val commitQueue = new ConcurrentLinkedQueue[OffsetRange]\n+  protected val commitCallback = new AtomicReference[OffsetCommitCallback]\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange]): Unit = {\n+    commitAsync(offsetRanges, null)\n+  }\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   * @param callback Only the most recently provided callback will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange], callback: OffsetCommitCallback): Unit = {\n+    commitCallback.set(callback)\n+    commitQueue.addAll(ju.Arrays.asList(offsetRanges: _*))\n+  }\n+\n+  protected def commitAll(): Unit = {\n+    val m = new ju.HashMap[TopicPartition, OffsetAndMetadata]()\n+    val it = commitQueue.iterator()\n+    while (it.hasNext) {\n+      val osr = it.next\n+      val tp = osr.topicPartition\n+      val x = m.get(tp)\n+      val offset = if (null == x) { osr.untilOffset } else { Math.max(x.offset, osr.untilOffset) }\n+      m.put(tp, new OffsetAndMetadata(offset))\n+    }\n+    if (!m.isEmpty) {\n+      consumer.commitAsync(m, commitCallback.get)\n+    }\n+  }\n+\n+  private[streaming]\n+  class DirectKafkaInputDStreamCheckpointData extends DStreamCheckpointData(this) {\n+    def batchForTime: mutable.HashMap[Time, Array[(String, Int, Long, Long)]] = {\n+      data.asInstanceOf[mutable.HashMap[Time, Array[OffsetRange.OffsetRangeTuple]]]\n+    }\n+\n+    override def update(time: Time) {\n+      batchForTime.clear()\n+      generatedRDDs.foreach { kv =>\n+        val a = kv._2.asInstanceOf[KafkaRDD[K, V]].offsetRanges.map(_.toTuple).toArray\n+        batchForTime += kv._1 -> a\n+      }\n+    }\n+\n+    override def cleanup(time: Time) { }\n+\n+    override def restore() {"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "I'll open a separate pr for 0.8 cleanups then\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-23T00:59:52Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+import java.util.concurrent.ConcurrentLinkedQueue\n+import java.util.concurrent.atomic.AtomicReference\n+\n+import scala.annotation.tailrec\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.reflect.ClassTag\n+\n+import org.apache.kafka.clients.consumer._\n+import org.apache.kafka.common.{ PartitionInfo, TopicPartition }\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.scheduler.{RateController, StreamInputInfo}\n+import org.apache.spark.streaming.scheduler.rate.RateEstimator\n+\n+/**\n+ *  A stream of {@link org.apache.spark.streaming.kafka.KafkaRDD} where\n+ * each given Kafka topic/partition corresponds to an RDD partition.\n+ * The spark configuration spark.streaming.kafka.maxRatePerPartition gives the maximum number\n+ *  of messages\n+ * per second that each '''partition''' will accept.\n+ * @param preferredHosts map from TopicPartition to preferred host for processing that partition.\n+ * In most cases, use [[DirectKafkaInputDStream.preferConsistent]]\n+ * Use [[DirectKafkaInputDStream.preferBrokers]] if your executors are on same nodes as brokers.\n+ * @param executorKafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.html#newconsumerconfigs\">\n+ * configuration parameters</a>.\n+ *   Requires  \"bootstrap.servers\" to be set with Kafka broker(s),\n+ *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+ * @param driverConsumer zero-argument function for you to construct a Kafka Consumer,\n+ *  and subscribe topics or assign partitions.\n+ *  This consumer will be used on the driver to query for offsets only, not messages.\n+ *  See <a href=\"http://kafka.apache.org/documentation.html#newconsumerapi\">Consumer doc</a>\n+ * @tparam K type of Kafka message key\n+ * @tparam V type of Kafka message value\n+ */\n+@Experimental\n+class DirectKafkaInputDStream[K: ClassTag, V: ClassTag] private[spark] (\n+    _ssc: StreamingContext,\n+    preferredHosts: ju.Map[TopicPartition, String],\n+    executorKafkaParams: ju.Map[String, Object],\n+    driverConsumer: () => Consumer[K, V]\n+  ) extends InputDStream[ConsumerRecord[K, V]](_ssc) with Logging {\n+\n+  @transient private var kc: Consumer[K, V] = null\n+  def consumer(): Consumer[K, V] = this.synchronized {\n+    if (null == kc) {\n+      kc = driverConsumer()\n+    }\n+    kc\n+  }\n+  consumer()\n+\n+  override def persist(newLevel: StorageLevel): DStream[ConsumerRecord[K, V]] = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  protected def getBrokers = {\n+    val c = consumer\n+    val result = new ju.HashMap[TopicPartition, String]()\n+    val hosts = new ju.HashMap[TopicPartition, String]()\n+    val assignments = c.assignment().iterator()\n+    while (assignments.hasNext()) {\n+      val tp: TopicPartition = assignments.next()\n+      if (null == hosts.get(tp)) {\n+        val infos = c.partitionsFor(tp.topic).iterator()\n+        while (infos.hasNext()) {\n+          val i = infos.next()\n+          hosts.put(new TopicPartition(i.topic(), i.partition()), i.leader.host())\n+        }\n+      }\n+      result.put(tp, hosts.get(tp))\n+    }\n+    result\n+  }\n+\n+  protected def getPreferredHosts: ju.Map[TopicPartition, String] = {\n+    if (preferredHosts == DirectKafkaInputDStream.preferBrokers) {\n+      getBrokers\n+    } else {\n+      preferredHosts\n+    }\n+  }\n+\n+  // Keep this consistent with how other streams are named (e.g. \"Flume polling stream [2]\")\n+  private[streaming] override def name: String = s\"Kafka 0.10 direct stream [$id]\"\n+\n+  protected[streaming] override val checkpointData =\n+    new DirectKafkaInputDStreamCheckpointData\n+\n+\n+  /**\n+   * Asynchronously maintains & sends new rate limits to the receiver through the receiver tracker.\n+   */\n+  override protected[streaming] val rateController: Option[RateController] = {\n+    if (RateController.isBackPressureEnabled(ssc.conf)) {\n+      Some(new DirectKafkaRateController(id,\n+        RateEstimator.create(ssc.conf, context.graph.batchDuration)))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  private val maxRateLimitPerPartition: Int = context.sparkContext.getConf.getInt(\n+    \"spark.streaming.kafka.maxRatePerPartition\", 0)\n+\n+  protected[streaming] def maxMessagesPerPartition(\n+    offsets: Map[TopicPartition, Long]): Option[Map[TopicPartition, Long]] = {\n+    val estimatedRateLimit = rateController.map(_.getLatestRate().toInt)\n+\n+    // calculate a per-partition rate limit based on current lag\n+    val effectiveRateLimitPerPartition = estimatedRateLimit.filter(_ > 0) match {\n+      case Some(rate) =>\n+        val lagPerPartition = offsets.map { case (tp, offset) =>\n+          tp -> Math.max(offset - currentOffsets(tp), 0)\n+        }\n+        val totalLag = lagPerPartition.values.sum\n+\n+        lagPerPartition.map { case (tp, lag) =>\n+          val backpressureRate = Math.round(lag / totalLag.toFloat * rate)\n+          tp -> (if (maxRateLimitPerPartition > 0) {\n+            Math.min(backpressureRate, maxRateLimitPerPartition)} else backpressureRate)\n+        }\n+      case None => offsets.map { case (tp, offset) => tp -> maxRateLimitPerPartition }\n+    }\n+\n+    if (effectiveRateLimitPerPartition.values.sum > 0) {\n+      val secsPerBatch = context.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some(effectiveRateLimitPerPartition.map {\n+        case (tp, limit) => tp -> (secsPerBatch * limit).toLong\n+      })\n+    } else {\n+      None\n+    }\n+  }\n+\n+  protected var currentOffsets = Map[TopicPartition, Long]()\n+\n+  protected def latestOffsets(): Map[TopicPartition, Long] = {\n+    val c = consumer\n+    c.poll(0)\n+    val parts = c.assignment().asScala\n+\n+    // make sure new partitions are reflected in currentOffsets\n+    val newPartitions = parts.diff(currentOffsets.keySet)\n+    currentOffsets = currentOffsets ++ newPartitions.map(tp => tp -> c.position(tp)).toMap\n+    c.pause(newPartitions.asJava)\n+\n+    c.seekToEnd(currentOffsets.keySet.asJava)\n+    parts.map(tp => tp -> c.position(tp)).toMap\n+  }\n+\n+  // limits the maximum number of messages per partition\n+  protected def clamp(\n+    offsets: Map[TopicPartition, Long]): Map[TopicPartition, Long] = {\n+\n+    maxMessagesPerPartition(offsets).map { mmp =>\n+      mmp.map { case (tp, messages) =>\n+          val uo = offsets(tp)\n+          tp -> Math.min(currentOffsets(tp) + messages, uo)\n+      }\n+    }.getOrElse(offsets)\n+  }\n+\n+  override def compute(validTime: Time): Option[KafkaRDD[K, V]] = {\n+    val untilOffsets = clamp(latestOffsets())\n+    val offsetRanges = untilOffsets.map { case (tp, uo) =>\n+      val fo = currentOffsets(tp)\n+      OffsetRange(tp.topic, tp.partition, fo, uo)\n+    }\n+    val rdd = new KafkaRDD[K, V](\n+      context.sparkContext, executorKafkaParams, offsetRanges.toArray, getPreferredHosts, true)\n+\n+    // Report the record number and metadata of this batch interval to InputInfoTracker.\n+    val description = offsetRanges.filter { offsetRange =>\n+      // Don't display empty ranges.\n+      offsetRange.fromOffset != offsetRange.untilOffset\n+    }.map { offsetRange =>\n+      s\"topic: ${offsetRange.topic}\\tpartition: ${offsetRange.partition}\\t\" +\n+        s\"offsets: ${offsetRange.fromOffset} to ${offsetRange.untilOffset}\"\n+    }.mkString(\"\\n\")\n+    // Copy offsetRanges to immutable.List to prevent from being modified by the user\n+    val metadata = Map(\n+      \"offsets\" -> offsetRanges.toList,\n+      StreamInputInfo.METADATA_KEY_DESCRIPTION -> description)\n+    val inputInfo = StreamInputInfo(id, rdd.count, metadata)\n+    ssc.scheduler.inputInfoTracker.reportInfo(validTime, inputInfo)\n+\n+    currentOffsets = untilOffsets\n+    commitAll()\n+    Some(rdd)\n+  }\n+\n+  override def start(): Unit = {\n+    val c = consumer\n+    c.poll(0)\n+    if (currentOffsets.isEmpty) {\n+      currentOffsets = c.assignment().asScala.map { tp =>\n+        tp -> c.position(tp)\n+      }.toMap\n+    }\n+\n+    // don't actually want to consume any messages, so pause all partitions\n+    c.pause(currentOffsets.keySet.asJava)\n+  }\n+\n+  override def stop(): Unit = this.synchronized {\n+    if (kc != null) {\n+      kc.close()\n+    }\n+  }\n+\n+  protected val commitQueue = new ConcurrentLinkedQueue[OffsetRange]\n+  protected val commitCallback = new AtomicReference[OffsetCommitCallback]\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange]): Unit = {\n+    commitAsync(offsetRanges, null)\n+  }\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   * @param callback Only the most recently provided callback will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange], callback: OffsetCommitCallback): Unit = {\n+    commitCallback.set(callback)\n+    commitQueue.addAll(ju.Arrays.asList(offsetRanges: _*))\n+  }\n+\n+  protected def commitAll(): Unit = {\n+    val m = new ju.HashMap[TopicPartition, OffsetAndMetadata]()\n+    val it = commitQueue.iterator()\n+    while (it.hasNext) {\n+      val osr = it.next\n+      val tp = osr.topicPartition\n+      val x = m.get(tp)\n+      val offset = if (null == x) { osr.untilOffset } else { Math.max(x.offset, osr.untilOffset) }\n+      m.put(tp, new OffsetAndMetadata(offset))\n+    }\n+    if (!m.isEmpty) {\n+      consumer.commitAsync(m, commitCallback.get)\n+    }\n+  }\n+\n+  private[streaming]\n+  class DirectKafkaInputDStreamCheckpointData extends DStreamCheckpointData(this) {\n+    def batchForTime: mutable.HashMap[Time, Array[(String, Int, Long, Long)]] = {\n+      data.asInstanceOf[mutable.HashMap[Time, Array[OffsetRange.OffsetRangeTuple]]]\n+    }\n+\n+    override def update(time: Time) {\n+      batchForTime.clear()\n+      generatedRDDs.foreach { kv =>\n+        val a = kv._2.asInstanceOf[KafkaRDD[K, V]].offsetRanges.map(_.toTuple).toArray\n+        batchForTime += kv._1 -> a\n+      }\n+    }\n+\n+    override def cleanup(time: Time) { }\n+\n+    override def restore() {"
  }],
  "prId": 11863
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "`preferBrokers` and `preferConsistent` are missing features?\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-22T21:01:38Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+import java.util.concurrent.ConcurrentLinkedQueue\n+import java.util.concurrent.atomic.AtomicReference\n+\n+import scala.annotation.tailrec\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.reflect.ClassTag\n+\n+import org.apache.kafka.clients.consumer._\n+import org.apache.kafka.common.{ PartitionInfo, TopicPartition }\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.scheduler.{RateController, StreamInputInfo}\n+import org.apache.spark.streaming.scheduler.rate.RateEstimator\n+\n+/**\n+ *  A stream of {@link org.apache.spark.streaming.kafka.KafkaRDD} where\n+ * each given Kafka topic/partition corresponds to an RDD partition.\n+ * The spark configuration spark.streaming.kafka.maxRatePerPartition gives the maximum number\n+ *  of messages\n+ * per second that each '''partition''' will accept.\n+ * @param preferredHosts map from TopicPartition to preferred host for processing that partition.\n+ * In most cases, use [[DirectKafkaInputDStream.preferConsistent]]\n+ * Use [[DirectKafkaInputDStream.preferBrokers]] if your executors are on same nodes as brokers.\n+ * @param executorKafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.html#newconsumerconfigs\">\n+ * configuration parameters</a>.\n+ *   Requires  \"bootstrap.servers\" to be set with Kafka broker(s),\n+ *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+ * @param driverConsumer zero-argument function for you to construct a Kafka Consumer,\n+ *  and subscribe topics or assign partitions.\n+ *  This consumer will be used on the driver to query for offsets only, not messages.\n+ *  See <a href=\"http://kafka.apache.org/documentation.html#newconsumerapi\">Consumer doc</a>\n+ * @tparam K type of Kafka message key\n+ * @tparam V type of Kafka message value\n+ */\n+@Experimental\n+class DirectKafkaInputDStream[K: ClassTag, V: ClassTag] private[spark] (\n+    _ssc: StreamingContext,\n+    preferredHosts: ju.Map[TopicPartition, String],\n+    executorKafkaParams: ju.Map[String, Object],\n+    driverConsumer: () => Consumer[K, V]\n+  ) extends InputDStream[ConsumerRecord[K, V]](_ssc) with Logging {\n+\n+  @transient private var kc: Consumer[K, V] = null\n+  def consumer(): Consumer[K, V] = this.synchronized {\n+    if (null == kc) {\n+      kc = driverConsumer()\n+    }\n+    kc\n+  }\n+  consumer()\n+\n+  override def persist(newLevel: StorageLevel): DStream[ConsumerRecord[K, V]] = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  protected def getBrokers = {\n+    val c = consumer\n+    val result = new ju.HashMap[TopicPartition, String]()\n+    val hosts = new ju.HashMap[TopicPartition, String]()\n+    val assignments = c.assignment().iterator()\n+    while (assignments.hasNext()) {\n+      val tp: TopicPartition = assignments.next()\n+      if (null == hosts.get(tp)) {\n+        val infos = c.partitionsFor(tp.topic).iterator()\n+        while (infos.hasNext()) {\n+          val i = infos.next()\n+          hosts.put(new TopicPartition(i.topic(), i.partition()), i.leader.host())\n+        }\n+      }\n+      result.put(tp, hosts.get(tp))\n+    }\n+    result\n+  }\n+\n+  protected def getPreferredHosts: ju.Map[TopicPartition, String] = {\n+    if (preferredHosts == DirectKafkaInputDStream.preferBrokers) {\n+      getBrokers\n+    } else {\n+      preferredHosts\n+    }\n+  }\n+\n+  // Keep this consistent with how other streams are named (e.g. \"Flume polling stream [2]\")\n+  private[streaming] override def name: String = s\"Kafka 0.10 direct stream [$id]\"\n+\n+  protected[streaming] override val checkpointData =\n+    new DirectKafkaInputDStreamCheckpointData\n+\n+\n+  /**\n+   * Asynchronously maintains & sends new rate limits to the receiver through the receiver tracker.\n+   */\n+  override protected[streaming] val rateController: Option[RateController] = {\n+    if (RateController.isBackPressureEnabled(ssc.conf)) {\n+      Some(new DirectKafkaRateController(id,\n+        RateEstimator.create(ssc.conf, context.graph.batchDuration)))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  private val maxRateLimitPerPartition: Int = context.sparkContext.getConf.getInt(\n+    \"spark.streaming.kafka.maxRatePerPartition\", 0)\n+\n+  protected[streaming] def maxMessagesPerPartition(\n+    offsets: Map[TopicPartition, Long]): Option[Map[TopicPartition, Long]] = {\n+    val estimatedRateLimit = rateController.map(_.getLatestRate().toInt)\n+\n+    // calculate a per-partition rate limit based on current lag\n+    val effectiveRateLimitPerPartition = estimatedRateLimit.filter(_ > 0) match {\n+      case Some(rate) =>\n+        val lagPerPartition = offsets.map { case (tp, offset) =>\n+          tp -> Math.max(offset - currentOffsets(tp), 0)\n+        }\n+        val totalLag = lagPerPartition.values.sum\n+\n+        lagPerPartition.map { case (tp, lag) =>\n+          val backpressureRate = Math.round(lag / totalLag.toFloat * rate)\n+          tp -> (if (maxRateLimitPerPartition > 0) {\n+            Math.min(backpressureRate, maxRateLimitPerPartition)} else backpressureRate)\n+        }\n+      case None => offsets.map { case (tp, offset) => tp -> maxRateLimitPerPartition }\n+    }\n+\n+    if (effectiveRateLimitPerPartition.values.sum > 0) {\n+      val secsPerBatch = context.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some(effectiveRateLimitPerPartition.map {\n+        case (tp, limit) => tp -> (secsPerBatch * limit).toLong\n+      })\n+    } else {\n+      None\n+    }\n+  }\n+\n+  protected var currentOffsets = Map[TopicPartition, Long]()\n+\n+  protected def latestOffsets(): Map[TopicPartition, Long] = {\n+    val c = consumer\n+    c.poll(0)\n+    val parts = c.assignment().asScala\n+\n+    // make sure new partitions are reflected in currentOffsets\n+    val newPartitions = parts.diff(currentOffsets.keySet)\n+    currentOffsets = currentOffsets ++ newPartitions.map(tp => tp -> c.position(tp)).toMap\n+    c.pause(newPartitions.asJava)\n+\n+    c.seekToEnd(currentOffsets.keySet.asJava)\n+    parts.map(tp => tp -> c.position(tp)).toMap\n+  }\n+\n+  // limits the maximum number of messages per partition\n+  protected def clamp(\n+    offsets: Map[TopicPartition, Long]): Map[TopicPartition, Long] = {\n+\n+    maxMessagesPerPartition(offsets).map { mmp =>\n+      mmp.map { case (tp, messages) =>\n+          val uo = offsets(tp)\n+          tp -> Math.min(currentOffsets(tp) + messages, uo)\n+      }\n+    }.getOrElse(offsets)\n+  }\n+\n+  override def compute(validTime: Time): Option[KafkaRDD[K, V]] = {\n+    val untilOffsets = clamp(latestOffsets())\n+    val offsetRanges = untilOffsets.map { case (tp, uo) =>\n+      val fo = currentOffsets(tp)\n+      OffsetRange(tp.topic, tp.partition, fo, uo)\n+    }\n+    val rdd = new KafkaRDD[K, V](\n+      context.sparkContext, executorKafkaParams, offsetRanges.toArray, getPreferredHosts, true)\n+\n+    // Report the record number and metadata of this batch interval to InputInfoTracker.\n+    val description = offsetRanges.filter { offsetRange =>\n+      // Don't display empty ranges.\n+      offsetRange.fromOffset != offsetRange.untilOffset\n+    }.map { offsetRange =>\n+      s\"topic: ${offsetRange.topic}\\tpartition: ${offsetRange.partition}\\t\" +\n+        s\"offsets: ${offsetRange.fromOffset} to ${offsetRange.untilOffset}\"\n+    }.mkString(\"\\n\")\n+    // Copy offsetRanges to immutable.List to prevent from being modified by the user\n+    val metadata = Map(\n+      \"offsets\" -> offsetRanges.toList,\n+      StreamInputInfo.METADATA_KEY_DESCRIPTION -> description)\n+    val inputInfo = StreamInputInfo(id, rdd.count, metadata)\n+    ssc.scheduler.inputInfoTracker.reportInfo(validTime, inputInfo)\n+\n+    currentOffsets = untilOffsets\n+    commitAll()\n+    Some(rdd)\n+  }\n+\n+  override def start(): Unit = {\n+    val c = consumer\n+    c.poll(0)\n+    if (currentOffsets.isEmpty) {\n+      currentOffsets = c.assignment().asScala.map { tp =>\n+        tp -> c.position(tp)\n+      }.toMap\n+    }\n+\n+    // don't actually want to consume any messages, so pause all partitions\n+    c.pause(currentOffsets.keySet.asJava)\n+  }\n+\n+  override def stop(): Unit = this.synchronized {\n+    if (kc != null) {\n+      kc.close()\n+    }\n+  }\n+\n+  protected val commitQueue = new ConcurrentLinkedQueue[OffsetRange]\n+  protected val commitCallback = new AtomicReference[OffsetCommitCallback]\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange]): Unit = {\n+    commitAsync(offsetRanges, null)\n+  }\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   * @param callback Only the most recently provided callback will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange], callback: OffsetCommitCallback): Unit = {\n+    commitCallback.set(callback)\n+    commitQueue.addAll(ju.Arrays.asList(offsetRanges: _*))\n+  }\n+\n+  protected def commitAll(): Unit = {\n+    val m = new ju.HashMap[TopicPartition, OffsetAndMetadata]()\n+    val it = commitQueue.iterator()\n+    while (it.hasNext) {\n+      val osr = it.next\n+      val tp = osr.topicPartition\n+      val x = m.get(tp)\n+      val offset = if (null == x) { osr.untilOffset } else { Math.max(x.offset, osr.untilOffset) }\n+      m.put(tp, new OffsetAndMetadata(offset))\n+    }\n+    if (!m.isEmpty) {\n+      consumer.commitAsync(m, commitCallback.get)\n+    }\n+  }\n+\n+  private[streaming]\n+  class DirectKafkaInputDStreamCheckpointData extends DStreamCheckpointData(this) {\n+    def batchForTime: mutable.HashMap[Time, Array[(String, Int, Long, Long)]] = {\n+      data.asInstanceOf[mutable.HashMap[Time, Array[OffsetRange.OffsetRangeTuple]]]\n+    }\n+\n+    override def update(time: Time) {\n+      batchForTime.clear()\n+      generatedRDDs.foreach { kv =>\n+        val a = kv._2.asInstanceOf[KafkaRDD[K, V]].offsetRanges.map(_.toTuple).toArray\n+        batchForTime += kv._1 -> a\n+      }\n+    }\n+\n+    override def cleanup(time: Time) { }\n+\n+    override def restore() {\n+      batchForTime.toSeq.sortBy(_._1)(Time.ordering).foreach { case (t, b) =>\n+         logInfo(s\"Restoring KafkaRDD for time $t ${b.mkString(\"[\", \", \", \"]\")}\")\n+         generatedRDDs += t -> new KafkaRDD[K, V](\n+           context.sparkContext,\n+           executorKafkaParams,\n+           b.map(OffsetRange(_)),\n+           getPreferredHosts,\n+           // during restore, it's possible same partition will be consumed from multiple\n+           // threads, so dont use cache\n+           false\n+         )\n+      }\n+    }\n+  }\n+\n+  /**\n+   * A RateController to retrieve the rate from RateEstimator.\n+   */\n+  private[streaming] class DirectKafkaRateController(id: Int, estimator: RateEstimator)\n+    extends RateController(id, estimator) {\n+    override def publish(rate: Long): Unit = ()\n+  }\n+}\n+\n+/**\n+ * Companion object that provides methods to create instances of [[DirectKafkaInputDStream]]\n+ */\n+@Experimental\n+object DirectKafkaInputDStream extends Logging {\n+  import org.apache.spark.streaming.api.java.{ JavaInputDStream, JavaStreamingContext }\n+  import org.apache.spark.api.java.function.{ Function0 => JFunction0 }\n+\n+  /** Prefer to run on kafka brokers, if they are on same hosts as executors */\n+  val preferBrokers: ju.Map[TopicPartition, String] = null\n+  /** Prefer a consistent executor per TopicPartition, evenly from all executors */\n+  val preferConsistent: ju.Map[TopicPartition, String] = ju.Collections.emptyMap()"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "no, those two special cases are handled by getPreferredHosts\n\nThis could be a full-fledged sum type with three subclasses (brokers, consistent, user defined map), but I don't really think there's a fourth case, and having to construct an instance of such a class just to pass in a map would make it more awkward to use.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-22T21:13:08Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+import java.util.concurrent.ConcurrentLinkedQueue\n+import java.util.concurrent.atomic.AtomicReference\n+\n+import scala.annotation.tailrec\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.reflect.ClassTag\n+\n+import org.apache.kafka.clients.consumer._\n+import org.apache.kafka.common.{ PartitionInfo, TopicPartition }\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.scheduler.{RateController, StreamInputInfo}\n+import org.apache.spark.streaming.scheduler.rate.RateEstimator\n+\n+/**\n+ *  A stream of {@link org.apache.spark.streaming.kafka.KafkaRDD} where\n+ * each given Kafka topic/partition corresponds to an RDD partition.\n+ * The spark configuration spark.streaming.kafka.maxRatePerPartition gives the maximum number\n+ *  of messages\n+ * per second that each '''partition''' will accept.\n+ * @param preferredHosts map from TopicPartition to preferred host for processing that partition.\n+ * In most cases, use [[DirectKafkaInputDStream.preferConsistent]]\n+ * Use [[DirectKafkaInputDStream.preferBrokers]] if your executors are on same nodes as brokers.\n+ * @param executorKafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.html#newconsumerconfigs\">\n+ * configuration parameters</a>.\n+ *   Requires  \"bootstrap.servers\" to be set with Kafka broker(s),\n+ *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+ * @param driverConsumer zero-argument function for you to construct a Kafka Consumer,\n+ *  and subscribe topics or assign partitions.\n+ *  This consumer will be used on the driver to query for offsets only, not messages.\n+ *  See <a href=\"http://kafka.apache.org/documentation.html#newconsumerapi\">Consumer doc</a>\n+ * @tparam K type of Kafka message key\n+ * @tparam V type of Kafka message value\n+ */\n+@Experimental\n+class DirectKafkaInputDStream[K: ClassTag, V: ClassTag] private[spark] (\n+    _ssc: StreamingContext,\n+    preferredHosts: ju.Map[TopicPartition, String],\n+    executorKafkaParams: ju.Map[String, Object],\n+    driverConsumer: () => Consumer[K, V]\n+  ) extends InputDStream[ConsumerRecord[K, V]](_ssc) with Logging {\n+\n+  @transient private var kc: Consumer[K, V] = null\n+  def consumer(): Consumer[K, V] = this.synchronized {\n+    if (null == kc) {\n+      kc = driverConsumer()\n+    }\n+    kc\n+  }\n+  consumer()\n+\n+  override def persist(newLevel: StorageLevel): DStream[ConsumerRecord[K, V]] = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  protected def getBrokers = {\n+    val c = consumer\n+    val result = new ju.HashMap[TopicPartition, String]()\n+    val hosts = new ju.HashMap[TopicPartition, String]()\n+    val assignments = c.assignment().iterator()\n+    while (assignments.hasNext()) {\n+      val tp: TopicPartition = assignments.next()\n+      if (null == hosts.get(tp)) {\n+        val infos = c.partitionsFor(tp.topic).iterator()\n+        while (infos.hasNext()) {\n+          val i = infos.next()\n+          hosts.put(new TopicPartition(i.topic(), i.partition()), i.leader.host())\n+        }\n+      }\n+      result.put(tp, hosts.get(tp))\n+    }\n+    result\n+  }\n+\n+  protected def getPreferredHosts: ju.Map[TopicPartition, String] = {\n+    if (preferredHosts == DirectKafkaInputDStream.preferBrokers) {\n+      getBrokers\n+    } else {\n+      preferredHosts\n+    }\n+  }\n+\n+  // Keep this consistent with how other streams are named (e.g. \"Flume polling stream [2]\")\n+  private[streaming] override def name: String = s\"Kafka 0.10 direct stream [$id]\"\n+\n+  protected[streaming] override val checkpointData =\n+    new DirectKafkaInputDStreamCheckpointData\n+\n+\n+  /**\n+   * Asynchronously maintains & sends new rate limits to the receiver through the receiver tracker.\n+   */\n+  override protected[streaming] val rateController: Option[RateController] = {\n+    if (RateController.isBackPressureEnabled(ssc.conf)) {\n+      Some(new DirectKafkaRateController(id,\n+        RateEstimator.create(ssc.conf, context.graph.batchDuration)))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  private val maxRateLimitPerPartition: Int = context.sparkContext.getConf.getInt(\n+    \"spark.streaming.kafka.maxRatePerPartition\", 0)\n+\n+  protected[streaming] def maxMessagesPerPartition(\n+    offsets: Map[TopicPartition, Long]): Option[Map[TopicPartition, Long]] = {\n+    val estimatedRateLimit = rateController.map(_.getLatestRate().toInt)\n+\n+    // calculate a per-partition rate limit based on current lag\n+    val effectiveRateLimitPerPartition = estimatedRateLimit.filter(_ > 0) match {\n+      case Some(rate) =>\n+        val lagPerPartition = offsets.map { case (tp, offset) =>\n+          tp -> Math.max(offset - currentOffsets(tp), 0)\n+        }\n+        val totalLag = lagPerPartition.values.sum\n+\n+        lagPerPartition.map { case (tp, lag) =>\n+          val backpressureRate = Math.round(lag / totalLag.toFloat * rate)\n+          tp -> (if (maxRateLimitPerPartition > 0) {\n+            Math.min(backpressureRate, maxRateLimitPerPartition)} else backpressureRate)\n+        }\n+      case None => offsets.map { case (tp, offset) => tp -> maxRateLimitPerPartition }\n+    }\n+\n+    if (effectiveRateLimitPerPartition.values.sum > 0) {\n+      val secsPerBatch = context.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some(effectiveRateLimitPerPartition.map {\n+        case (tp, limit) => tp -> (secsPerBatch * limit).toLong\n+      })\n+    } else {\n+      None\n+    }\n+  }\n+\n+  protected var currentOffsets = Map[TopicPartition, Long]()\n+\n+  protected def latestOffsets(): Map[TopicPartition, Long] = {\n+    val c = consumer\n+    c.poll(0)\n+    val parts = c.assignment().asScala\n+\n+    // make sure new partitions are reflected in currentOffsets\n+    val newPartitions = parts.diff(currentOffsets.keySet)\n+    currentOffsets = currentOffsets ++ newPartitions.map(tp => tp -> c.position(tp)).toMap\n+    c.pause(newPartitions.asJava)\n+\n+    c.seekToEnd(currentOffsets.keySet.asJava)\n+    parts.map(tp => tp -> c.position(tp)).toMap\n+  }\n+\n+  // limits the maximum number of messages per partition\n+  protected def clamp(\n+    offsets: Map[TopicPartition, Long]): Map[TopicPartition, Long] = {\n+\n+    maxMessagesPerPartition(offsets).map { mmp =>\n+      mmp.map { case (tp, messages) =>\n+          val uo = offsets(tp)\n+          tp -> Math.min(currentOffsets(tp) + messages, uo)\n+      }\n+    }.getOrElse(offsets)\n+  }\n+\n+  override def compute(validTime: Time): Option[KafkaRDD[K, V]] = {\n+    val untilOffsets = clamp(latestOffsets())\n+    val offsetRanges = untilOffsets.map { case (tp, uo) =>\n+      val fo = currentOffsets(tp)\n+      OffsetRange(tp.topic, tp.partition, fo, uo)\n+    }\n+    val rdd = new KafkaRDD[K, V](\n+      context.sparkContext, executorKafkaParams, offsetRanges.toArray, getPreferredHosts, true)\n+\n+    // Report the record number and metadata of this batch interval to InputInfoTracker.\n+    val description = offsetRanges.filter { offsetRange =>\n+      // Don't display empty ranges.\n+      offsetRange.fromOffset != offsetRange.untilOffset\n+    }.map { offsetRange =>\n+      s\"topic: ${offsetRange.topic}\\tpartition: ${offsetRange.partition}\\t\" +\n+        s\"offsets: ${offsetRange.fromOffset} to ${offsetRange.untilOffset}\"\n+    }.mkString(\"\\n\")\n+    // Copy offsetRanges to immutable.List to prevent from being modified by the user\n+    val metadata = Map(\n+      \"offsets\" -> offsetRanges.toList,\n+      StreamInputInfo.METADATA_KEY_DESCRIPTION -> description)\n+    val inputInfo = StreamInputInfo(id, rdd.count, metadata)\n+    ssc.scheduler.inputInfoTracker.reportInfo(validTime, inputInfo)\n+\n+    currentOffsets = untilOffsets\n+    commitAll()\n+    Some(rdd)\n+  }\n+\n+  override def start(): Unit = {\n+    val c = consumer\n+    c.poll(0)\n+    if (currentOffsets.isEmpty) {\n+      currentOffsets = c.assignment().asScala.map { tp =>\n+        tp -> c.position(tp)\n+      }.toMap\n+    }\n+\n+    // don't actually want to consume any messages, so pause all partitions\n+    c.pause(currentOffsets.keySet.asJava)\n+  }\n+\n+  override def stop(): Unit = this.synchronized {\n+    if (kc != null) {\n+      kc.close()\n+    }\n+  }\n+\n+  protected val commitQueue = new ConcurrentLinkedQueue[OffsetRange]\n+  protected val commitCallback = new AtomicReference[OffsetCommitCallback]\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange]): Unit = {\n+    commitAsync(offsetRanges, null)\n+  }\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   * @param callback Only the most recently provided callback will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange], callback: OffsetCommitCallback): Unit = {\n+    commitCallback.set(callback)\n+    commitQueue.addAll(ju.Arrays.asList(offsetRanges: _*))\n+  }\n+\n+  protected def commitAll(): Unit = {\n+    val m = new ju.HashMap[TopicPartition, OffsetAndMetadata]()\n+    val it = commitQueue.iterator()\n+    while (it.hasNext) {\n+      val osr = it.next\n+      val tp = osr.topicPartition\n+      val x = m.get(tp)\n+      val offset = if (null == x) { osr.untilOffset } else { Math.max(x.offset, osr.untilOffset) }\n+      m.put(tp, new OffsetAndMetadata(offset))\n+    }\n+    if (!m.isEmpty) {\n+      consumer.commitAsync(m, commitCallback.get)\n+    }\n+  }\n+\n+  private[streaming]\n+  class DirectKafkaInputDStreamCheckpointData extends DStreamCheckpointData(this) {\n+    def batchForTime: mutable.HashMap[Time, Array[(String, Int, Long, Long)]] = {\n+      data.asInstanceOf[mutable.HashMap[Time, Array[OffsetRange.OffsetRangeTuple]]]\n+    }\n+\n+    override def update(time: Time) {\n+      batchForTime.clear()\n+      generatedRDDs.foreach { kv =>\n+        val a = kv._2.asInstanceOf[KafkaRDD[K, V]].offsetRanges.map(_.toTuple).toArray\n+        batchForTime += kv._1 -> a\n+      }\n+    }\n+\n+    override def cleanup(time: Time) { }\n+\n+    override def restore() {\n+      batchForTime.toSeq.sortBy(_._1)(Time.ordering).foreach { case (t, b) =>\n+         logInfo(s\"Restoring KafkaRDD for time $t ${b.mkString(\"[\", \", \", \"]\")}\")\n+         generatedRDDs += t -> new KafkaRDD[K, V](\n+           context.sparkContext,\n+           executorKafkaParams,\n+           b.map(OffsetRange(_)),\n+           getPreferredHosts,\n+           // during restore, it's possible same partition will be consumed from multiple\n+           // threads, so dont use cache\n+           false\n+         )\n+      }\n+    }\n+  }\n+\n+  /**\n+   * A RateController to retrieve the rate from RateEstimator.\n+   */\n+  private[streaming] class DirectKafkaRateController(id: Int, estimator: RateEstimator)\n+    extends RateController(id, estimator) {\n+    override def publish(rate: Long): Unit = ()\n+  }\n+}\n+\n+/**\n+ * Companion object that provides methods to create instances of [[DirectKafkaInputDStream]]\n+ */\n+@Experimental\n+object DirectKafkaInputDStream extends Logging {\n+  import org.apache.spark.streaming.api.java.{ JavaInputDStream, JavaStreamingContext }\n+  import org.apache.spark.api.java.function.{ Function0 => JFunction0 }\n+\n+  /** Prefer to run on kafka brokers, if they are on same hosts as executors */\n+  val preferBrokers: ju.Map[TopicPartition, String] = null\n+  /** Prefer a consistent executor per TopicPartition, evenly from all executors */\n+  val preferConsistent: ju.Map[TopicPartition, String] = ju.Collections.emptyMap()"
  }, {
    "author": {
      "login": "zsxwing"
    },
    "body": "IMO, these APIs are pretty weird.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-22T21:15:24Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+import java.util.concurrent.ConcurrentLinkedQueue\n+import java.util.concurrent.atomic.AtomicReference\n+\n+import scala.annotation.tailrec\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.reflect.ClassTag\n+\n+import org.apache.kafka.clients.consumer._\n+import org.apache.kafka.common.{ PartitionInfo, TopicPartition }\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.scheduler.{RateController, StreamInputInfo}\n+import org.apache.spark.streaming.scheduler.rate.RateEstimator\n+\n+/**\n+ *  A stream of {@link org.apache.spark.streaming.kafka.KafkaRDD} where\n+ * each given Kafka topic/partition corresponds to an RDD partition.\n+ * The spark configuration spark.streaming.kafka.maxRatePerPartition gives the maximum number\n+ *  of messages\n+ * per second that each '''partition''' will accept.\n+ * @param preferredHosts map from TopicPartition to preferred host for processing that partition.\n+ * In most cases, use [[DirectKafkaInputDStream.preferConsistent]]\n+ * Use [[DirectKafkaInputDStream.preferBrokers]] if your executors are on same nodes as brokers.\n+ * @param executorKafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.html#newconsumerconfigs\">\n+ * configuration parameters</a>.\n+ *   Requires  \"bootstrap.servers\" to be set with Kafka broker(s),\n+ *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+ * @param driverConsumer zero-argument function for you to construct a Kafka Consumer,\n+ *  and subscribe topics or assign partitions.\n+ *  This consumer will be used on the driver to query for offsets only, not messages.\n+ *  See <a href=\"http://kafka.apache.org/documentation.html#newconsumerapi\">Consumer doc</a>\n+ * @tparam K type of Kafka message key\n+ * @tparam V type of Kafka message value\n+ */\n+@Experimental\n+class DirectKafkaInputDStream[K: ClassTag, V: ClassTag] private[spark] (\n+    _ssc: StreamingContext,\n+    preferredHosts: ju.Map[TopicPartition, String],\n+    executorKafkaParams: ju.Map[String, Object],\n+    driverConsumer: () => Consumer[K, V]\n+  ) extends InputDStream[ConsumerRecord[K, V]](_ssc) with Logging {\n+\n+  @transient private var kc: Consumer[K, V] = null\n+  def consumer(): Consumer[K, V] = this.synchronized {\n+    if (null == kc) {\n+      kc = driverConsumer()\n+    }\n+    kc\n+  }\n+  consumer()\n+\n+  override def persist(newLevel: StorageLevel): DStream[ConsumerRecord[K, V]] = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  protected def getBrokers = {\n+    val c = consumer\n+    val result = new ju.HashMap[TopicPartition, String]()\n+    val hosts = new ju.HashMap[TopicPartition, String]()\n+    val assignments = c.assignment().iterator()\n+    while (assignments.hasNext()) {\n+      val tp: TopicPartition = assignments.next()\n+      if (null == hosts.get(tp)) {\n+        val infos = c.partitionsFor(tp.topic).iterator()\n+        while (infos.hasNext()) {\n+          val i = infos.next()\n+          hosts.put(new TopicPartition(i.topic(), i.partition()), i.leader.host())\n+        }\n+      }\n+      result.put(tp, hosts.get(tp))\n+    }\n+    result\n+  }\n+\n+  protected def getPreferredHosts: ju.Map[TopicPartition, String] = {\n+    if (preferredHosts == DirectKafkaInputDStream.preferBrokers) {\n+      getBrokers\n+    } else {\n+      preferredHosts\n+    }\n+  }\n+\n+  // Keep this consistent with how other streams are named (e.g. \"Flume polling stream [2]\")\n+  private[streaming] override def name: String = s\"Kafka 0.10 direct stream [$id]\"\n+\n+  protected[streaming] override val checkpointData =\n+    new DirectKafkaInputDStreamCheckpointData\n+\n+\n+  /**\n+   * Asynchronously maintains & sends new rate limits to the receiver through the receiver tracker.\n+   */\n+  override protected[streaming] val rateController: Option[RateController] = {\n+    if (RateController.isBackPressureEnabled(ssc.conf)) {\n+      Some(new DirectKafkaRateController(id,\n+        RateEstimator.create(ssc.conf, context.graph.batchDuration)))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  private val maxRateLimitPerPartition: Int = context.sparkContext.getConf.getInt(\n+    \"spark.streaming.kafka.maxRatePerPartition\", 0)\n+\n+  protected[streaming] def maxMessagesPerPartition(\n+    offsets: Map[TopicPartition, Long]): Option[Map[TopicPartition, Long]] = {\n+    val estimatedRateLimit = rateController.map(_.getLatestRate().toInt)\n+\n+    // calculate a per-partition rate limit based on current lag\n+    val effectiveRateLimitPerPartition = estimatedRateLimit.filter(_ > 0) match {\n+      case Some(rate) =>\n+        val lagPerPartition = offsets.map { case (tp, offset) =>\n+          tp -> Math.max(offset - currentOffsets(tp), 0)\n+        }\n+        val totalLag = lagPerPartition.values.sum\n+\n+        lagPerPartition.map { case (tp, lag) =>\n+          val backpressureRate = Math.round(lag / totalLag.toFloat * rate)\n+          tp -> (if (maxRateLimitPerPartition > 0) {\n+            Math.min(backpressureRate, maxRateLimitPerPartition)} else backpressureRate)\n+        }\n+      case None => offsets.map { case (tp, offset) => tp -> maxRateLimitPerPartition }\n+    }\n+\n+    if (effectiveRateLimitPerPartition.values.sum > 0) {\n+      val secsPerBatch = context.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some(effectiveRateLimitPerPartition.map {\n+        case (tp, limit) => tp -> (secsPerBatch * limit).toLong\n+      })\n+    } else {\n+      None\n+    }\n+  }\n+\n+  protected var currentOffsets = Map[TopicPartition, Long]()\n+\n+  protected def latestOffsets(): Map[TopicPartition, Long] = {\n+    val c = consumer\n+    c.poll(0)\n+    val parts = c.assignment().asScala\n+\n+    // make sure new partitions are reflected in currentOffsets\n+    val newPartitions = parts.diff(currentOffsets.keySet)\n+    currentOffsets = currentOffsets ++ newPartitions.map(tp => tp -> c.position(tp)).toMap\n+    c.pause(newPartitions.asJava)\n+\n+    c.seekToEnd(currentOffsets.keySet.asJava)\n+    parts.map(tp => tp -> c.position(tp)).toMap\n+  }\n+\n+  // limits the maximum number of messages per partition\n+  protected def clamp(\n+    offsets: Map[TopicPartition, Long]): Map[TopicPartition, Long] = {\n+\n+    maxMessagesPerPartition(offsets).map { mmp =>\n+      mmp.map { case (tp, messages) =>\n+          val uo = offsets(tp)\n+          tp -> Math.min(currentOffsets(tp) + messages, uo)\n+      }\n+    }.getOrElse(offsets)\n+  }\n+\n+  override def compute(validTime: Time): Option[KafkaRDD[K, V]] = {\n+    val untilOffsets = clamp(latestOffsets())\n+    val offsetRanges = untilOffsets.map { case (tp, uo) =>\n+      val fo = currentOffsets(tp)\n+      OffsetRange(tp.topic, tp.partition, fo, uo)\n+    }\n+    val rdd = new KafkaRDD[K, V](\n+      context.sparkContext, executorKafkaParams, offsetRanges.toArray, getPreferredHosts, true)\n+\n+    // Report the record number and metadata of this batch interval to InputInfoTracker.\n+    val description = offsetRanges.filter { offsetRange =>\n+      // Don't display empty ranges.\n+      offsetRange.fromOffset != offsetRange.untilOffset\n+    }.map { offsetRange =>\n+      s\"topic: ${offsetRange.topic}\\tpartition: ${offsetRange.partition}\\t\" +\n+        s\"offsets: ${offsetRange.fromOffset} to ${offsetRange.untilOffset}\"\n+    }.mkString(\"\\n\")\n+    // Copy offsetRanges to immutable.List to prevent from being modified by the user\n+    val metadata = Map(\n+      \"offsets\" -> offsetRanges.toList,\n+      StreamInputInfo.METADATA_KEY_DESCRIPTION -> description)\n+    val inputInfo = StreamInputInfo(id, rdd.count, metadata)\n+    ssc.scheduler.inputInfoTracker.reportInfo(validTime, inputInfo)\n+\n+    currentOffsets = untilOffsets\n+    commitAll()\n+    Some(rdd)\n+  }\n+\n+  override def start(): Unit = {\n+    val c = consumer\n+    c.poll(0)\n+    if (currentOffsets.isEmpty) {\n+      currentOffsets = c.assignment().asScala.map { tp =>\n+        tp -> c.position(tp)\n+      }.toMap\n+    }\n+\n+    // don't actually want to consume any messages, so pause all partitions\n+    c.pause(currentOffsets.keySet.asJava)\n+  }\n+\n+  override def stop(): Unit = this.synchronized {\n+    if (kc != null) {\n+      kc.close()\n+    }\n+  }\n+\n+  protected val commitQueue = new ConcurrentLinkedQueue[OffsetRange]\n+  protected val commitCallback = new AtomicReference[OffsetCommitCallback]\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange]): Unit = {\n+    commitAsync(offsetRanges, null)\n+  }\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   * @param callback Only the most recently provided callback will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange], callback: OffsetCommitCallback): Unit = {\n+    commitCallback.set(callback)\n+    commitQueue.addAll(ju.Arrays.asList(offsetRanges: _*))\n+  }\n+\n+  protected def commitAll(): Unit = {\n+    val m = new ju.HashMap[TopicPartition, OffsetAndMetadata]()\n+    val it = commitQueue.iterator()\n+    while (it.hasNext) {\n+      val osr = it.next\n+      val tp = osr.topicPartition\n+      val x = m.get(tp)\n+      val offset = if (null == x) { osr.untilOffset } else { Math.max(x.offset, osr.untilOffset) }\n+      m.put(tp, new OffsetAndMetadata(offset))\n+    }\n+    if (!m.isEmpty) {\n+      consumer.commitAsync(m, commitCallback.get)\n+    }\n+  }\n+\n+  private[streaming]\n+  class DirectKafkaInputDStreamCheckpointData extends DStreamCheckpointData(this) {\n+    def batchForTime: mutable.HashMap[Time, Array[(String, Int, Long, Long)]] = {\n+      data.asInstanceOf[mutable.HashMap[Time, Array[OffsetRange.OffsetRangeTuple]]]\n+    }\n+\n+    override def update(time: Time) {\n+      batchForTime.clear()\n+      generatedRDDs.foreach { kv =>\n+        val a = kv._2.asInstanceOf[KafkaRDD[K, V]].offsetRanges.map(_.toTuple).toArray\n+        batchForTime += kv._1 -> a\n+      }\n+    }\n+\n+    override def cleanup(time: Time) { }\n+\n+    override def restore() {\n+      batchForTime.toSeq.sortBy(_._1)(Time.ordering).foreach { case (t, b) =>\n+         logInfo(s\"Restoring KafkaRDD for time $t ${b.mkString(\"[\", \", \", \"]\")}\")\n+         generatedRDDs += t -> new KafkaRDD[K, V](\n+           context.sparkContext,\n+           executorKafkaParams,\n+           b.map(OffsetRange(_)),\n+           getPreferredHosts,\n+           // during restore, it's possible same partition will be consumed from multiple\n+           // threads, so dont use cache\n+           false\n+         )\n+      }\n+    }\n+  }\n+\n+  /**\n+   * A RateController to retrieve the rate from RateEstimator.\n+   */\n+  private[streaming] class DirectKafkaRateController(id: Int, estimator: RateEstimator)\n+    extends RateController(id, estimator) {\n+    override def publish(rate: Long): Unit = ()\n+  }\n+}\n+\n+/**\n+ * Companion object that provides methods to create instances of [[DirectKafkaInputDStream]]\n+ */\n+@Experimental\n+object DirectKafkaInputDStream extends Logging {\n+  import org.apache.spark.streaming.api.java.{ JavaInputDStream, JavaStreamingContext }\n+  import org.apache.spark.api.java.function.{ Function0 => JFunction0 }\n+\n+  /** Prefer to run on kafka brokers, if they are on same hosts as executors */\n+  val preferBrokers: ju.Map[TopicPartition, String] = null\n+  /** Prefer a consistent executor per TopicPartition, evenly from all executors */\n+  val preferConsistent: ju.Map[TopicPartition, String] = ju.Collections.emptyMap()"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "Do you have a suggested alternative?  Really it's just a user-specified mapping from TopicPartition to host, with two common cases (use the brokers, or use a consistent host) that they probably don't want to deal with manually constructing themselves.  A Map seems like it does the job.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-22T21:31:31Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+import java.util.concurrent.ConcurrentLinkedQueue\n+import java.util.concurrent.atomic.AtomicReference\n+\n+import scala.annotation.tailrec\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.reflect.ClassTag\n+\n+import org.apache.kafka.clients.consumer._\n+import org.apache.kafka.common.{ PartitionInfo, TopicPartition }\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.scheduler.{RateController, StreamInputInfo}\n+import org.apache.spark.streaming.scheduler.rate.RateEstimator\n+\n+/**\n+ *  A stream of {@link org.apache.spark.streaming.kafka.KafkaRDD} where\n+ * each given Kafka topic/partition corresponds to an RDD partition.\n+ * The spark configuration spark.streaming.kafka.maxRatePerPartition gives the maximum number\n+ *  of messages\n+ * per second that each '''partition''' will accept.\n+ * @param preferredHosts map from TopicPartition to preferred host for processing that partition.\n+ * In most cases, use [[DirectKafkaInputDStream.preferConsistent]]\n+ * Use [[DirectKafkaInputDStream.preferBrokers]] if your executors are on same nodes as brokers.\n+ * @param executorKafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.html#newconsumerconfigs\">\n+ * configuration parameters</a>.\n+ *   Requires  \"bootstrap.servers\" to be set with Kafka broker(s),\n+ *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+ * @param driverConsumer zero-argument function for you to construct a Kafka Consumer,\n+ *  and subscribe topics or assign partitions.\n+ *  This consumer will be used on the driver to query for offsets only, not messages.\n+ *  See <a href=\"http://kafka.apache.org/documentation.html#newconsumerapi\">Consumer doc</a>\n+ * @tparam K type of Kafka message key\n+ * @tparam V type of Kafka message value\n+ */\n+@Experimental\n+class DirectKafkaInputDStream[K: ClassTag, V: ClassTag] private[spark] (\n+    _ssc: StreamingContext,\n+    preferredHosts: ju.Map[TopicPartition, String],\n+    executorKafkaParams: ju.Map[String, Object],\n+    driverConsumer: () => Consumer[K, V]\n+  ) extends InputDStream[ConsumerRecord[K, V]](_ssc) with Logging {\n+\n+  @transient private var kc: Consumer[K, V] = null\n+  def consumer(): Consumer[K, V] = this.synchronized {\n+    if (null == kc) {\n+      kc = driverConsumer()\n+    }\n+    kc\n+  }\n+  consumer()\n+\n+  override def persist(newLevel: StorageLevel): DStream[ConsumerRecord[K, V]] = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  protected def getBrokers = {\n+    val c = consumer\n+    val result = new ju.HashMap[TopicPartition, String]()\n+    val hosts = new ju.HashMap[TopicPartition, String]()\n+    val assignments = c.assignment().iterator()\n+    while (assignments.hasNext()) {\n+      val tp: TopicPartition = assignments.next()\n+      if (null == hosts.get(tp)) {\n+        val infos = c.partitionsFor(tp.topic).iterator()\n+        while (infos.hasNext()) {\n+          val i = infos.next()\n+          hosts.put(new TopicPartition(i.topic(), i.partition()), i.leader.host())\n+        }\n+      }\n+      result.put(tp, hosts.get(tp))\n+    }\n+    result\n+  }\n+\n+  protected def getPreferredHosts: ju.Map[TopicPartition, String] = {\n+    if (preferredHosts == DirectKafkaInputDStream.preferBrokers) {\n+      getBrokers\n+    } else {\n+      preferredHosts\n+    }\n+  }\n+\n+  // Keep this consistent with how other streams are named (e.g. \"Flume polling stream [2]\")\n+  private[streaming] override def name: String = s\"Kafka 0.10 direct stream [$id]\"\n+\n+  protected[streaming] override val checkpointData =\n+    new DirectKafkaInputDStreamCheckpointData\n+\n+\n+  /**\n+   * Asynchronously maintains & sends new rate limits to the receiver through the receiver tracker.\n+   */\n+  override protected[streaming] val rateController: Option[RateController] = {\n+    if (RateController.isBackPressureEnabled(ssc.conf)) {\n+      Some(new DirectKafkaRateController(id,\n+        RateEstimator.create(ssc.conf, context.graph.batchDuration)))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  private val maxRateLimitPerPartition: Int = context.sparkContext.getConf.getInt(\n+    \"spark.streaming.kafka.maxRatePerPartition\", 0)\n+\n+  protected[streaming] def maxMessagesPerPartition(\n+    offsets: Map[TopicPartition, Long]): Option[Map[TopicPartition, Long]] = {\n+    val estimatedRateLimit = rateController.map(_.getLatestRate().toInt)\n+\n+    // calculate a per-partition rate limit based on current lag\n+    val effectiveRateLimitPerPartition = estimatedRateLimit.filter(_ > 0) match {\n+      case Some(rate) =>\n+        val lagPerPartition = offsets.map { case (tp, offset) =>\n+          tp -> Math.max(offset - currentOffsets(tp), 0)\n+        }\n+        val totalLag = lagPerPartition.values.sum\n+\n+        lagPerPartition.map { case (tp, lag) =>\n+          val backpressureRate = Math.round(lag / totalLag.toFloat * rate)\n+          tp -> (if (maxRateLimitPerPartition > 0) {\n+            Math.min(backpressureRate, maxRateLimitPerPartition)} else backpressureRate)\n+        }\n+      case None => offsets.map { case (tp, offset) => tp -> maxRateLimitPerPartition }\n+    }\n+\n+    if (effectiveRateLimitPerPartition.values.sum > 0) {\n+      val secsPerBatch = context.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some(effectiveRateLimitPerPartition.map {\n+        case (tp, limit) => tp -> (secsPerBatch * limit).toLong\n+      })\n+    } else {\n+      None\n+    }\n+  }\n+\n+  protected var currentOffsets = Map[TopicPartition, Long]()\n+\n+  protected def latestOffsets(): Map[TopicPartition, Long] = {\n+    val c = consumer\n+    c.poll(0)\n+    val parts = c.assignment().asScala\n+\n+    // make sure new partitions are reflected in currentOffsets\n+    val newPartitions = parts.diff(currentOffsets.keySet)\n+    currentOffsets = currentOffsets ++ newPartitions.map(tp => tp -> c.position(tp)).toMap\n+    c.pause(newPartitions.asJava)\n+\n+    c.seekToEnd(currentOffsets.keySet.asJava)\n+    parts.map(tp => tp -> c.position(tp)).toMap\n+  }\n+\n+  // limits the maximum number of messages per partition\n+  protected def clamp(\n+    offsets: Map[TopicPartition, Long]): Map[TopicPartition, Long] = {\n+\n+    maxMessagesPerPartition(offsets).map { mmp =>\n+      mmp.map { case (tp, messages) =>\n+          val uo = offsets(tp)\n+          tp -> Math.min(currentOffsets(tp) + messages, uo)\n+      }\n+    }.getOrElse(offsets)\n+  }\n+\n+  override def compute(validTime: Time): Option[KafkaRDD[K, V]] = {\n+    val untilOffsets = clamp(latestOffsets())\n+    val offsetRanges = untilOffsets.map { case (tp, uo) =>\n+      val fo = currentOffsets(tp)\n+      OffsetRange(tp.topic, tp.partition, fo, uo)\n+    }\n+    val rdd = new KafkaRDD[K, V](\n+      context.sparkContext, executorKafkaParams, offsetRanges.toArray, getPreferredHosts, true)\n+\n+    // Report the record number and metadata of this batch interval to InputInfoTracker.\n+    val description = offsetRanges.filter { offsetRange =>\n+      // Don't display empty ranges.\n+      offsetRange.fromOffset != offsetRange.untilOffset\n+    }.map { offsetRange =>\n+      s\"topic: ${offsetRange.topic}\\tpartition: ${offsetRange.partition}\\t\" +\n+        s\"offsets: ${offsetRange.fromOffset} to ${offsetRange.untilOffset}\"\n+    }.mkString(\"\\n\")\n+    // Copy offsetRanges to immutable.List to prevent from being modified by the user\n+    val metadata = Map(\n+      \"offsets\" -> offsetRanges.toList,\n+      StreamInputInfo.METADATA_KEY_DESCRIPTION -> description)\n+    val inputInfo = StreamInputInfo(id, rdd.count, metadata)\n+    ssc.scheduler.inputInfoTracker.reportInfo(validTime, inputInfo)\n+\n+    currentOffsets = untilOffsets\n+    commitAll()\n+    Some(rdd)\n+  }\n+\n+  override def start(): Unit = {\n+    val c = consumer\n+    c.poll(0)\n+    if (currentOffsets.isEmpty) {\n+      currentOffsets = c.assignment().asScala.map { tp =>\n+        tp -> c.position(tp)\n+      }.toMap\n+    }\n+\n+    // don't actually want to consume any messages, so pause all partitions\n+    c.pause(currentOffsets.keySet.asJava)\n+  }\n+\n+  override def stop(): Unit = this.synchronized {\n+    if (kc != null) {\n+      kc.close()\n+    }\n+  }\n+\n+  protected val commitQueue = new ConcurrentLinkedQueue[OffsetRange]\n+  protected val commitCallback = new AtomicReference[OffsetCommitCallback]\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange]): Unit = {\n+    commitAsync(offsetRanges, null)\n+  }\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   * @param callback Only the most recently provided callback will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange], callback: OffsetCommitCallback): Unit = {\n+    commitCallback.set(callback)\n+    commitQueue.addAll(ju.Arrays.asList(offsetRanges: _*))\n+  }\n+\n+  protected def commitAll(): Unit = {\n+    val m = new ju.HashMap[TopicPartition, OffsetAndMetadata]()\n+    val it = commitQueue.iterator()\n+    while (it.hasNext) {\n+      val osr = it.next\n+      val tp = osr.topicPartition\n+      val x = m.get(tp)\n+      val offset = if (null == x) { osr.untilOffset } else { Math.max(x.offset, osr.untilOffset) }\n+      m.put(tp, new OffsetAndMetadata(offset))\n+    }\n+    if (!m.isEmpty) {\n+      consumer.commitAsync(m, commitCallback.get)\n+    }\n+  }\n+\n+  private[streaming]\n+  class DirectKafkaInputDStreamCheckpointData extends DStreamCheckpointData(this) {\n+    def batchForTime: mutable.HashMap[Time, Array[(String, Int, Long, Long)]] = {\n+      data.asInstanceOf[mutable.HashMap[Time, Array[OffsetRange.OffsetRangeTuple]]]\n+    }\n+\n+    override def update(time: Time) {\n+      batchForTime.clear()\n+      generatedRDDs.foreach { kv =>\n+        val a = kv._2.asInstanceOf[KafkaRDD[K, V]].offsetRanges.map(_.toTuple).toArray\n+        batchForTime += kv._1 -> a\n+      }\n+    }\n+\n+    override def cleanup(time: Time) { }\n+\n+    override def restore() {\n+      batchForTime.toSeq.sortBy(_._1)(Time.ordering).foreach { case (t, b) =>\n+         logInfo(s\"Restoring KafkaRDD for time $t ${b.mkString(\"[\", \", \", \"]\")}\")\n+         generatedRDDs += t -> new KafkaRDD[K, V](\n+           context.sparkContext,\n+           executorKafkaParams,\n+           b.map(OffsetRange(_)),\n+           getPreferredHosts,\n+           // during restore, it's possible same partition will be consumed from multiple\n+           // threads, so dont use cache\n+           false\n+         )\n+      }\n+    }\n+  }\n+\n+  /**\n+   * A RateController to retrieve the rate from RateEstimator.\n+   */\n+  private[streaming] class DirectKafkaRateController(id: Int, estimator: RateEstimator)\n+    extends RateController(id, estimator) {\n+    override def publish(rate: Long): Unit = ()\n+  }\n+}\n+\n+/**\n+ * Companion object that provides methods to create instances of [[DirectKafkaInputDStream]]\n+ */\n+@Experimental\n+object DirectKafkaInputDStream extends Logging {\n+  import org.apache.spark.streaming.api.java.{ JavaInputDStream, JavaStreamingContext }\n+  import org.apache.spark.api.java.function.{ Function0 => JFunction0 }\n+\n+  /** Prefer to run on kafka brokers, if they are on same hosts as executors */\n+  val preferBrokers: ju.Map[TopicPartition, String] = null\n+  /** Prefer a consistent executor per TopicPartition, evenly from all executors */\n+  val preferConsistent: ju.Map[TopicPartition, String] = ju.Collections.emptyMap()"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Is the user expected to the set the map here? Like `DirectKafkaInputDStream.preferBrokers = Map(...)`. If so that is a very weird non-functional API, that will be very awkward in Java. \n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-22T23:49:04Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+import java.util.concurrent.ConcurrentLinkedQueue\n+import java.util.concurrent.atomic.AtomicReference\n+\n+import scala.annotation.tailrec\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.reflect.ClassTag\n+\n+import org.apache.kafka.clients.consumer._\n+import org.apache.kafka.common.{ PartitionInfo, TopicPartition }\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.scheduler.{RateController, StreamInputInfo}\n+import org.apache.spark.streaming.scheduler.rate.RateEstimator\n+\n+/**\n+ *  A stream of {@link org.apache.spark.streaming.kafka.KafkaRDD} where\n+ * each given Kafka topic/partition corresponds to an RDD partition.\n+ * The spark configuration spark.streaming.kafka.maxRatePerPartition gives the maximum number\n+ *  of messages\n+ * per second that each '''partition''' will accept.\n+ * @param preferredHosts map from TopicPartition to preferred host for processing that partition.\n+ * In most cases, use [[DirectKafkaInputDStream.preferConsistent]]\n+ * Use [[DirectKafkaInputDStream.preferBrokers]] if your executors are on same nodes as brokers.\n+ * @param executorKafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.html#newconsumerconfigs\">\n+ * configuration parameters</a>.\n+ *   Requires  \"bootstrap.servers\" to be set with Kafka broker(s),\n+ *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+ * @param driverConsumer zero-argument function for you to construct a Kafka Consumer,\n+ *  and subscribe topics or assign partitions.\n+ *  This consumer will be used on the driver to query for offsets only, not messages.\n+ *  See <a href=\"http://kafka.apache.org/documentation.html#newconsumerapi\">Consumer doc</a>\n+ * @tparam K type of Kafka message key\n+ * @tparam V type of Kafka message value\n+ */\n+@Experimental\n+class DirectKafkaInputDStream[K: ClassTag, V: ClassTag] private[spark] (\n+    _ssc: StreamingContext,\n+    preferredHosts: ju.Map[TopicPartition, String],\n+    executorKafkaParams: ju.Map[String, Object],\n+    driverConsumer: () => Consumer[K, V]\n+  ) extends InputDStream[ConsumerRecord[K, V]](_ssc) with Logging {\n+\n+  @transient private var kc: Consumer[K, V] = null\n+  def consumer(): Consumer[K, V] = this.synchronized {\n+    if (null == kc) {\n+      kc = driverConsumer()\n+    }\n+    kc\n+  }\n+  consumer()\n+\n+  override def persist(newLevel: StorageLevel): DStream[ConsumerRecord[K, V]] = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  protected def getBrokers = {\n+    val c = consumer\n+    val result = new ju.HashMap[TopicPartition, String]()\n+    val hosts = new ju.HashMap[TopicPartition, String]()\n+    val assignments = c.assignment().iterator()\n+    while (assignments.hasNext()) {\n+      val tp: TopicPartition = assignments.next()\n+      if (null == hosts.get(tp)) {\n+        val infos = c.partitionsFor(tp.topic).iterator()\n+        while (infos.hasNext()) {\n+          val i = infos.next()\n+          hosts.put(new TopicPartition(i.topic(), i.partition()), i.leader.host())\n+        }\n+      }\n+      result.put(tp, hosts.get(tp))\n+    }\n+    result\n+  }\n+\n+  protected def getPreferredHosts: ju.Map[TopicPartition, String] = {\n+    if (preferredHosts == DirectKafkaInputDStream.preferBrokers) {\n+      getBrokers\n+    } else {\n+      preferredHosts\n+    }\n+  }\n+\n+  // Keep this consistent with how other streams are named (e.g. \"Flume polling stream [2]\")\n+  private[streaming] override def name: String = s\"Kafka 0.10 direct stream [$id]\"\n+\n+  protected[streaming] override val checkpointData =\n+    new DirectKafkaInputDStreamCheckpointData\n+\n+\n+  /**\n+   * Asynchronously maintains & sends new rate limits to the receiver through the receiver tracker.\n+   */\n+  override protected[streaming] val rateController: Option[RateController] = {\n+    if (RateController.isBackPressureEnabled(ssc.conf)) {\n+      Some(new DirectKafkaRateController(id,\n+        RateEstimator.create(ssc.conf, context.graph.batchDuration)))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  private val maxRateLimitPerPartition: Int = context.sparkContext.getConf.getInt(\n+    \"spark.streaming.kafka.maxRatePerPartition\", 0)\n+\n+  protected[streaming] def maxMessagesPerPartition(\n+    offsets: Map[TopicPartition, Long]): Option[Map[TopicPartition, Long]] = {\n+    val estimatedRateLimit = rateController.map(_.getLatestRate().toInt)\n+\n+    // calculate a per-partition rate limit based on current lag\n+    val effectiveRateLimitPerPartition = estimatedRateLimit.filter(_ > 0) match {\n+      case Some(rate) =>\n+        val lagPerPartition = offsets.map { case (tp, offset) =>\n+          tp -> Math.max(offset - currentOffsets(tp), 0)\n+        }\n+        val totalLag = lagPerPartition.values.sum\n+\n+        lagPerPartition.map { case (tp, lag) =>\n+          val backpressureRate = Math.round(lag / totalLag.toFloat * rate)\n+          tp -> (if (maxRateLimitPerPartition > 0) {\n+            Math.min(backpressureRate, maxRateLimitPerPartition)} else backpressureRate)\n+        }\n+      case None => offsets.map { case (tp, offset) => tp -> maxRateLimitPerPartition }\n+    }\n+\n+    if (effectiveRateLimitPerPartition.values.sum > 0) {\n+      val secsPerBatch = context.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some(effectiveRateLimitPerPartition.map {\n+        case (tp, limit) => tp -> (secsPerBatch * limit).toLong\n+      })\n+    } else {\n+      None\n+    }\n+  }\n+\n+  protected var currentOffsets = Map[TopicPartition, Long]()\n+\n+  protected def latestOffsets(): Map[TopicPartition, Long] = {\n+    val c = consumer\n+    c.poll(0)\n+    val parts = c.assignment().asScala\n+\n+    // make sure new partitions are reflected in currentOffsets\n+    val newPartitions = parts.diff(currentOffsets.keySet)\n+    currentOffsets = currentOffsets ++ newPartitions.map(tp => tp -> c.position(tp)).toMap\n+    c.pause(newPartitions.asJava)\n+\n+    c.seekToEnd(currentOffsets.keySet.asJava)\n+    parts.map(tp => tp -> c.position(tp)).toMap\n+  }\n+\n+  // limits the maximum number of messages per partition\n+  protected def clamp(\n+    offsets: Map[TopicPartition, Long]): Map[TopicPartition, Long] = {\n+\n+    maxMessagesPerPartition(offsets).map { mmp =>\n+      mmp.map { case (tp, messages) =>\n+          val uo = offsets(tp)\n+          tp -> Math.min(currentOffsets(tp) + messages, uo)\n+      }\n+    }.getOrElse(offsets)\n+  }\n+\n+  override def compute(validTime: Time): Option[KafkaRDD[K, V]] = {\n+    val untilOffsets = clamp(latestOffsets())\n+    val offsetRanges = untilOffsets.map { case (tp, uo) =>\n+      val fo = currentOffsets(tp)\n+      OffsetRange(tp.topic, tp.partition, fo, uo)\n+    }\n+    val rdd = new KafkaRDD[K, V](\n+      context.sparkContext, executorKafkaParams, offsetRanges.toArray, getPreferredHosts, true)\n+\n+    // Report the record number and metadata of this batch interval to InputInfoTracker.\n+    val description = offsetRanges.filter { offsetRange =>\n+      // Don't display empty ranges.\n+      offsetRange.fromOffset != offsetRange.untilOffset\n+    }.map { offsetRange =>\n+      s\"topic: ${offsetRange.topic}\\tpartition: ${offsetRange.partition}\\t\" +\n+        s\"offsets: ${offsetRange.fromOffset} to ${offsetRange.untilOffset}\"\n+    }.mkString(\"\\n\")\n+    // Copy offsetRanges to immutable.List to prevent from being modified by the user\n+    val metadata = Map(\n+      \"offsets\" -> offsetRanges.toList,\n+      StreamInputInfo.METADATA_KEY_DESCRIPTION -> description)\n+    val inputInfo = StreamInputInfo(id, rdd.count, metadata)\n+    ssc.scheduler.inputInfoTracker.reportInfo(validTime, inputInfo)\n+\n+    currentOffsets = untilOffsets\n+    commitAll()\n+    Some(rdd)\n+  }\n+\n+  override def start(): Unit = {\n+    val c = consumer\n+    c.poll(0)\n+    if (currentOffsets.isEmpty) {\n+      currentOffsets = c.assignment().asScala.map { tp =>\n+        tp -> c.position(tp)\n+      }.toMap\n+    }\n+\n+    // don't actually want to consume any messages, so pause all partitions\n+    c.pause(currentOffsets.keySet.asJava)\n+  }\n+\n+  override def stop(): Unit = this.synchronized {\n+    if (kc != null) {\n+      kc.close()\n+    }\n+  }\n+\n+  protected val commitQueue = new ConcurrentLinkedQueue[OffsetRange]\n+  protected val commitCallback = new AtomicReference[OffsetCommitCallback]\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange]): Unit = {\n+    commitAsync(offsetRanges, null)\n+  }\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   * @param callback Only the most recently provided callback will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange], callback: OffsetCommitCallback): Unit = {\n+    commitCallback.set(callback)\n+    commitQueue.addAll(ju.Arrays.asList(offsetRanges: _*))\n+  }\n+\n+  protected def commitAll(): Unit = {\n+    val m = new ju.HashMap[TopicPartition, OffsetAndMetadata]()\n+    val it = commitQueue.iterator()\n+    while (it.hasNext) {\n+      val osr = it.next\n+      val tp = osr.topicPartition\n+      val x = m.get(tp)\n+      val offset = if (null == x) { osr.untilOffset } else { Math.max(x.offset, osr.untilOffset) }\n+      m.put(tp, new OffsetAndMetadata(offset))\n+    }\n+    if (!m.isEmpty) {\n+      consumer.commitAsync(m, commitCallback.get)\n+    }\n+  }\n+\n+  private[streaming]\n+  class DirectKafkaInputDStreamCheckpointData extends DStreamCheckpointData(this) {\n+    def batchForTime: mutable.HashMap[Time, Array[(String, Int, Long, Long)]] = {\n+      data.asInstanceOf[mutable.HashMap[Time, Array[OffsetRange.OffsetRangeTuple]]]\n+    }\n+\n+    override def update(time: Time) {\n+      batchForTime.clear()\n+      generatedRDDs.foreach { kv =>\n+        val a = kv._2.asInstanceOf[KafkaRDD[K, V]].offsetRanges.map(_.toTuple).toArray\n+        batchForTime += kv._1 -> a\n+      }\n+    }\n+\n+    override def cleanup(time: Time) { }\n+\n+    override def restore() {\n+      batchForTime.toSeq.sortBy(_._1)(Time.ordering).foreach { case (t, b) =>\n+         logInfo(s\"Restoring KafkaRDD for time $t ${b.mkString(\"[\", \", \", \"]\")}\")\n+         generatedRDDs += t -> new KafkaRDD[K, V](\n+           context.sparkContext,\n+           executorKafkaParams,\n+           b.map(OffsetRange(_)),\n+           getPreferredHosts,\n+           // during restore, it's possible same partition will be consumed from multiple\n+           // threads, so dont use cache\n+           false\n+         )\n+      }\n+    }\n+  }\n+\n+  /**\n+   * A RateController to retrieve the rate from RateEstimator.\n+   */\n+  private[streaming] class DirectKafkaRateController(id: Int, estimator: RateEstimator)\n+    extends RateController(id, estimator) {\n+    override def publish(rate: Long): Unit = ()\n+  }\n+}\n+\n+/**\n+ * Companion object that provides methods to create instances of [[DirectKafkaInputDStream]]\n+ */\n+@Experimental\n+object DirectKafkaInputDStream extends Logging {\n+  import org.apache.spark.streaming.api.java.{ JavaInputDStream, JavaStreamingContext }\n+  import org.apache.spark.api.java.function.{ Function0 => JFunction0 }\n+\n+  /** Prefer to run on kafka brokers, if they are on same hosts as executors */\n+  val preferBrokers: ju.Map[TopicPartition, String] = null\n+  /** Prefer a consistent executor per TopicPartition, evenly from all executors */\n+  val preferConsistent: ju.Map[TopicPartition, String] = ju.Collections.emptyMap()"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "No, it's a val.  The intent is to pass one of 3 things to the preferredHosts argument to the constructor for the stream: preferBrokers, preferConsistent, or your own implementation of a map from topicpartition to host.  This is documented in the constructor for the stream.  \n\nIntroducing a TopicPartitionHostMappingStrategyFactory or something like that just for this seems like total overkill.\n\nWithout a reasonably stable mapping from topicpartition to executor, the cache is going to be basically unusable.  Without ultimate control over which partition goes where, someone with topics that differ greatly in throughput is going to be screwed if the default consistent mapping puts all of their low volume topics on one executor and all of their high volume topics on another.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-23T01:03:40Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+import java.util.concurrent.ConcurrentLinkedQueue\n+import java.util.concurrent.atomic.AtomicReference\n+\n+import scala.annotation.tailrec\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.reflect.ClassTag\n+\n+import org.apache.kafka.clients.consumer._\n+import org.apache.kafka.common.{ PartitionInfo, TopicPartition }\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.scheduler.{RateController, StreamInputInfo}\n+import org.apache.spark.streaming.scheduler.rate.RateEstimator\n+\n+/**\n+ *  A stream of {@link org.apache.spark.streaming.kafka.KafkaRDD} where\n+ * each given Kafka topic/partition corresponds to an RDD partition.\n+ * The spark configuration spark.streaming.kafka.maxRatePerPartition gives the maximum number\n+ *  of messages\n+ * per second that each '''partition''' will accept.\n+ * @param preferredHosts map from TopicPartition to preferred host for processing that partition.\n+ * In most cases, use [[DirectKafkaInputDStream.preferConsistent]]\n+ * Use [[DirectKafkaInputDStream.preferBrokers]] if your executors are on same nodes as brokers.\n+ * @param executorKafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.html#newconsumerconfigs\">\n+ * configuration parameters</a>.\n+ *   Requires  \"bootstrap.servers\" to be set with Kafka broker(s),\n+ *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+ * @param driverConsumer zero-argument function for you to construct a Kafka Consumer,\n+ *  and subscribe topics or assign partitions.\n+ *  This consumer will be used on the driver to query for offsets only, not messages.\n+ *  See <a href=\"http://kafka.apache.org/documentation.html#newconsumerapi\">Consumer doc</a>\n+ * @tparam K type of Kafka message key\n+ * @tparam V type of Kafka message value\n+ */\n+@Experimental\n+class DirectKafkaInputDStream[K: ClassTag, V: ClassTag] private[spark] (\n+    _ssc: StreamingContext,\n+    preferredHosts: ju.Map[TopicPartition, String],\n+    executorKafkaParams: ju.Map[String, Object],\n+    driverConsumer: () => Consumer[K, V]\n+  ) extends InputDStream[ConsumerRecord[K, V]](_ssc) with Logging {\n+\n+  @transient private var kc: Consumer[K, V] = null\n+  def consumer(): Consumer[K, V] = this.synchronized {\n+    if (null == kc) {\n+      kc = driverConsumer()\n+    }\n+    kc\n+  }\n+  consumer()\n+\n+  override def persist(newLevel: StorageLevel): DStream[ConsumerRecord[K, V]] = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  protected def getBrokers = {\n+    val c = consumer\n+    val result = new ju.HashMap[TopicPartition, String]()\n+    val hosts = new ju.HashMap[TopicPartition, String]()\n+    val assignments = c.assignment().iterator()\n+    while (assignments.hasNext()) {\n+      val tp: TopicPartition = assignments.next()\n+      if (null == hosts.get(tp)) {\n+        val infos = c.partitionsFor(tp.topic).iterator()\n+        while (infos.hasNext()) {\n+          val i = infos.next()\n+          hosts.put(new TopicPartition(i.topic(), i.partition()), i.leader.host())\n+        }\n+      }\n+      result.put(tp, hosts.get(tp))\n+    }\n+    result\n+  }\n+\n+  protected def getPreferredHosts: ju.Map[TopicPartition, String] = {\n+    if (preferredHosts == DirectKafkaInputDStream.preferBrokers) {\n+      getBrokers\n+    } else {\n+      preferredHosts\n+    }\n+  }\n+\n+  // Keep this consistent with how other streams are named (e.g. \"Flume polling stream [2]\")\n+  private[streaming] override def name: String = s\"Kafka 0.10 direct stream [$id]\"\n+\n+  protected[streaming] override val checkpointData =\n+    new DirectKafkaInputDStreamCheckpointData\n+\n+\n+  /**\n+   * Asynchronously maintains & sends new rate limits to the receiver through the receiver tracker.\n+   */\n+  override protected[streaming] val rateController: Option[RateController] = {\n+    if (RateController.isBackPressureEnabled(ssc.conf)) {\n+      Some(new DirectKafkaRateController(id,\n+        RateEstimator.create(ssc.conf, context.graph.batchDuration)))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  private val maxRateLimitPerPartition: Int = context.sparkContext.getConf.getInt(\n+    \"spark.streaming.kafka.maxRatePerPartition\", 0)\n+\n+  protected[streaming] def maxMessagesPerPartition(\n+    offsets: Map[TopicPartition, Long]): Option[Map[TopicPartition, Long]] = {\n+    val estimatedRateLimit = rateController.map(_.getLatestRate().toInt)\n+\n+    // calculate a per-partition rate limit based on current lag\n+    val effectiveRateLimitPerPartition = estimatedRateLimit.filter(_ > 0) match {\n+      case Some(rate) =>\n+        val lagPerPartition = offsets.map { case (tp, offset) =>\n+          tp -> Math.max(offset - currentOffsets(tp), 0)\n+        }\n+        val totalLag = lagPerPartition.values.sum\n+\n+        lagPerPartition.map { case (tp, lag) =>\n+          val backpressureRate = Math.round(lag / totalLag.toFloat * rate)\n+          tp -> (if (maxRateLimitPerPartition > 0) {\n+            Math.min(backpressureRate, maxRateLimitPerPartition)} else backpressureRate)\n+        }\n+      case None => offsets.map { case (tp, offset) => tp -> maxRateLimitPerPartition }\n+    }\n+\n+    if (effectiveRateLimitPerPartition.values.sum > 0) {\n+      val secsPerBatch = context.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some(effectiveRateLimitPerPartition.map {\n+        case (tp, limit) => tp -> (secsPerBatch * limit).toLong\n+      })\n+    } else {\n+      None\n+    }\n+  }\n+\n+  protected var currentOffsets = Map[TopicPartition, Long]()\n+\n+  protected def latestOffsets(): Map[TopicPartition, Long] = {\n+    val c = consumer\n+    c.poll(0)\n+    val parts = c.assignment().asScala\n+\n+    // make sure new partitions are reflected in currentOffsets\n+    val newPartitions = parts.diff(currentOffsets.keySet)\n+    currentOffsets = currentOffsets ++ newPartitions.map(tp => tp -> c.position(tp)).toMap\n+    c.pause(newPartitions.asJava)\n+\n+    c.seekToEnd(currentOffsets.keySet.asJava)\n+    parts.map(tp => tp -> c.position(tp)).toMap\n+  }\n+\n+  // limits the maximum number of messages per partition\n+  protected def clamp(\n+    offsets: Map[TopicPartition, Long]): Map[TopicPartition, Long] = {\n+\n+    maxMessagesPerPartition(offsets).map { mmp =>\n+      mmp.map { case (tp, messages) =>\n+          val uo = offsets(tp)\n+          tp -> Math.min(currentOffsets(tp) + messages, uo)\n+      }\n+    }.getOrElse(offsets)\n+  }\n+\n+  override def compute(validTime: Time): Option[KafkaRDD[K, V]] = {\n+    val untilOffsets = clamp(latestOffsets())\n+    val offsetRanges = untilOffsets.map { case (tp, uo) =>\n+      val fo = currentOffsets(tp)\n+      OffsetRange(tp.topic, tp.partition, fo, uo)\n+    }\n+    val rdd = new KafkaRDD[K, V](\n+      context.sparkContext, executorKafkaParams, offsetRanges.toArray, getPreferredHosts, true)\n+\n+    // Report the record number and metadata of this batch interval to InputInfoTracker.\n+    val description = offsetRanges.filter { offsetRange =>\n+      // Don't display empty ranges.\n+      offsetRange.fromOffset != offsetRange.untilOffset\n+    }.map { offsetRange =>\n+      s\"topic: ${offsetRange.topic}\\tpartition: ${offsetRange.partition}\\t\" +\n+        s\"offsets: ${offsetRange.fromOffset} to ${offsetRange.untilOffset}\"\n+    }.mkString(\"\\n\")\n+    // Copy offsetRanges to immutable.List to prevent from being modified by the user\n+    val metadata = Map(\n+      \"offsets\" -> offsetRanges.toList,\n+      StreamInputInfo.METADATA_KEY_DESCRIPTION -> description)\n+    val inputInfo = StreamInputInfo(id, rdd.count, metadata)\n+    ssc.scheduler.inputInfoTracker.reportInfo(validTime, inputInfo)\n+\n+    currentOffsets = untilOffsets\n+    commitAll()\n+    Some(rdd)\n+  }\n+\n+  override def start(): Unit = {\n+    val c = consumer\n+    c.poll(0)\n+    if (currentOffsets.isEmpty) {\n+      currentOffsets = c.assignment().asScala.map { tp =>\n+        tp -> c.position(tp)\n+      }.toMap\n+    }\n+\n+    // don't actually want to consume any messages, so pause all partitions\n+    c.pause(currentOffsets.keySet.asJava)\n+  }\n+\n+  override def stop(): Unit = this.synchronized {\n+    if (kc != null) {\n+      kc.close()\n+    }\n+  }\n+\n+  protected val commitQueue = new ConcurrentLinkedQueue[OffsetRange]\n+  protected val commitCallback = new AtomicReference[OffsetCommitCallback]\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange]): Unit = {\n+    commitAsync(offsetRanges, null)\n+  }\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   * @param callback Only the most recently provided callback will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange], callback: OffsetCommitCallback): Unit = {\n+    commitCallback.set(callback)\n+    commitQueue.addAll(ju.Arrays.asList(offsetRanges: _*))\n+  }\n+\n+  protected def commitAll(): Unit = {\n+    val m = new ju.HashMap[TopicPartition, OffsetAndMetadata]()\n+    val it = commitQueue.iterator()\n+    while (it.hasNext) {\n+      val osr = it.next\n+      val tp = osr.topicPartition\n+      val x = m.get(tp)\n+      val offset = if (null == x) { osr.untilOffset } else { Math.max(x.offset, osr.untilOffset) }\n+      m.put(tp, new OffsetAndMetadata(offset))\n+    }\n+    if (!m.isEmpty) {\n+      consumer.commitAsync(m, commitCallback.get)\n+    }\n+  }\n+\n+  private[streaming]\n+  class DirectKafkaInputDStreamCheckpointData extends DStreamCheckpointData(this) {\n+    def batchForTime: mutable.HashMap[Time, Array[(String, Int, Long, Long)]] = {\n+      data.asInstanceOf[mutable.HashMap[Time, Array[OffsetRange.OffsetRangeTuple]]]\n+    }\n+\n+    override def update(time: Time) {\n+      batchForTime.clear()\n+      generatedRDDs.foreach { kv =>\n+        val a = kv._2.asInstanceOf[KafkaRDD[K, V]].offsetRanges.map(_.toTuple).toArray\n+        batchForTime += kv._1 -> a\n+      }\n+    }\n+\n+    override def cleanup(time: Time) { }\n+\n+    override def restore() {\n+      batchForTime.toSeq.sortBy(_._1)(Time.ordering).foreach { case (t, b) =>\n+         logInfo(s\"Restoring KafkaRDD for time $t ${b.mkString(\"[\", \", \", \"]\")}\")\n+         generatedRDDs += t -> new KafkaRDD[K, V](\n+           context.sparkContext,\n+           executorKafkaParams,\n+           b.map(OffsetRange(_)),\n+           getPreferredHosts,\n+           // during restore, it's possible same partition will be consumed from multiple\n+           // threads, so dont use cache\n+           false\n+         )\n+      }\n+    }\n+  }\n+\n+  /**\n+   * A RateController to retrieve the rate from RateEstimator.\n+   */\n+  private[streaming] class DirectKafkaRateController(id: Int, estimator: RateEstimator)\n+    extends RateController(id, estimator) {\n+    override def publish(rate: Long): Unit = ()\n+  }\n+}\n+\n+/**\n+ * Companion object that provides methods to create instances of [[DirectKafkaInputDStream]]\n+ */\n+@Experimental\n+object DirectKafkaInputDStream extends Logging {\n+  import org.apache.spark.streaming.api.java.{ JavaInputDStream, JavaStreamingContext }\n+  import org.apache.spark.api.java.function.{ Function0 => JFunction0 }\n+\n+  /** Prefer to run on kafka brokers, if they are on same hosts as executors */\n+  val preferBrokers: ju.Map[TopicPartition, String] = null\n+  /** Prefer a consistent executor per TopicPartition, evenly from all executors */\n+  val preferConsistent: ju.Map[TopicPartition, String] = ju.Collections.emptyMap()"
  }],
  "prId": 11863
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "Why needs this instead of using `KafkaConsumer` directly? To avoid multiple overloads with different KafkaConsumer's parameters?\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-22T21:13:12Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+import java.util.concurrent.ConcurrentLinkedQueue\n+import java.util.concurrent.atomic.AtomicReference\n+\n+import scala.annotation.tailrec\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.reflect.ClassTag\n+\n+import org.apache.kafka.clients.consumer._\n+import org.apache.kafka.common.{ PartitionInfo, TopicPartition }\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.scheduler.{RateController, StreamInputInfo}\n+import org.apache.spark.streaming.scheduler.rate.RateEstimator\n+\n+/**\n+ *  A stream of {@link org.apache.spark.streaming.kafka.KafkaRDD} where\n+ * each given Kafka topic/partition corresponds to an RDD partition.\n+ * The spark configuration spark.streaming.kafka.maxRatePerPartition gives the maximum number\n+ *  of messages\n+ * per second that each '''partition''' will accept.\n+ * @param preferredHosts map from TopicPartition to preferred host for processing that partition.\n+ * In most cases, use [[DirectKafkaInputDStream.preferConsistent]]\n+ * Use [[DirectKafkaInputDStream.preferBrokers]] if your executors are on same nodes as brokers.\n+ * @param executorKafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.html#newconsumerconfigs\">\n+ * configuration parameters</a>.\n+ *   Requires  \"bootstrap.servers\" to be set with Kafka broker(s),\n+ *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+ * @param driverConsumer zero-argument function for you to construct a Kafka Consumer,\n+ *  and subscribe topics or assign partitions.\n+ *  This consumer will be used on the driver to query for offsets only, not messages.\n+ *  See <a href=\"http://kafka.apache.org/documentation.html#newconsumerapi\">Consumer doc</a>\n+ * @tparam K type of Kafka message key\n+ * @tparam V type of Kafka message value\n+ */\n+@Experimental\n+class DirectKafkaInputDStream[K: ClassTag, V: ClassTag] private[spark] (\n+    _ssc: StreamingContext,\n+    preferredHosts: ju.Map[TopicPartition, String],\n+    executorKafkaParams: ju.Map[String, Object],\n+    driverConsumer: () => Consumer[K, V]\n+  ) extends InputDStream[ConsumerRecord[K, V]](_ssc) with Logging {\n+\n+  @transient private var kc: Consumer[K, V] = null\n+  def consumer(): Consumer[K, V] = this.synchronized {\n+    if (null == kc) {\n+      kc = driverConsumer()\n+    }\n+    kc\n+  }\n+  consumer()\n+\n+  override def persist(newLevel: StorageLevel): DStream[ConsumerRecord[K, V]] = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  protected def getBrokers = {\n+    val c = consumer\n+    val result = new ju.HashMap[TopicPartition, String]()\n+    val hosts = new ju.HashMap[TopicPartition, String]()\n+    val assignments = c.assignment().iterator()\n+    while (assignments.hasNext()) {\n+      val tp: TopicPartition = assignments.next()\n+      if (null == hosts.get(tp)) {\n+        val infos = c.partitionsFor(tp.topic).iterator()\n+        while (infos.hasNext()) {\n+          val i = infos.next()\n+          hosts.put(new TopicPartition(i.topic(), i.partition()), i.leader.host())\n+        }\n+      }\n+      result.put(tp, hosts.get(tp))\n+    }\n+    result\n+  }\n+\n+  protected def getPreferredHosts: ju.Map[TopicPartition, String] = {\n+    if (preferredHosts == DirectKafkaInputDStream.preferBrokers) {\n+      getBrokers\n+    } else {\n+      preferredHosts\n+    }\n+  }\n+\n+  // Keep this consistent with how other streams are named (e.g. \"Flume polling stream [2]\")\n+  private[streaming] override def name: String = s\"Kafka 0.10 direct stream [$id]\"\n+\n+  protected[streaming] override val checkpointData =\n+    new DirectKafkaInputDStreamCheckpointData\n+\n+\n+  /**\n+   * Asynchronously maintains & sends new rate limits to the receiver through the receiver tracker.\n+   */\n+  override protected[streaming] val rateController: Option[RateController] = {\n+    if (RateController.isBackPressureEnabled(ssc.conf)) {\n+      Some(new DirectKafkaRateController(id,\n+        RateEstimator.create(ssc.conf, context.graph.batchDuration)))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  private val maxRateLimitPerPartition: Int = context.sparkContext.getConf.getInt(\n+    \"spark.streaming.kafka.maxRatePerPartition\", 0)\n+\n+  protected[streaming] def maxMessagesPerPartition(\n+    offsets: Map[TopicPartition, Long]): Option[Map[TopicPartition, Long]] = {\n+    val estimatedRateLimit = rateController.map(_.getLatestRate().toInt)\n+\n+    // calculate a per-partition rate limit based on current lag\n+    val effectiveRateLimitPerPartition = estimatedRateLimit.filter(_ > 0) match {\n+      case Some(rate) =>\n+        val lagPerPartition = offsets.map { case (tp, offset) =>\n+          tp -> Math.max(offset - currentOffsets(tp), 0)\n+        }\n+        val totalLag = lagPerPartition.values.sum\n+\n+        lagPerPartition.map { case (tp, lag) =>\n+          val backpressureRate = Math.round(lag / totalLag.toFloat * rate)\n+          tp -> (if (maxRateLimitPerPartition > 0) {\n+            Math.min(backpressureRate, maxRateLimitPerPartition)} else backpressureRate)\n+        }\n+      case None => offsets.map { case (tp, offset) => tp -> maxRateLimitPerPartition }\n+    }\n+\n+    if (effectiveRateLimitPerPartition.values.sum > 0) {\n+      val secsPerBatch = context.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some(effectiveRateLimitPerPartition.map {\n+        case (tp, limit) => tp -> (secsPerBatch * limit).toLong\n+      })\n+    } else {\n+      None\n+    }\n+  }\n+\n+  protected var currentOffsets = Map[TopicPartition, Long]()\n+\n+  protected def latestOffsets(): Map[TopicPartition, Long] = {\n+    val c = consumer\n+    c.poll(0)\n+    val parts = c.assignment().asScala\n+\n+    // make sure new partitions are reflected in currentOffsets\n+    val newPartitions = parts.diff(currentOffsets.keySet)\n+    currentOffsets = currentOffsets ++ newPartitions.map(tp => tp -> c.position(tp)).toMap\n+    c.pause(newPartitions.asJava)\n+\n+    c.seekToEnd(currentOffsets.keySet.asJava)\n+    parts.map(tp => tp -> c.position(tp)).toMap\n+  }\n+\n+  // limits the maximum number of messages per partition\n+  protected def clamp(\n+    offsets: Map[TopicPartition, Long]): Map[TopicPartition, Long] = {\n+\n+    maxMessagesPerPartition(offsets).map { mmp =>\n+      mmp.map { case (tp, messages) =>\n+          val uo = offsets(tp)\n+          tp -> Math.min(currentOffsets(tp) + messages, uo)\n+      }\n+    }.getOrElse(offsets)\n+  }\n+\n+  override def compute(validTime: Time): Option[KafkaRDD[K, V]] = {\n+    val untilOffsets = clamp(latestOffsets())\n+    val offsetRanges = untilOffsets.map { case (tp, uo) =>\n+      val fo = currentOffsets(tp)\n+      OffsetRange(tp.topic, tp.partition, fo, uo)\n+    }\n+    val rdd = new KafkaRDD[K, V](\n+      context.sparkContext, executorKafkaParams, offsetRanges.toArray, getPreferredHosts, true)\n+\n+    // Report the record number and metadata of this batch interval to InputInfoTracker.\n+    val description = offsetRanges.filter { offsetRange =>\n+      // Don't display empty ranges.\n+      offsetRange.fromOffset != offsetRange.untilOffset\n+    }.map { offsetRange =>\n+      s\"topic: ${offsetRange.topic}\\tpartition: ${offsetRange.partition}\\t\" +\n+        s\"offsets: ${offsetRange.fromOffset} to ${offsetRange.untilOffset}\"\n+    }.mkString(\"\\n\")\n+    // Copy offsetRanges to immutable.List to prevent from being modified by the user\n+    val metadata = Map(\n+      \"offsets\" -> offsetRanges.toList,\n+      StreamInputInfo.METADATA_KEY_DESCRIPTION -> description)\n+    val inputInfo = StreamInputInfo(id, rdd.count, metadata)\n+    ssc.scheduler.inputInfoTracker.reportInfo(validTime, inputInfo)\n+\n+    currentOffsets = untilOffsets\n+    commitAll()\n+    Some(rdd)\n+  }\n+\n+  override def start(): Unit = {\n+    val c = consumer\n+    c.poll(0)\n+    if (currentOffsets.isEmpty) {\n+      currentOffsets = c.assignment().asScala.map { tp =>\n+        tp -> c.position(tp)\n+      }.toMap\n+    }\n+\n+    // don't actually want to consume any messages, so pause all partitions\n+    c.pause(currentOffsets.keySet.asJava)\n+  }\n+\n+  override def stop(): Unit = this.synchronized {\n+    if (kc != null) {\n+      kc.close()\n+    }\n+  }\n+\n+  protected val commitQueue = new ConcurrentLinkedQueue[OffsetRange]\n+  protected val commitCallback = new AtomicReference[OffsetCommitCallback]\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange]): Unit = {\n+    commitAsync(offsetRanges, null)\n+  }\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   * @param callback Only the most recently provided callback will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange], callback: OffsetCommitCallback): Unit = {\n+    commitCallback.set(callback)\n+    commitQueue.addAll(ju.Arrays.asList(offsetRanges: _*))\n+  }\n+\n+  protected def commitAll(): Unit = {\n+    val m = new ju.HashMap[TopicPartition, OffsetAndMetadata]()\n+    val it = commitQueue.iterator()\n+    while (it.hasNext) {\n+      val osr = it.next\n+      val tp = osr.topicPartition\n+      val x = m.get(tp)\n+      val offset = if (null == x) { osr.untilOffset } else { Math.max(x.offset, osr.untilOffset) }\n+      m.put(tp, new OffsetAndMetadata(offset))\n+    }\n+    if (!m.isEmpty) {\n+      consumer.commitAsync(m, commitCallback.get)\n+    }\n+  }\n+\n+  private[streaming]\n+  class DirectKafkaInputDStreamCheckpointData extends DStreamCheckpointData(this) {\n+    def batchForTime: mutable.HashMap[Time, Array[(String, Int, Long, Long)]] = {\n+      data.asInstanceOf[mutable.HashMap[Time, Array[OffsetRange.OffsetRangeTuple]]]\n+    }\n+\n+    override def update(time: Time) {\n+      batchForTime.clear()\n+      generatedRDDs.foreach { kv =>\n+        val a = kv._2.asInstanceOf[KafkaRDD[K, V]].offsetRanges.map(_.toTuple).toArray\n+        batchForTime += kv._1 -> a\n+      }\n+    }\n+\n+    override def cleanup(time: Time) { }\n+\n+    override def restore() {\n+      batchForTime.toSeq.sortBy(_._1)(Time.ordering).foreach { case (t, b) =>\n+         logInfo(s\"Restoring KafkaRDD for time $t ${b.mkString(\"[\", \", \", \"]\")}\")\n+         generatedRDDs += t -> new KafkaRDD[K, V](\n+           context.sparkContext,\n+           executorKafkaParams,\n+           b.map(OffsetRange(_)),\n+           getPreferredHosts,\n+           // during restore, it's possible same partition will be consumed from multiple\n+           // threads, so dont use cache\n+           false\n+         )\n+      }\n+    }\n+  }\n+\n+  /**\n+   * A RateController to retrieve the rate from RateEstimator.\n+   */\n+  private[streaming] class DirectKafkaRateController(id: Int, estimator: RateEstimator)\n+    extends RateController(id, estimator) {\n+    override def publish(rate: Long): Unit = ()\n+  }\n+}\n+\n+/**\n+ * Companion object that provides methods to create instances of [[DirectKafkaInputDStream]]\n+ */\n+@Experimental\n+object DirectKafkaInputDStream extends Logging {\n+  import org.apache.spark.streaming.api.java.{ JavaInputDStream, JavaStreamingContext }\n+  import org.apache.spark.api.java.function.{ Function0 => JFunction0 }\n+\n+  /** Prefer to run on kafka brokers, if they are on same hosts as executors */\n+  val preferBrokers: ju.Map[TopicPartition, String] = null\n+  /** Prefer a consistent executor per TopicPartition, evenly from all executors */\n+  val preferConsistent: ju.Map[TopicPartition, String] = ju.Collections.emptyMap()\n+\n+  /**\n+   * Scala constructor\n+   * @param preferredHosts map from TopicPartition to preferred host for processing that partition.\n+   * In most cases, use [[DirectKafkaInputDStream.preferConsistent]]\n+   * Use [[DirectKafkaInputDStream.preferBrokers]] if your executors are on same nodes as brokers.\n+   * @param executorKafkaParams Kafka\n+   * <a href=\"http://kafka.apache.org/documentation.html#newconsumerconfigs\">\n+   * configuration parameters</a>.\n+   *   Requires  \"bootstrap.servers\" to be set with Kafka broker(s),\n+   *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+   * @param driverConsumer zero-argument function for you to construct a Kafka Consumer,\n+   *  and subscribe topics or assign partitions.\n+   *  This consumer will be used on the driver to query for offsets only, not messages.\n+   *  See <a href=\"http://kafka.apache.org/documentation.html#newconsumerapi\">Consumer doc</a>\n+   * @tparam K type of Kafka message key\n+   * @tparam V type of Kafka message value\n+   */\n+  def apply[K: ClassTag, V: ClassTag](\n+      ssc: StreamingContext,\n+      preferredHosts: ju.Map[TopicPartition, String],\n+      executorKafkaParams: ju.Map[String, Object],\n+      driverConsumer: () => Consumer[K, V]"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "You need to be able to reconstruct a properly setup consumer after checkpoint recovery.  There are lots of different ways to set up a KafkaConsumer, and they require post-object-instantiation calls to e.g. subscribe or assign, with fixed or dynamic topics, etc.\n\nTrying to do multiple overloads would get into an explosion problem pretty quickly, especially when you consider some of the quirks in the underlying API that need to be worked around for special cases.\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-22T21:27:57Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+import java.util.concurrent.ConcurrentLinkedQueue\n+import java.util.concurrent.atomic.AtomicReference\n+\n+import scala.annotation.tailrec\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.reflect.ClassTag\n+\n+import org.apache.kafka.clients.consumer._\n+import org.apache.kafka.common.{ PartitionInfo, TopicPartition }\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.scheduler.{RateController, StreamInputInfo}\n+import org.apache.spark.streaming.scheduler.rate.RateEstimator\n+\n+/**\n+ *  A stream of {@link org.apache.spark.streaming.kafka.KafkaRDD} where\n+ * each given Kafka topic/partition corresponds to an RDD partition.\n+ * The spark configuration spark.streaming.kafka.maxRatePerPartition gives the maximum number\n+ *  of messages\n+ * per second that each '''partition''' will accept.\n+ * @param preferredHosts map from TopicPartition to preferred host for processing that partition.\n+ * In most cases, use [[DirectKafkaInputDStream.preferConsistent]]\n+ * Use [[DirectKafkaInputDStream.preferBrokers]] if your executors are on same nodes as brokers.\n+ * @param executorKafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.html#newconsumerconfigs\">\n+ * configuration parameters</a>.\n+ *   Requires  \"bootstrap.servers\" to be set with Kafka broker(s),\n+ *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+ * @param driverConsumer zero-argument function for you to construct a Kafka Consumer,\n+ *  and subscribe topics or assign partitions.\n+ *  This consumer will be used on the driver to query for offsets only, not messages.\n+ *  See <a href=\"http://kafka.apache.org/documentation.html#newconsumerapi\">Consumer doc</a>\n+ * @tparam K type of Kafka message key\n+ * @tparam V type of Kafka message value\n+ */\n+@Experimental\n+class DirectKafkaInputDStream[K: ClassTag, V: ClassTag] private[spark] (\n+    _ssc: StreamingContext,\n+    preferredHosts: ju.Map[TopicPartition, String],\n+    executorKafkaParams: ju.Map[String, Object],\n+    driverConsumer: () => Consumer[K, V]\n+  ) extends InputDStream[ConsumerRecord[K, V]](_ssc) with Logging {\n+\n+  @transient private var kc: Consumer[K, V] = null\n+  def consumer(): Consumer[K, V] = this.synchronized {\n+    if (null == kc) {\n+      kc = driverConsumer()\n+    }\n+    kc\n+  }\n+  consumer()\n+\n+  override def persist(newLevel: StorageLevel): DStream[ConsumerRecord[K, V]] = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  protected def getBrokers = {\n+    val c = consumer\n+    val result = new ju.HashMap[TopicPartition, String]()\n+    val hosts = new ju.HashMap[TopicPartition, String]()\n+    val assignments = c.assignment().iterator()\n+    while (assignments.hasNext()) {\n+      val tp: TopicPartition = assignments.next()\n+      if (null == hosts.get(tp)) {\n+        val infos = c.partitionsFor(tp.topic).iterator()\n+        while (infos.hasNext()) {\n+          val i = infos.next()\n+          hosts.put(new TopicPartition(i.topic(), i.partition()), i.leader.host())\n+        }\n+      }\n+      result.put(tp, hosts.get(tp))\n+    }\n+    result\n+  }\n+\n+  protected def getPreferredHosts: ju.Map[TopicPartition, String] = {\n+    if (preferredHosts == DirectKafkaInputDStream.preferBrokers) {\n+      getBrokers\n+    } else {\n+      preferredHosts\n+    }\n+  }\n+\n+  // Keep this consistent with how other streams are named (e.g. \"Flume polling stream [2]\")\n+  private[streaming] override def name: String = s\"Kafka 0.10 direct stream [$id]\"\n+\n+  protected[streaming] override val checkpointData =\n+    new DirectKafkaInputDStreamCheckpointData\n+\n+\n+  /**\n+   * Asynchronously maintains & sends new rate limits to the receiver through the receiver tracker.\n+   */\n+  override protected[streaming] val rateController: Option[RateController] = {\n+    if (RateController.isBackPressureEnabled(ssc.conf)) {\n+      Some(new DirectKafkaRateController(id,\n+        RateEstimator.create(ssc.conf, context.graph.batchDuration)))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  private val maxRateLimitPerPartition: Int = context.sparkContext.getConf.getInt(\n+    \"spark.streaming.kafka.maxRatePerPartition\", 0)\n+\n+  protected[streaming] def maxMessagesPerPartition(\n+    offsets: Map[TopicPartition, Long]): Option[Map[TopicPartition, Long]] = {\n+    val estimatedRateLimit = rateController.map(_.getLatestRate().toInt)\n+\n+    // calculate a per-partition rate limit based on current lag\n+    val effectiveRateLimitPerPartition = estimatedRateLimit.filter(_ > 0) match {\n+      case Some(rate) =>\n+        val lagPerPartition = offsets.map { case (tp, offset) =>\n+          tp -> Math.max(offset - currentOffsets(tp), 0)\n+        }\n+        val totalLag = lagPerPartition.values.sum\n+\n+        lagPerPartition.map { case (tp, lag) =>\n+          val backpressureRate = Math.round(lag / totalLag.toFloat * rate)\n+          tp -> (if (maxRateLimitPerPartition > 0) {\n+            Math.min(backpressureRate, maxRateLimitPerPartition)} else backpressureRate)\n+        }\n+      case None => offsets.map { case (tp, offset) => tp -> maxRateLimitPerPartition }\n+    }\n+\n+    if (effectiveRateLimitPerPartition.values.sum > 0) {\n+      val secsPerBatch = context.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some(effectiveRateLimitPerPartition.map {\n+        case (tp, limit) => tp -> (secsPerBatch * limit).toLong\n+      })\n+    } else {\n+      None\n+    }\n+  }\n+\n+  protected var currentOffsets = Map[TopicPartition, Long]()\n+\n+  protected def latestOffsets(): Map[TopicPartition, Long] = {\n+    val c = consumer\n+    c.poll(0)\n+    val parts = c.assignment().asScala\n+\n+    // make sure new partitions are reflected in currentOffsets\n+    val newPartitions = parts.diff(currentOffsets.keySet)\n+    currentOffsets = currentOffsets ++ newPartitions.map(tp => tp -> c.position(tp)).toMap\n+    c.pause(newPartitions.asJava)\n+\n+    c.seekToEnd(currentOffsets.keySet.asJava)\n+    parts.map(tp => tp -> c.position(tp)).toMap\n+  }\n+\n+  // limits the maximum number of messages per partition\n+  protected def clamp(\n+    offsets: Map[TopicPartition, Long]): Map[TopicPartition, Long] = {\n+\n+    maxMessagesPerPartition(offsets).map { mmp =>\n+      mmp.map { case (tp, messages) =>\n+          val uo = offsets(tp)\n+          tp -> Math.min(currentOffsets(tp) + messages, uo)\n+      }\n+    }.getOrElse(offsets)\n+  }\n+\n+  override def compute(validTime: Time): Option[KafkaRDD[K, V]] = {\n+    val untilOffsets = clamp(latestOffsets())\n+    val offsetRanges = untilOffsets.map { case (tp, uo) =>\n+      val fo = currentOffsets(tp)\n+      OffsetRange(tp.topic, tp.partition, fo, uo)\n+    }\n+    val rdd = new KafkaRDD[K, V](\n+      context.sparkContext, executorKafkaParams, offsetRanges.toArray, getPreferredHosts, true)\n+\n+    // Report the record number and metadata of this batch interval to InputInfoTracker.\n+    val description = offsetRanges.filter { offsetRange =>\n+      // Don't display empty ranges.\n+      offsetRange.fromOffset != offsetRange.untilOffset\n+    }.map { offsetRange =>\n+      s\"topic: ${offsetRange.topic}\\tpartition: ${offsetRange.partition}\\t\" +\n+        s\"offsets: ${offsetRange.fromOffset} to ${offsetRange.untilOffset}\"\n+    }.mkString(\"\\n\")\n+    // Copy offsetRanges to immutable.List to prevent from being modified by the user\n+    val metadata = Map(\n+      \"offsets\" -> offsetRanges.toList,\n+      StreamInputInfo.METADATA_KEY_DESCRIPTION -> description)\n+    val inputInfo = StreamInputInfo(id, rdd.count, metadata)\n+    ssc.scheduler.inputInfoTracker.reportInfo(validTime, inputInfo)\n+\n+    currentOffsets = untilOffsets\n+    commitAll()\n+    Some(rdd)\n+  }\n+\n+  override def start(): Unit = {\n+    val c = consumer\n+    c.poll(0)\n+    if (currentOffsets.isEmpty) {\n+      currentOffsets = c.assignment().asScala.map { tp =>\n+        tp -> c.position(tp)\n+      }.toMap\n+    }\n+\n+    // don't actually want to consume any messages, so pause all partitions\n+    c.pause(currentOffsets.keySet.asJava)\n+  }\n+\n+  override def stop(): Unit = this.synchronized {\n+    if (kc != null) {\n+      kc.close()\n+    }\n+  }\n+\n+  protected val commitQueue = new ConcurrentLinkedQueue[OffsetRange]\n+  protected val commitCallback = new AtomicReference[OffsetCommitCallback]\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange]): Unit = {\n+    commitAsync(offsetRanges, null)\n+  }\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   * @param callback Only the most recently provided callback will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange], callback: OffsetCommitCallback): Unit = {\n+    commitCallback.set(callback)\n+    commitQueue.addAll(ju.Arrays.asList(offsetRanges: _*))\n+  }\n+\n+  protected def commitAll(): Unit = {\n+    val m = new ju.HashMap[TopicPartition, OffsetAndMetadata]()\n+    val it = commitQueue.iterator()\n+    while (it.hasNext) {\n+      val osr = it.next\n+      val tp = osr.topicPartition\n+      val x = m.get(tp)\n+      val offset = if (null == x) { osr.untilOffset } else { Math.max(x.offset, osr.untilOffset) }\n+      m.put(tp, new OffsetAndMetadata(offset))\n+    }\n+    if (!m.isEmpty) {\n+      consumer.commitAsync(m, commitCallback.get)\n+    }\n+  }\n+\n+  private[streaming]\n+  class DirectKafkaInputDStreamCheckpointData extends DStreamCheckpointData(this) {\n+    def batchForTime: mutable.HashMap[Time, Array[(String, Int, Long, Long)]] = {\n+      data.asInstanceOf[mutable.HashMap[Time, Array[OffsetRange.OffsetRangeTuple]]]\n+    }\n+\n+    override def update(time: Time) {\n+      batchForTime.clear()\n+      generatedRDDs.foreach { kv =>\n+        val a = kv._2.asInstanceOf[KafkaRDD[K, V]].offsetRanges.map(_.toTuple).toArray\n+        batchForTime += kv._1 -> a\n+      }\n+    }\n+\n+    override def cleanup(time: Time) { }\n+\n+    override def restore() {\n+      batchForTime.toSeq.sortBy(_._1)(Time.ordering).foreach { case (t, b) =>\n+         logInfo(s\"Restoring KafkaRDD for time $t ${b.mkString(\"[\", \", \", \"]\")}\")\n+         generatedRDDs += t -> new KafkaRDD[K, V](\n+           context.sparkContext,\n+           executorKafkaParams,\n+           b.map(OffsetRange(_)),\n+           getPreferredHosts,\n+           // during restore, it's possible same partition will be consumed from multiple\n+           // threads, so dont use cache\n+           false\n+         )\n+      }\n+    }\n+  }\n+\n+  /**\n+   * A RateController to retrieve the rate from RateEstimator.\n+   */\n+  private[streaming] class DirectKafkaRateController(id: Int, estimator: RateEstimator)\n+    extends RateController(id, estimator) {\n+    override def publish(rate: Long): Unit = ()\n+  }\n+}\n+\n+/**\n+ * Companion object that provides methods to create instances of [[DirectKafkaInputDStream]]\n+ */\n+@Experimental\n+object DirectKafkaInputDStream extends Logging {\n+  import org.apache.spark.streaming.api.java.{ JavaInputDStream, JavaStreamingContext }\n+  import org.apache.spark.api.java.function.{ Function0 => JFunction0 }\n+\n+  /** Prefer to run on kafka brokers, if they are on same hosts as executors */\n+  val preferBrokers: ju.Map[TopicPartition, String] = null\n+  /** Prefer a consistent executor per TopicPartition, evenly from all executors */\n+  val preferConsistent: ju.Map[TopicPartition, String] = ju.Collections.emptyMap()\n+\n+  /**\n+   * Scala constructor\n+   * @param preferredHosts map from TopicPartition to preferred host for processing that partition.\n+   * In most cases, use [[DirectKafkaInputDStream.preferConsistent]]\n+   * Use [[DirectKafkaInputDStream.preferBrokers]] if your executors are on same nodes as brokers.\n+   * @param executorKafkaParams Kafka\n+   * <a href=\"http://kafka.apache.org/documentation.html#newconsumerconfigs\">\n+   * configuration parameters</a>.\n+   *   Requires  \"bootstrap.servers\" to be set with Kafka broker(s),\n+   *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+   * @param driverConsumer zero-argument function for you to construct a Kafka Consumer,\n+   *  and subscribe topics or assign partitions.\n+   *  This consumer will be used on the driver to query for offsets only, not messages.\n+   *  See <a href=\"http://kafka.apache.org/documentation.html#newconsumerapi\">Consumer doc</a>\n+   * @tparam K type of Kafka message key\n+   * @tparam V type of Kafka message value\n+   */\n+  def apply[K: ClassTag, V: ClassTag](\n+      ssc: StreamingContext,\n+      preferredHosts: ju.Map[TopicPartition, String],\n+      executorKafkaParams: ju.Map[String, Object],\n+      driverConsumer: () => Consumer[K, V]"
  }],
  "prId": 11863
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: imports are generally at the top\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-22T23:05:23Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+import java.util.concurrent.ConcurrentLinkedQueue\n+import java.util.concurrent.atomic.AtomicReference\n+\n+import scala.annotation.tailrec\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.reflect.ClassTag\n+\n+import org.apache.kafka.clients.consumer._\n+import org.apache.kafka.common.{ PartitionInfo, TopicPartition }\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.scheduler.{RateController, StreamInputInfo}\n+import org.apache.spark.streaming.scheduler.rate.RateEstimator\n+\n+/**\n+ *  A stream of {@link org.apache.spark.streaming.kafka.KafkaRDD} where\n+ * each given Kafka topic/partition corresponds to an RDD partition.\n+ * The spark configuration spark.streaming.kafka.maxRatePerPartition gives the maximum number\n+ *  of messages\n+ * per second that each '''partition''' will accept.\n+ * @param preferredHosts map from TopicPartition to preferred host for processing that partition.\n+ * In most cases, use [[DirectKafkaInputDStream.preferConsistent]]\n+ * Use [[DirectKafkaInputDStream.preferBrokers]] if your executors are on same nodes as brokers.\n+ * @param executorKafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.html#newconsumerconfigs\">\n+ * configuration parameters</a>.\n+ *   Requires  \"bootstrap.servers\" to be set with Kafka broker(s),\n+ *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+ * @param driverConsumer zero-argument function for you to construct a Kafka Consumer,\n+ *  and subscribe topics or assign partitions.\n+ *  This consumer will be used on the driver to query for offsets only, not messages.\n+ *  See <a href=\"http://kafka.apache.org/documentation.html#newconsumerapi\">Consumer doc</a>\n+ * @tparam K type of Kafka message key\n+ * @tparam V type of Kafka message value\n+ */\n+@Experimental\n+class DirectKafkaInputDStream[K: ClassTag, V: ClassTag] private[spark] (\n+    _ssc: StreamingContext,\n+    preferredHosts: ju.Map[TopicPartition, String],\n+    executorKafkaParams: ju.Map[String, Object],\n+    driverConsumer: () => Consumer[K, V]\n+  ) extends InputDStream[ConsumerRecord[K, V]](_ssc) with Logging {\n+\n+  @transient private var kc: Consumer[K, V] = null\n+  def consumer(): Consumer[K, V] = this.synchronized {\n+    if (null == kc) {\n+      kc = driverConsumer()\n+    }\n+    kc\n+  }\n+  consumer()\n+\n+  override def persist(newLevel: StorageLevel): DStream[ConsumerRecord[K, V]] = {\n+    log.error(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  protected def getBrokers = {\n+    val c = consumer\n+    val result = new ju.HashMap[TopicPartition, String]()\n+    val hosts = new ju.HashMap[TopicPartition, String]()\n+    val assignments = c.assignment().iterator()\n+    while (assignments.hasNext()) {\n+      val tp: TopicPartition = assignments.next()\n+      if (null == hosts.get(tp)) {\n+        val infos = c.partitionsFor(tp.topic).iterator()\n+        while (infos.hasNext()) {\n+          val i = infos.next()\n+          hosts.put(new TopicPartition(i.topic(), i.partition()), i.leader.host())\n+        }\n+      }\n+      result.put(tp, hosts.get(tp))\n+    }\n+    result\n+  }\n+\n+  protected def getPreferredHosts: ju.Map[TopicPartition, String] = {\n+    if (preferredHosts == DirectKafkaInputDStream.preferBrokers) {\n+      getBrokers\n+    } else {\n+      preferredHosts\n+    }\n+  }\n+\n+  // Keep this consistent with how other streams are named (e.g. \"Flume polling stream [2]\")\n+  private[streaming] override def name: String = s\"Kafka 0.10 direct stream [$id]\"\n+\n+  protected[streaming] override val checkpointData =\n+    new DirectKafkaInputDStreamCheckpointData\n+\n+\n+  /**\n+   * Asynchronously maintains & sends new rate limits to the receiver through the receiver tracker.\n+   */\n+  override protected[streaming] val rateController: Option[RateController] = {\n+    if (RateController.isBackPressureEnabled(ssc.conf)) {\n+      Some(new DirectKafkaRateController(id,\n+        RateEstimator.create(ssc.conf, context.graph.batchDuration)))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  private val maxRateLimitPerPartition: Int = context.sparkContext.getConf.getInt(\n+    \"spark.streaming.kafka.maxRatePerPartition\", 0)\n+\n+  protected[streaming] def maxMessagesPerPartition(\n+    offsets: Map[TopicPartition, Long]): Option[Map[TopicPartition, Long]] = {\n+    val estimatedRateLimit = rateController.map(_.getLatestRate().toInt)\n+\n+    // calculate a per-partition rate limit based on current lag\n+    val effectiveRateLimitPerPartition = estimatedRateLimit.filter(_ > 0) match {\n+      case Some(rate) =>\n+        val lagPerPartition = offsets.map { case (tp, offset) =>\n+          tp -> Math.max(offset - currentOffsets(tp), 0)\n+        }\n+        val totalLag = lagPerPartition.values.sum\n+\n+        lagPerPartition.map { case (tp, lag) =>\n+          val backpressureRate = Math.round(lag / totalLag.toFloat * rate)\n+          tp -> (if (maxRateLimitPerPartition > 0) {\n+            Math.min(backpressureRate, maxRateLimitPerPartition)} else backpressureRate)\n+        }\n+      case None => offsets.map { case (tp, offset) => tp -> maxRateLimitPerPartition }\n+    }\n+\n+    if (effectiveRateLimitPerPartition.values.sum > 0) {\n+      val secsPerBatch = context.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some(effectiveRateLimitPerPartition.map {\n+        case (tp, limit) => tp -> (secsPerBatch * limit).toLong\n+      })\n+    } else {\n+      None\n+    }\n+  }\n+\n+  protected var currentOffsets = Map[TopicPartition, Long]()\n+\n+  protected def latestOffsets(): Map[TopicPartition, Long] = {\n+    val c = consumer\n+    c.poll(0)\n+    val parts = c.assignment().asScala\n+\n+    // make sure new partitions are reflected in currentOffsets\n+    val newPartitions = parts.diff(currentOffsets.keySet)\n+    currentOffsets = currentOffsets ++ newPartitions.map(tp => tp -> c.position(tp)).toMap\n+    c.pause(newPartitions.asJava)\n+\n+    c.seekToEnd(currentOffsets.keySet.asJava)\n+    parts.map(tp => tp -> c.position(tp)).toMap\n+  }\n+\n+  // limits the maximum number of messages per partition\n+  protected def clamp(\n+    offsets: Map[TopicPartition, Long]): Map[TopicPartition, Long] = {\n+\n+    maxMessagesPerPartition(offsets).map { mmp =>\n+      mmp.map { case (tp, messages) =>\n+          val uo = offsets(tp)\n+          tp -> Math.min(currentOffsets(tp) + messages, uo)\n+      }\n+    }.getOrElse(offsets)\n+  }\n+\n+  override def compute(validTime: Time): Option[KafkaRDD[K, V]] = {\n+    val untilOffsets = clamp(latestOffsets())\n+    val offsetRanges = untilOffsets.map { case (tp, uo) =>\n+      val fo = currentOffsets(tp)\n+      OffsetRange(tp.topic, tp.partition, fo, uo)\n+    }\n+    val rdd = new KafkaRDD[K, V](\n+      context.sparkContext, executorKafkaParams, offsetRanges.toArray, getPreferredHosts, true)\n+\n+    // Report the record number and metadata of this batch interval to InputInfoTracker.\n+    val description = offsetRanges.filter { offsetRange =>\n+      // Don't display empty ranges.\n+      offsetRange.fromOffset != offsetRange.untilOffset\n+    }.map { offsetRange =>\n+      s\"topic: ${offsetRange.topic}\\tpartition: ${offsetRange.partition}\\t\" +\n+        s\"offsets: ${offsetRange.fromOffset} to ${offsetRange.untilOffset}\"\n+    }.mkString(\"\\n\")\n+    // Copy offsetRanges to immutable.List to prevent from being modified by the user\n+    val metadata = Map(\n+      \"offsets\" -> offsetRanges.toList,\n+      StreamInputInfo.METADATA_KEY_DESCRIPTION -> description)\n+    val inputInfo = StreamInputInfo(id, rdd.count, metadata)\n+    ssc.scheduler.inputInfoTracker.reportInfo(validTime, inputInfo)\n+\n+    currentOffsets = untilOffsets\n+    commitAll()\n+    Some(rdd)\n+  }\n+\n+  override def start(): Unit = {\n+    val c = consumer\n+    c.poll(0)\n+    if (currentOffsets.isEmpty) {\n+      currentOffsets = c.assignment().asScala.map { tp =>\n+        tp -> c.position(tp)\n+      }.toMap\n+    }\n+\n+    // don't actually want to consume any messages, so pause all partitions\n+    c.pause(currentOffsets.keySet.asJava)\n+  }\n+\n+  override def stop(): Unit = this.synchronized {\n+    if (kc != null) {\n+      kc.close()\n+    }\n+  }\n+\n+  protected val commitQueue = new ConcurrentLinkedQueue[OffsetRange]\n+  protected val commitCallback = new AtomicReference[OffsetCommitCallback]\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange]): Unit = {\n+    commitAsync(offsetRanges, null)\n+  }\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   * @param callback Only the most recently provided callback will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange], callback: OffsetCommitCallback): Unit = {\n+    commitCallback.set(callback)\n+    commitQueue.addAll(ju.Arrays.asList(offsetRanges: _*))\n+  }\n+\n+  protected def commitAll(): Unit = {\n+    val m = new ju.HashMap[TopicPartition, OffsetAndMetadata]()\n+    val it = commitQueue.iterator()\n+    while (it.hasNext) {\n+      val osr = it.next\n+      val tp = osr.topicPartition\n+      val x = m.get(tp)\n+      val offset = if (null == x) { osr.untilOffset } else { Math.max(x.offset, osr.untilOffset) }\n+      m.put(tp, new OffsetAndMetadata(offset))\n+    }\n+    if (!m.isEmpty) {\n+      consumer.commitAsync(m, commitCallback.get)\n+    }\n+  }\n+\n+  private[streaming]\n+  class DirectKafkaInputDStreamCheckpointData extends DStreamCheckpointData(this) {\n+    def batchForTime: mutable.HashMap[Time, Array[(String, Int, Long, Long)]] = {\n+      data.asInstanceOf[mutable.HashMap[Time, Array[OffsetRange.OffsetRangeTuple]]]\n+    }\n+\n+    override def update(time: Time) {\n+      batchForTime.clear()\n+      generatedRDDs.foreach { kv =>\n+        val a = kv._2.asInstanceOf[KafkaRDD[K, V]].offsetRanges.map(_.toTuple).toArray\n+        batchForTime += kv._1 -> a\n+      }\n+    }\n+\n+    override def cleanup(time: Time) { }\n+\n+    override def restore() {\n+      batchForTime.toSeq.sortBy(_._1)(Time.ordering).foreach { case (t, b) =>\n+         logInfo(s\"Restoring KafkaRDD for time $t ${b.mkString(\"[\", \", \", \"]\")}\")\n+         generatedRDDs += t -> new KafkaRDD[K, V](\n+           context.sparkContext,\n+           executorKafkaParams,\n+           b.map(OffsetRange(_)),\n+           getPreferredHosts,\n+           // during restore, it's possible same partition will be consumed from multiple\n+           // threads, so dont use cache\n+           false\n+         )\n+      }\n+    }\n+  }\n+\n+  /**\n+   * A RateController to retrieve the rate from RateEstimator.\n+   */\n+  private[streaming] class DirectKafkaRateController(id: Int, estimator: RateEstimator)\n+    extends RateController(id, estimator) {\n+    override def publish(rate: Long): Unit = ()\n+  }\n+}\n+\n+/**\n+ * Companion object that provides methods to create instances of [[DirectKafkaInputDStream]]\n+ */\n+@Experimental\n+object DirectKafkaInputDStream extends Logging {\n+  import org.apache.spark.streaming.api.java.{ JavaInputDStream, JavaStreamingContext }"
  }],
  "prId": 11863
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Can add explain (and add them as docs) on what this does for the `newPartitions`? And what does `c.pause` and `c.seektoEnd` do in this context?\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-27T06:57:29Z",
    "diffHunk": "@@ -0,0 +1,318 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+import java.util.concurrent.ConcurrentLinkedQueue\n+import java.util.concurrent.atomic.AtomicReference\n+\n+import scala.annotation.tailrec\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.reflect.ClassTag\n+\n+import org.apache.kafka.clients.consumer._\n+import org.apache.kafka.common.{ PartitionInfo, TopicPartition }\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.scheduler.{RateController, StreamInputInfo}\n+import org.apache.spark.streaming.scheduler.rate.RateEstimator\n+\n+/**\n+ *  A stream of [[KafkaRDD]] where\n+ * each given Kafka topic/partition corresponds to an RDD partition.\n+ * The spark configuration spark.streaming.kafka.maxRatePerPartition gives the maximum number\n+ *  of messages\n+ * per second that each '''partition''' will accept.\n+ * @param locationStrategy In most cases, pass in [[PreferConsistent]],\n+ *   see [[LocationStrategy]] for more details.\n+ * @param executorKafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.html#newconsumerconfigs\">\n+ * configuration parameters</a>.\n+ *   Requires  \"bootstrap.servers\" to be set with Kafka broker(s),\n+ *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+ * @param driverConsumer zero-argument function for you to construct a Kafka Consumer,\n+ *  and subscribe topics or assign partitions.\n+ *  This consumer will be used on the driver to query for offsets only, not messages.\n+ *  See <a href=\"http://kafka.apache.org/documentation.html#newconsumerapi\">Consumer doc</a>\n+ * @tparam K type of Kafka message key\n+ * @tparam V type of Kafka message value\n+ */\n+@Experimental\n+private[spark] class DirectKafkaInputDStream[K: ClassTag, V: ClassTag](\n+    _ssc: StreamingContext,\n+    locationStrategy: LocationStrategy,\n+    consumerStrategy: ConsumerStrategy[K, V]\n+  ) extends InputDStream[ConsumerRecord[K, V]](_ssc) with Logging with CanCommitOffsets {\n+\n+  val executorKafkaParams = {\n+    val ekp = new ju.HashMap[String, Object](consumerStrategy.executorKafkaParams)\n+    KafkaUtils.fixKafkaParams(ekp)\n+    ekp\n+  }\n+\n+  protected var currentOffsets = Map[TopicPartition, Long]()\n+\n+  @transient private var kc: Consumer[K, V] = null\n+  def consumer(): Consumer[K, V] = this.synchronized {\n+    if (null == kc) {\n+      kc = consumerStrategy.onStart(currentOffsets)\n+    }\n+    kc\n+  }\n+\n+  override def persist(newLevel: StorageLevel): DStream[ConsumerRecord[K, V]] = {\n+    logError(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  protected def getBrokers = {\n+    val c = consumer\n+    val result = new ju.HashMap[TopicPartition, String]()\n+    val hosts = new ju.HashMap[TopicPartition, String]()\n+    val assignments = c.assignment().iterator()\n+    while (assignments.hasNext()) {\n+      val tp: TopicPartition = assignments.next()\n+      if (null == hosts.get(tp)) {\n+        val infos = c.partitionsFor(tp.topic).iterator()\n+        while (infos.hasNext()) {\n+          val i = infos.next()\n+          hosts.put(new TopicPartition(i.topic(), i.partition()), i.leader.host())\n+        }\n+      }\n+      result.put(tp, hosts.get(tp))\n+    }\n+    result\n+  }\n+\n+  protected def getPreferredHosts: ju.Map[TopicPartition, String] = {\n+    locationStrategy match {\n+      case PreferBrokers => getBrokers\n+      case PreferConsistent => ju.Collections.emptyMap[TopicPartition, String]()\n+      case PreferFixed(hostMap) => hostMap\n+    }\n+  }\n+\n+  // Keep this consistent with how other streams are named (e.g. \"Flume polling stream [2]\")\n+  private[streaming] override def name: String = s\"Kafka 0.10 direct stream [$id]\"\n+\n+  protected[streaming] override val checkpointData =\n+    new DirectKafkaInputDStreamCheckpointData\n+\n+\n+  /**\n+   * Asynchronously maintains & sends new rate limits to the receiver through the receiver tracker.\n+   */\n+  override protected[streaming] val rateController: Option[RateController] = {\n+    if (RateController.isBackPressureEnabled(ssc.conf)) {\n+      Some(new DirectKafkaRateController(id,\n+        RateEstimator.create(ssc.conf, context.graph.batchDuration)))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  private val maxRateLimitPerPartition: Int = context.sparkContext.getConf.getInt(\n+    \"spark.streaming.kafka.maxRatePerPartition\", 0)\n+\n+  protected[streaming] def maxMessagesPerPartition(\n+    offsets: Map[TopicPartition, Long]): Option[Map[TopicPartition, Long]] = {\n+    val estimatedRateLimit = rateController.map(_.getLatestRate().toInt)\n+\n+    // calculate a per-partition rate limit based on current lag\n+    val effectiveRateLimitPerPartition = estimatedRateLimit.filter(_ > 0) match {\n+      case Some(rate) =>\n+        val lagPerPartition = offsets.map { case (tp, offset) =>\n+          tp -> Math.max(offset - currentOffsets(tp), 0)\n+        }\n+        val totalLag = lagPerPartition.values.sum\n+\n+        lagPerPartition.map { case (tp, lag) =>\n+          val backpressureRate = Math.round(lag / totalLag.toFloat * rate)\n+          tp -> (if (maxRateLimitPerPartition > 0) {\n+            Math.min(backpressureRate, maxRateLimitPerPartition)} else backpressureRate)\n+        }\n+      case None => offsets.map { case (tp, offset) => tp -> maxRateLimitPerPartition }\n+    }\n+\n+    if (effectiveRateLimitPerPartition.values.sum > 0) {\n+      val secsPerBatch = context.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some(effectiveRateLimitPerPartition.map {\n+        case (tp, limit) => tp -> (secsPerBatch * limit).toLong\n+      })\n+    } else {\n+      None\n+    }\n+  }\n+\n+  protected def latestOffsets(): Map[TopicPartition, Long] = {"
  }],
  "prId": 11863
}, {
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "`commitAsync` is just best-effort, or must be done? What will happen if `commitAsync` for `batch 1` fails but `commitAsync` for `batch 2` successes?\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-29T21:41:15Z",
    "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+import java.util.concurrent.ConcurrentLinkedQueue\n+import java.util.concurrent.atomic.AtomicReference\n+\n+import scala.annotation.tailrec\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.kafka.clients.consumer._\n+import org.apache.kafka.common.{ PartitionInfo, TopicPartition }\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.scheduler.{RateController, StreamInputInfo}\n+import org.apache.spark.streaming.scheduler.rate.RateEstimator\n+\n+/**\n+ *  A stream of [[KafkaRDD]] where\n+ * each given Kafka topic/partition corresponds to an RDD partition.\n+ * The spark configuration spark.streaming.kafka.maxRatePerPartition gives the maximum number\n+ *  of messages\n+ * per second that each '''partition''' will accept.\n+ * @param locationStrategy In most cases, pass in [[PreferConsistent]],\n+ *   see [[LocationStrategy]] for more details.\n+ * @param executorKafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.html#newconsumerconfigs\">\n+ * configuration parameters</a>.\n+ *   Requires  \"bootstrap.servers\" to be set with Kafka broker(s),\n+ *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+ * @param driverConsumer zero-argument function for you to construct a Kafka Consumer,\n+ *  and subscribe topics or assign partitions.\n+ *  This consumer will be used on the driver to query for offsets only, not messages.\n+ *  See <a href=\"http://kafka.apache.org/documentation.html#newconsumerapi\">Consumer doc</a>\n+ * @tparam K type of Kafka message key\n+ * @tparam V type of Kafka message value\n+ */\n+private[spark] class DirectKafkaInputDStream[K, V](\n+    _ssc: StreamingContext,\n+    locationStrategy: LocationStrategy,\n+    consumerStrategy: ConsumerStrategy[K, V]\n+  ) extends InputDStream[ConsumerRecord[K, V]](_ssc) with Logging with CanCommitOffsets {\n+\n+  val executorKafkaParams = {\n+    val ekp = new ju.HashMap[String, Object](consumerStrategy.executorKafkaParams)\n+    KafkaUtils.fixKafkaParams(ekp)\n+    ekp\n+  }\n+\n+  protected var currentOffsets = Map[TopicPartition, Long]()\n+\n+  @transient private var kc: Consumer[K, V] = null\n+  def consumer(): Consumer[K, V] = this.synchronized {\n+    if (null == kc) {\n+      kc = consumerStrategy.onStart(currentOffsets)\n+    }\n+    kc\n+  }\n+\n+  override def persist(newLevel: StorageLevel): DStream[ConsumerRecord[K, V]] = {\n+    logError(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  protected def getBrokers = {\n+    val c = consumer\n+    val result = new ju.HashMap[TopicPartition, String]()\n+    val hosts = new ju.HashMap[TopicPartition, String]()\n+    val assignments = c.assignment().iterator()\n+    while (assignments.hasNext()) {\n+      val tp: TopicPartition = assignments.next()\n+      if (null == hosts.get(tp)) {\n+        val infos = c.partitionsFor(tp.topic).iterator()\n+        while (infos.hasNext()) {\n+          val i = infos.next()\n+          hosts.put(new TopicPartition(i.topic(), i.partition()), i.leader.host())\n+        }\n+      }\n+      result.put(tp, hosts.get(tp))\n+    }\n+    result\n+  }\n+\n+  protected def getPreferredHosts: ju.Map[TopicPartition, String] = {\n+    locationStrategy match {\n+      case PreferBrokers => getBrokers\n+      case PreferConsistent => ju.Collections.emptyMap[TopicPartition, String]()\n+      case PreferFixed(hostMap) => hostMap\n+    }\n+  }\n+\n+  // Keep this consistent with how other streams are named (e.g. \"Flume polling stream [2]\")\n+  private[streaming] override def name: String = s\"Kafka 0.10 direct stream [$id]\"\n+\n+  protected[streaming] override val checkpointData =\n+    new DirectKafkaInputDStreamCheckpointData\n+\n+\n+  /**\n+   * Asynchronously maintains & sends new rate limits to the receiver through the receiver tracker.\n+   */\n+  override protected[streaming] val rateController: Option[RateController] = {\n+    if (RateController.isBackPressureEnabled(ssc.conf)) {\n+      Some(new DirectKafkaRateController(id,\n+        RateEstimator.create(ssc.conf, context.graph.batchDuration)))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  private val maxRateLimitPerPartition: Int = context.sparkContext.getConf.getInt(\n+    \"spark.streaming.kafka.maxRatePerPartition\", 0)\n+\n+  protected[streaming] def maxMessagesPerPartition(\n+    offsets: Map[TopicPartition, Long]): Option[Map[TopicPartition, Long]] = {\n+    val estimatedRateLimit = rateController.map(_.getLatestRate().toInt)\n+\n+    // calculate a per-partition rate limit based on current lag\n+    val effectiveRateLimitPerPartition = estimatedRateLimit.filter(_ > 0) match {\n+      case Some(rate) =>\n+        val lagPerPartition = offsets.map { case (tp, offset) =>\n+          tp -> Math.max(offset - currentOffsets(tp), 0)\n+        }\n+        val totalLag = lagPerPartition.values.sum\n+\n+        lagPerPartition.map { case (tp, lag) =>\n+          val backpressureRate = Math.round(lag / totalLag.toFloat * rate)\n+          tp -> (if (maxRateLimitPerPartition > 0) {\n+            Math.min(backpressureRate, maxRateLimitPerPartition)} else backpressureRate)\n+        }\n+      case None => offsets.map { case (tp, offset) => tp -> maxRateLimitPerPartition }\n+    }\n+\n+    if (effectiveRateLimitPerPartition.values.sum > 0) {\n+      val secsPerBatch = context.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some(effectiveRateLimitPerPartition.map {\n+        case (tp, limit) => tp -> (secsPerBatch * limit).toLong\n+      })\n+    } else {\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Returns the latest (highest) available offsets, taking new partitions into account.\n+   */\n+  protected def latestOffsets(): Map[TopicPartition, Long] = {\n+    val c = consumer\n+    c.poll(0)\n+    val parts = c.assignment().asScala\n+\n+    // make sure new partitions are reflected in currentOffsets\n+    val newPartitions = parts.diff(currentOffsets.keySet)\n+    // position for new partitions determined by auto.offset.reset if no commit\n+    currentOffsets = currentOffsets ++ newPartitions.map(tp => tp -> c.position(tp)).toMap\n+    // don't want to consume messages, so pause\n+    c.pause(newPartitions.asJava)\n+    // find latest available offsets\n+    c.seekToEnd(currentOffsets.keySet.asJava)\n+    parts.map(tp => tp -> c.position(tp)).toMap\n+  }\n+\n+  // limits the maximum number of messages per partition\n+  protected def clamp(\n+    offsets: Map[TopicPartition, Long]): Map[TopicPartition, Long] = {\n+\n+    maxMessagesPerPartition(offsets).map { mmp =>\n+      mmp.map { case (tp, messages) =>\n+          val uo = offsets(tp)\n+          tp -> Math.min(currentOffsets(tp) + messages, uo)\n+      }\n+    }.getOrElse(offsets)\n+  }\n+\n+  override def compute(validTime: Time): Option[KafkaRDD[K, V]] = {\n+    val untilOffsets = clamp(latestOffsets())\n+    val offsetRanges = untilOffsets.map { case (tp, uo) =>\n+      val fo = currentOffsets(tp)\n+      OffsetRange(tp.topic, tp.partition, fo, uo)\n+    }\n+    val rdd = new KafkaRDD[K, V](\n+      context.sparkContext, executorKafkaParams, offsetRanges.toArray, getPreferredHosts, true)\n+\n+    // Report the record number and metadata of this batch interval to InputInfoTracker.\n+    val description = offsetRanges.filter { offsetRange =>\n+      // Don't display empty ranges.\n+      offsetRange.fromOffset != offsetRange.untilOffset\n+    }.map { offsetRange =>\n+      s\"topic: ${offsetRange.topic}\\tpartition: ${offsetRange.partition}\\t\" +\n+        s\"offsets: ${offsetRange.fromOffset} to ${offsetRange.untilOffset}\"\n+    }.mkString(\"\\n\")\n+    // Copy offsetRanges to immutable.List to prevent from being modified by the user\n+    val metadata = Map(\n+      \"offsets\" -> offsetRanges.toList,\n+      StreamInputInfo.METADATA_KEY_DESCRIPTION -> description)\n+    val inputInfo = StreamInputInfo(id, rdd.count, metadata)\n+    ssc.scheduler.inputInfoTracker.reportInfo(validTime, inputInfo)\n+\n+    currentOffsets = untilOffsets\n+    commitAll()\n+    Some(rdd)\n+  }\n+\n+  override def start(): Unit = {\n+    val c = consumer\n+    c.poll(0)\n+    if (currentOffsets.isEmpty) {\n+      currentOffsets = c.assignment().asScala.map { tp =>\n+        tp -> c.position(tp)\n+      }.toMap\n+    }\n+\n+    // don't actually want to consume any messages, so pause all partitions\n+    c.pause(currentOffsets.keySet.asJava)\n+  }\n+\n+  override def stop(): Unit = this.synchronized {\n+    if (kc != null) {\n+      kc.close()\n+    }\n+  }\n+\n+  protected val commitQueue = new ConcurrentLinkedQueue[OffsetRange]\n+  protected val commitCallback = new AtomicReference[OffsetCommitCallback]\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange]): Unit = {\n+    commitAsync(offsetRanges, null)\n+  }\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   * @param callback Only the most recently provided callback will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange], callback: OffsetCommitCallback): Unit = {\n+    commitCallback.set(callback)\n+    commitQueue.addAll(ju.Arrays.asList(offsetRanges: _*))\n+  }\n+\n+  protected def commitAll(): Unit = {\n+    val m = new ju.HashMap[TopicPartition, OffsetAndMetadata]()\n+    val it = commitQueue.iterator()\n+    while (it.hasNext) {\n+      val osr = it.next\n+      val tp = osr.topicPartition\n+      val x = m.get(tp)\n+      val offset = if (null == x) { osr.untilOffset } else { Math.max(x.offset, osr.untilOffset) }\n+      m.put(tp, new OffsetAndMetadata(offset))\n+    }\n+    if (!m.isEmpty) {\n+      consumer.commitAsync(m, commitCallback.get)"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "If you're talking about whether users need to even use this interface, it depends on whether they want to store offsets in Kafka.\n\nAs far as whether commitAsync must be done, the attempt will be made, with a callback on success or failure.\n\nIf batch 1 fails and batch 2 succeeds everything should be fine, since it's the highest consumed offset that matters\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-29T21:52:28Z",
    "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+import java.util.concurrent.ConcurrentLinkedQueue\n+import java.util.concurrent.atomic.AtomicReference\n+\n+import scala.annotation.tailrec\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.kafka.clients.consumer._\n+import org.apache.kafka.common.{ PartitionInfo, TopicPartition }\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.scheduler.{RateController, StreamInputInfo}\n+import org.apache.spark.streaming.scheduler.rate.RateEstimator\n+\n+/**\n+ *  A stream of [[KafkaRDD]] where\n+ * each given Kafka topic/partition corresponds to an RDD partition.\n+ * The spark configuration spark.streaming.kafka.maxRatePerPartition gives the maximum number\n+ *  of messages\n+ * per second that each '''partition''' will accept.\n+ * @param locationStrategy In most cases, pass in [[PreferConsistent]],\n+ *   see [[LocationStrategy]] for more details.\n+ * @param executorKafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.html#newconsumerconfigs\">\n+ * configuration parameters</a>.\n+ *   Requires  \"bootstrap.servers\" to be set with Kafka broker(s),\n+ *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+ * @param driverConsumer zero-argument function for you to construct a Kafka Consumer,\n+ *  and subscribe topics or assign partitions.\n+ *  This consumer will be used on the driver to query for offsets only, not messages.\n+ *  See <a href=\"http://kafka.apache.org/documentation.html#newconsumerapi\">Consumer doc</a>\n+ * @tparam K type of Kafka message key\n+ * @tparam V type of Kafka message value\n+ */\n+private[spark] class DirectKafkaInputDStream[K, V](\n+    _ssc: StreamingContext,\n+    locationStrategy: LocationStrategy,\n+    consumerStrategy: ConsumerStrategy[K, V]\n+  ) extends InputDStream[ConsumerRecord[K, V]](_ssc) with Logging with CanCommitOffsets {\n+\n+  val executorKafkaParams = {\n+    val ekp = new ju.HashMap[String, Object](consumerStrategy.executorKafkaParams)\n+    KafkaUtils.fixKafkaParams(ekp)\n+    ekp\n+  }\n+\n+  protected var currentOffsets = Map[TopicPartition, Long]()\n+\n+  @transient private var kc: Consumer[K, V] = null\n+  def consumer(): Consumer[K, V] = this.synchronized {\n+    if (null == kc) {\n+      kc = consumerStrategy.onStart(currentOffsets)\n+    }\n+    kc\n+  }\n+\n+  override def persist(newLevel: StorageLevel): DStream[ConsumerRecord[K, V]] = {\n+    logError(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  protected def getBrokers = {\n+    val c = consumer\n+    val result = new ju.HashMap[TopicPartition, String]()\n+    val hosts = new ju.HashMap[TopicPartition, String]()\n+    val assignments = c.assignment().iterator()\n+    while (assignments.hasNext()) {\n+      val tp: TopicPartition = assignments.next()\n+      if (null == hosts.get(tp)) {\n+        val infos = c.partitionsFor(tp.topic).iterator()\n+        while (infos.hasNext()) {\n+          val i = infos.next()\n+          hosts.put(new TopicPartition(i.topic(), i.partition()), i.leader.host())\n+        }\n+      }\n+      result.put(tp, hosts.get(tp))\n+    }\n+    result\n+  }\n+\n+  protected def getPreferredHosts: ju.Map[TopicPartition, String] = {\n+    locationStrategy match {\n+      case PreferBrokers => getBrokers\n+      case PreferConsistent => ju.Collections.emptyMap[TopicPartition, String]()\n+      case PreferFixed(hostMap) => hostMap\n+    }\n+  }\n+\n+  // Keep this consistent with how other streams are named (e.g. \"Flume polling stream [2]\")\n+  private[streaming] override def name: String = s\"Kafka 0.10 direct stream [$id]\"\n+\n+  protected[streaming] override val checkpointData =\n+    new DirectKafkaInputDStreamCheckpointData\n+\n+\n+  /**\n+   * Asynchronously maintains & sends new rate limits to the receiver through the receiver tracker.\n+   */\n+  override protected[streaming] val rateController: Option[RateController] = {\n+    if (RateController.isBackPressureEnabled(ssc.conf)) {\n+      Some(new DirectKafkaRateController(id,\n+        RateEstimator.create(ssc.conf, context.graph.batchDuration)))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  private val maxRateLimitPerPartition: Int = context.sparkContext.getConf.getInt(\n+    \"spark.streaming.kafka.maxRatePerPartition\", 0)\n+\n+  protected[streaming] def maxMessagesPerPartition(\n+    offsets: Map[TopicPartition, Long]): Option[Map[TopicPartition, Long]] = {\n+    val estimatedRateLimit = rateController.map(_.getLatestRate().toInt)\n+\n+    // calculate a per-partition rate limit based on current lag\n+    val effectiveRateLimitPerPartition = estimatedRateLimit.filter(_ > 0) match {\n+      case Some(rate) =>\n+        val lagPerPartition = offsets.map { case (tp, offset) =>\n+          tp -> Math.max(offset - currentOffsets(tp), 0)\n+        }\n+        val totalLag = lagPerPartition.values.sum\n+\n+        lagPerPartition.map { case (tp, lag) =>\n+          val backpressureRate = Math.round(lag / totalLag.toFloat * rate)\n+          tp -> (if (maxRateLimitPerPartition > 0) {\n+            Math.min(backpressureRate, maxRateLimitPerPartition)} else backpressureRate)\n+        }\n+      case None => offsets.map { case (tp, offset) => tp -> maxRateLimitPerPartition }\n+    }\n+\n+    if (effectiveRateLimitPerPartition.values.sum > 0) {\n+      val secsPerBatch = context.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some(effectiveRateLimitPerPartition.map {\n+        case (tp, limit) => tp -> (secsPerBatch * limit).toLong\n+      })\n+    } else {\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Returns the latest (highest) available offsets, taking new partitions into account.\n+   */\n+  protected def latestOffsets(): Map[TopicPartition, Long] = {\n+    val c = consumer\n+    c.poll(0)\n+    val parts = c.assignment().asScala\n+\n+    // make sure new partitions are reflected in currentOffsets\n+    val newPartitions = parts.diff(currentOffsets.keySet)\n+    // position for new partitions determined by auto.offset.reset if no commit\n+    currentOffsets = currentOffsets ++ newPartitions.map(tp => tp -> c.position(tp)).toMap\n+    // don't want to consume messages, so pause\n+    c.pause(newPartitions.asJava)\n+    // find latest available offsets\n+    c.seekToEnd(currentOffsets.keySet.asJava)\n+    parts.map(tp => tp -> c.position(tp)).toMap\n+  }\n+\n+  // limits the maximum number of messages per partition\n+  protected def clamp(\n+    offsets: Map[TopicPartition, Long]): Map[TopicPartition, Long] = {\n+\n+    maxMessagesPerPartition(offsets).map { mmp =>\n+      mmp.map { case (tp, messages) =>\n+          val uo = offsets(tp)\n+          tp -> Math.min(currentOffsets(tp) + messages, uo)\n+      }\n+    }.getOrElse(offsets)\n+  }\n+\n+  override def compute(validTime: Time): Option[KafkaRDD[K, V]] = {\n+    val untilOffsets = clamp(latestOffsets())\n+    val offsetRanges = untilOffsets.map { case (tp, uo) =>\n+      val fo = currentOffsets(tp)\n+      OffsetRange(tp.topic, tp.partition, fo, uo)\n+    }\n+    val rdd = new KafkaRDD[K, V](\n+      context.sparkContext, executorKafkaParams, offsetRanges.toArray, getPreferredHosts, true)\n+\n+    // Report the record number and metadata of this batch interval to InputInfoTracker.\n+    val description = offsetRanges.filter { offsetRange =>\n+      // Don't display empty ranges.\n+      offsetRange.fromOffset != offsetRange.untilOffset\n+    }.map { offsetRange =>\n+      s\"topic: ${offsetRange.topic}\\tpartition: ${offsetRange.partition}\\t\" +\n+        s\"offsets: ${offsetRange.fromOffset} to ${offsetRange.untilOffset}\"\n+    }.mkString(\"\\n\")\n+    // Copy offsetRanges to immutable.List to prevent from being modified by the user\n+    val metadata = Map(\n+      \"offsets\" -> offsetRanges.toList,\n+      StreamInputInfo.METADATA_KEY_DESCRIPTION -> description)\n+    val inputInfo = StreamInputInfo(id, rdd.count, metadata)\n+    ssc.scheduler.inputInfoTracker.reportInfo(validTime, inputInfo)\n+\n+    currentOffsets = untilOffsets\n+    commitAll()\n+    Some(rdd)\n+  }\n+\n+  override def start(): Unit = {\n+    val c = consumer\n+    c.poll(0)\n+    if (currentOffsets.isEmpty) {\n+      currentOffsets = c.assignment().asScala.map { tp =>\n+        tp -> c.position(tp)\n+      }.toMap\n+    }\n+\n+    // don't actually want to consume any messages, so pause all partitions\n+    c.pause(currentOffsets.keySet.asJava)\n+  }\n+\n+  override def stop(): Unit = this.synchronized {\n+    if (kc != null) {\n+      kc.close()\n+    }\n+  }\n+\n+  protected val commitQueue = new ConcurrentLinkedQueue[OffsetRange]\n+  protected val commitCallback = new AtomicReference[OffsetCommitCallback]\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange]): Unit = {\n+    commitAsync(offsetRanges, null)\n+  }\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   * @param callback Only the most recently provided callback will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange], callback: OffsetCommitCallback): Unit = {\n+    commitCallback.set(callback)\n+    commitQueue.addAll(ju.Arrays.asList(offsetRanges: _*))\n+  }\n+\n+  protected def commitAll(): Unit = {\n+    val m = new ju.HashMap[TopicPartition, OffsetAndMetadata]()\n+    val it = commitQueue.iterator()\n+    while (it.hasNext) {\n+      val osr = it.next\n+      val tp = osr.topicPartition\n+      val x = m.get(tp)\n+      val offset = if (null == x) { osr.untilOffset } else { Math.max(x.offset, osr.untilOffset) }\n+      m.put(tp, new OffsetAndMetadata(offset))\n+    }\n+    if (!m.isEmpty) {\n+      consumer.commitAsync(m, commitCallback.get)"
  }, {
    "author": {
      "login": "zsxwing"
    },
    "body": "Got it. Thanks!\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-29T21:54:52Z",
    "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+import java.util.concurrent.ConcurrentLinkedQueue\n+import java.util.concurrent.atomic.AtomicReference\n+\n+import scala.annotation.tailrec\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.kafka.clients.consumer._\n+import org.apache.kafka.common.{ PartitionInfo, TopicPartition }\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.scheduler.{RateController, StreamInputInfo}\n+import org.apache.spark.streaming.scheduler.rate.RateEstimator\n+\n+/**\n+ *  A stream of [[KafkaRDD]] where\n+ * each given Kafka topic/partition corresponds to an RDD partition.\n+ * The spark configuration spark.streaming.kafka.maxRatePerPartition gives the maximum number\n+ *  of messages\n+ * per second that each '''partition''' will accept.\n+ * @param locationStrategy In most cases, pass in [[PreferConsistent]],\n+ *   see [[LocationStrategy]] for more details.\n+ * @param executorKafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.html#newconsumerconfigs\">\n+ * configuration parameters</a>.\n+ *   Requires  \"bootstrap.servers\" to be set with Kafka broker(s),\n+ *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+ * @param driverConsumer zero-argument function for you to construct a Kafka Consumer,\n+ *  and subscribe topics or assign partitions.\n+ *  This consumer will be used on the driver to query for offsets only, not messages.\n+ *  See <a href=\"http://kafka.apache.org/documentation.html#newconsumerapi\">Consumer doc</a>\n+ * @tparam K type of Kafka message key\n+ * @tparam V type of Kafka message value\n+ */\n+private[spark] class DirectKafkaInputDStream[K, V](\n+    _ssc: StreamingContext,\n+    locationStrategy: LocationStrategy,\n+    consumerStrategy: ConsumerStrategy[K, V]\n+  ) extends InputDStream[ConsumerRecord[K, V]](_ssc) with Logging with CanCommitOffsets {\n+\n+  val executorKafkaParams = {\n+    val ekp = new ju.HashMap[String, Object](consumerStrategy.executorKafkaParams)\n+    KafkaUtils.fixKafkaParams(ekp)\n+    ekp\n+  }\n+\n+  protected var currentOffsets = Map[TopicPartition, Long]()\n+\n+  @transient private var kc: Consumer[K, V] = null\n+  def consumer(): Consumer[K, V] = this.synchronized {\n+    if (null == kc) {\n+      kc = consumerStrategy.onStart(currentOffsets)\n+    }\n+    kc\n+  }\n+\n+  override def persist(newLevel: StorageLevel): DStream[ConsumerRecord[K, V]] = {\n+    logError(\"Kafka ConsumerRecord is not serializable. \" +\n+      \"Use .map to extract fields before calling .persist or .window\")\n+    super.persist(newLevel)\n+  }\n+\n+  protected def getBrokers = {\n+    val c = consumer\n+    val result = new ju.HashMap[TopicPartition, String]()\n+    val hosts = new ju.HashMap[TopicPartition, String]()\n+    val assignments = c.assignment().iterator()\n+    while (assignments.hasNext()) {\n+      val tp: TopicPartition = assignments.next()\n+      if (null == hosts.get(tp)) {\n+        val infos = c.partitionsFor(tp.topic).iterator()\n+        while (infos.hasNext()) {\n+          val i = infos.next()\n+          hosts.put(new TopicPartition(i.topic(), i.partition()), i.leader.host())\n+        }\n+      }\n+      result.put(tp, hosts.get(tp))\n+    }\n+    result\n+  }\n+\n+  protected def getPreferredHosts: ju.Map[TopicPartition, String] = {\n+    locationStrategy match {\n+      case PreferBrokers => getBrokers\n+      case PreferConsistent => ju.Collections.emptyMap[TopicPartition, String]()\n+      case PreferFixed(hostMap) => hostMap\n+    }\n+  }\n+\n+  // Keep this consistent with how other streams are named (e.g. \"Flume polling stream [2]\")\n+  private[streaming] override def name: String = s\"Kafka 0.10 direct stream [$id]\"\n+\n+  protected[streaming] override val checkpointData =\n+    new DirectKafkaInputDStreamCheckpointData\n+\n+\n+  /**\n+   * Asynchronously maintains & sends new rate limits to the receiver through the receiver tracker.\n+   */\n+  override protected[streaming] val rateController: Option[RateController] = {\n+    if (RateController.isBackPressureEnabled(ssc.conf)) {\n+      Some(new DirectKafkaRateController(id,\n+        RateEstimator.create(ssc.conf, context.graph.batchDuration)))\n+    } else {\n+      None\n+    }\n+  }\n+\n+  private val maxRateLimitPerPartition: Int = context.sparkContext.getConf.getInt(\n+    \"spark.streaming.kafka.maxRatePerPartition\", 0)\n+\n+  protected[streaming] def maxMessagesPerPartition(\n+    offsets: Map[TopicPartition, Long]): Option[Map[TopicPartition, Long]] = {\n+    val estimatedRateLimit = rateController.map(_.getLatestRate().toInt)\n+\n+    // calculate a per-partition rate limit based on current lag\n+    val effectiveRateLimitPerPartition = estimatedRateLimit.filter(_ > 0) match {\n+      case Some(rate) =>\n+        val lagPerPartition = offsets.map { case (tp, offset) =>\n+          tp -> Math.max(offset - currentOffsets(tp), 0)\n+        }\n+        val totalLag = lagPerPartition.values.sum\n+\n+        lagPerPartition.map { case (tp, lag) =>\n+          val backpressureRate = Math.round(lag / totalLag.toFloat * rate)\n+          tp -> (if (maxRateLimitPerPartition > 0) {\n+            Math.min(backpressureRate, maxRateLimitPerPartition)} else backpressureRate)\n+        }\n+      case None => offsets.map { case (tp, offset) => tp -> maxRateLimitPerPartition }\n+    }\n+\n+    if (effectiveRateLimitPerPartition.values.sum > 0) {\n+      val secsPerBatch = context.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some(effectiveRateLimitPerPartition.map {\n+        case (tp, limit) => tp -> (secsPerBatch * limit).toLong\n+      })\n+    } else {\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Returns the latest (highest) available offsets, taking new partitions into account.\n+   */\n+  protected def latestOffsets(): Map[TopicPartition, Long] = {\n+    val c = consumer\n+    c.poll(0)\n+    val parts = c.assignment().asScala\n+\n+    // make sure new partitions are reflected in currentOffsets\n+    val newPartitions = parts.diff(currentOffsets.keySet)\n+    // position for new partitions determined by auto.offset.reset if no commit\n+    currentOffsets = currentOffsets ++ newPartitions.map(tp => tp -> c.position(tp)).toMap\n+    // don't want to consume messages, so pause\n+    c.pause(newPartitions.asJava)\n+    // find latest available offsets\n+    c.seekToEnd(currentOffsets.keySet.asJava)\n+    parts.map(tp => tp -> c.position(tp)).toMap\n+  }\n+\n+  // limits the maximum number of messages per partition\n+  protected def clamp(\n+    offsets: Map[TopicPartition, Long]): Map[TopicPartition, Long] = {\n+\n+    maxMessagesPerPartition(offsets).map { mmp =>\n+      mmp.map { case (tp, messages) =>\n+          val uo = offsets(tp)\n+          tp -> Math.min(currentOffsets(tp) + messages, uo)\n+      }\n+    }.getOrElse(offsets)\n+  }\n+\n+  override def compute(validTime: Time): Option[KafkaRDD[K, V]] = {\n+    val untilOffsets = clamp(latestOffsets())\n+    val offsetRanges = untilOffsets.map { case (tp, uo) =>\n+      val fo = currentOffsets(tp)\n+      OffsetRange(tp.topic, tp.partition, fo, uo)\n+    }\n+    val rdd = new KafkaRDD[K, V](\n+      context.sparkContext, executorKafkaParams, offsetRanges.toArray, getPreferredHosts, true)\n+\n+    // Report the record number and metadata of this batch interval to InputInfoTracker.\n+    val description = offsetRanges.filter { offsetRange =>\n+      // Don't display empty ranges.\n+      offsetRange.fromOffset != offsetRange.untilOffset\n+    }.map { offsetRange =>\n+      s\"topic: ${offsetRange.topic}\\tpartition: ${offsetRange.partition}\\t\" +\n+        s\"offsets: ${offsetRange.fromOffset} to ${offsetRange.untilOffset}\"\n+    }.mkString(\"\\n\")\n+    // Copy offsetRanges to immutable.List to prevent from being modified by the user\n+    val metadata = Map(\n+      \"offsets\" -> offsetRanges.toList,\n+      StreamInputInfo.METADATA_KEY_DESCRIPTION -> description)\n+    val inputInfo = StreamInputInfo(id, rdd.count, metadata)\n+    ssc.scheduler.inputInfoTracker.reportInfo(validTime, inputInfo)\n+\n+    currentOffsets = untilOffsets\n+    commitAll()\n+    Some(rdd)\n+  }\n+\n+  override def start(): Unit = {\n+    val c = consumer\n+    c.poll(0)\n+    if (currentOffsets.isEmpty) {\n+      currentOffsets = c.assignment().asScala.map { tp =>\n+        tp -> c.position(tp)\n+      }.toMap\n+    }\n+\n+    // don't actually want to consume any messages, so pause all partitions\n+    c.pause(currentOffsets.keySet.asJava)\n+  }\n+\n+  override def stop(): Unit = this.synchronized {\n+    if (kc != null) {\n+      kc.close()\n+    }\n+  }\n+\n+  protected val commitQueue = new ConcurrentLinkedQueue[OffsetRange]\n+  protected val commitCallback = new AtomicReference[OffsetCommitCallback]\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange]): Unit = {\n+    commitAsync(offsetRanges, null)\n+  }\n+\n+  /**\n+   * Queue up offset ranges for commit to Kafka at a future time.  Threadsafe.\n+   * @param offsetRanges The maximum untilOffset for a given partition will be used at commit.\n+   * @param callback Only the most recently provided callback will be used at commit.\n+   */\n+  def commitAsync(offsetRanges: Array[OffsetRange], callback: OffsetCommitCallback): Unit = {\n+    commitCallback.set(callback)\n+    commitQueue.addAll(ju.Arrays.asList(offsetRanges: _*))\n+  }\n+\n+  protected def commitAll(): Unit = {\n+    val m = new ju.HashMap[TopicPartition, OffsetAndMetadata]()\n+    val it = commitQueue.iterator()\n+    while (it.hasNext) {\n+      val osr = it.next\n+      val tp = osr.topicPartition\n+      val x = m.get(tp)\n+      val offset = if (null == x) { osr.untilOffset } else { Math.max(x.offset, osr.untilOffset) }\n+      m.put(tp, new OffsetAndMetadata(offset))\n+    }\n+    if (!m.isEmpty) {\n+      consumer.commitAsync(m, commitCallback.get)"
  }],
  "prId": 11863
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "fix docs\n",
    "commit": "cffb0e0fb89808732c3ab3c1c7d83049549e2e2d",
    "createdAt": "2016-06-29T23:33:11Z",
    "diffHunk": "@@ -0,0 +1,320 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.{ util => ju }\n+import java.util.concurrent.ConcurrentLinkedQueue\n+import java.util.concurrent.atomic.AtomicReference\n+\n+import scala.annotation.tailrec\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+\n+import org.apache.kafka.clients.consumer._\n+import org.apache.kafka.common.{ PartitionInfo, TopicPartition }\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+import org.apache.spark.streaming.scheduler.{RateController, StreamInputInfo}\n+import org.apache.spark.streaming.scheduler.rate.RateEstimator\n+\n+/**\n+ *  A stream of [[KafkaRDD]] where\n+ * each given Kafka topic/partition corresponds to an RDD partition.\n+ * The spark configuration spark.streaming.kafka.maxRatePerPartition gives the maximum number\n+ *  of messages\n+ * per second that each '''partition''' will accept.\n+ * @param locationStrategy In most cases, pass in [[PreferConsistent]],\n+ *   see [[LocationStrategy]] for more details.\n+ * @param executorKafkaParams Kafka\n+ * <a href=\"http://kafka.apache.org/documentation.html#newconsumerconfigs\">\n+ * configuration parameters</a>.\n+ *   Requires  \"bootstrap.servers\" to be set with Kafka broker(s),\n+ *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+ * @param driverConsumer zero-argument function for you to construct a Kafka Consumer,"
  }],
  "prId": 11863
}]