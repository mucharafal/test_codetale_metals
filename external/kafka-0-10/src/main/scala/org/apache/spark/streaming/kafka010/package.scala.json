[{
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Last thing: I think you can take off the scalastyle:ignore?",
    "commit": "064dab5b796847ca8b203ccb6505c7288632ec56",
    "createdAt": "2019-04-04T09:18:29Z",
    "diffHunk": "@@ -17,7 +17,52 @@\n \n package org.apache.spark.streaming\n \n+import org.apache.spark.internal.config.ConfigBuilder\n+\n /**\n  * Spark Integration for Kafka 0.10\n  */\n-package object kafka010 //scalastyle:ignore\n+package object kafka010 { //scalastyle:ignore",
    "line": 10
  }, {
    "author": {
      "login": "beliefer"
    },
    "body": "> Last thing: I think you can take off the scalastyle:ignore?\r\n\r\nThe author of this object use this comment. I don't know the reason. If you think this comment is useless, I will remove it.\r\nIf I remove the `scalastyle:ignore`, I think must adjust the name of object `kafka010` and adjust the package path `org/apache/spark/streaming/kafka010/`. is this motion will lead some backward compatibility issue?",
    "commit": "064dab5b796847ca8b203ccb6505c7288632ec56",
    "createdAt": "2019-04-04T10:58:35Z",
    "diffHunk": "@@ -17,7 +17,52 @@\n \n package org.apache.spark.streaming\n \n+import org.apache.spark.internal.config.ConfigBuilder\n+\n /**\n  * Spark Integration for Kafka 0.10\n  */\n-package object kafka010 //scalastyle:ignore\n+package object kafka010 { //scalastyle:ignore",
    "line": 10
  }, {
    "author": {
      "login": "attilapiros"
    },
    "body": "As I know the reason is the digits in the object name, like here:\r\n\r\nhttps://github.com/apache/spark/blob/1d95dea30788b9f64c5e304d908b85936aafb238/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/package.scala#L21-L23",
    "commit": "064dab5b796847ca8b203ccb6505c7288632ec56",
    "createdAt": "2019-04-04T11:42:58Z",
    "diffHunk": "@@ -17,7 +17,52 @@\n \n package org.apache.spark.streaming\n \n+import org.apache.spark.internal.config.ConfigBuilder\n+\n /**\n  * Spark Integration for Kafka 0.10\n  */\n-package object kafka010 //scalastyle:ignore\n+package object kafka010 { //scalastyle:ignore",
    "line": 10
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "Oh OK, I thought it was because it's empty. Leave it then.",
    "commit": "064dab5b796847ca8b203ccb6505c7288632ec56",
    "createdAt": "2019-04-04T12:50:22Z",
    "diffHunk": "@@ -17,7 +17,52 @@\n \n package org.apache.spark.streaming\n \n+import org.apache.spark.internal.config.ConfigBuilder\n+\n /**\n  * Spark Integration for Kafka 0.10\n  */\n-package object kafka010 //scalastyle:ignore\n+package object kafka010 { //scalastyle:ignore",
    "line": 10
  }],
  "prId": 24267
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "Why not `.timeConf(TimeUnit.MILLISECONDS)`?",
    "commit": "064dab5b796847ca8b203ccb6505c7288632ec56",
    "createdAt": "2019-04-04T11:49:10Z",
    "diffHunk": "@@ -17,7 +17,52 @@\n \n package org.apache.spark.streaming\n \n+import org.apache.spark.internal.config.ConfigBuilder\n+\n /**\n  * Spark Integration for Kafka 0.10\n  */\n-package object kafka010 //scalastyle:ignore\n+package object kafka010 { //scalastyle:ignore\n+\n+  private[spark] val CONSUMER_CACHE_ENABLED =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.cache.enabled\")\n+      .booleanConf\n+      .createWithDefault(true)\n+\n+  private[spark] val CONSUMER_POLL_MS =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.poll.ms\")\n+    .longConf",
    "line": 19
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "Good question; unfortunately the key has \".ms\" so might look funny to specify `spark.streaming.kafka.consumer.poll.ms=10s`. I don't mind leaving it as-is for now as I think we took the same stance with other similar situations (?)",
    "commit": "064dab5b796847ca8b203ccb6505c7288632ec56",
    "createdAt": "2019-04-05T18:47:24Z",
    "diffHunk": "@@ -17,7 +17,52 @@\n \n package org.apache.spark.streaming\n \n+import org.apache.spark.internal.config.ConfigBuilder\n+\n /**\n  * Spark Integration for Kafka 0.10\n  */\n-package object kafka010 //scalastyle:ignore\n+package object kafka010 { //scalastyle:ignore\n+\n+  private[spark] val CONSUMER_CACHE_ENABLED =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.cache.enabled\")\n+      .booleanConf\n+      .createWithDefault(true)\n+\n+  private[spark] val CONSUMER_POLL_MS =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.poll.ms\")\n+    .longConf",
    "line": 19
  }, {
    "author": {
      "login": "beliefer"
    },
    "body": "> Why not `.timeConf(TimeUnit.MILLISECONDS)`?\r\n\r\nI think both method is good, so I think It is not necessary to use this.",
    "commit": "064dab5b796847ca8b203ccb6505c7288632ec56",
    "createdAt": "2019-04-06T11:00:24Z",
    "diffHunk": "@@ -17,7 +17,52 @@\n \n package org.apache.spark.streaming\n \n+import org.apache.spark.internal.config.ConfigBuilder\n+\n /**\n  * Spark Integration for Kafka 0.10\n  */\n-package object kafka010 //scalastyle:ignore\n+package object kafka010 { //scalastyle:ignore\n+\n+  private[spark] val CONSUMER_CACHE_ENABLED =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.cache.enabled\")\n+      .booleanConf\n+      .createWithDefault(true)\n+\n+  private[spark] val CONSUMER_POLL_MS =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.poll.ms\")\n+    .longConf",
    "line": 19
  }, {
    "author": {
      "login": "beliefer"
    },
    "body": "> Good question; unfortunately the key has \".ms\" so might look funny to specify `spark.streaming.kafka.consumer.poll.ms=10s`. I don't mind leaving it as-is for now as I think we took the same stance with other similar situations (?)\r\n\r\nI think both method is good, so I think It is not necessary to use `timeConf`.",
    "commit": "064dab5b796847ca8b203ccb6505c7288632ec56",
    "createdAt": "2019-04-06T11:01:03Z",
    "diffHunk": "@@ -17,7 +17,52 @@\n \n package org.apache.spark.streaming\n \n+import org.apache.spark.internal.config.ConfigBuilder\n+\n /**\n  * Spark Integration for Kafka 0.10\n  */\n-package object kafka010 //scalastyle:ignore\n+package object kafka010 { //scalastyle:ignore\n+\n+  private[spark] val CONSUMER_CACHE_ENABLED =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.cache.enabled\")\n+      .booleanConf\n+      .createWithDefault(true)\n+\n+  private[spark] val CONSUMER_POLL_MS =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.poll.ms\")\n+    .longConf",
    "line": 19
  }],
  "prId": 24267
}, {
  "comments": [{
    "author": {
      "login": "attilapiros"
    },
    "body": "Nit: remove extra new line after '}'",
    "commit": "064dab5b796847ca8b203ccb6505c7288632ec56",
    "createdAt": "2019-04-04T11:49:51Z",
    "diffHunk": "@@ -17,7 +17,52 @@\n \n package org.apache.spark.streaming\n \n+import org.apache.spark.internal.config.ConfigBuilder\n+\n /**\n  * Spark Integration for Kafka 0.10\n  */\n-package object kafka010 //scalastyle:ignore\n+package object kafka010 { //scalastyle:ignore\n+\n+  private[spark] val CONSUMER_CACHE_ENABLED =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.cache.enabled\")\n+      .booleanConf\n+      .createWithDefault(true)\n+\n+  private[spark] val CONSUMER_POLL_MS =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.poll.ms\")\n+    .longConf\n+    .createOptional\n+\n+  private[spark] val CONSUMER_CACHE_INITIAL_CAPACITY =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.cache.initialCapacity\")\n+    .intConf\n+    .createWithDefault(16)\n+\n+  private[spark] val CONSUMER_CACHE_MAX_CAPACITY =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.cache.maxCapacity\")\n+    .intConf\n+    .createWithDefault(64)\n+\n+  private[spark] val CONSUMER_CACHE_LOAD_FACTOR =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.cache.loadFactor\")\n+    .doubleConf\n+    .createWithDefault(0.75)\n+\n+  private[spark] val MAX_RATE_PER_PARTITION =\n+    ConfigBuilder(\"spark.streaming.kafka.maxRatePerPartition\")\n+    .longConf\n+    .createWithDefault(0)\n+\n+  private[spark] val MIN_RATE_PER_PARTITION =\n+    ConfigBuilder(\"spark.streaming.kafka.minRatePerPartition\")\n+    .longConf\n+    .createWithDefault(1)\n+\n+  private[spark] val ALLOW_NON_CONSECUTIVE_OFFSETS =\n+    ConfigBuilder(\"spark.streaming.kafka.allowNonConsecutiveOffsets\")\n+    .booleanConf\n+    .createWithDefault(false)\n+\n+}\n+",
    "line": 53
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "Yeah could remove it but it doesn't really matter IMHO. Some files have a newline at the end some don't.",
    "commit": "064dab5b796847ca8b203ccb6505c7288632ec56",
    "createdAt": "2019-04-05T18:47:49Z",
    "diffHunk": "@@ -17,7 +17,52 @@\n \n package org.apache.spark.streaming\n \n+import org.apache.spark.internal.config.ConfigBuilder\n+\n /**\n  * Spark Integration for Kafka 0.10\n  */\n-package object kafka010 //scalastyle:ignore\n+package object kafka010 { //scalastyle:ignore\n+\n+  private[spark] val CONSUMER_CACHE_ENABLED =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.cache.enabled\")\n+      .booleanConf\n+      .createWithDefault(true)\n+\n+  private[spark] val CONSUMER_POLL_MS =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.poll.ms\")\n+    .longConf\n+    .createOptional\n+\n+  private[spark] val CONSUMER_CACHE_INITIAL_CAPACITY =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.cache.initialCapacity\")\n+    .intConf\n+    .createWithDefault(16)\n+\n+  private[spark] val CONSUMER_CACHE_MAX_CAPACITY =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.cache.maxCapacity\")\n+    .intConf\n+    .createWithDefault(64)\n+\n+  private[spark] val CONSUMER_CACHE_LOAD_FACTOR =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.cache.loadFactor\")\n+    .doubleConf\n+    .createWithDefault(0.75)\n+\n+  private[spark] val MAX_RATE_PER_PARTITION =\n+    ConfigBuilder(\"spark.streaming.kafka.maxRatePerPartition\")\n+    .longConf\n+    .createWithDefault(0)\n+\n+  private[spark] val MIN_RATE_PER_PARTITION =\n+    ConfigBuilder(\"spark.streaming.kafka.minRatePerPartition\")\n+    .longConf\n+    .createWithDefault(1)\n+\n+  private[spark] val ALLOW_NON_CONSECUTIVE_OFFSETS =\n+    ConfigBuilder(\"spark.streaming.kafka.allowNonConsecutiveOffsets\")\n+    .booleanConf\n+    .createWithDefault(false)\n+\n+}\n+",
    "line": 53
  }, {
    "author": {
      "login": "attilapiros"
    },
    "body": "I see. Thanks for informing me! I won't raise this next time.",
    "commit": "064dab5b796847ca8b203ccb6505c7288632ec56",
    "createdAt": "2019-04-05T19:03:50Z",
    "diffHunk": "@@ -17,7 +17,52 @@\n \n package org.apache.spark.streaming\n \n+import org.apache.spark.internal.config.ConfigBuilder\n+\n /**\n  * Spark Integration for Kafka 0.10\n  */\n-package object kafka010 //scalastyle:ignore\n+package object kafka010 { //scalastyle:ignore\n+\n+  private[spark] val CONSUMER_CACHE_ENABLED =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.cache.enabled\")\n+      .booleanConf\n+      .createWithDefault(true)\n+\n+  private[spark] val CONSUMER_POLL_MS =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.poll.ms\")\n+    .longConf\n+    .createOptional\n+\n+  private[spark] val CONSUMER_CACHE_INITIAL_CAPACITY =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.cache.initialCapacity\")\n+    .intConf\n+    .createWithDefault(16)\n+\n+  private[spark] val CONSUMER_CACHE_MAX_CAPACITY =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.cache.maxCapacity\")\n+    .intConf\n+    .createWithDefault(64)\n+\n+  private[spark] val CONSUMER_CACHE_LOAD_FACTOR =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.cache.loadFactor\")\n+    .doubleConf\n+    .createWithDefault(0.75)\n+\n+  private[spark] val MAX_RATE_PER_PARTITION =\n+    ConfigBuilder(\"spark.streaming.kafka.maxRatePerPartition\")\n+    .longConf\n+    .createWithDefault(0)\n+\n+  private[spark] val MIN_RATE_PER_PARTITION =\n+    ConfigBuilder(\"spark.streaming.kafka.minRatePerPartition\")\n+    .longConf\n+    .createWithDefault(1)\n+\n+  private[spark] val ALLOW_NON_CONSECUTIVE_OFFSETS =\n+    ConfigBuilder(\"spark.streaming.kafka.allowNonConsecutiveOffsets\")\n+    .booleanConf\n+    .createWithDefault(false)\n+\n+}\n+",
    "line": 53
  }, {
    "author": {
      "login": "beliefer"
    },
    "body": "> Nit: remove extra new line after '}'\r\n\r\nSpark code check need have a blank line at the end of the file.",
    "commit": "064dab5b796847ca8b203ccb6505c7288632ec56",
    "createdAt": "2019-04-06T11:04:16Z",
    "diffHunk": "@@ -17,7 +17,52 @@\n \n package org.apache.spark.streaming\n \n+import org.apache.spark.internal.config.ConfigBuilder\n+\n /**\n  * Spark Integration for Kafka 0.10\n  */\n-package object kafka010 //scalastyle:ignore\n+package object kafka010 { //scalastyle:ignore\n+\n+  private[spark] val CONSUMER_CACHE_ENABLED =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.cache.enabled\")\n+      .booleanConf\n+      .createWithDefault(true)\n+\n+  private[spark] val CONSUMER_POLL_MS =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.poll.ms\")\n+    .longConf\n+    .createOptional\n+\n+  private[spark] val CONSUMER_CACHE_INITIAL_CAPACITY =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.cache.initialCapacity\")\n+    .intConf\n+    .createWithDefault(16)\n+\n+  private[spark] val CONSUMER_CACHE_MAX_CAPACITY =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.cache.maxCapacity\")\n+    .intConf\n+    .createWithDefault(64)\n+\n+  private[spark] val CONSUMER_CACHE_LOAD_FACTOR =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.cache.loadFactor\")\n+    .doubleConf\n+    .createWithDefault(0.75)\n+\n+  private[spark] val MAX_RATE_PER_PARTITION =\n+    ConfigBuilder(\"spark.streaming.kafka.maxRatePerPartition\")\n+    .longConf\n+    .createWithDefault(0)\n+\n+  private[spark] val MIN_RATE_PER_PARTITION =\n+    ConfigBuilder(\"spark.streaming.kafka.minRatePerPartition\")\n+    .longConf\n+    .createWithDefault(1)\n+\n+  private[spark] val ALLOW_NON_CONSECUTIVE_OFFSETS =\n+    ConfigBuilder(\"spark.streaming.kafka.allowNonConsecutiveOffsets\")\n+    .booleanConf\n+    .createWithDefault(false)\n+\n+}\n+",
    "line": 53
  }, {
    "author": {
      "login": "beliefer"
    },
    "body": "> Yeah could remove it but it doesn't really matter IMHO. Some files have a newline at the end some don't.\r\n\r\nSpark code check need have a blank line at the end of the file. So I think there don't have to make a change.",
    "commit": "064dab5b796847ca8b203ccb6505c7288632ec56",
    "createdAt": "2019-04-06T11:05:57Z",
    "diffHunk": "@@ -17,7 +17,52 @@\n \n package org.apache.spark.streaming\n \n+import org.apache.spark.internal.config.ConfigBuilder\n+\n /**\n  * Spark Integration for Kafka 0.10\n  */\n-package object kafka010 //scalastyle:ignore\n+package object kafka010 { //scalastyle:ignore\n+\n+  private[spark] val CONSUMER_CACHE_ENABLED =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.cache.enabled\")\n+      .booleanConf\n+      .createWithDefault(true)\n+\n+  private[spark] val CONSUMER_POLL_MS =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.poll.ms\")\n+    .longConf\n+    .createOptional\n+\n+  private[spark] val CONSUMER_CACHE_INITIAL_CAPACITY =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.cache.initialCapacity\")\n+    .intConf\n+    .createWithDefault(16)\n+\n+  private[spark] val CONSUMER_CACHE_MAX_CAPACITY =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.cache.maxCapacity\")\n+    .intConf\n+    .createWithDefault(64)\n+\n+  private[spark] val CONSUMER_CACHE_LOAD_FACTOR =\n+    ConfigBuilder(\"spark.streaming.kafka.consumer.cache.loadFactor\")\n+    .doubleConf\n+    .createWithDefault(0.75)\n+\n+  private[spark] val MAX_RATE_PER_PARTITION =\n+    ConfigBuilder(\"spark.streaming.kafka.maxRatePerPartition\")\n+    .longConf\n+    .createWithDefault(0)\n+\n+  private[spark] val MIN_RATE_PER_PARTITION =\n+    ConfigBuilder(\"spark.streaming.kafka.minRatePerPartition\")\n+    .longConf\n+    .createWithDefault(1)\n+\n+  private[spark] val ALLOW_NON_CONSECUTIVE_OFFSETS =\n+    ConfigBuilder(\"spark.streaming.kafka.allowNonConsecutiveOffsets\")\n+    .booleanConf\n+    .createWithDefault(false)\n+\n+}\n+",
    "line": 53
  }],
  "prId": 24267
}]