[{
  "comments": [{
    "author": {
      "login": "tedyu"
    },
    "body": "Add class level document, please.\n",
    "commit": "30f7343018d8d3bf8e8f1511a56511d35aaa025f",
    "createdAt": "2014-03-21T21:38:14Z",
    "diffHunk": "@@ -0,0 +1,47 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.nosql.hbase\n+\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.hadoop.io.Text\n+import org.apache.spark.TaskContext\n+import org.apache.spark.rdd.RDD\n+\n+object HBaseUtils {"
  }],
  "prId": 194
}, {
  "comments": [{
    "author": {
      "login": "tedyu"
    },
    "body": "Should this be enclosed in finally block ?\n",
    "commit": "30f7343018d8d3bf8e8f1511a56511d35aaa025f",
    "createdAt": "2014-03-21T21:40:45Z",
    "diffHunk": "@@ -0,0 +1,47 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.nosql.hbase\n+\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.hadoop.io.Text\n+import org.apache.spark.TaskContext\n+import org.apache.spark.rdd.RDD\n+\n+object HBaseUtils {\n+  /**\n+   * Save RDD to HBase\n+   */\n+  def saveAsHBaseTable(rdd: RDD[Text], zkHost: String, zkPort: String, zkNode: String,\n+                       table: String, rowkeyType: String, columns: List[HBaseColumn], delimiter: Char) {\n+    val conf = new HBaseConf(zkHost, zkPort, zkNode, table, rowkeyType, columns, delimiter)\n+\n+    def writeToHBase(iter: Iterator[Text]) {\n+      val writer = new SparkHBaseWriter(conf)\n+      writer.init()\n+\n+      while (iter.hasNext) {\n+        val record = iter.next()\n+        writer.write(record)\n+      }\n+\n+      writer.close()"
  }],
  "prId": 194
}, {
  "comments": [{
    "author": {
      "login": "mridulm"
    },
    "body": "if init fails, how does close behave ? is it a noop ?\nbtw, since close can throw exceptions of its own, might be good idea to wrap it in try/catch - so that if finally is getting hit via an exception in the try block, user will know the actual reason for the exception.\n",
    "commit": "30f7343018d8d3bf8e8f1511a56511d35aaa025f",
    "createdAt": "2014-03-22T14:57:47Z",
    "diffHunk": "@@ -0,0 +1,60 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.nosql.hbase\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * A public object that provide HBase supports.\n+ * You could save RDD into HBase through [[org.apache.spark.nosql.hbase.HBaseUtils.saveAsHBaseTable]] method.\n+ */\n+object HBaseUtils {\n+\n+  /**\n+   * Save [[org.apache.spark.rdd.RDD[Text]]] as a HBase table\n+   * @param rdd [[org.apache.spark.rdd.RDD[Text]]]\n+   * @param zkHost the zookeeper hosts. e.g. \"10.232.98.10,10.232.98.11,10.232.98.12\"\n+   * @param zkPort the zookeeper client listening port. e.g. \"2181\"\n+   * @param zkNode the zookeeper znode of HBase. e.g. \"hbase-apache\"\n+   * @param table the name of table which we save records\n+   * @param rowkeyType the type of rowkey. [[org.apache.spark.nosql.hbase.HBaseType]]\n+   * @param columns the column list. [[org.apache.spark.nosql.hbase.HBaseColumn]]\n+   * @param delimiter the delimiter which used to split record into fields\n+   */\n+  def saveAsHBaseTable(rdd: RDD[Text], zkHost: String, zkPort: String, zkNode: String,\n+                       table: String, rowkeyType: String, columns: List[HBaseColumn], delimiter: Char) {\n+    val conf = new HBaseConf(zkHost, zkPort, zkNode, table, rowkeyType, columns, delimiter)\n+\n+    def writeToHBase(iter: Iterator[Text]) {\n+      val writer = new SparkHBaseWriter(conf)\n+\n+      try {\n+        writer.init()\n+        while (iter.hasNext) {\n+          val record = iter.next()\n+          writer.write(record)\n+        }\n+      } finally {\n+        writer.close()\n+      }"
  }, {
    "author": {
      "login": "haosdent"
    },
    "body": "@mridulm Thanks for your great help! I am not a native English speaker. Do you mean the code should looks like this?\n\n``` java\n    def writeToHBase(iter: Iterator[Text]) {\n      val writer = new SparkHBaseWriter(conf)\n\n      writer.init()\n      while (iter.hasNext) {\n        val record = iter.next()\n        writer.write(record)\n      }\n      writer.close()\n    }\n```\n",
    "commit": "30f7343018d8d3bf8e8f1511a56511d35aaa025f",
    "createdAt": "2014-03-22T15:13:33Z",
    "diffHunk": "@@ -0,0 +1,60 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.nosql.hbase\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * A public object that provide HBase supports.\n+ * You could save RDD into HBase through [[org.apache.spark.nosql.hbase.HBaseUtils.saveAsHBaseTable]] method.\n+ */\n+object HBaseUtils {\n+\n+  /**\n+   * Save [[org.apache.spark.rdd.RDD[Text]]] as a HBase table\n+   * @param rdd [[org.apache.spark.rdd.RDD[Text]]]\n+   * @param zkHost the zookeeper hosts. e.g. \"10.232.98.10,10.232.98.11,10.232.98.12\"\n+   * @param zkPort the zookeeper client listening port. e.g. \"2181\"\n+   * @param zkNode the zookeeper znode of HBase. e.g. \"hbase-apache\"\n+   * @param table the name of table which we save records\n+   * @param rowkeyType the type of rowkey. [[org.apache.spark.nosql.hbase.HBaseType]]\n+   * @param columns the column list. [[org.apache.spark.nosql.hbase.HBaseColumn]]\n+   * @param delimiter the delimiter which used to split record into fields\n+   */\n+  def saveAsHBaseTable(rdd: RDD[Text], zkHost: String, zkPort: String, zkNode: String,\n+                       table: String, rowkeyType: String, columns: List[HBaseColumn], delimiter: Char) {\n+    val conf = new HBaseConf(zkHost, zkPort, zkNode, table, rowkeyType, columns, delimiter)\n+\n+    def writeToHBase(iter: Iterator[Text]) {\n+      val writer = new SparkHBaseWriter(conf)\n+\n+      try {\n+        writer.init()\n+        while (iter.hasNext) {\n+          val record = iter.next()\n+          writer.write(record)\n+        }\n+      } finally {\n+        writer.close()\n+      }"
  }, {
    "author": {
      "login": "mridulm"
    },
    "body": "No, what you have currently is much better than this.\nSo there are two issues here :\n\n1) If init() throws an exception, and we get to finally block; how will writer.close() behave ?\na) Will it throw an exception ?\nb) Will it ignore ? (I would hope this)\nc) Is it undefined ? In which case we cant rely on this.\n\n2) As part of executing the loop, where we are calling writer.write; it is possible for an exception to be thrown.\nWhich gets us to the finally block where we try to close the writer -> which can again throw an IOException (from what I recall about writer's javadoc).\nNow what user will see in the logs will be the second exception - and the reason for that, the first writer.write exception (or some other in the try block) will be lost.\n\nTo handle (2), within the finally block, simply wrap the writer.close() in a try/catch and log the message & ignore there.\nTo handle (1), we will need to know what the expectation from init() is.\n",
    "commit": "30f7343018d8d3bf8e8f1511a56511d35aaa025f",
    "createdAt": "2014-03-22T15:23:38Z",
    "diffHunk": "@@ -0,0 +1,60 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.nosql.hbase\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * A public object that provide HBase supports.\n+ * You could save RDD into HBase through [[org.apache.spark.nosql.hbase.HBaseUtils.saveAsHBaseTable]] method.\n+ */\n+object HBaseUtils {\n+\n+  /**\n+   * Save [[org.apache.spark.rdd.RDD[Text]]] as a HBase table\n+   * @param rdd [[org.apache.spark.rdd.RDD[Text]]]\n+   * @param zkHost the zookeeper hosts. e.g. \"10.232.98.10,10.232.98.11,10.232.98.12\"\n+   * @param zkPort the zookeeper client listening port. e.g. \"2181\"\n+   * @param zkNode the zookeeper znode of HBase. e.g. \"hbase-apache\"\n+   * @param table the name of table which we save records\n+   * @param rowkeyType the type of rowkey. [[org.apache.spark.nosql.hbase.HBaseType]]\n+   * @param columns the column list. [[org.apache.spark.nosql.hbase.HBaseColumn]]\n+   * @param delimiter the delimiter which used to split record into fields\n+   */\n+  def saveAsHBaseTable(rdd: RDD[Text], zkHost: String, zkPort: String, zkNode: String,\n+                       table: String, rowkeyType: String, columns: List[HBaseColumn], delimiter: Char) {\n+    val conf = new HBaseConf(zkHost, zkPort, zkNode, table, rowkeyType, columns, delimiter)\n+\n+    def writeToHBase(iter: Iterator[Text]) {\n+      val writer = new SparkHBaseWriter(conf)\n+\n+      try {\n+        writer.init()\n+        while (iter.hasNext) {\n+          val record = iter.next()\n+          writer.write(record)\n+        }\n+      } finally {\n+        writer.close()\n+      }"
  }, {
    "author": {
      "login": "haosdent"
    },
    "body": "@mridulm Thank you for your clarify. Let me fix this.\n",
    "commit": "30f7343018d8d3bf8e8f1511a56511d35aaa025f",
    "createdAt": "2014-03-22T15:28:40Z",
    "diffHunk": "@@ -0,0 +1,60 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.nosql.hbase\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * A public object that provide HBase supports.\n+ * You could save RDD into HBase through [[org.apache.spark.nosql.hbase.HBaseUtils.saveAsHBaseTable]] method.\n+ */\n+object HBaseUtils {\n+\n+  /**\n+   * Save [[org.apache.spark.rdd.RDD[Text]]] as a HBase table\n+   * @param rdd [[org.apache.spark.rdd.RDD[Text]]]\n+   * @param zkHost the zookeeper hosts. e.g. \"10.232.98.10,10.232.98.11,10.232.98.12\"\n+   * @param zkPort the zookeeper client listening port. e.g. \"2181\"\n+   * @param zkNode the zookeeper znode of HBase. e.g. \"hbase-apache\"\n+   * @param table the name of table which we save records\n+   * @param rowkeyType the type of rowkey. [[org.apache.spark.nosql.hbase.HBaseType]]\n+   * @param columns the column list. [[org.apache.spark.nosql.hbase.HBaseColumn]]\n+   * @param delimiter the delimiter which used to split record into fields\n+   */\n+  def saveAsHBaseTable(rdd: RDD[Text], zkHost: String, zkPort: String, zkNode: String,\n+                       table: String, rowkeyType: String, columns: List[HBaseColumn], delimiter: Char) {\n+    val conf = new HBaseConf(zkHost, zkPort, zkNode, table, rowkeyType, columns, delimiter)\n+\n+    def writeToHBase(iter: Iterator[Text]) {\n+      val writer = new SparkHBaseWriter(conf)\n+\n+      try {\n+        writer.init()\n+        while (iter.hasNext) {\n+          val record = iter.next()\n+          writer.write(record)\n+        }\n+      } finally {\n+        writer.close()\n+      }"
  }, {
    "author": {
      "login": "tedyu"
    },
    "body": "See my comment in close() method below.\nIf init() throws exception, htable would be null. With an additional null check, writer.close() should be a no-op.\n",
    "commit": "30f7343018d8d3bf8e8f1511a56511d35aaa025f",
    "createdAt": "2014-03-22T16:00:58Z",
    "diffHunk": "@@ -0,0 +1,60 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.nosql.hbase\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * A public object that provide HBase supports.\n+ * You could save RDD into HBase through [[org.apache.spark.nosql.hbase.HBaseUtils.saveAsHBaseTable]] method.\n+ */\n+object HBaseUtils {\n+\n+  /**\n+   * Save [[org.apache.spark.rdd.RDD[Text]]] as a HBase table\n+   * @param rdd [[org.apache.spark.rdd.RDD[Text]]]\n+   * @param zkHost the zookeeper hosts. e.g. \"10.232.98.10,10.232.98.11,10.232.98.12\"\n+   * @param zkPort the zookeeper client listening port. e.g. \"2181\"\n+   * @param zkNode the zookeeper znode of HBase. e.g. \"hbase-apache\"\n+   * @param table the name of table which we save records\n+   * @param rowkeyType the type of rowkey. [[org.apache.spark.nosql.hbase.HBaseType]]\n+   * @param columns the column list. [[org.apache.spark.nosql.hbase.HBaseColumn]]\n+   * @param delimiter the delimiter which used to split record into fields\n+   */\n+  def saveAsHBaseTable(rdd: RDD[Text], zkHost: String, zkPort: String, zkNode: String,\n+                       table: String, rowkeyType: String, columns: List[HBaseColumn], delimiter: Char) {\n+    val conf = new HBaseConf(zkHost, zkPort, zkNode, table, rowkeyType, columns, delimiter)\n+\n+    def writeToHBase(iter: Iterator[Text]) {\n+      val writer = new SparkHBaseWriter(conf)\n+\n+      try {\n+        writer.init()\n+        while (iter.hasNext) {\n+          val record = iter.next()\n+          writer.write(record)\n+        }\n+      } finally {\n+        writer.close()\n+      }"
  }],
  "prId": 194
}, {
  "comments": [{
    "author": {
      "login": "tedyu"
    },
    "body": "nit: should read:\nformat of records in RDD should look\n",
    "commit": "30f7343018d8d3bf8e8f1511a56511d35aaa025f",
    "createdAt": "2014-03-22T17:19:15Z",
    "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.nosql.hbase\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * A public object that provide HBase supports.\n+ * You could save RDD into HBase through [[org.apache.spark.nosql.hbase.HBaseUtils.saveAsHBaseTable]] method.\n+ */\n+object HBaseUtils {\n+\n+  /**\n+   * Save [[org.apache.spark.rdd.RDD[Text]]] as a HBase table\n+   *\n+   * The format of record in RDD should looks like this:"
  }],
  "prId": 194
}, {
  "comments": [{
    "author": {
      "login": "tedyu"
    },
    "body": "nit: should read:\nprovides HBase support\n",
    "commit": "30f7343018d8d3bf8e8f1511a56511d35aaa025f",
    "createdAt": "2014-03-22T17:19:55Z",
    "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.nosql.hbase\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * A public object that provide HBase supports."
  }, {
    "author": {
      "login": "haosdent"
    },
    "body": "my poor english~\n",
    "commit": "30f7343018d8d3bf8e8f1511a56511d35aaa025f",
    "createdAt": "2014-03-22T17:24:34Z",
    "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.nosql.hbase\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * A public object that provide HBase supports."
  }],
  "prId": 194
}, {
  "comments": [{
    "author": {
      "login": "tedyu"
    },
    "body": "I thought try/catch blocks would be added here.\n",
    "commit": "30f7343018d8d3bf8e8f1511a56511d35aaa025f",
    "createdAt": "2014-03-22T17:25:30Z",
    "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.nosql.hbase\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * A public object that provide HBase supports.\n+ * You could save RDD into HBase through [[org.apache.spark.nosql.hbase.HBaseUtils.saveAsHBaseTable]] method.\n+ */\n+object HBaseUtils {\n+\n+  /**\n+   * Save [[org.apache.spark.rdd.RDD[Text]]] as a HBase table\n+   *\n+   * The format of record in RDD should looks like this:\n+   *   rowkey|delimiter|column|delimiter|column|delimiter|...\n+   * For example (if delimiter is \",\"):\n+   *   0001,apple,banana\n+   * \"0001\" is rowkey field while \"apple\" and \"banana\" are column fields.\n+   *\n+   * @param rdd [[org.apache.spark.rdd.RDD[Text]]]\n+   * @param zkHost the zookeeper hosts. e.g. \"10.232.98.10,10.232.98.11,10.232.98.12\"\n+   * @param zkPort the zookeeper client listening port. e.g. \"2181\"\n+   * @param zkNode the zookeeper znode of HBase. e.g. \"hbase-apache\"\n+   * @param table the name of table which we save records\n+   * @param rowkeyType the type of rowkey. [[org.apache.spark.nosql.hbase.HBaseType]]\n+   * @param columns the column list. [[org.apache.spark.nosql.hbase.HBaseColumn]]\n+   * @param delimiter the delimiter which used to split record into fields\n+   */\n+  def saveAsHBaseTable(rdd: RDD[Text], zkHost: String, zkPort: String, zkNode: String,\n+                       table: String, rowkeyType: String, columns: List[HBaseColumn], delimiter: Char) {\n+    val conf = new HBaseConf(zkHost, zkPort, zkNode, table, rowkeyType, columns, delimiter)\n+\n+    def writeToHBase(iter: Iterator[Text]) {\n+      val writer = new SparkHBaseWriter(conf)\n+\n+      try {\n+        writer.init()\n+\n+        while (iter.hasNext) {\n+          val record = iter.next()\n+          writer.write(record)\n+        }\n+      } finally {\n+        writer.close()"
  }, {
    "author": {
      "login": "haosdent"
    },
    "body": "Yep, move it out would be better. Have already update this.\n",
    "commit": "30f7343018d8d3bf8e8f1511a56511d35aaa025f",
    "createdAt": "2014-03-22T17:38:26Z",
    "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.nosql.hbase\n+\n+import org.apache.hadoop.io.Text\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * A public object that provide HBase supports.\n+ * You could save RDD into HBase through [[org.apache.spark.nosql.hbase.HBaseUtils.saveAsHBaseTable]] method.\n+ */\n+object HBaseUtils {\n+\n+  /**\n+   * Save [[org.apache.spark.rdd.RDD[Text]]] as a HBase table\n+   *\n+   * The format of record in RDD should looks like this:\n+   *   rowkey|delimiter|column|delimiter|column|delimiter|...\n+   * For example (if delimiter is \",\"):\n+   *   0001,apple,banana\n+   * \"0001\" is rowkey field while \"apple\" and \"banana\" are column fields.\n+   *\n+   * @param rdd [[org.apache.spark.rdd.RDD[Text]]]\n+   * @param zkHost the zookeeper hosts. e.g. \"10.232.98.10,10.232.98.11,10.232.98.12\"\n+   * @param zkPort the zookeeper client listening port. e.g. \"2181\"\n+   * @param zkNode the zookeeper znode of HBase. e.g. \"hbase-apache\"\n+   * @param table the name of table which we save records\n+   * @param rowkeyType the type of rowkey. [[org.apache.spark.nosql.hbase.HBaseType]]\n+   * @param columns the column list. [[org.apache.spark.nosql.hbase.HBaseColumn]]\n+   * @param delimiter the delimiter which used to split record into fields\n+   */\n+  def saveAsHBaseTable(rdd: RDD[Text], zkHost: String, zkPort: String, zkNode: String,\n+                       table: String, rowkeyType: String, columns: List[HBaseColumn], delimiter: Char) {\n+    val conf = new HBaseConf(zkHost, zkPort, zkNode, table, rowkeyType, columns, delimiter)\n+\n+    def writeToHBase(iter: Iterator[Text]) {\n+      val writer = new SparkHBaseWriter(conf)\n+\n+      try {\n+        writer.init()\n+\n+        while (iter.hasNext) {\n+          val record = iter.next()\n+          writer.write(record)\n+        }\n+      } finally {\n+        writer.close()"
  }],
  "prId": 194
}]