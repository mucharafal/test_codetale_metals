[{
  "comments": [{
    "author": {
      "login": "mridulm"
    },
    "body": "If this is internal to hbase package, restrict visibility to under hbase and not apache.\n",
    "commit": "30f7343018d8d3bf8e8f1511a56511d35aaa025f",
    "createdAt": "2014-03-22T14:59:10Z",
    "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.nosql.hbase\n+\n+import org.apache.hadoop.hbase.client.{Put, HTable}\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.commons.codec.binary.Hex\n+import org.apache.hadoop.hbase.HConstants\n+import org.apache.hadoop.conf.Configuration\n+import java.io.IOException\n+\n+/**\n+ * Internal helper class that saves an RDD using a HBase OutputFormat. This is only public\n+ * because we need to access this class from the `spark` package to use some package-private HBase\n+ * functions, but this class should not be used directly by users.\n+ */\n+private[apache]"
  }],
  "prId": 194
}, {
  "comments": [{
    "author": {
      "login": "mridulm"
    },
    "body": "rename as toByteArray\n",
    "commit": "30f7343018d8d3bf8e8f1511a56511d35aaa025f",
    "createdAt": "2014-03-22T14:59:44Z",
    "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.nosql.hbase\n+\n+import org.apache.hadoop.hbase.client.{Put, HTable}\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.commons.codec.binary.Hex\n+import org.apache.hadoop.hbase.HConstants\n+import org.apache.hadoop.conf.Configuration\n+import java.io.IOException\n+\n+/**\n+ * Internal helper class that saves an RDD using a HBase OutputFormat. This is only public\n+ * because we need to access this class from the `spark` package to use some package-private HBase\n+ * functions, but this class should not be used directly by users.\n+ */\n+private[apache]\n+class SparkHBaseWriter(conf: HBaseConf) {\n+\n+  private var htable: HTable = null\n+\n+  val zkHost = conf.zkHost\n+  val zkPort = conf.zkPort\n+  val zkNode = conf.zkNode\n+  val table = conf.table\n+  val rowkeyType = conf.rowkeyType\n+  val columns = conf.columns\n+  val delimiter = conf.delimiter\n+\n+  def init() {\n+    val conf = new Configuration()\n+    conf.set(HConstants.ZOOKEEPER_QUORUM, zkHost)\n+    conf.set(HConstants.ZOOKEEPER_CLIENT_PORT, zkPort)\n+    conf.set(HConstants.ZOOKEEPER_ZNODE_PARENT, zkNode)\n+    htable = new HTable(conf, table)\n+    // Use default writebuffersize to submit batch puts\n+    htable.setAutoFlush(false)\n+  }\n+\n+  /**\n+   * Convert field to bytes\n+   * @param field split by delimiter from record\n+   * @param kind the type of field\n+   * @return\n+   */\n+  def toByteArr(field: String, kind: String) = kind match {"
  }, {
    "author": {
      "login": "mridulm"
    },
    "body": "or getBytes() whichever sounds better : both are used in java api for different reasons.\n",
    "commit": "30f7343018d8d3bf8e8f1511a56511d35aaa025f",
    "createdAt": "2014-03-22T15:26:35Z",
    "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.nosql.hbase\n+\n+import org.apache.hadoop.hbase.client.{Put, HTable}\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.commons.codec.binary.Hex\n+import org.apache.hadoop.hbase.HConstants\n+import org.apache.hadoop.conf.Configuration\n+import java.io.IOException\n+\n+/**\n+ * Internal helper class that saves an RDD using a HBase OutputFormat. This is only public\n+ * because we need to access this class from the `spark` package to use some package-private HBase\n+ * functions, but this class should not be used directly by users.\n+ */\n+private[apache]\n+class SparkHBaseWriter(conf: HBaseConf) {\n+\n+  private var htable: HTable = null\n+\n+  val zkHost = conf.zkHost\n+  val zkPort = conf.zkPort\n+  val zkNode = conf.zkNode\n+  val table = conf.table\n+  val rowkeyType = conf.rowkeyType\n+  val columns = conf.columns\n+  val delimiter = conf.delimiter\n+\n+  def init() {\n+    val conf = new Configuration()\n+    conf.set(HConstants.ZOOKEEPER_QUORUM, zkHost)\n+    conf.set(HConstants.ZOOKEEPER_CLIENT_PORT, zkPort)\n+    conf.set(HConstants.ZOOKEEPER_ZNODE_PARENT, zkNode)\n+    htable = new HTable(conf, table)\n+    // Use default writebuffersize to submit batch puts\n+    htable.setAutoFlush(false)\n+  }\n+\n+  /**\n+   * Convert field to bytes\n+   * @param field split by delimiter from record\n+   * @param kind the type of field\n+   * @return\n+   */\n+  def toByteArr(field: String, kind: String) = kind match {"
  }],
  "prId": 194
}, {
  "comments": [{
    "author": {
      "login": "mridulm"
    },
    "body": "Is the assumption here that fields(0) is the pkey and rest are column values ?\nis that a generic assumption we can rely on ? I did not see this constraint mentioned anyway (maybe I missed it ?)\n",
    "commit": "30f7343018d8d3bf8e8f1511a56511d35aaa025f",
    "createdAt": "2014-03-22T15:07:21Z",
    "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.nosql.hbase\n+\n+import org.apache.hadoop.hbase.client.{Put, HTable}\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.commons.codec.binary.Hex\n+import org.apache.hadoop.hbase.HConstants\n+import org.apache.hadoop.conf.Configuration\n+import java.io.IOException\n+\n+/**\n+ * Internal helper class that saves an RDD using a HBase OutputFormat. This is only public\n+ * because we need to access this class from the `spark` package to use some package-private HBase\n+ * functions, but this class should not be used directly by users.\n+ */\n+private[apache]\n+class SparkHBaseWriter(conf: HBaseConf) {\n+\n+  private var htable: HTable = null\n+\n+  val zkHost = conf.zkHost\n+  val zkPort = conf.zkPort\n+  val zkNode = conf.zkNode\n+  val table = conf.table\n+  val rowkeyType = conf.rowkeyType\n+  val columns = conf.columns\n+  val delimiter = conf.delimiter\n+\n+  def init() {\n+    val conf = new Configuration()\n+    conf.set(HConstants.ZOOKEEPER_QUORUM, zkHost)\n+    conf.set(HConstants.ZOOKEEPER_CLIENT_PORT, zkPort)\n+    conf.set(HConstants.ZOOKEEPER_ZNODE_PARENT, zkNode)\n+    htable = new HTable(conf, table)\n+    // Use default writebuffersize to submit batch puts\n+    htable.setAutoFlush(false)\n+  }\n+\n+  /**\n+   * Convert field to bytes\n+   * @param field split by delimiter from record\n+   * @param kind the type of field\n+   * @return\n+   */\n+  def toByteArr(field: String, kind: String) = kind match {\n+    case HBaseType.Boolean => Bytes.toBytes(field.toBoolean)\n+    case HBaseType.Short => Bytes.toBytes(field.toShort)\n+    case HBaseType.Int => Bytes.toBytes(field.toInt)\n+    case HBaseType.Long => Bytes.toBytes(field.toLong)\n+    case HBaseType.Float => Bytes.toBytes(field.toFloat)\n+    case HBaseType.Double => Bytes.toBytes(field.toDouble)\n+    case HBaseType.String => Bytes.toBytes(field)\n+    case HBaseType.Bytes => Hex.decodeHex(field.toCharArray)\n+    case _ => throw new IOException(\"Unsupported data type.\")\n+  }\n+\n+  /**\n+   * Convert a string record to [[org.apache.hadoop.hbase.client.Put]]\n+   * @param record\n+   * @return\n+   */\n+  def parseRecord(record: String) = {\n+    val fields = record.split(delimiter)\n+    val put = new Put(toByteArr(fields(0), rowkeyType))\n+\n+    List.range(1, fields.size) foreach {\n+      i => put.add(columns(i - 1).family, columns(i - 1).qualifier,\n+        toByteArr(fields(i), columns(i - 1).typ))\n+    }\n+\n+    put"
  }, {
    "author": {
      "login": "haosdent"
    },
    "body": "> Is the assumption here that fields(0) is the pkey and rest are column values ?\n\n@mridulm Yes. Should I write this assumption to scala document? Or use another better assumption?\n",
    "commit": "30f7343018d8d3bf8e8f1511a56511d35aaa025f",
    "createdAt": "2014-03-22T15:18:13Z",
    "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.nosql.hbase\n+\n+import org.apache.hadoop.hbase.client.{Put, HTable}\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.commons.codec.binary.Hex\n+import org.apache.hadoop.hbase.HConstants\n+import org.apache.hadoop.conf.Configuration\n+import java.io.IOException\n+\n+/**\n+ * Internal helper class that saves an RDD using a HBase OutputFormat. This is only public\n+ * because we need to access this class from the `spark` package to use some package-private HBase\n+ * functions, but this class should not be used directly by users.\n+ */\n+private[apache]\n+class SparkHBaseWriter(conf: HBaseConf) {\n+\n+  private var htable: HTable = null\n+\n+  val zkHost = conf.zkHost\n+  val zkPort = conf.zkPort\n+  val zkNode = conf.zkNode\n+  val table = conf.table\n+  val rowkeyType = conf.rowkeyType\n+  val columns = conf.columns\n+  val delimiter = conf.delimiter\n+\n+  def init() {\n+    val conf = new Configuration()\n+    conf.set(HConstants.ZOOKEEPER_QUORUM, zkHost)\n+    conf.set(HConstants.ZOOKEEPER_CLIENT_PORT, zkPort)\n+    conf.set(HConstants.ZOOKEEPER_ZNODE_PARENT, zkNode)\n+    htable = new HTable(conf, table)\n+    // Use default writebuffersize to submit batch puts\n+    htable.setAutoFlush(false)\n+  }\n+\n+  /**\n+   * Convert field to bytes\n+   * @param field split by delimiter from record\n+   * @param kind the type of field\n+   * @return\n+   */\n+  def toByteArr(field: String, kind: String) = kind match {\n+    case HBaseType.Boolean => Bytes.toBytes(field.toBoolean)\n+    case HBaseType.Short => Bytes.toBytes(field.toShort)\n+    case HBaseType.Int => Bytes.toBytes(field.toInt)\n+    case HBaseType.Long => Bytes.toBytes(field.toLong)\n+    case HBaseType.Float => Bytes.toBytes(field.toFloat)\n+    case HBaseType.Double => Bytes.toBytes(field.toDouble)\n+    case HBaseType.String => Bytes.toBytes(field)\n+    case HBaseType.Bytes => Hex.decodeHex(field.toCharArray)\n+    case _ => throw new IOException(\"Unsupported data type.\")\n+  }\n+\n+  /**\n+   * Convert a string record to [[org.apache.hadoop.hbase.client.Put]]\n+   * @param record\n+   * @return\n+   */\n+  def parseRecord(record: String) = {\n+    val fields = record.split(delimiter)\n+    val put = new Put(toByteArr(fields(0), rowkeyType))\n+\n+    List.range(1, fields.size) foreach {\n+      i => put.add(columns(i - 1).family, columns(i - 1).qualifier,\n+        toByteArr(fields(i), columns(i - 1).typ))\n+    }\n+\n+    put"
  }, {
    "author": {
      "login": "mridulm"
    },
    "body": "I dont know how hbase is normally used by users - so I cant comment unfortunately.\nIt was not clear if this was the assumption, but I inferred it based on the \"- 1\" in rest of the code, etc.\n",
    "commit": "30f7343018d8d3bf8e8f1511a56511d35aaa025f",
    "createdAt": "2014-03-22T15:24:58Z",
    "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.nosql.hbase\n+\n+import org.apache.hadoop.hbase.client.{Put, HTable}\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.commons.codec.binary.Hex\n+import org.apache.hadoop.hbase.HConstants\n+import org.apache.hadoop.conf.Configuration\n+import java.io.IOException\n+\n+/**\n+ * Internal helper class that saves an RDD using a HBase OutputFormat. This is only public\n+ * because we need to access this class from the `spark` package to use some package-private HBase\n+ * functions, but this class should not be used directly by users.\n+ */\n+private[apache]\n+class SparkHBaseWriter(conf: HBaseConf) {\n+\n+  private var htable: HTable = null\n+\n+  val zkHost = conf.zkHost\n+  val zkPort = conf.zkPort\n+  val zkNode = conf.zkNode\n+  val table = conf.table\n+  val rowkeyType = conf.rowkeyType\n+  val columns = conf.columns\n+  val delimiter = conf.delimiter\n+\n+  def init() {\n+    val conf = new Configuration()\n+    conf.set(HConstants.ZOOKEEPER_QUORUM, zkHost)\n+    conf.set(HConstants.ZOOKEEPER_CLIENT_PORT, zkPort)\n+    conf.set(HConstants.ZOOKEEPER_ZNODE_PARENT, zkNode)\n+    htable = new HTable(conf, table)\n+    // Use default writebuffersize to submit batch puts\n+    htable.setAutoFlush(false)\n+  }\n+\n+  /**\n+   * Convert field to bytes\n+   * @param field split by delimiter from record\n+   * @param kind the type of field\n+   * @return\n+   */\n+  def toByteArr(field: String, kind: String) = kind match {\n+    case HBaseType.Boolean => Bytes.toBytes(field.toBoolean)\n+    case HBaseType.Short => Bytes.toBytes(field.toShort)\n+    case HBaseType.Int => Bytes.toBytes(field.toInt)\n+    case HBaseType.Long => Bytes.toBytes(field.toLong)\n+    case HBaseType.Float => Bytes.toBytes(field.toFloat)\n+    case HBaseType.Double => Bytes.toBytes(field.toDouble)\n+    case HBaseType.String => Bytes.toBytes(field)\n+    case HBaseType.Bytes => Hex.decodeHex(field.toCharArray)\n+    case _ => throw new IOException(\"Unsupported data type.\")\n+  }\n+\n+  /**\n+   * Convert a string record to [[org.apache.hadoop.hbase.client.Put]]\n+   * @param record\n+   * @return\n+   */\n+  def parseRecord(record: String) = {\n+    val fields = record.split(delimiter)\n+    val put = new Put(toByteArr(fields(0), rowkeyType))\n+\n+    List.range(1, fields.size) foreach {\n+      i => put.add(columns(i - 1).family, columns(i - 1).qualifier,\n+        toByteArr(fields(i), columns(i - 1).typ))\n+    }\n+\n+    put"
  }, {
    "author": {
      "login": "haosdent"
    },
    "body": "@mridulm, @tedyu is the PMC of Apache HBase project, maybe he have some better advice about this. I would update this if he have a better idea.\n",
    "commit": "30f7343018d8d3bf8e8f1511a56511d35aaa025f",
    "createdAt": "2014-03-22T15:33:47Z",
    "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.nosql.hbase\n+\n+import org.apache.hadoop.hbase.client.{Put, HTable}\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.commons.codec.binary.Hex\n+import org.apache.hadoop.hbase.HConstants\n+import org.apache.hadoop.conf.Configuration\n+import java.io.IOException\n+\n+/**\n+ * Internal helper class that saves an RDD using a HBase OutputFormat. This is only public\n+ * because we need to access this class from the `spark` package to use some package-private HBase\n+ * functions, but this class should not be used directly by users.\n+ */\n+private[apache]\n+class SparkHBaseWriter(conf: HBaseConf) {\n+\n+  private var htable: HTable = null\n+\n+  val zkHost = conf.zkHost\n+  val zkPort = conf.zkPort\n+  val zkNode = conf.zkNode\n+  val table = conf.table\n+  val rowkeyType = conf.rowkeyType\n+  val columns = conf.columns\n+  val delimiter = conf.delimiter\n+\n+  def init() {\n+    val conf = new Configuration()\n+    conf.set(HConstants.ZOOKEEPER_QUORUM, zkHost)\n+    conf.set(HConstants.ZOOKEEPER_CLIENT_PORT, zkPort)\n+    conf.set(HConstants.ZOOKEEPER_ZNODE_PARENT, zkNode)\n+    htable = new HTable(conf, table)\n+    // Use default writebuffersize to submit batch puts\n+    htable.setAutoFlush(false)\n+  }\n+\n+  /**\n+   * Convert field to bytes\n+   * @param field split by delimiter from record\n+   * @param kind the type of field\n+   * @return\n+   */\n+  def toByteArr(field: String, kind: String) = kind match {\n+    case HBaseType.Boolean => Bytes.toBytes(field.toBoolean)\n+    case HBaseType.Short => Bytes.toBytes(field.toShort)\n+    case HBaseType.Int => Bytes.toBytes(field.toInt)\n+    case HBaseType.Long => Bytes.toBytes(field.toLong)\n+    case HBaseType.Float => Bytes.toBytes(field.toFloat)\n+    case HBaseType.Double => Bytes.toBytes(field.toDouble)\n+    case HBaseType.String => Bytes.toBytes(field)\n+    case HBaseType.Bytes => Hex.decodeHex(field.toCharArray)\n+    case _ => throw new IOException(\"Unsupported data type.\")\n+  }\n+\n+  /**\n+   * Convert a string record to [[org.apache.hadoop.hbase.client.Put]]\n+   * @param record\n+   * @return\n+   */\n+  def parseRecord(record: String) = {\n+    val fields = record.split(delimiter)\n+    val put = new Put(toByteArr(fields(0), rowkeyType))\n+\n+    List.range(1, fields.size) foreach {\n+      i => put.add(columns(i - 1).family, columns(i - 1).qualifier,\n+        toByteArr(fields(i), columns(i - 1).typ))\n+    }\n+\n+    put"
  }],
  "prId": 194
}, {
  "comments": [{
    "author": {
      "login": "tedyu"
    },
    "body": "If there is problem in init(), htable may be null.\nAdd a null check here.\n",
    "commit": "30f7343018d8d3bf8e8f1511a56511d35aaa025f",
    "createdAt": "2014-03-22T15:57:46Z",
    "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.nosql.hbase\n+\n+import org.apache.hadoop.hbase.client.{Put, HTable}\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.commons.codec.binary.Hex\n+import org.apache.hadoop.hbase.HConstants\n+import org.apache.hadoop.conf.Configuration\n+import java.io.IOException\n+\n+/**\n+ * Internal helper class that saves an RDD using a HBase OutputFormat. This is only public\n+ * because we need to access this class from the `spark` package to use some package-private HBase\n+ * functions, but this class should not be used directly by users.\n+ */\n+private[apache]\n+class SparkHBaseWriter(conf: HBaseConf) {\n+\n+  private var htable: HTable = null\n+\n+  val zkHost = conf.zkHost\n+  val zkPort = conf.zkPort\n+  val zkNode = conf.zkNode\n+  val table = conf.table\n+  val rowkeyType = conf.rowkeyType\n+  val columns = conf.columns\n+  val delimiter = conf.delimiter\n+\n+  def init() {\n+    val conf = new Configuration()\n+    conf.set(HConstants.ZOOKEEPER_QUORUM, zkHost)\n+    conf.set(HConstants.ZOOKEEPER_CLIENT_PORT, zkPort)\n+    conf.set(HConstants.ZOOKEEPER_ZNODE_PARENT, zkNode)\n+    htable = new HTable(conf, table)\n+    // Use default writebuffersize to submit batch puts\n+    htable.setAutoFlush(false)\n+  }\n+\n+  /**\n+   * Convert field to bytes\n+   * @param field split by delimiter from record\n+   * @param kind the type of field\n+   * @return\n+   */\n+  def toByteArr(field: String, kind: String) = kind match {\n+    case HBaseType.Boolean => Bytes.toBytes(field.toBoolean)\n+    case HBaseType.Short => Bytes.toBytes(field.toShort)\n+    case HBaseType.Int => Bytes.toBytes(field.toInt)\n+    case HBaseType.Long => Bytes.toBytes(field.toLong)\n+    case HBaseType.Float => Bytes.toBytes(field.toFloat)\n+    case HBaseType.Double => Bytes.toBytes(field.toDouble)\n+    case HBaseType.String => Bytes.toBytes(field)\n+    case HBaseType.Bytes => Hex.decodeHex(field.toCharArray)\n+    case _ => throw new IOException(\"Unsupported data type.\")\n+  }\n+\n+  /**\n+   * Convert a string record to [[org.apache.hadoop.hbase.client.Put]]\n+   * @param record\n+   * @return\n+   */\n+  def parseRecord(record: String) = {\n+    val fields = record.split(delimiter)\n+    val put = new Put(toByteArr(fields(0), rowkeyType))\n+\n+    List.range(1, fields.size) foreach {\n+      i => put.add(columns(i - 1).family, columns(i - 1).qualifier,\n+        toByteArr(fields(i), columns(i - 1).typ))\n+    }\n+\n+    put\n+  }\n+\n+  def write(record: Text) {\n+    val put = parseRecord(record.toString)\n+    htable.put(put)\n+  }\n+\n+  def close() {\n+    htable.close()"
  }],
  "prId": 194
}, {
  "comments": [{
    "author": {
      "login": "tedyu"
    },
    "body": "See 9.3.1.1. under http://hbase.apache.org/book.html#client.connections\n\nCan be done in follow-on JIRA\n",
    "commit": "30f7343018d8d3bf8e8f1511a56511d35aaa025f",
    "createdAt": "2014-03-22T17:24:54Z",
    "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.nosql.hbase\n+\n+import org.apache.hadoop.hbase.client.{Put, HTable}\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.commons.codec.binary.Hex\n+import org.apache.hadoop.hbase.HConstants\n+import org.apache.hadoop.conf.Configuration\n+import java.io.IOException\n+import org.apache.spark.Logging\n+\n+/**\n+ * Internal helper class that saves an RDD using a HBase OutputFormat. This is only public\n+ * because we need to access this class from the `hbase` package to use some package-private HBase\n+ * functions, but this class should not be used directly by users.\n+ */\n+private[hbase]\n+class SparkHBaseWriter(conf: HBaseConf)\n+  extends Logging {\n+\n+  private var htable: HTable = null\n+\n+  val zkHost = conf.zkHost\n+  val zkPort = conf.zkPort\n+  val zkNode = conf.zkNode\n+  val table = conf.table\n+  val rowkeyType = conf.rowkeyType\n+  val columns = conf.columns\n+  val delimiter = conf.delimiter\n+\n+  def init() {\n+    val conf = new Configuration()\n+    conf.set(HConstants.ZOOKEEPER_QUORUM, zkHost)\n+    conf.set(HConstants.ZOOKEEPER_CLIENT_PORT, zkPort)\n+    conf.set(HConstants.ZOOKEEPER_ZNODE_PARENT, zkNode)\n+    htable = new HTable(conf, table)",
    "line": 56
  }, {
    "author": {
      "login": "haosdent"
    },
    "body": "The dependence of HBase in Spark is 0.94.6, so I use new HTable() here.\nShould I update it to 0.94.11? @tedyu \n",
    "commit": "30f7343018d8d3bf8e8f1511a56511d35aaa025f",
    "createdAt": "2014-03-22T17:32:04Z",
    "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.nosql.hbase\n+\n+import org.apache.hadoop.hbase.client.{Put, HTable}\n+import org.apache.hadoop.io.Text\n+import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.commons.codec.binary.Hex\n+import org.apache.hadoop.hbase.HConstants\n+import org.apache.hadoop.conf.Configuration\n+import java.io.IOException\n+import org.apache.spark.Logging\n+\n+/**\n+ * Internal helper class that saves an RDD using a HBase OutputFormat. This is only public\n+ * because we need to access this class from the `hbase` package to use some package-private HBase\n+ * functions, but this class should not be used directly by users.\n+ */\n+private[hbase]\n+class SparkHBaseWriter(conf: HBaseConf)\n+  extends Logging {\n+\n+  private var htable: HTable = null\n+\n+  val zkHost = conf.zkHost\n+  val zkPort = conf.zkPort\n+  val zkNode = conf.zkNode\n+  val table = conf.table\n+  val rowkeyType = conf.rowkeyType\n+  val columns = conf.columns\n+  val delimiter = conf.delimiter\n+\n+  def init() {\n+    val conf = new Configuration()\n+    conf.set(HConstants.ZOOKEEPER_QUORUM, zkHost)\n+    conf.set(HConstants.ZOOKEEPER_CLIENT_PORT, zkPort)\n+    conf.set(HConstants.ZOOKEEPER_ZNODE_PARENT, zkNode)\n+    htable = new HTable(conf, table)",
    "line": 56
  }],
  "prId": 194
}]