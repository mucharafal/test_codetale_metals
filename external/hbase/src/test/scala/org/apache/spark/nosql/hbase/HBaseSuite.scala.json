[{
  "comments": [{
    "author": {
      "login": "tedyu"
    },
    "body": "Where are htable and rs closed ?\n",
    "commit": "30f7343018d8d3bf8e8f1511a56511d35aaa025f",
    "createdAt": "2014-03-22T17:24:02Z",
    "diffHunk": "@@ -0,0 +1,48 @@\n+package org.apache.spark.nosql.hbase\n+\n+import org.scalatest.FunSuite\n+import org.apache.hadoop.io.Text\n+import org.apache.spark.{SparkContext, LocalSparkContext}\n+import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.hadoop.hbase.{HConstants, HBaseTestingUtility}\n+import org.apache.hadoop.hbase.client.{Scan, HTable}\n+\n+class HBaseSuite extends FunSuite with LocalSparkContext {\n+\n+  test(\"write SequenceFile using HBase\") {\n+    sc = new SparkContext(\"local\", \"test\")\n+    val nums = sc.makeRDD(1 to 3).map(x => new Text(\"a\" + x + \" 1.0\"))\n+\n+    val table = \"test\"\n+    val rowkeyType = HBaseType.String\n+    val cfBytes = Bytes.toBytes(\"cf\")\n+    val qualBytes = Bytes.toBytes(\"qual0\")\n+    val columns = List[HBaseColumn](new HBaseColumn(cfBytes, qualBytes, HBaseType.Float))\n+    val delimiter = ' '\n+\n+    val util = new HBaseTestingUtility()\n+    util.startMiniCluster()\n+    util.createTable(Bytes.toBytes(table), cfBytes)\n+    val conf = util.getConfiguration\n+    val zkHost = conf.get(HConstants.ZOOKEEPER_QUORUM)\n+    val zkPort = conf.get(HConstants.ZOOKEEPER_CLIENT_PORT)\n+    val zkNode = conf.get(HConstants.ZOOKEEPER_ZNODE_PARENT)\n+\n+    HBaseUtils.saveAsHBaseTable(nums, zkHost, zkPort, zkNode, table, rowkeyType, columns, delimiter)\n+\n+    val htable = new HTable(conf, table)\n+    val scan = new Scan()\n+    val rs = htable.getScanner(scan)"
  }, {
    "author": {
      "login": "haosdent"
    },
    "body": "Sorry, I forgot it. Have fixed it.\n",
    "commit": "30f7343018d8d3bf8e8f1511a56511d35aaa025f",
    "createdAt": "2014-03-22T17:30:17Z",
    "diffHunk": "@@ -0,0 +1,48 @@\n+package org.apache.spark.nosql.hbase\n+\n+import org.scalatest.FunSuite\n+import org.apache.hadoop.io.Text\n+import org.apache.spark.{SparkContext, LocalSparkContext}\n+import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.hadoop.hbase.{HConstants, HBaseTestingUtility}\n+import org.apache.hadoop.hbase.client.{Scan, HTable}\n+\n+class HBaseSuite extends FunSuite with LocalSparkContext {\n+\n+  test(\"write SequenceFile using HBase\") {\n+    sc = new SparkContext(\"local\", \"test\")\n+    val nums = sc.makeRDD(1 to 3).map(x => new Text(\"a\" + x + \" 1.0\"))\n+\n+    val table = \"test\"\n+    val rowkeyType = HBaseType.String\n+    val cfBytes = Bytes.toBytes(\"cf\")\n+    val qualBytes = Bytes.toBytes(\"qual0\")\n+    val columns = List[HBaseColumn](new HBaseColumn(cfBytes, qualBytes, HBaseType.Float))\n+    val delimiter = ' '\n+\n+    val util = new HBaseTestingUtility()\n+    util.startMiniCluster()\n+    util.createTable(Bytes.toBytes(table), cfBytes)\n+    val conf = util.getConfiguration\n+    val zkHost = conf.get(HConstants.ZOOKEEPER_QUORUM)\n+    val zkPort = conf.get(HConstants.ZOOKEEPER_CLIENT_PORT)\n+    val zkNode = conf.get(HConstants.ZOOKEEPER_ZNODE_PARENT)\n+\n+    HBaseUtils.saveAsHBaseTable(nums, zkHost, zkPort, zkNode, table, rowkeyType, columns, delimiter)\n+\n+    val htable = new HTable(conf, table)\n+    val scan = new Scan()\n+    val rs = htable.getScanner(scan)"
  }],
  "prId": 194
}]