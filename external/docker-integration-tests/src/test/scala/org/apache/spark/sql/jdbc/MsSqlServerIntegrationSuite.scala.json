[{
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Do you need this method if it's not reused?",
    "commit": "ec5315d9ec600547cadc30861636159f25e33afc",
    "createdAt": "2019-08-03T14:55:36Z",
    "diffHunk": "@@ -202,4 +205,32 @@ class MsSqlServerIntegrationSuite extends DockerJDBCIntegrationSuite {\n     df2.write.jdbc(jdbcUrl, \"datescopy\", new Properties)\n     df3.write.jdbc(jdbcUrl, \"stringscopy\", new Properties)\n   }\n+\n+  def create_df() : DataFrame = {"
  }, {
    "author": {
      "login": "shivsood"
    },
    "body": "removed. 'll get this out as a method if i have additional test that share this. As of now, no. So removing this.",
    "commit": "ec5315d9ec600547cadc30861636159f25e33afc",
    "createdAt": "2019-08-05T21:03:37Z",
    "diffHunk": "@@ -202,4 +205,32 @@ class MsSqlServerIntegrationSuite extends DockerJDBCIntegrationSuite {\n     df2.write.jdbc(jdbcUrl, \"datescopy\", new Properties)\n     df3.write.jdbc(jdbcUrl, \"stringscopy\", new Properties)\n   }\n+\n+  def create_df() : DataFrame = {"
  }],
  "prId": 25344
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "You need spaces after commas",
    "commit": "ec5315d9ec600547cadc30861636159f25e33afc",
    "createdAt": "2019-08-03T14:55:54Z",
    "diffHunk": "@@ -202,4 +205,32 @@ class MsSqlServerIntegrationSuite extends DockerJDBCIntegrationSuite {\n     df2.write.jdbc(jdbcUrl, \"datescopy\", new Properties)\n     df3.write.jdbc(jdbcUrl, \"stringscopy\", new Properties)\n   }\n+\n+  def create_df() : DataFrame = {\n+    val tableSchema = StructType(Seq(\n+      StructField(\"serialNum\",ByteType,true)\n+    ))\n+\n+    val tableData = Seq (\n+      Row(10)\n+    )\n+\n+    spark.createDataFrame(spark.sparkContext.parallelize(tableData),tableSchema)\n+  }\n+\n+  test(\"SPARK-28151 Test write table with BYTETYPE\") {\n+    val df1 = create_df()\n+    df1.write\n+      .format(\"jdbc\")\n+      .mode(\"overwrite\")\n+      .option(\"url\",jdbcUrl)\n+      .option(\"dbtable\",\"testTable\")"
  }, {
    "author": {
      "login": "shivsood"
    },
    "body": "fixed. Sorry about this. I realize dev/stylecheck should be run manually for test cases. I presumed its get run automatically when i build. Apparently not for test code.",
    "commit": "ec5315d9ec600547cadc30861636159f25e33afc",
    "createdAt": "2019-08-05T21:03:05Z",
    "diffHunk": "@@ -202,4 +205,32 @@ class MsSqlServerIntegrationSuite extends DockerJDBCIntegrationSuite {\n     df2.write.jdbc(jdbcUrl, \"datescopy\", new Properties)\n     df3.write.jdbc(jdbcUrl, \"stringscopy\", new Properties)\n   }\n+\n+  def create_df() : DataFrame = {\n+    val tableSchema = StructType(Seq(\n+      StructField(\"serialNum\",ByteType,true)\n+    ))\n+\n+    val tableData = Seq (\n+      Row(10)\n+    )\n+\n+    spark.createDataFrame(spark.sparkContext.parallelize(tableData),tableSchema)\n+  }\n+\n+  test(\"SPARK-28151 Test write table with BYTETYPE\") {\n+    val df1 = create_df()\n+    df1.write\n+      .format(\"jdbc\")\n+      .mode(\"overwrite\")\n+      .option(\"url\",jdbcUrl)\n+      .option(\"dbtable\",\"testTable\")"
  }],
  "prId": 25344
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Extra spaces here, and there's no need to wrap this",
    "commit": "ec5315d9ec600547cadc30861636159f25e33afc",
    "createdAt": "2019-08-03T14:56:06Z",
    "diffHunk": "@@ -202,4 +205,32 @@ class MsSqlServerIntegrationSuite extends DockerJDBCIntegrationSuite {\n     df2.write.jdbc(jdbcUrl, \"datescopy\", new Properties)\n     df3.write.jdbc(jdbcUrl, \"stringscopy\", new Properties)\n   }\n+\n+  def create_df() : DataFrame = {\n+    val tableSchema = StructType(Seq(\n+      StructField(\"serialNum\",ByteType,true)\n+    ))\n+\n+    val tableData = Seq ("
  }, {
    "author": {
      "login": "shivsood"
    },
    "body": "\r\nfixed. Sorry about this. I realize dev/stylecheck should be run manually for test cases. I presumed its get run automatically when i build. Apparently not for test code.",
    "commit": "ec5315d9ec600547cadc30861636159f25e33afc",
    "createdAt": "2019-08-05T21:02:45Z",
    "diffHunk": "@@ -202,4 +205,32 @@ class MsSqlServerIntegrationSuite extends DockerJDBCIntegrationSuite {\n     df2.write.jdbc(jdbcUrl, \"datescopy\", new Properties)\n     df3.write.jdbc(jdbcUrl, \"stringscopy\", new Properties)\n   }\n+\n+  def create_df() : DataFrame = {\n+    val tableSchema = StructType(Seq(\n+      StructField(\"serialNum\",ByteType,true)\n+    ))\n+\n+    val tableData = Seq ("
  }],
  "prId": 25344
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Do you want to test anything about the result here?",
    "commit": "ec5315d9ec600547cadc30861636159f25e33afc",
    "createdAt": "2019-08-06T20:10:17Z",
    "diffHunk": "@@ -202,4 +204,25 @@ class MsSqlServerIntegrationSuite extends DockerJDBCIntegrationSuite {\n     df2.write.jdbc(jdbcUrl, \"datescopy\", new Properties)\n     df3.write.jdbc(jdbcUrl, \"stringscopy\", new Properties)\n   }\n+\n+  test(\"SPARK-28151 Test write table with BYTETYPE\") {\n+    val tableSchema = StructType(Seq(StructField(\"serialNum\", ByteType, true)))\n+    val tableData = Seq(Row(10))\n+    val df1 = spark.createDataFrame(\n+      spark.sparkContext.parallelize(tableData),\n+      tableSchema)\n+\n+    df1.write\n+      .format(\"jdbc\")\n+      .mode(\"overwrite\")\n+      .option(\"url\", jdbcUrl)\n+      .option(\"dbtable\", \"testTable\")\n+      .save()\n+    val df2 = spark.read\n+      .format(\"jdbc\")\n+      .option(\"url\", jdbcUrl)\n+      .option(\"dbtable\", \"byteTable\")\n+      .load()\n+    df2.show()"
  }, {
    "author": {
      "login": "shivsood"
    },
    "body": "Yes, would add a check of row counts.",
    "commit": "ec5315d9ec600547cadc30861636159f25e33afc",
    "createdAt": "2019-08-06T22:23:25Z",
    "diffHunk": "@@ -202,4 +204,25 @@ class MsSqlServerIntegrationSuite extends DockerJDBCIntegrationSuite {\n     df2.write.jdbc(jdbcUrl, \"datescopy\", new Properties)\n     df3.write.jdbc(jdbcUrl, \"stringscopy\", new Properties)\n   }\n+\n+  test(\"SPARK-28151 Test write table with BYTETYPE\") {\n+    val tableSchema = StructType(Seq(StructField(\"serialNum\", ByteType, true)))\n+    val tableData = Seq(Row(10))\n+    val df1 = spark.createDataFrame(\n+      spark.sparkContext.parallelize(tableData),\n+      tableSchema)\n+\n+    df1.write\n+      .format(\"jdbc\")\n+      .mode(\"overwrite\")\n+      .option(\"url\", jdbcUrl)\n+      .option(\"dbtable\", \"testTable\")\n+      .save()\n+    val df2 = spark.read\n+      .format(\"jdbc\")\n+      .option(\"url\", jdbcUrl)\n+      .option(\"dbtable\", \"byteTable\")\n+      .load()\n+    df2.show()"
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "I'd go ahead and make the test more meaningful; you should test the contents. Also I'd test values besides 10, like negative values and larger than 127.",
    "commit": "ec5315d9ec600547cadc30861636159f25e33afc",
    "createdAt": "2019-08-12T13:33:10Z",
    "diffHunk": "@@ -202,4 +204,25 @@ class MsSqlServerIntegrationSuite extends DockerJDBCIntegrationSuite {\n     df2.write.jdbc(jdbcUrl, \"datescopy\", new Properties)\n     df3.write.jdbc(jdbcUrl, \"stringscopy\", new Properties)\n   }\n+\n+  test(\"SPARK-28151 Test write table with BYTETYPE\") {\n+    val tableSchema = StructType(Seq(StructField(\"serialNum\", ByteType, true)))\n+    val tableData = Seq(Row(10))\n+    val df1 = spark.createDataFrame(\n+      spark.sparkContext.parallelize(tableData),\n+      tableSchema)\n+\n+    df1.write\n+      .format(\"jdbc\")\n+      .mode(\"overwrite\")\n+      .option(\"url\", jdbcUrl)\n+      .option(\"dbtable\", \"testTable\")\n+      .save()\n+    val df2 = spark.read\n+      .format(\"jdbc\")\n+      .option(\"url\", jdbcUrl)\n+      .option(\"dbtable\", \"byteTable\")\n+      .load()\n+    df2.show()"
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "@shivsood if you update the test a bit I think we can add this.",
    "commit": "ec5315d9ec600547cadc30861636159f25e33afc",
    "createdAt": "2019-08-15T14:35:22Z",
    "diffHunk": "@@ -202,4 +204,25 @@ class MsSqlServerIntegrationSuite extends DockerJDBCIntegrationSuite {\n     df2.write.jdbc(jdbcUrl, \"datescopy\", new Properties)\n     df3.write.jdbc(jdbcUrl, \"stringscopy\", new Properties)\n   }\n+\n+  test(\"SPARK-28151 Test write table with BYTETYPE\") {\n+    val tableSchema = StructType(Seq(StructField(\"serialNum\", ByteType, true)))\n+    val tableData = Seq(Row(10))\n+    val df1 = spark.createDataFrame(\n+      spark.sparkContext.parallelize(tableData),\n+      tableSchema)\n+\n+    df1.write\n+      .format(\"jdbc\")\n+      .mode(\"overwrite\")\n+      .option(\"url\", jdbcUrl)\n+      .option(\"dbtable\", \"testTable\")\n+      .save()\n+    val df2 = spark.read\n+      .format(\"jdbc\")\n+      .option(\"url\", jdbcUrl)\n+      .option(\"dbtable\", \"byteTable\")\n+      .load()\n+    df2.show()"
  }, {
    "author": {
      "login": "shivsood"
    },
    "body": "@srowen Will update this test soon.  IMO the blocking issue is the exception that i am getting in Catalyst engine ( refer below and how tested section in PR). The fix fails on 3.0 master and branch2-4.  Note : I tested on 2.4.1 and the fix passes. To me looks like some check added between 2.4.1 and now. \r\n\r\n19/08/02 18:25:44 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1197 bytes result sent to driver\r\n19/08/02 18:25:44 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 43 ms on localhost (executor driver) (1/2)\r\n19/08/02 18:25:44 INFO CodeGenerator: Code generated in 14.586963 ms\r\n19/08/02 18:25:44 ERROR Executor: Exception in task 1.0 in stage 7.0 (TID 8)\r\njava.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of tinyint\r\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, serialNum), ByteType) AS serialNum#231",
    "commit": "ec5315d9ec600547cadc30861636159f25e33afc",
    "createdAt": "2019-08-15T18:41:55Z",
    "diffHunk": "@@ -202,4 +204,25 @@ class MsSqlServerIntegrationSuite extends DockerJDBCIntegrationSuite {\n     df2.write.jdbc(jdbcUrl, \"datescopy\", new Properties)\n     df3.write.jdbc(jdbcUrl, \"stringscopy\", new Properties)\n   }\n+\n+  test(\"SPARK-28151 Test write table with BYTETYPE\") {\n+    val tableSchema = StructType(Seq(StructField(\"serialNum\", ByteType, true)))\n+    val tableData = Seq(Row(10))\n+    val df1 = spark.createDataFrame(\n+      spark.sparkContext.parallelize(tableData),\n+      tableSchema)\n+\n+    df1.write\n+      .format(\"jdbc\")\n+      .mode(\"overwrite\")\n+      .option(\"url\", jdbcUrl)\n+      .option(\"dbtable\", \"testTable\")\n+      .save()\n+    val df2 = spark.read\n+      .format(\"jdbc\")\n+      .option(\"url\", jdbcUrl)\n+      .option(\"dbtable\", \"byteTable\")\n+      .load()\n+    df2.show()"
  }, {
    "author": {
      "login": "shivsood"
    },
    "body": "@srowen @dongjoon-hyun back on this after a while. I have resolved the issue now. The test pass successfully now. Also i have added additional test to 1. test a broader range of ByteType values and also check the col type post read to assert the returned type is ByteType. Test case now pass fully.",
    "commit": "ec5315d9ec600547cadc30861636159f25e33afc",
    "createdAt": "2019-10-15T00:28:04Z",
    "diffHunk": "@@ -202,4 +204,25 @@ class MsSqlServerIntegrationSuite extends DockerJDBCIntegrationSuite {\n     df2.write.jdbc(jdbcUrl, \"datescopy\", new Properties)\n     df3.write.jdbc(jdbcUrl, \"stringscopy\", new Properties)\n   }\n+\n+  test(\"SPARK-28151 Test write table with BYTETYPE\") {\n+    val tableSchema = StructType(Seq(StructField(\"serialNum\", ByteType, true)))\n+    val tableData = Seq(Row(10))\n+    val df1 = spark.createDataFrame(\n+      spark.sparkContext.parallelize(tableData),\n+      tableSchema)\n+\n+    df1.write\n+      .format(\"jdbc\")\n+      .mode(\"overwrite\")\n+      .option(\"url\", jdbcUrl)\n+      .option(\"dbtable\", \"testTable\")\n+      .save()\n+    val df2 = spark.read\n+      .format(\"jdbc\")\n+      .option(\"url\", jdbcUrl)\n+      .option(\"dbtable\", \"byteTable\")\n+      .load()\n+    df2.show()"
  }],
  "prId": 25344
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Let's remove this. I'll give the updated short test code.",
    "commit": "ec5315d9ec600547cadc30861636159f25e33afc",
    "createdAt": "2019-10-15T04:37:05Z",
    "diffHunk": "@@ -21,6 +21,8 @@ import java.math.BigDecimal\n import java.sql.{Connection, Date, Timestamp}\n import java.util.Properties\n \n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.types._"
  }],
  "prId": 25344
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "```scala\r\n-  test(\"SPARK-28151 Test write table with BYTETYPE\") {\r\n-    val df : DataFrame = {\r\n-      val schema = StructType(Seq(\r\n-        StructField(\"a\", ByteType, true)\r\n-      ))\r\n-      val data = Seq(\r\n-        Row(-127.toByte),\r\n-        Row(0.toByte),\r\n-        Row(1.toByte),\r\n-        Row(38.toByte),\r\n-        Row(128.toByte)\r\n-      )\r\n-      spark.createDataFrame(spark.sparkContext.parallelize(data), schema)\r\n-    }\r\n+  test(\"Write table with BYTETYPE\") {\r\n+    import testImplicits._\r\n+\r\n+    val df = Seq(-127.toByte, 0.toByte, 1.toByte, 38.toByte, 128.toByte).toDF(\"a\")\r\n+\r\n```",
    "commit": "ec5315d9ec600547cadc30861636159f25e33afc",
    "createdAt": "2019-10-15T04:38:05Z",
    "diffHunk": "@@ -202,4 +204,36 @@ class MsSqlServerIntegrationSuite extends DockerJDBCIntegrationSuite {\n     df2.write.jdbc(jdbcUrl, \"datescopy\", new Properties)\n     df3.write.jdbc(jdbcUrl, \"stringscopy\", new Properties)\n   }\n+\n+  test(\"SPARK-28151 Test write table with BYTETYPE\") {\n+    val df : DataFrame = {\n+      val schema = StructType(Seq(\n+        StructField(\"a\", ByteType, true)\n+      ))\n+      val data = Seq(\n+        Row(-127.toByte),\n+        Row(0.toByte),\n+        Row(1.toByte),\n+        Row(38.toByte),\n+        Row(128.toByte)\n+      )\n+      spark.createDataFrame(spark.sparkContext.parallelize(data), schema)\n+    }"
  }],
  "prId": 25344
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "```scala\r\n-    val df_copy = spark.read\r\n+    val df2 = spark.read\r\n```",
    "commit": "ec5315d9ec600547cadc30861636159f25e33afc",
    "createdAt": "2019-10-15T04:38:29Z",
    "diffHunk": "@@ -202,4 +204,36 @@ class MsSqlServerIntegrationSuite extends DockerJDBCIntegrationSuite {\n     df2.write.jdbc(jdbcUrl, \"datescopy\", new Properties)\n     df3.write.jdbc(jdbcUrl, \"stringscopy\", new Properties)\n   }\n+\n+  test(\"SPARK-28151 Test write table with BYTETYPE\") {\n+    val df : DataFrame = {\n+      val schema = StructType(Seq(\n+        StructField(\"a\", ByteType, true)\n+      ))\n+      val data = Seq(\n+        Row(-127.toByte),\n+        Row(0.toByte),\n+        Row(1.toByte),\n+        Row(38.toByte),\n+        Row(128.toByte)\n+      )\n+      spark.createDataFrame(spark.sparkContext.parallelize(data), schema)\n+    }\n+    val tablename = \"bytetable\"\n+    df.write\n+      .format(\"jdbc\")\n+      .mode(\"overwrite\")\n+      .option(\"url\", jdbcUrl)\n+      .option(\"dbtable\", tablename)\n+      .save()\n+    val df_copy = spark.read"
  }],
  "prId": 25344
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "```scala\r\n-    assert(df.count == df_copy.count)\r\n-    val rows = df_copy.collect()\r\n+    assert(df.count == df2.count)\r\n+    val rows = df2.collect()\r\n```",
    "commit": "ec5315d9ec600547cadc30861636159f25e33afc",
    "createdAt": "2019-10-15T04:38:56Z",
    "diffHunk": "@@ -202,4 +204,36 @@ class MsSqlServerIntegrationSuite extends DockerJDBCIntegrationSuite {\n     df2.write.jdbc(jdbcUrl, \"datescopy\", new Properties)\n     df3.write.jdbc(jdbcUrl, \"stringscopy\", new Properties)\n   }\n+\n+  test(\"SPARK-28151 Test write table with BYTETYPE\") {\n+    val df : DataFrame = {\n+      val schema = StructType(Seq(\n+        StructField(\"a\", ByteType, true)\n+      ))\n+      val data = Seq(\n+        Row(-127.toByte),\n+        Row(0.toByte),\n+        Row(1.toByte),\n+        Row(38.toByte),\n+        Row(128.toByte)\n+      )\n+      spark.createDataFrame(spark.sparkContext.parallelize(data), schema)\n+    }\n+    val tablename = \"bytetable\"\n+    df.write\n+      .format(\"jdbc\")\n+      .mode(\"overwrite\")\n+      .option(\"url\", jdbcUrl)\n+      .option(\"dbtable\", tablename)\n+      .save()\n+    val df_copy = spark.read\n+      .format(\"jdbc\")\n+      .option(\"url\", jdbcUrl)\n+      .option(\"dbtable\", tablename)\n+      .load()\n+    assert(df.count == df_copy.count)\n+    val rows = df_copy.collect()"
  }],
  "prId": 25344
}]