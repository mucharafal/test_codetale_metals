[{
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "nit `Required data schema in the batch scan.`?",
    "commit": "fc98bd53130f67a4ea140d70a393c0ba4ba9c83e",
    "createdAt": "2019-07-05T02:34:06Z",
    "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.v2.avro\n+\n+import java.net.URI\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.file.DataFileReader\n+import org.apache.avro.generic.{GenericDatumReader, GenericRecord}\n+import org.apache.avro.mapred.FsInput\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.avro.{AvroDeserializer, AvroOptions}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.v2._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.v2.reader.PartitionReader\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+/**\n+ * A factory used to create AVRO readers.\n+ *\n+ * @param sqlConf SQL configuration.\n+ * @param broadcastedConf Broadcast serializable Hadoop Configuration.\n+ * @param dataSchema Schema of AVRO files.\n+ * @param readDataSchema Required schema of AVRO files."
  }],
  "prId": 25017
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Shall we have the same comment above this line in order not to forget that?\r\n```scala\r\n      // TODO Removes this check once `FileFormat` gets a general file filtering interface method.\r\n      // Doing input file filtering is improper because we may generate empty tasks that process no\r\n      // input files but stress the scheduler. We should probably add a more general input file\r\n      // filtering mechanism for `FileFormat` data sources. See SPARK-16317.\r\n```",
    "commit": "fc98bd53130f67a4ea140d70a393c0ba4ba9c83e",
    "createdAt": "2019-07-05T02:38:35Z",
    "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.v2.avro\n+\n+import java.net.URI\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.file.DataFileReader\n+import org.apache.avro.generic.{GenericDatumReader, GenericRecord}\n+import org.apache.avro.mapred.FsInput\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.avro.{AvroDeserializer, AvroOptions}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.v2._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.v2.reader.PartitionReader\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+/**\n+ * A factory used to create AVRO readers.\n+ *\n+ * @param sqlConf SQL configuration.\n+ * @param broadcastedConf Broadcast serializable Hadoop Configuration.\n+ * @param dataSchema Schema of AVRO files.\n+ * @param readDataSchema Required schema of AVRO files.\n+ * @param partitionSchema Schema of partitions.\n+ * @param options Options for parsing AVRO files.\n+ */\n+case class AvroPartitionReaderFactory(\n+    sqlConf: SQLConf,\n+    broadcastedConf: Broadcast[SerializableConfiguration],\n+    dataSchema: StructType,\n+    readDataSchema: StructType,\n+    partitionSchema: StructType,\n+    options: Map[String, String]) extends FilePartitionReaderFactory with Logging {\n+\n+  override def buildReader(partitionedFile: PartitionedFile): PartitionReader[InternalRow] = {\n+    val conf = broadcastedConf.value.value\n+    val parsedOptions = new AvroOptions(options, conf)\n+    val userProvidedSchema = parsedOptions.schema.map(new Schema.Parser().parse)\n+\n+    if (parsedOptions.ignoreExtension || partitionedFile.filePath.endsWith(\".avro\")) {",
    "line": 64
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "Actually, there is an option `pathGlobFilter` for it. I have marked it as deprecated in #24518. \r\nI think we can still support it in 3.0. So I am not sure what to comment here.",
    "commit": "fc98bd53130f67a4ea140d70a393c0ba4ba9c83e",
    "createdAt": "2019-07-05T06:31:23Z",
    "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.v2.avro\n+\n+import java.net.URI\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.file.DataFileReader\n+import org.apache.avro.generic.{GenericDatumReader, GenericRecord}\n+import org.apache.avro.mapred.FsInput\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.avro.{AvroDeserializer, AvroOptions}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.v2._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.v2.reader.PartitionReader\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+/**\n+ * A factory used to create AVRO readers.\n+ *\n+ * @param sqlConf SQL configuration.\n+ * @param broadcastedConf Broadcast serializable Hadoop Configuration.\n+ * @param dataSchema Schema of AVRO files.\n+ * @param readDataSchema Required schema of AVRO files.\n+ * @param partitionSchema Schema of partitions.\n+ * @param options Options for parsing AVRO files.\n+ */\n+case class AvroPartitionReaderFactory(\n+    sqlConf: SQLConf,\n+    broadcastedConf: Broadcast[SerializableConfiguration],\n+    dataSchema: StructType,\n+    readDataSchema: StructType,\n+    partitionSchema: StructType,\n+    options: Map[String, String]) extends FilePartitionReaderFactory with Logging {\n+\n+  override def buildReader(partitionedFile: PartitionedFile): PartitionReader[InternalRow] = {\n+    val conf = broadcastedConf.value.value\n+    val parsedOptions = new AvroOptions(options, conf)\n+    val userProvidedSchema = parsedOptions.schema.map(new Schema.Parser().parse)\n+\n+    if (parsedOptions.ignoreExtension || partitionedFile.filePath.endsWith(\".avro\")) {",
    "line": 64
  }],
  "prId": 25017
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "nit. Redundant empty line.",
    "commit": "fc98bd53130f67a4ea140d70a393c0ba4ba9c83e",
    "createdAt": "2019-07-05T02:41:07Z",
    "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.v2.avro\n+\n+import java.net.URI\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.file.DataFileReader\n+import org.apache.avro.generic.{GenericDatumReader, GenericRecord}\n+import org.apache.avro.mapred.FsInput\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.avro.{AvroDeserializer, AvroOptions}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.v2._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.sources.v2.reader.PartitionReader\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+/**\n+ * A factory used to create AVRO readers.\n+ *\n+ * @param sqlConf SQL configuration.\n+ * @param broadcastedConf Broadcast serializable Hadoop Configuration.\n+ * @param dataSchema Schema of AVRO files.\n+ * @param readDataSchema Required schema of AVRO files.\n+ * @param partitionSchema Schema of partitions.\n+ * @param options Options for parsing AVRO files.\n+ */\n+case class AvroPartitionReaderFactory(\n+    sqlConf: SQLConf,\n+    broadcastedConf: Broadcast[SerializableConfiguration],\n+    dataSchema: StructType,\n+    readDataSchema: StructType,\n+    partitionSchema: StructType,\n+    options: Map[String, String]) extends FilePartitionReaderFactory with Logging {\n+\n+  override def buildReader(partitionedFile: PartitionedFile): PartitionReader[InternalRow] = {\n+    val conf = broadcastedConf.value.value\n+    val parsedOptions = new AvroOptions(options, conf)\n+    val userProvidedSchema = parsedOptions.schema.map(new Schema.Parser().parse)\n+\n+    if (parsedOptions.ignoreExtension || partitionedFile.filePath.endsWith(\".avro\")) {\n+      val reader = {\n+        val in = new FsInput(new Path(new URI(partitionedFile.filePath)), conf)\n+        try {\n+          val datumReader = userProvidedSchema match {\n+            case Some(userSchema) => new GenericDatumReader[GenericRecord](userSchema)\n+            case _ => new GenericDatumReader[GenericRecord]()\n+          }\n+          DataFileReader.openReader(in, datumReader)\n+        } catch {\n+          case NonFatal(e) =>\n+            logError(\"Exception while opening DataFileReader\", e)\n+            in.close()\n+            throw e\n+        }\n+      }\n+\n+      // Ensure that the reader is closed even if the task fails or doesn't consume the entire\n+      // iterator of records.\n+      Option(TaskContext.get()).foreach { taskContext =>\n+        taskContext.addTaskCompletionListener[Unit] { _ =>\n+          reader.close()\n+        }\n+      }\n+\n+      reader.sync(partitionedFile.start)\n+      val stop = partitionedFile.start + partitionedFile.length\n+\n+      val deserializer =\n+        new AvroDeserializer(userProvidedSchema.getOrElse(reader.getSchema), readDataSchema)\n+\n+"
  }],
  "prId": 25017
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "```\r\n-import org.apache.spark.sql.execution.datasources.v2._\r\n+import org.apache.spark.sql.execution.datasources.v2.{EmptyPartitionReader, FilePartitionReaderFactory, PartitionReaderWithPartitionValues}\r\n```",
    "commit": "fc98bd53130f67a4ea140d70a393c0ba4ba9c83e",
    "createdAt": "2019-07-05T03:31:17Z",
    "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.v2.avro\n+\n+import java.net.URI\n+\n+import scala.util.control.NonFatal\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.file.DataFileReader\n+import org.apache.avro.generic.{GenericDatumReader, GenericRecord}\n+import org.apache.avro.mapred.FsInput\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.TaskContext\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.avro.{AvroDeserializer, AvroOptions}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.v2._"
  }],
  "prId": 25017
}]