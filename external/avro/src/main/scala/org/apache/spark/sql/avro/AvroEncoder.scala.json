[{
  "comments": [{
    "author": {
      "login": "bdrillard"
    },
    "body": "Nit: extraneous newline",
    "commit": "dfae1b08209c3c60beeaeb4dacacf1571047b460",
    "createdAt": "2018-10-30T14:15:16Z",
    "diffHunk": "@@ -0,0 +1,534 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.avro\n+\n+import java.io._\n+import java.util.{Map => JMap}\n+\n+import scala.collection.JavaConverters._\n+import scala.language.existentials\n+import scala.reflect.ClassTag\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.Schema.Parser\n+import org.apache.avro.Schema.Type._\n+import org.apache.avro.generic.{GenericData, IndexedRecord}\n+import org.apache.avro.reflect.ReflectData\n+import org.apache.avro.specific.SpecificRecord\n+\n+import org.apache.spark.sql.Encoder\n+import org.apache.spark.sql.avro.SchemaConverters._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.{GetColumnByOrdinal, UnresolvedExtractValue}\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.codegen._\n+import org.apache.spark.sql.catalyst.expressions.codegen.Block._\n+import org.apache.spark.sql.catalyst.expressions.objects.{LambdaVariable => _, _}\n+import org.apache.spark.sql.catalyst.util.{ArrayBasedMapData, GenericArrayData}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * A Spark-SQL Encoder for Avro objects\n+ */\n+object AvroEncoder {\n+  /**\n+   * Provides an Encoder for Avro objects of the given class\n+   *\n+   * @param avroClass the class of the Avro object for which to generate the Encoder\n+   * @tparam T the type of the Avro class, must implement SpecificRecord\n+   * @return an Encoder for the given Avro class\n+   */\n+  def of[T <: SpecificRecord](avroClass: Class[T]): Encoder[T] = {\n+    AvroExpressionEncoder.of(avroClass)\n+  }\n+  /**\n+   * Provides an Encoder for Avro objects implementing the given schema\n+   *\n+   * @param avroSchema the Schema of the Avro object for which to generate the Encoder\n+   * @tparam T the type of the Avro class that implements the Schema, must implement IndexedRecord\n+   * @return an Encoder for the given Avro Schema\n+   */\n+  def of[T <: IndexedRecord](avroSchema: Schema): Encoder[T] = {\n+    AvroExpressionEncoder.of(avroSchema)\n+  }\n+}\n+\n+class SerializableSchema(@transient var value: Schema) extends Externalizable {\n+  def this() = this(null)\n+  override def readExternal(in: ObjectInput): Unit = {\n+    value = new Parser().parse(in.readObject().asInstanceOf[String])\n+  }\n+  override def writeExternal(out: ObjectOutput): Unit = out.writeObject(value.toString)\n+  def resolveUnion(datum: Any): Int = GenericData.get.resolveUnion(value, datum)\n+}\n+\n+object AvroExpressionEncoder {\n+\n+  def of[T <: SpecificRecord](avroClass: Class[T]): ExpressionEncoder[T] = {\n+    val schema = avroClass.getMethod(\"getClassSchema\").invoke(null).asInstanceOf[Schema]\n+    assert(toSqlType(schema).dataType.isInstanceOf[StructType])\n+    val serializer = AvroTypeInference.serializerFor(avroClass, schema)\n+    val deserializer = AvroTypeInference.deserializerFor(schema)\n+    new ExpressionEncoder[T](\n+      serializer,\n+      deserializer,\n+      ClassTag[T](avroClass))\n+  }\n+\n+  def of[T <: IndexedRecord](schema: Schema): ExpressionEncoder[T] = {\n+    assert(toSqlType(schema).dataType.isInstanceOf[StructType])\n+    val avroClass = Option(ReflectData.get.getClass(schema))\n+      .map(_.asSubclass(classOf[SpecificRecord]))\n+      .getOrElse(classOf[GenericData.Record])\n+    val serializer = AvroTypeInference.serializerFor(avroClass, schema)\n+    val deserializer = AvroTypeInference.deserializerFor(schema)\n+    new ExpressionEncoder[T](\n+      serializer,\n+      deserializer,\n+      ClassTag[T](avroClass))\n+  }\n+}\n+\n+/**\n+ * Utilities for providing Avro object serializers and deserializers\n+ */\n+private object AvroTypeInference {\n+\n+  /**\n+   * Translates an Avro Schema type to a proper SQL DataType. The Java Objects that back data in\n+   * generated Generic and Specific records sometimes do not align with those suggested by Avro\n+   * ReflectData, so we infer the proper SQL DataType to serialize and deserialize based on\n+   * nullability and the wrapping Schema type.\n+   */\n+  private def inferExternalType(avroSchema: Schema): DataType = {\n+    toSqlType(avroSchema) match {\n+      // the non-nullable primitive types\n+      case SchemaType(BooleanType, false) => BooleanType\n+      case SchemaType(IntegerType, false) => IntegerType\n+      case SchemaType(LongType, false) =>\n+        if (avroSchema.getType == UNION) {\n+          ObjectType(classOf[java.lang.Number])\n+        } else {\n+          LongType\n+        }\n+      case SchemaType(FloatType, false) => FloatType\n+      case SchemaType(DoubleType, false) =>\n+        if (avroSchema.getType == UNION) {\n+          ObjectType(classOf[java.lang.Number])\n+        } else {\n+          DoubleType\n+        }\n+      // the nullable primitive types\n+      case SchemaType(BooleanType, true) => ObjectType(classOf[java.lang.Boolean])\n+      case SchemaType(IntegerType, true) => ObjectType(classOf[java.lang.Integer])\n+      case SchemaType(LongType, true) => ObjectType(classOf[java.lang.Long])\n+      case SchemaType(FloatType, true) => ObjectType(classOf[java.lang.Float])\n+      case SchemaType(DoubleType, true) => ObjectType(classOf[java.lang.Double])\n+      // the binary types\n+      case SchemaType(BinaryType, _) =>\n+        if (avroSchema.getType == FIXED) {\n+          Option(ReflectData.get.getClass(avroSchema))\n+            .map(ObjectType(_))\n+            .getOrElse(ObjectType(classOf[GenericData.Fixed]))\n+        } else {\n+          ObjectType(classOf[java.nio.ByteBuffer])\n+        }\n+      // the referenced types\n+      case SchemaType(ArrayType(_, _), _) =>\n+        ObjectType(classOf[java.util.List[Object]])\n+      case SchemaType(StringType, _) =>\n+        avroSchema.getType match {\n+          case ENUM =>\n+            Option(ReflectData.get.getClass(avroSchema))\n+              .map(ObjectType(_))\n+              .getOrElse(ObjectType(classOf[GenericData.EnumSymbol]))\n+          case _ =>\n+            ObjectType(classOf[CharSequence])\n+        }\n+      case SchemaType(StructType(_), _) =>\n+        Option(ReflectData.get.getClass(avroSchema))\n+          .map(ObjectType(_))\n+          .getOrElse(ObjectType(classOf[GenericData.Record]))\n+      case SchemaType(MapType(_, _, _), _) =>\n+        ObjectType(classOf[java.util.Map[Object, Object]])\n+    }\n+  }\n+\n+  /**\n+   * Returns an expression that can be used to deserialize an InternalRow to an Avro object of\n+   * type `T` that implements IndexedRecord and is compatible with the given Schema. The Spark SQL\n+   * representation is located at ordinal 0 of a row, i.e. `GetColumnByOrdinal(0, _)`. Nested\n+   * will have their fields accessed using `UnresolvedExtractValue`.\n+   */\n+  def deserializerFor[T <: IndexedRecord] (avroSchema: Schema): Expression = {\n+    deserializerFor(avroSchema, GetColumnByOrdinal(0, inferExternalType(avroSchema)))\n+  }\n+\n+  private def deserializerFor(avroSchema: Schema, path: Expression): Expression = {\n+    /** Returns the current path with a sub-field extracted. */\n+    def addToPath(part: String): Expression = UnresolvedExtractValue(path, Literal(part))\n+\n+    avroSchema.getType match {\n+      case BOOLEAN =>\n+        NewInstance(\n+          classOf[java.lang.Boolean],\n+          path :: Nil,\n+          ObjectType(classOf[java.lang.Boolean]))\n+      case INT =>\n+        NewInstance(\n+          classOf[java.lang.Integer],\n+          path :: Nil,\n+          ObjectType(classOf[java.lang.Integer]))\n+      case LONG =>\n+        NewInstance(\n+          classOf[java.lang.Long],\n+          path :: Nil,\n+          ObjectType(classOf[java.lang.Long]))\n+      case FLOAT =>\n+        NewInstance(\n+          classOf[java.lang.Float],\n+          path :: Nil,\n+          ObjectType(classOf[java.lang.Float]))\n+      case DOUBLE =>\n+        NewInstance(\n+          classOf[java.lang.Double],\n+          path :: Nil,\n+          ObjectType(classOf[java.lang.Double]))\n+\n+      case BYTES =>\n+        StaticInvoke(\n+          classOf[java.nio.ByteBuffer],\n+          ObjectType(classOf[java.nio.ByteBuffer]),\n+          \"wrap\",\n+          path :: Nil)\n+      case FIXED =>\n+        val fixedClass = Option(ReflectData.get.getClass(avroSchema))\n+          .getOrElse(classOf[GenericData.Fixed])\n+        if (fixedClass == classOf[GenericData.Fixed]) {\n+          NewInstance(\n+            fixedClass,\n+            Invoke(\n+              Literal.fromObject(\n+                new SerializableSchema(avroSchema),\n+                ObjectType(classOf[SerializableSchema])),\n+              \"value\",\n+              ObjectType(classOf[Schema]),\n+              Nil) ::\n+              path ::\n+              Nil,\n+            ObjectType(fixedClass))\n+        } else {\n+          NewInstance(\n+            fixedClass,\n+            path :: Nil,\n+            ObjectType(fixedClass))\n+        }\n+\n+      case STRING =>\n+        Invoke(path, \"toString\", ObjectType(classOf[String]))\n+\n+      case ENUM =>\n+        val enumClass = Option(ReflectData.get.getClass(avroSchema))\n+          .getOrElse(classOf[GenericData.EnumSymbol])\n+        if (enumClass == classOf[GenericData.EnumSymbol]) {\n+          NewInstance(\n+            enumClass,\n+            Invoke(\n+              Literal.fromObject(\n+                new SerializableSchema(avroSchema),\n+                ObjectType(classOf[SerializableSchema])),\n+              \"value\",\n+              ObjectType(classOf[Schema]),\n+              Nil) ::\n+              Invoke(path, \"toString\", ObjectType(classOf[String])) ::\n+              Nil,\n+            ObjectType(enumClass))\n+        } else {\n+          StaticInvoke(\n+            enumClass,\n+            ObjectType(enumClass),\n+            \"valueOf\",\n+            Invoke(path, \"toString\", ObjectType(classOf[String])) :: Nil)\n+        }\n+\n+      case ARRAY =>\n+        val elementSchema = avroSchema.getElementType\n+        val elementType = toSqlType(elementSchema).dataType\n+        val array = Invoke(\n+          MapObjects(element =>\n+            deserializerFor(elementSchema, element),\n+            path,\n+            elementType),\n+          \"array\",\n+          ObjectType(classOf[Array[Any]]))\n+        StaticInvoke(\n+          classOf[java.util.Arrays],\n+          ObjectType(classOf[java.util.List[Object]]),\n+          \"asList\",\n+          array :: Nil)\n+\n+      case MAP =>\n+        val valueSchema = avroSchema.getValueType\n+        val valueType = inferExternalType(valueSchema) match {\n+          case t if t == ObjectType(classOf[java.lang.CharSequence]) =>\n+            StringType\n+          case other => other\n+        }\n+        val keyData = Invoke(\n+          MapObjects(\n+            p => deserializerFor(Schema.create(STRING), p),\n+            Invoke(path, \"keyArray\", ArrayType(StringType)),\n+            StringType),\n+          \"array\",\n+          ObjectType(classOf[Array[Any]]))\n+        val valueData = Invoke(\n+          MapObjects(\n+            p => deserializerFor(valueSchema, p),\n+            Invoke(path, \"valueArray\", ArrayType(valueType)),\n+            valueType),\n+          \"array\",\n+          ObjectType(classOf[Array[Any]]))\n+        StaticInvoke(\n+          ArrayBasedMapData.getClass,\n+          ObjectType(classOf[JMap[_, _]]),\n+          \"toJavaMap\",\n+          keyData :: valueData :: Nil)\n+\n+      case UNION =>\n+        val (resolvedSchema, _) =\n+          org.apache.spark.sql.avro.SchemaConverters.resolveUnionType(avroSchema, Set.empty)\n+        if (resolvedSchema.getType == RECORD &&\n+          avroSchema.getTypes.asScala.filterNot(_.getType == NULL).length > 1) {\n+          // A Union resolved to a record that originally had more than 1 type when filtered\n+          // of its nulls must be complex\n+          val bottom = Literal.create(null, ObjectType(classOf[Object])).asInstanceOf[Expression]\n+          resolvedSchema.getFields.asScala.foldLeft(bottom) {\n+            (tree: Expression, field: Schema.Field) =>\n+              val fieldValue = ObjectCast(\n+                deserializerFor(field.schema, addToPath(field.name)),\n+                ObjectType(classOf[Object]))\n+              If(IsNull(fieldValue), tree, fieldValue)\n+          }\n+        } else {\n+          deserializerFor(resolvedSchema, path)\n+        }\n+\n+      case RECORD =>\n+        val args = avroSchema.getFields.asScala.map { field =>\n+          val position = Literal(field.pos)\n+          val argument = deserializerFor(field.schema, addToPath(field.name))\n+          (position, argument)\n+        }.toList\n+        val recordClass = Option(ReflectData.get.getClass(avroSchema))\n+          .getOrElse(classOf[GenericData.Record])\n+        val newInstance = if (recordClass == classOf[GenericData.Record]) {\n+          NewInstance(\n+            recordClass,\n+            Invoke(\n+              Literal.fromObject(\n+                new SerializableSchema(avroSchema),\n+                ObjectType(classOf[SerializableSchema])),\n+              \"value\",\n+              ObjectType(classOf[Schema]),\n+              Nil) :: Nil,\n+            ObjectType(recordClass))\n+        } else {\n+          NewInstance(\n+            recordClass,\n+            Nil,\n+            ObjectType(recordClass))\n+        }\n+        val result = InitializeAvroObject(newInstance, args)\n+\n+        If(IsNull(path), Literal.create(null, ObjectType(recordClass)), result)\n+\n+      case NULL =>\n+        // Encountering NULL at this level implies it was the type of a Field, which should never\n+        // be the case.\n+        throw new IncompatibleSchemaException(\"Null type should only be used in Union types\")\n+    }\n+  }\n+\n+  /**\n+   * Returns an expression that can be used to serialize an Avro object with a class of type `T`\n+   * that is compatible with the given Schema to an InternalRow\n+   */\n+  def serializerFor[T <: IndexedRecord](\n+      avroClass: Class[T], avroSchema: Schema): Expression = {\n+    val inputObject = BoundReference(0, ObjectType(avroClass), nullable = true)\n+    val nullSafeInput = AssertNotNull(inputObject, Seq(\"top level\"))\n+    serializerFor(nullSafeInput, avroSchema)\n+  }\n+\n+  def serializerFor(\n+      inputObject: Expression,\n+      avroSchema: Schema): Expression = {\n+\n+    def toCatalystArray(inputObject: Expression, schema: Schema): Expression = {\n+      val elementType = inferExternalType(schema)\n+      if (elementType.isInstanceOf[ObjectType]) {\n+        MapObjects(element =>\n+          serializerFor(element, schema),\n+          Invoke(\n+            inputObject,\n+            \"toArray\",\n+            ObjectType(classOf[Array[Object]])),\n+          elementType)\n+      } else {\n+        NewInstance(\n+          classOf[GenericArrayData],\n+          inputObject :: Nil,\n+          dataType = ArrayType(elementType, containsNull = false))\n+      }\n+    }\n+\n+    def toCatalystMap(inputObject: Expression, schema: Schema): Expression = {\n+      val valueSchema = schema.getValueType\n+      val valueType = inferExternalType(valueSchema)\n+      ExternalMapToCatalyst(\n+        inputObject,\n+        ObjectType(classOf[org.apache.avro.util.Utf8]),\n+        serializerFor(_, Schema.create(STRING)),\n+        keyNullable = true,\n+        valueType,\n+        serializerFor(_, valueSchema),\n+        valueNullable = true)\n+    }\n+\n+"
  }, {
    "author": {
      "login": "xuanyuanking"
    },
    "body": "Thanks, done in 6c73a94.",
    "commit": "dfae1b08209c3c60beeaeb4dacacf1571047b460",
    "createdAt": "2018-10-31T16:14:13Z",
    "diffHunk": "@@ -0,0 +1,534 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.avro\n+\n+import java.io._\n+import java.util.{Map => JMap}\n+\n+import scala.collection.JavaConverters._\n+import scala.language.existentials\n+import scala.reflect.ClassTag\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.Schema.Parser\n+import org.apache.avro.Schema.Type._\n+import org.apache.avro.generic.{GenericData, IndexedRecord}\n+import org.apache.avro.reflect.ReflectData\n+import org.apache.avro.specific.SpecificRecord\n+\n+import org.apache.spark.sql.Encoder\n+import org.apache.spark.sql.avro.SchemaConverters._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.{GetColumnByOrdinal, UnresolvedExtractValue}\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.codegen._\n+import org.apache.spark.sql.catalyst.expressions.codegen.Block._\n+import org.apache.spark.sql.catalyst.expressions.objects.{LambdaVariable => _, _}\n+import org.apache.spark.sql.catalyst.util.{ArrayBasedMapData, GenericArrayData}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * A Spark-SQL Encoder for Avro objects\n+ */\n+object AvroEncoder {\n+  /**\n+   * Provides an Encoder for Avro objects of the given class\n+   *\n+   * @param avroClass the class of the Avro object for which to generate the Encoder\n+   * @tparam T the type of the Avro class, must implement SpecificRecord\n+   * @return an Encoder for the given Avro class\n+   */\n+  def of[T <: SpecificRecord](avroClass: Class[T]): Encoder[T] = {\n+    AvroExpressionEncoder.of(avroClass)\n+  }\n+  /**\n+   * Provides an Encoder for Avro objects implementing the given schema\n+   *\n+   * @param avroSchema the Schema of the Avro object for which to generate the Encoder\n+   * @tparam T the type of the Avro class that implements the Schema, must implement IndexedRecord\n+   * @return an Encoder for the given Avro Schema\n+   */\n+  def of[T <: IndexedRecord](avroSchema: Schema): Encoder[T] = {\n+    AvroExpressionEncoder.of(avroSchema)\n+  }\n+}\n+\n+class SerializableSchema(@transient var value: Schema) extends Externalizable {\n+  def this() = this(null)\n+  override def readExternal(in: ObjectInput): Unit = {\n+    value = new Parser().parse(in.readObject().asInstanceOf[String])\n+  }\n+  override def writeExternal(out: ObjectOutput): Unit = out.writeObject(value.toString)\n+  def resolveUnion(datum: Any): Int = GenericData.get.resolveUnion(value, datum)\n+}\n+\n+object AvroExpressionEncoder {\n+\n+  def of[T <: SpecificRecord](avroClass: Class[T]): ExpressionEncoder[T] = {\n+    val schema = avroClass.getMethod(\"getClassSchema\").invoke(null).asInstanceOf[Schema]\n+    assert(toSqlType(schema).dataType.isInstanceOf[StructType])\n+    val serializer = AvroTypeInference.serializerFor(avroClass, schema)\n+    val deserializer = AvroTypeInference.deserializerFor(schema)\n+    new ExpressionEncoder[T](\n+      serializer,\n+      deserializer,\n+      ClassTag[T](avroClass))\n+  }\n+\n+  def of[T <: IndexedRecord](schema: Schema): ExpressionEncoder[T] = {\n+    assert(toSqlType(schema).dataType.isInstanceOf[StructType])\n+    val avroClass = Option(ReflectData.get.getClass(schema))\n+      .map(_.asSubclass(classOf[SpecificRecord]))\n+      .getOrElse(classOf[GenericData.Record])\n+    val serializer = AvroTypeInference.serializerFor(avroClass, schema)\n+    val deserializer = AvroTypeInference.deserializerFor(schema)\n+    new ExpressionEncoder[T](\n+      serializer,\n+      deserializer,\n+      ClassTag[T](avroClass))\n+  }\n+}\n+\n+/**\n+ * Utilities for providing Avro object serializers and deserializers\n+ */\n+private object AvroTypeInference {\n+\n+  /**\n+   * Translates an Avro Schema type to a proper SQL DataType. The Java Objects that back data in\n+   * generated Generic and Specific records sometimes do not align with those suggested by Avro\n+   * ReflectData, so we infer the proper SQL DataType to serialize and deserialize based on\n+   * nullability and the wrapping Schema type.\n+   */\n+  private def inferExternalType(avroSchema: Schema): DataType = {\n+    toSqlType(avroSchema) match {\n+      // the non-nullable primitive types\n+      case SchemaType(BooleanType, false) => BooleanType\n+      case SchemaType(IntegerType, false) => IntegerType\n+      case SchemaType(LongType, false) =>\n+        if (avroSchema.getType == UNION) {\n+          ObjectType(classOf[java.lang.Number])\n+        } else {\n+          LongType\n+        }\n+      case SchemaType(FloatType, false) => FloatType\n+      case SchemaType(DoubleType, false) =>\n+        if (avroSchema.getType == UNION) {\n+          ObjectType(classOf[java.lang.Number])\n+        } else {\n+          DoubleType\n+        }\n+      // the nullable primitive types\n+      case SchemaType(BooleanType, true) => ObjectType(classOf[java.lang.Boolean])\n+      case SchemaType(IntegerType, true) => ObjectType(classOf[java.lang.Integer])\n+      case SchemaType(LongType, true) => ObjectType(classOf[java.lang.Long])\n+      case SchemaType(FloatType, true) => ObjectType(classOf[java.lang.Float])\n+      case SchemaType(DoubleType, true) => ObjectType(classOf[java.lang.Double])\n+      // the binary types\n+      case SchemaType(BinaryType, _) =>\n+        if (avroSchema.getType == FIXED) {\n+          Option(ReflectData.get.getClass(avroSchema))\n+            .map(ObjectType(_))\n+            .getOrElse(ObjectType(classOf[GenericData.Fixed]))\n+        } else {\n+          ObjectType(classOf[java.nio.ByteBuffer])\n+        }\n+      // the referenced types\n+      case SchemaType(ArrayType(_, _), _) =>\n+        ObjectType(classOf[java.util.List[Object]])\n+      case SchemaType(StringType, _) =>\n+        avroSchema.getType match {\n+          case ENUM =>\n+            Option(ReflectData.get.getClass(avroSchema))\n+              .map(ObjectType(_))\n+              .getOrElse(ObjectType(classOf[GenericData.EnumSymbol]))\n+          case _ =>\n+            ObjectType(classOf[CharSequence])\n+        }\n+      case SchemaType(StructType(_), _) =>\n+        Option(ReflectData.get.getClass(avroSchema))\n+          .map(ObjectType(_))\n+          .getOrElse(ObjectType(classOf[GenericData.Record]))\n+      case SchemaType(MapType(_, _, _), _) =>\n+        ObjectType(classOf[java.util.Map[Object, Object]])\n+    }\n+  }\n+\n+  /**\n+   * Returns an expression that can be used to deserialize an InternalRow to an Avro object of\n+   * type `T` that implements IndexedRecord and is compatible with the given Schema. The Spark SQL\n+   * representation is located at ordinal 0 of a row, i.e. `GetColumnByOrdinal(0, _)`. Nested\n+   * will have their fields accessed using `UnresolvedExtractValue`.\n+   */\n+  def deserializerFor[T <: IndexedRecord] (avroSchema: Schema): Expression = {\n+    deserializerFor(avroSchema, GetColumnByOrdinal(0, inferExternalType(avroSchema)))\n+  }\n+\n+  private def deserializerFor(avroSchema: Schema, path: Expression): Expression = {\n+    /** Returns the current path with a sub-field extracted. */\n+    def addToPath(part: String): Expression = UnresolvedExtractValue(path, Literal(part))\n+\n+    avroSchema.getType match {\n+      case BOOLEAN =>\n+        NewInstance(\n+          classOf[java.lang.Boolean],\n+          path :: Nil,\n+          ObjectType(classOf[java.lang.Boolean]))\n+      case INT =>\n+        NewInstance(\n+          classOf[java.lang.Integer],\n+          path :: Nil,\n+          ObjectType(classOf[java.lang.Integer]))\n+      case LONG =>\n+        NewInstance(\n+          classOf[java.lang.Long],\n+          path :: Nil,\n+          ObjectType(classOf[java.lang.Long]))\n+      case FLOAT =>\n+        NewInstance(\n+          classOf[java.lang.Float],\n+          path :: Nil,\n+          ObjectType(classOf[java.lang.Float]))\n+      case DOUBLE =>\n+        NewInstance(\n+          classOf[java.lang.Double],\n+          path :: Nil,\n+          ObjectType(classOf[java.lang.Double]))\n+\n+      case BYTES =>\n+        StaticInvoke(\n+          classOf[java.nio.ByteBuffer],\n+          ObjectType(classOf[java.nio.ByteBuffer]),\n+          \"wrap\",\n+          path :: Nil)\n+      case FIXED =>\n+        val fixedClass = Option(ReflectData.get.getClass(avroSchema))\n+          .getOrElse(classOf[GenericData.Fixed])\n+        if (fixedClass == classOf[GenericData.Fixed]) {\n+          NewInstance(\n+            fixedClass,\n+            Invoke(\n+              Literal.fromObject(\n+                new SerializableSchema(avroSchema),\n+                ObjectType(classOf[SerializableSchema])),\n+              \"value\",\n+              ObjectType(classOf[Schema]),\n+              Nil) ::\n+              path ::\n+              Nil,\n+            ObjectType(fixedClass))\n+        } else {\n+          NewInstance(\n+            fixedClass,\n+            path :: Nil,\n+            ObjectType(fixedClass))\n+        }\n+\n+      case STRING =>\n+        Invoke(path, \"toString\", ObjectType(classOf[String]))\n+\n+      case ENUM =>\n+        val enumClass = Option(ReflectData.get.getClass(avroSchema))\n+          .getOrElse(classOf[GenericData.EnumSymbol])\n+        if (enumClass == classOf[GenericData.EnumSymbol]) {\n+          NewInstance(\n+            enumClass,\n+            Invoke(\n+              Literal.fromObject(\n+                new SerializableSchema(avroSchema),\n+                ObjectType(classOf[SerializableSchema])),\n+              \"value\",\n+              ObjectType(classOf[Schema]),\n+              Nil) ::\n+              Invoke(path, \"toString\", ObjectType(classOf[String])) ::\n+              Nil,\n+            ObjectType(enumClass))\n+        } else {\n+          StaticInvoke(\n+            enumClass,\n+            ObjectType(enumClass),\n+            \"valueOf\",\n+            Invoke(path, \"toString\", ObjectType(classOf[String])) :: Nil)\n+        }\n+\n+      case ARRAY =>\n+        val elementSchema = avroSchema.getElementType\n+        val elementType = toSqlType(elementSchema).dataType\n+        val array = Invoke(\n+          MapObjects(element =>\n+            deserializerFor(elementSchema, element),\n+            path,\n+            elementType),\n+          \"array\",\n+          ObjectType(classOf[Array[Any]]))\n+        StaticInvoke(\n+          classOf[java.util.Arrays],\n+          ObjectType(classOf[java.util.List[Object]]),\n+          \"asList\",\n+          array :: Nil)\n+\n+      case MAP =>\n+        val valueSchema = avroSchema.getValueType\n+        val valueType = inferExternalType(valueSchema) match {\n+          case t if t == ObjectType(classOf[java.lang.CharSequence]) =>\n+            StringType\n+          case other => other\n+        }\n+        val keyData = Invoke(\n+          MapObjects(\n+            p => deserializerFor(Schema.create(STRING), p),\n+            Invoke(path, \"keyArray\", ArrayType(StringType)),\n+            StringType),\n+          \"array\",\n+          ObjectType(classOf[Array[Any]]))\n+        val valueData = Invoke(\n+          MapObjects(\n+            p => deserializerFor(valueSchema, p),\n+            Invoke(path, \"valueArray\", ArrayType(valueType)),\n+            valueType),\n+          \"array\",\n+          ObjectType(classOf[Array[Any]]))\n+        StaticInvoke(\n+          ArrayBasedMapData.getClass,\n+          ObjectType(classOf[JMap[_, _]]),\n+          \"toJavaMap\",\n+          keyData :: valueData :: Nil)\n+\n+      case UNION =>\n+        val (resolvedSchema, _) =\n+          org.apache.spark.sql.avro.SchemaConverters.resolveUnionType(avroSchema, Set.empty)\n+        if (resolvedSchema.getType == RECORD &&\n+          avroSchema.getTypes.asScala.filterNot(_.getType == NULL).length > 1) {\n+          // A Union resolved to a record that originally had more than 1 type when filtered\n+          // of its nulls must be complex\n+          val bottom = Literal.create(null, ObjectType(classOf[Object])).asInstanceOf[Expression]\n+          resolvedSchema.getFields.asScala.foldLeft(bottom) {\n+            (tree: Expression, field: Schema.Field) =>\n+              val fieldValue = ObjectCast(\n+                deserializerFor(field.schema, addToPath(field.name)),\n+                ObjectType(classOf[Object]))\n+              If(IsNull(fieldValue), tree, fieldValue)\n+          }\n+        } else {\n+          deserializerFor(resolvedSchema, path)\n+        }\n+\n+      case RECORD =>\n+        val args = avroSchema.getFields.asScala.map { field =>\n+          val position = Literal(field.pos)\n+          val argument = deserializerFor(field.schema, addToPath(field.name))\n+          (position, argument)\n+        }.toList\n+        val recordClass = Option(ReflectData.get.getClass(avroSchema))\n+          .getOrElse(classOf[GenericData.Record])\n+        val newInstance = if (recordClass == classOf[GenericData.Record]) {\n+          NewInstance(\n+            recordClass,\n+            Invoke(\n+              Literal.fromObject(\n+                new SerializableSchema(avroSchema),\n+                ObjectType(classOf[SerializableSchema])),\n+              \"value\",\n+              ObjectType(classOf[Schema]),\n+              Nil) :: Nil,\n+            ObjectType(recordClass))\n+        } else {\n+          NewInstance(\n+            recordClass,\n+            Nil,\n+            ObjectType(recordClass))\n+        }\n+        val result = InitializeAvroObject(newInstance, args)\n+\n+        If(IsNull(path), Literal.create(null, ObjectType(recordClass)), result)\n+\n+      case NULL =>\n+        // Encountering NULL at this level implies it was the type of a Field, which should never\n+        // be the case.\n+        throw new IncompatibleSchemaException(\"Null type should only be used in Union types\")\n+    }\n+  }\n+\n+  /**\n+   * Returns an expression that can be used to serialize an Avro object with a class of type `T`\n+   * that is compatible with the given Schema to an InternalRow\n+   */\n+  def serializerFor[T <: IndexedRecord](\n+      avroClass: Class[T], avroSchema: Schema): Expression = {\n+    val inputObject = BoundReference(0, ObjectType(avroClass), nullable = true)\n+    val nullSafeInput = AssertNotNull(inputObject, Seq(\"top level\"))\n+    serializerFor(nullSafeInput, avroSchema)\n+  }\n+\n+  def serializerFor(\n+      inputObject: Expression,\n+      avroSchema: Schema): Expression = {\n+\n+    def toCatalystArray(inputObject: Expression, schema: Schema): Expression = {\n+      val elementType = inferExternalType(schema)\n+      if (elementType.isInstanceOf[ObjectType]) {\n+        MapObjects(element =>\n+          serializerFor(element, schema),\n+          Invoke(\n+            inputObject,\n+            \"toArray\",\n+            ObjectType(classOf[Array[Object]])),\n+          elementType)\n+      } else {\n+        NewInstance(\n+          classOf[GenericArrayData],\n+          inputObject :: Nil,\n+          dataType = ArrayType(elementType, containsNull = false))\n+      }\n+    }\n+\n+    def toCatalystMap(inputObject: Expression, schema: Schema): Expression = {\n+      val valueSchema = schema.getValueType\n+      val valueType = inferExternalType(valueSchema)\n+      ExternalMapToCatalyst(\n+        inputObject,\n+        ObjectType(classOf[org.apache.avro.util.Utf8]),\n+        serializerFor(_, Schema.create(STRING)),\n+        keyNullable = true,\n+        valueType,\n+        serializerFor(_, valueSchema),\n+        valueNullable = true)\n+    }\n+\n+"
  }],
  "prId": 22878
}, {
  "comments": [{
    "author": {
      "login": "bdrillard"
    },
    "body": "In the previous databricks/spark-avro#217, user @mtraynham points out we should use `CharSequence` as the object type:\r\n\r\n```suggestion\r\n        ObjectType(classOf[java.lang.CharSequence]),\r\n```\r\n\r\nSee [comment](https://github.com/databricks/spark-avro/pull/217#issuecomment-405305815) and [suggestion](https://github.com/databricks/spark-avro/pull/217#issuecomment-405385022).",
    "commit": "dfae1b08209c3c60beeaeb4dacacf1571047b460",
    "createdAt": "2018-10-30T14:23:02Z",
    "diffHunk": "@@ -0,0 +1,534 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.avro\n+\n+import java.io._\n+import java.util.{Map => JMap}\n+\n+import scala.collection.JavaConverters._\n+import scala.language.existentials\n+import scala.reflect.ClassTag\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.Schema.Parser\n+import org.apache.avro.Schema.Type._\n+import org.apache.avro.generic.{GenericData, IndexedRecord}\n+import org.apache.avro.reflect.ReflectData\n+import org.apache.avro.specific.SpecificRecord\n+\n+import org.apache.spark.sql.Encoder\n+import org.apache.spark.sql.avro.SchemaConverters._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.{GetColumnByOrdinal, UnresolvedExtractValue}\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.codegen._\n+import org.apache.spark.sql.catalyst.expressions.codegen.Block._\n+import org.apache.spark.sql.catalyst.expressions.objects.{LambdaVariable => _, _}\n+import org.apache.spark.sql.catalyst.util.{ArrayBasedMapData, GenericArrayData}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * A Spark-SQL Encoder for Avro objects\n+ */\n+object AvroEncoder {\n+  /**\n+   * Provides an Encoder for Avro objects of the given class\n+   *\n+   * @param avroClass the class of the Avro object for which to generate the Encoder\n+   * @tparam T the type of the Avro class, must implement SpecificRecord\n+   * @return an Encoder for the given Avro class\n+   */\n+  def of[T <: SpecificRecord](avroClass: Class[T]): Encoder[T] = {\n+    AvroExpressionEncoder.of(avroClass)\n+  }\n+  /**\n+   * Provides an Encoder for Avro objects implementing the given schema\n+   *\n+   * @param avroSchema the Schema of the Avro object for which to generate the Encoder\n+   * @tparam T the type of the Avro class that implements the Schema, must implement IndexedRecord\n+   * @return an Encoder for the given Avro Schema\n+   */\n+  def of[T <: IndexedRecord](avroSchema: Schema): Encoder[T] = {\n+    AvroExpressionEncoder.of(avroSchema)\n+  }\n+}\n+\n+class SerializableSchema(@transient var value: Schema) extends Externalizable {\n+  def this() = this(null)\n+  override def readExternal(in: ObjectInput): Unit = {\n+    value = new Parser().parse(in.readObject().asInstanceOf[String])\n+  }\n+  override def writeExternal(out: ObjectOutput): Unit = out.writeObject(value.toString)\n+  def resolveUnion(datum: Any): Int = GenericData.get.resolveUnion(value, datum)\n+}\n+\n+object AvroExpressionEncoder {\n+\n+  def of[T <: SpecificRecord](avroClass: Class[T]): ExpressionEncoder[T] = {\n+    val schema = avroClass.getMethod(\"getClassSchema\").invoke(null).asInstanceOf[Schema]\n+    assert(toSqlType(schema).dataType.isInstanceOf[StructType])\n+    val serializer = AvroTypeInference.serializerFor(avroClass, schema)\n+    val deserializer = AvroTypeInference.deserializerFor(schema)\n+    new ExpressionEncoder[T](\n+      serializer,\n+      deserializer,\n+      ClassTag[T](avroClass))\n+  }\n+\n+  def of[T <: IndexedRecord](schema: Schema): ExpressionEncoder[T] = {\n+    assert(toSqlType(schema).dataType.isInstanceOf[StructType])\n+    val avroClass = Option(ReflectData.get.getClass(schema))\n+      .map(_.asSubclass(classOf[SpecificRecord]))\n+      .getOrElse(classOf[GenericData.Record])\n+    val serializer = AvroTypeInference.serializerFor(avroClass, schema)\n+    val deserializer = AvroTypeInference.deserializerFor(schema)\n+    new ExpressionEncoder[T](\n+      serializer,\n+      deserializer,\n+      ClassTag[T](avroClass))\n+  }\n+}\n+\n+/**\n+ * Utilities for providing Avro object serializers and deserializers\n+ */\n+private object AvroTypeInference {\n+\n+  /**\n+   * Translates an Avro Schema type to a proper SQL DataType. The Java Objects that back data in\n+   * generated Generic and Specific records sometimes do not align with those suggested by Avro\n+   * ReflectData, so we infer the proper SQL DataType to serialize and deserialize based on\n+   * nullability and the wrapping Schema type.\n+   */\n+  private def inferExternalType(avroSchema: Schema): DataType = {\n+    toSqlType(avroSchema) match {\n+      // the non-nullable primitive types\n+      case SchemaType(BooleanType, false) => BooleanType\n+      case SchemaType(IntegerType, false) => IntegerType\n+      case SchemaType(LongType, false) =>\n+        if (avroSchema.getType == UNION) {\n+          ObjectType(classOf[java.lang.Number])\n+        } else {\n+          LongType\n+        }\n+      case SchemaType(FloatType, false) => FloatType\n+      case SchemaType(DoubleType, false) =>\n+        if (avroSchema.getType == UNION) {\n+          ObjectType(classOf[java.lang.Number])\n+        } else {\n+          DoubleType\n+        }\n+      // the nullable primitive types\n+      case SchemaType(BooleanType, true) => ObjectType(classOf[java.lang.Boolean])\n+      case SchemaType(IntegerType, true) => ObjectType(classOf[java.lang.Integer])\n+      case SchemaType(LongType, true) => ObjectType(classOf[java.lang.Long])\n+      case SchemaType(FloatType, true) => ObjectType(classOf[java.lang.Float])\n+      case SchemaType(DoubleType, true) => ObjectType(classOf[java.lang.Double])\n+      // the binary types\n+      case SchemaType(BinaryType, _) =>\n+        if (avroSchema.getType == FIXED) {\n+          Option(ReflectData.get.getClass(avroSchema))\n+            .map(ObjectType(_))\n+            .getOrElse(ObjectType(classOf[GenericData.Fixed]))\n+        } else {\n+          ObjectType(classOf[java.nio.ByteBuffer])\n+        }\n+      // the referenced types\n+      case SchemaType(ArrayType(_, _), _) =>\n+        ObjectType(classOf[java.util.List[Object]])\n+      case SchemaType(StringType, _) =>\n+        avroSchema.getType match {\n+          case ENUM =>\n+            Option(ReflectData.get.getClass(avroSchema))\n+              .map(ObjectType(_))\n+              .getOrElse(ObjectType(classOf[GenericData.EnumSymbol]))\n+          case _ =>\n+            ObjectType(classOf[CharSequence])\n+        }\n+      case SchemaType(StructType(_), _) =>\n+        Option(ReflectData.get.getClass(avroSchema))\n+          .map(ObjectType(_))\n+          .getOrElse(ObjectType(classOf[GenericData.Record]))\n+      case SchemaType(MapType(_, _, _), _) =>\n+        ObjectType(classOf[java.util.Map[Object, Object]])\n+    }\n+  }\n+\n+  /**\n+   * Returns an expression that can be used to deserialize an InternalRow to an Avro object of\n+   * type `T` that implements IndexedRecord and is compatible with the given Schema. The Spark SQL\n+   * representation is located at ordinal 0 of a row, i.e. `GetColumnByOrdinal(0, _)`. Nested\n+   * will have their fields accessed using `UnresolvedExtractValue`.\n+   */\n+  def deserializerFor[T <: IndexedRecord] (avroSchema: Schema): Expression = {\n+    deserializerFor(avroSchema, GetColumnByOrdinal(0, inferExternalType(avroSchema)))\n+  }\n+\n+  private def deserializerFor(avroSchema: Schema, path: Expression): Expression = {\n+    /** Returns the current path with a sub-field extracted. */\n+    def addToPath(part: String): Expression = UnresolvedExtractValue(path, Literal(part))\n+\n+    avroSchema.getType match {\n+      case BOOLEAN =>\n+        NewInstance(\n+          classOf[java.lang.Boolean],\n+          path :: Nil,\n+          ObjectType(classOf[java.lang.Boolean]))\n+      case INT =>\n+        NewInstance(\n+          classOf[java.lang.Integer],\n+          path :: Nil,\n+          ObjectType(classOf[java.lang.Integer]))\n+      case LONG =>\n+        NewInstance(\n+          classOf[java.lang.Long],\n+          path :: Nil,\n+          ObjectType(classOf[java.lang.Long]))\n+      case FLOAT =>\n+        NewInstance(\n+          classOf[java.lang.Float],\n+          path :: Nil,\n+          ObjectType(classOf[java.lang.Float]))\n+      case DOUBLE =>\n+        NewInstance(\n+          classOf[java.lang.Double],\n+          path :: Nil,\n+          ObjectType(classOf[java.lang.Double]))\n+\n+      case BYTES =>\n+        StaticInvoke(\n+          classOf[java.nio.ByteBuffer],\n+          ObjectType(classOf[java.nio.ByteBuffer]),\n+          \"wrap\",\n+          path :: Nil)\n+      case FIXED =>\n+        val fixedClass = Option(ReflectData.get.getClass(avroSchema))\n+          .getOrElse(classOf[GenericData.Fixed])\n+        if (fixedClass == classOf[GenericData.Fixed]) {\n+          NewInstance(\n+            fixedClass,\n+            Invoke(\n+              Literal.fromObject(\n+                new SerializableSchema(avroSchema),\n+                ObjectType(classOf[SerializableSchema])),\n+              \"value\",\n+              ObjectType(classOf[Schema]),\n+              Nil) ::\n+              path ::\n+              Nil,\n+            ObjectType(fixedClass))\n+        } else {\n+          NewInstance(\n+            fixedClass,\n+            path :: Nil,\n+            ObjectType(fixedClass))\n+        }\n+\n+      case STRING =>\n+        Invoke(path, \"toString\", ObjectType(classOf[String]))\n+\n+      case ENUM =>\n+        val enumClass = Option(ReflectData.get.getClass(avroSchema))\n+          .getOrElse(classOf[GenericData.EnumSymbol])\n+        if (enumClass == classOf[GenericData.EnumSymbol]) {\n+          NewInstance(\n+            enumClass,\n+            Invoke(\n+              Literal.fromObject(\n+                new SerializableSchema(avroSchema),\n+                ObjectType(classOf[SerializableSchema])),\n+              \"value\",\n+              ObjectType(classOf[Schema]),\n+              Nil) ::\n+              Invoke(path, \"toString\", ObjectType(classOf[String])) ::\n+              Nil,\n+            ObjectType(enumClass))\n+        } else {\n+          StaticInvoke(\n+            enumClass,\n+            ObjectType(enumClass),\n+            \"valueOf\",\n+            Invoke(path, \"toString\", ObjectType(classOf[String])) :: Nil)\n+        }\n+\n+      case ARRAY =>\n+        val elementSchema = avroSchema.getElementType\n+        val elementType = toSqlType(elementSchema).dataType\n+        val array = Invoke(\n+          MapObjects(element =>\n+            deserializerFor(elementSchema, element),\n+            path,\n+            elementType),\n+          \"array\",\n+          ObjectType(classOf[Array[Any]]))\n+        StaticInvoke(\n+          classOf[java.util.Arrays],\n+          ObjectType(classOf[java.util.List[Object]]),\n+          \"asList\",\n+          array :: Nil)\n+\n+      case MAP =>\n+        val valueSchema = avroSchema.getValueType\n+        val valueType = inferExternalType(valueSchema) match {\n+          case t if t == ObjectType(classOf[java.lang.CharSequence]) =>\n+            StringType\n+          case other => other\n+        }\n+        val keyData = Invoke(\n+          MapObjects(\n+            p => deserializerFor(Schema.create(STRING), p),\n+            Invoke(path, \"keyArray\", ArrayType(StringType)),\n+            StringType),\n+          \"array\",\n+          ObjectType(classOf[Array[Any]]))\n+        val valueData = Invoke(\n+          MapObjects(\n+            p => deserializerFor(valueSchema, p),\n+            Invoke(path, \"valueArray\", ArrayType(valueType)),\n+            valueType),\n+          \"array\",\n+          ObjectType(classOf[Array[Any]]))\n+        StaticInvoke(\n+          ArrayBasedMapData.getClass,\n+          ObjectType(classOf[JMap[_, _]]),\n+          \"toJavaMap\",\n+          keyData :: valueData :: Nil)\n+\n+      case UNION =>\n+        val (resolvedSchema, _) =\n+          org.apache.spark.sql.avro.SchemaConverters.resolveUnionType(avroSchema, Set.empty)\n+        if (resolvedSchema.getType == RECORD &&\n+          avroSchema.getTypes.asScala.filterNot(_.getType == NULL).length > 1) {\n+          // A Union resolved to a record that originally had more than 1 type when filtered\n+          // of its nulls must be complex\n+          val bottom = Literal.create(null, ObjectType(classOf[Object])).asInstanceOf[Expression]\n+          resolvedSchema.getFields.asScala.foldLeft(bottom) {\n+            (tree: Expression, field: Schema.Field) =>\n+              val fieldValue = ObjectCast(\n+                deserializerFor(field.schema, addToPath(field.name)),\n+                ObjectType(classOf[Object]))\n+              If(IsNull(fieldValue), tree, fieldValue)\n+          }\n+        } else {\n+          deserializerFor(resolvedSchema, path)\n+        }\n+\n+      case RECORD =>\n+        val args = avroSchema.getFields.asScala.map { field =>\n+          val position = Literal(field.pos)\n+          val argument = deserializerFor(field.schema, addToPath(field.name))\n+          (position, argument)\n+        }.toList\n+        val recordClass = Option(ReflectData.get.getClass(avroSchema))\n+          .getOrElse(classOf[GenericData.Record])\n+        val newInstance = if (recordClass == classOf[GenericData.Record]) {\n+          NewInstance(\n+            recordClass,\n+            Invoke(\n+              Literal.fromObject(\n+                new SerializableSchema(avroSchema),\n+                ObjectType(classOf[SerializableSchema])),\n+              \"value\",\n+              ObjectType(classOf[Schema]),\n+              Nil) :: Nil,\n+            ObjectType(recordClass))\n+        } else {\n+          NewInstance(\n+            recordClass,\n+            Nil,\n+            ObjectType(recordClass))\n+        }\n+        val result = InitializeAvroObject(newInstance, args)\n+\n+        If(IsNull(path), Literal.create(null, ObjectType(recordClass)), result)\n+\n+      case NULL =>\n+        // Encountering NULL at this level implies it was the type of a Field, which should never\n+        // be the case.\n+        throw new IncompatibleSchemaException(\"Null type should only be used in Union types\")\n+    }\n+  }\n+\n+  /**\n+   * Returns an expression that can be used to serialize an Avro object with a class of type `T`\n+   * that is compatible with the given Schema to an InternalRow\n+   */\n+  def serializerFor[T <: IndexedRecord](\n+      avroClass: Class[T], avroSchema: Schema): Expression = {\n+    val inputObject = BoundReference(0, ObjectType(avroClass), nullable = true)\n+    val nullSafeInput = AssertNotNull(inputObject, Seq(\"top level\"))\n+    serializerFor(nullSafeInput, avroSchema)\n+  }\n+\n+  def serializerFor(\n+      inputObject: Expression,\n+      avroSchema: Schema): Expression = {\n+\n+    def toCatalystArray(inputObject: Expression, schema: Schema): Expression = {\n+      val elementType = inferExternalType(schema)\n+      if (elementType.isInstanceOf[ObjectType]) {\n+        MapObjects(element =>\n+          serializerFor(element, schema),\n+          Invoke(\n+            inputObject,\n+            \"toArray\",\n+            ObjectType(classOf[Array[Object]])),\n+          elementType)\n+      } else {\n+        NewInstance(\n+          classOf[GenericArrayData],\n+          inputObject :: Nil,\n+          dataType = ArrayType(elementType, containsNull = false))\n+      }\n+    }\n+\n+    def toCatalystMap(inputObject: Expression, schema: Schema): Expression = {\n+      val valueSchema = schema.getValueType\n+      val valueType = inferExternalType(valueSchema)\n+      ExternalMapToCatalyst(\n+        inputObject,\n+        ObjectType(classOf[org.apache.avro.util.Utf8]),"
  }, {
    "author": {
      "login": "xuanyuanking"
    },
    "body": "Ah, thanks for reminding, I missed this.",
    "commit": "dfae1b08209c3c60beeaeb4dacacf1571047b460",
    "createdAt": "2018-10-31T16:08:28Z",
    "diffHunk": "@@ -0,0 +1,534 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.avro\n+\n+import java.io._\n+import java.util.{Map => JMap}\n+\n+import scala.collection.JavaConverters._\n+import scala.language.existentials\n+import scala.reflect.ClassTag\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.Schema.Parser\n+import org.apache.avro.Schema.Type._\n+import org.apache.avro.generic.{GenericData, IndexedRecord}\n+import org.apache.avro.reflect.ReflectData\n+import org.apache.avro.specific.SpecificRecord\n+\n+import org.apache.spark.sql.Encoder\n+import org.apache.spark.sql.avro.SchemaConverters._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.{GetColumnByOrdinal, UnresolvedExtractValue}\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.codegen._\n+import org.apache.spark.sql.catalyst.expressions.codegen.Block._\n+import org.apache.spark.sql.catalyst.expressions.objects.{LambdaVariable => _, _}\n+import org.apache.spark.sql.catalyst.util.{ArrayBasedMapData, GenericArrayData}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * A Spark-SQL Encoder for Avro objects\n+ */\n+object AvroEncoder {\n+  /**\n+   * Provides an Encoder for Avro objects of the given class\n+   *\n+   * @param avroClass the class of the Avro object for which to generate the Encoder\n+   * @tparam T the type of the Avro class, must implement SpecificRecord\n+   * @return an Encoder for the given Avro class\n+   */\n+  def of[T <: SpecificRecord](avroClass: Class[T]): Encoder[T] = {\n+    AvroExpressionEncoder.of(avroClass)\n+  }\n+  /**\n+   * Provides an Encoder for Avro objects implementing the given schema\n+   *\n+   * @param avroSchema the Schema of the Avro object for which to generate the Encoder\n+   * @tparam T the type of the Avro class that implements the Schema, must implement IndexedRecord\n+   * @return an Encoder for the given Avro Schema\n+   */\n+  def of[T <: IndexedRecord](avroSchema: Schema): Encoder[T] = {\n+    AvroExpressionEncoder.of(avroSchema)\n+  }\n+}\n+\n+class SerializableSchema(@transient var value: Schema) extends Externalizable {\n+  def this() = this(null)\n+  override def readExternal(in: ObjectInput): Unit = {\n+    value = new Parser().parse(in.readObject().asInstanceOf[String])\n+  }\n+  override def writeExternal(out: ObjectOutput): Unit = out.writeObject(value.toString)\n+  def resolveUnion(datum: Any): Int = GenericData.get.resolveUnion(value, datum)\n+}\n+\n+object AvroExpressionEncoder {\n+\n+  def of[T <: SpecificRecord](avroClass: Class[T]): ExpressionEncoder[T] = {\n+    val schema = avroClass.getMethod(\"getClassSchema\").invoke(null).asInstanceOf[Schema]\n+    assert(toSqlType(schema).dataType.isInstanceOf[StructType])\n+    val serializer = AvroTypeInference.serializerFor(avroClass, schema)\n+    val deserializer = AvroTypeInference.deserializerFor(schema)\n+    new ExpressionEncoder[T](\n+      serializer,\n+      deserializer,\n+      ClassTag[T](avroClass))\n+  }\n+\n+  def of[T <: IndexedRecord](schema: Schema): ExpressionEncoder[T] = {\n+    assert(toSqlType(schema).dataType.isInstanceOf[StructType])\n+    val avroClass = Option(ReflectData.get.getClass(schema))\n+      .map(_.asSubclass(classOf[SpecificRecord]))\n+      .getOrElse(classOf[GenericData.Record])\n+    val serializer = AvroTypeInference.serializerFor(avroClass, schema)\n+    val deserializer = AvroTypeInference.deserializerFor(schema)\n+    new ExpressionEncoder[T](\n+      serializer,\n+      deserializer,\n+      ClassTag[T](avroClass))\n+  }\n+}\n+\n+/**\n+ * Utilities for providing Avro object serializers and deserializers\n+ */\n+private object AvroTypeInference {\n+\n+  /**\n+   * Translates an Avro Schema type to a proper SQL DataType. The Java Objects that back data in\n+   * generated Generic and Specific records sometimes do not align with those suggested by Avro\n+   * ReflectData, so we infer the proper SQL DataType to serialize and deserialize based on\n+   * nullability and the wrapping Schema type.\n+   */\n+  private def inferExternalType(avroSchema: Schema): DataType = {\n+    toSqlType(avroSchema) match {\n+      // the non-nullable primitive types\n+      case SchemaType(BooleanType, false) => BooleanType\n+      case SchemaType(IntegerType, false) => IntegerType\n+      case SchemaType(LongType, false) =>\n+        if (avroSchema.getType == UNION) {\n+          ObjectType(classOf[java.lang.Number])\n+        } else {\n+          LongType\n+        }\n+      case SchemaType(FloatType, false) => FloatType\n+      case SchemaType(DoubleType, false) =>\n+        if (avroSchema.getType == UNION) {\n+          ObjectType(classOf[java.lang.Number])\n+        } else {\n+          DoubleType\n+        }\n+      // the nullable primitive types\n+      case SchemaType(BooleanType, true) => ObjectType(classOf[java.lang.Boolean])\n+      case SchemaType(IntegerType, true) => ObjectType(classOf[java.lang.Integer])\n+      case SchemaType(LongType, true) => ObjectType(classOf[java.lang.Long])\n+      case SchemaType(FloatType, true) => ObjectType(classOf[java.lang.Float])\n+      case SchemaType(DoubleType, true) => ObjectType(classOf[java.lang.Double])\n+      // the binary types\n+      case SchemaType(BinaryType, _) =>\n+        if (avroSchema.getType == FIXED) {\n+          Option(ReflectData.get.getClass(avroSchema))\n+            .map(ObjectType(_))\n+            .getOrElse(ObjectType(classOf[GenericData.Fixed]))\n+        } else {\n+          ObjectType(classOf[java.nio.ByteBuffer])\n+        }\n+      // the referenced types\n+      case SchemaType(ArrayType(_, _), _) =>\n+        ObjectType(classOf[java.util.List[Object]])\n+      case SchemaType(StringType, _) =>\n+        avroSchema.getType match {\n+          case ENUM =>\n+            Option(ReflectData.get.getClass(avroSchema))\n+              .map(ObjectType(_))\n+              .getOrElse(ObjectType(classOf[GenericData.EnumSymbol]))\n+          case _ =>\n+            ObjectType(classOf[CharSequence])\n+        }\n+      case SchemaType(StructType(_), _) =>\n+        Option(ReflectData.get.getClass(avroSchema))\n+          .map(ObjectType(_))\n+          .getOrElse(ObjectType(classOf[GenericData.Record]))\n+      case SchemaType(MapType(_, _, _), _) =>\n+        ObjectType(classOf[java.util.Map[Object, Object]])\n+    }\n+  }\n+\n+  /**\n+   * Returns an expression that can be used to deserialize an InternalRow to an Avro object of\n+   * type `T` that implements IndexedRecord and is compatible with the given Schema. The Spark SQL\n+   * representation is located at ordinal 0 of a row, i.e. `GetColumnByOrdinal(0, _)`. Nested\n+   * will have their fields accessed using `UnresolvedExtractValue`.\n+   */\n+  def deserializerFor[T <: IndexedRecord] (avroSchema: Schema): Expression = {\n+    deserializerFor(avroSchema, GetColumnByOrdinal(0, inferExternalType(avroSchema)))\n+  }\n+\n+  private def deserializerFor(avroSchema: Schema, path: Expression): Expression = {\n+    /** Returns the current path with a sub-field extracted. */\n+    def addToPath(part: String): Expression = UnresolvedExtractValue(path, Literal(part))\n+\n+    avroSchema.getType match {\n+      case BOOLEAN =>\n+        NewInstance(\n+          classOf[java.lang.Boolean],\n+          path :: Nil,\n+          ObjectType(classOf[java.lang.Boolean]))\n+      case INT =>\n+        NewInstance(\n+          classOf[java.lang.Integer],\n+          path :: Nil,\n+          ObjectType(classOf[java.lang.Integer]))\n+      case LONG =>\n+        NewInstance(\n+          classOf[java.lang.Long],\n+          path :: Nil,\n+          ObjectType(classOf[java.lang.Long]))\n+      case FLOAT =>\n+        NewInstance(\n+          classOf[java.lang.Float],\n+          path :: Nil,\n+          ObjectType(classOf[java.lang.Float]))\n+      case DOUBLE =>\n+        NewInstance(\n+          classOf[java.lang.Double],\n+          path :: Nil,\n+          ObjectType(classOf[java.lang.Double]))\n+\n+      case BYTES =>\n+        StaticInvoke(\n+          classOf[java.nio.ByteBuffer],\n+          ObjectType(classOf[java.nio.ByteBuffer]),\n+          \"wrap\",\n+          path :: Nil)\n+      case FIXED =>\n+        val fixedClass = Option(ReflectData.get.getClass(avroSchema))\n+          .getOrElse(classOf[GenericData.Fixed])\n+        if (fixedClass == classOf[GenericData.Fixed]) {\n+          NewInstance(\n+            fixedClass,\n+            Invoke(\n+              Literal.fromObject(\n+                new SerializableSchema(avroSchema),\n+                ObjectType(classOf[SerializableSchema])),\n+              \"value\",\n+              ObjectType(classOf[Schema]),\n+              Nil) ::\n+              path ::\n+              Nil,\n+            ObjectType(fixedClass))\n+        } else {\n+          NewInstance(\n+            fixedClass,\n+            path :: Nil,\n+            ObjectType(fixedClass))\n+        }\n+\n+      case STRING =>\n+        Invoke(path, \"toString\", ObjectType(classOf[String]))\n+\n+      case ENUM =>\n+        val enumClass = Option(ReflectData.get.getClass(avroSchema))\n+          .getOrElse(classOf[GenericData.EnumSymbol])\n+        if (enumClass == classOf[GenericData.EnumSymbol]) {\n+          NewInstance(\n+            enumClass,\n+            Invoke(\n+              Literal.fromObject(\n+                new SerializableSchema(avroSchema),\n+                ObjectType(classOf[SerializableSchema])),\n+              \"value\",\n+              ObjectType(classOf[Schema]),\n+              Nil) ::\n+              Invoke(path, \"toString\", ObjectType(classOf[String])) ::\n+              Nil,\n+            ObjectType(enumClass))\n+        } else {\n+          StaticInvoke(\n+            enumClass,\n+            ObjectType(enumClass),\n+            \"valueOf\",\n+            Invoke(path, \"toString\", ObjectType(classOf[String])) :: Nil)\n+        }\n+\n+      case ARRAY =>\n+        val elementSchema = avroSchema.getElementType\n+        val elementType = toSqlType(elementSchema).dataType\n+        val array = Invoke(\n+          MapObjects(element =>\n+            deserializerFor(elementSchema, element),\n+            path,\n+            elementType),\n+          \"array\",\n+          ObjectType(classOf[Array[Any]]))\n+        StaticInvoke(\n+          classOf[java.util.Arrays],\n+          ObjectType(classOf[java.util.List[Object]]),\n+          \"asList\",\n+          array :: Nil)\n+\n+      case MAP =>\n+        val valueSchema = avroSchema.getValueType\n+        val valueType = inferExternalType(valueSchema) match {\n+          case t if t == ObjectType(classOf[java.lang.CharSequence]) =>\n+            StringType\n+          case other => other\n+        }\n+        val keyData = Invoke(\n+          MapObjects(\n+            p => deserializerFor(Schema.create(STRING), p),\n+            Invoke(path, \"keyArray\", ArrayType(StringType)),\n+            StringType),\n+          \"array\",\n+          ObjectType(classOf[Array[Any]]))\n+        val valueData = Invoke(\n+          MapObjects(\n+            p => deserializerFor(valueSchema, p),\n+            Invoke(path, \"valueArray\", ArrayType(valueType)),\n+            valueType),\n+          \"array\",\n+          ObjectType(classOf[Array[Any]]))\n+        StaticInvoke(\n+          ArrayBasedMapData.getClass,\n+          ObjectType(classOf[JMap[_, _]]),\n+          \"toJavaMap\",\n+          keyData :: valueData :: Nil)\n+\n+      case UNION =>\n+        val (resolvedSchema, _) =\n+          org.apache.spark.sql.avro.SchemaConverters.resolveUnionType(avroSchema, Set.empty)\n+        if (resolvedSchema.getType == RECORD &&\n+          avroSchema.getTypes.asScala.filterNot(_.getType == NULL).length > 1) {\n+          // A Union resolved to a record that originally had more than 1 type when filtered\n+          // of its nulls must be complex\n+          val bottom = Literal.create(null, ObjectType(classOf[Object])).asInstanceOf[Expression]\n+          resolvedSchema.getFields.asScala.foldLeft(bottom) {\n+            (tree: Expression, field: Schema.Field) =>\n+              val fieldValue = ObjectCast(\n+                deserializerFor(field.schema, addToPath(field.name)),\n+                ObjectType(classOf[Object]))\n+              If(IsNull(fieldValue), tree, fieldValue)\n+          }\n+        } else {\n+          deserializerFor(resolvedSchema, path)\n+        }\n+\n+      case RECORD =>\n+        val args = avroSchema.getFields.asScala.map { field =>\n+          val position = Literal(field.pos)\n+          val argument = deserializerFor(field.schema, addToPath(field.name))\n+          (position, argument)\n+        }.toList\n+        val recordClass = Option(ReflectData.get.getClass(avroSchema))\n+          .getOrElse(classOf[GenericData.Record])\n+        val newInstance = if (recordClass == classOf[GenericData.Record]) {\n+          NewInstance(\n+            recordClass,\n+            Invoke(\n+              Literal.fromObject(\n+                new SerializableSchema(avroSchema),\n+                ObjectType(classOf[SerializableSchema])),\n+              \"value\",\n+              ObjectType(classOf[Schema]),\n+              Nil) :: Nil,\n+            ObjectType(recordClass))\n+        } else {\n+          NewInstance(\n+            recordClass,\n+            Nil,\n+            ObjectType(recordClass))\n+        }\n+        val result = InitializeAvroObject(newInstance, args)\n+\n+        If(IsNull(path), Literal.create(null, ObjectType(recordClass)), result)\n+\n+      case NULL =>\n+        // Encountering NULL at this level implies it was the type of a Field, which should never\n+        // be the case.\n+        throw new IncompatibleSchemaException(\"Null type should only be used in Union types\")\n+    }\n+  }\n+\n+  /**\n+   * Returns an expression that can be used to serialize an Avro object with a class of type `T`\n+   * that is compatible with the given Schema to an InternalRow\n+   */\n+  def serializerFor[T <: IndexedRecord](\n+      avroClass: Class[T], avroSchema: Schema): Expression = {\n+    val inputObject = BoundReference(0, ObjectType(avroClass), nullable = true)\n+    val nullSafeInput = AssertNotNull(inputObject, Seq(\"top level\"))\n+    serializerFor(nullSafeInput, avroSchema)\n+  }\n+\n+  def serializerFor(\n+      inputObject: Expression,\n+      avroSchema: Schema): Expression = {\n+\n+    def toCatalystArray(inputObject: Expression, schema: Schema): Expression = {\n+      val elementType = inferExternalType(schema)\n+      if (elementType.isInstanceOf[ObjectType]) {\n+        MapObjects(element =>\n+          serializerFor(element, schema),\n+          Invoke(\n+            inputObject,\n+            \"toArray\",\n+            ObjectType(classOf[Array[Object]])),\n+          elementType)\n+      } else {\n+        NewInstance(\n+          classOf[GenericArrayData],\n+          inputObject :: Nil,\n+          dataType = ArrayType(elementType, containsNull = false))\n+      }\n+    }\n+\n+    def toCatalystMap(inputObject: Expression, schema: Schema): Expression = {\n+      val valueSchema = schema.getValueType\n+      val valueType = inferExternalType(valueSchema)\n+      ExternalMapToCatalyst(\n+        inputObject,\n+        ObjectType(classOf[org.apache.avro.util.Utf8]),"
  }],
  "prId": 22878
}, {
  "comments": [{
    "author": {
      "login": "gengliangwang"
    },
    "body": "In `from_avro`, we are using avro schema in json format string, should we consider change to that?",
    "commit": "dfae1b08209c3c60beeaeb4dacacf1571047b460",
    "createdAt": "2018-11-01T13:28:53Z",
    "diffHunk": "@@ -0,0 +1,533 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.avro\n+\n+import java.io._\n+import java.util.{Map => JMap}\n+\n+import scala.collection.JavaConverters._\n+import scala.language.existentials\n+import scala.reflect.ClassTag\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.Schema.Parser\n+import org.apache.avro.Schema.Type._\n+import org.apache.avro.generic.{GenericData, IndexedRecord}\n+import org.apache.avro.reflect.ReflectData\n+import org.apache.avro.specific.SpecificRecord\n+\n+import org.apache.spark.sql.Encoder\n+import org.apache.spark.sql.avro.SchemaConverters._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.{GetColumnByOrdinal, UnresolvedExtractValue}\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.codegen._\n+import org.apache.spark.sql.catalyst.expressions.codegen.Block._\n+import org.apache.spark.sql.catalyst.expressions.objects.{LambdaVariable => _, _}\n+import org.apache.spark.sql.catalyst.util.{ArrayBasedMapData, GenericArrayData}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * A Spark-SQL Encoder for Avro objects\n+ */\n+object AvroEncoder {\n+  /**\n+   * Provides an Encoder for Avro objects of the given class\n+   *\n+   * @param avroClass the class of the Avro object for which to generate the Encoder\n+   * @tparam T the type of the Avro class, must implement SpecificRecord\n+   * @return an Encoder for the given Avro class\n+   */\n+  def of[T <: SpecificRecord](avroClass: Class[T]): Encoder[T] = {\n+    AvroExpressionEncoder.of(avroClass)\n+  }\n+  /**\n+   * Provides an Encoder for Avro objects implementing the given schema\n+   *\n+   * @param avroSchema the Schema of the Avro object for which to generate the Encoder\n+   * @tparam T the type of the Avro class that implements the Schema, must implement IndexedRecord\n+   * @return an Encoder for the given Avro Schema\n+   */\n+  def of[T <: IndexedRecord](avroSchema: Schema): Encoder[T] = {",
    "line": 68
  }, {
    "author": {
      "login": "xuanyuanking"
    },
    "body": "Make sense, maybe the pass a schema could be kept and add a new one? Done in 933695c and corresponding tests.",
    "commit": "dfae1b08209c3c60beeaeb4dacacf1571047b460",
    "createdAt": "2018-11-02T09:41:34Z",
    "diffHunk": "@@ -0,0 +1,533 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.avro\n+\n+import java.io._\n+import java.util.{Map => JMap}\n+\n+import scala.collection.JavaConverters._\n+import scala.language.existentials\n+import scala.reflect.ClassTag\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.Schema.Parser\n+import org.apache.avro.Schema.Type._\n+import org.apache.avro.generic.{GenericData, IndexedRecord}\n+import org.apache.avro.reflect.ReflectData\n+import org.apache.avro.specific.SpecificRecord\n+\n+import org.apache.spark.sql.Encoder\n+import org.apache.spark.sql.avro.SchemaConverters._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.{GetColumnByOrdinal, UnresolvedExtractValue}\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.codegen._\n+import org.apache.spark.sql.catalyst.expressions.codegen.Block._\n+import org.apache.spark.sql.catalyst.expressions.objects.{LambdaVariable => _, _}\n+import org.apache.spark.sql.catalyst.util.{ArrayBasedMapData, GenericArrayData}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * A Spark-SQL Encoder for Avro objects\n+ */\n+object AvroEncoder {\n+  /**\n+   * Provides an Encoder for Avro objects of the given class\n+   *\n+   * @param avroClass the class of the Avro object for which to generate the Encoder\n+   * @tparam T the type of the Avro class, must implement SpecificRecord\n+   * @return an Encoder for the given Avro class\n+   */\n+  def of[T <: SpecificRecord](avroClass: Class[T]): Encoder[T] = {\n+    AvroExpressionEncoder.of(avroClass)\n+  }\n+  /**\n+   * Provides an Encoder for Avro objects implementing the given schema\n+   *\n+   * @param avroSchema the Schema of the Avro object for which to generate the Encoder\n+   * @tparam T the type of the Avro class that implements the Schema, must implement IndexedRecord\n+   * @return an Encoder for the given Avro Schema\n+   */\n+  def of[T <: IndexedRecord](avroSchema: Schema): Encoder[T] = {",
    "line": 68
  }],
  "prId": 22878
}, {
  "comments": [{
    "author": {
      "login": "gengliangwang"
    },
    "body": "I try compiling with latest code, get such error `match may not be exhaustive`. Need to handle the default case, e.g. throw exceptions.",
    "commit": "dfae1b08209c3c60beeaeb4dacacf1571047b460",
    "createdAt": "2018-12-13T07:22:49Z",
    "diffHunk": "@@ -0,0 +1,547 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.avro\n+\n+import java.io._\n+import java.util.{Map => JMap}\n+\n+import scala.collection.JavaConverters._\n+import scala.language.existentials\n+import scala.reflect.ClassTag\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.Schema.Parser\n+import org.apache.avro.Schema.Type._\n+import org.apache.avro.generic.{GenericData, IndexedRecord}\n+import org.apache.avro.reflect.ReflectData\n+import org.apache.avro.specific.SpecificRecord\n+\n+import org.apache.spark.sql.Encoder\n+import org.apache.spark.sql.avro.SchemaConverters._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.{GetColumnByOrdinal, UnresolvedExtractValue}\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.codegen._\n+import org.apache.spark.sql.catalyst.expressions.codegen.Block._\n+import org.apache.spark.sql.catalyst.expressions.objects.{LambdaVariable => _, _}\n+import org.apache.spark.sql.catalyst.util.{ArrayBasedMapData, GenericArrayData}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * A Spark-SQL Encoder for Avro objects\n+ */\n+object AvroEncoder {\n+  /**\n+   * Provides an Encoder for Avro objects of the given class\n+   *\n+   * @param avroClass the class of the Avro object for which to generate the Encoder\n+   * @tparam T the type of the Avro class, must implement SpecificRecord\n+   * @return an Encoder for the given Avro class\n+   */\n+  def of[T <: SpecificRecord](avroClass: Class[T]): Encoder[T] = {\n+    AvroExpressionEncoder.of(avroClass)\n+  }\n+\n+  /**\n+   * Provides an Encoder for Avro objects implementing the given schema\n+   *\n+   * @param avroSchema the Schema of the Avro object for which to generate the Encoder\n+   * @tparam T the type of the Avro class that implements the Schema, must implement IndexedRecord\n+   * @return an Encoder for the given Avro Schema\n+   */\n+  def of[T <: IndexedRecord](avroSchema: Schema): Encoder[T] = {\n+    AvroExpressionEncoder.of(avroSchema)\n+  }\n+\n+  /**\n+   * Provides an Encoder for Avro objects implementing the given schema\n+   *\n+   * @param jsonFormatSchema the json string represented Schema of the Avro object\n+   *                         for which to generate the Encoder\n+   * @tparam T the type of the Avro class that implements the Schema, must implement IndexedRecord\n+   * @return an Encoder for the given Avro Schema\n+   */\n+  def of[T <: IndexedRecord](jsonFormatSchema: String): Encoder[T] = {\n+    val avroSchema = new Schema.Parser().parse(jsonFormatSchema)\n+    AvroExpressionEncoder.of(avroSchema)\n+  }\n+}\n+\n+class SerializableSchema(@transient var value: Schema) extends Externalizable {\n+  def this() = this(null)\n+  override def readExternal(in: ObjectInput): Unit = {\n+    value = new Parser().parse(in.readObject().asInstanceOf[String])\n+  }\n+  override def writeExternal(out: ObjectOutput): Unit = out.writeObject(value.toString)\n+  def resolveUnion(datum: Any): Int = GenericData.get.resolveUnion(value, datum)\n+}\n+\n+object AvroExpressionEncoder {\n+\n+  def of[T <: SpecificRecord](avroClass: Class[T]): ExpressionEncoder[T] = {\n+    val schema = avroClass.getMethod(\"getClassSchema\").invoke(null).asInstanceOf[Schema]\n+    assert(toSqlType(schema).dataType.isInstanceOf[StructType])\n+    val serializer = AvroTypeInference.serializerFor(avroClass, schema)\n+    val deserializer = AvroTypeInference.deserializerFor(schema)\n+    new ExpressionEncoder[T](\n+      serializer,\n+      deserializer,\n+      ClassTag[T](avroClass))\n+  }\n+\n+  def of[T <: IndexedRecord](schema: Schema): ExpressionEncoder[T] = {\n+    assert(toSqlType(schema).dataType.isInstanceOf[StructType])\n+    val avroClass = Option(ReflectData.get.getClass(schema))\n+      .map(_.asSubclass(classOf[SpecificRecord]))\n+      .getOrElse(classOf[GenericData.Record])\n+    val serializer = AvroTypeInference.serializerFor(avroClass, schema)\n+    val deserializer = AvroTypeInference.deserializerFor(schema)\n+    new ExpressionEncoder[T](\n+      serializer,\n+      deserializer,\n+      ClassTag[T](avroClass))\n+  }\n+}\n+\n+/**\n+ * Utilities for providing Avro object serializers and deserializers\n+ */\n+private object AvroTypeInference {\n+\n+  /**\n+   * Translates an Avro Schema type to a proper SQL DataType. The Java Objects that back data in\n+   * generated Generic and Specific records sometimes do not align with those suggested by Avro\n+   * ReflectData, so we infer the proper SQL DataType to serialize and deserialize based on\n+   * nullability and the wrapping Schema type.\n+   */\n+  private def inferExternalType(avroSchema: Schema): DataType = {\n+    toSqlType(avroSchema) match {",
    "line": 134
  }, {
    "author": {
      "login": "xuanyuanking"
    },
    "body": "Yep, has done in c35a40d.",
    "commit": "dfae1b08209c3c60beeaeb4dacacf1571047b460",
    "createdAt": "2019-04-11T08:59:01Z",
    "diffHunk": "@@ -0,0 +1,547 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.avro\n+\n+import java.io._\n+import java.util.{Map => JMap}\n+\n+import scala.collection.JavaConverters._\n+import scala.language.existentials\n+import scala.reflect.ClassTag\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.Schema.Parser\n+import org.apache.avro.Schema.Type._\n+import org.apache.avro.generic.{GenericData, IndexedRecord}\n+import org.apache.avro.reflect.ReflectData\n+import org.apache.avro.specific.SpecificRecord\n+\n+import org.apache.spark.sql.Encoder\n+import org.apache.spark.sql.avro.SchemaConverters._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.{GetColumnByOrdinal, UnresolvedExtractValue}\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.codegen._\n+import org.apache.spark.sql.catalyst.expressions.codegen.Block._\n+import org.apache.spark.sql.catalyst.expressions.objects.{LambdaVariable => _, _}\n+import org.apache.spark.sql.catalyst.util.{ArrayBasedMapData, GenericArrayData}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * A Spark-SQL Encoder for Avro objects\n+ */\n+object AvroEncoder {\n+  /**\n+   * Provides an Encoder for Avro objects of the given class\n+   *\n+   * @param avroClass the class of the Avro object for which to generate the Encoder\n+   * @tparam T the type of the Avro class, must implement SpecificRecord\n+   * @return an Encoder for the given Avro class\n+   */\n+  def of[T <: SpecificRecord](avroClass: Class[T]): Encoder[T] = {\n+    AvroExpressionEncoder.of(avroClass)\n+  }\n+\n+  /**\n+   * Provides an Encoder for Avro objects implementing the given schema\n+   *\n+   * @param avroSchema the Schema of the Avro object for which to generate the Encoder\n+   * @tparam T the type of the Avro class that implements the Schema, must implement IndexedRecord\n+   * @return an Encoder for the given Avro Schema\n+   */\n+  def of[T <: IndexedRecord](avroSchema: Schema): Encoder[T] = {\n+    AvroExpressionEncoder.of(avroSchema)\n+  }\n+\n+  /**\n+   * Provides an Encoder for Avro objects implementing the given schema\n+   *\n+   * @param jsonFormatSchema the json string represented Schema of the Avro object\n+   *                         for which to generate the Encoder\n+   * @tparam T the type of the Avro class that implements the Schema, must implement IndexedRecord\n+   * @return an Encoder for the given Avro Schema\n+   */\n+  def of[T <: IndexedRecord](jsonFormatSchema: String): Encoder[T] = {\n+    val avroSchema = new Schema.Parser().parse(jsonFormatSchema)\n+    AvroExpressionEncoder.of(avroSchema)\n+  }\n+}\n+\n+class SerializableSchema(@transient var value: Schema) extends Externalizable {\n+  def this() = this(null)\n+  override def readExternal(in: ObjectInput): Unit = {\n+    value = new Parser().parse(in.readObject().asInstanceOf[String])\n+  }\n+  override def writeExternal(out: ObjectOutput): Unit = out.writeObject(value.toString)\n+  def resolveUnion(datum: Any): Int = GenericData.get.resolveUnion(value, datum)\n+}\n+\n+object AvroExpressionEncoder {\n+\n+  def of[T <: SpecificRecord](avroClass: Class[T]): ExpressionEncoder[T] = {\n+    val schema = avroClass.getMethod(\"getClassSchema\").invoke(null).asInstanceOf[Schema]\n+    assert(toSqlType(schema).dataType.isInstanceOf[StructType])\n+    val serializer = AvroTypeInference.serializerFor(avroClass, schema)\n+    val deserializer = AvroTypeInference.deserializerFor(schema)\n+    new ExpressionEncoder[T](\n+      serializer,\n+      deserializer,\n+      ClassTag[T](avroClass))\n+  }\n+\n+  def of[T <: IndexedRecord](schema: Schema): ExpressionEncoder[T] = {\n+    assert(toSqlType(schema).dataType.isInstanceOf[StructType])\n+    val avroClass = Option(ReflectData.get.getClass(schema))\n+      .map(_.asSubclass(classOf[SpecificRecord]))\n+      .getOrElse(classOf[GenericData.Record])\n+    val serializer = AvroTypeInference.serializerFor(avroClass, schema)\n+    val deserializer = AvroTypeInference.deserializerFor(schema)\n+    new ExpressionEncoder[T](\n+      serializer,\n+      deserializer,\n+      ClassTag[T](avroClass))\n+  }\n+}\n+\n+/**\n+ * Utilities for providing Avro object serializers and deserializers\n+ */\n+private object AvroTypeInference {\n+\n+  /**\n+   * Translates an Avro Schema type to a proper SQL DataType. The Java Objects that back data in\n+   * generated Generic and Specific records sometimes do not align with those suggested by Avro\n+   * ReflectData, so we infer the proper SQL DataType to serialize and deserialize based on\n+   * nullability and the wrapping Schema type.\n+   */\n+  private def inferExternalType(avroSchema: Schema): DataType = {\n+    toSqlType(avroSchema) match {",
    "line": 134
  }],
  "prId": 22878
}, {
  "comments": [{
    "author": {
      "login": "kunkun-tang"
    },
    "body": "When avroSchema is a Union{null, Fixed}, it results in a java.io.ByteBuffer. Have we considered this case?",
    "commit": "dfae1b08209c3c60beeaeb4dacacf1571047b460",
    "createdAt": "2019-02-22T21:26:31Z",
    "diffHunk": "@@ -0,0 +1,549 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.avro\n+\n+import java.io._\n+import java.util.{Map => JMap}\n+\n+import scala.collection.JavaConverters._\n+import scala.language.existentials\n+import scala.reflect.ClassTag\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.Schema.Parser\n+import org.apache.avro.Schema.Type._\n+import org.apache.avro.generic.{GenericData, IndexedRecord}\n+import org.apache.avro.reflect.ReflectData\n+import org.apache.avro.specific.SpecificRecord\n+\n+import org.apache.spark.sql.Encoder\n+import org.apache.spark.sql.avro.SchemaConverters._\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.analysis.{GetColumnByOrdinal, UnresolvedExtractValue}\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.codegen._\n+import org.apache.spark.sql.catalyst.expressions.codegen.Block._\n+import org.apache.spark.sql.catalyst.expressions.objects.{LambdaVariable => _, _}\n+import org.apache.spark.sql.catalyst.util.{ArrayBasedMapData, GenericArrayData}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+/**\n+ * A Spark-SQL Encoder for Avro objects\n+ */\n+object AvroEncoder {\n+  /**\n+   * Provides an Encoder for Avro objects of the given class\n+   *\n+   * @param avroClass the class of the Avro object for which to generate the Encoder\n+   * @tparam T the type of the Avro class, must implement SpecificRecord\n+   * @return an Encoder for the given Avro class\n+   */\n+  def of[T <: SpecificRecord](avroClass: Class[T]): Encoder[T] = {\n+    AvroExpressionEncoder.of(avroClass)\n+  }\n+\n+  /**\n+   * Provides an Encoder for Avro objects implementing the given schema\n+   *\n+   * @param avroSchema the Schema of the Avro object for which to generate the Encoder\n+   * @tparam T the type of the Avro class that implements the Schema, must implement IndexedRecord\n+   * @return an Encoder for the given Avro Schema\n+   */\n+  def of[T <: IndexedRecord](avroSchema: Schema): Encoder[T] = {\n+    AvroExpressionEncoder.of(avroSchema)\n+  }\n+\n+  /**\n+   * Provides an Encoder for Avro objects implementing the given schema\n+   *\n+   * @param jsonFormatSchema the json string represented Schema of the Avro object\n+   *                         for which to generate the Encoder\n+   * @tparam T the type of the Avro class that implements the Schema, must implement IndexedRecord\n+   * @return an Encoder for the given Avro Schema\n+   */\n+  def of[T <: IndexedRecord](jsonFormatSchema: String): Encoder[T] = {\n+    val avroSchema = new Schema.Parser().parse(jsonFormatSchema)\n+    AvroExpressionEncoder.of(avroSchema)\n+  }\n+}\n+\n+class SerializableSchema(@transient var value: Schema) extends Externalizable {\n+  def this() = this(null)\n+  override def readExternal(in: ObjectInput): Unit = {\n+    value = new Parser().parse(in.readObject().asInstanceOf[String])\n+  }\n+  override def writeExternal(out: ObjectOutput): Unit = out.writeObject(value.toString)\n+  def resolveUnion(datum: Any): Int = GenericData.get.resolveUnion(value, datum)\n+}\n+\n+object AvroExpressionEncoder {\n+\n+  def of[T <: SpecificRecord](avroClass: Class[T]): ExpressionEncoder[T] = {\n+    val schema = avroClass.getMethod(\"getClassSchema\").invoke(null).asInstanceOf[Schema]\n+    assert(toSqlType(schema).dataType.isInstanceOf[StructType])\n+    val serializer = AvroTypeInference.serializerFor(avroClass, schema)\n+    val deserializer = AvroTypeInference.deserializerFor(schema)\n+    new ExpressionEncoder[T](\n+      serializer,\n+      deserializer,\n+      ClassTag[T](avroClass))\n+  }\n+\n+  def of[T <: IndexedRecord](schema: Schema): ExpressionEncoder[T] = {\n+    assert(toSqlType(schema).dataType.isInstanceOf[StructType])\n+    val avroClass = Option(ReflectData.get.getClass(schema))\n+      .map(_.asSubclass(classOf[SpecificRecord]))\n+      .getOrElse(classOf[GenericData.Record])\n+    val serializer = AvroTypeInference.serializerFor(avroClass, schema)\n+    val deserializer = AvroTypeInference.deserializerFor(schema)\n+    new ExpressionEncoder[T](\n+      serializer,\n+      deserializer,\n+      ClassTag[T](avroClass))\n+  }\n+}\n+\n+/**\n+ * Utilities for providing Avro object serializers and deserializers\n+ */\n+private object AvroTypeInference {\n+\n+  /**\n+   * Translates an Avro Schema type to a proper SQL DataType. The Java Objects that back data in\n+   * generated Generic and Specific records sometimes do not align with those suggested by Avro\n+   * ReflectData, so we infer the proper SQL DataType to serialize and deserialize based on\n+   * nullability and the wrapping Schema type.\n+   */\n+  private def inferExternalType(avroSchema: Schema): DataType = {\n+    toSqlType(avroSchema) match {\n+      // the non-nullable primitive types\n+      case SchemaType(BooleanType, false) => BooleanType\n+      case SchemaType(IntegerType, false) => IntegerType\n+      case SchemaType(LongType, false) =>\n+        if (avroSchema.getType == UNION) {\n+          ObjectType(classOf[java.lang.Number])\n+        } else {\n+          LongType\n+        }\n+      case SchemaType(FloatType, false) => FloatType\n+      case SchemaType(DoubleType, false) =>\n+        if (avroSchema.getType == UNION) {\n+          ObjectType(classOf[java.lang.Number])\n+        } else {\n+          DoubleType\n+        }\n+      // the nullable primitive types\n+      case SchemaType(BooleanType, true) => ObjectType(classOf[java.lang.Boolean])\n+      case SchemaType(IntegerType, true) => ObjectType(classOf[java.lang.Integer])\n+      case SchemaType(LongType, true) => ObjectType(classOf[java.lang.Long])\n+      case SchemaType(FloatType, true) => ObjectType(classOf[java.lang.Float])\n+      case SchemaType(DoubleType, true) => ObjectType(classOf[java.lang.Double])\n+      // the binary types\n+      case SchemaType(BinaryType, _) =>",
    "line": 158
  }],
  "prId": 22878
}]