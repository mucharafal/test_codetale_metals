[{
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Can we don't expose this as an option for now?  IIUC, this compression level only applies to deflate, right? Also, this option looks not for keeping the same options from the thrid party as well.",
    "commit": "5f83902e2876745f8be245681e7cb41d69421778",
    "createdAt": "2018-07-24T01:39:58Z",
    "diffHunk": "@@ -68,4 +70,25 @@ class AvroOptions(\n       .map(_.toBoolean)\n       .getOrElse(!ignoreFilesWithoutExtension)\n   }\n+\n+  /**\n+   * The `compression` option allows to specify a compression codec used in write.\n+   * Currently supported codecs are `uncompressed`, `snappy` and `deflate`.\n+   * If the option is not set, the `snappy` compression is used by default.\n+   */\n+  val compression: String = parameters.get(\"compression\").getOrElse(sqlConf.avroCompressionCodec)\n+\n+\n+  /**\n+   * Level of compression in the range of 1..9 inclusive. 1 - for fast, 9 - for best compression.\n+   * If the compression level is not set for `deflate` compression, the current value of SQL\n+   * config `spark.sql.avro.deflate.level` is used by default. For other compressions, the default\n+   * value is `6`.\n+   */\n+  val compressionLevel: Int = {"
  }, {
    "author": {
      "login": "MaxGekk"
    },
    "body": "I added the option keeping in mind other compression codecs can be added in the future, for example zstandard. For those codecs, the level could be useful too. Another point is specifying compression level together with compression codec in Avro options looks more natural comparing to SQL global settings:\r\n\r\n```\r\ndf.write\r\n  .options(Map(\"compression\" -> \"deflate\", \"compressionLevel\" -> \"9\"))\r\n  .format(\"avro\")\r\n  .save(deflateDir)\r\n```\r\nvs\r\n```\r\nspark.conf.set(\"spark.sql.avro.deflate.level\", \"9\")\r\ndf.write\r\n  .option(\"compression\", \"deflate\"))\r\n  .format(\"avro\")\r\n  .save(deflateDir)\r\n```",
    "commit": "5f83902e2876745f8be245681e7cb41d69421778",
    "createdAt": "2018-07-24T13:37:46Z",
    "diffHunk": "@@ -68,4 +70,25 @@ class AvroOptions(\n       .map(_.toBoolean)\n       .getOrElse(!ignoreFilesWithoutExtension)\n   }\n+\n+  /**\n+   * The `compression` option allows to specify a compression codec used in write.\n+   * Currently supported codecs are `uncompressed`, `snappy` and `deflate`.\n+   * If the option is not set, the `snappy` compression is used by default.\n+   */\n+  val compression: String = parameters.get(\"compression\").getOrElse(sqlConf.avroCompressionCodec)\n+\n+\n+  /**\n+   * Level of compression in the range of 1..9 inclusive. 1 - for fast, 9 - for best compression.\n+   * If the compression level is not set for `deflate` compression, the current value of SQL\n+   * config `spark.sql.avro.deflate.level` is used by default. For other compressions, the default\n+   * value is `6`.\n+   */\n+  val compressionLevel: Int = {"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Yea, I know that could be useful in some ways but I was thinking we should better not add this just for now. Thing is, it sounds currently too specific to one compression option in Avro for now .. There are many options to expose in, for example in CSV datasource too in this way .. ",
    "commit": "5f83902e2876745f8be245681e7cb41d69421778",
    "createdAt": "2018-07-24T13:49:39Z",
    "diffHunk": "@@ -68,4 +70,25 @@ class AvroOptions(\n       .map(_.toBoolean)\n       .getOrElse(!ignoreFilesWithoutExtension)\n   }\n+\n+  /**\n+   * The `compression` option allows to specify a compression codec used in write.\n+   * Currently supported codecs are `uncompressed`, `snappy` and `deflate`.\n+   * If the option is not set, the `snappy` compression is used by default.\n+   */\n+  val compression: String = parameters.get(\"compression\").getOrElse(sqlConf.avroCompressionCodec)\n+\n+\n+  /**\n+   * Level of compression in the range of 1..9 inclusive. 1 - for fast, 9 - for best compression.\n+   * If the compression level is not set for `deflate` compression, the current value of SQL\n+   * config `spark.sql.avro.deflate.level` is used by default. For other compressions, the default\n+   * value is `6`.\n+   */\n+  val compressionLevel: Int = {"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Also, to be honest, I wonder users would want to change compression level often ..",
    "commit": "5f83902e2876745f8be245681e7cb41d69421778",
    "createdAt": "2018-07-24T13:51:00Z",
    "diffHunk": "@@ -68,4 +70,25 @@ class AvroOptions(\n       .map(_.toBoolean)\n       .getOrElse(!ignoreFilesWithoutExtension)\n   }\n+\n+  /**\n+   * The `compression` option allows to specify a compression codec used in write.\n+   * Currently supported codecs are `uncompressed`, `snappy` and `deflate`.\n+   * If the option is not set, the `snappy` compression is used by default.\n+   */\n+  val compression: String = parameters.get(\"compression\").getOrElse(sqlConf.avroCompressionCodec)\n+\n+\n+  /**\n+   * Level of compression in the range of 1..9 inclusive. 1 - for fast, 9 - for best compression.\n+   * If the compression level is not set for `deflate` compression, the current value of SQL\n+   * config `spark.sql.avro.deflate.level` is used by default. For other compressions, the default\n+   * value is `6`.\n+   */\n+  val compressionLevel: Int = {"
  }],
  "prId": 21837
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "We may just get SQLConf by calling `SQLConf.get` without passing it in.",
    "commit": "5f83902e2876745f8be245681e7cb41d69421778",
    "createdAt": "2018-07-27T01:41:18Z",
    "diffHunk": "@@ -21,16 +21,18 @@ import org.apache.hadoop.conf.Configuration\n \n import org.apache.spark.internal.Logging\n import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n+import org.apache.spark.sql.internal.SQLConf\n \n /**\n  * Options for Avro Reader and Writer stored in case insensitive manner.\n  */\n class AvroOptions(\n     @transient val parameters: CaseInsensitiveMap[String],\n-    @transient val conf: Configuration) extends Logging with Serializable {\n+    @transient val conf: Configuration,\n+    @transient val sqlConf: SQLConf) extends Logging with Serializable {"
  }],
  "prId": 21837
}]