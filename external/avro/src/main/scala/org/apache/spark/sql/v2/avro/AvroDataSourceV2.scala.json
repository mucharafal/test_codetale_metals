[{
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "Why is this not identical to the metadata passed to `createTable`? Is there a reason not to pass the table properties as well as the read options?",
    "commit": "cfbe0a75f80e88d4a5831785d05fb9b708c5ada3",
    "createdAt": "2019-09-03T23:22:43Z",
    "diffHunk": "@@ -35,7 +36,10 @@ class AvroDataSourceV2 extends FileDataSourceV2 {\n     AvroTable(tableName, sparkSession, options, paths, None, fallbackFileFormat)\n   }\n \n-  override def getTable(options: CaseInsensitiveStringMap, schema: StructType): Table = {\n+  override def getTable(\n+      options: CaseInsensitiveStringMap,\n+      schema: StructType,\n+      partitions: Array[Transform]): Table = {"
  }, {
    "author": {
      "login": "brkyvz"
    },
    "body": "I agree, this should look similar to `createTable`",
    "commit": "cfbe0a75f80e88d4a5831785d05fb9b708c5ada3",
    "createdAt": "2019-09-06T19:58:15Z",
    "diffHunk": "@@ -35,7 +36,10 @@ class AvroDataSourceV2 extends FileDataSourceV2 {\n     AvroTable(tableName, sparkSession, options, paths, None, fallbackFileFormat)\n   }\n \n-  override def getTable(options: CaseInsensitiveStringMap, schema: StructType): Table = {\n+  override def getTable(\n+      options: CaseInsensitiveStringMap,\n+      schema: StructType,\n+      partitions: Array[Transform]): Table = {"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "read options should be passed in `Table.newScanBuilder`. The `options` here is the table properties.",
    "commit": "cfbe0a75f80e88d4a5831785d05fb9b708c5ada3",
    "createdAt": "2019-09-12T14:50:51Z",
    "diffHunk": "@@ -35,7 +36,10 @@ class AvroDataSourceV2 extends FileDataSourceV2 {\n     AvroTable(tableName, sparkSession, options, paths, None, fallbackFileFormat)\n   }\n \n-  override def getTable(options: CaseInsensitiveStringMap, schema: StructType): Table = {\n+  override def getTable(\n+      options: CaseInsensitiveStringMap,\n+      schema: StructType,\n+      partitions: Array[Transform]): Table = {"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "But we do have a problem here. Table properties are case sensitive while scan options are case insensitive.\r\n\r\nThink about 2 cases:\r\n1. `spark.read.format(\"myFormat\").options(...).schema(...).load()`.\r\nWe need to get the table with the user-specifed options and schema. When scan the table, we need to use the user-specified options as scan options. The problem is, `DataFrameReader.options` specifies both table properties and scan options in this case.\r\n2. `CREATE TABLE t USING myFormat TABLEPROP ...` and then `spark.read.options(...).table(\"t\")`\r\nIn this case, `DataFrameReader.options` only specifies scan options.\r\n\r\nIdeally, `TableProvider.getTable` takes table properties which should be case sensitive. However, `DataFrameReader.options` also specifies scan options which should be case insensitive.\r\n\r\nI don't have a good idea now. Maybe it's OK to treat this as a special table which accepts case insensitive table properties.",
    "commit": "cfbe0a75f80e88d4a5831785d05fb9b708c5ada3",
    "createdAt": "2019-09-12T15:31:53Z",
    "diffHunk": "@@ -35,7 +36,10 @@ class AvroDataSourceV2 extends FileDataSourceV2 {\n     AvroTable(tableName, sparkSession, options, paths, None, fallbackFileFormat)\n   }\n \n-  override def getTable(options: CaseInsensitiveStringMap, schema: StructType): Table = {\n+  override def getTable(\n+      options: CaseInsensitiveStringMap,\n+      schema: StructType,\n+      partitions: Array[Transform]): Table = {"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "Or we can make table properties case insensitive.",
    "commit": "cfbe0a75f80e88d4a5831785d05fb9b708c5ada3",
    "createdAt": "2019-09-12T15:32:36Z",
    "diffHunk": "@@ -35,7 +36,10 @@ class AvroDataSourceV2 extends FileDataSourceV2 {\n     AvroTable(tableName, sparkSession, options, paths, None, fallbackFileFormat)\n   }\n \n-  override def getTable(options: CaseInsensitiveStringMap, schema: StructType): Table = {\n+  override def getTable(\n+      options: CaseInsensitiveStringMap,\n+      schema: StructType,\n+      partitions: Array[Transform]): Table = {"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "This interface should pass the table properties. There is no need to pass read or write options at this point, unless they can't be separated from table properties (as in the `DataFrameReader` case). The read options and write options should be passed to the logical plan -- this is added in #25681: https://github.com/apache/spark/pull/25681/files#diff-94fbd986b04087223f53697d4b6cab24R275\r\n\r\nI propose passing table properties as a string map (java.util) through this interface. When the properties come from the metastore, then this is fine. When the properties come from `DataFrameReader.option` (or the write equivalent) then the original case sensitive map should be passed. Then the read options should additionally be passed to the correct plan node so that the physical plan can push them into the scan or the write.",
    "commit": "cfbe0a75f80e88d4a5831785d05fb9b708c5ada3",
    "createdAt": "2019-09-13T22:30:42Z",
    "diffHunk": "@@ -35,7 +36,10 @@ class AvroDataSourceV2 extends FileDataSourceV2 {\n     AvroTable(tableName, sparkSession, options, paths, None, fallbackFileFormat)\n   }\n \n-  override def getTable(options: CaseInsensitiveStringMap, schema: StructType): Table = {\n+  override def getTable(\n+      options: CaseInsensitiveStringMap,\n+      schema: StructType,\n+      partitions: Array[Transform]): Table = {"
  }],
  "prId": 25651
}]