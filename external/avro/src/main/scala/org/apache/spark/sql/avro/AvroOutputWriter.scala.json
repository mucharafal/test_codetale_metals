[{
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "This will be probably new to Spark 2.4.0. Do we need this?",
    "commit": "3979bad51df55ffb24dcb28cf823c5c68b407337",
    "createdAt": "2018-07-11T02:57:06Z",
    "diffHunk": "@@ -0,0 +1,169 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.avro\n+\n+import java.io.{IOException, OutputStream}\n+import java.nio.ByteBuffer\n+import java.sql.{Date, Timestamp}\n+import java.util.HashMap\n+\n+import scala.collection.immutable.Map\n+\n+import org.apache.avro.{Schema, SchemaBuilder}\n+import org.apache.avro.generic.GenericData.Record\n+import org.apache.avro.generic.GenericRecord\n+import org.apache.avro.mapred.AvroKey\n+import org.apache.avro.mapreduce.AvroKeyOutputFormat\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.io.NullWritable\n+import org.apache.hadoop.mapreduce.{RecordWriter, TaskAttemptContext}\n+\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.execution.datasources.OutputWriter\n+import org.apache.spark.sql.types._\n+\n+// NOTE: This class is instantiated and used on executor side only, no need to be serializable.\n+private[avro] class AvroOutputWriter(\n+    path: String,\n+    context: TaskAttemptContext,\n+    schema: StructType,\n+    recordName: String,\n+    recordNamespace: String) extends OutputWriter {\n+\n+  private lazy val converter = createConverterToAvro(schema, recordName, recordNamespace)\n+  // copy of the old conversion logic after api change in SPARK-19085\n+  private lazy val internalRowConverter =\n+    CatalystTypeConverters.createToScalaConverter(schema).asInstanceOf[InternalRow => Row]\n+\n+  /**\n+   * Overrides the couple of methods responsible for generating the output streams / files so\n+   * that the data can be correctly partitioned\n+   */\n+  private val recordWriter: RecordWriter[AvroKey[GenericRecord], NullWritable] =\n+    new AvroKeyOutputFormat[GenericRecord]() {\n+\n+      override def getDefaultWorkFile(context: TaskAttemptContext, extension: String): Path = {\n+        new Path(path)\n+      }\n+\n+      @throws(classOf[IOException])\n+      override def getAvroFileOutputStream(c: TaskAttemptContext): OutputStream = {\n+        val path = getDefaultWorkFile(context, \".avro\")\n+        path.getFileSystem(context.getConfiguration).create(path)\n+      }\n+\n+    }.getRecordWriter(context)\n+\n+  // this is the new api in spark 2.2+\n+  def write(row: InternalRow): Unit = {\n+    write(internalRowConverter(row))\n+  }\n+\n+  // api in spark 2.0 - 2.1"
  }],
  "prId": 21742
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "nit: add `overwrite`",
    "commit": "3979bad51df55ffb24dcb28cf823c5c68b407337",
    "createdAt": "2018-07-12T15:02:37Z",
    "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.avro\n+\n+import java.io.{IOException, OutputStream}\n+import java.nio.ByteBuffer\n+import java.sql.{Date, Timestamp}\n+import java.util.HashMap\n+\n+import scala.collection.immutable.Map\n+\n+import org.apache.avro.{Schema, SchemaBuilder}\n+import org.apache.avro.generic.GenericData.Record\n+import org.apache.avro.generic.GenericRecord\n+import org.apache.avro.mapred.AvroKey\n+import org.apache.avro.mapreduce.AvroKeyOutputFormat\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.io.NullWritable\n+import org.apache.hadoop.mapreduce.{RecordWriter, TaskAttemptContext}\n+\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}\n+import org.apache.spark.sql.execution.datasources.OutputWriter\n+import org.apache.spark.sql.types._\n+\n+// NOTE: This class is instantiated and used on executor side only, no need to be serializable.\n+private[avro] class AvroOutputWriter(\n+    path: String,\n+    context: TaskAttemptContext,\n+    schema: StructType,\n+    recordName: String,\n+    recordNamespace: String) extends OutputWriter {\n+\n+  private lazy val converter = createConverterToAvro(schema, recordName, recordNamespace)\n+  // copy of the old conversion logic after api change in SPARK-19085\n+  private lazy val internalRowConverter =\n+    CatalystTypeConverters.createToScalaConverter(schema).asInstanceOf[InternalRow => Row]\n+\n+  /**\n+   * Overrides the couple of methods responsible for generating the output streams / files so\n+   * that the data can be correctly partitioned\n+   */\n+  private val recordWriter: RecordWriter[AvroKey[GenericRecord], NullWritable] =\n+    new AvroKeyOutputFormat[GenericRecord]() {\n+\n+      override def getDefaultWorkFile(context: TaskAttemptContext, extension: String): Path = {\n+        new Path(path)\n+      }\n+\n+      @throws(classOf[IOException])\n+      override def getAvroFileOutputStream(c: TaskAttemptContext): OutputStream = {\n+        val path = getDefaultWorkFile(context, \".avro\")\n+        path.getFileSystem(context.getConfiguration).create(path)\n+      }\n+\n+    }.getRecordWriter(context)\n+\n+  def write(internalRow: InternalRow): Unit = {"
  }],
  "prId": 21742
}]