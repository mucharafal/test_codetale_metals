[{
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Previously, this was the following (`sharedState.sparkContext.hadoopConfiguration` + `SQLConf`). Is `job.getConfiguration` enough for `Avro`?\r\n```scala\r\nval parsedOptions = new AvroOptions(options, spark.sessionState.newHadoopConf())\r\n```",
    "commit": "fc98bd53130f67a4ea140d70a393c0ba4ba9c83e",
    "createdAt": "2019-07-05T02:27:27Z",
    "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.avro\n+\n+import java.io.{FileNotFoundException, IOException}\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.file.DataFileConstants.{BZIP2_CODEC, DEFLATE_CODEC, SNAPPY_CODEC, XZ_CODEC}\n+import org.apache.avro.file.DataFileReader\n+import org.apache.avro.generic.{GenericDatumReader, GenericRecord}\n+import org.apache.avro.mapred.{AvroOutputFormat, FsInput}\n+import org.apache.avro.mapreduce.AvroJob\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.FileStatus\n+import org.apache.hadoop.mapreduce.Job\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.execution.datasources.OutputWriterFactory\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+object AvroUtils extends Logging {\n+  def inferSchema(\n+      spark: SparkSession,\n+      options: Map[String, String],\n+      files: Seq[FileStatus]): Option[StructType] = {\n+    val conf = spark.sessionState.newHadoopConf()\n+    if (options.contains(\"ignoreExtension\")) {\n+      logWarning(s\"Option ${AvroOptions.ignoreExtensionKey} is deprecated. Please use the \" +\n+        \"general data source option pathGlobFilter for filtering file names.\")\n+    }\n+    val parsedOptions = new AvroOptions(options, conf)\n+\n+    // User can specify an optional avro json schema.\n+    val avroSchema = parsedOptions.schema\n+      .map(new Schema.Parser().parse)\n+      .getOrElse {\n+        inferAvroSchemaFromFiles(files, conf, parsedOptions.ignoreExtension,\n+          spark.sessionState.conf.ignoreCorruptFiles)\n+      }\n+\n+    SchemaConverters.toSqlType(avroSchema).dataType match {\n+      case t: StructType => Some(t)\n+      case _ => throw new RuntimeException(\n+        s\"\"\"Avro schema cannot be converted to a Spark SQL StructType:\n+           |\n+           |${avroSchema.toString(true)}\n+           |\"\"\".stripMargin)\n+    }\n+  }\n+\n+  def supportsDataType(dataType: DataType): Boolean = dataType match {\n+    case _: AtomicType => true\n+\n+    case st: StructType => st.forall { f => supportsDataType(f.dataType) }\n+\n+    case ArrayType(elementType, _) => supportsDataType(elementType)\n+\n+    case MapType(keyType, valueType, _) =>\n+      supportsDataType(keyType) && supportsDataType(valueType)\n+\n+    case udt: UserDefinedType[_] => supportsDataType(udt.sqlType)\n+\n+    case _: NullType => true\n+\n+    case _ => false\n+  }\n+\n+  def prepareWrite(\n+      sqlConf: SQLConf,\n+      job: Job,\n+      options: Map[String, String],\n+      dataSchema: StructType): OutputWriterFactory = {\n+    val parsedOptions = new AvroOptions(options, job.getConfiguration)",
    "line": 91
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "Yes, it is enough. Orc/Parquet also use the configuration from `job`.",
    "commit": "fc98bd53130f67a4ea140d70a393c0ba4ba9c83e",
    "createdAt": "2019-07-05T06:22:10Z",
    "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.sql.avro\n+\n+import java.io.{FileNotFoundException, IOException}\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.file.DataFileConstants.{BZIP2_CODEC, DEFLATE_CODEC, SNAPPY_CODEC, XZ_CODEC}\n+import org.apache.avro.file.DataFileReader\n+import org.apache.avro.generic.{GenericDatumReader, GenericRecord}\n+import org.apache.avro.mapred.{AvroOutputFormat, FsInput}\n+import org.apache.avro.mapreduce.AvroJob\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.FileStatus\n+import org.apache.hadoop.mapreduce.Job\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.execution.datasources.OutputWriterFactory\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+import org.apache.spark.util.Utils\n+\n+object AvroUtils extends Logging {\n+  def inferSchema(\n+      spark: SparkSession,\n+      options: Map[String, String],\n+      files: Seq[FileStatus]): Option[StructType] = {\n+    val conf = spark.sessionState.newHadoopConf()\n+    if (options.contains(\"ignoreExtension\")) {\n+      logWarning(s\"Option ${AvroOptions.ignoreExtensionKey} is deprecated. Please use the \" +\n+        \"general data source option pathGlobFilter for filtering file names.\")\n+    }\n+    val parsedOptions = new AvroOptions(options, conf)\n+\n+    // User can specify an optional avro json schema.\n+    val avroSchema = parsedOptions.schema\n+      .map(new Schema.Parser().parse)\n+      .getOrElse {\n+        inferAvroSchemaFromFiles(files, conf, parsedOptions.ignoreExtension,\n+          spark.sessionState.conf.ignoreCorruptFiles)\n+      }\n+\n+    SchemaConverters.toSqlType(avroSchema).dataType match {\n+      case t: StructType => Some(t)\n+      case _ => throw new RuntimeException(\n+        s\"\"\"Avro schema cannot be converted to a Spark SQL StructType:\n+           |\n+           |${avroSchema.toString(true)}\n+           |\"\"\".stripMargin)\n+    }\n+  }\n+\n+  def supportsDataType(dataType: DataType): Boolean = dataType match {\n+    case _: AtomicType => true\n+\n+    case st: StructType => st.forall { f => supportsDataType(f.dataType) }\n+\n+    case ArrayType(elementType, _) => supportsDataType(elementType)\n+\n+    case MapType(keyType, valueType, _) =>\n+      supportsDataType(keyType) && supportsDataType(valueType)\n+\n+    case udt: UserDefinedType[_] => supportsDataType(udt.sqlType)\n+\n+    case _: NullType => true\n+\n+    case _ => false\n+  }\n+\n+  def prepareWrite(\n+      sqlConf: SQLConf,\n+      job: Job,\n+      options: Map[String, String],\n+      dataSchema: StructType): OutputWriterFactory = {\n+    val parsedOptions = new AvroOptions(options, job.getConfiguration)",
    "line": 91
  }],
  "prId": 25017
}]