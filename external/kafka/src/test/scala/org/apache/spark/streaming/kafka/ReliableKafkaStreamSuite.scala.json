[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Throughout the testsuites, within each unit test, could you add comments describing what each step is doing? If can be at the granularity of `// Do this and then verify that`. \n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-07T04:03:33Z",
    "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.io.File\n+\n+import scala.collection.mutable\n+\n+import kafka.serializer.StringDecoder\n+import kafka.utils.{ZkUtils, ZKGroupTopicDirs}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.StreamingContext\n+import org.apache.spark.util.Utils\n+\n+class ReliableKafkaStreamSuite extends KafkaStreamSuite {"
  }],
  "prId": 2991
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: spelling mistake\n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-12T10:00:35Z",
    "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.io.File\n+\n+import scala.collection.mutable\n+\n+import kafka.serializer.StringDecoder\n+import kafka.utils.{ZkUtils, ZKGroupTopicDirs}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.StreamingContext\n+import org.apache.spark.util.Utils\n+\n+class ReliableKafkaStreamSuite extends KafkaStreamSuite {\n+  import KafkaTestUtils._\n+\n+  test(\"Reliable Kafka input stream\") {\n+    val sparkConf = new SparkConf()\n+      .setMaster(master)\n+      .setAppName(framework)\n+      .set(\"spark.streaming.receiver.writeAheadLog.enable\", \"true\")\n+    val ssc = new StreamingContext(sparkConf, batchDuration)\n+    val checkpointDir = s\"${System.getProperty(\"java.io.tmpdir\", \"/tmp\")}/\" +\n+      s\"test-checkpoint${random.nextInt(10000)}\"\n+    Utils.registerShutdownDeleteDir(new File(checkpointDir))\n+    ssc.checkpoint(checkpointDir)\n+\n+    val topic = \"test\"\n+    val sent = Map(\"a\" -> 1, \"b\" -> 1, \"c\" -> 1)\n+    createTopic(topic)\n+    produceAndSendMessage(topic, sent)\n+\n+    val kafkaParams = Map(\"zookeeper.connect\" -> s\"$zkHost:$zkPort\",\n+      \"group.id\" -> s\"test-consumer-${random.nextInt(10000)}\",\n+      \"auto.offset.reset\" -> \"smallest\")\n+\n+    val stream = KafkaUtils.createStream[String, String, StringDecoder, StringDecoder](\n+      ssc,\n+      kafkaParams,\n+      Map(topic -> 1),\n+      StorageLevel.MEMORY_ONLY)\n+    val result = new mutable.HashMap[String, Long]()\n+    stream.map { case (k, v) => v }\n+      .foreachRDD { r =>\n+        val ret = r.collect()\n+        ret.foreach { v =>\n+          val count = result.getOrElseUpdate(v, 0) + 1\n+          result.put(v, count)\n+        }\n+      }\n+    ssc.start()\n+    ssc.awaitTermination(3000)\n+\n+    // A basic process verification for ReliableKafkaReceiver.\n+    // Verify whether received message number is equal to the sent message number.\n+    assert(sent.size === result.size)\n+    // Verify whether each message is the same as the data to be verified.\n+    sent.keys.foreach { k => assert(sent(k) === result(k).toInt) }\n+\n+    ssc.stop()\n+  }\n+\n+  test(\"Verify the offset commit\") {\n+    // Verify the corretness of offset commit mechanism."
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "This is still there :)\n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-13T03:47:25Z",
    "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.io.File\n+\n+import scala.collection.mutable\n+\n+import kafka.serializer.StringDecoder\n+import kafka.utils.{ZkUtils, ZKGroupTopicDirs}\n+\n+import org.apache.spark.SparkConf\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.StreamingContext\n+import org.apache.spark.util.Utils\n+\n+class ReliableKafkaStreamSuite extends KafkaStreamSuite {\n+  import KafkaTestUtils._\n+\n+  test(\"Reliable Kafka input stream\") {\n+    val sparkConf = new SparkConf()\n+      .setMaster(master)\n+      .setAppName(framework)\n+      .set(\"spark.streaming.receiver.writeAheadLog.enable\", \"true\")\n+    val ssc = new StreamingContext(sparkConf, batchDuration)\n+    val checkpointDir = s\"${System.getProperty(\"java.io.tmpdir\", \"/tmp\")}/\" +\n+      s\"test-checkpoint${random.nextInt(10000)}\"\n+    Utils.registerShutdownDeleteDir(new File(checkpointDir))\n+    ssc.checkpoint(checkpointDir)\n+\n+    val topic = \"test\"\n+    val sent = Map(\"a\" -> 1, \"b\" -> 1, \"c\" -> 1)\n+    createTopic(topic)\n+    produceAndSendMessage(topic, sent)\n+\n+    val kafkaParams = Map(\"zookeeper.connect\" -> s\"$zkHost:$zkPort\",\n+      \"group.id\" -> s\"test-consumer-${random.nextInt(10000)}\",\n+      \"auto.offset.reset\" -> \"smallest\")\n+\n+    val stream = KafkaUtils.createStream[String, String, StringDecoder, StringDecoder](\n+      ssc,\n+      kafkaParams,\n+      Map(topic -> 1),\n+      StorageLevel.MEMORY_ONLY)\n+    val result = new mutable.HashMap[String, Long]()\n+    stream.map { case (k, v) => v }\n+      .foreachRDD { r =>\n+        val ret = r.collect()\n+        ret.foreach { v =>\n+          val count = result.getOrElseUpdate(v, 0) + 1\n+          result.put(v, count)\n+        }\n+      }\n+    ssc.start()\n+    ssc.awaitTermination(3000)\n+\n+    // A basic process verification for ReliableKafkaReceiver.\n+    // Verify whether received message number is equal to the sent message number.\n+    assert(sent.size === result.size)\n+    // Verify whether each message is the same as the data to be verified.\n+    sent.keys.foreach { k => assert(sent(k) === result(k).toInt) }\n+\n+    ssc.stop()\n+  }\n+\n+  test(\"Verify the offset commit\") {\n+    // Verify the corretness of offset commit mechanism."
  }],
  "prId": 2991
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "This test is still commented out. \n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-13T03:48:48Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.io.File\n+\n+import scala.collection.mutable\n+import scala.concurrent.duration._\n+import scala.language.postfixOps\n+import scala.util.Random\n+\n+import kafka.serializer.StringDecoder\n+import kafka.utils.{ZKGroupTopicDirs, ZkUtils}\n+import org.scalatest.concurrent.Eventually\n+\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.StreamingContext\n+import org.apache.spark.util.Utils\n+\n+class ReliableKafkaStreamSuite extends KafkaStreamSuiteBase with Eventually {\n+  val topic = \"topic\"\n+  val data = Map(\"a\" -> 10, \"b\" -> 10, \"c\" -> 10)\n+  var groupId: String = _\n+  var kafkaParams: Map[String, String] = _\n+\n+  before {\n+    beforeFunction()  // call this first to start ZK and Kafka\n+    groupId = s\"test-consumer-${Random.nextInt(10000)}\"\n+    kafkaParams = Map(\n+      \"zookeeper.connect\" -> zkAddress,\n+      \"group.id\" -> groupId,\n+      \"auto.offset.reset\" -> \"smallest\"\n+    )\n+  }\n+\n+  after {\n+    afterFunction()\n+  }\n+\n+  test(\"Reliable Kafka input stream\") {\n+    sparkConf.set(\"spark.streaming.receiver.writeAheadLog.enable\", \"true\")\n+    ssc = new StreamingContext(sparkConf, batchDuration)\n+    val checkpointDir = s\"${System.getProperty(\"java.io.tmpdir\", \"/tmp\")}/\" +\n+      s\"test-checkpoint${Random.nextInt(10000)}\"\n+    Utils.registerShutdownDeleteDir(new File(checkpointDir))\n+    ssc.checkpoint(checkpointDir)\n+    createTopic(topic)\n+    produceAndSendMessage(topic, data)\n+\n+    val stream = KafkaUtils.createStream[String, String, StringDecoder, StringDecoder](\n+      ssc,\n+      kafkaParams,\n+      Map(topic -> 1),\n+      StorageLevel.MEMORY_ONLY)\n+    val result = new mutable.HashMap[String, Long]()\n+    stream.map { case (k, v) => v }.foreachRDD { r =>\n+        val ret = r.collect()\n+        ret.foreach { v =>\n+          val count = result.getOrElseUpdate(v, 0) + 1\n+          result.put(v, count)\n+        }\n+      }\n+    ssc.start()\n+    eventually(timeout(10000 milliseconds), interval(100 milliseconds)) {\n+      // A basic process verification for ReliableKafkaReceiver.\n+      // Verify whether received message number is equal to the sent message number.\n+      assert(data.size === result.size)\n+      // Verify whether each message is the same as the data to be verified.\n+      data.keys.foreach { k => assert(data(k) === result(k).toInt) }\n+    }\n+    ssc.stop()\n+  }\n+/*"
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "I will do it.\n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-13T04:30:37Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.io.File\n+\n+import scala.collection.mutable\n+import scala.concurrent.duration._\n+import scala.language.postfixOps\n+import scala.util.Random\n+\n+import kafka.serializer.StringDecoder\n+import kafka.utils.{ZKGroupTopicDirs, ZkUtils}\n+import org.scalatest.concurrent.Eventually\n+\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.StreamingContext\n+import org.apache.spark.util.Utils\n+\n+class ReliableKafkaStreamSuite extends KafkaStreamSuiteBase with Eventually {\n+  val topic = \"topic\"\n+  val data = Map(\"a\" -> 10, \"b\" -> 10, \"c\" -> 10)\n+  var groupId: String = _\n+  var kafkaParams: Map[String, String] = _\n+\n+  before {\n+    beforeFunction()  // call this first to start ZK and Kafka\n+    groupId = s\"test-consumer-${Random.nextInt(10000)}\"\n+    kafkaParams = Map(\n+      \"zookeeper.connect\" -> zkAddress,\n+      \"group.id\" -> groupId,\n+      \"auto.offset.reset\" -> \"smallest\"\n+    )\n+  }\n+\n+  after {\n+    afterFunction()\n+  }\n+\n+  test(\"Reliable Kafka input stream\") {\n+    sparkConf.set(\"spark.streaming.receiver.writeAheadLog.enable\", \"true\")\n+    ssc = new StreamingContext(sparkConf, batchDuration)\n+    val checkpointDir = s\"${System.getProperty(\"java.io.tmpdir\", \"/tmp\")}/\" +\n+      s\"test-checkpoint${Random.nextInt(10000)}\"\n+    Utils.registerShutdownDeleteDir(new File(checkpointDir))\n+    ssc.checkpoint(checkpointDir)\n+    createTopic(topic)\n+    produceAndSendMessage(topic, data)\n+\n+    val stream = KafkaUtils.createStream[String, String, StringDecoder, StringDecoder](\n+      ssc,\n+      kafkaParams,\n+      Map(topic -> 1),\n+      StorageLevel.MEMORY_ONLY)\n+    val result = new mutable.HashMap[String, Long]()\n+    stream.map { case (k, v) => v }.foreachRDD { r =>\n+        val ret = r.collect()\n+        ret.foreach { v =>\n+          val count = result.getOrElseUpdate(v, 0) + 1\n+          result.put(v, count)\n+        }\n+      }\n+    ssc.start()\n+    eventually(timeout(10000 milliseconds), interval(100 milliseconds)) {\n+      // A basic process verification for ReliableKafkaReceiver.\n+      // Verify whether received message number is equal to the sent message number.\n+      assert(data.size === result.size)\n+      // Verify whether each message is the same as the data to be verified.\n+      data.keys.foreach { k => assert(data(k) === result(k).toInt) }\n+    }\n+    ssc.stop()\n+  }\n+/*"
  }],
  "prId": 2991
}]