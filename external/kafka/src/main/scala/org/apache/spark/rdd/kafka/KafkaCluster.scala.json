[{
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "It would probably be good to make this `private[spark]` and keep it as an internal utility.\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2014-12-30T21:35:00Z",
    "diffHunk": "@@ -0,0 +1,305 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.util.control.NonFatal\n+import scala.collection.mutable.ArrayBuffer\n+import java.util.Properties\n+import kafka.api._\n+import kafka.common.{ErrorMapping, OffsetMetadataAndError, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+\n+/**\n+  * Convenience methods for interacting with a Kafka cluster.\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form\n+  */\n+class KafkaCluster(val kafkaParams: Map[String, String]) {"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "The rdd would be really unpleasant to actually use without the convenience methods exposed by KafkaCluster, especially if you're keeping your offsets in zookeeper and doing idempotent writes.\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2014-12-30T21:43:09Z",
    "diffHunk": "@@ -0,0 +1,305 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.util.control.NonFatal\n+import scala.collection.mutable.ArrayBuffer\n+import java.util.Properties\n+import kafka.api._\n+import kafka.common.{ErrorMapping, OffsetMetadataAndError, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+\n+/**\n+  * Convenience methods for interacting with a Kafka cluster.\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form\n+  */\n+class KafkaCluster(val kafkaParams: Map[String, String]) {"
  }, {
    "author": {
      "login": "pwendell"
    },
    "body": "I see - sorry let me look more, I didn't realize this is necessary for users.\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2014-12-30T21:52:14Z",
    "diffHunk": "@@ -0,0 +1,305 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.util.control.NonFatal\n+import scala.collection.mutable.ArrayBuffer\n+import java.util.Properties\n+import kafka.api._\n+import kafka.common.{ErrorMapping, OffsetMetadataAndError, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+\n+/**\n+  * Convenience methods for interacting with a Kafka cluster.\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form\n+  */\n+class KafkaCluster(val kafkaParams: Map[String, String]) {"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "for example\n\nhttps://github.com/koeninger/kafka-exactly-once/blob/master/src/main/scala/example/IdempotentExample.scala#L60\n\nWe also use it for doing things like e.g. starting a stream at the leader offsets before a given time\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2014-12-30T21:58:20Z",
    "diffHunk": "@@ -0,0 +1,305 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.util.control.NonFatal\n+import scala.collection.mutable.ArrayBuffer\n+import java.util.Properties\n+import kafka.api._\n+import kafka.common.{ErrorMapping, OffsetMetadataAndError, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+\n+/**\n+  * Convenience methods for interacting with a Kafka cluster.\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form\n+  */\n+class KafkaCluster(val kafkaParams: Map[String, String]) {"
  }],
  "prId": 3798
}, {
  "comments": [{
    "author": {
      "login": "helena"
    },
    "body": "containsKey is less expensive than contains\n\n```\n props.putAll(scala.collection.JavaConversions.mapAsJavaMap (kafkaParams)) \n Seq(\"zookeeper.connect\", \"group.id\").collect { \n  case s if props containsKey s => props.setProperty(s, \"\") \n}\n```\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-08T12:40:51Z",
    "diffHunk": "@@ -0,0 +1,313 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.util.control.NonFatal\n+import scala.collection.mutable.ArrayBuffer\n+import java.util.Properties\n+import kafka.api._\n+import kafka.common.{ErrorMapping, OffsetMetadataAndError, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+\n+/**\n+  * Convenience methods for interacting with a Kafka cluster.\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form\n+  */\n+class KafkaCluster(val kafkaParams: Map[String, String]) extends Serializable {\n+  import KafkaCluster.Err\n+\n+  val seedBrokers: Array[(String, Int)] =\n+    kafkaParams.get(\"metadata.broker.list\")\n+      .orElse(kafkaParams.get(\"bootstrap.servers\"))\n+      .getOrElse(throw new Exception(\"Must specify metadata.broker.list or bootstrap.servers\"))\n+      .split(\",\").map { hp =>\n+        val hpa = hp.split(\":\")\n+        (hpa(0), hpa(1).toInt)\n+      }\n+\n+  // ConsumerConfig isn't serializable\n+  @transient private var _config: ConsumerConfig = null\n+\n+  def config: ConsumerConfig = this.synchronized {\n+    if (_config == null) {\n+      _config = KafkaCluster.consumerConfig(kafkaParams)\n+    }\n+    _config\n+  }\n+\n+  def connect(host: String, port: Int): SimpleConsumer =\n+    new SimpleConsumer(host, port, config.socketTimeoutMs,\n+      config.socketReceiveBufferBytes, config.clientId)\n+\n+  def connect(hostAndPort: (String, Int)): SimpleConsumer =\n+    connect(hostAndPort._1, hostAndPort._2)\n+\n+  def connectLeader(topic: String, partition: Int): Either[Err, SimpleConsumer] =\n+    findLeader(topic, partition).right.map(connect)\n+\n+  def findLeader(topic: String, partition: Int): Either[Err, (String, Int)] = {\n+    val req = TopicMetadataRequest(TopicMetadataRequest.CurrentVersion,\n+      0, config.clientId, Seq(topic))\n+    val errs = new Err\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp: TopicMetadataResponse = consumer.send(req)\n+      resp.topicsMetadata.find(_.topic == topic).flatMap { t =>\n+        t.partitionsMetadata.find(_.partitionId == partition)\n+      }.foreach { partitionMeta =>\n+        partitionMeta.leader.foreach { leader =>\n+          return Right((leader.host, leader.port))\n+        }\n+      }\n+    }\n+    Left(errs)\n+  }\n+\n+  def findLeaders(\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, (String, Int)]] = {\n+    getPartitionMetadata(topicAndPartitions.map(_.topic)).right.flatMap { tms =>\n+      val result = tms.flatMap { tm: TopicMetadata =>\n+        tm.partitionsMetadata.flatMap { pm =>\n+          val tp = TopicAndPartition(tm.topic, pm.partitionId)\n+          if (topicAndPartitions(tp)) {\n+            pm.leader.map { l =>\n+              tp -> (l.host -> l.port)\n+            }\n+          } else {\n+            None\n+          }\n+        }\n+      }.toMap\n+      if (result.keys.size == topicAndPartitions.size) {\n+        Right(result)\n+      } else {\n+        val missing = topicAndPartitions.diff(result.keys.toSet)\n+        val err = new Err\n+        err.append(new Exception(s\"Couldn't find leaders for ${missing}\"))\n+        Left(err)\n+      }\n+    }\n+  }\n+\n+  def getPartitions(topics: Set[String]): Either[Err, Set[TopicAndPartition]] =\n+    getPartitionMetadata(topics).right.map { r =>\n+      r.flatMap { tm: TopicMetadata =>\n+        tm.partitionsMetadata.map { pm =>\n+          TopicAndPartition(tm.topic, pm.partitionId)\n+        }    \n+      }\n+    }\n+\n+  def getPartitionMetadata(topics: Set[String]): Either[Err, Set[TopicMetadata]] = {\n+    val req = TopicMetadataRequest(TopicMetadataRequest.CurrentVersion,\n+      0, config.clientId, topics.toSeq)\n+    val errs = new Err\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp: TopicMetadataResponse = consumer.send(req)\n+      // error codes here indicate missing / just created topic,\n+      // repeating on a different broker wont be useful\n+      return Right(resp.topicsMetadata.toSet)\n+    }\n+    Left(errs)\n+  }\n+\n+  def getLatestLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, Long]] =\n+    getLeaderOffsets(topicAndPartitions, OffsetRequest.LatestTime)\n+\n+  def getEarliestLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, Long]] =\n+    getLeaderOffsets(topicAndPartitions, OffsetRequest.EarliestTime)\n+\n+  def getLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition],\n+    before: Long\n+  ): Either[Err, Map[TopicAndPartition, Long]] =\n+    getLeaderOffsets(topicAndPartitions, before, 1).right.map { r =>\n+      r.map { kv =>\n+        // mapValues isnt serializable, see SI-7005\n+        kv._1 -> kv._2.head\n+      }\n+    }\n+\n+  private def flip[K, V](m: Map[K, V]): Map[V, Seq[K]] =\n+    m.groupBy(_._2).map { kv =>\n+      kv._1 -> kv._2.keys.toSeq\n+    }\n+\n+  def getLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition],\n+    before: Long,\n+    maxNumOffsets: Int\n+  ): Either[Err, Map[TopicAndPartition, Seq[Long]]] = {\n+    findLeaders(topicAndPartitions).right.flatMap { tpToLeader =>\n+      val leaderToTp: Map[(String, Int), Seq[TopicAndPartition]] = flip(tpToLeader)\n+      val leaders = leaderToTp.keys\n+      var result = Map[TopicAndPartition, Seq[Long]]()\n+      val errs = new Err\n+      withBrokers(leaders, errs) { consumer =>\n+        val needed: Seq[TopicAndPartition] = leaderToTp((consumer.host, consumer.port))\n+        val req = OffsetRequest(\n+          needed.map { tp =>\n+            tp -> PartitionOffsetRequestInfo(before, maxNumOffsets)\n+          }.toMap\n+        )\n+        val resp = consumer.getOffsetsBefore(req)\n+        val respMap = resp.partitionErrorAndOffsets\n+        needed.foreach { tp =>\n+          respMap.get(tp).foreach { errAndOffsets =>\n+            if (errAndOffsets.error == ErrorMapping.NoError) {\n+              if (errAndOffsets.offsets.nonEmpty) {\n+                result += tp -> errAndOffsets.offsets\n+              } else {\n+                errs.append(new Exception(\n+                  s\"Empty offsets for ${tp}, is ${before} before log beginning?\"))\n+              }\n+            } else {\n+              errs.append(ErrorMapping.exceptionFor(errAndOffsets.error))\n+            }\n+          }\n+        }\n+        if (result.keys.size == topicAndPartitions.size) {\n+          return Right(result)\n+        }\n+      }\n+      val missing = topicAndPartitions.diff(result.keys.toSet)\n+      errs.append(new Exception(s\"Couldn't find leader offsets for ${missing}\"))\n+      Left(errs)\n+    }\n+  }\n+\n+  def getConsumerOffsets(\n+    groupId: String,\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, Long]] = {\n+    getConsumerOffsetMetadata(groupId, topicAndPartitions).right.map { r =>\n+      r.map { kv =>\n+        kv._1 -> kv._2.offset\n+      }\n+    }\n+  }\n+\n+  def getConsumerOffsetMetadata(\n+    groupId: String,\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, OffsetMetadataAndError]] = {\n+    var result = Map[TopicAndPartition, OffsetMetadataAndError]()\n+    val req = OffsetFetchRequest(groupId, topicAndPartitions.toSeq)\n+    val errs = new Err\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp = consumer.fetchOffsets(req)\n+      val respMap = resp.requestInfo\n+      val needed = topicAndPartitions.diff(result.keys.toSet)\n+      needed.foreach { tp =>\n+        respMap.get(tp).foreach { offsetMeta =>\n+          if (offsetMeta.error == ErrorMapping.NoError) {\n+            result += tp -> offsetMeta\n+          } else {\n+            errs.append(ErrorMapping.exceptionFor(offsetMeta.error))\n+          }\n+        }\n+      }\n+      if (result.keys.size == topicAndPartitions.size) {\n+        return Right(result)\n+      }\n+    }\n+    val missing = topicAndPartitions.diff(result.keys.toSet)\n+    errs.append(new Exception(s\"Couldn't find consumer offsets for ${missing}\"))\n+    Left(errs)\n+  }\n+\n+  def setConsumerOffsets(\n+    groupId: String,\n+    offsets: Map[TopicAndPartition, Long]\n+  ): Either[Err, Map[TopicAndPartition, Short]] = {\n+    setConsumerOffsetMetadata(groupId, offsets.map { kv =>\n+      kv._1 -> OffsetMetadataAndError(kv._2)\n+    })\n+  }\n+\n+  def setConsumerOffsetMetadata(\n+    groupId: String,\n+    metadata: Map[TopicAndPartition, OffsetMetadataAndError]\n+  ): Either[Err, Map[TopicAndPartition, Short]] = {\n+    var result = Map[TopicAndPartition, Short]()\n+    val req = OffsetCommitRequest(groupId, metadata)\n+    val errs = new Err\n+    val topicAndPartitions = metadata.keys.toSet\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp = consumer.commitOffsets(req)\n+      val respMap = resp.requestInfo\n+      val needed = topicAndPartitions.diff(result.keys.toSet)\n+      needed.foreach { tp =>\n+        respMap.get(tp).foreach { err =>\n+          if (err == ErrorMapping.NoError) {\n+            result += tp -> err\n+          } else {\n+            errs.append(ErrorMapping.exceptionFor(err))\n+          }\n+        }\n+      }\n+      if (result.keys.size == topicAndPartitions.size) {\n+        return Right(result)\n+      }\n+    }\n+    val missing = topicAndPartitions.diff(result.keys.toSet)\n+    errs.append(new Exception(s\"Couldn't set offsets for ${missing}\"))\n+    Left(errs)\n+  }\n+\n+  private def withBrokers(brokers: Iterable[(String, Int)], errs: Err)\n+    (fn: SimpleConsumer => Any): Unit = {\n+    brokers.foreach { hp =>\n+      var consumer: SimpleConsumer = null\n+      try {\n+        consumer = connect(hp)\n+        fn(consumer)\n+      } catch {\n+        case NonFatal(e) =>\n+          errs.append(e)\n+      } finally {\n+        if (consumer != null) consumer.close()\n+      }\n+    }\n+  }\n+}\n+\n+object KafkaCluster {\n+  type Err = ArrayBuffer[Throwable]\n+\n+  /** Make a consumer config without requiring group.id or zookeeper.connect,\n+    * since communicating with brokers also needs common settings such as timeout\n+    */\n+  def consumerConfig(kafkaParams: Map[String, String]): ConsumerConfig = {\n+    val props = new Properties()\n+    kafkaParams.foreach(param => props.put(param._1, param._2))\n+    Seq(\"zookeeper.connect\", \"group.id\").foreach { s =>\n+      if (!props.contains(s)) {\n+        props.setProperty(s, \"\")\n+      }\n+    }"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "I wouldn't use the collect (even though it allows you to do the partial function) because it is actually obfuscating the intention here, because it is perceived as a filter. foreach makes more sense.\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-27T04:30:32Z",
    "diffHunk": "@@ -0,0 +1,313 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.util.control.NonFatal\n+import scala.collection.mutable.ArrayBuffer\n+import java.util.Properties\n+import kafka.api._\n+import kafka.common.{ErrorMapping, OffsetMetadataAndError, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+\n+/**\n+  * Convenience methods for interacting with a Kafka cluster.\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form\n+  */\n+class KafkaCluster(val kafkaParams: Map[String, String]) extends Serializable {\n+  import KafkaCluster.Err\n+\n+  val seedBrokers: Array[(String, Int)] =\n+    kafkaParams.get(\"metadata.broker.list\")\n+      .orElse(kafkaParams.get(\"bootstrap.servers\"))\n+      .getOrElse(throw new Exception(\"Must specify metadata.broker.list or bootstrap.servers\"))\n+      .split(\",\").map { hp =>\n+        val hpa = hp.split(\":\")\n+        (hpa(0), hpa(1).toInt)\n+      }\n+\n+  // ConsumerConfig isn't serializable\n+  @transient private var _config: ConsumerConfig = null\n+\n+  def config: ConsumerConfig = this.synchronized {\n+    if (_config == null) {\n+      _config = KafkaCluster.consumerConfig(kafkaParams)\n+    }\n+    _config\n+  }\n+\n+  def connect(host: String, port: Int): SimpleConsumer =\n+    new SimpleConsumer(host, port, config.socketTimeoutMs,\n+      config.socketReceiveBufferBytes, config.clientId)\n+\n+  def connect(hostAndPort: (String, Int)): SimpleConsumer =\n+    connect(hostAndPort._1, hostAndPort._2)\n+\n+  def connectLeader(topic: String, partition: Int): Either[Err, SimpleConsumer] =\n+    findLeader(topic, partition).right.map(connect)\n+\n+  def findLeader(topic: String, partition: Int): Either[Err, (String, Int)] = {\n+    val req = TopicMetadataRequest(TopicMetadataRequest.CurrentVersion,\n+      0, config.clientId, Seq(topic))\n+    val errs = new Err\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp: TopicMetadataResponse = consumer.send(req)\n+      resp.topicsMetadata.find(_.topic == topic).flatMap { t =>\n+        t.partitionsMetadata.find(_.partitionId == partition)\n+      }.foreach { partitionMeta =>\n+        partitionMeta.leader.foreach { leader =>\n+          return Right((leader.host, leader.port))\n+        }\n+      }\n+    }\n+    Left(errs)\n+  }\n+\n+  def findLeaders(\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, (String, Int)]] = {\n+    getPartitionMetadata(topicAndPartitions.map(_.topic)).right.flatMap { tms =>\n+      val result = tms.flatMap { tm: TopicMetadata =>\n+        tm.partitionsMetadata.flatMap { pm =>\n+          val tp = TopicAndPartition(tm.topic, pm.partitionId)\n+          if (topicAndPartitions(tp)) {\n+            pm.leader.map { l =>\n+              tp -> (l.host -> l.port)\n+            }\n+          } else {\n+            None\n+          }\n+        }\n+      }.toMap\n+      if (result.keys.size == topicAndPartitions.size) {\n+        Right(result)\n+      } else {\n+        val missing = topicAndPartitions.diff(result.keys.toSet)\n+        val err = new Err\n+        err.append(new Exception(s\"Couldn't find leaders for ${missing}\"))\n+        Left(err)\n+      }\n+    }\n+  }\n+\n+  def getPartitions(topics: Set[String]): Either[Err, Set[TopicAndPartition]] =\n+    getPartitionMetadata(topics).right.map { r =>\n+      r.flatMap { tm: TopicMetadata =>\n+        tm.partitionsMetadata.map { pm =>\n+          TopicAndPartition(tm.topic, pm.partitionId)\n+        }    \n+      }\n+    }\n+\n+  def getPartitionMetadata(topics: Set[String]): Either[Err, Set[TopicMetadata]] = {\n+    val req = TopicMetadataRequest(TopicMetadataRequest.CurrentVersion,\n+      0, config.clientId, topics.toSeq)\n+    val errs = new Err\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp: TopicMetadataResponse = consumer.send(req)\n+      // error codes here indicate missing / just created topic,\n+      // repeating on a different broker wont be useful\n+      return Right(resp.topicsMetadata.toSet)\n+    }\n+    Left(errs)\n+  }\n+\n+  def getLatestLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, Long]] =\n+    getLeaderOffsets(topicAndPartitions, OffsetRequest.LatestTime)\n+\n+  def getEarliestLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, Long]] =\n+    getLeaderOffsets(topicAndPartitions, OffsetRequest.EarliestTime)\n+\n+  def getLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition],\n+    before: Long\n+  ): Either[Err, Map[TopicAndPartition, Long]] =\n+    getLeaderOffsets(topicAndPartitions, before, 1).right.map { r =>\n+      r.map { kv =>\n+        // mapValues isnt serializable, see SI-7005\n+        kv._1 -> kv._2.head\n+      }\n+    }\n+\n+  private def flip[K, V](m: Map[K, V]): Map[V, Seq[K]] =\n+    m.groupBy(_._2).map { kv =>\n+      kv._1 -> kv._2.keys.toSeq\n+    }\n+\n+  def getLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition],\n+    before: Long,\n+    maxNumOffsets: Int\n+  ): Either[Err, Map[TopicAndPartition, Seq[Long]]] = {\n+    findLeaders(topicAndPartitions).right.flatMap { tpToLeader =>\n+      val leaderToTp: Map[(String, Int), Seq[TopicAndPartition]] = flip(tpToLeader)\n+      val leaders = leaderToTp.keys\n+      var result = Map[TopicAndPartition, Seq[Long]]()\n+      val errs = new Err\n+      withBrokers(leaders, errs) { consumer =>\n+        val needed: Seq[TopicAndPartition] = leaderToTp((consumer.host, consumer.port))\n+        val req = OffsetRequest(\n+          needed.map { tp =>\n+            tp -> PartitionOffsetRequestInfo(before, maxNumOffsets)\n+          }.toMap\n+        )\n+        val resp = consumer.getOffsetsBefore(req)\n+        val respMap = resp.partitionErrorAndOffsets\n+        needed.foreach { tp =>\n+          respMap.get(tp).foreach { errAndOffsets =>\n+            if (errAndOffsets.error == ErrorMapping.NoError) {\n+              if (errAndOffsets.offsets.nonEmpty) {\n+                result += tp -> errAndOffsets.offsets\n+              } else {\n+                errs.append(new Exception(\n+                  s\"Empty offsets for ${tp}, is ${before} before log beginning?\"))\n+              }\n+            } else {\n+              errs.append(ErrorMapping.exceptionFor(errAndOffsets.error))\n+            }\n+          }\n+        }\n+        if (result.keys.size == topicAndPartitions.size) {\n+          return Right(result)\n+        }\n+      }\n+      val missing = topicAndPartitions.diff(result.keys.toSet)\n+      errs.append(new Exception(s\"Couldn't find leader offsets for ${missing}\"))\n+      Left(errs)\n+    }\n+  }\n+\n+  def getConsumerOffsets(\n+    groupId: String,\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, Long]] = {\n+    getConsumerOffsetMetadata(groupId, topicAndPartitions).right.map { r =>\n+      r.map { kv =>\n+        kv._1 -> kv._2.offset\n+      }\n+    }\n+  }\n+\n+  def getConsumerOffsetMetadata(\n+    groupId: String,\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, OffsetMetadataAndError]] = {\n+    var result = Map[TopicAndPartition, OffsetMetadataAndError]()\n+    val req = OffsetFetchRequest(groupId, topicAndPartitions.toSeq)\n+    val errs = new Err\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp = consumer.fetchOffsets(req)\n+      val respMap = resp.requestInfo\n+      val needed = topicAndPartitions.diff(result.keys.toSet)\n+      needed.foreach { tp =>\n+        respMap.get(tp).foreach { offsetMeta =>\n+          if (offsetMeta.error == ErrorMapping.NoError) {\n+            result += tp -> offsetMeta\n+          } else {\n+            errs.append(ErrorMapping.exceptionFor(offsetMeta.error))\n+          }\n+        }\n+      }\n+      if (result.keys.size == topicAndPartitions.size) {\n+        return Right(result)\n+      }\n+    }\n+    val missing = topicAndPartitions.diff(result.keys.toSet)\n+    errs.append(new Exception(s\"Couldn't find consumer offsets for ${missing}\"))\n+    Left(errs)\n+  }\n+\n+  def setConsumerOffsets(\n+    groupId: String,\n+    offsets: Map[TopicAndPartition, Long]\n+  ): Either[Err, Map[TopicAndPartition, Short]] = {\n+    setConsumerOffsetMetadata(groupId, offsets.map { kv =>\n+      kv._1 -> OffsetMetadataAndError(kv._2)\n+    })\n+  }\n+\n+  def setConsumerOffsetMetadata(\n+    groupId: String,\n+    metadata: Map[TopicAndPartition, OffsetMetadataAndError]\n+  ): Either[Err, Map[TopicAndPartition, Short]] = {\n+    var result = Map[TopicAndPartition, Short]()\n+    val req = OffsetCommitRequest(groupId, metadata)\n+    val errs = new Err\n+    val topicAndPartitions = metadata.keys.toSet\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp = consumer.commitOffsets(req)\n+      val respMap = resp.requestInfo\n+      val needed = topicAndPartitions.diff(result.keys.toSet)\n+      needed.foreach { tp =>\n+        respMap.get(tp).foreach { err =>\n+          if (err == ErrorMapping.NoError) {\n+            result += tp -> err\n+          } else {\n+            errs.append(ErrorMapping.exceptionFor(err))\n+          }\n+        }\n+      }\n+      if (result.keys.size == topicAndPartitions.size) {\n+        return Right(result)\n+      }\n+    }\n+    val missing = topicAndPartitions.diff(result.keys.toSet)\n+    errs.append(new Exception(s\"Couldn't set offsets for ${missing}\"))\n+    Left(errs)\n+  }\n+\n+  private def withBrokers(brokers: Iterable[(String, Int)], errs: Err)\n+    (fn: SimpleConsumer => Any): Unit = {\n+    brokers.foreach { hp =>\n+      var consumer: SimpleConsumer = null\n+      try {\n+        consumer = connect(hp)\n+        fn(consumer)\n+      } catch {\n+        case NonFatal(e) =>\n+          errs.append(e)\n+      } finally {\n+        if (consumer != null) consumer.close()\n+      }\n+    }\n+  }\n+}\n+\n+object KafkaCluster {\n+  type Err = ArrayBuffer[Throwable]\n+\n+  /** Make a consumer config without requiring group.id or zookeeper.connect,\n+    * since communicating with brokers also needs common settings such as timeout\n+    */\n+  def consumerConfig(kafkaParams: Map[String, String]): ConsumerConfig = {\n+    val props = new Properties()\n+    kafkaParams.foreach(param => props.put(param._1, param._2))\n+    Seq(\"zookeeper.connect\", \"group.id\").foreach { s =>\n+      if (!props.contains(s)) {\n+        props.setProperty(s, \"\")\n+      }\n+    }"
  }],
  "prId": 3798
}, {
  "comments": [{
    "author": {
      "login": "helena"
    },
    "body": "Or you can stay away from java nulls     \n\n```\nvar consumer: Option[SimpleConsumer] = None\ntry {\n  consumer = Some(connect(hp))\n  consumer map (c => fn(c)) \n} catch {\n  case NonFatal(e) => errs.append(e)\n} finally consumer map (_.close())\n```\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-08T12:51:45Z",
    "diffHunk": "@@ -0,0 +1,313 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.util.control.NonFatal\n+import scala.collection.mutable.ArrayBuffer\n+import java.util.Properties\n+import kafka.api._\n+import kafka.common.{ErrorMapping, OffsetMetadataAndError, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+\n+/**\n+  * Convenience methods for interacting with a Kafka cluster.\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form\n+  */\n+class KafkaCluster(val kafkaParams: Map[String, String]) extends Serializable {\n+  import KafkaCluster.Err\n+\n+  val seedBrokers: Array[(String, Int)] =\n+    kafkaParams.get(\"metadata.broker.list\")\n+      .orElse(kafkaParams.get(\"bootstrap.servers\"))\n+      .getOrElse(throw new Exception(\"Must specify metadata.broker.list or bootstrap.servers\"))\n+      .split(\",\").map { hp =>\n+        val hpa = hp.split(\":\")\n+        (hpa(0), hpa(1).toInt)\n+      }\n+\n+  // ConsumerConfig isn't serializable\n+  @transient private var _config: ConsumerConfig = null\n+\n+  def config: ConsumerConfig = this.synchronized {\n+    if (_config == null) {\n+      _config = KafkaCluster.consumerConfig(kafkaParams)\n+    }\n+    _config\n+  }\n+\n+  def connect(host: String, port: Int): SimpleConsumer =\n+    new SimpleConsumer(host, port, config.socketTimeoutMs,\n+      config.socketReceiveBufferBytes, config.clientId)\n+\n+  def connect(hostAndPort: (String, Int)): SimpleConsumer =\n+    connect(hostAndPort._1, hostAndPort._2)\n+\n+  def connectLeader(topic: String, partition: Int): Either[Err, SimpleConsumer] =\n+    findLeader(topic, partition).right.map(connect)\n+\n+  def findLeader(topic: String, partition: Int): Either[Err, (String, Int)] = {\n+    val req = TopicMetadataRequest(TopicMetadataRequest.CurrentVersion,\n+      0, config.clientId, Seq(topic))\n+    val errs = new Err\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp: TopicMetadataResponse = consumer.send(req)\n+      resp.topicsMetadata.find(_.topic == topic).flatMap { t =>\n+        t.partitionsMetadata.find(_.partitionId == partition)\n+      }.foreach { partitionMeta =>\n+        partitionMeta.leader.foreach { leader =>\n+          return Right((leader.host, leader.port))\n+        }\n+      }\n+    }\n+    Left(errs)\n+  }\n+\n+  def findLeaders(\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, (String, Int)]] = {\n+    getPartitionMetadata(topicAndPartitions.map(_.topic)).right.flatMap { tms =>\n+      val result = tms.flatMap { tm: TopicMetadata =>\n+        tm.partitionsMetadata.flatMap { pm =>\n+          val tp = TopicAndPartition(tm.topic, pm.partitionId)\n+          if (topicAndPartitions(tp)) {\n+            pm.leader.map { l =>\n+              tp -> (l.host -> l.port)\n+            }\n+          } else {\n+            None\n+          }\n+        }\n+      }.toMap\n+      if (result.keys.size == topicAndPartitions.size) {\n+        Right(result)\n+      } else {\n+        val missing = topicAndPartitions.diff(result.keys.toSet)\n+        val err = new Err\n+        err.append(new Exception(s\"Couldn't find leaders for ${missing}\"))\n+        Left(err)\n+      }\n+    }\n+  }\n+\n+  def getPartitions(topics: Set[String]): Either[Err, Set[TopicAndPartition]] =\n+    getPartitionMetadata(topics).right.map { r =>\n+      r.flatMap { tm: TopicMetadata =>\n+        tm.partitionsMetadata.map { pm =>\n+          TopicAndPartition(tm.topic, pm.partitionId)\n+        }    \n+      }\n+    }\n+\n+  def getPartitionMetadata(topics: Set[String]): Either[Err, Set[TopicMetadata]] = {\n+    val req = TopicMetadataRequest(TopicMetadataRequest.CurrentVersion,\n+      0, config.clientId, topics.toSeq)\n+    val errs = new Err\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp: TopicMetadataResponse = consumer.send(req)\n+      // error codes here indicate missing / just created topic,\n+      // repeating on a different broker wont be useful\n+      return Right(resp.topicsMetadata.toSet)\n+    }\n+    Left(errs)\n+  }\n+\n+  def getLatestLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, Long]] =\n+    getLeaderOffsets(topicAndPartitions, OffsetRequest.LatestTime)\n+\n+  def getEarliestLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, Long]] =\n+    getLeaderOffsets(topicAndPartitions, OffsetRequest.EarliestTime)\n+\n+  def getLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition],\n+    before: Long\n+  ): Either[Err, Map[TopicAndPartition, Long]] =\n+    getLeaderOffsets(topicAndPartitions, before, 1).right.map { r =>\n+      r.map { kv =>\n+        // mapValues isnt serializable, see SI-7005\n+        kv._1 -> kv._2.head\n+      }\n+    }\n+\n+  private def flip[K, V](m: Map[K, V]): Map[V, Seq[K]] =\n+    m.groupBy(_._2).map { kv =>\n+      kv._1 -> kv._2.keys.toSeq\n+    }\n+\n+  def getLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition],\n+    before: Long,\n+    maxNumOffsets: Int\n+  ): Either[Err, Map[TopicAndPartition, Seq[Long]]] = {\n+    findLeaders(topicAndPartitions).right.flatMap { tpToLeader =>\n+      val leaderToTp: Map[(String, Int), Seq[TopicAndPartition]] = flip(tpToLeader)\n+      val leaders = leaderToTp.keys\n+      var result = Map[TopicAndPartition, Seq[Long]]()\n+      val errs = new Err\n+      withBrokers(leaders, errs) { consumer =>\n+        val needed: Seq[TopicAndPartition] = leaderToTp((consumer.host, consumer.port))\n+        val req = OffsetRequest(\n+          needed.map { tp =>\n+            tp -> PartitionOffsetRequestInfo(before, maxNumOffsets)\n+          }.toMap\n+        )\n+        val resp = consumer.getOffsetsBefore(req)\n+        val respMap = resp.partitionErrorAndOffsets\n+        needed.foreach { tp =>\n+          respMap.get(tp).foreach { errAndOffsets =>\n+            if (errAndOffsets.error == ErrorMapping.NoError) {\n+              if (errAndOffsets.offsets.nonEmpty) {\n+                result += tp -> errAndOffsets.offsets\n+              } else {\n+                errs.append(new Exception(\n+                  s\"Empty offsets for ${tp}, is ${before} before log beginning?\"))\n+              }\n+            } else {\n+              errs.append(ErrorMapping.exceptionFor(errAndOffsets.error))\n+            }\n+          }\n+        }\n+        if (result.keys.size == topicAndPartitions.size) {\n+          return Right(result)\n+        }\n+      }\n+      val missing = topicAndPartitions.diff(result.keys.toSet)\n+      errs.append(new Exception(s\"Couldn't find leader offsets for ${missing}\"))\n+      Left(errs)\n+    }\n+  }\n+\n+  def getConsumerOffsets(\n+    groupId: String,\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, Long]] = {\n+    getConsumerOffsetMetadata(groupId, topicAndPartitions).right.map { r =>\n+      r.map { kv =>\n+        kv._1 -> kv._2.offset\n+      }\n+    }\n+  }\n+\n+  def getConsumerOffsetMetadata(\n+    groupId: String,\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, OffsetMetadataAndError]] = {\n+    var result = Map[TopicAndPartition, OffsetMetadataAndError]()\n+    val req = OffsetFetchRequest(groupId, topicAndPartitions.toSeq)\n+    val errs = new Err\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp = consumer.fetchOffsets(req)\n+      val respMap = resp.requestInfo\n+      val needed = topicAndPartitions.diff(result.keys.toSet)\n+      needed.foreach { tp =>\n+        respMap.get(tp).foreach { offsetMeta =>\n+          if (offsetMeta.error == ErrorMapping.NoError) {\n+            result += tp -> offsetMeta\n+          } else {\n+            errs.append(ErrorMapping.exceptionFor(offsetMeta.error))\n+          }\n+        }\n+      }\n+      if (result.keys.size == topicAndPartitions.size) {\n+        return Right(result)\n+      }\n+    }\n+    val missing = topicAndPartitions.diff(result.keys.toSet)\n+    errs.append(new Exception(s\"Couldn't find consumer offsets for ${missing}\"))\n+    Left(errs)\n+  }\n+\n+  def setConsumerOffsets(\n+    groupId: String,\n+    offsets: Map[TopicAndPartition, Long]\n+  ): Either[Err, Map[TopicAndPartition, Short]] = {\n+    setConsumerOffsetMetadata(groupId, offsets.map { kv =>\n+      kv._1 -> OffsetMetadataAndError(kv._2)\n+    })\n+  }\n+\n+  def setConsumerOffsetMetadata(\n+    groupId: String,\n+    metadata: Map[TopicAndPartition, OffsetMetadataAndError]\n+  ): Either[Err, Map[TopicAndPartition, Short]] = {\n+    var result = Map[TopicAndPartition, Short]()\n+    val req = OffsetCommitRequest(groupId, metadata)\n+    val errs = new Err\n+    val topicAndPartitions = metadata.keys.toSet\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp = consumer.commitOffsets(req)\n+      val respMap = resp.requestInfo\n+      val needed = topicAndPartitions.diff(result.keys.toSet)\n+      needed.foreach { tp =>\n+        respMap.get(tp).foreach { err =>\n+          if (err == ErrorMapping.NoError) {\n+            result += tp -> err\n+          } else {\n+            errs.append(ErrorMapping.exceptionFor(err))\n+          }\n+        }\n+      }\n+      if (result.keys.size == topicAndPartitions.size) {\n+        return Right(result)\n+      }\n+    }\n+    val missing = topicAndPartitions.diff(result.keys.toSet)\n+    errs.append(new Exception(s\"Couldn't set offsets for ${missing}\"))\n+    Left(errs)\n+  }\n+\n+  private def withBrokers(brokers: Iterable[(String, Int)], errs: Err)\n+    (fn: SimpleConsumer => Any): Unit = {\n+    brokers.foreach { hp =>\n+      var consumer: SimpleConsumer = null\n+      try {\n+        consumer = connect(hp)\n+        fn(consumer)\n+      } catch {\n+        case NonFatal(e) =>\n+          errs.append(e)\n+      } finally {\n+        if (consumer != null) consumer.close()\n+      }"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "I appreciate the feedback, but it's impossible to \"stay away from java nulls\" in a jvm language, without runtime checks.\n\nDespite propaganda to the contrary, option.map is not a replacement for null checks.\n\nThe code you wrote can still throw a null pointer exception (if SimpleConsumer returns null, for instance).\n\nYou can hide the null pointer check inside of Option.apply instead of using Some, but at that point I'd rather just be explicit about what is going on.\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-08T14:21:37Z",
    "diffHunk": "@@ -0,0 +1,313 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.util.control.NonFatal\n+import scala.collection.mutable.ArrayBuffer\n+import java.util.Properties\n+import kafka.api._\n+import kafka.common.{ErrorMapping, OffsetMetadataAndError, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+\n+/**\n+  * Convenience methods for interacting with a Kafka cluster.\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form\n+  */\n+class KafkaCluster(val kafkaParams: Map[String, String]) extends Serializable {\n+  import KafkaCluster.Err\n+\n+  val seedBrokers: Array[(String, Int)] =\n+    kafkaParams.get(\"metadata.broker.list\")\n+      .orElse(kafkaParams.get(\"bootstrap.servers\"))\n+      .getOrElse(throw new Exception(\"Must specify metadata.broker.list or bootstrap.servers\"))\n+      .split(\",\").map { hp =>\n+        val hpa = hp.split(\":\")\n+        (hpa(0), hpa(1).toInt)\n+      }\n+\n+  // ConsumerConfig isn't serializable\n+  @transient private var _config: ConsumerConfig = null\n+\n+  def config: ConsumerConfig = this.synchronized {\n+    if (_config == null) {\n+      _config = KafkaCluster.consumerConfig(kafkaParams)\n+    }\n+    _config\n+  }\n+\n+  def connect(host: String, port: Int): SimpleConsumer =\n+    new SimpleConsumer(host, port, config.socketTimeoutMs,\n+      config.socketReceiveBufferBytes, config.clientId)\n+\n+  def connect(hostAndPort: (String, Int)): SimpleConsumer =\n+    connect(hostAndPort._1, hostAndPort._2)\n+\n+  def connectLeader(topic: String, partition: Int): Either[Err, SimpleConsumer] =\n+    findLeader(topic, partition).right.map(connect)\n+\n+  def findLeader(topic: String, partition: Int): Either[Err, (String, Int)] = {\n+    val req = TopicMetadataRequest(TopicMetadataRequest.CurrentVersion,\n+      0, config.clientId, Seq(topic))\n+    val errs = new Err\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp: TopicMetadataResponse = consumer.send(req)\n+      resp.topicsMetadata.find(_.topic == topic).flatMap { t =>\n+        t.partitionsMetadata.find(_.partitionId == partition)\n+      }.foreach { partitionMeta =>\n+        partitionMeta.leader.foreach { leader =>\n+          return Right((leader.host, leader.port))\n+        }\n+      }\n+    }\n+    Left(errs)\n+  }\n+\n+  def findLeaders(\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, (String, Int)]] = {\n+    getPartitionMetadata(topicAndPartitions.map(_.topic)).right.flatMap { tms =>\n+      val result = tms.flatMap { tm: TopicMetadata =>\n+        tm.partitionsMetadata.flatMap { pm =>\n+          val tp = TopicAndPartition(tm.topic, pm.partitionId)\n+          if (topicAndPartitions(tp)) {\n+            pm.leader.map { l =>\n+              tp -> (l.host -> l.port)\n+            }\n+          } else {\n+            None\n+          }\n+        }\n+      }.toMap\n+      if (result.keys.size == topicAndPartitions.size) {\n+        Right(result)\n+      } else {\n+        val missing = topicAndPartitions.diff(result.keys.toSet)\n+        val err = new Err\n+        err.append(new Exception(s\"Couldn't find leaders for ${missing}\"))\n+        Left(err)\n+      }\n+    }\n+  }\n+\n+  def getPartitions(topics: Set[String]): Either[Err, Set[TopicAndPartition]] =\n+    getPartitionMetadata(topics).right.map { r =>\n+      r.flatMap { tm: TopicMetadata =>\n+        tm.partitionsMetadata.map { pm =>\n+          TopicAndPartition(tm.topic, pm.partitionId)\n+        }    \n+      }\n+    }\n+\n+  def getPartitionMetadata(topics: Set[String]): Either[Err, Set[TopicMetadata]] = {\n+    val req = TopicMetadataRequest(TopicMetadataRequest.CurrentVersion,\n+      0, config.clientId, topics.toSeq)\n+    val errs = new Err\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp: TopicMetadataResponse = consumer.send(req)\n+      // error codes here indicate missing / just created topic,\n+      // repeating on a different broker wont be useful\n+      return Right(resp.topicsMetadata.toSet)\n+    }\n+    Left(errs)\n+  }\n+\n+  def getLatestLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, Long]] =\n+    getLeaderOffsets(topicAndPartitions, OffsetRequest.LatestTime)\n+\n+  def getEarliestLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, Long]] =\n+    getLeaderOffsets(topicAndPartitions, OffsetRequest.EarliestTime)\n+\n+  def getLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition],\n+    before: Long\n+  ): Either[Err, Map[TopicAndPartition, Long]] =\n+    getLeaderOffsets(topicAndPartitions, before, 1).right.map { r =>\n+      r.map { kv =>\n+        // mapValues isnt serializable, see SI-7005\n+        kv._1 -> kv._2.head\n+      }\n+    }\n+\n+  private def flip[K, V](m: Map[K, V]): Map[V, Seq[K]] =\n+    m.groupBy(_._2).map { kv =>\n+      kv._1 -> kv._2.keys.toSeq\n+    }\n+\n+  def getLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition],\n+    before: Long,\n+    maxNumOffsets: Int\n+  ): Either[Err, Map[TopicAndPartition, Seq[Long]]] = {\n+    findLeaders(topicAndPartitions).right.flatMap { tpToLeader =>\n+      val leaderToTp: Map[(String, Int), Seq[TopicAndPartition]] = flip(tpToLeader)\n+      val leaders = leaderToTp.keys\n+      var result = Map[TopicAndPartition, Seq[Long]]()\n+      val errs = new Err\n+      withBrokers(leaders, errs) { consumer =>\n+        val needed: Seq[TopicAndPartition] = leaderToTp((consumer.host, consumer.port))\n+        val req = OffsetRequest(\n+          needed.map { tp =>\n+            tp -> PartitionOffsetRequestInfo(before, maxNumOffsets)\n+          }.toMap\n+        )\n+        val resp = consumer.getOffsetsBefore(req)\n+        val respMap = resp.partitionErrorAndOffsets\n+        needed.foreach { tp =>\n+          respMap.get(tp).foreach { errAndOffsets =>\n+            if (errAndOffsets.error == ErrorMapping.NoError) {\n+              if (errAndOffsets.offsets.nonEmpty) {\n+                result += tp -> errAndOffsets.offsets\n+              } else {\n+                errs.append(new Exception(\n+                  s\"Empty offsets for ${tp}, is ${before} before log beginning?\"))\n+              }\n+            } else {\n+              errs.append(ErrorMapping.exceptionFor(errAndOffsets.error))\n+            }\n+          }\n+        }\n+        if (result.keys.size == topicAndPartitions.size) {\n+          return Right(result)\n+        }\n+      }\n+      val missing = topicAndPartitions.diff(result.keys.toSet)\n+      errs.append(new Exception(s\"Couldn't find leader offsets for ${missing}\"))\n+      Left(errs)\n+    }\n+  }\n+\n+  def getConsumerOffsets(\n+    groupId: String,\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, Long]] = {\n+    getConsumerOffsetMetadata(groupId, topicAndPartitions).right.map { r =>\n+      r.map { kv =>\n+        kv._1 -> kv._2.offset\n+      }\n+    }\n+  }\n+\n+  def getConsumerOffsetMetadata(\n+    groupId: String,\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, OffsetMetadataAndError]] = {\n+    var result = Map[TopicAndPartition, OffsetMetadataAndError]()\n+    val req = OffsetFetchRequest(groupId, topicAndPartitions.toSeq)\n+    val errs = new Err\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp = consumer.fetchOffsets(req)\n+      val respMap = resp.requestInfo\n+      val needed = topicAndPartitions.diff(result.keys.toSet)\n+      needed.foreach { tp =>\n+        respMap.get(tp).foreach { offsetMeta =>\n+          if (offsetMeta.error == ErrorMapping.NoError) {\n+            result += tp -> offsetMeta\n+          } else {\n+            errs.append(ErrorMapping.exceptionFor(offsetMeta.error))\n+          }\n+        }\n+      }\n+      if (result.keys.size == topicAndPartitions.size) {\n+        return Right(result)\n+      }\n+    }\n+    val missing = topicAndPartitions.diff(result.keys.toSet)\n+    errs.append(new Exception(s\"Couldn't find consumer offsets for ${missing}\"))\n+    Left(errs)\n+  }\n+\n+  def setConsumerOffsets(\n+    groupId: String,\n+    offsets: Map[TopicAndPartition, Long]\n+  ): Either[Err, Map[TopicAndPartition, Short]] = {\n+    setConsumerOffsetMetadata(groupId, offsets.map { kv =>\n+      kv._1 -> OffsetMetadataAndError(kv._2)\n+    })\n+  }\n+\n+  def setConsumerOffsetMetadata(\n+    groupId: String,\n+    metadata: Map[TopicAndPartition, OffsetMetadataAndError]\n+  ): Either[Err, Map[TopicAndPartition, Short]] = {\n+    var result = Map[TopicAndPartition, Short]()\n+    val req = OffsetCommitRequest(groupId, metadata)\n+    val errs = new Err\n+    val topicAndPartitions = metadata.keys.toSet\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp = consumer.commitOffsets(req)\n+      val respMap = resp.requestInfo\n+      val needed = topicAndPartitions.diff(result.keys.toSet)\n+      needed.foreach { tp =>\n+        respMap.get(tp).foreach { err =>\n+          if (err == ErrorMapping.NoError) {\n+            result += tp -> err\n+          } else {\n+            errs.append(ErrorMapping.exceptionFor(err))\n+          }\n+        }\n+      }\n+      if (result.keys.size == topicAndPartitions.size) {\n+        return Right(result)\n+      }\n+    }\n+    val missing = topicAndPartitions.diff(result.keys.toSet)\n+    errs.append(new Exception(s\"Couldn't set offsets for ${missing}\"))\n+    Left(errs)\n+  }\n+\n+  private def withBrokers(brokers: Iterable[(String, Int)], errs: Err)\n+    (fn: SimpleConsumer => Any): Unit = {\n+    brokers.foreach { hp =>\n+      var consumer: SimpleConsumer = null\n+      try {\n+        consumer = connect(hp)\n+        fn(consumer)\n+      } catch {\n+        case NonFatal(e) =>\n+          errs.append(e)\n+      } finally {\n+        if (consumer != null) consumer.close()\n+      }"
  }, {
    "author": {
      "login": "helena"
    },
    "body": "True, it could because I was not conclusive on the suggestion adding None where applicable. catch => None\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-08T16:41:45Z",
    "diffHunk": "@@ -0,0 +1,313 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.util.control.NonFatal\n+import scala.collection.mutable.ArrayBuffer\n+import java.util.Properties\n+import kafka.api._\n+import kafka.common.{ErrorMapping, OffsetMetadataAndError, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+\n+/**\n+  * Convenience methods for interacting with a Kafka cluster.\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form\n+  */\n+class KafkaCluster(val kafkaParams: Map[String, String]) extends Serializable {\n+  import KafkaCluster.Err\n+\n+  val seedBrokers: Array[(String, Int)] =\n+    kafkaParams.get(\"metadata.broker.list\")\n+      .orElse(kafkaParams.get(\"bootstrap.servers\"))\n+      .getOrElse(throw new Exception(\"Must specify metadata.broker.list or bootstrap.servers\"))\n+      .split(\",\").map { hp =>\n+        val hpa = hp.split(\":\")\n+        (hpa(0), hpa(1).toInt)\n+      }\n+\n+  // ConsumerConfig isn't serializable\n+  @transient private var _config: ConsumerConfig = null\n+\n+  def config: ConsumerConfig = this.synchronized {\n+    if (_config == null) {\n+      _config = KafkaCluster.consumerConfig(kafkaParams)\n+    }\n+    _config\n+  }\n+\n+  def connect(host: String, port: Int): SimpleConsumer =\n+    new SimpleConsumer(host, port, config.socketTimeoutMs,\n+      config.socketReceiveBufferBytes, config.clientId)\n+\n+  def connect(hostAndPort: (String, Int)): SimpleConsumer =\n+    connect(hostAndPort._1, hostAndPort._2)\n+\n+  def connectLeader(topic: String, partition: Int): Either[Err, SimpleConsumer] =\n+    findLeader(topic, partition).right.map(connect)\n+\n+  def findLeader(topic: String, partition: Int): Either[Err, (String, Int)] = {\n+    val req = TopicMetadataRequest(TopicMetadataRequest.CurrentVersion,\n+      0, config.clientId, Seq(topic))\n+    val errs = new Err\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp: TopicMetadataResponse = consumer.send(req)\n+      resp.topicsMetadata.find(_.topic == topic).flatMap { t =>\n+        t.partitionsMetadata.find(_.partitionId == partition)\n+      }.foreach { partitionMeta =>\n+        partitionMeta.leader.foreach { leader =>\n+          return Right((leader.host, leader.port))\n+        }\n+      }\n+    }\n+    Left(errs)\n+  }\n+\n+  def findLeaders(\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, (String, Int)]] = {\n+    getPartitionMetadata(topicAndPartitions.map(_.topic)).right.flatMap { tms =>\n+      val result = tms.flatMap { tm: TopicMetadata =>\n+        tm.partitionsMetadata.flatMap { pm =>\n+          val tp = TopicAndPartition(tm.topic, pm.partitionId)\n+          if (topicAndPartitions(tp)) {\n+            pm.leader.map { l =>\n+              tp -> (l.host -> l.port)\n+            }\n+          } else {\n+            None\n+          }\n+        }\n+      }.toMap\n+      if (result.keys.size == topicAndPartitions.size) {\n+        Right(result)\n+      } else {\n+        val missing = topicAndPartitions.diff(result.keys.toSet)\n+        val err = new Err\n+        err.append(new Exception(s\"Couldn't find leaders for ${missing}\"))\n+        Left(err)\n+      }\n+    }\n+  }\n+\n+  def getPartitions(topics: Set[String]): Either[Err, Set[TopicAndPartition]] =\n+    getPartitionMetadata(topics).right.map { r =>\n+      r.flatMap { tm: TopicMetadata =>\n+        tm.partitionsMetadata.map { pm =>\n+          TopicAndPartition(tm.topic, pm.partitionId)\n+        }    \n+      }\n+    }\n+\n+  def getPartitionMetadata(topics: Set[String]): Either[Err, Set[TopicMetadata]] = {\n+    val req = TopicMetadataRequest(TopicMetadataRequest.CurrentVersion,\n+      0, config.clientId, topics.toSeq)\n+    val errs = new Err\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp: TopicMetadataResponse = consumer.send(req)\n+      // error codes here indicate missing / just created topic,\n+      // repeating on a different broker wont be useful\n+      return Right(resp.topicsMetadata.toSet)\n+    }\n+    Left(errs)\n+  }\n+\n+  def getLatestLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, Long]] =\n+    getLeaderOffsets(topicAndPartitions, OffsetRequest.LatestTime)\n+\n+  def getEarliestLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, Long]] =\n+    getLeaderOffsets(topicAndPartitions, OffsetRequest.EarliestTime)\n+\n+  def getLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition],\n+    before: Long\n+  ): Either[Err, Map[TopicAndPartition, Long]] =\n+    getLeaderOffsets(topicAndPartitions, before, 1).right.map { r =>\n+      r.map { kv =>\n+        // mapValues isnt serializable, see SI-7005\n+        kv._1 -> kv._2.head\n+      }\n+    }\n+\n+  private def flip[K, V](m: Map[K, V]): Map[V, Seq[K]] =\n+    m.groupBy(_._2).map { kv =>\n+      kv._1 -> kv._2.keys.toSeq\n+    }\n+\n+  def getLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition],\n+    before: Long,\n+    maxNumOffsets: Int\n+  ): Either[Err, Map[TopicAndPartition, Seq[Long]]] = {\n+    findLeaders(topicAndPartitions).right.flatMap { tpToLeader =>\n+      val leaderToTp: Map[(String, Int), Seq[TopicAndPartition]] = flip(tpToLeader)\n+      val leaders = leaderToTp.keys\n+      var result = Map[TopicAndPartition, Seq[Long]]()\n+      val errs = new Err\n+      withBrokers(leaders, errs) { consumer =>\n+        val needed: Seq[TopicAndPartition] = leaderToTp((consumer.host, consumer.port))\n+        val req = OffsetRequest(\n+          needed.map { tp =>\n+            tp -> PartitionOffsetRequestInfo(before, maxNumOffsets)\n+          }.toMap\n+        )\n+        val resp = consumer.getOffsetsBefore(req)\n+        val respMap = resp.partitionErrorAndOffsets\n+        needed.foreach { tp =>\n+          respMap.get(tp).foreach { errAndOffsets =>\n+            if (errAndOffsets.error == ErrorMapping.NoError) {\n+              if (errAndOffsets.offsets.nonEmpty) {\n+                result += tp -> errAndOffsets.offsets\n+              } else {\n+                errs.append(new Exception(\n+                  s\"Empty offsets for ${tp}, is ${before} before log beginning?\"))\n+              }\n+            } else {\n+              errs.append(ErrorMapping.exceptionFor(errAndOffsets.error))\n+            }\n+          }\n+        }\n+        if (result.keys.size == topicAndPartitions.size) {\n+          return Right(result)\n+        }\n+      }\n+      val missing = topicAndPartitions.diff(result.keys.toSet)\n+      errs.append(new Exception(s\"Couldn't find leader offsets for ${missing}\"))\n+      Left(errs)\n+    }\n+  }\n+\n+  def getConsumerOffsets(\n+    groupId: String,\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, Long]] = {\n+    getConsumerOffsetMetadata(groupId, topicAndPartitions).right.map { r =>\n+      r.map { kv =>\n+        kv._1 -> kv._2.offset\n+      }\n+    }\n+  }\n+\n+  def getConsumerOffsetMetadata(\n+    groupId: String,\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, OffsetMetadataAndError]] = {\n+    var result = Map[TopicAndPartition, OffsetMetadataAndError]()\n+    val req = OffsetFetchRequest(groupId, topicAndPartitions.toSeq)\n+    val errs = new Err\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp = consumer.fetchOffsets(req)\n+      val respMap = resp.requestInfo\n+      val needed = topicAndPartitions.diff(result.keys.toSet)\n+      needed.foreach { tp =>\n+        respMap.get(tp).foreach { offsetMeta =>\n+          if (offsetMeta.error == ErrorMapping.NoError) {\n+            result += tp -> offsetMeta\n+          } else {\n+            errs.append(ErrorMapping.exceptionFor(offsetMeta.error))\n+          }\n+        }\n+      }\n+      if (result.keys.size == topicAndPartitions.size) {\n+        return Right(result)\n+      }\n+    }\n+    val missing = topicAndPartitions.diff(result.keys.toSet)\n+    errs.append(new Exception(s\"Couldn't find consumer offsets for ${missing}\"))\n+    Left(errs)\n+  }\n+\n+  def setConsumerOffsets(\n+    groupId: String,\n+    offsets: Map[TopicAndPartition, Long]\n+  ): Either[Err, Map[TopicAndPartition, Short]] = {\n+    setConsumerOffsetMetadata(groupId, offsets.map { kv =>\n+      kv._1 -> OffsetMetadataAndError(kv._2)\n+    })\n+  }\n+\n+  def setConsumerOffsetMetadata(\n+    groupId: String,\n+    metadata: Map[TopicAndPartition, OffsetMetadataAndError]\n+  ): Either[Err, Map[TopicAndPartition, Short]] = {\n+    var result = Map[TopicAndPartition, Short]()\n+    val req = OffsetCommitRequest(groupId, metadata)\n+    val errs = new Err\n+    val topicAndPartitions = metadata.keys.toSet\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp = consumer.commitOffsets(req)\n+      val respMap = resp.requestInfo\n+      val needed = topicAndPartitions.diff(result.keys.toSet)\n+      needed.foreach { tp =>\n+        respMap.get(tp).foreach { err =>\n+          if (err == ErrorMapping.NoError) {\n+            result += tp -> err\n+          } else {\n+            errs.append(ErrorMapping.exceptionFor(err))\n+          }\n+        }\n+      }\n+      if (result.keys.size == topicAndPartitions.size) {\n+        return Right(result)\n+      }\n+    }\n+    val missing = topicAndPartitions.diff(result.keys.toSet)\n+    errs.append(new Exception(s\"Couldn't set offsets for ${missing}\"))\n+    Left(errs)\n+  }\n+\n+  private def withBrokers(brokers: Iterable[(String, Int)], errs: Err)\n+    (fn: SimpleConsumer => Any): Unit = {\n+    brokers.foreach { hp =>\n+      var consumer: SimpleConsumer = null\n+      try {\n+        consumer = connect(hp)\n+        fn(consumer)\n+      } catch {\n+        case NonFatal(e) =>\n+          errs.append(e)\n+      } finally {\n+        if (consumer != null) consumer.close()\n+      }"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "the use of null here is fine, and very clear. the pattern matching with the finally actually makes it much harder to understand what is going on.\n\none nitpick, you need to put curly braces around consumer.close(), i.e.\n\n``` scala\nif (consumer != null) {\n  consumer.close()\n}\n```\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-27T04:29:01Z",
    "diffHunk": "@@ -0,0 +1,313 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.util.control.NonFatal\n+import scala.collection.mutable.ArrayBuffer\n+import java.util.Properties\n+import kafka.api._\n+import kafka.common.{ErrorMapping, OffsetMetadataAndError, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+\n+/**\n+  * Convenience methods for interacting with a Kafka cluster.\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form\n+  */\n+class KafkaCluster(val kafkaParams: Map[String, String]) extends Serializable {\n+  import KafkaCluster.Err\n+\n+  val seedBrokers: Array[(String, Int)] =\n+    kafkaParams.get(\"metadata.broker.list\")\n+      .orElse(kafkaParams.get(\"bootstrap.servers\"))\n+      .getOrElse(throw new Exception(\"Must specify metadata.broker.list or bootstrap.servers\"))\n+      .split(\",\").map { hp =>\n+        val hpa = hp.split(\":\")\n+        (hpa(0), hpa(1).toInt)\n+      }\n+\n+  // ConsumerConfig isn't serializable\n+  @transient private var _config: ConsumerConfig = null\n+\n+  def config: ConsumerConfig = this.synchronized {\n+    if (_config == null) {\n+      _config = KafkaCluster.consumerConfig(kafkaParams)\n+    }\n+    _config\n+  }\n+\n+  def connect(host: String, port: Int): SimpleConsumer =\n+    new SimpleConsumer(host, port, config.socketTimeoutMs,\n+      config.socketReceiveBufferBytes, config.clientId)\n+\n+  def connect(hostAndPort: (String, Int)): SimpleConsumer =\n+    connect(hostAndPort._1, hostAndPort._2)\n+\n+  def connectLeader(topic: String, partition: Int): Either[Err, SimpleConsumer] =\n+    findLeader(topic, partition).right.map(connect)\n+\n+  def findLeader(topic: String, partition: Int): Either[Err, (String, Int)] = {\n+    val req = TopicMetadataRequest(TopicMetadataRequest.CurrentVersion,\n+      0, config.clientId, Seq(topic))\n+    val errs = new Err\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp: TopicMetadataResponse = consumer.send(req)\n+      resp.topicsMetadata.find(_.topic == topic).flatMap { t =>\n+        t.partitionsMetadata.find(_.partitionId == partition)\n+      }.foreach { partitionMeta =>\n+        partitionMeta.leader.foreach { leader =>\n+          return Right((leader.host, leader.port))\n+        }\n+      }\n+    }\n+    Left(errs)\n+  }\n+\n+  def findLeaders(\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, (String, Int)]] = {\n+    getPartitionMetadata(topicAndPartitions.map(_.topic)).right.flatMap { tms =>\n+      val result = tms.flatMap { tm: TopicMetadata =>\n+        tm.partitionsMetadata.flatMap { pm =>\n+          val tp = TopicAndPartition(tm.topic, pm.partitionId)\n+          if (topicAndPartitions(tp)) {\n+            pm.leader.map { l =>\n+              tp -> (l.host -> l.port)\n+            }\n+          } else {\n+            None\n+          }\n+        }\n+      }.toMap\n+      if (result.keys.size == topicAndPartitions.size) {\n+        Right(result)\n+      } else {\n+        val missing = topicAndPartitions.diff(result.keys.toSet)\n+        val err = new Err\n+        err.append(new Exception(s\"Couldn't find leaders for ${missing}\"))\n+        Left(err)\n+      }\n+    }\n+  }\n+\n+  def getPartitions(topics: Set[String]): Either[Err, Set[TopicAndPartition]] =\n+    getPartitionMetadata(topics).right.map { r =>\n+      r.flatMap { tm: TopicMetadata =>\n+        tm.partitionsMetadata.map { pm =>\n+          TopicAndPartition(tm.topic, pm.partitionId)\n+        }    \n+      }\n+    }\n+\n+  def getPartitionMetadata(topics: Set[String]): Either[Err, Set[TopicMetadata]] = {\n+    val req = TopicMetadataRequest(TopicMetadataRequest.CurrentVersion,\n+      0, config.clientId, topics.toSeq)\n+    val errs = new Err\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp: TopicMetadataResponse = consumer.send(req)\n+      // error codes here indicate missing / just created topic,\n+      // repeating on a different broker wont be useful\n+      return Right(resp.topicsMetadata.toSet)\n+    }\n+    Left(errs)\n+  }\n+\n+  def getLatestLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, Long]] =\n+    getLeaderOffsets(topicAndPartitions, OffsetRequest.LatestTime)\n+\n+  def getEarliestLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, Long]] =\n+    getLeaderOffsets(topicAndPartitions, OffsetRequest.EarliestTime)\n+\n+  def getLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition],\n+    before: Long\n+  ): Either[Err, Map[TopicAndPartition, Long]] =\n+    getLeaderOffsets(topicAndPartitions, before, 1).right.map { r =>\n+      r.map { kv =>\n+        // mapValues isnt serializable, see SI-7005\n+        kv._1 -> kv._2.head\n+      }\n+    }\n+\n+  private def flip[K, V](m: Map[K, V]): Map[V, Seq[K]] =\n+    m.groupBy(_._2).map { kv =>\n+      kv._1 -> kv._2.keys.toSeq\n+    }\n+\n+  def getLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition],\n+    before: Long,\n+    maxNumOffsets: Int\n+  ): Either[Err, Map[TopicAndPartition, Seq[Long]]] = {\n+    findLeaders(topicAndPartitions).right.flatMap { tpToLeader =>\n+      val leaderToTp: Map[(String, Int), Seq[TopicAndPartition]] = flip(tpToLeader)\n+      val leaders = leaderToTp.keys\n+      var result = Map[TopicAndPartition, Seq[Long]]()\n+      val errs = new Err\n+      withBrokers(leaders, errs) { consumer =>\n+        val needed: Seq[TopicAndPartition] = leaderToTp((consumer.host, consumer.port))\n+        val req = OffsetRequest(\n+          needed.map { tp =>\n+            tp -> PartitionOffsetRequestInfo(before, maxNumOffsets)\n+          }.toMap\n+        )\n+        val resp = consumer.getOffsetsBefore(req)\n+        val respMap = resp.partitionErrorAndOffsets\n+        needed.foreach { tp =>\n+          respMap.get(tp).foreach { errAndOffsets =>\n+            if (errAndOffsets.error == ErrorMapping.NoError) {\n+              if (errAndOffsets.offsets.nonEmpty) {\n+                result += tp -> errAndOffsets.offsets\n+              } else {\n+                errs.append(new Exception(\n+                  s\"Empty offsets for ${tp}, is ${before} before log beginning?\"))\n+              }\n+            } else {\n+              errs.append(ErrorMapping.exceptionFor(errAndOffsets.error))\n+            }\n+          }\n+        }\n+        if (result.keys.size == topicAndPartitions.size) {\n+          return Right(result)\n+        }\n+      }\n+      val missing = topicAndPartitions.diff(result.keys.toSet)\n+      errs.append(new Exception(s\"Couldn't find leader offsets for ${missing}\"))\n+      Left(errs)\n+    }\n+  }\n+\n+  def getConsumerOffsets(\n+    groupId: String,\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, Long]] = {\n+    getConsumerOffsetMetadata(groupId, topicAndPartitions).right.map { r =>\n+      r.map { kv =>\n+        kv._1 -> kv._2.offset\n+      }\n+    }\n+  }\n+\n+  def getConsumerOffsetMetadata(\n+    groupId: String,\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, OffsetMetadataAndError]] = {\n+    var result = Map[TopicAndPartition, OffsetMetadataAndError]()\n+    val req = OffsetFetchRequest(groupId, topicAndPartitions.toSeq)\n+    val errs = new Err\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp = consumer.fetchOffsets(req)\n+      val respMap = resp.requestInfo\n+      val needed = topicAndPartitions.diff(result.keys.toSet)\n+      needed.foreach { tp =>\n+        respMap.get(tp).foreach { offsetMeta =>\n+          if (offsetMeta.error == ErrorMapping.NoError) {\n+            result += tp -> offsetMeta\n+          } else {\n+            errs.append(ErrorMapping.exceptionFor(offsetMeta.error))\n+          }\n+        }\n+      }\n+      if (result.keys.size == topicAndPartitions.size) {\n+        return Right(result)\n+      }\n+    }\n+    val missing = topicAndPartitions.diff(result.keys.toSet)\n+    errs.append(new Exception(s\"Couldn't find consumer offsets for ${missing}\"))\n+    Left(errs)\n+  }\n+\n+  def setConsumerOffsets(\n+    groupId: String,\n+    offsets: Map[TopicAndPartition, Long]\n+  ): Either[Err, Map[TopicAndPartition, Short]] = {\n+    setConsumerOffsetMetadata(groupId, offsets.map { kv =>\n+      kv._1 -> OffsetMetadataAndError(kv._2)\n+    })\n+  }\n+\n+  def setConsumerOffsetMetadata(\n+    groupId: String,\n+    metadata: Map[TopicAndPartition, OffsetMetadataAndError]\n+  ): Either[Err, Map[TopicAndPartition, Short]] = {\n+    var result = Map[TopicAndPartition, Short]()\n+    val req = OffsetCommitRequest(groupId, metadata)\n+    val errs = new Err\n+    val topicAndPartitions = metadata.keys.toSet\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp = consumer.commitOffsets(req)\n+      val respMap = resp.requestInfo\n+      val needed = topicAndPartitions.diff(result.keys.toSet)\n+      needed.foreach { tp =>\n+        respMap.get(tp).foreach { err =>\n+          if (err == ErrorMapping.NoError) {\n+            result += tp -> err\n+          } else {\n+            errs.append(ErrorMapping.exceptionFor(err))\n+          }\n+        }\n+      }\n+      if (result.keys.size == topicAndPartitions.size) {\n+        return Right(result)\n+      }\n+    }\n+    val missing = topicAndPartitions.diff(result.keys.toSet)\n+    errs.append(new Exception(s\"Couldn't set offsets for ${missing}\"))\n+    Left(errs)\n+  }\n+\n+  private def withBrokers(brokers: Iterable[(String, Int)], errs: Err)\n+    (fn: SimpleConsumer => Any): Unit = {\n+    brokers.foreach { hp =>\n+      var consumer: SimpleConsumer = null\n+      try {\n+        consumer = connect(hp)\n+        fn(consumer)\n+      } catch {\n+        case NonFatal(e) =>\n+          errs.append(e)\n+      } finally {\n+        if (consumer != null) consumer.close()\n+      }"
  }],
  "prId": 3798
}, {
  "comments": [{
    "author": {
      "login": "helena"
    },
    "body": "return? how about just \n   Right(result)\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-08T12:54:57Z",
    "diffHunk": "@@ -0,0 +1,313 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.util.control.NonFatal\n+import scala.collection.mutable.ArrayBuffer\n+import java.util.Properties\n+import kafka.api._\n+import kafka.common.{ErrorMapping, OffsetMetadataAndError, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+\n+/**\n+  * Convenience methods for interacting with a Kafka cluster.\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form\n+  */\n+class KafkaCluster(val kafkaParams: Map[String, String]) extends Serializable {\n+  import KafkaCluster.Err\n+\n+  val seedBrokers: Array[(String, Int)] =\n+    kafkaParams.get(\"metadata.broker.list\")\n+      .orElse(kafkaParams.get(\"bootstrap.servers\"))\n+      .getOrElse(throw new Exception(\"Must specify metadata.broker.list or bootstrap.servers\"))\n+      .split(\",\").map { hp =>\n+        val hpa = hp.split(\":\")\n+        (hpa(0), hpa(1).toInt)\n+      }\n+\n+  // ConsumerConfig isn't serializable\n+  @transient private var _config: ConsumerConfig = null\n+\n+  def config: ConsumerConfig = this.synchronized {\n+    if (_config == null) {\n+      _config = KafkaCluster.consumerConfig(kafkaParams)\n+    }\n+    _config\n+  }\n+\n+  def connect(host: String, port: Int): SimpleConsumer =\n+    new SimpleConsumer(host, port, config.socketTimeoutMs,\n+      config.socketReceiveBufferBytes, config.clientId)\n+\n+  def connect(hostAndPort: (String, Int)): SimpleConsumer =\n+    connect(hostAndPort._1, hostAndPort._2)\n+\n+  def connectLeader(topic: String, partition: Int): Either[Err, SimpleConsumer] =\n+    findLeader(topic, partition).right.map(connect)\n+\n+  def findLeader(topic: String, partition: Int): Either[Err, (String, Int)] = {\n+    val req = TopicMetadataRequest(TopicMetadataRequest.CurrentVersion,\n+      0, config.clientId, Seq(topic))\n+    val errs = new Err\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp: TopicMetadataResponse = consumer.send(req)\n+      resp.topicsMetadata.find(_.topic == topic).flatMap { t =>\n+        t.partitionsMetadata.find(_.partitionId == partition)\n+      }.foreach { partitionMeta =>\n+        partitionMeta.leader.foreach { leader =>\n+          return Right((leader.host, leader.port))\n+        }\n+      }\n+    }\n+    Left(errs)\n+  }\n+\n+  def findLeaders(\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, (String, Int)]] = {\n+    getPartitionMetadata(topicAndPartitions.map(_.topic)).right.flatMap { tms =>\n+      val result = tms.flatMap { tm: TopicMetadata =>\n+        tm.partitionsMetadata.flatMap { pm =>\n+          val tp = TopicAndPartition(tm.topic, pm.partitionId)\n+          if (topicAndPartitions(tp)) {\n+            pm.leader.map { l =>\n+              tp -> (l.host -> l.port)\n+            }\n+          } else {\n+            None\n+          }\n+        }\n+      }.toMap\n+      if (result.keys.size == topicAndPartitions.size) {\n+        Right(result)\n+      } else {\n+        val missing = topicAndPartitions.diff(result.keys.toSet)\n+        val err = new Err\n+        err.append(new Exception(s\"Couldn't find leaders for ${missing}\"))\n+        Left(err)\n+      }\n+    }\n+  }\n+\n+  def getPartitions(topics: Set[String]): Either[Err, Set[TopicAndPartition]] =\n+    getPartitionMetadata(topics).right.map { r =>\n+      r.flatMap { tm: TopicMetadata =>\n+        tm.partitionsMetadata.map { pm =>\n+          TopicAndPartition(tm.topic, pm.partitionId)\n+        }    \n+      }\n+    }\n+\n+  def getPartitionMetadata(topics: Set[String]): Either[Err, Set[TopicMetadata]] = {\n+    val req = TopicMetadataRequest(TopicMetadataRequest.CurrentVersion,\n+      0, config.clientId, topics.toSeq)\n+    val errs = new Err\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp: TopicMetadataResponse = consumer.send(req)\n+      // error codes here indicate missing / just created topic,\n+      // repeating on a different broker wont be useful\n+      return Right(resp.topicsMetadata.toSet)\n+    }\n+    Left(errs)\n+  }\n+\n+  def getLatestLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, Long]] =\n+    getLeaderOffsets(topicAndPartitions, OffsetRequest.LatestTime)\n+\n+  def getEarliestLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, Long]] =\n+    getLeaderOffsets(topicAndPartitions, OffsetRequest.EarliestTime)\n+\n+  def getLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition],\n+    before: Long\n+  ): Either[Err, Map[TopicAndPartition, Long]] =\n+    getLeaderOffsets(topicAndPartitions, before, 1).right.map { r =>\n+      r.map { kv =>\n+        // mapValues isnt serializable, see SI-7005\n+        kv._1 -> kv._2.head\n+      }\n+    }\n+\n+  private def flip[K, V](m: Map[K, V]): Map[V, Seq[K]] =\n+    m.groupBy(_._2).map { kv =>\n+      kv._1 -> kv._2.keys.toSeq\n+    }\n+\n+  def getLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition],\n+    before: Long,\n+    maxNumOffsets: Int\n+  ): Either[Err, Map[TopicAndPartition, Seq[Long]]] = {\n+    findLeaders(topicAndPartitions).right.flatMap { tpToLeader =>\n+      val leaderToTp: Map[(String, Int), Seq[TopicAndPartition]] = flip(tpToLeader)\n+      val leaders = leaderToTp.keys\n+      var result = Map[TopicAndPartition, Seq[Long]]()\n+      val errs = new Err\n+      withBrokers(leaders, errs) { consumer =>\n+        val needed: Seq[TopicAndPartition] = leaderToTp((consumer.host, consumer.port))\n+        val req = OffsetRequest(\n+          needed.map { tp =>\n+            tp -> PartitionOffsetRequestInfo(before, maxNumOffsets)\n+          }.toMap\n+        )\n+        val resp = consumer.getOffsetsBefore(req)\n+        val respMap = resp.partitionErrorAndOffsets\n+        needed.foreach { tp =>\n+          respMap.get(tp).foreach { errAndOffsets =>\n+            if (errAndOffsets.error == ErrorMapping.NoError) {\n+              if (errAndOffsets.offsets.nonEmpty) {\n+                result += tp -> errAndOffsets.offsets\n+              } else {\n+                errs.append(new Exception(\n+                  s\"Empty offsets for ${tp}, is ${before} before log beginning?\"))\n+              }\n+            } else {\n+              errs.append(ErrorMapping.exceptionFor(errAndOffsets.error))\n+            }\n+          }\n+        }\n+        if (result.keys.size == topicAndPartitions.size) {\n+          return Right(result)\n+        }\n+      }\n+      val missing = topicAndPartitions.diff(result.keys.toSet)\n+      errs.append(new Exception(s\"Couldn't find leader offsets for ${missing}\"))\n+      Left(errs)\n+    }\n+  }\n+\n+  def getConsumerOffsets(\n+    groupId: String,\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, Long]] = {\n+    getConsumerOffsetMetadata(groupId, topicAndPartitions).right.map { r =>\n+      r.map { kv =>\n+        kv._1 -> kv._2.offset\n+      }\n+    }\n+  }\n+\n+  def getConsumerOffsetMetadata(\n+    groupId: String,\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, OffsetMetadataAndError]] = {\n+    var result = Map[TopicAndPartition, OffsetMetadataAndError]()\n+    val req = OffsetFetchRequest(groupId, topicAndPartitions.toSeq)\n+    val errs = new Err\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp = consumer.fetchOffsets(req)\n+      val respMap = resp.requestInfo\n+      val needed = topicAndPartitions.diff(result.keys.toSet)\n+      needed.foreach { tp =>\n+        respMap.get(tp).foreach { offsetMeta =>\n+          if (offsetMeta.error == ErrorMapping.NoError) {\n+            result += tp -> offsetMeta\n+          } else {\n+            errs.append(ErrorMapping.exceptionFor(offsetMeta.error))\n+          }\n+        }\n+      }\n+      if (result.keys.size == topicAndPartitions.size) {\n+        return Right(result)\n+      }\n+    }\n+    val missing = topicAndPartitions.diff(result.keys.toSet)\n+    errs.append(new Exception(s\"Couldn't find consumer offsets for ${missing}\"))\n+    Left(errs)\n+  }\n+\n+  def setConsumerOffsets(\n+    groupId: String,\n+    offsets: Map[TopicAndPartition, Long]\n+  ): Either[Err, Map[TopicAndPartition, Short]] = {\n+    setConsumerOffsetMetadata(groupId, offsets.map { kv =>\n+      kv._1 -> OffsetMetadataAndError(kv._2)\n+    })\n+  }\n+\n+  def setConsumerOffsetMetadata(\n+    groupId: String,\n+    metadata: Map[TopicAndPartition, OffsetMetadataAndError]\n+  ): Either[Err, Map[TopicAndPartition, Short]] = {\n+    var result = Map[TopicAndPartition, Short]()\n+    val req = OffsetCommitRequest(groupId, metadata)\n+    val errs = new Err\n+    val topicAndPartitions = metadata.keys.toSet\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp = consumer.commitOffsets(req)\n+      val respMap = resp.requestInfo\n+      val needed = topicAndPartitions.diff(result.keys.toSet)\n+      needed.foreach { tp =>\n+        respMap.get(tp).foreach { err =>\n+          if (err == ErrorMapping.NoError) {\n+            result += tp -> err\n+          } else {\n+            errs.append(ErrorMapping.exceptionFor(err))\n+          }\n+        }\n+      }\n+      if (result.keys.size == topicAndPartitions.size) {\n+        return Right(result)"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "That's an early return from withBrokers, which would otherwise keep running the closure on each broker.\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-08T14:23:46Z",
    "diffHunk": "@@ -0,0 +1,313 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.util.control.NonFatal\n+import scala.collection.mutable.ArrayBuffer\n+import java.util.Properties\n+import kafka.api._\n+import kafka.common.{ErrorMapping, OffsetMetadataAndError, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+\n+/**\n+  * Convenience methods for interacting with a Kafka cluster.\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form\n+  */\n+class KafkaCluster(val kafkaParams: Map[String, String]) extends Serializable {\n+  import KafkaCluster.Err\n+\n+  val seedBrokers: Array[(String, Int)] =\n+    kafkaParams.get(\"metadata.broker.list\")\n+      .orElse(kafkaParams.get(\"bootstrap.servers\"))\n+      .getOrElse(throw new Exception(\"Must specify metadata.broker.list or bootstrap.servers\"))\n+      .split(\",\").map { hp =>\n+        val hpa = hp.split(\":\")\n+        (hpa(0), hpa(1).toInt)\n+      }\n+\n+  // ConsumerConfig isn't serializable\n+  @transient private var _config: ConsumerConfig = null\n+\n+  def config: ConsumerConfig = this.synchronized {\n+    if (_config == null) {\n+      _config = KafkaCluster.consumerConfig(kafkaParams)\n+    }\n+    _config\n+  }\n+\n+  def connect(host: String, port: Int): SimpleConsumer =\n+    new SimpleConsumer(host, port, config.socketTimeoutMs,\n+      config.socketReceiveBufferBytes, config.clientId)\n+\n+  def connect(hostAndPort: (String, Int)): SimpleConsumer =\n+    connect(hostAndPort._1, hostAndPort._2)\n+\n+  def connectLeader(topic: String, partition: Int): Either[Err, SimpleConsumer] =\n+    findLeader(topic, partition).right.map(connect)\n+\n+  def findLeader(topic: String, partition: Int): Either[Err, (String, Int)] = {\n+    val req = TopicMetadataRequest(TopicMetadataRequest.CurrentVersion,\n+      0, config.clientId, Seq(topic))\n+    val errs = new Err\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp: TopicMetadataResponse = consumer.send(req)\n+      resp.topicsMetadata.find(_.topic == topic).flatMap { t =>\n+        t.partitionsMetadata.find(_.partitionId == partition)\n+      }.foreach { partitionMeta =>\n+        partitionMeta.leader.foreach { leader =>\n+          return Right((leader.host, leader.port))\n+        }\n+      }\n+    }\n+    Left(errs)\n+  }\n+\n+  def findLeaders(\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, (String, Int)]] = {\n+    getPartitionMetadata(topicAndPartitions.map(_.topic)).right.flatMap { tms =>\n+      val result = tms.flatMap { tm: TopicMetadata =>\n+        tm.partitionsMetadata.flatMap { pm =>\n+          val tp = TopicAndPartition(tm.topic, pm.partitionId)\n+          if (topicAndPartitions(tp)) {\n+            pm.leader.map { l =>\n+              tp -> (l.host -> l.port)\n+            }\n+          } else {\n+            None\n+          }\n+        }\n+      }.toMap\n+      if (result.keys.size == topicAndPartitions.size) {\n+        Right(result)\n+      } else {\n+        val missing = topicAndPartitions.diff(result.keys.toSet)\n+        val err = new Err\n+        err.append(new Exception(s\"Couldn't find leaders for ${missing}\"))\n+        Left(err)\n+      }\n+    }\n+  }\n+\n+  def getPartitions(topics: Set[String]): Either[Err, Set[TopicAndPartition]] =\n+    getPartitionMetadata(topics).right.map { r =>\n+      r.flatMap { tm: TopicMetadata =>\n+        tm.partitionsMetadata.map { pm =>\n+          TopicAndPartition(tm.topic, pm.partitionId)\n+        }    \n+      }\n+    }\n+\n+  def getPartitionMetadata(topics: Set[String]): Either[Err, Set[TopicMetadata]] = {\n+    val req = TopicMetadataRequest(TopicMetadataRequest.CurrentVersion,\n+      0, config.clientId, topics.toSeq)\n+    val errs = new Err\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp: TopicMetadataResponse = consumer.send(req)\n+      // error codes here indicate missing / just created topic,\n+      // repeating on a different broker wont be useful\n+      return Right(resp.topicsMetadata.toSet)\n+    }\n+    Left(errs)\n+  }\n+\n+  def getLatestLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, Long]] =\n+    getLeaderOffsets(topicAndPartitions, OffsetRequest.LatestTime)\n+\n+  def getEarliestLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, Long]] =\n+    getLeaderOffsets(topicAndPartitions, OffsetRequest.EarliestTime)\n+\n+  def getLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition],\n+    before: Long\n+  ): Either[Err, Map[TopicAndPartition, Long]] =\n+    getLeaderOffsets(topicAndPartitions, before, 1).right.map { r =>\n+      r.map { kv =>\n+        // mapValues isnt serializable, see SI-7005\n+        kv._1 -> kv._2.head\n+      }\n+    }\n+\n+  private def flip[K, V](m: Map[K, V]): Map[V, Seq[K]] =\n+    m.groupBy(_._2).map { kv =>\n+      kv._1 -> kv._2.keys.toSeq\n+    }\n+\n+  def getLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition],\n+    before: Long,\n+    maxNumOffsets: Int\n+  ): Either[Err, Map[TopicAndPartition, Seq[Long]]] = {\n+    findLeaders(topicAndPartitions).right.flatMap { tpToLeader =>\n+      val leaderToTp: Map[(String, Int), Seq[TopicAndPartition]] = flip(tpToLeader)\n+      val leaders = leaderToTp.keys\n+      var result = Map[TopicAndPartition, Seq[Long]]()\n+      val errs = new Err\n+      withBrokers(leaders, errs) { consumer =>\n+        val needed: Seq[TopicAndPartition] = leaderToTp((consumer.host, consumer.port))\n+        val req = OffsetRequest(\n+          needed.map { tp =>\n+            tp -> PartitionOffsetRequestInfo(before, maxNumOffsets)\n+          }.toMap\n+        )\n+        val resp = consumer.getOffsetsBefore(req)\n+        val respMap = resp.partitionErrorAndOffsets\n+        needed.foreach { tp =>\n+          respMap.get(tp).foreach { errAndOffsets =>\n+            if (errAndOffsets.error == ErrorMapping.NoError) {\n+              if (errAndOffsets.offsets.nonEmpty) {\n+                result += tp -> errAndOffsets.offsets\n+              } else {\n+                errs.append(new Exception(\n+                  s\"Empty offsets for ${tp}, is ${before} before log beginning?\"))\n+              }\n+            } else {\n+              errs.append(ErrorMapping.exceptionFor(errAndOffsets.error))\n+            }\n+          }\n+        }\n+        if (result.keys.size == topicAndPartitions.size) {\n+          return Right(result)\n+        }\n+      }\n+      val missing = topicAndPartitions.diff(result.keys.toSet)\n+      errs.append(new Exception(s\"Couldn't find leader offsets for ${missing}\"))\n+      Left(errs)\n+    }\n+  }\n+\n+  def getConsumerOffsets(\n+    groupId: String,\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, Long]] = {\n+    getConsumerOffsetMetadata(groupId, topicAndPartitions).right.map { r =>\n+      r.map { kv =>\n+        kv._1 -> kv._2.offset\n+      }\n+    }\n+  }\n+\n+  def getConsumerOffsetMetadata(\n+    groupId: String,\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, OffsetMetadataAndError]] = {\n+    var result = Map[TopicAndPartition, OffsetMetadataAndError]()\n+    val req = OffsetFetchRequest(groupId, topicAndPartitions.toSeq)\n+    val errs = new Err\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp = consumer.fetchOffsets(req)\n+      val respMap = resp.requestInfo\n+      val needed = topicAndPartitions.diff(result.keys.toSet)\n+      needed.foreach { tp =>\n+        respMap.get(tp).foreach { offsetMeta =>\n+          if (offsetMeta.error == ErrorMapping.NoError) {\n+            result += tp -> offsetMeta\n+          } else {\n+            errs.append(ErrorMapping.exceptionFor(offsetMeta.error))\n+          }\n+        }\n+      }\n+      if (result.keys.size == topicAndPartitions.size) {\n+        return Right(result)\n+      }\n+    }\n+    val missing = topicAndPartitions.diff(result.keys.toSet)\n+    errs.append(new Exception(s\"Couldn't find consumer offsets for ${missing}\"))\n+    Left(errs)\n+  }\n+\n+  def setConsumerOffsets(\n+    groupId: String,\n+    offsets: Map[TopicAndPartition, Long]\n+  ): Either[Err, Map[TopicAndPartition, Short]] = {\n+    setConsumerOffsetMetadata(groupId, offsets.map { kv =>\n+      kv._1 -> OffsetMetadataAndError(kv._2)\n+    })\n+  }\n+\n+  def setConsumerOffsetMetadata(\n+    groupId: String,\n+    metadata: Map[TopicAndPartition, OffsetMetadataAndError]\n+  ): Either[Err, Map[TopicAndPartition, Short]] = {\n+    var result = Map[TopicAndPartition, Short]()\n+    val req = OffsetCommitRequest(groupId, metadata)\n+    val errs = new Err\n+    val topicAndPartitions = metadata.keys.toSet\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp = consumer.commitOffsets(req)\n+      val respMap = resp.requestInfo\n+      val needed = topicAndPartitions.diff(result.keys.toSet)\n+      needed.foreach { tp =>\n+        respMap.get(tp).foreach { err =>\n+          if (err == ErrorMapping.NoError) {\n+            result += tp -> err\n+          } else {\n+            errs.append(ErrorMapping.exceptionFor(err))\n+          }\n+        }\n+      }\n+      if (result.keys.size == topicAndPartitions.size) {\n+        return Right(result)"
  }],
  "prId": 3798
}, {
  "comments": [{
    "author": {
      "login": "helena"
    },
    "body": "Do a { case (k,v) => vs accessing the tuples as ._1 etc\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-08T12:56:19Z",
    "diffHunk": "@@ -0,0 +1,313 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.util.control.NonFatal\n+import scala.collection.mutable.ArrayBuffer\n+import java.util.Properties\n+import kafka.api._\n+import kafka.common.{ErrorMapping, OffsetMetadataAndError, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+\n+/**\n+  * Convenience methods for interacting with a Kafka cluster.\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form\n+  */\n+class KafkaCluster(val kafkaParams: Map[String, String]) extends Serializable {\n+  import KafkaCluster.Err\n+\n+  val seedBrokers: Array[(String, Int)] =\n+    kafkaParams.get(\"metadata.broker.list\")\n+      .orElse(kafkaParams.get(\"bootstrap.servers\"))\n+      .getOrElse(throw new Exception(\"Must specify metadata.broker.list or bootstrap.servers\"))\n+      .split(\",\").map { hp =>\n+        val hpa = hp.split(\":\")\n+        (hpa(0), hpa(1).toInt)\n+      }\n+\n+  // ConsumerConfig isn't serializable\n+  @transient private var _config: ConsumerConfig = null\n+\n+  def config: ConsumerConfig = this.synchronized {\n+    if (_config == null) {\n+      _config = KafkaCluster.consumerConfig(kafkaParams)\n+    }\n+    _config\n+  }\n+\n+  def connect(host: String, port: Int): SimpleConsumer =\n+    new SimpleConsumer(host, port, config.socketTimeoutMs,\n+      config.socketReceiveBufferBytes, config.clientId)\n+\n+  def connect(hostAndPort: (String, Int)): SimpleConsumer =\n+    connect(hostAndPort._1, hostAndPort._2)\n+\n+  def connectLeader(topic: String, partition: Int): Either[Err, SimpleConsumer] =\n+    findLeader(topic, partition).right.map(connect)\n+\n+  def findLeader(topic: String, partition: Int): Either[Err, (String, Int)] = {\n+    val req = TopicMetadataRequest(TopicMetadataRequest.CurrentVersion,\n+      0, config.clientId, Seq(topic))\n+    val errs = new Err\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp: TopicMetadataResponse = consumer.send(req)\n+      resp.topicsMetadata.find(_.topic == topic).flatMap { t =>\n+        t.partitionsMetadata.find(_.partitionId == partition)\n+      }.foreach { partitionMeta =>\n+        partitionMeta.leader.foreach { leader =>\n+          return Right((leader.host, leader.port))\n+        }\n+      }\n+    }\n+    Left(errs)\n+  }\n+\n+  def findLeaders(\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, (String, Int)]] = {\n+    getPartitionMetadata(topicAndPartitions.map(_.topic)).right.flatMap { tms =>\n+      val result = tms.flatMap { tm: TopicMetadata =>\n+        tm.partitionsMetadata.flatMap { pm =>\n+          val tp = TopicAndPartition(tm.topic, pm.partitionId)\n+          if (topicAndPartitions(tp)) {\n+            pm.leader.map { l =>\n+              tp -> (l.host -> l.port)\n+            }\n+          } else {\n+            None\n+          }\n+        }\n+      }.toMap\n+      if (result.keys.size == topicAndPartitions.size) {\n+        Right(result)\n+      } else {\n+        val missing = topicAndPartitions.diff(result.keys.toSet)\n+        val err = new Err\n+        err.append(new Exception(s\"Couldn't find leaders for ${missing}\"))\n+        Left(err)\n+      }\n+    }\n+  }\n+\n+  def getPartitions(topics: Set[String]): Either[Err, Set[TopicAndPartition]] =\n+    getPartitionMetadata(topics).right.map { r =>\n+      r.flatMap { tm: TopicMetadata =>\n+        tm.partitionsMetadata.map { pm =>\n+          TopicAndPartition(tm.topic, pm.partitionId)\n+        }    \n+      }\n+    }\n+\n+  def getPartitionMetadata(topics: Set[String]): Either[Err, Set[TopicMetadata]] = {\n+    val req = TopicMetadataRequest(TopicMetadataRequest.CurrentVersion,\n+      0, config.clientId, topics.toSeq)\n+    val errs = new Err\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp: TopicMetadataResponse = consumer.send(req)\n+      // error codes here indicate missing / just created topic,\n+      // repeating on a different broker wont be useful\n+      return Right(resp.topicsMetadata.toSet)\n+    }\n+    Left(errs)\n+  }\n+\n+  def getLatestLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, Long]] =\n+    getLeaderOffsets(topicAndPartitions, OffsetRequest.LatestTime)\n+\n+  def getEarliestLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, Long]] =\n+    getLeaderOffsets(topicAndPartitions, OffsetRequest.EarliestTime)\n+\n+  def getLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition],\n+    before: Long\n+  ): Either[Err, Map[TopicAndPartition, Long]] =\n+    getLeaderOffsets(topicAndPartitions, before, 1).right.map { r =>\n+      r.map { kv =>\n+        // mapValues isnt serializable, see SI-7005\n+        kv._1 -> kv._2.head\n+      }\n+    }\n+\n+  private def flip[K, V](m: Map[K, V]): Map[V, Seq[K]] =\n+    m.groupBy(_._2).map { kv =>\n+      kv._1 -> kv._2.keys.toSeq\n+    }\n+\n+  def getLeaderOffsets(\n+    topicAndPartitions: Set[TopicAndPartition],\n+    before: Long,\n+    maxNumOffsets: Int\n+  ): Either[Err, Map[TopicAndPartition, Seq[Long]]] = {\n+    findLeaders(topicAndPartitions).right.flatMap { tpToLeader =>\n+      val leaderToTp: Map[(String, Int), Seq[TopicAndPartition]] = flip(tpToLeader)\n+      val leaders = leaderToTp.keys\n+      var result = Map[TopicAndPartition, Seq[Long]]()\n+      val errs = new Err\n+      withBrokers(leaders, errs) { consumer =>\n+        val needed: Seq[TopicAndPartition] = leaderToTp((consumer.host, consumer.port))\n+        val req = OffsetRequest(\n+          needed.map { tp =>\n+            tp -> PartitionOffsetRequestInfo(before, maxNumOffsets)\n+          }.toMap\n+        )\n+        val resp = consumer.getOffsetsBefore(req)\n+        val respMap = resp.partitionErrorAndOffsets\n+        needed.foreach { tp =>\n+          respMap.get(tp).foreach { errAndOffsets =>\n+            if (errAndOffsets.error == ErrorMapping.NoError) {\n+              if (errAndOffsets.offsets.nonEmpty) {\n+                result += tp -> errAndOffsets.offsets\n+              } else {\n+                errs.append(new Exception(\n+                  s\"Empty offsets for ${tp}, is ${before} before log beginning?\"))\n+              }\n+            } else {\n+              errs.append(ErrorMapping.exceptionFor(errAndOffsets.error))\n+            }\n+          }\n+        }\n+        if (result.keys.size == topicAndPartitions.size) {\n+          return Right(result)\n+        }\n+      }\n+      val missing = topicAndPartitions.diff(result.keys.toSet)\n+      errs.append(new Exception(s\"Couldn't find leader offsets for ${missing}\"))\n+      Left(errs)\n+    }\n+  }\n+\n+  def getConsumerOffsets(\n+    groupId: String,\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, Long]] = {\n+    getConsumerOffsetMetadata(groupId, topicAndPartitions).right.map { r =>\n+      r.map { kv =>\n+        kv._1 -> kv._2.offset\n+      }\n+    }\n+  }\n+\n+  def getConsumerOffsetMetadata(\n+    groupId: String,\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, OffsetMetadataAndError]] = {\n+    var result = Map[TopicAndPartition, OffsetMetadataAndError]()\n+    val req = OffsetFetchRequest(groupId, topicAndPartitions.toSeq)\n+    val errs = new Err\n+    withBrokers(seedBrokers, errs) { consumer =>\n+      val resp = consumer.fetchOffsets(req)\n+      val respMap = resp.requestInfo\n+      val needed = topicAndPartitions.diff(result.keys.toSet)\n+      needed.foreach { tp =>\n+        respMap.get(tp).foreach { offsetMeta =>\n+          if (offsetMeta.error == ErrorMapping.NoError) {\n+            result += tp -> offsetMeta\n+          } else {\n+            errs.append(ErrorMapping.exceptionFor(offsetMeta.error))\n+          }\n+        }\n+      }\n+      if (result.keys.size == topicAndPartitions.size) {\n+        return Right(result)\n+      }\n+    }\n+    val missing = topicAndPartitions.diff(result.keys.toSet)\n+    errs.append(new Exception(s\"Couldn't find consumer offsets for ${missing}\"))\n+    Left(errs)\n+  }\n+\n+  def setConsumerOffsets(\n+    groupId: String,\n+    offsets: Map[TopicAndPartition, Long]\n+  ): Either[Err, Map[TopicAndPartition, Short]] = {\n+    setConsumerOffsetMetadata(groupId, offsets.map { kv =>"
  }],
  "prId": 3798
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "can you turn all scaladoc style into javadoc for this pr? \n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-26T22:49:56Z",
    "diffHunk": "@@ -0,0 +1,318 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.util.control.NonFatal\n+import scala.util.Random\n+import scala.collection.mutable.ArrayBuffer\n+import java.util.Properties\n+import kafka.api._\n+import kafka.common.{ErrorMapping, OffsetMetadataAndError, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+\n+/**"
  }],
  "prId": 3798
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "better remove this method since it doesn't do much ...\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-26T22:52:34Z",
    "diffHunk": "@@ -0,0 +1,318 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.util.control.NonFatal\n+import scala.util.Random\n+import scala.collection.mutable.ArrayBuffer\n+import java.util.Properties\n+import kafka.api._\n+import kafka.common.{ErrorMapping, OffsetMetadataAndError, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+\n+/**\n+  * Convenience methods for interacting with a Kafka cluster.\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form\n+  */\n+class KafkaCluster(val kafkaParams: Map[String, String]) extends Serializable {\n+  import KafkaCluster.{Err, LeaderOffset}\n+\n+  val seedBrokers: Array[(String, Int)] =\n+    kafkaParams.get(\"metadata.broker.list\")\n+      .orElse(kafkaParams.get(\"bootstrap.servers\"))\n+      .getOrElse(throw new Exception(\"Must specify metadata.broker.list or bootstrap.servers\"))\n+      .split(\",\").map { hp =>\n+        val hpa = hp.split(\":\")\n+        (hpa(0), hpa(1).toInt)\n+      }\n+\n+  // ConsumerConfig isn't serializable\n+  @transient private var _config: ConsumerConfig = null\n+\n+  def config: ConsumerConfig = this.synchronized {\n+    if (_config == null) {\n+      _config = KafkaCluster.consumerConfig(kafkaParams)\n+    }\n+    _config\n+  }\n+\n+  def connect(host: String, port: Int): SimpleConsumer =\n+    new SimpleConsumer(host, port, config.socketTimeoutMs,\n+      config.socketReceiveBufferBytes, config.clientId)\n+\n+  def connect(hostAndPort: (String, Int)): SimpleConsumer ="
  }],
  "prId": 3798
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "this block of code is pretty hard to understand with so many level of nesting. can you rewrite it? maybe by introducing variables and adding comments to explain what is going on. overall I feel this PR went slightly overboard with Scala. With no explicit type, intermediate variable, and comment, it is pretty hard to understand a lot of blocks\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-27T04:24:29Z",
    "diffHunk": "@@ -0,0 +1,318 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.util.control.NonFatal\n+import scala.util.Random\n+import scala.collection.mutable.ArrayBuffer\n+import java.util.Properties\n+import kafka.api._\n+import kafka.common.{ErrorMapping, OffsetMetadataAndError, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+\n+/**\n+  * Convenience methods for interacting with a Kafka cluster.\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form\n+  */\n+class KafkaCluster(val kafkaParams: Map[String, String]) extends Serializable {\n+  import KafkaCluster.{Err, LeaderOffset}\n+\n+  val seedBrokers: Array[(String, Int)] =\n+    kafkaParams.get(\"metadata.broker.list\")\n+      .orElse(kafkaParams.get(\"bootstrap.servers\"))\n+      .getOrElse(throw new Exception(\"Must specify metadata.broker.list or bootstrap.servers\"))\n+      .split(\",\").map { hp =>\n+        val hpa = hp.split(\":\")\n+        (hpa(0), hpa(1).toInt)\n+      }\n+\n+  // ConsumerConfig isn't serializable\n+  @transient private var _config: ConsumerConfig = null\n+\n+  def config: ConsumerConfig = this.synchronized {\n+    if (_config == null) {\n+      _config = KafkaCluster.consumerConfig(kafkaParams)\n+    }\n+    _config\n+  }\n+\n+  def connect(host: String, port: Int): SimpleConsumer =\n+    new SimpleConsumer(host, port, config.socketTimeoutMs,\n+      config.socketReceiveBufferBytes, config.clientId)\n+\n+  def connect(hostAndPort: (String, Int)): SimpleConsumer =\n+    connect(hostAndPort._1, hostAndPort._2)\n+\n+  def connectLeader(topic: String, partition: Int): Either[Err, SimpleConsumer] =\n+    findLeader(topic, partition).right.map(connect)\n+\n+  def findLeader(topic: String, partition: Int): Either[Err, (String, Int)] = {\n+    val req = TopicMetadataRequest(TopicMetadataRequest.CurrentVersion,\n+      0, config.clientId, Seq(topic))\n+    val errs = new Err\n+    withBrokers(Random.shuffle(seedBrokers), errs) { consumer =>\n+      val resp: TopicMetadataResponse = consumer.send(req)\n+      resp.topicsMetadata.find(_.topic == topic).flatMap { t =>"
  }],
  "prId": 3798
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "this block also\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-27T04:26:15Z",
    "diffHunk": "@@ -0,0 +1,318 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.util.control.NonFatal\n+import scala.util.Random\n+import scala.collection.mutable.ArrayBuffer\n+import java.util.Properties\n+import kafka.api._\n+import kafka.common.{ErrorMapping, OffsetMetadataAndError, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+\n+/**\n+  * Convenience methods for interacting with a Kafka cluster.\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form\n+  */\n+class KafkaCluster(val kafkaParams: Map[String, String]) extends Serializable {\n+  import KafkaCluster.{Err, LeaderOffset}\n+\n+  val seedBrokers: Array[(String, Int)] =\n+    kafkaParams.get(\"metadata.broker.list\")\n+      .orElse(kafkaParams.get(\"bootstrap.servers\"))\n+      .getOrElse(throw new Exception(\"Must specify metadata.broker.list or bootstrap.servers\"))\n+      .split(\",\").map { hp =>\n+        val hpa = hp.split(\":\")\n+        (hpa(0), hpa(1).toInt)\n+      }\n+\n+  // ConsumerConfig isn't serializable\n+  @transient private var _config: ConsumerConfig = null\n+\n+  def config: ConsumerConfig = this.synchronized {\n+    if (_config == null) {\n+      _config = KafkaCluster.consumerConfig(kafkaParams)\n+    }\n+    _config\n+  }\n+\n+  def connect(host: String, port: Int): SimpleConsumer =\n+    new SimpleConsumer(host, port, config.socketTimeoutMs,\n+      config.socketReceiveBufferBytes, config.clientId)\n+\n+  def connect(hostAndPort: (String, Int)): SimpleConsumer =\n+    connect(hostAndPort._1, hostAndPort._2)\n+\n+  def connectLeader(topic: String, partition: Int): Either[Err, SimpleConsumer] =\n+    findLeader(topic, partition).right.map(connect)\n+\n+  def findLeader(topic: String, partition: Int): Either[Err, (String, Int)] = {\n+    val req = TopicMetadataRequest(TopicMetadataRequest.CurrentVersion,\n+      0, config.clientId, Seq(topic))\n+    val errs = new Err\n+    withBrokers(Random.shuffle(seedBrokers), errs) { consumer =>\n+      val resp: TopicMetadataResponse = consumer.send(req)\n+      resp.topicsMetadata.find(_.topic == topic).flatMap { t =>\n+        t.partitionsMetadata.find(_.partitionId == partition)\n+      }.foreach { partitionMeta =>\n+        partitionMeta.leader.foreach { leader =>\n+          return Right((leader.host, leader.port))\n+        }\n+      }\n+    }\n+    Left(errs)\n+  }\n+\n+  def findLeaders(\n+    topicAndPartitions: Set[TopicAndPartition]\n+  ): Either[Err, Map[TopicAndPartition, (String, Int)]] = {\n+    getPartitionMetadata(topicAndPartitions.map(_.topic)).right.flatMap { tms =>\n+      val result = tms.flatMap { tm: TopicMetadata =>\n+        tm.partitionsMetadata.flatMap { pm =>"
  }],
  "prId": 3798
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Why does this need to be serializable?\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-29T02:57:03Z",
    "diffHunk": "@@ -0,0 +1,320 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.util.control.NonFatal\n+import scala.util.Random\n+import scala.collection.mutable.ArrayBuffer\n+import java.util.Properties\n+import kafka.api._\n+import kafka.common.{ErrorMapping, OffsetMetadataAndError, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+\n+/**\n+  * Convenience methods for interacting with a Kafka cluster.\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form\n+  */\n+private[spark]\n+class KafkaCluster(val kafkaParams: Map[String, String]) extends Serializable {"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "As I recall, it's because the stream has a kafka cluster as a member value, and it needs to be able to be checkpointed.  The current design of KafkaCluster is essentially stateless aside from configuration.\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-29T04:38:12Z",
    "diffHunk": "@@ -0,0 +1,320 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.util.control.NonFatal\n+import scala.util.Random\n+import scala.collection.mutable.ArrayBuffer\n+import java.util.Properties\n+import kafka.api._\n+import kafka.common.{ErrorMapping, OffsetMetadataAndError, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+\n+/**\n+  * Convenience methods for interacting with a Kafka cluster.\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form\n+  */\n+private[spark]\n+class KafkaCluster(val kafkaParams: Map[String, String]) extends Serializable {"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Hmmm, that increases the checkpoint size. Though not by much. A utility class like this should not be serialized with the DAG of DStreams, but that's okay for now. We can deal with later. \n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-29T05:24:46Z",
    "diffHunk": "@@ -0,0 +1,320 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.util.control.NonFatal\n+import scala.util.Random\n+import scala.collection.mutable.ArrayBuffer\n+import java.util.Properties\n+import kafka.api._\n+import kafka.common.{ErrorMapping, OffsetMetadataAndError, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+\n+/**\n+  * Convenience methods for interacting with a Kafka cluster.\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form\n+  */\n+private[spark]\n+class KafkaCluster(val kafkaParams: Map[String, String]) extends Serializable {"
  }],
  "prId": 3798
}]