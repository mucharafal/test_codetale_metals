[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Please use Scala string interpolation.\n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-11T02:48:48Z",
    "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.util.Properties\n+import java.util.concurrent.Executors\n+\n+import scala.collection.Map\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.consumer._\n+import kafka.serializer.Decoder\n+import kafka.utils.VerifiableProperties\n+import kafka.utils.ZKStringSerializer\n+import org.I0Itec.zkclient._\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.receiver.Receiver\n+\n+private[streaming]\n+class KafkaReceiver[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag](\n+    kafkaParams: Map[String, String],\n+    topics: Map[String, Int],\n+    storageLevel: StorageLevel\n+  ) extends Receiver[Any](storageLevel) with Logging {\n+\n+  // Connection to Kafka\n+  var consumerConnector: ConsumerConnector = null\n+\n+  def onStop() {\n+    if (consumerConnector != null) {\n+      consumerConnector.shutdown()\n+    }\n+  }\n+\n+  def onStart() {\n+\n+    logInfo(\"Starting Kafka Consumer Stream with group: \" + kafkaParams(\"group.id\"))\n+\n+    // Kafka connection properties\n+    val props = new Properties()\n+    kafkaParams.foreach(param => props.put(param._1, param._2))\n+\n+    val zkConnect = kafkaParams(\"zookeeper.connect\")\n+    // Create the connection to the cluster\n+    logInfo(\"Connecting to Zookeeper: \" + zkConnect)\n+    val consumerConfig = new ConsumerConfig(props)\n+    consumerConnector = Consumer.create(consumerConfig)\n+    logInfo(\"Connected to \" + zkConnect)\n+\n+    // When auto.offset.reset is defined, it is our responsibility to try and whack the\n+    // consumer group zk node.\n+    if (kafkaParams.contains(\"auto.offset.reset\")) {\n+      tryZookeeperConsumerGroupCleanup(zkConnect, kafkaParams(\"group.id\"))\n+    }\n+\n+    val keyDecoder = classTag[U].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+      .newInstance(consumerConfig.props)\n+      .asInstanceOf[Decoder[K]]\n+    val valueDecoder = classTag[T].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+      .newInstance(consumerConfig.props)\n+      .asInstanceOf[Decoder[V]]\n+\n+    // Create Threads for each Topic/Message Stream we are listening\n+    val topicMessageStreams = consumerConnector.createMessageStreams(\n+      topics, keyDecoder, valueDecoder)\n+\n+    val executorPool = Executors.newFixedThreadPool(topics.values.sum)\n+    try {\n+      // Start the messages handler for each partition\n+      topicMessageStreams.values.foreach { streams =>\n+        streams.foreach { stream => executorPool.submit(new MessageHandler(stream)) }\n+      }\n+    } finally {\n+      executorPool.shutdown() // Just causes threads to terminate after work is done\n+    }\n+  }\n+\n+  // Handles Kafka Messages\n+  private class MessageHandler[K: ClassTag, V: ClassTag](stream: KafkaStream[K, V])\n+    extends Runnable {\n+    def run() {\n+      logInfo(\"Starting MessageHandler.\")\n+      try {\n+        for (msgAndMetadata <- stream) {\n+          store((msgAndMetadata.key, msgAndMetadata.message))\n+        }\n+      } catch {\n+        case e: Throwable => logError(\"Error handling message; exiting\", e)\n+      }\n+    }\n+  }\n+\n+  // It is our responsibility to delete the consumer group when specifying auto.offset.reset. This\n+  // is because Kafka 0.7.2 only honors this param when the group is not in zookeeper.\n+  //\n+  // The kafka high level consumer doesn't expose setting offsets currently, this is a trick copied\n+  // from Kafka's ConsoleConsumer. See code related to 'auto.offset.reset' when it is set to\n+  // 'smallest'/'largest':\n+  // scalastyle:off\n+  // https://github.com/apache/kafka/blob/0.7.2/core/src/main/scala/kafka/consumer/ConsoleConsumer.scala\n+  // scalastyle:on\n+  private def tryZookeeperConsumerGroupCleanup(zkUrl: String, groupId: String) {\n+    val dir = \"/consumers/\" + groupId\n+    logInfo(\"Cleaning up temporary Zookeeper data under \" + dir + \".\")"
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "Hi TD, this is a quite old code and should be removed since we already update Kafka to 0.8, would you mind taking a look at this PR #1420 , it is pending for a long while.\n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-11T03:07:32Z",
    "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.util.Properties\n+import java.util.concurrent.Executors\n+\n+import scala.collection.Map\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.consumer._\n+import kafka.serializer.Decoder\n+import kafka.utils.VerifiableProperties\n+import kafka.utils.ZKStringSerializer\n+import org.I0Itec.zkclient._\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.receiver.Receiver\n+\n+private[streaming]\n+class KafkaReceiver[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag](\n+    kafkaParams: Map[String, String],\n+    topics: Map[String, Int],\n+    storageLevel: StorageLevel\n+  ) extends Receiver[Any](storageLevel) with Logging {\n+\n+  // Connection to Kafka\n+  var consumerConnector: ConsumerConnector = null\n+\n+  def onStop() {\n+    if (consumerConnector != null) {\n+      consumerConnector.shutdown()\n+    }\n+  }\n+\n+  def onStart() {\n+\n+    logInfo(\"Starting Kafka Consumer Stream with group: \" + kafkaParams(\"group.id\"))\n+\n+    // Kafka connection properties\n+    val props = new Properties()\n+    kafkaParams.foreach(param => props.put(param._1, param._2))\n+\n+    val zkConnect = kafkaParams(\"zookeeper.connect\")\n+    // Create the connection to the cluster\n+    logInfo(\"Connecting to Zookeeper: \" + zkConnect)\n+    val consumerConfig = new ConsumerConfig(props)\n+    consumerConnector = Consumer.create(consumerConfig)\n+    logInfo(\"Connected to \" + zkConnect)\n+\n+    // When auto.offset.reset is defined, it is our responsibility to try and whack the\n+    // consumer group zk node.\n+    if (kafkaParams.contains(\"auto.offset.reset\")) {\n+      tryZookeeperConsumerGroupCleanup(zkConnect, kafkaParams(\"group.id\"))\n+    }\n+\n+    val keyDecoder = classTag[U].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+      .newInstance(consumerConfig.props)\n+      .asInstanceOf[Decoder[K]]\n+    val valueDecoder = classTag[T].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+      .newInstance(consumerConfig.props)\n+      .asInstanceOf[Decoder[V]]\n+\n+    // Create Threads for each Topic/Message Stream we are listening\n+    val topicMessageStreams = consumerConnector.createMessageStreams(\n+      topics, keyDecoder, valueDecoder)\n+\n+    val executorPool = Executors.newFixedThreadPool(topics.values.sum)\n+    try {\n+      // Start the messages handler for each partition\n+      topicMessageStreams.values.foreach { streams =>\n+        streams.foreach { stream => executorPool.submit(new MessageHandler(stream)) }\n+      }\n+    } finally {\n+      executorPool.shutdown() // Just causes threads to terminate after work is done\n+    }\n+  }\n+\n+  // Handles Kafka Messages\n+  private class MessageHandler[K: ClassTag, V: ClassTag](stream: KafkaStream[K, V])\n+    extends Runnable {\n+    def run() {\n+      logInfo(\"Starting MessageHandler.\")\n+      try {\n+        for (msgAndMetadata <- stream) {\n+          store((msgAndMetadata.key, msgAndMetadata.message))\n+        }\n+      } catch {\n+        case e: Throwable => logError(\"Error handling message; exiting\", e)\n+      }\n+    }\n+  }\n+\n+  // It is our responsibility to delete the consumer group when specifying auto.offset.reset. This\n+  // is because Kafka 0.7.2 only honors this param when the group is not in zookeeper.\n+  //\n+  // The kafka high level consumer doesn't expose setting offsets currently, this is a trick copied\n+  // from Kafka's ConsoleConsumer. See code related to 'auto.offset.reset' when it is set to\n+  // 'smallest'/'largest':\n+  // scalastyle:off\n+  // https://github.com/apache/kafka/blob/0.7.2/core/src/main/scala/kafka/consumer/ConsoleConsumer.scala\n+  // scalastyle:on\n+  private def tryZookeeperConsumerGroupCleanup(zkUrl: String, groupId: String) {\n+    val dir = \"/consumers/\" + groupId\n+    logInfo(\"Cleaning up temporary Zookeeper data under \" + dir + \".\")"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Right, sorry, I should merge that. But if I merge that, you will have to update this PR.\n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-11T03:31:08Z",
    "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.util.Properties\n+import java.util.concurrent.Executors\n+\n+import scala.collection.Map\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.consumer._\n+import kafka.serializer.Decoder\n+import kafka.utils.VerifiableProperties\n+import kafka.utils.ZKStringSerializer\n+import org.I0Itec.zkclient._\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.receiver.Receiver\n+\n+private[streaming]\n+class KafkaReceiver[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag](\n+    kafkaParams: Map[String, String],\n+    topics: Map[String, Int],\n+    storageLevel: StorageLevel\n+  ) extends Receiver[Any](storageLevel) with Logging {\n+\n+  // Connection to Kafka\n+  var consumerConnector: ConsumerConnector = null\n+\n+  def onStop() {\n+    if (consumerConnector != null) {\n+      consumerConnector.shutdown()\n+    }\n+  }\n+\n+  def onStart() {\n+\n+    logInfo(\"Starting Kafka Consumer Stream with group: \" + kafkaParams(\"group.id\"))\n+\n+    // Kafka connection properties\n+    val props = new Properties()\n+    kafkaParams.foreach(param => props.put(param._1, param._2))\n+\n+    val zkConnect = kafkaParams(\"zookeeper.connect\")\n+    // Create the connection to the cluster\n+    logInfo(\"Connecting to Zookeeper: \" + zkConnect)\n+    val consumerConfig = new ConsumerConfig(props)\n+    consumerConnector = Consumer.create(consumerConfig)\n+    logInfo(\"Connected to \" + zkConnect)\n+\n+    // When auto.offset.reset is defined, it is our responsibility to try and whack the\n+    // consumer group zk node.\n+    if (kafkaParams.contains(\"auto.offset.reset\")) {\n+      tryZookeeperConsumerGroupCleanup(zkConnect, kafkaParams(\"group.id\"))\n+    }\n+\n+    val keyDecoder = classTag[U].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+      .newInstance(consumerConfig.props)\n+      .asInstanceOf[Decoder[K]]\n+    val valueDecoder = classTag[T].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+      .newInstance(consumerConfig.props)\n+      .asInstanceOf[Decoder[V]]\n+\n+    // Create Threads for each Topic/Message Stream we are listening\n+    val topicMessageStreams = consumerConnector.createMessageStreams(\n+      topics, keyDecoder, valueDecoder)\n+\n+    val executorPool = Executors.newFixedThreadPool(topics.values.sum)\n+    try {\n+      // Start the messages handler for each partition\n+      topicMessageStreams.values.foreach { streams =>\n+        streams.foreach { stream => executorPool.submit(new MessageHandler(stream)) }\n+      }\n+    } finally {\n+      executorPool.shutdown() // Just causes threads to terminate after work is done\n+    }\n+  }\n+\n+  // Handles Kafka Messages\n+  private class MessageHandler[K: ClassTag, V: ClassTag](stream: KafkaStream[K, V])\n+    extends Runnable {\n+    def run() {\n+      logInfo(\"Starting MessageHandler.\")\n+      try {\n+        for (msgAndMetadata <- stream) {\n+          store((msgAndMetadata.key, msgAndMetadata.message))\n+        }\n+      } catch {\n+        case e: Throwable => logError(\"Error handling message; exiting\", e)\n+      }\n+    }\n+  }\n+\n+  // It is our responsibility to delete the consumer group when specifying auto.offset.reset. This\n+  // is because Kafka 0.7.2 only honors this param when the group is not in zookeeper.\n+  //\n+  // The kafka high level consumer doesn't expose setting offsets currently, this is a trick copied\n+  // from Kafka's ConsoleConsumer. See code related to 'auto.offset.reset' when it is set to\n+  // 'smallest'/'largest':\n+  // scalastyle:off\n+  // https://github.com/apache/kafka/blob/0.7.2/core/src/main/scala/kafka/consumer/ConsoleConsumer.scala\n+  // scalastyle:on\n+  private def tryZookeeperConsumerGroupCleanup(zkUrl: String, groupId: String) {\n+    val dir = \"/consumers/\" + groupId\n+    logInfo(\"Cleaning up temporary Zookeeper data under \" + dir + \".\")"
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "Yeah, I will.\n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-11T06:35:50Z",
    "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.util.Properties\n+import java.util.concurrent.Executors\n+\n+import scala.collection.Map\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.consumer._\n+import kafka.serializer.Decoder\n+import kafka.utils.VerifiableProperties\n+import kafka.utils.ZKStringSerializer\n+import org.I0Itec.zkclient._\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.receiver.Receiver\n+\n+private[streaming]\n+class KafkaReceiver[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag](\n+    kafkaParams: Map[String, String],\n+    topics: Map[String, Int],\n+    storageLevel: StorageLevel\n+  ) extends Receiver[Any](storageLevel) with Logging {\n+\n+  // Connection to Kafka\n+  var consumerConnector: ConsumerConnector = null\n+\n+  def onStop() {\n+    if (consumerConnector != null) {\n+      consumerConnector.shutdown()\n+    }\n+  }\n+\n+  def onStart() {\n+\n+    logInfo(\"Starting Kafka Consumer Stream with group: \" + kafkaParams(\"group.id\"))\n+\n+    // Kafka connection properties\n+    val props = new Properties()\n+    kafkaParams.foreach(param => props.put(param._1, param._2))\n+\n+    val zkConnect = kafkaParams(\"zookeeper.connect\")\n+    // Create the connection to the cluster\n+    logInfo(\"Connecting to Zookeeper: \" + zkConnect)\n+    val consumerConfig = new ConsumerConfig(props)\n+    consumerConnector = Consumer.create(consumerConfig)\n+    logInfo(\"Connected to \" + zkConnect)\n+\n+    // When auto.offset.reset is defined, it is our responsibility to try and whack the\n+    // consumer group zk node.\n+    if (kafkaParams.contains(\"auto.offset.reset\")) {\n+      tryZookeeperConsumerGroupCleanup(zkConnect, kafkaParams(\"group.id\"))\n+    }\n+\n+    val keyDecoder = classTag[U].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+      .newInstance(consumerConfig.props)\n+      .asInstanceOf[Decoder[K]]\n+    val valueDecoder = classTag[T].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+      .newInstance(consumerConfig.props)\n+      .asInstanceOf[Decoder[V]]\n+\n+    // Create Threads for each Topic/Message Stream we are listening\n+    val topicMessageStreams = consumerConnector.createMessageStreams(\n+      topics, keyDecoder, valueDecoder)\n+\n+    val executorPool = Executors.newFixedThreadPool(topics.values.sum)\n+    try {\n+      // Start the messages handler for each partition\n+      topicMessageStreams.values.foreach { streams =>\n+        streams.foreach { stream => executorPool.submit(new MessageHandler(stream)) }\n+      }\n+    } finally {\n+      executorPool.shutdown() // Just causes threads to terminate after work is done\n+    }\n+  }\n+\n+  // Handles Kafka Messages\n+  private class MessageHandler[K: ClassTag, V: ClassTag](stream: KafkaStream[K, V])\n+    extends Runnable {\n+    def run() {\n+      logInfo(\"Starting MessageHandler.\")\n+      try {\n+        for (msgAndMetadata <- stream) {\n+          store((msgAndMetadata.key, msgAndMetadata.message))\n+        }\n+      } catch {\n+        case e: Throwable => logError(\"Error handling message; exiting\", e)\n+      }\n+    }\n+  }\n+\n+  // It is our responsibility to delete the consumer group when specifying auto.offset.reset. This\n+  // is because Kafka 0.7.2 only honors this param when the group is not in zookeeper.\n+  //\n+  // The kafka high level consumer doesn't expose setting offsets currently, this is a trick copied\n+  // from Kafka's ConsoleConsumer. See code related to 'auto.offset.reset' when it is set to\n+  // 'smallest'/'largest':\n+  // scalastyle:off\n+  // https://github.com/apache/kafka/blob/0.7.2/core/src/main/scala/kafka/consumer/ConsoleConsumer.scala\n+  // scalastyle:on\n+  private def tryZookeeperConsumerGroupCleanup(zkUrl: String, groupId: String) {\n+    val dir = \"/consumers/\" + groupId\n+    logInfo(\"Cleaning up temporary Zookeeper data under \" + dir + \".\")"
  }],
  "prId": 2991
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Removed unnecessary capitalization in threads, topic, message, stream\n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-11T03:22:09Z",
    "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.util.Properties\n+import java.util.concurrent.Executors\n+\n+import scala.collection.Map\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.consumer._\n+import kafka.serializer.Decoder\n+import kafka.utils.VerifiableProperties\n+import kafka.utils.ZKStringSerializer\n+import org.I0Itec.zkclient._\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.receiver.Receiver\n+\n+private[streaming]\n+class KafkaReceiver[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag](\n+    kafkaParams: Map[String, String],\n+    topics: Map[String, Int],\n+    storageLevel: StorageLevel\n+  ) extends Receiver[Any](storageLevel) with Logging {\n+\n+  // Connection to Kafka\n+  var consumerConnector: ConsumerConnector = null\n+\n+  def onStop() {\n+    if (consumerConnector != null) {\n+      consumerConnector.shutdown()\n+    }\n+  }\n+\n+  def onStart() {\n+\n+    logInfo(\"Starting Kafka Consumer Stream with group: \" + kafkaParams(\"group.id\"))\n+\n+    // Kafka connection properties\n+    val props = new Properties()\n+    kafkaParams.foreach(param => props.put(param._1, param._2))\n+\n+    val zkConnect = kafkaParams(\"zookeeper.connect\")\n+    // Create the connection to the cluster\n+    logInfo(\"Connecting to Zookeeper: \" + zkConnect)\n+    val consumerConfig = new ConsumerConfig(props)\n+    consumerConnector = Consumer.create(consumerConfig)\n+    logInfo(\"Connected to \" + zkConnect)\n+\n+    // When auto.offset.reset is defined, it is our responsibility to try and whack the\n+    // consumer group zk node.\n+    if (kafkaParams.contains(\"auto.offset.reset\")) {\n+      tryZookeeperConsumerGroupCleanup(zkConnect, kafkaParams(\"group.id\"))\n+    }\n+\n+    val keyDecoder = classTag[U].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+      .newInstance(consumerConfig.props)\n+      .asInstanceOf[Decoder[K]]\n+    val valueDecoder = classTag[T].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+      .newInstance(consumerConfig.props)\n+      .asInstanceOf[Decoder[V]]\n+\n+    // Create Threads for each Topic/Message Stream we are listening"
  }],
  "prId": 2991
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "This should be probably be Receiver[(K, V)]. I know that this is how it is in the existing receiver, but this probably wrong.\n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-11T09:19:11Z",
    "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.util.Properties\n+import java.util.concurrent.Executors\n+\n+import scala.collection.Map\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.consumer._\n+import kafka.serializer.Decoder\n+import kafka.utils.VerifiableProperties\n+import kafka.utils.ZKStringSerializer\n+import org.I0Itec.zkclient._\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.receiver.Receiver\n+\n+private[streaming]\n+class KafkaReceiver[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag](\n+    kafkaParams: Map[String, String],\n+    topics: Map[String, Int],\n+    storageLevel: StorageLevel\n+  ) extends Receiver[Any](storageLevel) with Logging {"
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "OK, I will fix this.\n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-11T09:32:09Z",
    "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.util.Properties\n+import java.util.concurrent.Executors\n+\n+import scala.collection.Map\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.consumer._\n+import kafka.serializer.Decoder\n+import kafka.utils.VerifiableProperties\n+import kafka.utils.ZKStringSerializer\n+import org.I0Itec.zkclient._\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.receiver.Receiver\n+\n+private[streaming]\n+class KafkaReceiver[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag](\n+    kafkaParams: Map[String, String],\n+    topics: Map[String, Int],\n+    storageLevel: StorageLevel\n+  ) extends Receiver[Any](storageLevel) with Logging {"
  }],
  "prId": 2991
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Can you replace this with a while loop? Scala's for loops are less efficient that plain old while loop. \n\nhttp://dynamicsofprogramming.blogspot.co.uk/2013/01/loop-performance-and-local-variables-in.html\n\nNot sure how much performance different it is, but better to use while loop any ways\n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-12T02:00:19Z",
    "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.util.Properties\n+import java.util.concurrent.Executors\n+\n+import scala.collection.Map\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.consumer._\n+import kafka.serializer.Decoder\n+import kafka.utils.VerifiableProperties\n+import kafka.utils.ZKStringSerializer\n+import org.I0Itec.zkclient._\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.receiver.Receiver\n+\n+private[streaming]\n+class KafkaReceiver[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag](\n+    kafkaParams: Map[String, String],\n+    topics: Map[String, Int],\n+    storageLevel: StorageLevel\n+  ) extends Receiver[Any](storageLevel) with Logging {\n+\n+  // Connection to Kafka\n+  var consumerConnector: ConsumerConnector = null\n+\n+  def onStop() {\n+    if (consumerConnector != null) {\n+      consumerConnector.shutdown()\n+    }\n+  }\n+\n+  def onStart() {\n+\n+    logInfo(\"Starting Kafka Consumer Stream with group: \" + kafkaParams(\"group.id\"))\n+\n+    // Kafka connection properties\n+    val props = new Properties()\n+    kafkaParams.foreach(param => props.put(param._1, param._2))\n+\n+    val zkConnect = kafkaParams(\"zookeeper.connect\")\n+    // Create the connection to the cluster\n+    logInfo(\"Connecting to Zookeeper: \" + zkConnect)\n+    val consumerConfig = new ConsumerConfig(props)\n+    consumerConnector = Consumer.create(consumerConfig)\n+    logInfo(\"Connected to \" + zkConnect)\n+\n+    // When auto.offset.reset is defined, it is our responsibility to try and whack the\n+    // consumer group zk node.\n+    if (kafkaParams.contains(\"auto.offset.reset\")) {\n+      tryZookeeperConsumerGroupCleanup(zkConnect, kafkaParams(\"group.id\"))\n+    }\n+\n+    val keyDecoder = classTag[U].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+      .newInstance(consumerConfig.props)\n+      .asInstanceOf[Decoder[K]]\n+    val valueDecoder = classTag[T].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+      .newInstance(consumerConfig.props)\n+      .asInstanceOf[Decoder[V]]\n+\n+    // Create Threads for each Topic/Message Stream we are listening\n+    val topicMessageStreams = consumerConnector.createMessageStreams(\n+      topics, keyDecoder, valueDecoder)\n+\n+    val executorPool = Executors.newFixedThreadPool(topics.values.sum)\n+    try {\n+      // Start the messages handler for each partition\n+      topicMessageStreams.values.foreach { streams =>\n+        streams.foreach { stream => executorPool.submit(new MessageHandler(stream)) }\n+      }\n+    } finally {\n+      executorPool.shutdown() // Just causes threads to terminate after work is done\n+    }\n+  }\n+\n+  // Handles Kafka Messages\n+  private class MessageHandler[K: ClassTag, V: ClassTag](stream: KafkaStream[K, V])\n+    extends Runnable {\n+    def run() {\n+      logInfo(\"Starting MessageHandler.\")\n+      try {\n+        for (msgAndMetadata <- stream) {"
  }],
  "prId": 2991
}]