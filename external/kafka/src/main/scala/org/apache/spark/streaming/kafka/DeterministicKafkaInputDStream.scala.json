[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "This is a good attempt at introducing maxRate! However, what is not clear to me is how is this used. Is it this limit applied on each topic+partition? Or is it the global number of records received per second across all topics and partitions?\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2014-12-30T20:34:21Z",
    "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import scala.annotation.tailrec\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.message.MessageAndMetadata\n+import kafka.serializer.Decoder\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.rdd.kafka.{KafkaCluster, KafkaRDD}\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+\n+/** A stream of {@link org.apache.spark.rdd.kafka.KafkaRDD} where\n+  * each given Kafka topic/partition corresponds to an RDD partition.\n+  * The spark configuration spark.streaming.receiver.maxRate gives the maximum number of messages\n+  * per second that each '''partition''' will accept.\n+  * Starting offsets are specified in advance,\n+  * and this DStream is not responsible for committing offsets,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the stream\n+  * @param messageHandler function for translating each message into the desired type\n+  * @param maxRetries maximum number of times in a row to retry getting leaders' offsets\n+  */\n+class DeterministicKafkaInputDStream[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    @transient ssc_ : StreamingContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R,\n+    maxRetries: Int = 1\n+) extends InputDStream[R](ssc_) with Logging {\n+\n+  private val kc = new KafkaCluster(kafkaParams)\n+\n+  private val maxMessagesPerPartition: Option[Long] = {\n+    val ratePerSec = ssc.sparkContext.getConf.getInt(\"spark.streaming.receiver.maxRate\", 0)"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Sorry, that part was obvious. Didnt see the name `perPartition` in the name. However, there in lies the difference with the maxRate defined with Receivers. maxRate for receivers defines the global number of records to be received through a receiver, not per partition. \n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2014-12-30T20:39:30Z",
    "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import scala.annotation.tailrec\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.message.MessageAndMetadata\n+import kafka.serializer.Decoder\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.rdd.kafka.{KafkaCluster, KafkaRDD}\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+\n+/** A stream of {@link org.apache.spark.rdd.kafka.KafkaRDD} where\n+  * each given Kafka topic/partition corresponds to an RDD partition.\n+  * The spark configuration spark.streaming.receiver.maxRate gives the maximum number of messages\n+  * per second that each '''partition''' will accept.\n+  * Starting offsets are specified in advance,\n+  * and this DStream is not responsible for committing offsets,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the stream\n+  * @param messageHandler function for translating each message into the desired type\n+  * @param maxRetries maximum number of times in a row to retry getting leaders' offsets\n+  */\n+class DeterministicKafkaInputDStream[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    @transient ssc_ : StreamingContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R,\n+    maxRetries: Int = 1\n+) extends InputDStream[R](ssc_) with Logging {\n+\n+  private val kc = new KafkaCluster(kafkaParams)\n+\n+  private val maxMessagesPerPartition: Option[Long] = {\n+    val ratePerSec = ssc.sparkContext.getConf.getInt(\"spark.streaming.receiver.maxRate\", 0)"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "You can have multiple receivers per topic, and the closest receiver-based analogue to my approach would be 1 receiver per partition, hence why I set it up that way.  The semantics are documented in the scaladoc for the class.\n\nIf we want people to be able to configure a very granular per-topic-per-partition maximum per batch we can, but it should probably be done as an (optional) argument rather than a configuration property.\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2014-12-30T20:52:54Z",
    "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import scala.annotation.tailrec\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.message.MessageAndMetadata\n+import kafka.serializer.Decoder\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.rdd.kafka.{KafkaCluster, KafkaRDD}\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+\n+/** A stream of {@link org.apache.spark.rdd.kafka.KafkaRDD} where\n+  * each given Kafka topic/partition corresponds to an RDD partition.\n+  * The spark configuration spark.streaming.receiver.maxRate gives the maximum number of messages\n+  * per second that each '''partition''' will accept.\n+  * Starting offsets are specified in advance,\n+  * and this DStream is not responsible for committing offsets,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the stream\n+  * @param messageHandler function for translating each message into the desired type\n+  * @param maxRetries maximum number of times in a row to retry getting leaders' offsets\n+  */\n+class DeterministicKafkaInputDStream[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    @transient ssc_ : StreamingContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R,\n+    maxRetries: Int = 1\n+) extends InputDStream[R](ssc_) with Logging {\n+\n+  private val kc = new KafkaCluster(kafkaParams)\n+\n+  private val maxMessagesPerPartition: Option[Long] = {\n+    val ratePerSec = ssc.sparkContext.getConf.getInt(\"spark.streaming.receiver.maxRate\", 0)"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Yeah, I get it. However, this still has a different performance characteristics than receiver based sources. Sometimes it is better (as you are \"always\" pulling data in parallel across cluster, instead of the default 1 receiver), sometimes it is worse (window operations that require past data which needs to be pulled from Kafka every time). For this method to be viable alternative to the existing, we ideally have to make sure that the performance characteristics of this method is >= performance of the existing method under all situations. Then using this method will be justified even if the behavior is different. \n\nOn that note, here is an idea of what we can do to handle the lower performance case of window ops. We can store the data pulled from the Kafka as blocks in the BlockManager, so that subsequent accesses to the data (due to window or stateful ops) can be faster. One way to do this is to implement a `KafkaBackedBlockRDD`. This is similar to `WriteAheadLogBackedBlockRDD` which has the logic to either read from BlockManager if block is present, or reload the data from the WriteAheadLog based on file segment info. `KafkaBackedBlockRDD` can be similar, either read from BlockManager, or load it from Kafka based on offsets.\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-04T02:56:53Z",
    "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import scala.annotation.tailrec\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.message.MessageAndMetadata\n+import kafka.serializer.Decoder\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.rdd.kafka.{KafkaCluster, KafkaRDD}\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+\n+/** A stream of {@link org.apache.spark.rdd.kafka.KafkaRDD} where\n+  * each given Kafka topic/partition corresponds to an RDD partition.\n+  * The spark configuration spark.streaming.receiver.maxRate gives the maximum number of messages\n+  * per second that each '''partition''' will accept.\n+  * Starting offsets are specified in advance,\n+  * and this DStream is not responsible for committing offsets,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the stream\n+  * @param messageHandler function for translating each message into the desired type\n+  * @param maxRetries maximum number of times in a row to retry getting leaders' offsets\n+  */\n+class DeterministicKafkaInputDStream[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    @transient ssc_ : StreamingContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R,\n+    maxRetries: Int = 1\n+) extends InputDStream[R](ssc_) with Logging {\n+\n+  private val kc = new KafkaCluster(kafkaParams)\n+\n+  private val maxMessagesPerPartition: Option[Long] = {\n+    val ratePerSec = ssc.sparkContext.getConf.getInt(\"spark.streaming.receiver.maxRate\", 0)"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "I'm not sure specifically what you mean by \"window operations that require\npast data which needs to be pulled from Kafka every time\". The current\nKafkaRDD code has a log every time compute() is called on the rdd to pull\ndata from kafka, and for a job with a window operation, I only see compute\ncalled once for a given offset range, not repeatedly every time.\n\nRegarding the bigger question of how this approach stacks up to the two\nexisting approaches... they're all different approaches with different\ntradeoffs, I don't think one has to win.  I'd still have a use for the\noriginal receiver based class (not the WAL one), especially if SPARK-3146\nor SPARK-4960 ever get merged.\n\nOn Sat, Jan 3, 2015 at 8:57 PM, Tathagata Das notifications@github.com\nwrote:\n\n> In\n> external/kafka/src/main/scala/org/apache/spark/streaming/kafka/DeterministicKafkaInputDStream.scala\n> https://github.com/apache/spark/pull/3798#discussion-diff-22436219:\n> \n> > -  K: ClassTag,\n> > -  V: ClassTag,\n> > -  U <: Decoder[_]: ClassTag,\n> > -  T <: Decoder[_]: ClassTag,\n> > -  R: ClassTag](\n> > -    @transient ssc_ : StreamingContext,\n> > -    val kafkaParams: Map[String, String],\n> > -    val fromOffsets: Map[TopicAndPartition, Long],\n> > -    messageHandler: MessageAndMetadata[K, V] => R,\n> > -    maxRetries: Int = 1\n> >   +) extends InputDStream[R](ssc_) with Logging {\n> >   +\n> > -  private val kc = new KafkaCluster(kafkaParams)\n> >   +\n> > -  private val maxMessagesPerPartition: Option[Long] = {\n> > -    val ratePerSec = ssc.sparkContext.getConf.getInt(\"spark.streaming.receiver.maxRate\", 0)\n> \n> Yeah, I get it. However, this still has a different performance\n> characteristics than receiver based sources. Sometimes it is better (as you\n> are \"always\" pulling data in parallel across cluster, instead of the\n> default 1 receiver), sometimes it is worse (window operations that require\n> past data which needs to be pulled from Kafka every time). For this method\n> to be viable alternative to the existing, we ideally have to make sure that\n> the performance characteristics of this method is >= performance of the\n> existing method under all situations. Then using this method will be\n> justified even if the behavior is different.\n> \n> On that note, here is an idea of what we can do. We can store the data\n> pulled from the Kafka as blocks in the BlockManager, so that subsequent\n> accesses to the data (due to window or stateful ops) can be faster. One way\n> to do this is to implement a KafkaBackedBlockRDD. This is similar to\n> WriteAheadLogBackedBlockRDD which has the logic to either read from\n> BlockManager if block is present, or reload the data from the WriteAheadLog\n> based on file segment info. KafkaBackedBlockRDD can be similar, either\n> read from BlockManager, or load it from Kafka based on offsets.\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/spark/pull/3798/files#r22436219.\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-05T03:39:17Z",
    "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import scala.annotation.tailrec\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.message.MessageAndMetadata\n+import kafka.serializer.Decoder\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.rdd.kafka.{KafkaCluster, KafkaRDD}\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+\n+/** A stream of {@link org.apache.spark.rdd.kafka.KafkaRDD} where\n+  * each given Kafka topic/partition corresponds to an RDD partition.\n+  * The spark configuration spark.streaming.receiver.maxRate gives the maximum number of messages\n+  * per second that each '''partition''' will accept.\n+  * Starting offsets are specified in advance,\n+  * and this DStream is not responsible for committing offsets,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the stream\n+  * @param messageHandler function for translating each message into the desired type\n+  * @param maxRetries maximum number of times in a row to retry getting leaders' offsets\n+  */\n+class DeterministicKafkaInputDStream[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    @transient ssc_ : StreamingContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R,\n+    maxRetries: Int = 1\n+) extends InputDStream[R](ssc_) with Logging {\n+\n+  private val kc = new KafkaCluster(kafkaParams)\n+\n+  private val maxMessagesPerPartition: Option[Long] = {\n+    val ratePerSec = ssc.sparkContext.getConf.getInt(\"spark.streaming.receiver.maxRate\", 0)"
  }],
  "prId": 3798
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "a cleaner logic is to use min(curr +mmp, v)\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2014-12-30T20:36:38Z",
    "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import scala.annotation.tailrec\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.message.MessageAndMetadata\n+import kafka.serializer.Decoder\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.rdd.kafka.{KafkaCluster, KafkaRDD}\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+\n+/** A stream of {@link org.apache.spark.rdd.kafka.KafkaRDD} where\n+  * each given Kafka topic/partition corresponds to an RDD partition.\n+  * The spark configuration spark.streaming.receiver.maxRate gives the maximum number of messages\n+  * per second that each '''partition''' will accept.\n+  * Starting offsets are specified in advance,\n+  * and this DStream is not responsible for committing offsets,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the stream\n+  * @param messageHandler function for translating each message into the desired type\n+  * @param maxRetries maximum number of times in a row to retry getting leaders' offsets\n+  */\n+class DeterministicKafkaInputDStream[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    @transient ssc_ : StreamingContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R,\n+    maxRetries: Int = 1\n+) extends InputDStream[R](ssc_) with Logging {\n+\n+  private val kc = new KafkaCluster(kafkaParams)\n+\n+  private val maxMessagesPerPartition: Option[Long] = {\n+    val ratePerSec = ssc.sparkContext.getConf.getInt(\"spark.streaming.receiver.maxRate\", 0)\n+    if (ratePerSec > 0) {\n+      val secsPerBatch = ssc.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some((secsPerBatch * ratePerSec).toLong)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  // TODO based on the design of InputDStream's lastValidTime, it appears there isn't a\n+  // thread safety concern with private mutable state, but is this certain?\n+  private var currentOffsets = fromOffsets\n+\n+  @tailrec\n+  private def latestLeaderOffsets(retries: Int): Map[TopicAndPartition, Long] = {\n+    val o = kc.getLatestLeaderOffsets(currentOffsets.keys.toSet)\n+    // Either.fold would confuse @tailrec, do it manually\n+    if (o.isLeft) {\n+      val err = o.left.get.toString\n+      if (retries <= 0) {\n+        throw new Exception(err)\n+      } else {\n+        log.error(err)\n+        Thread.sleep(kc.config.refreshLeaderBackoffMs)\n+        latestLeaderOffsets(retries - 1)\n+      }\n+    } else {\n+      o.right.get\n+    }\n+  }\n+\n+  private def clamp(leaderOffsets: Map[TopicAndPartition, Long]): Map[TopicAndPartition, Long] = {\n+    maxMessagesPerPartition.map { mmp =>\n+      leaderOffsets.map { kv =>\n+        val (k, v) = kv\n+        val curr = currentOffsets(k)\n+        val diff = v - curr\n+        if (diff > mmp) (k, curr + mmp) else (k, v)"
  }],
  "prId": 3798
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "There isnt a thread concern here. For all these class fields, there is a concern with checkpointing necessary for driver fault recovery. Lets talk about this in the main thread.\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2014-12-30T20:53:25Z",
    "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import scala.annotation.tailrec\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.message.MessageAndMetadata\n+import kafka.serializer.Decoder\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.rdd.kafka.{KafkaCluster, KafkaRDD}\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+\n+/** A stream of {@link org.apache.spark.rdd.kafka.KafkaRDD} where\n+  * each given Kafka topic/partition corresponds to an RDD partition.\n+  * The spark configuration spark.streaming.receiver.maxRate gives the maximum number of messages\n+  * per second that each '''partition''' will accept.\n+  * Starting offsets are specified in advance,\n+  * and this DStream is not responsible for committing offsets,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the stream\n+  * @param messageHandler function for translating each message into the desired type\n+  * @param maxRetries maximum number of times in a row to retry getting leaders' offsets\n+  */\n+class DeterministicKafkaInputDStream[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    @transient ssc_ : StreamingContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R,\n+    maxRetries: Int = 1\n+) extends InputDStream[R](ssc_) with Logging {\n+\n+  private val kc = new KafkaCluster(kafkaParams)\n+\n+  private val maxMessagesPerPartition: Option[Long] = {\n+    val ratePerSec = ssc.sparkContext.getConf.getInt(\"spark.streaming.receiver.maxRate\", 0)\n+    if (ratePerSec > 0) {\n+      val secsPerBatch = ssc.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some((secsPerBatch * ratePerSec).toLong)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  // TODO based on the design of InputDStream's lastValidTime, it appears there isn't a\n+  // thread safety concern with private mutable state, but is this certain?\n+  private var currentOffsets = fromOffsets"
  }],
  "prId": 3798
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "I'm slightly confused about how Kafka messages are assigned to batches in this stream. From what I can tell, I give the stream a start offset, and then the first batch will include everything from that start offset until \"now\" (i.e. when the program is running) and subsequent batches will include any new messages delivered in that time period.\n\nIs that right? If so I find it a bit confusing to call it `DeterministicKafkaInputDStream`, because the assignment of Kafka messages to batches is not actually deterministic (i.e. I could run the same program twice and the batches would be different each time). Maybe I missed something?\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2014-12-30T21:30:37Z",
    "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import scala.annotation.tailrec\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.message.MessageAndMetadata\n+import kafka.serializer.Decoder\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.rdd.kafka.{KafkaCluster, KafkaRDD}\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+\n+/** A stream of {@link org.apache.spark.rdd.kafka.KafkaRDD} where\n+  * each given Kafka topic/partition corresponds to an RDD partition.\n+  * The spark configuration spark.streaming.receiver.maxRate gives the maximum number of messages\n+  * per second that each '''partition''' will accept.\n+  * Starting offsets are specified in advance,\n+  * and this DStream is not responsible for committing offsets,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the stream\n+  * @param messageHandler function for translating each message into the desired type\n+  * @param maxRetries maximum number of times in a row to retry getting leaders' offsets\n+  */\n+class DeterministicKafkaInputDStream[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    @transient ssc_ : StreamingContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R,\n+    maxRetries: Int = 1\n+) extends InputDStream[R](ssc_) with Logging {\n+\n+  private val kc = new KafkaCluster(kafkaParams)\n+\n+  private val maxMessagesPerPartition: Option[Long] = {\n+    val ratePerSec = ssc.sparkContext.getConf.getInt(\"spark.streaming.receiver.maxRate\", 0)\n+    if (ratePerSec > 0) {\n+      val secsPerBatch = ssc.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some((secsPerBatch * ratePerSec).toLong)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  // TODO based on the design of InputDStream's lastValidTime, it appears there isn't a\n+  // thread safety concern with private mutable state, but is this certain?\n+  private var currentOffsets = fromOffsets\n+\n+  @tailrec\n+  private def latestLeaderOffsets(retries: Int): Map[TopicAndPartition, Long] = {\n+    val o = kc.getLatestLeaderOffsets(currentOffsets.keys.toSet)\n+    // Either.fold would confuse @tailrec, do it manually\n+    if (o.isLeft) {\n+      val err = o.left.get.toString\n+      if (retries <= 0) {\n+        throw new Exception(err)\n+      } else {\n+        log.error(err)\n+        Thread.sleep(kc.config.refreshLeaderBackoffMs)\n+        latestLeaderOffsets(retries - 1)\n+      }\n+    } else {\n+      o.right.get\n+    }\n+  }\n+\n+  private def clamp(leaderOffsets: Map[TopicAndPartition, Long]): Map[TopicAndPartition, Long] = {\n+    maxMessagesPerPartition.map { mmp =>\n+      leaderOffsets.map { kv =>\n+        val (k, v) = kv\n+        val curr = currentOffsets(k)\n+        val diff = v - curr\n+        if (diff > mmp) (k, curr + mmp) else (k, v)\n+      }\n+    }.getOrElse(leaderOffsets)\n+  }\n+\n+  override def compute(validTime: Time): Option[KafkaRDD[K, V, U, T, R]] = {"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "Yeah, you're understanding it correctly.\n\nI'm not perfectly happy with the name either - suggestions?\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2014-12-30T21:41:06Z",
    "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import scala.annotation.tailrec\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.message.MessageAndMetadata\n+import kafka.serializer.Decoder\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.rdd.kafka.{KafkaCluster, KafkaRDD}\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+\n+/** A stream of {@link org.apache.spark.rdd.kafka.KafkaRDD} where\n+  * each given Kafka topic/partition corresponds to an RDD partition.\n+  * The spark configuration spark.streaming.receiver.maxRate gives the maximum number of messages\n+  * per second that each '''partition''' will accept.\n+  * Starting offsets are specified in advance,\n+  * and this DStream is not responsible for committing offsets,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the stream\n+  * @param messageHandler function for translating each message into the desired type\n+  * @param maxRetries maximum number of times in a row to retry getting leaders' offsets\n+  */\n+class DeterministicKafkaInputDStream[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    @transient ssc_ : StreamingContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R,\n+    maxRetries: Int = 1\n+) extends InputDStream[R](ssc_) with Logging {\n+\n+  private val kc = new KafkaCluster(kafkaParams)\n+\n+  private val maxMessagesPerPartition: Option[Long] = {\n+    val ratePerSec = ssc.sparkContext.getConf.getInt(\"spark.streaming.receiver.maxRate\", 0)\n+    if (ratePerSec > 0) {\n+      val secsPerBatch = ssc.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some((secsPerBatch * ratePerSec).toLong)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  // TODO based on the design of InputDStream's lastValidTime, it appears there isn't a\n+  // thread safety concern with private mutable state, but is this certain?\n+  private var currentOffsets = fromOffsets\n+\n+  @tailrec\n+  private def latestLeaderOffsets(retries: Int): Map[TopicAndPartition, Long] = {\n+    val o = kc.getLatestLeaderOffsets(currentOffsets.keys.toSet)\n+    // Either.fold would confuse @tailrec, do it manually\n+    if (o.isLeft) {\n+      val err = o.left.get.toString\n+      if (retries <= 0) {\n+        throw new Exception(err)\n+      } else {\n+        log.error(err)\n+        Thread.sleep(kc.config.refreshLeaderBackoffMs)\n+        latestLeaderOffsets(retries - 1)\n+      }\n+    } else {\n+      o.right.get\n+    }\n+  }\n+\n+  private def clamp(leaderOffsets: Map[TopicAndPartition, Long]): Map[TopicAndPartition, Long] = {\n+    maxMessagesPerPartition.map { mmp =>\n+      leaderOffsets.map { kv =>\n+        val (k, v) = kv\n+        val curr = currentOffsets(k)\n+        val diff = v - curr\n+        if (diff > mmp) (k, curr + mmp) else (k, v)\n+      }\n+    }.getOrElse(leaderOffsets)\n+  }\n+\n+  override def compute(validTime: Time): Option[KafkaRDD[K, V, U, T, R]] = {"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "I believe your interpretation is correct. And lets focus on the logic than the names. I think this logic is good, but its not driver fault-tolerant. But I think we can extend this approach to make it so. \n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2014-12-30T22:20:20Z",
    "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import scala.annotation.tailrec\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.message.MessageAndMetadata\n+import kafka.serializer.Decoder\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.rdd.kafka.{KafkaCluster, KafkaRDD}\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+\n+/** A stream of {@link org.apache.spark.rdd.kafka.KafkaRDD} where\n+  * each given Kafka topic/partition corresponds to an RDD partition.\n+  * The spark configuration spark.streaming.receiver.maxRate gives the maximum number of messages\n+  * per second that each '''partition''' will accept.\n+  * Starting offsets are specified in advance,\n+  * and this DStream is not responsible for committing offsets,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the stream\n+  * @param messageHandler function for translating each message into the desired type\n+  * @param maxRetries maximum number of times in a row to retry getting leaders' offsets\n+  */\n+class DeterministicKafkaInputDStream[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    @transient ssc_ : StreamingContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R,\n+    maxRetries: Int = 1\n+) extends InputDStream[R](ssc_) with Logging {\n+\n+  private val kc = new KafkaCluster(kafkaParams)\n+\n+  private val maxMessagesPerPartition: Option[Long] = {\n+    val ratePerSec = ssc.sparkContext.getConf.getInt(\"spark.streaming.receiver.maxRate\", 0)\n+    if (ratePerSec > 0) {\n+      val secsPerBatch = ssc.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some((secsPerBatch * ratePerSec).toLong)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  // TODO based on the design of InputDStream's lastValidTime, it appears there isn't a\n+  // thread safety concern with private mutable state, but is this certain?\n+  private var currentOffsets = fromOffsets\n+\n+  @tailrec\n+  private def latestLeaderOffsets(retries: Int): Map[TopicAndPartition, Long] = {\n+    val o = kc.getLatestLeaderOffsets(currentOffsets.keys.toSet)\n+    // Either.fold would confuse @tailrec, do it manually\n+    if (o.isLeft) {\n+      val err = o.left.get.toString\n+      if (retries <= 0) {\n+        throw new Exception(err)\n+      } else {\n+        log.error(err)\n+        Thread.sleep(kc.config.refreshLeaderBackoffMs)\n+        latestLeaderOffsets(retries - 1)\n+      }\n+    } else {\n+      o.right.get\n+    }\n+  }\n+\n+  private def clamp(leaderOffsets: Map[TopicAndPartition, Long]): Map[TopicAndPartition, Long] = {\n+    maxMessagesPerPartition.map { mmp =>\n+      leaderOffsets.map { kv =>\n+        val (k, v) = kv\n+        val curr = currentOffsets(k)\n+        val diff = v - curr\n+        if (diff > mmp) (k, curr + mmp) else (k, v)\n+      }\n+    }.getOrElse(leaderOffsets)\n+  }\n+\n+  override def compute(validTime: Time): Option[KafkaRDD[K, V, U, T, R]] = {"
  }, {
    "author": {
      "login": "harishreedharan"
    },
    "body": "I like the approach, but like @tdas said above, what happens on driver failure? We'd end up with different batches right? We'd have to persist the batching information somehow to ensure that we can regenerate the original batches.\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-22T22:55:15Z",
    "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import scala.annotation.tailrec\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.message.MessageAndMetadata\n+import kafka.serializer.Decoder\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.rdd.kafka.{KafkaCluster, KafkaRDD}\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+\n+/** A stream of {@link org.apache.spark.rdd.kafka.KafkaRDD} where\n+  * each given Kafka topic/partition corresponds to an RDD partition.\n+  * The spark configuration spark.streaming.receiver.maxRate gives the maximum number of messages\n+  * per second that each '''partition''' will accept.\n+  * Starting offsets are specified in advance,\n+  * and this DStream is not responsible for committing offsets,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the stream\n+  * @param messageHandler function for translating each message into the desired type\n+  * @param maxRetries maximum number of times in a row to retry getting leaders' offsets\n+  */\n+class DeterministicKafkaInputDStream[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    @transient ssc_ : StreamingContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R,\n+    maxRetries: Int = 1\n+) extends InputDStream[R](ssc_) with Logging {\n+\n+  private val kc = new KafkaCluster(kafkaParams)\n+\n+  private val maxMessagesPerPartition: Option[Long] = {\n+    val ratePerSec = ssc.sparkContext.getConf.getInt(\"spark.streaming.receiver.maxRate\", 0)\n+    if (ratePerSec > 0) {\n+      val secsPerBatch = ssc.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some((secsPerBatch * ratePerSec).toLong)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  // TODO based on the design of InputDStream's lastValidTime, it appears there isn't a\n+  // thread safety concern with private mutable state, but is this certain?\n+  private var currentOffsets = fromOffsets\n+\n+  @tailrec\n+  private def latestLeaderOffsets(retries: Int): Map[TopicAndPartition, Long] = {\n+    val o = kc.getLatestLeaderOffsets(currentOffsets.keys.toSet)\n+    // Either.fold would confuse @tailrec, do it manually\n+    if (o.isLeft) {\n+      val err = o.left.get.toString\n+      if (retries <= 0) {\n+        throw new Exception(err)\n+      } else {\n+        log.error(err)\n+        Thread.sleep(kc.config.refreshLeaderBackoffMs)\n+        latestLeaderOffsets(retries - 1)\n+      }\n+    } else {\n+      o.right.get\n+    }\n+  }\n+\n+  private def clamp(leaderOffsets: Map[TopicAndPartition, Long]): Map[TopicAndPartition, Long] = {\n+    maxMessagesPerPartition.map { mmp =>\n+      leaderOffsets.map { kv =>\n+        val (k, v) = kv\n+        val curr = currentOffsets(k)\n+        val diff = v - curr\n+        if (diff > mmp) (k, curr + mmp) else (k, v)\n+      }\n+    }.getOrElse(leaderOffsets)\n+  }\n+\n+  override def compute(validTime: Time): Option[KafkaRDD[K, V, U, T, R]] = {"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "Maybe I'm misunderstanding the question, but I already updated the PR a week ago to checkpoint the partitions from generatedRdds.\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-22T23:05:35Z",
    "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import scala.annotation.tailrec\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.message.MessageAndMetadata\n+import kafka.serializer.Decoder\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.rdd.kafka.{KafkaCluster, KafkaRDD}\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+\n+/** A stream of {@link org.apache.spark.rdd.kafka.KafkaRDD} where\n+  * each given Kafka topic/partition corresponds to an RDD partition.\n+  * The spark configuration spark.streaming.receiver.maxRate gives the maximum number of messages\n+  * per second that each '''partition''' will accept.\n+  * Starting offsets are specified in advance,\n+  * and this DStream is not responsible for committing offsets,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the stream\n+  * @param messageHandler function for translating each message into the desired type\n+  * @param maxRetries maximum number of times in a row to retry getting leaders' offsets\n+  */\n+class DeterministicKafkaInputDStream[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    @transient ssc_ : StreamingContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R,\n+    maxRetries: Int = 1\n+) extends InputDStream[R](ssc_) with Logging {\n+\n+  private val kc = new KafkaCluster(kafkaParams)\n+\n+  private val maxMessagesPerPartition: Option[Long] = {\n+    val ratePerSec = ssc.sparkContext.getConf.getInt(\"spark.streaming.receiver.maxRate\", 0)\n+    if (ratePerSec > 0) {\n+      val secsPerBatch = ssc.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some((secsPerBatch * ratePerSec).toLong)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  // TODO based on the design of InputDStream's lastValidTime, it appears there isn't a\n+  // thread safety concern with private mutable state, but is this certain?\n+  private var currentOffsets = fromOffsets\n+\n+  @tailrec\n+  private def latestLeaderOffsets(retries: Int): Map[TopicAndPartition, Long] = {\n+    val o = kc.getLatestLeaderOffsets(currentOffsets.keys.toSet)\n+    // Either.fold would confuse @tailrec, do it manually\n+    if (o.isLeft) {\n+      val err = o.left.get.toString\n+      if (retries <= 0) {\n+        throw new Exception(err)\n+      } else {\n+        log.error(err)\n+        Thread.sleep(kc.config.refreshLeaderBackoffMs)\n+        latestLeaderOffsets(retries - 1)\n+      }\n+    } else {\n+      o.right.get\n+    }\n+  }\n+\n+  private def clamp(leaderOffsets: Map[TopicAndPartition, Long]): Map[TopicAndPartition, Long] = {\n+    maxMessagesPerPartition.map { mmp =>\n+      leaderOffsets.map { kv =>\n+        val (k, v) = kv\n+        val curr = currentOffsets(k)\n+        val diff = v - curr\n+        if (diff > mmp) (k, curr + mmp) else (k, v)\n+      }\n+    }.getOrElse(leaderOffsets)\n+  }\n+\n+  override def compute(validTime: Time): Option[KafkaRDD[K, V, U, T, R]] = {"
  }, {
    "author": {
      "login": "harishreedharan"
    },
    "body": "Sorry, posted the comment before I reached that part of the code. I realized after I posted this. Thanks!\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-22T23:07:06Z",
    "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import scala.annotation.tailrec\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.message.MessageAndMetadata\n+import kafka.serializer.Decoder\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.rdd.kafka.{KafkaCluster, KafkaRDD}\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+\n+/** A stream of {@link org.apache.spark.rdd.kafka.KafkaRDD} where\n+  * each given Kafka topic/partition corresponds to an RDD partition.\n+  * The spark configuration spark.streaming.receiver.maxRate gives the maximum number of messages\n+  * per second that each '''partition''' will accept.\n+  * Starting offsets are specified in advance,\n+  * and this DStream is not responsible for committing offsets,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the stream\n+  * @param messageHandler function for translating each message into the desired type\n+  * @param maxRetries maximum number of times in a row to retry getting leaders' offsets\n+  */\n+class DeterministicKafkaInputDStream[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    @transient ssc_ : StreamingContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R,\n+    maxRetries: Int = 1\n+) extends InputDStream[R](ssc_) with Logging {\n+\n+  private val kc = new KafkaCluster(kafkaParams)\n+\n+  private val maxMessagesPerPartition: Option[Long] = {\n+    val ratePerSec = ssc.sparkContext.getConf.getInt(\"spark.streaming.receiver.maxRate\", 0)\n+    if (ratePerSec > 0) {\n+      val secsPerBatch = ssc.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some((secsPerBatch * ratePerSec).toLong)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  // TODO based on the design of InputDStream's lastValidTime, it appears there isn't a\n+  // thread safety concern with private mutable state, but is this certain?\n+  private var currentOffsets = fromOffsets\n+\n+  @tailrec\n+  private def latestLeaderOffsets(retries: Int): Map[TopicAndPartition, Long] = {\n+    val o = kc.getLatestLeaderOffsets(currentOffsets.keys.toSet)\n+    // Either.fold would confuse @tailrec, do it manually\n+    if (o.isLeft) {\n+      val err = o.left.get.toString\n+      if (retries <= 0) {\n+        throw new Exception(err)\n+      } else {\n+        log.error(err)\n+        Thread.sleep(kc.config.refreshLeaderBackoffMs)\n+        latestLeaderOffsets(retries - 1)\n+      }\n+    } else {\n+      o.right.get\n+    }\n+  }\n+\n+  private def clamp(leaderOffsets: Map[TopicAndPartition, Long]): Map[TopicAndPartition, Long] = {\n+    maxMessagesPerPartition.map { mmp =>\n+      leaderOffsets.map { kv =>\n+        val (k, v) = kv\n+        val curr = currentOffsets(k)\n+        val diff = v - curr\n+        if (diff > mmp) (k, curr + mmp) else (k, v)\n+      }\n+    }.getOrElse(leaderOffsets)\n+  }\n+\n+  override def compute(validTime: Time): Option[KafkaRDD[K, V, U, T, R]] = {"
  }],
  "prId": 3798
}, {
  "comments": [{
    "author": {
      "login": "helena"
    },
    "body": "another Thread.sleep - why? Is there another way to do this?\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-08T11:59:22Z",
    "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import scala.annotation.tailrec\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.message.MessageAndMetadata\n+import kafka.serializer.Decoder\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.rdd.kafka.{KafkaCluster, KafkaRDD}\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+\n+/** A stream of {@link org.apache.spark.rdd.kafka.KafkaRDD} where\n+  * each given Kafka topic/partition corresponds to an RDD partition.\n+  * The spark configuration spark.streaming.receiver.maxRate gives the maximum number of messages\n+  * per second that each '''partition''' will accept.\n+  * Starting offsets are specified in advance,\n+  * and this DStream is not responsible for committing offsets,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the stream\n+  * @param messageHandler function for translating each message into the desired type\n+  * @param maxRetries maximum number of times in a row to retry getting leaders' offsets\n+  */\n+class DeterministicKafkaInputDStream[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    @transient ssc_ : StreamingContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R,\n+    maxRetries: Int = 1\n+) extends InputDStream[R](ssc_) with Logging {\n+\n+  private val kc = new KafkaCluster(kafkaParams)\n+\n+  private val maxMessagesPerPartition: Option[Long] = {\n+    val ratePerSec = context.sparkContext.getConf.getInt(\"spark.streaming.receiver.maxRate\", 0)\n+    if (ratePerSec > 0) {\n+      val secsPerBatch = context.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some((secsPerBatch * ratePerSec).toLong)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  private var currentOffsets = fromOffsets\n+\n+  @tailrec\n+  private def latestLeaderOffsets(retries: Int): Map[TopicAndPartition, Long] = {\n+    val o = kc.getLatestLeaderOffsets(currentOffsets.keys.toSet)\n+    // Either.fold would confuse @tailrec, do it manually\n+    if (o.isLeft) {\n+      val err = o.left.get.toString\n+      if (retries <= 0) {\n+        throw new Exception(err)\n+      } else {\n+        log.error(err)\n+        Thread.sleep(kc.config.refreshLeaderBackoffMs)"
  }],
  "prId": 3798
}, {
  "comments": [{
    "author": {
      "login": "helena"
    },
    "body": "currentOffsets.keySet\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-08T12:04:44Z",
    "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import scala.annotation.tailrec\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.message.MessageAndMetadata\n+import kafka.serializer.Decoder\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.rdd.kafka.{KafkaCluster, KafkaRDD}\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+\n+/** A stream of {@link org.apache.spark.rdd.kafka.KafkaRDD} where\n+  * each given Kafka topic/partition corresponds to an RDD partition.\n+  * The spark configuration spark.streaming.receiver.maxRate gives the maximum number of messages\n+  * per second that each '''partition''' will accept.\n+  * Starting offsets are specified in advance,\n+  * and this DStream is not responsible for committing offsets,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the stream\n+  * @param messageHandler function for translating each message into the desired type\n+  * @param maxRetries maximum number of times in a row to retry getting leaders' offsets\n+  */\n+class DeterministicKafkaInputDStream[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    @transient ssc_ : StreamingContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R,\n+    maxRetries: Int = 1\n+) extends InputDStream[R](ssc_) with Logging {\n+\n+  private val kc = new KafkaCluster(kafkaParams)\n+\n+  private val maxMessagesPerPartition: Option[Long] = {\n+    val ratePerSec = context.sparkContext.getConf.getInt(\"spark.streaming.receiver.maxRate\", 0)\n+    if (ratePerSec > 0) {\n+      val secsPerBatch = context.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some((secsPerBatch * ratePerSec).toLong)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  private var currentOffsets = fromOffsets\n+\n+  @tailrec\n+  private def latestLeaderOffsets(retries: Int): Map[TopicAndPartition, Long] = {\n+    val o = kc.getLatestLeaderOffsets(currentOffsets.keys.toSet)"
  }],
  "prId": 3798
}, {
  "comments": [{
    "author": {
      "login": "helena"
    },
    "body": "Perhaps use scala math vs Java, and case (k,v) in one line vs creating a val\n`maxMessagesPerPartition.map { mmp =>\n      leaderOffsets.map { case (k, v) => k -> math.min(currentOffsets(k) + mmp, v) }\n    }.getOrElse(leaderOffsets)`\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-08T12:15:40Z",
    "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import scala.annotation.tailrec\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.message.MessageAndMetadata\n+import kafka.serializer.Decoder\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.rdd.kafka.{KafkaCluster, KafkaRDD}\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+\n+/** A stream of {@link org.apache.spark.rdd.kafka.KafkaRDD} where\n+  * each given Kafka topic/partition corresponds to an RDD partition.\n+  * The spark configuration spark.streaming.receiver.maxRate gives the maximum number of messages\n+  * per second that each '''partition''' will accept.\n+  * Starting offsets are specified in advance,\n+  * and this DStream is not responsible for committing offsets,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the stream\n+  * @param messageHandler function for translating each message into the desired type\n+  * @param maxRetries maximum number of times in a row to retry getting leaders' offsets\n+  */\n+class DeterministicKafkaInputDStream[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    @transient ssc_ : StreamingContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R,\n+    maxRetries: Int = 1\n+) extends InputDStream[R](ssc_) with Logging {\n+\n+  private val kc = new KafkaCluster(kafkaParams)\n+\n+  private val maxMessagesPerPartition: Option[Long] = {\n+    val ratePerSec = context.sparkContext.getConf.getInt(\"spark.streaming.receiver.maxRate\", 0)\n+    if (ratePerSec > 0) {\n+      val secsPerBatch = context.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some((secsPerBatch * ratePerSec).toLong)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  private var currentOffsets = fromOffsets\n+\n+  @tailrec\n+  private def latestLeaderOffsets(retries: Int): Map[TopicAndPartition, Long] = {\n+    val o = kc.getLatestLeaderOffsets(currentOffsets.keys.toSet)\n+    // Either.fold would confuse @tailrec, do it manually\n+    if (o.isLeft) {\n+      val err = o.left.get.toString\n+      if (retries <= 0) {\n+        throw new Exception(err)\n+      } else {\n+        log.error(err)\n+        Thread.sleep(kc.config.refreshLeaderBackoffMs)\n+        latestLeaderOffsets(retries - 1)\n+      }\n+    } else {\n+      o.right.get\n+    }\n+  }\n+\n+  private def clamp(leaderOffsets: Map[TopicAndPartition, Long]): Map[TopicAndPartition, Long] = {\n+    maxMessagesPerPartition.map { mmp =>\n+      leaderOffsets.map { kv =>"
  }],
  "prId": 3798
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "looks like this line is too long ...\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-26T22:21:00Z",
    "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+\n+import scala.annotation.tailrec\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.message.MessageAndMetadata\n+import kafka.serializer.Decoder\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.rdd.kafka.{KafkaCluster, KafkaRDD, KafkaRDDPartition}\n+import org.apache.spark.rdd.kafka.KafkaCluster.LeaderOffset\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+\n+/** A stream of {@link org.apache.spark.rdd.kafka.KafkaRDD} where\n+  * each given Kafka topic/partition corresponds to an RDD partition.\n+  * The spark configuration spark.streaming.receiver.maxRate gives the maximum number of messages\n+  * per second that each '''partition''' will accept.\n+  * Starting offsets are specified in advance,\n+  * and this DStream is not responsible for committing offsets,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the stream\n+  * @param messageHandler function for translating each message into the desired type\n+  * @param maxRetries maximum number of times in a row to retry getting leaders' offsets\n+  */\n+class DeterministicKafkaInputDStream[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    @transient ssc_ : StreamingContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R,\n+    maxRetries: Int = 1\n+) extends InputDStream[R](ssc_) with Logging {\n+\n+  protected[streaming] override val checkpointData = new DeterministicKafkaInputDStreamCheckpointData"
  }],
  "prId": 3798
}, {
  "comments": [{
    "author": {
      "login": "jerryshao"
    },
    "body": "Can we make this type `(Int, String, Int, Long, Long, String, Int)` a specific type name, it is not so straightforward to understand the meaning of this large tuple.\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-02-02T01:50:46Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+\n+import scala.annotation.tailrec\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.message.MessageAndMetadata\n+import kafka.serializer.Decoder\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.streaming.kafka.KafkaCluster.LeaderOffset\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+\n+/** A stream of {@link org.apache.spark.rdd.kafka.KafkaRDD} where\n+  * each given Kafka topic/partition corresponds to an RDD partition.\n+  * The spark configuration spark.streaming.receiver.maxRate gives the maximum number of messages\n+  * per second that each '''partition''' will accept.\n+  * Starting offsets are specified in advance,\n+  * and this DStream is not responsible for committing offsets,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the stream\n+  * @param messageHandler function for translating each message into the desired type\n+  * @param maxRetries maximum number of times in a row to retry getting leaders' offsets\n+  */\n+private[streaming]\n+class DeterministicKafkaInputDStream[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    @transient ssc_ : StreamingContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R,\n+    maxRetries: Int\n+) extends InputDStream[R](ssc_) with Logging {\n+\n+  protected[streaming] override val checkpointData =\n+    new DeterministicKafkaInputDStreamCheckpointData\n+\n+  protected val kc = new KafkaCluster(kafkaParams)\n+\n+  protected val maxMessagesPerPartition: Option[Long] = {\n+    val ratePerSec = context.sparkContext.getConf.getInt(\"spark.streaming.receiver.maxRate\", 0)\n+    if (ratePerSec > 0) {\n+      val secsPerBatch = context.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some((secsPerBatch * ratePerSec).toLong)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  protected var currentOffsets = fromOffsets\n+\n+  @tailrec\n+  protected final def latestLeaderOffsets(retries: Int): Map[TopicAndPartition, LeaderOffset] = {\n+    val o = kc.getLatestLeaderOffsets(currentOffsets.keySet)\n+    // Either.fold would confuse @tailrec, do it manually\n+    if (o.isLeft) {\n+      val err = o.left.get.toString\n+      if (retries <= 0) {\n+        throw new Exception(err)\n+      } else {\n+        log.error(err)\n+        Thread.sleep(kc.config.refreshLeaderBackoffMs)\n+        latestLeaderOffsets(retries - 1)\n+      }\n+    } else {\n+      o.right.get\n+    }\n+  }\n+\n+  protected def clamp(\n+    leaderOffsets: Map[TopicAndPartition, LeaderOffset]): Map[TopicAndPartition, LeaderOffset] = {\n+    maxMessagesPerPartition.map { mmp =>\n+      leaderOffsets.map { case (tp, lo) =>\n+        tp -> lo.copy(offset = Math.min(currentOffsets(tp) + mmp, lo.offset))\n+      }\n+    }.getOrElse(leaderOffsets)\n+  }\n+\n+  override def compute(validTime: Time): Option[KafkaRDD[K, V, U, T, R]] = {\n+    val untilOffsets = clamp(latestLeaderOffsets(maxRetries))\n+    val rdd = KafkaRDD[K, V, U, T, R](\n+      context.sparkContext, kafkaParams, currentOffsets, untilOffsets, messageHandler)\n+\n+    currentOffsets = untilOffsets.map(kv => kv._1 -> kv._2.offset)\n+    Some(rdd)\n+  }\n+\n+  override def start(): Unit = {\n+  }\n+\n+  def stop(): Unit = {\n+  }\n+\n+  private[streaming]\n+  class DeterministicKafkaInputDStreamCheckpointData extends DStreamCheckpointData(this) {\n+    def batchForTime = data.asInstanceOf[mutable.HashMap[\n+      Time, Array[(Int, String, Int, Long, Long, String, Int)]]]"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "@koeninger Yes, please making this a specific type. \n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-02-02T23:07:02Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+\n+import scala.annotation.tailrec\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.message.MessageAndMetadata\n+import kafka.serializer.Decoder\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.streaming.kafka.KafkaCluster.LeaderOffset\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+\n+/** A stream of {@link org.apache.spark.rdd.kafka.KafkaRDD} where\n+  * each given Kafka topic/partition corresponds to an RDD partition.\n+  * The spark configuration spark.streaming.receiver.maxRate gives the maximum number of messages\n+  * per second that each '''partition''' will accept.\n+  * Starting offsets are specified in advance,\n+  * and this DStream is not responsible for committing offsets,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the stream\n+  * @param messageHandler function for translating each message into the desired type\n+  * @param maxRetries maximum number of times in a row to retry getting leaders' offsets\n+  */\n+private[streaming]\n+class DeterministicKafkaInputDStream[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    @transient ssc_ : StreamingContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R,\n+    maxRetries: Int\n+) extends InputDStream[R](ssc_) with Logging {\n+\n+  protected[streaming] override val checkpointData =\n+    new DeterministicKafkaInputDStreamCheckpointData\n+\n+  protected val kc = new KafkaCluster(kafkaParams)\n+\n+  protected val maxMessagesPerPartition: Option[Long] = {\n+    val ratePerSec = context.sparkContext.getConf.getInt(\"spark.streaming.receiver.maxRate\", 0)\n+    if (ratePerSec > 0) {\n+      val secsPerBatch = context.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some((secsPerBatch * ratePerSec).toLong)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  protected var currentOffsets = fromOffsets\n+\n+  @tailrec\n+  protected final def latestLeaderOffsets(retries: Int): Map[TopicAndPartition, LeaderOffset] = {\n+    val o = kc.getLatestLeaderOffsets(currentOffsets.keySet)\n+    // Either.fold would confuse @tailrec, do it manually\n+    if (o.isLeft) {\n+      val err = o.left.get.toString\n+      if (retries <= 0) {\n+        throw new Exception(err)\n+      } else {\n+        log.error(err)\n+        Thread.sleep(kc.config.refreshLeaderBackoffMs)\n+        latestLeaderOffsets(retries - 1)\n+      }\n+    } else {\n+      o.right.get\n+    }\n+  }\n+\n+  protected def clamp(\n+    leaderOffsets: Map[TopicAndPartition, LeaderOffset]): Map[TopicAndPartition, LeaderOffset] = {\n+    maxMessagesPerPartition.map { mmp =>\n+      leaderOffsets.map { case (tp, lo) =>\n+        tp -> lo.copy(offset = Math.min(currentOffsets(tp) + mmp, lo.offset))\n+      }\n+    }.getOrElse(leaderOffsets)\n+  }\n+\n+  override def compute(validTime: Time): Option[KafkaRDD[K, V, U, T, R]] = {\n+    val untilOffsets = clamp(latestLeaderOffsets(maxRetries))\n+    val rdd = KafkaRDD[K, V, U, T, R](\n+      context.sparkContext, kafkaParams, currentOffsets, untilOffsets, messageHandler)\n+\n+    currentOffsets = untilOffsets.map(kv => kv._1 -> kv._2.offset)\n+    Some(rdd)\n+  }\n+\n+  override def start(): Unit = {\n+  }\n+\n+  def stop(): Unit = {\n+  }\n+\n+  private[streaming]\n+  class DeterministicKafkaInputDStreamCheckpointData extends DStreamCheckpointData(this) {\n+    def batchForTime = data.asInstanceOf[mutable.HashMap[\n+      Time, Array[(Int, String, Int, Long, Long, String, Int)]]]"
  }],
  "prId": 3798
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "The link doc link is wrong. \n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-02-03T00:02:52Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+\n+import scala.annotation.tailrec\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.message.MessageAndMetadata\n+import kafka.serializer.Decoder\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.streaming.kafka.KafkaCluster.LeaderOffset\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+\n+/** A stream of {@link org.apache.spark.rdd.kafka.KafkaRDD} where"
  }],
  "prId": 3798
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Lets rename this configuration. Its very confusing to overload this configuration because the system does not behave in the same way. `receiver.maxRate` applies to receivers which is not used by this stream. In fact the mechanism used here is very specific to this input stream and applies to not other input stream. So lets rename it to something like \n`spark.streaming.kafka.maxRatePerPartition`\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-02-03T00:07:32Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+\n+import scala.annotation.tailrec\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.message.MessageAndMetadata\n+import kafka.serializer.Decoder\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.streaming.kafka.KafkaCluster.LeaderOffset\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+\n+/** A stream of {@link org.apache.spark.rdd.kafka.KafkaRDD} where\n+  * each given Kafka topic/partition corresponds to an RDD partition.\n+  * The spark configuration spark.streaming.receiver.maxRate gives the maximum number of messages\n+  * per second that each '''partition''' will accept.\n+  * Starting offsets are specified in advance,\n+  * and this DStream is not responsible for committing offsets,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the stream\n+  * @param messageHandler function for translating each message into the desired type\n+  * @param maxRetries maximum number of times in a row to retry getting leaders' offsets\n+  */\n+private[streaming]\n+class DeterministicKafkaInputDStream[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    @transient ssc_ : StreamingContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R,\n+    maxRetries: Int\n+) extends InputDStream[R](ssc_) with Logging {\n+\n+  protected[streaming] override val checkpointData =\n+    new DeterministicKafkaInputDStreamCheckpointData\n+\n+  protected val kc = new KafkaCluster(kafkaParams)\n+\n+  protected val maxMessagesPerPartition: Option[Long] = {\n+    val ratePerSec = context.sparkContext.getConf.getInt(\"spark.streaming.receiver.maxRate\", 0)"
  }],
  "prId": 3798
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Exception --> SparkException\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-02-03T00:09:16Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+\n+import scala.annotation.tailrec\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.message.MessageAndMetadata\n+import kafka.serializer.Decoder\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.streaming.kafka.KafkaCluster.LeaderOffset\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+\n+/** A stream of {@link org.apache.spark.rdd.kafka.KafkaRDD} where\n+  * each given Kafka topic/partition corresponds to an RDD partition.\n+  * The spark configuration spark.streaming.receiver.maxRate gives the maximum number of messages\n+  * per second that each '''partition''' will accept.\n+  * Starting offsets are specified in advance,\n+  * and this DStream is not responsible for committing offsets,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the stream\n+  * @param messageHandler function for translating each message into the desired type\n+  * @param maxRetries maximum number of times in a row to retry getting leaders' offsets\n+  */\n+private[streaming]\n+class DeterministicKafkaInputDStream[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    @transient ssc_ : StreamingContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R,\n+    maxRetries: Int\n+) extends InputDStream[R](ssc_) with Logging {\n+\n+  protected[streaming] override val checkpointData =\n+    new DeterministicKafkaInputDStreamCheckpointData\n+\n+  protected val kc = new KafkaCluster(kafkaParams)\n+\n+  protected val maxMessagesPerPartition: Option[Long] = {\n+    val ratePerSec = context.sparkContext.getConf.getInt(\"spark.streaming.receiver.maxRate\", 0)\n+    if (ratePerSec > 0) {\n+      val secsPerBatch = context.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some((secsPerBatch * ratePerSec).toLong)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  protected var currentOffsets = fromOffsets\n+\n+  @tailrec\n+  protected final def latestLeaderOffsets(retries: Int): Map[TopicAndPartition, LeaderOffset] = {\n+    val o = kc.getLatestLeaderOffsets(currentOffsets.keySet)\n+    // Either.fold would confuse @tailrec, do it manually\n+    if (o.isLeft) {\n+      val err = o.left.get.toString\n+      if (retries <= 0) {\n+        throw new Exception(err)"
  }],
  "prId": 3798
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "This maxRetries should be configurable through SparkConf like maxRate.\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-02-03T00:11:00Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+\n+import scala.annotation.tailrec\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.message.MessageAndMetadata\n+import kafka.serializer.Decoder\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.streaming.kafka.KafkaCluster.LeaderOffset\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+\n+/** A stream of {@link org.apache.spark.rdd.kafka.KafkaRDD} where\n+  * each given Kafka topic/partition corresponds to an RDD partition.\n+  * The spark configuration spark.streaming.receiver.maxRate gives the maximum number of messages\n+  * per second that each '''partition''' will accept.\n+  * Starting offsets are specified in advance,\n+  * and this DStream is not responsible for committing offsets,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the stream\n+  * @param messageHandler function for translating each message into the desired type\n+  * @param maxRetries maximum number of times in a row to retry getting leaders' offsets\n+  */\n+private[streaming]\n+class DeterministicKafkaInputDStream[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    @transient ssc_ : StreamingContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R,\n+    maxRetries: Int"
  }],
  "prId": 3798
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "This function should be part of KafkaCluster. The getLatestLeaderOffset could take an optional parameter of retries. At least the retry loop should be within KafkaCluster, which can return `Either[Err, Map[...]]` and then this method can throw exception as necessary. \n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-02-03T00:12:02Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+\n+import scala.annotation.tailrec\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.message.MessageAndMetadata\n+import kafka.serializer.Decoder\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.streaming.kafka.KafkaCluster.LeaderOffset\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+\n+/** A stream of {@link org.apache.spark.rdd.kafka.KafkaRDD} where\n+  * each given Kafka topic/partition corresponds to an RDD partition.\n+  * The spark configuration spark.streaming.receiver.maxRate gives the maximum number of messages\n+  * per second that each '''partition''' will accept.\n+  * Starting offsets are specified in advance,\n+  * and this DStream is not responsible for committing offsets,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the stream\n+  * @param messageHandler function for translating each message into the desired type\n+  * @param maxRetries maximum number of times in a row to retry getting leaders' offsets\n+  */\n+private[streaming]\n+class DeterministicKafkaInputDStream[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    @transient ssc_ : StreamingContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R,\n+    maxRetries: Int\n+) extends InputDStream[R](ssc_) with Logging {\n+\n+  protected[streaming] override val checkpointData =\n+    new DeterministicKafkaInputDStreamCheckpointData\n+\n+  protected val kc = new KafkaCluster(kafkaParams)\n+\n+  protected val maxMessagesPerPartition: Option[Long] = {\n+    val ratePerSec = context.sparkContext.getConf.getInt(\"spark.streaming.receiver.maxRate\", 0)\n+    if (ratePerSec > 0) {\n+      val secsPerBatch = context.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some((secsPerBatch * ratePerSec).toLong)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  protected var currentOffsets = fromOffsets\n+\n+  @tailrec\n+  protected final def latestLeaderOffsets(retries: Int): Map[TopicAndPartition, LeaderOffset] = {"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "I don't want KafkaCluster throwing exceptions though\n\nOn Mon, Feb 2, 2015 at 6:12 PM, Tathagata Das notifications@github.com\nwrote:\n\n> In\n> external/kafka/src/main/scala/org/apache/spark/streaming/kafka/DeterministicKafkaInputDStream.scala\n> https://github.com/apache/spark/pull/3798#discussion_r23971976:\n> \n> > -  protected val kc = new KafkaCluster(kafkaParams)\n> >   +\n> > -  protected val maxMessagesPerPartition: Option[Long] = {\n> > -    val ratePerSec = context.sparkContext.getConf.getInt(\"spark.streaming.receiver.maxRate\", 0)\n> > -    if (ratePerSec > 0) {\n> > -      val secsPerBatch = context.graph.batchDuration.milliseconds.toDouble / 1000\n> > -      Some((secsPerBatch \\* ratePerSec).toLong)\n> > -    } else {\n> > -      None\n> > -    }\n> > -  }\n> >   +\n> > -  protected var currentOffsets = fromOffsets\n> >   +\n> > -  @tailrec\n> > -  protected final def latestLeaderOffsets(retries: Int): Map[TopicAndPartition, LeaderOffset] = {\n> \n> This function should be part of KafkaCluster. The getLatestLeaderOffset\n> could take an optional parameter of retries.\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/spark/pull/3798/files#r23971976.\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-02-03T00:25:07Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+\n+import scala.annotation.tailrec\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.message.MessageAndMetadata\n+import kafka.serializer.Decoder\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.streaming.kafka.KafkaCluster.LeaderOffset\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+\n+/** A stream of {@link org.apache.spark.rdd.kafka.KafkaRDD} where\n+  * each given Kafka topic/partition corresponds to an RDD partition.\n+  * The spark configuration spark.streaming.receiver.maxRate gives the maximum number of messages\n+  * per second that each '''partition''' will accept.\n+  * Starting offsets are specified in advance,\n+  * and this DStream is not responsible for committing offsets,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the stream\n+  * @param messageHandler function for translating each message into the desired type\n+  * @param maxRetries maximum number of times in a row to retry getting leaders' offsets\n+  */\n+private[streaming]\n+class DeterministicKafkaInputDStream[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    @transient ssc_ : StreamingContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R,\n+    maxRetries: Int\n+) extends InputDStream[R](ssc_) with Logging {\n+\n+  protected[streaming] override val checkpointData =\n+    new DeterministicKafkaInputDStreamCheckpointData\n+\n+  protected val kc = new KafkaCluster(kafkaParams)\n+\n+  protected val maxMessagesPerPartition: Option[Long] = {\n+    val ratePerSec = context.sparkContext.getConf.getInt(\"spark.streaming.receiver.maxRate\", 0)\n+    if (ratePerSec > 0) {\n+      val secsPerBatch = context.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some((secsPerBatch * ratePerSec).toLong)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  protected var currentOffsets = fromOffsets\n+\n+  @tailrec\n+  protected final def latestLeaderOffsets(retries: Int): Map[TopicAndPartition, LeaderOffset] = {"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Nono, KafkaCluster wont. Method in this DStream class will throw the exception based on the returned `Either[Err, ...]`\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-02-03T00:41:48Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+\n+import scala.annotation.tailrec\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.message.MessageAndMetadata\n+import kafka.serializer.Decoder\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.streaming.kafka.KafkaCluster.LeaderOffset\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+\n+/** A stream of {@link org.apache.spark.rdd.kafka.KafkaRDD} where\n+  * each given Kafka topic/partition corresponds to an RDD partition.\n+  * The spark configuration spark.streaming.receiver.maxRate gives the maximum number of messages\n+  * per second that each '''partition''' will accept.\n+  * Starting offsets are specified in advance,\n+  * and this DStream is not responsible for committing offsets,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the stream\n+  * @param messageHandler function for translating each message into the desired type\n+  * @param maxRetries maximum number of times in a row to retry getting leaders' offsets\n+  */\n+private[streaming]\n+class DeterministicKafkaInputDStream[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    @transient ssc_ : StreamingContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R,\n+    maxRetries: Int\n+) extends InputDStream[R](ssc_) with Logging {\n+\n+  protected[streaming] override val checkpointData =\n+    new DeterministicKafkaInputDStreamCheckpointData\n+\n+  protected val kc = new KafkaCluster(kafkaParams)\n+\n+  protected val maxMessagesPerPartition: Option[Long] = {\n+    val ratePerSec = context.sparkContext.getConf.getInt(\"spark.streaming.receiver.maxRate\", 0)\n+    if (ratePerSec > 0) {\n+      val secsPerBatch = context.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some((secsPerBatch * ratePerSec).toLong)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  protected var currentOffsets = fromOffsets\n+\n+  @tailrec\n+  protected final def latestLeaderOffsets(retries: Int): Map[TopicAndPartition, LeaderOffset] = {"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "All of the methods in kafka cluster are currently based on the idea of trying (at most) all of the brokers, then giving up and letting the caller establish an error handling policy.\n\nSleeping and retrying may not in general be the correct error handling policy.  I know it is for the input dstream's usage right here, but that doesn't make sense to bake into KafkaCluster.\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-02-03T01:09:01Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+\n+import scala.annotation.tailrec\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.message.MessageAndMetadata\n+import kafka.serializer.Decoder\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.streaming.kafka.KafkaCluster.LeaderOffset\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+\n+/** A stream of {@link org.apache.spark.rdd.kafka.KafkaRDD} where\n+  * each given Kafka topic/partition corresponds to an RDD partition.\n+  * The spark configuration spark.streaming.receiver.maxRate gives the maximum number of messages\n+  * per second that each '''partition''' will accept.\n+  * Starting offsets are specified in advance,\n+  * and this DStream is not responsible for committing offsets,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the stream\n+  * @param messageHandler function for translating each message into the desired type\n+  * @param maxRetries maximum number of times in a row to retry getting leaders' offsets\n+  */\n+private[streaming]\n+class DeterministicKafkaInputDStream[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    @transient ssc_ : StreamingContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R,\n+    maxRetries: Int\n+) extends InputDStream[R](ssc_) with Logging {\n+\n+  protected[streaming] override val checkpointData =\n+    new DeterministicKafkaInputDStreamCheckpointData\n+\n+  protected val kc = new KafkaCluster(kafkaParams)\n+\n+  protected val maxMessagesPerPartition: Option[Long] = {\n+    val ratePerSec = context.sparkContext.getConf.getInt(\"spark.streaming.receiver.maxRate\", 0)\n+    if (ratePerSec > 0) {\n+      val secsPerBatch = context.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some((secsPerBatch * ratePerSec).toLong)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  protected var currentOffsets = fromOffsets\n+\n+  @tailrec\n+  protected final def latestLeaderOffsets(retries: Int): Map[TopicAndPartition, LeaderOffset] = {"
  }],
  "prId": 3798
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Please add some documentation on what this method does. Its not obvious for some one trying to understand the code that this is effectively limiting the rate. \n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-02-03T00:12:55Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+\n+import scala.annotation.tailrec\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.message.MessageAndMetadata\n+import kafka.serializer.Decoder\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.streaming.kafka.KafkaCluster.LeaderOffset\n+import org.apache.spark.streaming.{StreamingContext, Time}\n+import org.apache.spark.streaming.dstream._\n+\n+/** A stream of {@link org.apache.spark.rdd.kafka.KafkaRDD} where\n+  * each given Kafka topic/partition corresponds to an RDD partition.\n+  * The spark configuration spark.streaming.receiver.maxRate gives the maximum number of messages\n+  * per second that each '''partition''' will accept.\n+  * Starting offsets are specified in advance,\n+  * and this DStream is not responsible for committing offsets,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the stream\n+  * @param messageHandler function for translating each message into the desired type\n+  * @param maxRetries maximum number of times in a row to retry getting leaders' offsets\n+  */\n+private[streaming]\n+class DeterministicKafkaInputDStream[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    @transient ssc_ : StreamingContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R,\n+    maxRetries: Int\n+) extends InputDStream[R](ssc_) with Logging {\n+\n+  protected[streaming] override val checkpointData =\n+    new DeterministicKafkaInputDStreamCheckpointData\n+\n+  protected val kc = new KafkaCluster(kafkaParams)\n+\n+  protected val maxMessagesPerPartition: Option[Long] = {\n+    val ratePerSec = context.sparkContext.getConf.getInt(\"spark.streaming.receiver.maxRate\", 0)\n+    if (ratePerSec > 0) {\n+      val secsPerBatch = context.graph.batchDuration.milliseconds.toDouble / 1000\n+      Some((secsPerBatch * ratePerSec).toLong)\n+    } else {\n+      None\n+    }\n+  }\n+\n+  protected var currentOffsets = fromOffsets\n+\n+  @tailrec\n+  protected final def latestLeaderOffsets(retries: Int): Map[TopicAndPartition, LeaderOffset] = {\n+    val o = kc.getLatestLeaderOffsets(currentOffsets.keySet)\n+    // Either.fold would confuse @tailrec, do it manually\n+    if (o.isLeft) {\n+      val err = o.left.get.toString\n+      if (retries <= 0) {\n+        throw new Exception(err)\n+      } else {\n+        log.error(err)\n+        Thread.sleep(kc.config.refreshLeaderBackoffMs)\n+        latestLeaderOffsets(retries - 1)\n+      }\n+    } else {\n+      o.right.get\n+    }\n+  }\n+\n+  protected def clamp("
  }],
  "prId": 3798
}]