[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Why is there a drop here? Doesnt the response return messages for the requested offset?\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2014-12-30T20:00:14Z",
    "diffHunk": "@@ -0,0 +1,157 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.spark.{Logging, Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.NextIterator\n+\n+import java.util.Properties\n+import kafka.api.FetchRequestBuilder\n+import kafka.common.{ErrorMapping, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+import kafka.message.{MessageAndMetadata, MessageAndOffset}\n+import kafka.serializer.Decoder\n+import kafka.utils.VerifiableProperties\n+\n+case class KafkaRDDPartition(\n+  override val index: Int,\n+  topic: String,\n+  partition: Int,\n+  fromOffset: Long,\n+  untilOffset: Long\n+) extends Partition\n+\n+/** A batch-oriented interface for consuming from Kafka.\n+  * Each given Kafka topic/partition corresponds to an RDD partition.\n+  * Starting and ending offsets are specified in advance,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the batch\n+  * @param untilOffsets per-topic/partition Kafka offsets defining the (exclusive)\n+  *  ending point of the batch\n+  * @param messageHandler function for translating each message into the desired type\n+  */\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    sc: SparkContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    val untilOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R\n+  ) extends RDD[R](sc, Nil) with Logging {\n+\n+  assert(fromOffsets.keys == untilOffsets.keys,\n+    \"Must provide both from and until offsets for each topic/partition\")\n+\n+  override def getPartitions: Array[Partition] = fromOffsets.zipWithIndex.map { kvi =>\n+    val ((tp, from), index) = kvi\n+    new KafkaRDDPartition(index, tp.topic, tp.partition, from, untilOffsets(tp))\n+  }.toArray\n+\n+  override def compute(thePart: Partition, context: TaskContext) = {\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    if (part.fromOffset >= part.untilOffset) {\n+      log.warn(\"Beginning offset is same or after ending offset \" +\n+        s\"skipping ${part.topic} ${part.partition}\")\n+      Iterator.empty\n+    } else {\n+      new NextIterator[R] {\n+        context.addTaskCompletionListener{ context => closeIfNeeded() }\n+\n+        val kc = new KafkaCluster(kafkaParams)\n+        log.info(s\"Computing topic ${part.topic}, partition ${part.partition} \" +\n+          s\"offsets ${part.fromOffset} -> ${part.untilOffset}\")\n+        val keyDecoder = classTag[U].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+          .newInstance(kc.config.props)\n+          .asInstanceOf[Decoder[K]]\n+        val valueDecoder = classTag[T].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+          .newInstance(kc.config.props)\n+          .asInstanceOf[Decoder[V]]\n+        val consumer: SimpleConsumer = kc.connectLeader(part.topic, part.partition).fold(\n+          errs => throw new Exception(\n+            s\"Couldn't connect to leader for topic ${part.topic} ${part.partition}: \" +\n+              errs.mkString(\"\\n\")),\n+          consumer => consumer\n+        )\n+        var requestOffset = part.fromOffset\n+        var iter: Iterator[MessageAndOffset] = null\n+\n+        override def close() = consumer.close()\n+\n+        override def getNext: R = {\n+          if (iter == null || !iter.hasNext) {\n+            val req = new FetchRequestBuilder().\n+              addFetch(part.topic, part.partition, requestOffset, kc.config.fetchMessageMaxBytes).\n+              build()\n+            val resp = consumer.fetch(req)\n+            if (resp.hasError) {\n+              val err = resp.errorCode(part.topic, part.partition)\n+              if (err == ErrorMapping.LeaderNotAvailableCode ||\n+                err == ErrorMapping.NotLeaderForPartitionCode) {\n+                log.error(s\"Lost leader for topic ${part.topic} partition ${part.partition}, \" +\n+                  s\" sleeping for ${kc.config.refreshLeaderBackoffMs}ms\")\n+                Thread.sleep(kc.config.refreshLeaderBackoffMs)\n+              }\n+              // Let normal rdd retry sort out reconnect attempts\n+              throw ErrorMapping.exceptionFor(err)\n+            }\n+            iter = resp.messageSet(part.topic, part.partition)\n+              .iterator\n+              .dropWhile(_.offset < requestOffset)"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "https://cwiki.apache.org/confluence/display/KAFKA/0.8.0+SimpleConsumer+Example\n\n\"Also note that we are explicitly checking that the offset being read is\nnot less than the offset that we requested. This is needed since if Kafka\nis compressing the messages, the fetch request will return an entire\ncompressed block even if the requested offset isn't the beginning of the\ncompressed block. Thus a message we saw previously may be returned again.\"\n\nOn Tue, Dec 30, 2014 at 2:00 PM, Tathagata Das notifications@github.com\nwrote:\n\n> In external/kafka/src/main/scala/org/apache/spark/rdd/kafka/KafkaRDD.scala\n> https://github.com/apache/spark/pull/3798#discussion-diff-22362167:\n> \n> > -              build()\n> > -            val resp = consumer.fetch(req)\n> > -            if (resp.hasError) {\n> > -              val err = resp.errorCode(part.topic, part.partition)\n> > -              if (err == ErrorMapping.LeaderNotAvailableCode ||\n> > -                err == ErrorMapping.NotLeaderForPartitionCode) {\n> > -                log.error(s\"Lost leader for topic ${part.topic} partition ${part.partition}, \" +\n> > -                  s\" sleeping for ${kc.config.refreshLeaderBackoffMs}ms\")\n> > -                Thread.sleep(kc.config.refreshLeaderBackoffMs)\n> > -              }\n> > -              // Let normal rdd retry sort out reconnect attempts\n> > -              throw ErrorMapping.exceptionFor(err)\n> > -            }\n> > -            iter = resp.messageSet(part.topic, part.partition)\n> > -              .iterator\n> > -              .dropWhile(_.offset < requestOffset)\n> \n> Why is there a drop here? Doesnt the response return messages for the\n> requested offset?\n> \n> â€”\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/spark/pull/3798/files#r22362167.\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2014-12-30T20:05:01Z",
    "diffHunk": "@@ -0,0 +1,157 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.spark.{Logging, Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.NextIterator\n+\n+import java.util.Properties\n+import kafka.api.FetchRequestBuilder\n+import kafka.common.{ErrorMapping, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+import kafka.message.{MessageAndMetadata, MessageAndOffset}\n+import kafka.serializer.Decoder\n+import kafka.utils.VerifiableProperties\n+\n+case class KafkaRDDPartition(\n+  override val index: Int,\n+  topic: String,\n+  partition: Int,\n+  fromOffset: Long,\n+  untilOffset: Long\n+) extends Partition\n+\n+/** A batch-oriented interface for consuming from Kafka.\n+  * Each given Kafka topic/partition corresponds to an RDD partition.\n+  * Starting and ending offsets are specified in advance,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the batch\n+  * @param untilOffsets per-topic/partition Kafka offsets defining the (exclusive)\n+  *  ending point of the batch\n+  * @param messageHandler function for translating each message into the desired type\n+  */\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    sc: SparkContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    val untilOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R\n+  ) extends RDD[R](sc, Nil) with Logging {\n+\n+  assert(fromOffsets.keys == untilOffsets.keys,\n+    \"Must provide both from and until offsets for each topic/partition\")\n+\n+  override def getPartitions: Array[Partition] = fromOffsets.zipWithIndex.map { kvi =>\n+    val ((tp, from), index) = kvi\n+    new KafkaRDDPartition(index, tp.topic, tp.partition, from, untilOffsets(tp))\n+  }.toArray\n+\n+  override def compute(thePart: Partition, context: TaskContext) = {\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    if (part.fromOffset >= part.untilOffset) {\n+      log.warn(\"Beginning offset is same or after ending offset \" +\n+        s\"skipping ${part.topic} ${part.partition}\")\n+      Iterator.empty\n+    } else {\n+      new NextIterator[R] {\n+        context.addTaskCompletionListener{ context => closeIfNeeded() }\n+\n+        val kc = new KafkaCluster(kafkaParams)\n+        log.info(s\"Computing topic ${part.topic}, partition ${part.partition} \" +\n+          s\"offsets ${part.fromOffset} -> ${part.untilOffset}\")\n+        val keyDecoder = classTag[U].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+          .newInstance(kc.config.props)\n+          .asInstanceOf[Decoder[K]]\n+        val valueDecoder = classTag[T].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+          .newInstance(kc.config.props)\n+          .asInstanceOf[Decoder[V]]\n+        val consumer: SimpleConsumer = kc.connectLeader(part.topic, part.partition).fold(\n+          errs => throw new Exception(\n+            s\"Couldn't connect to leader for topic ${part.topic} ${part.partition}: \" +\n+              errs.mkString(\"\\n\")),\n+          consumer => consumer\n+        )\n+        var requestOffset = part.fromOffset\n+        var iter: Iterator[MessageAndOffset] = null\n+\n+        override def close() = consumer.close()\n+\n+        override def getNext: R = {\n+          if (iter == null || !iter.hasNext) {\n+            val req = new FetchRequestBuilder().\n+              addFetch(part.topic, part.partition, requestOffset, kc.config.fetchMessageMaxBytes).\n+              build()\n+            val resp = consumer.fetch(req)\n+            if (resp.hasError) {\n+              val err = resp.errorCode(part.topic, part.partition)\n+              if (err == ErrorMapping.LeaderNotAvailableCode ||\n+                err == ErrorMapping.NotLeaderForPartitionCode) {\n+                log.error(s\"Lost leader for topic ${part.topic} partition ${part.partition}, \" +\n+                  s\" sleeping for ${kc.config.refreshLeaderBackoffMs}ms\")\n+                Thread.sleep(kc.config.refreshLeaderBackoffMs)\n+              }\n+              // Let normal rdd retry sort out reconnect attempts\n+              throw ErrorMapping.exceptionFor(err)\n+            }\n+            iter = resp.messageSet(part.topic, part.partition)\n+              .iterator\n+              .dropWhile(_.offset < requestOffset)"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Wow! That was not intuitive. Worth mentioning this in the code.\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-05T23:30:53Z",
    "diffHunk": "@@ -0,0 +1,157 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.spark.{Logging, Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.NextIterator\n+\n+import java.util.Properties\n+import kafka.api.FetchRequestBuilder\n+import kafka.common.{ErrorMapping, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+import kafka.message.{MessageAndMetadata, MessageAndOffset}\n+import kafka.serializer.Decoder\n+import kafka.utils.VerifiableProperties\n+\n+case class KafkaRDDPartition(\n+  override val index: Int,\n+  topic: String,\n+  partition: Int,\n+  fromOffset: Long,\n+  untilOffset: Long\n+) extends Partition\n+\n+/** A batch-oriented interface for consuming from Kafka.\n+  * Each given Kafka topic/partition corresponds to an RDD partition.\n+  * Starting and ending offsets are specified in advance,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the batch\n+  * @param untilOffsets per-topic/partition Kafka offsets defining the (exclusive)\n+  *  ending point of the batch\n+  * @param messageHandler function for translating each message into the desired type\n+  */\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    sc: SparkContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    val untilOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R\n+  ) extends RDD[R](sc, Nil) with Logging {\n+\n+  assert(fromOffsets.keys == untilOffsets.keys,\n+    \"Must provide both from and until offsets for each topic/partition\")\n+\n+  override def getPartitions: Array[Partition] = fromOffsets.zipWithIndex.map { kvi =>\n+    val ((tp, from), index) = kvi\n+    new KafkaRDDPartition(index, tp.topic, tp.partition, from, untilOffsets(tp))\n+  }.toArray\n+\n+  override def compute(thePart: Partition, context: TaskContext) = {\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    if (part.fromOffset >= part.untilOffset) {\n+      log.warn(\"Beginning offset is same or after ending offset \" +\n+        s\"skipping ${part.topic} ${part.partition}\")\n+      Iterator.empty\n+    } else {\n+      new NextIterator[R] {\n+        context.addTaskCompletionListener{ context => closeIfNeeded() }\n+\n+        val kc = new KafkaCluster(kafkaParams)\n+        log.info(s\"Computing topic ${part.topic}, partition ${part.partition} \" +\n+          s\"offsets ${part.fromOffset} -> ${part.untilOffset}\")\n+        val keyDecoder = classTag[U].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+          .newInstance(kc.config.props)\n+          .asInstanceOf[Decoder[K]]\n+        val valueDecoder = classTag[T].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+          .newInstance(kc.config.props)\n+          .asInstanceOf[Decoder[V]]\n+        val consumer: SimpleConsumer = kc.connectLeader(part.topic, part.partition).fold(\n+          errs => throw new Exception(\n+            s\"Couldn't connect to leader for topic ${part.topic} ${part.partition}: \" +\n+              errs.mkString(\"\\n\")),\n+          consumer => consumer\n+        )\n+        var requestOffset = part.fromOffset\n+        var iter: Iterator[MessageAndOffset] = null\n+\n+        override def close() = consumer.close()\n+\n+        override def getNext: R = {\n+          if (iter == null || !iter.hasNext) {\n+            val req = new FetchRequestBuilder().\n+              addFetch(part.topic, part.partition, requestOffset, kc.config.fetchMessageMaxBytes).\n+              build()\n+            val resp = consumer.fetch(req)\n+            if (resp.hasError) {\n+              val err = resp.errorCode(part.topic, part.partition)\n+              if (err == ErrorMapping.LeaderNotAvailableCode ||\n+                err == ErrorMapping.NotLeaderForPartitionCode) {\n+                log.error(s\"Lost leader for topic ${part.topic} partition ${part.partition}, \" +\n+                  s\" sleeping for ${kc.config.refreshLeaderBackoffMs}ms\")\n+                Thread.sleep(kc.config.refreshLeaderBackoffMs)\n+              }\n+              // Let normal rdd retry sort out reconnect attempts\n+              throw ErrorMapping.exceptionFor(err)\n+            }\n+            iter = resp.messageSet(part.topic, part.partition)\n+              .iterator\n+              .dropWhile(_.offset < requestOffset)"
  }],
  "prId": 3798
}, {
  "comments": [{
    "author": {
      "login": "helena"
    },
    "body": "Why Thread.sleep? I would not want to use that in an async app.\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-08T11:57:49Z",
    "diffHunk": "@@ -0,0 +1,157 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.spark.{Logging, Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.NextIterator\n+\n+import java.util.Properties\n+import kafka.api.FetchRequestBuilder\n+import kafka.common.{ErrorMapping, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+import kafka.message.{MessageAndMetadata, MessageAndOffset}\n+import kafka.serializer.Decoder\n+import kafka.utils.VerifiableProperties\n+\n+case class KafkaRDDPartition(\n+  override val index: Int,\n+  topic: String,\n+  partition: Int,\n+  fromOffset: Long,\n+  untilOffset: Long\n+) extends Partition\n+\n+/** A batch-oriented interface for consuming from Kafka.\n+  * Each given Kafka topic/partition corresponds to an RDD partition.\n+  * Starting and ending offsets are specified in advance,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the batch\n+  * @param untilOffsets per-topic/partition Kafka offsets defining the (exclusive)\n+  *  ending point of the batch\n+  * @param messageHandler function for translating each message into the desired type\n+  */\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    sc: SparkContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    val untilOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R\n+  ) extends RDD[R](sc, Nil) with Logging {\n+\n+  assert(fromOffsets.keys == untilOffsets.keys,\n+    \"Must provide both from and until offsets for each topic/partition\")\n+\n+  override def getPartitions: Array[Partition] = fromOffsets.zipWithIndex.map { kvi =>\n+    val ((tp, from), index) = kvi\n+    new KafkaRDDPartition(index, tp.topic, tp.partition, from, untilOffsets(tp))\n+  }.toArray\n+\n+  override def compute(thePart: Partition, context: TaskContext) = {\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    if (part.fromOffset >= part.untilOffset) {\n+      log.warn(\"Beginning offset is same or after ending offset \" +\n+        s\"skipping ${part.topic} ${part.partition}\")\n+      Iterator.empty\n+    } else {\n+      new NextIterator[R] {\n+        context.addTaskCompletionListener{ context => closeIfNeeded() }\n+\n+        val kc = new KafkaCluster(kafkaParams)\n+        log.info(s\"Computing topic ${part.topic}, partition ${part.partition} \" +\n+          s\"offsets ${part.fromOffset} -> ${part.untilOffset}\")\n+        val keyDecoder = classTag[U].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+          .newInstance(kc.config.props)\n+          .asInstanceOf[Decoder[K]]\n+        val valueDecoder = classTag[T].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+          .newInstance(kc.config.props)\n+          .asInstanceOf[Decoder[V]]\n+        val consumer: SimpleConsumer = kc.connectLeader(part.topic, part.partition).fold(\n+          errs => throw new Exception(\n+            s\"Couldn't connect to leader for topic ${part.topic} ${part.partition}: \" +\n+              errs.mkString(\"\\n\")),\n+          consumer => consumer\n+        )\n+        var requestOffset = part.fromOffset\n+        var iter: Iterator[MessageAndOffset] = null\n+\n+        override def close() = consumer.close()\n+\n+        override def getNext: R = {\n+          if (iter == null || !iter.hasNext) {\n+            val req = new FetchRequestBuilder().\n+              addFetch(part.topic, part.partition, requestOffset, kc.config.fetchMessageMaxBytes).\n+              build()\n+            val resp = consumer.fetch(req)\n+            if (resp.hasError) {\n+              val err = resp.errorCode(part.topic, part.partition)\n+              if (err == ErrorMapping.LeaderNotAvailableCode ||\n+                err == ErrorMapping.NotLeaderForPartitionCode) {\n+                log.error(s\"Lost leader for topic ${part.topic} partition ${part.partition}, \" +\n+                  s\" sleeping for ${kc.config.refreshLeaderBackoffMs}ms\")\n+                Thread.sleep(kc.config.refreshLeaderBackoffMs)"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "That's only happening in an error case, and is being done to make sure that you don't burn through the maximum number of failed stages while leader election is happening.\n\nI'm pretty sure the existing spark kafka dstreams are doing the exact same thing, because that's what the kafka project's consumer code does in ConsumerFetcherManager.\n\nDo you have an alternative suggestion?\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-08T14:30:57Z",
    "diffHunk": "@@ -0,0 +1,157 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.spark.{Logging, Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.NextIterator\n+\n+import java.util.Properties\n+import kafka.api.FetchRequestBuilder\n+import kafka.common.{ErrorMapping, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+import kafka.message.{MessageAndMetadata, MessageAndOffset}\n+import kafka.serializer.Decoder\n+import kafka.utils.VerifiableProperties\n+\n+case class KafkaRDDPartition(\n+  override val index: Int,\n+  topic: String,\n+  partition: Int,\n+  fromOffset: Long,\n+  untilOffset: Long\n+) extends Partition\n+\n+/** A batch-oriented interface for consuming from Kafka.\n+  * Each given Kafka topic/partition corresponds to an RDD partition.\n+  * Starting and ending offsets are specified in advance,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the batch\n+  * @param untilOffsets per-topic/partition Kafka offsets defining the (exclusive)\n+  *  ending point of the batch\n+  * @param messageHandler function for translating each message into the desired type\n+  */\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    sc: SparkContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    val untilOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R\n+  ) extends RDD[R](sc, Nil) with Logging {\n+\n+  assert(fromOffsets.keys == untilOffsets.keys,\n+    \"Must provide both from and until offsets for each topic/partition\")\n+\n+  override def getPartitions: Array[Partition] = fromOffsets.zipWithIndex.map { kvi =>\n+    val ((tp, from), index) = kvi\n+    new KafkaRDDPartition(index, tp.topic, tp.partition, from, untilOffsets(tp))\n+  }.toArray\n+\n+  override def compute(thePart: Partition, context: TaskContext) = {\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    if (part.fromOffset >= part.untilOffset) {\n+      log.warn(\"Beginning offset is same or after ending offset \" +\n+        s\"skipping ${part.topic} ${part.partition}\")\n+      Iterator.empty\n+    } else {\n+      new NextIterator[R] {\n+        context.addTaskCompletionListener{ context => closeIfNeeded() }\n+\n+        val kc = new KafkaCluster(kafkaParams)\n+        log.info(s\"Computing topic ${part.topic}, partition ${part.partition} \" +\n+          s\"offsets ${part.fromOffset} -> ${part.untilOffset}\")\n+        val keyDecoder = classTag[U].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+          .newInstance(kc.config.props)\n+          .asInstanceOf[Decoder[K]]\n+        val valueDecoder = classTag[T].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+          .newInstance(kc.config.props)\n+          .asInstanceOf[Decoder[V]]\n+        val consumer: SimpleConsumer = kc.connectLeader(part.topic, part.partition).fold(\n+          errs => throw new Exception(\n+            s\"Couldn't connect to leader for topic ${part.topic} ${part.partition}: \" +\n+              errs.mkString(\"\\n\")),\n+          consumer => consumer\n+        )\n+        var requestOffset = part.fromOffset\n+        var iter: Iterator[MessageAndOffset] = null\n+\n+        override def close() = consumer.close()\n+\n+        override def getNext: R = {\n+          if (iter == null || !iter.hasNext) {\n+            val req = new FetchRequestBuilder().\n+              addFetch(part.topic, part.partition, requestOffset, kc.config.fetchMessageMaxBytes).\n+              build()\n+            val resp = consumer.fetch(req)\n+            if (resp.hasError) {\n+              val err = resp.errorCode(part.topic, part.partition)\n+              if (err == ErrorMapping.LeaderNotAvailableCode ||\n+                err == ErrorMapping.NotLeaderForPartitionCode) {\n+                log.error(s\"Lost leader for topic ${part.topic} partition ${part.partition}, \" +\n+                  s\" sleeping for ${kc.config.refreshLeaderBackoffMs}ms\")\n+                Thread.sleep(kc.config.refreshLeaderBackoffMs)"
  }, {
    "author": {
      "login": "helena"
    },
    "body": "Not without taking time to review the related code but Thread.sleep in async is a no IMHO and in every project I've worked in for the last 6 years (all scala async envs). But I don't ATM.\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-08T16:39:16Z",
    "diffHunk": "@@ -0,0 +1,157 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.spark.{Logging, Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.NextIterator\n+\n+import java.util.Properties\n+import kafka.api.FetchRequestBuilder\n+import kafka.common.{ErrorMapping, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+import kafka.message.{MessageAndMetadata, MessageAndOffset}\n+import kafka.serializer.Decoder\n+import kafka.utils.VerifiableProperties\n+\n+case class KafkaRDDPartition(\n+  override val index: Int,\n+  topic: String,\n+  partition: Int,\n+  fromOffset: Long,\n+  untilOffset: Long\n+) extends Partition\n+\n+/** A batch-oriented interface for consuming from Kafka.\n+  * Each given Kafka topic/partition corresponds to an RDD partition.\n+  * Starting and ending offsets are specified in advance,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the batch\n+  * @param untilOffsets per-topic/partition Kafka offsets defining the (exclusive)\n+  *  ending point of the batch\n+  * @param messageHandler function for translating each message into the desired type\n+  */\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    sc: SparkContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    val untilOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R\n+  ) extends RDD[R](sc, Nil) with Logging {\n+\n+  assert(fromOffsets.keys == untilOffsets.keys,\n+    \"Must provide both from and until offsets for each topic/partition\")\n+\n+  override def getPartitions: Array[Partition] = fromOffsets.zipWithIndex.map { kvi =>\n+    val ((tp, from), index) = kvi\n+    new KafkaRDDPartition(index, tp.topic, tp.partition, from, untilOffsets(tp))\n+  }.toArray\n+\n+  override def compute(thePart: Partition, context: TaskContext) = {\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    if (part.fromOffset >= part.untilOffset) {\n+      log.warn(\"Beginning offset is same or after ending offset \" +\n+        s\"skipping ${part.topic} ${part.partition}\")\n+      Iterator.empty\n+    } else {\n+      new NextIterator[R] {\n+        context.addTaskCompletionListener{ context => closeIfNeeded() }\n+\n+        val kc = new KafkaCluster(kafkaParams)\n+        log.info(s\"Computing topic ${part.topic}, partition ${part.partition} \" +\n+          s\"offsets ${part.fromOffset} -> ${part.untilOffset}\")\n+        val keyDecoder = classTag[U].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+          .newInstance(kc.config.props)\n+          .asInstanceOf[Decoder[K]]\n+        val valueDecoder = classTag[T].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+          .newInstance(kc.config.props)\n+          .asInstanceOf[Decoder[V]]\n+        val consumer: SimpleConsumer = kc.connectLeader(part.topic, part.partition).fold(\n+          errs => throw new Exception(\n+            s\"Couldn't connect to leader for topic ${part.topic} ${part.partition}: \" +\n+              errs.mkString(\"\\n\")),\n+          consumer => consumer\n+        )\n+        var requestOffset = part.fromOffset\n+        var iter: Iterator[MessageAndOffset] = null\n+\n+        override def close() = consumer.close()\n+\n+        override def getNext: R = {\n+          if (iter == null || !iter.hasNext) {\n+            val req = new FetchRequestBuilder().\n+              addFetch(part.topic, part.partition, requestOffset, kc.config.fetchMessageMaxBytes).\n+              build()\n+            val resp = consumer.fetch(req)\n+            if (resp.hasError) {\n+              val err = resp.errorCode(part.topic, part.partition)\n+              if (err == ErrorMapping.LeaderNotAvailableCode ||\n+                err == ErrorMapping.NotLeaderForPartitionCode) {\n+                log.error(s\"Lost leader for topic ${part.topic} partition ${part.partition}, \" +\n+                  s\" sleeping for ${kc.config.refreshLeaderBackoffMs}ms\")\n+                Thread.sleep(kc.config.refreshLeaderBackoffMs)"
  }],
  "prId": 3798
}, {
  "comments": [{
    "author": {
      "login": "helena"
    },
    "body": "You have 3 nested if statements here, I'd break something out to it's own private function\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-08T12:23:02Z",
    "diffHunk": "@@ -0,0 +1,157 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.spark.{Logging, Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.NextIterator\n+\n+import java.util.Properties\n+import kafka.api.FetchRequestBuilder\n+import kafka.common.{ErrorMapping, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+import kafka.message.{MessageAndMetadata, MessageAndOffset}\n+import kafka.serializer.Decoder\n+import kafka.utils.VerifiableProperties\n+\n+case class KafkaRDDPartition(\n+  override val index: Int,\n+  topic: String,\n+  partition: Int,\n+  fromOffset: Long,\n+  untilOffset: Long\n+) extends Partition\n+\n+/** A batch-oriented interface for consuming from Kafka.\n+  * Each given Kafka topic/partition corresponds to an RDD partition.\n+  * Starting and ending offsets are specified in advance,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param fromOffsets per-topic/partition Kafka offsets defining the (inclusive)\n+  *  starting point of the batch\n+  * @param untilOffsets per-topic/partition Kafka offsets defining the (exclusive)\n+  *  ending point of the batch\n+  * @param messageHandler function for translating each message into the desired type\n+  */\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    sc: SparkContext,\n+    val kafkaParams: Map[String, String],\n+    val fromOffsets: Map[TopicAndPartition, Long],\n+    val untilOffsets: Map[TopicAndPartition, Long],\n+    messageHandler: MessageAndMetadata[K, V] => R\n+  ) extends RDD[R](sc, Nil) with Logging {\n+\n+  assert(fromOffsets.keys == untilOffsets.keys,\n+    \"Must provide both from and until offsets for each topic/partition\")\n+\n+  override def getPartitions: Array[Partition] = fromOffsets.zipWithIndex.map { kvi =>\n+    val ((tp, from), index) = kvi\n+    new KafkaRDDPartition(index, tp.topic, tp.partition, from, untilOffsets(tp))\n+  }.toArray\n+\n+  override def compute(thePart: Partition, context: TaskContext) = {\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    if (part.fromOffset >= part.untilOffset) {\n+      log.warn(\"Beginning offset is same or after ending offset \" +\n+        s\"skipping ${part.topic} ${part.partition}\")\n+      Iterator.empty\n+    } else {\n+      new NextIterator[R] {\n+        context.addTaskCompletionListener{ context => closeIfNeeded() }\n+\n+        val kc = new KafkaCluster(kafkaParams)\n+        log.info(s\"Computing topic ${part.topic}, partition ${part.partition} \" +\n+          s\"offsets ${part.fromOffset} -> ${part.untilOffset}\")\n+        val keyDecoder = classTag[U].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+          .newInstance(kc.config.props)\n+          .asInstanceOf[Decoder[K]]\n+        val valueDecoder = classTag[T].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+          .newInstance(kc.config.props)\n+          .asInstanceOf[Decoder[V]]\n+        val consumer: SimpleConsumer = kc.connectLeader(part.topic, part.partition).fold(\n+          errs => throw new Exception(\n+            s\"Couldn't connect to leader for topic ${part.topic} ${part.partition}: \" +\n+              errs.mkString(\"\\n\")),\n+          consumer => consumer\n+        )\n+        var requestOffset = part.fromOffset\n+        var iter: Iterator[MessageAndOffset] = null\n+\n+        override def close() = consumer.close()\n+\n+        override def getNext: R = {\n+          if (iter == null || !iter.hasNext) {\n+            val req = new FetchRequestBuilder().\n+              addFetch(part.topic, part.partition, requestOffset, kc.config.fetchMessageMaxBytes).\n+              build()\n+            val resp = consumer.fetch(req)\n+            if (resp.hasError) {\n+              val err = resp.errorCode(part.topic, part.partition)\n+              if (err == ErrorMapping.LeaderNotAvailableCode ||"
  }],
  "prId": 3798
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "this level of conditional / structure nesting seems scary. Any chance we can reduce this?\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-26T23:06:19Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.spark.{Logging, Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.NextIterator\n+\n+import java.util.Properties\n+import kafka.api.{FetchRequestBuilder, FetchResponse}\n+import kafka.common.{ErrorMapping, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+import kafka.message.{MessageAndMetadata, MessageAndOffset}\n+import kafka.serializer.Decoder\n+import kafka.utils.VerifiableProperties\n+\n+/** A batch-oriented interface for consuming from Kafka.\n+  * Starting and ending offsets are specified in advance,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param batch Each KafkaRDDPartition in the batch corresponds to a\n+  *   range of offsets for a given Kafka topic/partition\n+  * @param messageHandler function for translating each message into the desired type\n+  */\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    sc: SparkContext,\n+    val kafkaParams: Map[String, String],\n+    val batch: Array[KafkaRDDPartition],\n+    messageHandler: MessageAndMetadata[K, V] => R\n+  ) extends RDD[R](sc, Nil) with Logging {\n+\n+  override def getPartitions: Array[Partition] = batch.asInstanceOf[Array[Partition]]\n+\n+  override def getPreferredLocations(thePart: Partition): Seq[String] = {\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    // TODO is additional hostname resolution necessary here\n+    Seq(part.host)\n+  }\n+\n+  override def compute(thePart: Partition, context: TaskContext): Iterator[R] = {\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    if (part.fromOffset >= part.untilOffset) {\n+      log.warn(\"Beginning offset is same or after ending offset \" +\n+        s\"skipping ${part.topic} ${part.partition}\")\n+      Iterator.empty\n+    } else {\n+      new NextIterator[R] {\n+        context.addTaskCompletionListener{ context => closeIfNeeded() }\n+\n+        log.info(s\"Computing topic ${part.topic}, partition ${part.partition} \" +\n+          s\"offsets ${part.fromOffset} -> ${part.untilOffset}\")\n+\n+        val kc = new KafkaCluster(kafkaParams)\n+        val keyDecoder = classTag[U].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+          .newInstance(kc.config.props)\n+          .asInstanceOf[Decoder[K]]\n+        val valueDecoder = classTag[T].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+          .newInstance(kc.config.props)\n+          .asInstanceOf[Decoder[V]]\n+        val consumer = connectLeader\n+        var requestOffset = part.fromOffset\n+        var iter: Iterator[MessageAndOffset] = null\n+\n+        // The idea is to use the provided preferred host, except on task retry atttempts,\n+        // to minimize number of kafka metadata requests\n+        private def connectLeader: SimpleConsumer = {\n+          if (context.attemptNumber > 0) {\n+            kc.connectLeader(part.topic, part.partition).fold(\n+              errs => throw new Exception(\n+                s\"Couldn't connect to leader for topic ${part.topic} ${part.partition}: \" +\n+                  errs.mkString(\"\\n\")),\n+              consumer => consumer\n+            )\n+          } else {\n+            kc.connect(part.host, part.port)\n+          }\n+        }\n+\n+        private def handleErr(resp: FetchResponse) {\n+          if (resp.hasError) {\n+            val err = resp.errorCode(part.topic, part.partition)\n+            if (err == ErrorMapping.LeaderNotAvailableCode ||\n+              err == ErrorMapping.NotLeaderForPartitionCode) {\n+              log.error(s\"Lost leader for topic ${part.topic} partition ${part.partition}, \" +\n+                s\" sleeping for ${kc.config.refreshLeaderBackoffMs}ms\")\n+              Thread.sleep(kc.config.refreshLeaderBackoffMs)\n+            }\n+            // Let normal rdd retry sort out reconnect attempts\n+            throw ErrorMapping.exceptionFor(err)\n+          }\n+        }\n+\n+        override def close() = consumer.close()\n+\n+        override def getNext: R = {\n+          if (iter == null || !iter.hasNext) {\n+            val req = new FetchRequestBuilder().\n+              addFetch(part.topic, part.partition, requestOffset, kc.config.fetchMessageMaxBytes).\n+              build()\n+            val resp = consumer.fetch(req)\n+            handleErr(resp)\n+            // kafka may return a batch that starts before the requested offset\n+            iter = resp.messageSet(part.topic, part.partition)\n+              .iterator\n+              .dropWhile(_.offset < requestOffset)\n+          }\n+          if (!iter.hasNext) {\n+            assert(requestOffset == part.untilOffset,\n+              s\"ran out of messages before reaching ending offset ${part.untilOffset} \" +\n+                s\"for topic ${part.topic} partition ${part.partition} start ${part.fromOffset}.\" +\n+                \" This should not happen, and indicates that messages may have been lost\")\n+            finished = true\n+            null.asInstanceOf[R]\n+          } else {\n+            val item = iter.next\n+            if (item.offset >= part.untilOffset) {\n+              assert(item.offset == part.untilOffset,\n+                s\"got ${item.offset} > ending offset ${part.untilOffset} \" +\n+                  s\"for topic ${part.topic} partition ${part.partition} start ${part.fromOffset}.\" +\n+                  \" This should not happen, and indicates a message may have been skipped\")\n+              finished = true\n+              null.asInstanceOf[R]\n+            } else {\n+              requestOffset = item.nextOffset\n+              messageHandler(new MessageAndMetadata(\n+                part.topic, part.partition, item.message, item.offset, keyDecoder, valueDecoder))\n+            }"
  }, {
    "author": {
      "login": "koeninger"
    },
    "body": "I agree that's one of the ugliest parts of the code, but unfortunately I\nthink those are the cases that need to be handled.  I'll see if there's a\nway to flatten it.\n\nOn Mon, Jan 26, 2015 at 5:06 PM, Reynold Xin notifications@github.com\nwrote:\n\n> In external/kafka/src/main/scala/org/apache/spark/rdd/kafka/KafkaRDD.scala\n> https://github.com/apache/spark/pull/3798#discussion_r23573027:\n> \n> > -            finished = true\n> > -            null.asInstanceOf[R]\n> > -          } else {\n> > -            val item = iter.next\n> > -            if (item.offset >= part.untilOffset) {\n> > -              assert(item.offset == part.untilOffset,\n> > -                s\"got ${item.offset} > ending offset ${part.untilOffset} \" +\n> > -                  s\"for topic ${part.topic} partition ${part.partition} start ${part.fromOffset}.\" +\n> > -                  \" This should not happen, and indicates a message may have been skipped\")\n> > -              finished = true\n> > -              null.asInstanceOf[R]\n> > -            } else {\n> > -              requestOffset = item.nextOffset\n> > -              messageHandler(new MessageAndMetadata(\n> > -                part.topic, part.partition, item.message, item.offset, keyDecoder, valueDecoder))\n> > -            }\n> \n> this level of conditional / structure nesting seems scary. Any chance we\n> can reduce this?\n> \n> â€”\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/spark/pull/3798/files#r23573027.\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-27T00:03:30Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.spark.{Logging, Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.NextIterator\n+\n+import java.util.Properties\n+import kafka.api.{FetchRequestBuilder, FetchResponse}\n+import kafka.common.{ErrorMapping, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+import kafka.message.{MessageAndMetadata, MessageAndOffset}\n+import kafka.serializer.Decoder\n+import kafka.utils.VerifiableProperties\n+\n+/** A batch-oriented interface for consuming from Kafka.\n+  * Starting and ending offsets are specified in advance,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param batch Each KafkaRDDPartition in the batch corresponds to a\n+  *   range of offsets for a given Kafka topic/partition\n+  * @param messageHandler function for translating each message into the desired type\n+  */\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    sc: SparkContext,\n+    val kafkaParams: Map[String, String],\n+    val batch: Array[KafkaRDDPartition],\n+    messageHandler: MessageAndMetadata[K, V] => R\n+  ) extends RDD[R](sc, Nil) with Logging {\n+\n+  override def getPartitions: Array[Partition] = batch.asInstanceOf[Array[Partition]]\n+\n+  override def getPreferredLocations(thePart: Partition): Seq[String] = {\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    // TODO is additional hostname resolution necessary here\n+    Seq(part.host)\n+  }\n+\n+  override def compute(thePart: Partition, context: TaskContext): Iterator[R] = {\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    if (part.fromOffset >= part.untilOffset) {\n+      log.warn(\"Beginning offset is same or after ending offset \" +\n+        s\"skipping ${part.topic} ${part.partition}\")\n+      Iterator.empty\n+    } else {\n+      new NextIterator[R] {\n+        context.addTaskCompletionListener{ context => closeIfNeeded() }\n+\n+        log.info(s\"Computing topic ${part.topic}, partition ${part.partition} \" +\n+          s\"offsets ${part.fromOffset} -> ${part.untilOffset}\")\n+\n+        val kc = new KafkaCluster(kafkaParams)\n+        val keyDecoder = classTag[U].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+          .newInstance(kc.config.props)\n+          .asInstanceOf[Decoder[K]]\n+        val valueDecoder = classTag[T].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+          .newInstance(kc.config.props)\n+          .asInstanceOf[Decoder[V]]\n+        val consumer = connectLeader\n+        var requestOffset = part.fromOffset\n+        var iter: Iterator[MessageAndOffset] = null\n+\n+        // The idea is to use the provided preferred host, except on task retry atttempts,\n+        // to minimize number of kafka metadata requests\n+        private def connectLeader: SimpleConsumer = {\n+          if (context.attemptNumber > 0) {\n+            kc.connectLeader(part.topic, part.partition).fold(\n+              errs => throw new Exception(\n+                s\"Couldn't connect to leader for topic ${part.topic} ${part.partition}: \" +\n+                  errs.mkString(\"\\n\")),\n+              consumer => consumer\n+            )\n+          } else {\n+            kc.connect(part.host, part.port)\n+          }\n+        }\n+\n+        private def handleErr(resp: FetchResponse) {\n+          if (resp.hasError) {\n+            val err = resp.errorCode(part.topic, part.partition)\n+            if (err == ErrorMapping.LeaderNotAvailableCode ||\n+              err == ErrorMapping.NotLeaderForPartitionCode) {\n+              log.error(s\"Lost leader for topic ${part.topic} partition ${part.partition}, \" +\n+                s\" sleeping for ${kc.config.refreshLeaderBackoffMs}ms\")\n+              Thread.sleep(kc.config.refreshLeaderBackoffMs)\n+            }\n+            // Let normal rdd retry sort out reconnect attempts\n+            throw ErrorMapping.exceptionFor(err)\n+          }\n+        }\n+\n+        override def close() = consumer.close()\n+\n+        override def getNext: R = {\n+          if (iter == null || !iter.hasNext) {\n+            val req = new FetchRequestBuilder().\n+              addFetch(part.topic, part.partition, requestOffset, kc.config.fetchMessageMaxBytes).\n+              build()\n+            val resp = consumer.fetch(req)\n+            handleErr(resp)\n+            // kafka may return a batch that starts before the requested offset\n+            iter = resp.messageSet(part.topic, part.partition)\n+              .iterator\n+              .dropWhile(_.offset < requestOffset)\n+          }\n+          if (!iter.hasNext) {\n+            assert(requestOffset == part.untilOffset,\n+              s\"ran out of messages before reaching ending offset ${part.untilOffset} \" +\n+                s\"for topic ${part.topic} partition ${part.partition} start ${part.fromOffset}.\" +\n+                \" This should not happen, and indicates that messages may have been lost\")\n+            finished = true\n+            null.asInstanceOf[R]\n+          } else {\n+            val item = iter.next\n+            if (item.offset >= part.untilOffset) {\n+              assert(item.offset == part.untilOffset,\n+                s\"got ${item.offset} > ending offset ${part.untilOffset} \" +\n+                  s\"for topic ${part.topic} partition ${part.partition} start ${part.fromOffset}.\" +\n+                  \" This should not happen, and indicates a message may have been skipped\")\n+              finished = true\n+              null.asInstanceOf[R]\n+            } else {\n+              requestOffset = item.nextOffset\n+              messageHandler(new MessageAndMetadata(\n+                part.topic, part.partition, item.message, item.offset, keyDecoder, valueDecoder))\n+            }"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "btw one way you can reduce is to use return to help your control flow. for example, if `part.fromOffset >= part.untilOffset`, you can explicitly return an iterator. That reduces one level. Many other ways you can do this.\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-27T04:36:19Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.spark.{Logging, Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.NextIterator\n+\n+import java.util.Properties\n+import kafka.api.{FetchRequestBuilder, FetchResponse}\n+import kafka.common.{ErrorMapping, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+import kafka.message.{MessageAndMetadata, MessageAndOffset}\n+import kafka.serializer.Decoder\n+import kafka.utils.VerifiableProperties\n+\n+/** A batch-oriented interface for consuming from Kafka.\n+  * Starting and ending offsets are specified in advance,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param batch Each KafkaRDDPartition in the batch corresponds to a\n+  *   range of offsets for a given Kafka topic/partition\n+  * @param messageHandler function for translating each message into the desired type\n+  */\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    sc: SparkContext,\n+    val kafkaParams: Map[String, String],\n+    val batch: Array[KafkaRDDPartition],\n+    messageHandler: MessageAndMetadata[K, V] => R\n+  ) extends RDD[R](sc, Nil) with Logging {\n+\n+  override def getPartitions: Array[Partition] = batch.asInstanceOf[Array[Partition]]\n+\n+  override def getPreferredLocations(thePart: Partition): Seq[String] = {\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    // TODO is additional hostname resolution necessary here\n+    Seq(part.host)\n+  }\n+\n+  override def compute(thePart: Partition, context: TaskContext): Iterator[R] = {\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    if (part.fromOffset >= part.untilOffset) {\n+      log.warn(\"Beginning offset is same or after ending offset \" +\n+        s\"skipping ${part.topic} ${part.partition}\")\n+      Iterator.empty\n+    } else {\n+      new NextIterator[R] {\n+        context.addTaskCompletionListener{ context => closeIfNeeded() }\n+\n+        log.info(s\"Computing topic ${part.topic}, partition ${part.partition} \" +\n+          s\"offsets ${part.fromOffset} -> ${part.untilOffset}\")\n+\n+        val kc = new KafkaCluster(kafkaParams)\n+        val keyDecoder = classTag[U].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+          .newInstance(kc.config.props)\n+          .asInstanceOf[Decoder[K]]\n+        val valueDecoder = classTag[T].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+          .newInstance(kc.config.props)\n+          .asInstanceOf[Decoder[V]]\n+        val consumer = connectLeader\n+        var requestOffset = part.fromOffset\n+        var iter: Iterator[MessageAndOffset] = null\n+\n+        // The idea is to use the provided preferred host, except on task retry atttempts,\n+        // to minimize number of kafka metadata requests\n+        private def connectLeader: SimpleConsumer = {\n+          if (context.attemptNumber > 0) {\n+            kc.connectLeader(part.topic, part.partition).fold(\n+              errs => throw new Exception(\n+                s\"Couldn't connect to leader for topic ${part.topic} ${part.partition}: \" +\n+                  errs.mkString(\"\\n\")),\n+              consumer => consumer\n+            )\n+          } else {\n+            kc.connect(part.host, part.port)\n+          }\n+        }\n+\n+        private def handleErr(resp: FetchResponse) {\n+          if (resp.hasError) {\n+            val err = resp.errorCode(part.topic, part.partition)\n+            if (err == ErrorMapping.LeaderNotAvailableCode ||\n+              err == ErrorMapping.NotLeaderForPartitionCode) {\n+              log.error(s\"Lost leader for topic ${part.topic} partition ${part.partition}, \" +\n+                s\" sleeping for ${kc.config.refreshLeaderBackoffMs}ms\")\n+              Thread.sleep(kc.config.refreshLeaderBackoffMs)\n+            }\n+            // Let normal rdd retry sort out reconnect attempts\n+            throw ErrorMapping.exceptionFor(err)\n+          }\n+        }\n+\n+        override def close() = consumer.close()\n+\n+        override def getNext: R = {\n+          if (iter == null || !iter.hasNext) {\n+            val req = new FetchRequestBuilder().\n+              addFetch(part.topic, part.partition, requestOffset, kc.config.fetchMessageMaxBytes).\n+              build()\n+            val resp = consumer.fetch(req)\n+            handleErr(resp)\n+            // kafka may return a batch that starts before the requested offset\n+            iter = resp.messageSet(part.topic, part.partition)\n+              .iterator\n+              .dropWhile(_.offset < requestOffset)\n+          }\n+          if (!iter.hasNext) {\n+            assert(requestOffset == part.untilOffset,\n+              s\"ran out of messages before reaching ending offset ${part.untilOffset} \" +\n+                s\"for topic ${part.topic} partition ${part.partition} start ${part.fromOffset}.\" +\n+                \" This should not happen, and indicates that messages may have been lost\")\n+            finished = true\n+            null.asInstanceOf[R]\n+          } else {\n+            val item = iter.next\n+            if (item.offset >= part.untilOffset) {\n+              assert(item.offset == part.untilOffset,\n+                s\"got ${item.offset} > ending offset ${part.untilOffset} \" +\n+                  s\"for topic ${part.topic} partition ${part.partition} start ${part.fromOffset}.\" +\n+                  \" This should not happen, and indicates a message may have been skipped\")\n+              finished = true\n+              null.asInstanceOf[R]\n+            } else {\n+              requestOffset = item.nextOffset\n+              messageHandler(new MessageAndMetadata(\n+                part.topic, part.partition, item.message, item.offset, keyDecoder, valueDecoder))\n+            }"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "A good way is to define subfunctions within meaningful names. Then the complex condition logic can broken down and will be easier to read. \n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-28T21:56:14Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.spark.{Logging, Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.NextIterator\n+\n+import java.util.Properties\n+import kafka.api.{FetchRequestBuilder, FetchResponse}\n+import kafka.common.{ErrorMapping, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+import kafka.message.{MessageAndMetadata, MessageAndOffset}\n+import kafka.serializer.Decoder\n+import kafka.utils.VerifiableProperties\n+\n+/** A batch-oriented interface for consuming from Kafka.\n+  * Starting and ending offsets are specified in advance,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param batch Each KafkaRDDPartition in the batch corresponds to a\n+  *   range of offsets for a given Kafka topic/partition\n+  * @param messageHandler function for translating each message into the desired type\n+  */\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    sc: SparkContext,\n+    val kafkaParams: Map[String, String],\n+    val batch: Array[KafkaRDDPartition],\n+    messageHandler: MessageAndMetadata[K, V] => R\n+  ) extends RDD[R](sc, Nil) with Logging {\n+\n+  override def getPartitions: Array[Partition] = batch.asInstanceOf[Array[Partition]]\n+\n+  override def getPreferredLocations(thePart: Partition): Seq[String] = {\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    // TODO is additional hostname resolution necessary here\n+    Seq(part.host)\n+  }\n+\n+  override def compute(thePart: Partition, context: TaskContext): Iterator[R] = {\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    if (part.fromOffset >= part.untilOffset) {\n+      log.warn(\"Beginning offset is same or after ending offset \" +\n+        s\"skipping ${part.topic} ${part.partition}\")\n+      Iterator.empty\n+    } else {\n+      new NextIterator[R] {\n+        context.addTaskCompletionListener{ context => closeIfNeeded() }\n+\n+        log.info(s\"Computing topic ${part.topic}, partition ${part.partition} \" +\n+          s\"offsets ${part.fromOffset} -> ${part.untilOffset}\")\n+\n+        val kc = new KafkaCluster(kafkaParams)\n+        val keyDecoder = classTag[U].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+          .newInstance(kc.config.props)\n+          .asInstanceOf[Decoder[K]]\n+        val valueDecoder = classTag[T].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+          .newInstance(kc.config.props)\n+          .asInstanceOf[Decoder[V]]\n+        val consumer = connectLeader\n+        var requestOffset = part.fromOffset\n+        var iter: Iterator[MessageAndOffset] = null\n+\n+        // The idea is to use the provided preferred host, except on task retry atttempts,\n+        // to minimize number of kafka metadata requests\n+        private def connectLeader: SimpleConsumer = {\n+          if (context.attemptNumber > 0) {\n+            kc.connectLeader(part.topic, part.partition).fold(\n+              errs => throw new Exception(\n+                s\"Couldn't connect to leader for topic ${part.topic} ${part.partition}: \" +\n+                  errs.mkString(\"\\n\")),\n+              consumer => consumer\n+            )\n+          } else {\n+            kc.connect(part.host, part.port)\n+          }\n+        }\n+\n+        private def handleErr(resp: FetchResponse) {\n+          if (resp.hasError) {\n+            val err = resp.errorCode(part.topic, part.partition)\n+            if (err == ErrorMapping.LeaderNotAvailableCode ||\n+              err == ErrorMapping.NotLeaderForPartitionCode) {\n+              log.error(s\"Lost leader for topic ${part.topic} partition ${part.partition}, \" +\n+                s\" sleeping for ${kc.config.refreshLeaderBackoffMs}ms\")\n+              Thread.sleep(kc.config.refreshLeaderBackoffMs)\n+            }\n+            // Let normal rdd retry sort out reconnect attempts\n+            throw ErrorMapping.exceptionFor(err)\n+          }\n+        }\n+\n+        override def close() = consumer.close()\n+\n+        override def getNext: R = {\n+          if (iter == null || !iter.hasNext) {\n+            val req = new FetchRequestBuilder().\n+              addFetch(part.topic, part.partition, requestOffset, kc.config.fetchMessageMaxBytes).\n+              build()\n+            val resp = consumer.fetch(req)\n+            handleErr(resp)\n+            // kafka may return a batch that starts before the requested offset\n+            iter = resp.messageSet(part.topic, part.partition)\n+              .iterator\n+              .dropWhile(_.offset < requestOffset)\n+          }\n+          if (!iter.hasNext) {\n+            assert(requestOffset == part.untilOffset,\n+              s\"ran out of messages before reaching ending offset ${part.untilOffset} \" +\n+                s\"for topic ${part.topic} partition ${part.partition} start ${part.fromOffset}.\" +\n+                \" This should not happen, and indicates that messages may have been lost\")\n+            finished = true\n+            null.asInstanceOf[R]\n+          } else {\n+            val item = iter.next\n+            if (item.offset >= part.untilOffset) {\n+              assert(item.offset == part.untilOffset,\n+                s\"got ${item.offset} > ending offset ${part.untilOffset} \" +\n+                  s\"for topic ${part.topic} partition ${part.partition} start ${part.fromOffset}.\" +\n+                  \" This should not happen, and indicates a message may have been skipped\")\n+              finished = true\n+              null.asInstanceOf[R]\n+            } else {\n+              requestOffset = item.nextOffset\n+              messageHandler(new MessageAndMetadata(\n+                part.topic, part.partition, item.message, item.offset, keyDecoder, valueDecoder))\n+            }"
  }],
  "prId": 3798
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "getNext should have parentheses (since it has side effect and it was defined with parentheses in NextIterator)\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-27T04:34:17Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.spark.{Logging, Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.NextIterator\n+\n+import java.util.Properties\n+import kafka.api.{FetchRequestBuilder, FetchResponse}\n+import kafka.common.{ErrorMapping, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+import kafka.message.{MessageAndMetadata, MessageAndOffset}\n+import kafka.serializer.Decoder\n+import kafka.utils.VerifiableProperties\n+\n+/** A batch-oriented interface for consuming from Kafka.\n+  * Starting and ending offsets are specified in advance,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param batch Each KafkaRDDPartition in the batch corresponds to a\n+  *   range of offsets for a given Kafka topic/partition\n+  * @param messageHandler function for translating each message into the desired type\n+  */\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    sc: SparkContext,\n+    val kafkaParams: Map[String, String],\n+    val batch: Array[KafkaRDDPartition],\n+    messageHandler: MessageAndMetadata[K, V] => R\n+  ) extends RDD[R](sc, Nil) with Logging {\n+\n+  override def getPartitions: Array[Partition] = batch.asInstanceOf[Array[Partition]]\n+\n+  override def getPreferredLocations(thePart: Partition): Seq[String] = {\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    // TODO is additional hostname resolution necessary here\n+    Seq(part.host)\n+  }\n+\n+  override def compute(thePart: Partition, context: TaskContext): Iterator[R] = {\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    if (part.fromOffset >= part.untilOffset) {\n+      log.warn(\"Beginning offset is same or after ending offset \" +\n+        s\"skipping ${part.topic} ${part.partition}\")\n+      Iterator.empty\n+    } else {\n+      new NextIterator[R] {\n+        context.addTaskCompletionListener{ context => closeIfNeeded() }\n+\n+        log.info(s\"Computing topic ${part.topic}, partition ${part.partition} \" +\n+          s\"offsets ${part.fromOffset} -> ${part.untilOffset}\")\n+\n+        val kc = new KafkaCluster(kafkaParams)\n+        val keyDecoder = classTag[U].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+          .newInstance(kc.config.props)\n+          .asInstanceOf[Decoder[K]]\n+        val valueDecoder = classTag[T].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+          .newInstance(kc.config.props)\n+          .asInstanceOf[Decoder[V]]\n+        val consumer = connectLeader\n+        var requestOffset = part.fromOffset\n+        var iter: Iterator[MessageAndOffset] = null\n+\n+        // The idea is to use the provided preferred host, except on task retry atttempts,\n+        // to minimize number of kafka metadata requests\n+        private def connectLeader: SimpleConsumer = {\n+          if (context.attemptNumber > 0) {\n+            kc.connectLeader(part.topic, part.partition).fold(\n+              errs => throw new Exception(\n+                s\"Couldn't connect to leader for topic ${part.topic} ${part.partition}: \" +\n+                  errs.mkString(\"\\n\")),\n+              consumer => consumer\n+            )\n+          } else {\n+            kc.connect(part.host, part.port)\n+          }\n+        }\n+\n+        private def handleErr(resp: FetchResponse) {\n+          if (resp.hasError) {\n+            val err = resp.errorCode(part.topic, part.partition)\n+            if (err == ErrorMapping.LeaderNotAvailableCode ||\n+              err == ErrorMapping.NotLeaderForPartitionCode) {\n+              log.error(s\"Lost leader for topic ${part.topic} partition ${part.partition}, \" +\n+                s\" sleeping for ${kc.config.refreshLeaderBackoffMs}ms\")\n+              Thread.sleep(kc.config.refreshLeaderBackoffMs)\n+            }\n+            // Let normal rdd retry sort out reconnect attempts\n+            throw ErrorMapping.exceptionFor(err)\n+          }\n+        }\n+\n+        override def close() = consumer.close()\n+\n+        override def getNext: R = {"
  }],
  "prId": 3798
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "move the dot to next line for line 125 and line 126 to be consistent\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-27T04:34:39Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.spark.{Logging, Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.NextIterator\n+\n+import java.util.Properties\n+import kafka.api.{FetchRequestBuilder, FetchResponse}\n+import kafka.common.{ErrorMapping, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+import kafka.message.{MessageAndMetadata, MessageAndOffset}\n+import kafka.serializer.Decoder\n+import kafka.utils.VerifiableProperties\n+\n+/** A batch-oriented interface for consuming from Kafka.\n+  * Starting and ending offsets are specified in advance,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param batch Each KafkaRDDPartition in the batch corresponds to a\n+  *   range of offsets for a given Kafka topic/partition\n+  * @param messageHandler function for translating each message into the desired type\n+  */\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    sc: SparkContext,\n+    val kafkaParams: Map[String, String],\n+    val batch: Array[KafkaRDDPartition],\n+    messageHandler: MessageAndMetadata[K, V] => R\n+  ) extends RDD[R](sc, Nil) with Logging {\n+\n+  override def getPartitions: Array[Partition] = batch.asInstanceOf[Array[Partition]]\n+\n+  override def getPreferredLocations(thePart: Partition): Seq[String] = {\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    // TODO is additional hostname resolution necessary here\n+    Seq(part.host)\n+  }\n+\n+  override def compute(thePart: Partition, context: TaskContext): Iterator[R] = {\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    if (part.fromOffset >= part.untilOffset) {\n+      log.warn(\"Beginning offset is same or after ending offset \" +\n+        s\"skipping ${part.topic} ${part.partition}\")\n+      Iterator.empty\n+    } else {\n+      new NextIterator[R] {\n+        context.addTaskCompletionListener{ context => closeIfNeeded() }\n+\n+        log.info(s\"Computing topic ${part.topic}, partition ${part.partition} \" +\n+          s\"offsets ${part.fromOffset} -> ${part.untilOffset}\")\n+\n+        val kc = new KafkaCluster(kafkaParams)\n+        val keyDecoder = classTag[U].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+          .newInstance(kc.config.props)\n+          .asInstanceOf[Decoder[K]]\n+        val valueDecoder = classTag[T].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+          .newInstance(kc.config.props)\n+          .asInstanceOf[Decoder[V]]\n+        val consumer = connectLeader\n+        var requestOffset = part.fromOffset\n+        var iter: Iterator[MessageAndOffset] = null\n+\n+        // The idea is to use the provided preferred host, except on task retry atttempts,\n+        // to minimize number of kafka metadata requests\n+        private def connectLeader: SimpleConsumer = {\n+          if (context.attemptNumber > 0) {\n+            kc.connectLeader(part.topic, part.partition).fold(\n+              errs => throw new Exception(\n+                s\"Couldn't connect to leader for topic ${part.topic} ${part.partition}: \" +\n+                  errs.mkString(\"\\n\")),\n+              consumer => consumer\n+            )\n+          } else {\n+            kc.connect(part.host, part.port)\n+          }\n+        }\n+\n+        private def handleErr(resp: FetchResponse) {\n+          if (resp.hasError) {\n+            val err = resp.errorCode(part.topic, part.partition)\n+            if (err == ErrorMapping.LeaderNotAvailableCode ||\n+              err == ErrorMapping.NotLeaderForPartitionCode) {\n+              log.error(s\"Lost leader for topic ${part.topic} partition ${part.partition}, \" +\n+                s\" sleeping for ${kc.config.refreshLeaderBackoffMs}ms\")\n+              Thread.sleep(kc.config.refreshLeaderBackoffMs)\n+            }\n+            // Let normal rdd retry sort out reconnect attempts\n+            throw ErrorMapping.exceptionFor(err)\n+          }\n+        }\n+\n+        override def close() = consumer.close()\n+\n+        override def getNext: R = {\n+          if (iter == null || !iter.hasNext) {\n+            val req = new FetchRequestBuilder().\n+              addFetch(part.topic, part.partition, requestOffset, kc.config.fetchMessageMaxBytes)."
  }],
  "prId": 3798
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "next should have parentheses\n",
    "commit": "1dc29415e3c0ac23a4207513686dfe5ee5ab2725",
    "createdAt": "2015-01-27T04:35:09Z",
    "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.rdd.kafka\n+\n+import scala.reflect.{classTag, ClassTag}\n+\n+import org.apache.spark.{Logging, Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.NextIterator\n+\n+import java.util.Properties\n+import kafka.api.{FetchRequestBuilder, FetchResponse}\n+import kafka.common.{ErrorMapping, TopicAndPartition}\n+import kafka.consumer.{ConsumerConfig, SimpleConsumer}\n+import kafka.message.{MessageAndMetadata, MessageAndOffset}\n+import kafka.serializer.Decoder\n+import kafka.utils.VerifiableProperties\n+\n+/** A batch-oriented interface for consuming from Kafka.\n+  * Starting and ending offsets are specified in advance,\n+  * so that you can control exactly-once semantics.\n+  * For an easy interface to Kafka-managed offsets,\n+  *  see {@link org.apache.spark.rdd.kafka.KafkaCluster}\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>.\n+  *   Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set with Kafka broker(s),\n+  *   NOT zookeeper servers, specified in host1:port1,host2:port2 form.\n+  * @param batch Each KafkaRDDPartition in the batch corresponds to a\n+  *   range of offsets for a given Kafka topic/partition\n+  * @param messageHandler function for translating each message into the desired type\n+  */\n+class KafkaRDD[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag,\n+  R: ClassTag](\n+    sc: SparkContext,\n+    val kafkaParams: Map[String, String],\n+    val batch: Array[KafkaRDDPartition],\n+    messageHandler: MessageAndMetadata[K, V] => R\n+  ) extends RDD[R](sc, Nil) with Logging {\n+\n+  override def getPartitions: Array[Partition] = batch.asInstanceOf[Array[Partition]]\n+\n+  override def getPreferredLocations(thePart: Partition): Seq[String] = {\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    // TODO is additional hostname resolution necessary here\n+    Seq(part.host)\n+  }\n+\n+  override def compute(thePart: Partition, context: TaskContext): Iterator[R] = {\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    if (part.fromOffset >= part.untilOffset) {\n+      log.warn(\"Beginning offset is same or after ending offset \" +\n+        s\"skipping ${part.topic} ${part.partition}\")\n+      Iterator.empty\n+    } else {\n+      new NextIterator[R] {\n+        context.addTaskCompletionListener{ context => closeIfNeeded() }\n+\n+        log.info(s\"Computing topic ${part.topic}, partition ${part.partition} \" +\n+          s\"offsets ${part.fromOffset} -> ${part.untilOffset}\")\n+\n+        val kc = new KafkaCluster(kafkaParams)\n+        val keyDecoder = classTag[U].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+          .newInstance(kc.config.props)\n+          .asInstanceOf[Decoder[K]]\n+        val valueDecoder = classTag[T].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+          .newInstance(kc.config.props)\n+          .asInstanceOf[Decoder[V]]\n+        val consumer = connectLeader\n+        var requestOffset = part.fromOffset\n+        var iter: Iterator[MessageAndOffset] = null\n+\n+        // The idea is to use the provided preferred host, except on task retry atttempts,\n+        // to minimize number of kafka metadata requests\n+        private def connectLeader: SimpleConsumer = {\n+          if (context.attemptNumber > 0) {\n+            kc.connectLeader(part.topic, part.partition).fold(\n+              errs => throw new Exception(\n+                s\"Couldn't connect to leader for topic ${part.topic} ${part.partition}: \" +\n+                  errs.mkString(\"\\n\")),\n+              consumer => consumer\n+            )\n+          } else {\n+            kc.connect(part.host, part.port)\n+          }\n+        }\n+\n+        private def handleErr(resp: FetchResponse) {\n+          if (resp.hasError) {\n+            val err = resp.errorCode(part.topic, part.partition)\n+            if (err == ErrorMapping.LeaderNotAvailableCode ||\n+              err == ErrorMapping.NotLeaderForPartitionCode) {\n+              log.error(s\"Lost leader for topic ${part.topic} partition ${part.partition}, \" +\n+                s\" sleeping for ${kc.config.refreshLeaderBackoffMs}ms\")\n+              Thread.sleep(kc.config.refreshLeaderBackoffMs)\n+            }\n+            // Let normal rdd retry sort out reconnect attempts\n+            throw ErrorMapping.exceptionFor(err)\n+          }\n+        }\n+\n+        override def close() = consumer.close()\n+\n+        override def getNext: R = {\n+          if (iter == null || !iter.hasNext) {\n+            val req = new FetchRequestBuilder().\n+              addFetch(part.topic, part.partition, requestOffset, kc.config.fetchMessageMaxBytes).\n+              build()\n+            val resp = consumer.fetch(req)\n+            handleErr(resp)\n+            // kafka may return a batch that starts before the requested offset\n+            iter = resp.messageSet(part.topic, part.partition)\n+              .iterator\n+              .dropWhile(_.offset < requestOffset)\n+          }\n+          if (!iter.hasNext) {\n+            assert(requestOffset == part.untilOffset,\n+              s\"ran out of messages before reaching ending offset ${part.untilOffset} \" +\n+                s\"for topic ${part.topic} partition ${part.partition} start ${part.fromOffset}.\" +\n+                \" This should not happen, and indicates that messages may have been lost\")\n+            finished = true\n+            null.asInstanceOf[R]\n+          } else {\n+            val item = iter.next"
  }],
  "prId": 3798
}]