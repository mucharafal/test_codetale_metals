[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Can you make the env() a def instead of lazy val. Easier to reason about when it get instantiated. In fact do you need a handle to env? You just need the conf. So the method can be `def conf() = SparkEnv.get.conf`\n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-07T03:06:35Z",
    "diffHunk": "@@ -0,0 +1,212 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.util.Properties\n+import java.util.concurrent.{ConcurrentHashMap, Executors}\n+\n+import scala.collection.Map\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.consumer.{Consumer, ConsumerConfig, ConsumerConnector}\n+import kafka.serializer.Decoder\n+import kafka.utils.{ZkUtils, ZKGroupTopicDirs, ZKStringSerializer, VerifiableProperties}\n+import org.I0Itec.zkclient.ZkClient\n+\n+import org.apache.spark.{SparkEnv, Logging}\n+import org.apache.spark.storage.{StreamBlockId, StorageLevel}\n+import org.apache.spark.streaming.receiver.{BlockGeneratorListener, BlockGenerator, Receiver}\n+\n+private[streaming]\n+class ReliableKafkaReceiver[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag](\n+    kafkaParams: Map[String, String],\n+    topics: Map[String, Int],\n+    storageLevel: StorageLevel)\n+    extends Receiver[Any](storageLevel) with Logging {\n+\n+  /** High level consumer to connect to Kafka */\n+  private var consumerConnector: ConsumerConnector = null\n+\n+  /** zkClient to connect to Zookeeper to commit the offsets */\n+  private var zkClient: ZkClient = null\n+\n+  private val groupId = kafkaParams(\"group.id\")\n+\n+  private lazy val env = SparkEnv.get"
  }],
  "prId": 2991
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Could you use var's instead of lazy val? Bascially, the reciever should be designed to be multiple lifecycles (start, stop) and `lazy val` causes initialization only in the first `start` and likely not cleared when `stop` is called. So its a better design pattern to use `vars` and explicit initialization on start(), rather than `lazy vals`. \n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-07T03:10:02Z",
    "diffHunk": "@@ -0,0 +1,212 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.util.Properties\n+import java.util.concurrent.{ConcurrentHashMap, Executors}\n+\n+import scala.collection.Map\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.consumer.{Consumer, ConsumerConfig, ConsumerConnector}\n+import kafka.serializer.Decoder\n+import kafka.utils.{ZkUtils, ZKGroupTopicDirs, ZKStringSerializer, VerifiableProperties}\n+import org.I0Itec.zkclient.ZkClient\n+\n+import org.apache.spark.{SparkEnv, Logging}\n+import org.apache.spark.storage.{StreamBlockId, StorageLevel}\n+import org.apache.spark.streaming.receiver.{BlockGeneratorListener, BlockGenerator, Receiver}\n+\n+private[streaming]\n+class ReliableKafkaReceiver[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag](\n+    kafkaParams: Map[String, String],\n+    topics: Map[String, Int],\n+    storageLevel: StorageLevel)\n+    extends Receiver[Any](storageLevel) with Logging {\n+\n+  /** High level consumer to connect to Kafka */\n+  private var consumerConnector: ConsumerConnector = null\n+\n+  /** zkClient to connect to Zookeeper to commit the offsets */\n+  private var zkClient: ZkClient = null\n+\n+  private val groupId = kafkaParams(\"group.id\")\n+\n+  private lazy val env = SparkEnv.get\n+\n+  private val AUTO_OFFSET_COMMIT = \"auto.commit.enable\"\n+\n+  /** A HashMap to manage the offset for each topic/partition, this HashMap is called in\n+    * synchronized block, so mutable HashMap will not meet concurrency issue */\n+  private lazy val topicPartitionOffsetMap = new mutable.HashMap[TopicAndPartition, Long]"
  }],
  "prId": 2991
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Please fix comment style. Multi lines docs should be \n\n```\n/**\n *\n */\n```\n\nhttps://cwiki.apache.org/confluence/display/SPARK/Spark+Code+Style+Guide#SparkCodeStyleGuide-Codedocumentationstyle\n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-07T03:12:36Z",
    "diffHunk": "@@ -0,0 +1,212 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.util.Properties\n+import java.util.concurrent.{ConcurrentHashMap, Executors}\n+\n+import scala.collection.Map\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.consumer.{Consumer, ConsumerConfig, ConsumerConnector}\n+import kafka.serializer.Decoder\n+import kafka.utils.{ZkUtils, ZKGroupTopicDirs, ZKStringSerializer, VerifiableProperties}\n+import org.I0Itec.zkclient.ZkClient\n+\n+import org.apache.spark.{SparkEnv, Logging}\n+import org.apache.spark.storage.{StreamBlockId, StorageLevel}\n+import org.apache.spark.streaming.receiver.{BlockGeneratorListener, BlockGenerator, Receiver}\n+\n+private[streaming]\n+class ReliableKafkaReceiver[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag](\n+    kafkaParams: Map[String, String],\n+    topics: Map[String, Int],\n+    storageLevel: StorageLevel)\n+    extends Receiver[Any](storageLevel) with Logging {\n+\n+  /** High level consumer to connect to Kafka */\n+  private var consumerConnector: ConsumerConnector = null\n+\n+  /** zkClient to connect to Zookeeper to commit the offsets */\n+  private var zkClient: ZkClient = null\n+\n+  private val groupId = kafkaParams(\"group.id\")\n+\n+  private lazy val env = SparkEnv.get\n+\n+  private val AUTO_OFFSET_COMMIT = \"auto.commit.enable\"\n+\n+  /** A HashMap to manage the offset for each topic/partition, this HashMap is called in\n+    * synchronized block, so mutable HashMap will not meet concurrency issue */\n+  private lazy val topicPartitionOffsetMap = new mutable.HashMap[TopicAndPartition, Long]\n+\n+  /** A concurrent HashMap to store the stream block id and related offset snapshot */\n+  private lazy val blockOffsetMap =\n+    new ConcurrentHashMap[StreamBlockId, Map[TopicAndPartition, Long]]\n+\n+  private lazy val blockGeneratorListener = new BlockGeneratorListener {\n+    override def onStoreData(data: Any, metadata: Any): Unit = {\n+      if (metadata != null) {\n+        val kafkaMetadata = metadata.asInstanceOf[(TopicAndPartition, Long)]\n+        topicPartitionOffsetMap.put(kafkaMetadata._1, kafkaMetadata._2)\n+      }\n+    }\n+\n+    override def onGenerateBlock(blockId: StreamBlockId): Unit = {\n+      // Get a snapshot of current offset map and store with related block id. Since this hook\n+      // function is called in synchronized block, so we can get the snapshot without explicit lock.\n+      val offsetSnapshot = topicPartitionOffsetMap.toMap\n+      blockOffsetMap.put(blockId, offsetSnapshot)\n+      topicPartitionOffsetMap.clear()\n+    }\n+\n+    override def onPushBlock(blockId: StreamBlockId, arrayBuffer: mutable.ArrayBuffer[_]): Unit = {\n+      store(arrayBuffer.asInstanceOf[mutable.ArrayBuffer[Any]])\n+\n+      // Commit and remove the related offsets.\n+      Option(blockOffsetMap.get(blockId)).foreach { offsetMap =>\n+        commitOffset(offsetMap)\n+      }\n+      blockOffsetMap.remove(blockId)\n+    }\n+\n+    override def onError(message: String, throwable: Throwable): Unit = {\n+      reportError(message, throwable)\n+    }\n+  }\n+\n+  /** Manage the BlockGenerator in receiver itself for better managing block store and offset"
  }],
  "prId": 2991
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Can you use `o.a.spark.util.Utils.newDaemonFixedThreadPool()` for this so that the thread's get nice names (easier to debug in jstack)\n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-07T03:49:37Z",
    "diffHunk": "@@ -0,0 +1,212 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.util.Properties\n+import java.util.concurrent.{ConcurrentHashMap, Executors}\n+\n+import scala.collection.Map\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.consumer.{Consumer, ConsumerConfig, ConsumerConnector}\n+import kafka.serializer.Decoder\n+import kafka.utils.{ZkUtils, ZKGroupTopicDirs, ZKStringSerializer, VerifiableProperties}\n+import org.I0Itec.zkclient.ZkClient\n+\n+import org.apache.spark.{SparkEnv, Logging}\n+import org.apache.spark.storage.{StreamBlockId, StorageLevel}\n+import org.apache.spark.streaming.receiver.{BlockGeneratorListener, BlockGenerator, Receiver}\n+\n+private[streaming]\n+class ReliableKafkaReceiver[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag](\n+    kafkaParams: Map[String, String],\n+    topics: Map[String, Int],\n+    storageLevel: StorageLevel)\n+    extends Receiver[Any](storageLevel) with Logging {\n+\n+  /** High level consumer to connect to Kafka */\n+  private var consumerConnector: ConsumerConnector = null\n+\n+  /** zkClient to connect to Zookeeper to commit the offsets */\n+  private var zkClient: ZkClient = null\n+\n+  private val groupId = kafkaParams(\"group.id\")\n+\n+  private lazy val env = SparkEnv.get\n+\n+  private val AUTO_OFFSET_COMMIT = \"auto.commit.enable\"\n+\n+  /** A HashMap to manage the offset for each topic/partition, this HashMap is called in\n+    * synchronized block, so mutable HashMap will not meet concurrency issue */\n+  private lazy val topicPartitionOffsetMap = new mutable.HashMap[TopicAndPartition, Long]\n+\n+  /** A concurrent HashMap to store the stream block id and related offset snapshot */\n+  private lazy val blockOffsetMap =\n+    new ConcurrentHashMap[StreamBlockId, Map[TopicAndPartition, Long]]\n+\n+  private lazy val blockGeneratorListener = new BlockGeneratorListener {\n+    override def onStoreData(data: Any, metadata: Any): Unit = {\n+      if (metadata != null) {\n+        val kafkaMetadata = metadata.asInstanceOf[(TopicAndPartition, Long)]\n+        topicPartitionOffsetMap.put(kafkaMetadata._1, kafkaMetadata._2)\n+      }\n+    }\n+\n+    override def onGenerateBlock(blockId: StreamBlockId): Unit = {\n+      // Get a snapshot of current offset map and store with related block id. Since this hook\n+      // function is called in synchronized block, so we can get the snapshot without explicit lock.\n+      val offsetSnapshot = topicPartitionOffsetMap.toMap\n+      blockOffsetMap.put(blockId, offsetSnapshot)\n+      topicPartitionOffsetMap.clear()\n+    }\n+\n+    override def onPushBlock(blockId: StreamBlockId, arrayBuffer: mutable.ArrayBuffer[_]): Unit = {\n+      store(arrayBuffer.asInstanceOf[mutable.ArrayBuffer[Any]])\n+\n+      // Commit and remove the related offsets.\n+      Option(blockOffsetMap.get(blockId)).foreach { offsetMap =>\n+        commitOffset(offsetMap)\n+      }\n+      blockOffsetMap.remove(blockId)\n+    }\n+\n+    override def onError(message: String, throwable: Throwable): Unit = {\n+      reportError(message, throwable)\n+    }\n+  }\n+\n+  /** Manage the BlockGenerator in receiver itself for better managing block store and offset\n+    * commit */\n+  private var blockGenerator: BlockGenerator = null\n+\n+  override def onStop(): Unit = {\n+    if (consumerConnector != null) {\n+      consumerConnector.shutdown()\n+      consumerConnector = null\n+    }\n+\n+    if (zkClient != null) {\n+      zkClient.close()\n+      zkClient = null\n+    }\n+\n+    blockGenerator.stop()\n+  }\n+\n+  override def onStart(): Unit = {\n+    logInfo(s\"Starting Kafka Consumer Stream with group: $groupId\")\n+\n+    blockGenerator = new BlockGenerator(blockGeneratorListener, streamId, env.conf)\n+\n+    if (kafkaParams.contains(AUTO_OFFSET_COMMIT) && kafkaParams(AUTO_OFFSET_COMMIT) == \"true\") {\n+      logWarning(s\"$AUTO_OFFSET_COMMIT should be set to false in ReliableKafkaReceiver, \" +\n+        \"otherwise we will manually set it to false to turn off auto offset commit in Kafka\")\n+    }\n+\n+    val props = new Properties()\n+    kafkaParams.foreach(param => props.put(param._1, param._2))\n+    // Manually set \"auto.commit.enable\" to \"false\" no matter user explicitly set it to true,\n+    // we have to make sure this property is set to false to turn off auto commit mechanism in\n+    // Kafka.\n+    props.setProperty(AUTO_OFFSET_COMMIT, \"false\")\n+\n+    val consumerConfig = new ConsumerConfig(props)\n+\n+    assert(consumerConfig.autoCommitEnable == false)\n+\n+    logInfo(s\"Connecting to Zookeeper: ${consumerConfig.zkConnect}\")\n+    consumerConnector = Consumer.create(consumerConfig)\n+    logInfo(s\"Connected to Zookeeper: ${consumerConfig.zkConnect}\")\n+\n+    zkClient = new ZkClient(consumerConfig.zkConnect, consumerConfig.zkSessionTimeoutMs,\n+      consumerConfig.zkConnectionTimeoutMs, ZKStringSerializer)\n+\n+    // start BlockGenerator\n+    blockGenerator.start()\n+\n+    val keyDecoder = classTag[U].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+      .newInstance(consumerConfig.props)\n+      .asInstanceOf[Decoder[K]]\n+\n+    val valueDecoder = classTag[T].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+      .newInstance(consumerConfig.props)\n+      .asInstanceOf[Decoder[V]]\n+\n+    val topicMessageStreams = consumerConnector.createMessageStreams(\n+      topics, keyDecoder, valueDecoder)\n+\n+    val executorPool = Executors.newFixedThreadPool(topics.values.sum)"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "This issue was not addressed in the latest changes.\n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-11T03:26:35Z",
    "diffHunk": "@@ -0,0 +1,212 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.util.Properties\n+import java.util.concurrent.{ConcurrentHashMap, Executors}\n+\n+import scala.collection.Map\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.consumer.{Consumer, ConsumerConfig, ConsumerConnector}\n+import kafka.serializer.Decoder\n+import kafka.utils.{ZkUtils, ZKGroupTopicDirs, ZKStringSerializer, VerifiableProperties}\n+import org.I0Itec.zkclient.ZkClient\n+\n+import org.apache.spark.{SparkEnv, Logging}\n+import org.apache.spark.storage.{StreamBlockId, StorageLevel}\n+import org.apache.spark.streaming.receiver.{BlockGeneratorListener, BlockGenerator, Receiver}\n+\n+private[streaming]\n+class ReliableKafkaReceiver[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag](\n+    kafkaParams: Map[String, String],\n+    topics: Map[String, Int],\n+    storageLevel: StorageLevel)\n+    extends Receiver[Any](storageLevel) with Logging {\n+\n+  /** High level consumer to connect to Kafka */\n+  private var consumerConnector: ConsumerConnector = null\n+\n+  /** zkClient to connect to Zookeeper to commit the offsets */\n+  private var zkClient: ZkClient = null\n+\n+  private val groupId = kafkaParams(\"group.id\")\n+\n+  private lazy val env = SparkEnv.get\n+\n+  private val AUTO_OFFSET_COMMIT = \"auto.commit.enable\"\n+\n+  /** A HashMap to manage the offset for each topic/partition, this HashMap is called in\n+    * synchronized block, so mutable HashMap will not meet concurrency issue */\n+  private lazy val topicPartitionOffsetMap = new mutable.HashMap[TopicAndPartition, Long]\n+\n+  /** A concurrent HashMap to store the stream block id and related offset snapshot */\n+  private lazy val blockOffsetMap =\n+    new ConcurrentHashMap[StreamBlockId, Map[TopicAndPartition, Long]]\n+\n+  private lazy val blockGeneratorListener = new BlockGeneratorListener {\n+    override def onStoreData(data: Any, metadata: Any): Unit = {\n+      if (metadata != null) {\n+        val kafkaMetadata = metadata.asInstanceOf[(TopicAndPartition, Long)]\n+        topicPartitionOffsetMap.put(kafkaMetadata._1, kafkaMetadata._2)\n+      }\n+    }\n+\n+    override def onGenerateBlock(blockId: StreamBlockId): Unit = {\n+      // Get a snapshot of current offset map and store with related block id. Since this hook\n+      // function is called in synchronized block, so we can get the snapshot without explicit lock.\n+      val offsetSnapshot = topicPartitionOffsetMap.toMap\n+      blockOffsetMap.put(blockId, offsetSnapshot)\n+      topicPartitionOffsetMap.clear()\n+    }\n+\n+    override def onPushBlock(blockId: StreamBlockId, arrayBuffer: mutable.ArrayBuffer[_]): Unit = {\n+      store(arrayBuffer.asInstanceOf[mutable.ArrayBuffer[Any]])\n+\n+      // Commit and remove the related offsets.\n+      Option(blockOffsetMap.get(blockId)).foreach { offsetMap =>\n+        commitOffset(offsetMap)\n+      }\n+      blockOffsetMap.remove(blockId)\n+    }\n+\n+    override def onError(message: String, throwable: Throwable): Unit = {\n+      reportError(message, throwable)\n+    }\n+  }\n+\n+  /** Manage the BlockGenerator in receiver itself for better managing block store and offset\n+    * commit */\n+  private var blockGenerator: BlockGenerator = null\n+\n+  override def onStop(): Unit = {\n+    if (consumerConnector != null) {\n+      consumerConnector.shutdown()\n+      consumerConnector = null\n+    }\n+\n+    if (zkClient != null) {\n+      zkClient.close()\n+      zkClient = null\n+    }\n+\n+    blockGenerator.stop()\n+  }\n+\n+  override def onStart(): Unit = {\n+    logInfo(s\"Starting Kafka Consumer Stream with group: $groupId\")\n+\n+    blockGenerator = new BlockGenerator(blockGeneratorListener, streamId, env.conf)\n+\n+    if (kafkaParams.contains(AUTO_OFFSET_COMMIT) && kafkaParams(AUTO_OFFSET_COMMIT) == \"true\") {\n+      logWarning(s\"$AUTO_OFFSET_COMMIT should be set to false in ReliableKafkaReceiver, \" +\n+        \"otherwise we will manually set it to false to turn off auto offset commit in Kafka\")\n+    }\n+\n+    val props = new Properties()\n+    kafkaParams.foreach(param => props.put(param._1, param._2))\n+    // Manually set \"auto.commit.enable\" to \"false\" no matter user explicitly set it to true,\n+    // we have to make sure this property is set to false to turn off auto commit mechanism in\n+    // Kafka.\n+    props.setProperty(AUTO_OFFSET_COMMIT, \"false\")\n+\n+    val consumerConfig = new ConsumerConfig(props)\n+\n+    assert(consumerConfig.autoCommitEnable == false)\n+\n+    logInfo(s\"Connecting to Zookeeper: ${consumerConfig.zkConnect}\")\n+    consumerConnector = Consumer.create(consumerConfig)\n+    logInfo(s\"Connected to Zookeeper: ${consumerConfig.zkConnect}\")\n+\n+    zkClient = new ZkClient(consumerConfig.zkConnect, consumerConfig.zkSessionTimeoutMs,\n+      consumerConfig.zkConnectionTimeoutMs, ZKStringSerializer)\n+\n+    // start BlockGenerator\n+    blockGenerator.start()\n+\n+    val keyDecoder = classTag[U].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+      .newInstance(consumerConfig.props)\n+      .asInstanceOf[Decoder[K]]\n+\n+    val valueDecoder = classTag[T].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+      .newInstance(consumerConfig.props)\n+      .asInstanceOf[Decoder[V]]\n+\n+    val topicMessageStreams = consumerConnector.createMessageStreams(\n+      topics, keyDecoder, valueDecoder)\n+\n+    val executorPool = Executors.newFixedThreadPool(topics.values.sum)"
  }],
  "prId": 2991
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Can you convert the runnable into a specific inner class. This code is very nested and therefore a little hard to read.\n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-07T03:51:24Z",
    "diffHunk": "@@ -0,0 +1,212 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.util.Properties\n+import java.util.concurrent.{ConcurrentHashMap, Executors}\n+\n+import scala.collection.Map\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.consumer.{Consumer, ConsumerConfig, ConsumerConnector}\n+import kafka.serializer.Decoder\n+import kafka.utils.{ZkUtils, ZKGroupTopicDirs, ZKStringSerializer, VerifiableProperties}\n+import org.I0Itec.zkclient.ZkClient\n+\n+import org.apache.spark.{SparkEnv, Logging}\n+import org.apache.spark.storage.{StreamBlockId, StorageLevel}\n+import org.apache.spark.streaming.receiver.{BlockGeneratorListener, BlockGenerator, Receiver}\n+\n+private[streaming]\n+class ReliableKafkaReceiver[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag](\n+    kafkaParams: Map[String, String],\n+    topics: Map[String, Int],\n+    storageLevel: StorageLevel)\n+    extends Receiver[Any](storageLevel) with Logging {\n+\n+  /** High level consumer to connect to Kafka */\n+  private var consumerConnector: ConsumerConnector = null\n+\n+  /** zkClient to connect to Zookeeper to commit the offsets */\n+  private var zkClient: ZkClient = null\n+\n+  private val groupId = kafkaParams(\"group.id\")\n+\n+  private lazy val env = SparkEnv.get\n+\n+  private val AUTO_OFFSET_COMMIT = \"auto.commit.enable\"\n+\n+  /** A HashMap to manage the offset for each topic/partition, this HashMap is called in\n+    * synchronized block, so mutable HashMap will not meet concurrency issue */\n+  private lazy val topicPartitionOffsetMap = new mutable.HashMap[TopicAndPartition, Long]\n+\n+  /** A concurrent HashMap to store the stream block id and related offset snapshot */\n+  private lazy val blockOffsetMap =\n+    new ConcurrentHashMap[StreamBlockId, Map[TopicAndPartition, Long]]\n+\n+  private lazy val blockGeneratorListener = new BlockGeneratorListener {\n+    override def onStoreData(data: Any, metadata: Any): Unit = {\n+      if (metadata != null) {\n+        val kafkaMetadata = metadata.asInstanceOf[(TopicAndPartition, Long)]\n+        topicPartitionOffsetMap.put(kafkaMetadata._1, kafkaMetadata._2)\n+      }\n+    }\n+\n+    override def onGenerateBlock(blockId: StreamBlockId): Unit = {\n+      // Get a snapshot of current offset map and store with related block id. Since this hook\n+      // function is called in synchronized block, so we can get the snapshot without explicit lock.\n+      val offsetSnapshot = topicPartitionOffsetMap.toMap\n+      blockOffsetMap.put(blockId, offsetSnapshot)\n+      topicPartitionOffsetMap.clear()\n+    }\n+\n+    override def onPushBlock(blockId: StreamBlockId, arrayBuffer: mutable.ArrayBuffer[_]): Unit = {\n+      store(arrayBuffer.asInstanceOf[mutable.ArrayBuffer[Any]])\n+\n+      // Commit and remove the related offsets.\n+      Option(blockOffsetMap.get(blockId)).foreach { offsetMap =>\n+        commitOffset(offsetMap)\n+      }\n+      blockOffsetMap.remove(blockId)\n+    }\n+\n+    override def onError(message: String, throwable: Throwable): Unit = {\n+      reportError(message, throwable)\n+    }\n+  }\n+\n+  /** Manage the BlockGenerator in receiver itself for better managing block store and offset\n+    * commit */\n+  private var blockGenerator: BlockGenerator = null\n+\n+  override def onStop(): Unit = {\n+    if (consumerConnector != null) {\n+      consumerConnector.shutdown()\n+      consumerConnector = null\n+    }\n+\n+    if (zkClient != null) {\n+      zkClient.close()\n+      zkClient = null\n+    }\n+\n+    blockGenerator.stop()\n+  }\n+\n+  override def onStart(): Unit = {\n+    logInfo(s\"Starting Kafka Consumer Stream with group: $groupId\")\n+\n+    blockGenerator = new BlockGenerator(blockGeneratorListener, streamId, env.conf)\n+\n+    if (kafkaParams.contains(AUTO_OFFSET_COMMIT) && kafkaParams(AUTO_OFFSET_COMMIT) == \"true\") {\n+      logWarning(s\"$AUTO_OFFSET_COMMIT should be set to false in ReliableKafkaReceiver, \" +\n+        \"otherwise we will manually set it to false to turn off auto offset commit in Kafka\")\n+    }\n+\n+    val props = new Properties()\n+    kafkaParams.foreach(param => props.put(param._1, param._2))\n+    // Manually set \"auto.commit.enable\" to \"false\" no matter user explicitly set it to true,\n+    // we have to make sure this property is set to false to turn off auto commit mechanism in\n+    // Kafka.\n+    props.setProperty(AUTO_OFFSET_COMMIT, \"false\")\n+\n+    val consumerConfig = new ConsumerConfig(props)\n+\n+    assert(consumerConfig.autoCommitEnable == false)\n+\n+    logInfo(s\"Connecting to Zookeeper: ${consumerConfig.zkConnect}\")\n+    consumerConnector = Consumer.create(consumerConfig)\n+    logInfo(s\"Connected to Zookeeper: ${consumerConfig.zkConnect}\")\n+\n+    zkClient = new ZkClient(consumerConfig.zkConnect, consumerConfig.zkSessionTimeoutMs,\n+      consumerConfig.zkConnectionTimeoutMs, ZKStringSerializer)\n+\n+    // start BlockGenerator\n+    blockGenerator.start()\n+\n+    val keyDecoder = classTag[U].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+      .newInstance(consumerConfig.props)\n+      .asInstanceOf[Decoder[K]]\n+\n+    val valueDecoder = classTag[T].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+      .newInstance(consumerConfig.props)\n+      .asInstanceOf[Decoder[V]]\n+\n+    val topicMessageStreams = consumerConnector.createMessageStreams(\n+      topics, keyDecoder, valueDecoder)\n+\n+    val executorPool = Executors.newFixedThreadPool(topics.values.sum)\n+\n+    try {\n+      topicMessageStreams.values.foreach { streams =>\n+        streams.foreach { stream =>\n+          executorPool.submit(new Runnable {"
  }],
  "prId": 2991
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Shouldnt this throw an error and the receiver be stopped? If offset cannot be committed isnt it a wrong execution that should not continue at all? If that is the case, then please call `stop(errorMessage)`\n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-07T04:01:34Z",
    "diffHunk": "@@ -0,0 +1,212 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.util.Properties\n+import java.util.concurrent.{ConcurrentHashMap, Executors}\n+\n+import scala.collection.Map\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.consumer.{Consumer, ConsumerConfig, ConsumerConnector}\n+import kafka.serializer.Decoder\n+import kafka.utils.{ZkUtils, ZKGroupTopicDirs, ZKStringSerializer, VerifiableProperties}\n+import org.I0Itec.zkclient.ZkClient\n+\n+import org.apache.spark.{SparkEnv, Logging}\n+import org.apache.spark.storage.{StreamBlockId, StorageLevel}\n+import org.apache.spark.streaming.receiver.{BlockGeneratorListener, BlockGenerator, Receiver}\n+\n+private[streaming]\n+class ReliableKafkaReceiver[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag](\n+    kafkaParams: Map[String, String],\n+    topics: Map[String, Int],\n+    storageLevel: StorageLevel)\n+    extends Receiver[Any](storageLevel) with Logging {\n+\n+  /** High level consumer to connect to Kafka */\n+  private var consumerConnector: ConsumerConnector = null\n+\n+  /** zkClient to connect to Zookeeper to commit the offsets */\n+  private var zkClient: ZkClient = null\n+\n+  private val groupId = kafkaParams(\"group.id\")\n+\n+  private lazy val env = SparkEnv.get\n+\n+  private val AUTO_OFFSET_COMMIT = \"auto.commit.enable\"\n+\n+  /** A HashMap to manage the offset for each topic/partition, this HashMap is called in\n+    * synchronized block, so mutable HashMap will not meet concurrency issue */\n+  private lazy val topicPartitionOffsetMap = new mutable.HashMap[TopicAndPartition, Long]\n+\n+  /** A concurrent HashMap to store the stream block id and related offset snapshot */\n+  private lazy val blockOffsetMap =\n+    new ConcurrentHashMap[StreamBlockId, Map[TopicAndPartition, Long]]\n+\n+  private lazy val blockGeneratorListener = new BlockGeneratorListener {\n+    override def onStoreData(data: Any, metadata: Any): Unit = {\n+      if (metadata != null) {\n+        val kafkaMetadata = metadata.asInstanceOf[(TopicAndPartition, Long)]\n+        topicPartitionOffsetMap.put(kafkaMetadata._1, kafkaMetadata._2)\n+      }\n+    }\n+\n+    override def onGenerateBlock(blockId: StreamBlockId): Unit = {\n+      // Get a snapshot of current offset map and store with related block id. Since this hook\n+      // function is called in synchronized block, so we can get the snapshot without explicit lock.\n+      val offsetSnapshot = topicPartitionOffsetMap.toMap\n+      blockOffsetMap.put(blockId, offsetSnapshot)\n+      topicPartitionOffsetMap.clear()\n+    }\n+\n+    override def onPushBlock(blockId: StreamBlockId, arrayBuffer: mutable.ArrayBuffer[_]): Unit = {\n+      store(arrayBuffer.asInstanceOf[mutable.ArrayBuffer[Any]])\n+\n+      // Commit and remove the related offsets.\n+      Option(blockOffsetMap.get(blockId)).foreach { offsetMap =>\n+        commitOffset(offsetMap)\n+      }\n+      blockOffsetMap.remove(blockId)\n+    }\n+\n+    override def onError(message: String, throwable: Throwable): Unit = {\n+      reportError(message, throwable)\n+    }\n+  }\n+\n+  /** Manage the BlockGenerator in receiver itself for better managing block store and offset\n+    * commit */\n+  private var blockGenerator: BlockGenerator = null\n+\n+  override def onStop(): Unit = {\n+    if (consumerConnector != null) {\n+      consumerConnector.shutdown()\n+      consumerConnector = null\n+    }\n+\n+    if (zkClient != null) {\n+      zkClient.close()\n+      zkClient = null\n+    }\n+\n+    blockGenerator.stop()\n+  }\n+\n+  override def onStart(): Unit = {\n+    logInfo(s\"Starting Kafka Consumer Stream with group: $groupId\")\n+\n+    blockGenerator = new BlockGenerator(blockGeneratorListener, streamId, env.conf)\n+\n+    if (kafkaParams.contains(AUTO_OFFSET_COMMIT) && kafkaParams(AUTO_OFFSET_COMMIT) == \"true\") {\n+      logWarning(s\"$AUTO_OFFSET_COMMIT should be set to false in ReliableKafkaReceiver, \" +\n+        \"otherwise we will manually set it to false to turn off auto offset commit in Kafka\")\n+    }\n+\n+    val props = new Properties()\n+    kafkaParams.foreach(param => props.put(param._1, param._2))\n+    // Manually set \"auto.commit.enable\" to \"false\" no matter user explicitly set it to true,\n+    // we have to make sure this property is set to false to turn off auto commit mechanism in\n+    // Kafka.\n+    props.setProperty(AUTO_OFFSET_COMMIT, \"false\")\n+\n+    val consumerConfig = new ConsumerConfig(props)\n+\n+    assert(consumerConfig.autoCommitEnable == false)\n+\n+    logInfo(s\"Connecting to Zookeeper: ${consumerConfig.zkConnect}\")\n+    consumerConnector = Consumer.create(consumerConfig)\n+    logInfo(s\"Connected to Zookeeper: ${consumerConfig.zkConnect}\")\n+\n+    zkClient = new ZkClient(consumerConfig.zkConnect, consumerConfig.zkSessionTimeoutMs,\n+      consumerConfig.zkConnectionTimeoutMs, ZKStringSerializer)\n+\n+    // start BlockGenerator\n+    blockGenerator.start()\n+\n+    val keyDecoder = classTag[U].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+      .newInstance(consumerConfig.props)\n+      .asInstanceOf[Decoder[K]]\n+\n+    val valueDecoder = classTag[T].runtimeClass.getConstructor(classOf[VerifiableProperties])\n+      .newInstance(consumerConfig.props)\n+      .asInstanceOf[Decoder[V]]\n+\n+    val topicMessageStreams = consumerConnector.createMessageStreams(\n+      topics, keyDecoder, valueDecoder)\n+\n+    val executorPool = Executors.newFixedThreadPool(topics.values.sum)\n+\n+    try {\n+      topicMessageStreams.values.foreach { streams =>\n+        streams.foreach { stream =>\n+          executorPool.submit(new Runnable {\n+            override def run(): Unit = {\n+              logInfo(s\"Starting message process thread ${Thread.currentThread().getId}.\")\n+              try {\n+                for (msgAndMetadata <- stream) {\n+                  val topicAndPartition = TopicAndPartition(\n+                    msgAndMetadata.topic, msgAndMetadata.partition)\n+                  val metadata = (topicAndPartition, msgAndMetadata.offset)\n+\n+                  blockGenerator += ((msgAndMetadata.key, msgAndMetadata.message), metadata)\n+                }\n+              } catch {\n+                case e: Throwable => logError(\"Error handling message; existing\", e)\n+              }\n+            }\n+          })\n+        }\n+      }\n+    } finally {\n+      executorPool.shutdown()\n+    }\n+  }\n+\n+  /**\n+   * Commit the offset of Kafka's topic/partition, the commit mechanism follow Kafka 0.8.x's\n+   * metadata schema in Zookeeper.\n+   */\n+  private def commitOffset(offsetMap: Map[TopicAndPartition, Long]): Unit = {\n+    if (zkClient == null) {\n+      logError(s\"zkClient $zkClient should be initialized at started\")"
  }],
  "prId": 2991
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Good to define the named class for this generator listener.\n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-07T10:30:07Z",
    "diffHunk": "@@ -0,0 +1,212 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.util.Properties\n+import java.util.concurrent.{ConcurrentHashMap, Executors}\n+\n+import scala.collection.Map\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.consumer.{Consumer, ConsumerConfig, ConsumerConnector}\n+import kafka.serializer.Decoder\n+import kafka.utils.{ZkUtils, ZKGroupTopicDirs, ZKStringSerializer, VerifiableProperties}\n+import org.I0Itec.zkclient.ZkClient\n+\n+import org.apache.spark.{SparkEnv, Logging}\n+import org.apache.spark.storage.{StreamBlockId, StorageLevel}\n+import org.apache.spark.streaming.receiver.{BlockGeneratorListener, BlockGenerator, Receiver}\n+\n+private[streaming]\n+class ReliableKafkaReceiver[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag](\n+    kafkaParams: Map[String, String],\n+    topics: Map[String, Int],\n+    storageLevel: StorageLevel)\n+    extends Receiver[Any](storageLevel) with Logging {\n+\n+  /** High level consumer to connect to Kafka */\n+  private var consumerConnector: ConsumerConnector = null\n+\n+  /** zkClient to connect to Zookeeper to commit the offsets */\n+  private var zkClient: ZkClient = null\n+\n+  private val groupId = kafkaParams(\"group.id\")\n+\n+  private lazy val env = SparkEnv.get\n+\n+  private val AUTO_OFFSET_COMMIT = \"auto.commit.enable\"\n+\n+  /** A HashMap to manage the offset for each topic/partition, this HashMap is called in\n+    * synchronized block, so mutable HashMap will not meet concurrency issue */\n+  private lazy val topicPartitionOffsetMap = new mutable.HashMap[TopicAndPartition, Long]\n+\n+  /** A concurrent HashMap to store the stream block id and related offset snapshot */\n+  private lazy val blockOffsetMap =\n+    new ConcurrentHashMap[StreamBlockId, Map[TopicAndPartition, Long]]\n+\n+  private lazy val blockGeneratorListener = new BlockGeneratorListener {"
  }],
  "prId": 2991
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Nit: Can you please co-locate the vals and the vars? All the vals first and then all the vars.\n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-11T02:49:44Z",
    "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.util.Properties\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.Map\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.consumer.{Consumer, ConsumerConfig, ConsumerConnector, KafkaStream}\n+import kafka.serializer.Decoder\n+import kafka.utils.{ZkUtils, ZKGroupTopicDirs, ZKStringSerializer, VerifiableProperties}\n+import org.I0Itec.zkclient.ZkClient\n+\n+import org.apache.spark.{SparkEnv, Logging}\n+import org.apache.spark.storage.{StreamBlockId, StorageLevel}\n+import org.apache.spark.streaming.receiver.{BlockGeneratorListener, BlockGenerator, Receiver}\n+import org.apache.spark.util.Utils\n+\n+private[streaming]\n+class ReliableKafkaReceiver[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag](\n+    kafkaParams: Map[String, String],\n+    topics: Map[String, Int],\n+    storageLevel: StorageLevel)\n+    extends Receiver[Any](storageLevel) with Logging {\n+\n+  /** High level consumer to connect to Kafka. */\n+  private var consumerConnector: ConsumerConnector = null\n+\n+  /** zkClient to connect to Zookeeper to commit the offsets. */\n+  private var zkClient: ZkClient = null\n+\n+  private val groupId = kafkaParams(\"group.id\")"
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "OK, I will.\n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-11T03:07:53Z",
    "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.util.Properties\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.Map\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.consumer.{Consumer, ConsumerConfig, ConsumerConnector, KafkaStream}\n+import kafka.serializer.Decoder\n+import kafka.utils.{ZkUtils, ZKGroupTopicDirs, ZKStringSerializer, VerifiableProperties}\n+import org.I0Itec.zkclient.ZkClient\n+\n+import org.apache.spark.{SparkEnv, Logging}\n+import org.apache.spark.storage.{StreamBlockId, StorageLevel}\n+import org.apache.spark.streaming.receiver.{BlockGeneratorListener, BlockGenerator, Receiver}\n+import org.apache.spark.util.Utils\n+\n+private[streaming]\n+class ReliableKafkaReceiver[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag](\n+    kafkaParams: Map[String, String],\n+    topics: Map[String, Int],\n+    storageLevel: StorageLevel)\n+    extends Receiver[Any](storageLevel) with Logging {\n+\n+  /** High level consumer to connect to Kafka. */\n+  private var consumerConnector: ConsumerConnector = null\n+\n+  /** zkClient to connect to Zookeeper to commit the offsets. */\n+  private var zkClient: ZkClient = null\n+\n+  private val groupId = kafkaParams(\"group.id\")"
  }],
  "prId": 2991
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "It is fine to leave it as is in this PR, but in future please use `//` comments or `/* ... */` comments for inline commenting. The `/** ... */` style is used only for scala docs.  See https://cwiki.apache.org/confluence/display/SPARK/Spark+Code+Style+Guide#SparkCodeStyleGuide-Codedocumentationstyle\n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-11T03:13:02Z",
    "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.util.Properties\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.Map\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.consumer.{Consumer, ConsumerConfig, ConsumerConnector, KafkaStream}\n+import kafka.serializer.Decoder\n+import kafka.utils.{ZkUtils, ZKGroupTopicDirs, ZKStringSerializer, VerifiableProperties}\n+import org.I0Itec.zkclient.ZkClient\n+\n+import org.apache.spark.{SparkEnv, Logging}\n+import org.apache.spark.storage.{StreamBlockId, StorageLevel}\n+import org.apache.spark.streaming.receiver.{BlockGeneratorListener, BlockGenerator, Receiver}\n+import org.apache.spark.util.Utils\n+\n+private[streaming]\n+class ReliableKafkaReceiver[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag](\n+    kafkaParams: Map[String, String],\n+    topics: Map[String, Int],\n+    storageLevel: StorageLevel)\n+    extends Receiver[Any](storageLevel) with Logging {\n+\n+  /** High level consumer to connect to Kafka. */\n+  private var consumerConnector: ConsumerConnector = null\n+\n+  /** zkClient to connect to Zookeeper to commit the offsets. */\n+  private var zkClient: ZkClient = null\n+\n+  private val groupId = kafkaParams(\"group.id\")\n+\n+  private def conf() = SparkEnv.get.conf\n+\n+  private val AUTO_OFFSET_COMMIT = \"auto.commit.enable\"\n+\n+  /**"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Well, on second thought, these comments could argued in both ways. They could be considered as inline comments or as  scala docs. They are fine to be as they are.\n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-11T03:14:02Z",
    "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.util.Properties\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.Map\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.consumer.{Consumer, ConsumerConfig, ConsumerConnector, KafkaStream}\n+import kafka.serializer.Decoder\n+import kafka.utils.{ZkUtils, ZKGroupTopicDirs, ZKStringSerializer, VerifiableProperties}\n+import org.I0Itec.zkclient.ZkClient\n+\n+import org.apache.spark.{SparkEnv, Logging}\n+import org.apache.spark.storage.{StreamBlockId, StorageLevel}\n+import org.apache.spark.streaming.receiver.{BlockGeneratorListener, BlockGenerator, Receiver}\n+import org.apache.spark.util.Utils\n+\n+private[streaming]\n+class ReliableKafkaReceiver[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag](\n+    kafkaParams: Map[String, String],\n+    topics: Map[String, Int],\n+    storageLevel: StorageLevel)\n+    extends Receiver[Any](storageLevel) with Logging {\n+\n+  /** High level consumer to connect to Kafka. */\n+  private var consumerConnector: ConsumerConnector = null\n+\n+  /** zkClient to connect to Zookeeper to commit the offsets. */\n+  private var zkClient: ZkClient = null\n+\n+  private val groupId = kafkaParams(\"group.id\")\n+\n+  private def conf() = SparkEnv.get.conf\n+\n+  private val AUTO_OFFSET_COMMIT = \"auto.commit.enable\"\n+\n+  /**"
  }],
  "prId": 2991
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Please add scala docs describing this class. It should have the information of how it is uses the `store(multipleObjects)` to get more reliability.\n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-11T04:06:57Z",
    "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.util.Properties\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.Map\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.consumer.{Consumer, ConsumerConfig, ConsumerConnector, KafkaStream}\n+import kafka.serializer.Decoder\n+import kafka.utils.{ZkUtils, ZKGroupTopicDirs, ZKStringSerializer, VerifiableProperties}\n+import org.I0Itec.zkclient.ZkClient\n+\n+import org.apache.spark.{SparkEnv, Logging}\n+import org.apache.spark.storage.{StreamBlockId, StorageLevel}\n+import org.apache.spark.streaming.receiver.{BlockGeneratorListener, BlockGenerator, Receiver}\n+import org.apache.spark.util.Utils\n+\n+private[streaming]"
  }],
  "prId": 2991
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "There is no synchronization between on the blockGenerator here. Since the insertion is synchronized with the block generator, getting the map also has to be synchronized on the same object. Otherwise the topicPartitionOffsetMap can mutate between the time the block is generate and the time the last element (that went into the block) was inserted.\nSo I think the logic is incorrect.\n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-12T10:05:14Z",
    "diffHunk": "@@ -0,0 +1,251 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.util.Properties\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.Map\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.consumer.{Consumer, ConsumerConfig, ConsumerConnector, KafkaStream}\n+import kafka.serializer.Decoder\n+import kafka.utils.{ZkUtils, ZKGroupTopicDirs, ZKStringSerializer, VerifiableProperties}\n+import org.I0Itec.zkclient.ZkClient\n+\n+import org.apache.spark.{SparkEnv, Logging}\n+import org.apache.spark.storage.{StreamBlockId, StorageLevel}\n+import org.apache.spark.streaming.receiver.{BlockGeneratorListener, BlockGenerator, Receiver}\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * ReliableKafkaReceiver offers the ability to reliably store data into BlockManager without loss.\n+ * It is turned off by default and will be enabled when\n+ * spark.streaming.receiver.writeAheadLog.enable is true. The difference compared to KafkaReceiver\n+ * is that this receiver manages topic-partition/offset itself and updates the offset information\n+ * after data is reliably stored as write-ahead log. Offsets will only be updated when data is\n+ * reliably stored, so the potential data loss problem of KafkaReceiver can be eliminated.\n+ *\n+ * Note: ReliableKafkaReceiver will set auto.commit.enable to false to turn off automatic offset\n+ * commit mechanism in Kafka consumer. So setting this configuration manually within kafkaParams\n+ * will not take effect.\n+ */\n+private[streaming]\n+class ReliableKafkaReceiver[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag](\n+    kafkaParams: Map[String, String],\n+    topics: Map[String, Int],\n+    storageLevel: StorageLevel)\n+    extends Receiver[(K, V)](storageLevel) with Logging {\n+\n+  private val groupId = kafkaParams(\"group.id\")\n+\n+  private val AUTO_OFFSET_COMMIT = \"auto.commit.enable\"\n+\n+  private def conf() = SparkEnv.get.conf\n+\n+  /** High level consumer to connect to Kafka. */\n+  private var consumerConnector: ConsumerConnector = null\n+\n+  /** zkClient to connect to Zookeeper to commit the offsets. */\n+  private var zkClient: ZkClient = null\n+\n+  /**\n+   * A HashMap to manage the offset for each topic/partition, this HashMap is called in\n+   * synchronized block, so mutable HashMap will not meet concurrency issue.\n+   */\n+  private var topicPartitionOffsetMap: mutable.HashMap[TopicAndPartition, Long] = null\n+\n+  /** A concurrent HashMap to store the stream block id and related offset snapshot. */\n+  private var blockOffsetMap: ConcurrentHashMap[StreamBlockId, Map[TopicAndPartition, Long]] = null\n+\n+  /**\n+   * Manage the BlockGenerator in receiver itself for better managing block store and offset\n+   * commit.\n+   */\n+  private var blockGenerator: BlockGenerator = null\n+\n+  /** Kafka offsets checkpoint listener to register into BlockGenerator for offsets checkpoint. */\n+  private final class OffsetCheckpointListener extends BlockGeneratorListener {\n+\n+    override def onGenerateBlock(blockId: StreamBlockId): Unit = {\n+      // Get a snapshot of current offset map and store with related block id. Since this hook\n+      // function is called in synchronized block, so we can get the snapshot without explicit lock.\n+      val offsetSnapshot = topicPartitionOffsetMap.toMap"
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "Because this hook function `onGenerateBlock` is called in function `updateCurrentBuffer`, which is synchronized with BlockGenerator object, so  I don't think we need to add another lock here. At the time `updateCurrentBuffer` is called, because we get a lock, so there will be no data add into BlockGenerator and insert offset into topicPartitionOffsetMap.\n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-12T12:34:56Z",
    "diffHunk": "@@ -0,0 +1,251 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.util.Properties\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.Map\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.consumer.{Consumer, ConsumerConfig, ConsumerConnector, KafkaStream}\n+import kafka.serializer.Decoder\n+import kafka.utils.{ZkUtils, ZKGroupTopicDirs, ZKStringSerializer, VerifiableProperties}\n+import org.I0Itec.zkclient.ZkClient\n+\n+import org.apache.spark.{SparkEnv, Logging}\n+import org.apache.spark.storage.{StreamBlockId, StorageLevel}\n+import org.apache.spark.streaming.receiver.{BlockGeneratorListener, BlockGenerator, Receiver}\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * ReliableKafkaReceiver offers the ability to reliably store data into BlockManager without loss.\n+ * It is turned off by default and will be enabled when\n+ * spark.streaming.receiver.writeAheadLog.enable is true. The difference compared to KafkaReceiver\n+ * is that this receiver manages topic-partition/offset itself and updates the offset information\n+ * after data is reliably stored as write-ahead log. Offsets will only be updated when data is\n+ * reliably stored, so the potential data loss problem of KafkaReceiver can be eliminated.\n+ *\n+ * Note: ReliableKafkaReceiver will set auto.commit.enable to false to turn off automatic offset\n+ * commit mechanism in Kafka consumer. So setting this configuration manually within kafkaParams\n+ * will not take effect.\n+ */\n+private[streaming]\n+class ReliableKafkaReceiver[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag](\n+    kafkaParams: Map[String, String],\n+    topics: Map[String, Int],\n+    storageLevel: StorageLevel)\n+    extends Receiver[(K, V)](storageLevel) with Logging {\n+\n+  private val groupId = kafkaParams(\"group.id\")\n+\n+  private val AUTO_OFFSET_COMMIT = \"auto.commit.enable\"\n+\n+  private def conf() = SparkEnv.get.conf\n+\n+  /** High level consumer to connect to Kafka. */\n+  private var consumerConnector: ConsumerConnector = null\n+\n+  /** zkClient to connect to Zookeeper to commit the offsets. */\n+  private var zkClient: ZkClient = null\n+\n+  /**\n+   * A HashMap to manage the offset for each topic/partition, this HashMap is called in\n+   * synchronized block, so mutable HashMap will not meet concurrency issue.\n+   */\n+  private var topicPartitionOffsetMap: mutable.HashMap[TopicAndPartition, Long] = null\n+\n+  /** A concurrent HashMap to store the stream block id and related offset snapshot. */\n+  private var blockOffsetMap: ConcurrentHashMap[StreamBlockId, Map[TopicAndPartition, Long]] = null\n+\n+  /**\n+   * Manage the BlockGenerator in receiver itself for better managing block store and offset\n+   * commit.\n+   */\n+  private var blockGenerator: BlockGenerator = null\n+\n+  /** Kafka offsets checkpoint listener to register into BlockGenerator for offsets checkpoint. */\n+  private final class OffsetCheckpointListener extends BlockGeneratorListener {\n+\n+    override def onGenerateBlock(blockId: StreamBlockId): Unit = {\n+      // Get a snapshot of current offset map and store with related block id. Since this hook\n+      // function is called in synchronized block, so we can get the snapshot without explicit lock.\n+      val offsetSnapshot = topicPartitionOffsetMap.toMap"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Aaaah, I get it but this is soooo non-intuitive. That synchronized is no in this file. This is a hard logic to understand. Maybe we should separate out the locked functionality into two locks. The lock of the BlockGenerator is used to replace the ArrayBuffer in the BlockGenerator, and in the ReliableKafkaReceiver, another lock is used to update the offsets. Even though there are two locks, thats a cleaner design as there is a clean separation of functionality in the locks - the BlockGenerator lock does not need to be concerned with the Receiver lock, and the Receiver lock should not have to worry about locks in BlockGenerator (as long as deadlock is avoided). To avoid deadlocks, the callbacks should not be called from within synchornized sections in the BlockGenerator.\n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-12T20:16:20Z",
    "diffHunk": "@@ -0,0 +1,251 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.util.Properties\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.Map\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.consumer.{Consumer, ConsumerConfig, ConsumerConnector, KafkaStream}\n+import kafka.serializer.Decoder\n+import kafka.utils.{ZkUtils, ZKGroupTopicDirs, ZKStringSerializer, VerifiableProperties}\n+import org.I0Itec.zkclient.ZkClient\n+\n+import org.apache.spark.{SparkEnv, Logging}\n+import org.apache.spark.storage.{StreamBlockId, StorageLevel}\n+import org.apache.spark.streaming.receiver.{BlockGeneratorListener, BlockGenerator, Receiver}\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * ReliableKafkaReceiver offers the ability to reliably store data into BlockManager without loss.\n+ * It is turned off by default and will be enabled when\n+ * spark.streaming.receiver.writeAheadLog.enable is true. The difference compared to KafkaReceiver\n+ * is that this receiver manages topic-partition/offset itself and updates the offset information\n+ * after data is reliably stored as write-ahead log. Offsets will only be updated when data is\n+ * reliably stored, so the potential data loss problem of KafkaReceiver can be eliminated.\n+ *\n+ * Note: ReliableKafkaReceiver will set auto.commit.enable to false to turn off automatic offset\n+ * commit mechanism in Kafka consumer. So setting this configuration manually within kafkaParams\n+ * will not take effect.\n+ */\n+private[streaming]\n+class ReliableKafkaReceiver[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag](\n+    kafkaParams: Map[String, String],\n+    topics: Map[String, Int],\n+    storageLevel: StorageLevel)\n+    extends Receiver[(K, V)](storageLevel) with Logging {\n+\n+  private val groupId = kafkaParams(\"group.id\")\n+\n+  private val AUTO_OFFSET_COMMIT = \"auto.commit.enable\"\n+\n+  private def conf() = SparkEnv.get.conf\n+\n+  /** High level consumer to connect to Kafka. */\n+  private var consumerConnector: ConsumerConnector = null\n+\n+  /** zkClient to connect to Zookeeper to commit the offsets. */\n+  private var zkClient: ZkClient = null\n+\n+  /**\n+   * A HashMap to manage the offset for each topic/partition, this HashMap is called in\n+   * synchronized block, so mutable HashMap will not meet concurrency issue.\n+   */\n+  private var topicPartitionOffsetMap: mutable.HashMap[TopicAndPartition, Long] = null\n+\n+  /** A concurrent HashMap to store the stream block id and related offset snapshot. */\n+  private var blockOffsetMap: ConcurrentHashMap[StreamBlockId, Map[TopicAndPartition, Long]] = null\n+\n+  /**\n+   * Manage the BlockGenerator in receiver itself for better managing block store and offset\n+   * commit.\n+   */\n+  private var blockGenerator: BlockGenerator = null\n+\n+  /** Kafka offsets checkpoint listener to register into BlockGenerator for offsets checkpoint. */\n+  private final class OffsetCheckpointListener extends BlockGeneratorListener {\n+\n+    override def onGenerateBlock(blockId: StreamBlockId): Unit = {\n+      // Get a snapshot of current offset map and store with related block id. Since this hook\n+      // function is called in synchronized block, so we can get the snapshot without explicit lock.\n+      val offsetSnapshot = topicPartitionOffsetMap.toMap"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Let me try to modify the code to give you a sense of the what I think is a better design. Only then will I know whether what I am thinking makes sense or not.\n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-12T20:17:23Z",
    "diffHunk": "@@ -0,0 +1,251 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.util.Properties\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.Map\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.consumer.{Consumer, ConsumerConfig, ConsumerConnector, KafkaStream}\n+import kafka.serializer.Decoder\n+import kafka.utils.{ZkUtils, ZKGroupTopicDirs, ZKStringSerializer, VerifiableProperties}\n+import org.I0Itec.zkclient.ZkClient\n+\n+import org.apache.spark.{SparkEnv, Logging}\n+import org.apache.spark.storage.{StreamBlockId, StorageLevel}\n+import org.apache.spark.streaming.receiver.{BlockGeneratorListener, BlockGenerator, Receiver}\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * ReliableKafkaReceiver offers the ability to reliably store data into BlockManager without loss.\n+ * It is turned off by default and will be enabled when\n+ * spark.streaming.receiver.writeAheadLog.enable is true. The difference compared to KafkaReceiver\n+ * is that this receiver manages topic-partition/offset itself and updates the offset information\n+ * after data is reliably stored as write-ahead log. Offsets will only be updated when data is\n+ * reliably stored, so the potential data loss problem of KafkaReceiver can be eliminated.\n+ *\n+ * Note: ReliableKafkaReceiver will set auto.commit.enable to false to turn off automatic offset\n+ * commit mechanism in Kafka consumer. So setting this configuration manually within kafkaParams\n+ * will not take effect.\n+ */\n+private[streaming]\n+class ReliableKafkaReceiver[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag](\n+    kafkaParams: Map[String, String],\n+    topics: Map[String, Int],\n+    storageLevel: StorageLevel)\n+    extends Receiver[(K, V)](storageLevel) with Logging {\n+\n+  private val groupId = kafkaParams(\"group.id\")\n+\n+  private val AUTO_OFFSET_COMMIT = \"auto.commit.enable\"\n+\n+  private def conf() = SparkEnv.get.conf\n+\n+  /** High level consumer to connect to Kafka. */\n+  private var consumerConnector: ConsumerConnector = null\n+\n+  /** zkClient to connect to Zookeeper to commit the offsets. */\n+  private var zkClient: ZkClient = null\n+\n+  /**\n+   * A HashMap to manage the offset for each topic/partition, this HashMap is called in\n+   * synchronized block, so mutable HashMap will not meet concurrency issue.\n+   */\n+  private var topicPartitionOffsetMap: mutable.HashMap[TopicAndPartition, Long] = null\n+\n+  /** A concurrent HashMap to store the stream block id and related offset snapshot. */\n+  private var blockOffsetMap: ConcurrentHashMap[StreamBlockId, Map[TopicAndPartition, Long]] = null\n+\n+  /**\n+   * Manage the BlockGenerator in receiver itself for better managing block store and offset\n+   * commit.\n+   */\n+  private var blockGenerator: BlockGenerator = null\n+\n+  /** Kafka offsets checkpoint listener to register into BlockGenerator for offsets checkpoint. */\n+  private final class OffsetCheckpointListener extends BlockGeneratorListener {\n+\n+    override def onGenerateBlock(blockId: StreamBlockId): Unit = {\n+      // Get a snapshot of current offset map and store with related block id. Since this hook\n+      // function is called in synchronized block, so we can get the snapshot without explicit lock.\n+      val offsetSnapshot = topicPartitionOffsetMap.toMap"
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "Yes, that sounds great, previously synchronized section in BlockGenerator is method, we can refine the lock section to make it better performance. \n\nThere's one thing I think should take care: add message into BlockGenerator and update offset should be in one synchronized section, and be exclusive to the stream block generation and offset snapshot cut (the latter two action also should be in the same synchronized section). Otherwise we will get wrong result potentially.\n\nIf we introduce 2 locks, we should also follow the previous rule and avoid dead lock. \n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-13T00:49:06Z",
    "diffHunk": "@@ -0,0 +1,251 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.util.Properties\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.Map\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.consumer.{Consumer, ConsumerConfig, ConsumerConnector, KafkaStream}\n+import kafka.serializer.Decoder\n+import kafka.utils.{ZkUtils, ZKGroupTopicDirs, ZKStringSerializer, VerifiableProperties}\n+import org.I0Itec.zkclient.ZkClient\n+\n+import org.apache.spark.{SparkEnv, Logging}\n+import org.apache.spark.storage.{StreamBlockId, StorageLevel}\n+import org.apache.spark.streaming.receiver.{BlockGeneratorListener, BlockGenerator, Receiver}\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * ReliableKafkaReceiver offers the ability to reliably store data into BlockManager without loss.\n+ * It is turned off by default and will be enabled when\n+ * spark.streaming.receiver.writeAheadLog.enable is true. The difference compared to KafkaReceiver\n+ * is that this receiver manages topic-partition/offset itself and updates the offset information\n+ * after data is reliably stored as write-ahead log. Offsets will only be updated when data is\n+ * reliably stored, so the potential data loss problem of KafkaReceiver can be eliminated.\n+ *\n+ * Note: ReliableKafkaReceiver will set auto.commit.enable to false to turn off automatic offset\n+ * commit mechanism in Kafka consumer. So setting this configuration manually within kafkaParams\n+ * will not take effect.\n+ */\n+private[streaming]\n+class ReliableKafkaReceiver[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag](\n+    kafkaParams: Map[String, String],\n+    topics: Map[String, Int],\n+    storageLevel: StorageLevel)\n+    extends Receiver[(K, V)](storageLevel) with Logging {\n+\n+  private val groupId = kafkaParams(\"group.id\")\n+\n+  private val AUTO_OFFSET_COMMIT = \"auto.commit.enable\"\n+\n+  private def conf() = SparkEnv.get.conf\n+\n+  /** High level consumer to connect to Kafka. */\n+  private var consumerConnector: ConsumerConnector = null\n+\n+  /** zkClient to connect to Zookeeper to commit the offsets. */\n+  private var zkClient: ZkClient = null\n+\n+  /**\n+   * A HashMap to manage the offset for each topic/partition, this HashMap is called in\n+   * synchronized block, so mutable HashMap will not meet concurrency issue.\n+   */\n+  private var topicPartitionOffsetMap: mutable.HashMap[TopicAndPartition, Long] = null\n+\n+  /** A concurrent HashMap to store the stream block id and related offset snapshot. */\n+  private var blockOffsetMap: ConcurrentHashMap[StreamBlockId, Map[TopicAndPartition, Long]] = null\n+\n+  /**\n+   * Manage the BlockGenerator in receiver itself for better managing block store and offset\n+   * commit.\n+   */\n+  private var blockGenerator: BlockGenerator = null\n+\n+  /** Kafka offsets checkpoint listener to register into BlockGenerator for offsets checkpoint. */\n+  private final class OffsetCheckpointListener extends BlockGeneratorListener {\n+\n+    override def onGenerateBlock(blockId: StreamBlockId): Unit = {\n+      // Get a snapshot of current offset map and store with related block id. Since this hook\n+      // function is called in synchronized block, so we can get the snapshot without explicit lock.\n+      val offsetSnapshot = topicPartitionOffsetMap.toMap"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "I spent some time on this today, and I am almost there. \nIn fact there were a bunch issues with the unit tests as well (leaking context in case of failure, tests being run twice, etc.). I refactored them and I have this branch. Not all the unit tests in the new testsuite pass (they are commented out) can you take a look.\n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-13T00:51:27Z",
    "diffHunk": "@@ -0,0 +1,251 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.util.Properties\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.Map\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.consumer.{Consumer, ConsumerConfig, ConsumerConnector, KafkaStream}\n+import kafka.serializer.Decoder\n+import kafka.utils.{ZkUtils, ZKGroupTopicDirs, ZKStringSerializer, VerifiableProperties}\n+import org.I0Itec.zkclient.ZkClient\n+\n+import org.apache.spark.{SparkEnv, Logging}\n+import org.apache.spark.storage.{StreamBlockId, StorageLevel}\n+import org.apache.spark.streaming.receiver.{BlockGeneratorListener, BlockGenerator, Receiver}\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * ReliableKafkaReceiver offers the ability to reliably store data into BlockManager without loss.\n+ * It is turned off by default and will be enabled when\n+ * spark.streaming.receiver.writeAheadLog.enable is true. The difference compared to KafkaReceiver\n+ * is that this receiver manages topic-partition/offset itself and updates the offset information\n+ * after data is reliably stored as write-ahead log. Offsets will only be updated when data is\n+ * reliably stored, so the potential data loss problem of KafkaReceiver can be eliminated.\n+ *\n+ * Note: ReliableKafkaReceiver will set auto.commit.enable to false to turn off automatic offset\n+ * commit mechanism in Kafka consumer. So setting this configuration manually within kafkaParams\n+ * will not take effect.\n+ */\n+private[streaming]\n+class ReliableKafkaReceiver[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag](\n+    kafkaParams: Map[String, String],\n+    topics: Map[String, Int],\n+    storageLevel: StorageLevel)\n+    extends Receiver[(K, V)](storageLevel) with Logging {\n+\n+  private val groupId = kafkaParams(\"group.id\")\n+\n+  private val AUTO_OFFSET_COMMIT = \"auto.commit.enable\"\n+\n+  private def conf() = SparkEnv.get.conf\n+\n+  /** High level consumer to connect to Kafka. */\n+  private var consumerConnector: ConsumerConnector = null\n+\n+  /** zkClient to connect to Zookeeper to commit the offsets. */\n+  private var zkClient: ZkClient = null\n+\n+  /**\n+   * A HashMap to manage the offset for each topic/partition, this HashMap is called in\n+   * synchronized block, so mutable HashMap will not meet concurrency issue.\n+   */\n+  private var topicPartitionOffsetMap: mutable.HashMap[TopicAndPartition, Long] = null\n+\n+  /** A concurrent HashMap to store the stream block id and related offset snapshot. */\n+  private var blockOffsetMap: ConcurrentHashMap[StreamBlockId, Map[TopicAndPartition, Long]] = null\n+\n+  /**\n+   * Manage the BlockGenerator in receiver itself for better managing block store and offset\n+   * commit.\n+   */\n+  private var blockGenerator: BlockGenerator = null\n+\n+  /** Kafka offsets checkpoint listener to register into BlockGenerator for offsets checkpoint. */\n+  private final class OffsetCheckpointListener extends BlockGeneratorListener {\n+\n+    override def onGenerateBlock(blockId: StreamBlockId): Unit = {\n+      // Get a snapshot of current offset map and store with related block id. Since this hook\n+      // function is called in synchronized block, so we can get the snapshot without explicit lock.\n+      val offsetSnapshot = topicPartitionOffsetMap.toMap"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Here is the Pull Request to your repository. Please take a look.\nhttps://github.com/jerryshao/apache-spark/pull/5\n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-13T00:56:13Z",
    "diffHunk": "@@ -0,0 +1,251 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.util.Properties\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.Map\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.consumer.{Consumer, ConsumerConfig, ConsumerConnector, KafkaStream}\n+import kafka.serializer.Decoder\n+import kafka.utils.{ZkUtils, ZKGroupTopicDirs, ZKStringSerializer, VerifiableProperties}\n+import org.I0Itec.zkclient.ZkClient\n+\n+import org.apache.spark.{SparkEnv, Logging}\n+import org.apache.spark.storage.{StreamBlockId, StorageLevel}\n+import org.apache.spark.streaming.receiver.{BlockGeneratorListener, BlockGenerator, Receiver}\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * ReliableKafkaReceiver offers the ability to reliably store data into BlockManager without loss.\n+ * It is turned off by default and will be enabled when\n+ * spark.streaming.receiver.writeAheadLog.enable is true. The difference compared to KafkaReceiver\n+ * is that this receiver manages topic-partition/offset itself and updates the offset information\n+ * after data is reliably stored as write-ahead log. Offsets will only be updated when data is\n+ * reliably stored, so the potential data loss problem of KafkaReceiver can be eliminated.\n+ *\n+ * Note: ReliableKafkaReceiver will set auto.commit.enable to false to turn off automatic offset\n+ * commit mechanism in Kafka consumer. So setting this configuration manually within kafkaParams\n+ * will not take effect.\n+ */\n+private[streaming]\n+class ReliableKafkaReceiver[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag](\n+    kafkaParams: Map[String, String],\n+    topics: Map[String, Int],\n+    storageLevel: StorageLevel)\n+    extends Receiver[(K, V)](storageLevel) with Logging {\n+\n+  private val groupId = kafkaParams(\"group.id\")\n+\n+  private val AUTO_OFFSET_COMMIT = \"auto.commit.enable\"\n+\n+  private def conf() = SparkEnv.get.conf\n+\n+  /** High level consumer to connect to Kafka. */\n+  private var consumerConnector: ConsumerConnector = null\n+\n+  /** zkClient to connect to Zookeeper to commit the offsets. */\n+  private var zkClient: ZkClient = null\n+\n+  /**\n+   * A HashMap to manage the offset for each topic/partition, this HashMap is called in\n+   * synchronized block, so mutable HashMap will not meet concurrency issue.\n+   */\n+  private var topicPartitionOffsetMap: mutable.HashMap[TopicAndPartition, Long] = null\n+\n+  /** A concurrent HashMap to store the stream block id and related offset snapshot. */\n+  private var blockOffsetMap: ConcurrentHashMap[StreamBlockId, Map[TopicAndPartition, Long]] = null\n+\n+  /**\n+   * Manage the BlockGenerator in receiver itself for better managing block store and offset\n+   * commit.\n+   */\n+  private var blockGenerator: BlockGenerator = null\n+\n+  /** Kafka offsets checkpoint listener to register into BlockGenerator for offsets checkpoint. */\n+  private final class OffsetCheckpointListener extends BlockGeneratorListener {\n+\n+    override def onGenerateBlock(blockId: StreamBlockId): Unit = {\n+      // Get a snapshot of current offset map and store with related block id. Since this hook\n+      // function is called in synchronized block, so we can get the snapshot without explicit lock.\n+      val offsetSnapshot = topicPartitionOffsetMap.toMap"
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "Yeah, I will, thanks a lot.\n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-13T00:58:07Z",
    "diffHunk": "@@ -0,0 +1,251 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.util.Properties\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import scala.collection.Map\n+import scala.collection.mutable\n+import scala.reflect.{classTag, ClassTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.consumer.{Consumer, ConsumerConfig, ConsumerConnector, KafkaStream}\n+import kafka.serializer.Decoder\n+import kafka.utils.{ZkUtils, ZKGroupTopicDirs, ZKStringSerializer, VerifiableProperties}\n+import org.I0Itec.zkclient.ZkClient\n+\n+import org.apache.spark.{SparkEnv, Logging}\n+import org.apache.spark.storage.{StreamBlockId, StorageLevel}\n+import org.apache.spark.streaming.receiver.{BlockGeneratorListener, BlockGenerator, Receiver}\n+import org.apache.spark.util.Utils\n+\n+\n+/**\n+ * ReliableKafkaReceiver offers the ability to reliably store data into BlockManager without loss.\n+ * It is turned off by default and will be enabled when\n+ * spark.streaming.receiver.writeAheadLog.enable is true. The difference compared to KafkaReceiver\n+ * is that this receiver manages topic-partition/offset itself and updates the offset information\n+ * after data is reliably stored as write-ahead log. Offsets will only be updated when data is\n+ * reliably stored, so the potential data loss problem of KafkaReceiver can be eliminated.\n+ *\n+ * Note: ReliableKafkaReceiver will set auto.commit.enable to false to turn off automatic offset\n+ * commit mechanism in Kafka consumer. So setting this configuration manually within kafkaParams\n+ * will not take effect.\n+ */\n+private[streaming]\n+class ReliableKafkaReceiver[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag](\n+    kafkaParams: Map[String, String],\n+    topics: Map[String, Int],\n+    storageLevel: StorageLevel)\n+    extends Receiver[(K, V)](storageLevel) with Logging {\n+\n+  private val groupId = kafkaParams(\"group.id\")\n+\n+  private val AUTO_OFFSET_COMMIT = \"auto.commit.enable\"\n+\n+  private def conf() = SparkEnv.get.conf\n+\n+  /** High level consumer to connect to Kafka. */\n+  private var consumerConnector: ConsumerConnector = null\n+\n+  /** zkClient to connect to Zookeeper to commit the offsets. */\n+  private var zkClient: ZkClient = null\n+\n+  /**\n+   * A HashMap to manage the offset for each topic/partition, this HashMap is called in\n+   * synchronized block, so mutable HashMap will not meet concurrency issue.\n+   */\n+  private var topicPartitionOffsetMap: mutable.HashMap[TopicAndPartition, Long] = null\n+\n+  /** A concurrent HashMap to store the stream block id and related offset snapshot. */\n+  private var blockOffsetMap: ConcurrentHashMap[StreamBlockId, Map[TopicAndPartition, Long]] = null\n+\n+  /**\n+   * Manage the BlockGenerator in receiver itself for better managing block store and offset\n+   * commit.\n+   */\n+  private var blockGenerator: BlockGenerator = null\n+\n+  /** Kafka offsets checkpoint listener to register into BlockGenerator for offsets checkpoint. */\n+  private final class OffsetCheckpointListener extends BlockGeneratorListener {\n+\n+    override def onGenerateBlock(blockId: StreamBlockId): Unit = {\n+      // Get a snapshot of current offset map and store with related block id. Since this hook\n+      // function is called in synchronized block, so we can get the snapshot without explicit lock.\n+      val offsetSnapshot = topicPartitionOffsetMap.toMap"
  }],
  "prId": 2991
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Good addition!\n",
    "commit": "5461f1c43b0e98aa7b583f14569eefd833b19df0",
    "createdAt": "2014-11-13T03:40:26Z",
    "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import java.util.Properties\n+import java.util.concurrent.{ThreadPoolExecutor, ConcurrentHashMap}\n+\n+import scala.collection.{Map, mutable}\n+import scala.reflect.{ClassTag, classTag}\n+\n+import kafka.common.TopicAndPartition\n+import kafka.consumer.{Consumer, ConsumerConfig, ConsumerConnector, KafkaStream}\n+import kafka.message.MessageAndMetadata\n+import kafka.serializer.Decoder\n+import kafka.utils.{VerifiableProperties, ZKGroupTopicDirs, ZKStringSerializer, ZkUtils}\n+import org.I0Itec.zkclient.ZkClient\n+\n+import org.apache.spark.{Logging, SparkEnv}\n+import org.apache.spark.storage.{StorageLevel, StreamBlockId}\n+import org.apache.spark.streaming.receiver.{BlockGenerator, BlockGeneratorListener, Receiver}\n+import org.apache.spark.util.Utils\n+\n+/**\n+ * ReliableKafkaReceiver offers the ability to reliably store data into BlockManager without loss.\n+ * It is turned off by default and will be enabled when\n+ * spark.streaming.receiver.writeAheadLog.enable is true. The difference compared to KafkaReceiver\n+ * is that this receiver manages topic-partition/offset itself and updates the offset information\n+ * after data is reliably stored as write-ahead log. Offsets will only be updated when data is\n+ * reliably stored, so the potential data loss problem of KafkaReceiver can be eliminated.\n+ *\n+ * Note: ReliableKafkaReceiver will set auto.commit.enable to false to turn off automatic offset\n+ * commit mechanism in Kafka consumer. So setting this configuration manually within kafkaParams\n+ * will not take effect.\n+ */\n+private[streaming]\n+class ReliableKafkaReceiver[\n+  K: ClassTag,\n+  V: ClassTag,\n+  U <: Decoder[_]: ClassTag,\n+  T <: Decoder[_]: ClassTag](\n+    kafkaParams: Map[String, String],\n+    topics: Map[String, Int],\n+    storageLevel: StorageLevel)\n+    extends Receiver[(K, V)](storageLevel) with Logging {\n+\n+  private val groupId = kafkaParams(\"group.id\")\n+  private val AUTO_OFFSET_COMMIT = \"auto.commit.enable\"\n+  private def conf = SparkEnv.get.conf\n+\n+  /** High level consumer to connect to Kafka. */\n+  private var consumerConnector: ConsumerConnector = null\n+\n+  /** zkClient to connect to Zookeeper to commit the offsets. */\n+  private var zkClient: ZkClient = null\n+\n+  /**\n+   * A HashMap to manage the offset for each topic/partition, this HashMap is called in\n+   * synchronized block, so mutable HashMap will not meet concurrency issue.\n+   */\n+  private var topicPartitionOffsetMap: mutable.HashMap[TopicAndPartition, Long] = null\n+\n+  /** A concurrent HashMap to store the stream block id and related offset snapshot. */\n+  private var blockOffsetMap: ConcurrentHashMap[StreamBlockId, Map[TopicAndPartition, Long]] = null\n+\n+  /**\n+   * Manage the BlockGenerator in receiver itself for better managing block store and offset\n+   * commit.\n+   */\n+  private var blockGenerator: BlockGenerator = null\n+\n+  /** Threadpool running the handlers for receiving message from multiple topics and partitions. */\n+  private var messageHandlerThreadPool: ThreadPoolExecutor = null\n+\n+  override def onStart(): Unit = {\n+    logInfo(s\"Starting Kafka Consumer Stream with group: $groupId\")\n+\n+    // Initialize the topic-partition / offset hash map.\n+    topicPartitionOffsetMap = new mutable.HashMap[TopicAndPartition, Long]\n+\n+    // Initialize the stream block id / offset snapshot hash map.\n+    blockOffsetMap = new ConcurrentHashMap[StreamBlockId, Map[TopicAndPartition, Long]]()\n+\n+    // Initialize the block generator for storing Kafka message.\n+    blockGenerator = new BlockGenerator(new GeneratedBlockHandler, streamId, conf)\n+\n+    if (kafkaParams.contains(AUTO_OFFSET_COMMIT) && kafkaParams(AUTO_OFFSET_COMMIT) == \"true\") {\n+      logWarning(s\"$AUTO_OFFSET_COMMIT should be set to false in ReliableKafkaReceiver, \" +",
    "line": 102
  }],
  "prId": 2991
}]