[{
  "comments": [{
    "author": {
      "login": "mariobriggs"
    },
    "body": "any particular reason u moved the errXXX methods here to KafkaUtils ?\n",
    "commit": "229b773f9c5c894088a29937f241ad0db48991ba",
    "createdAt": "2016-02-08T18:15:50Z",
    "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.streaming.kafka\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import kafka.common.TopicAndPartition\n+import kafka.serializer.Decoder\n+\n+import org.apache.spark.{Logging, Partition, SparkContext, TaskContext}\n+import org.apache.spark.partial.{BoundedDouble, PartialResult}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+  * A batch-oriented interface for consuming from Kafka.\n+  * Starting and ending offsets are specified in advance,\n+  * so that you can control exactly-once semantics.\n+  *\n+  * @param kafkaParams Kafka <a href=\"http://kafka.apache.org/documentation.html#configuration\">\n+  * configuration parameters</a>. Requires \"metadata.broker.list\" or \"bootstrap.servers\" to be set\n+  * with Kafka broker(s) specified in host1:port1,host2:port2 form.\n+  * @param offsetRanges offset ranges that define the Kafka data belonging to this RDD\n+  */\n+private[kafka]\n+abstract class KafkaRDDBase[\n+K: ClassTag,\n+V: ClassTag,\n+U <: Decoder[_] : ClassTag,\n+T <: Decoder[_] : ClassTag,\n+R: ClassTag] private[spark](\n+  sc: SparkContext,\n+  kafkaParams: Map[String, String],\n+  offsetRanges: Array[OffsetRange],\n+  leaders: Map[TopicAndPartition, (String, Int)]\n+) extends RDD[R](sc, Nil) with Logging with HasOffsetRanges {\n+\n+  override def count(): Long = offsetRanges.map(_.count).sum\n+\n+  override def countApprox(\n+    timeout: Long,\n+    confidence: Double = 0.95\n+  ): PartialResult[BoundedDouble] = {\n+    val c = count\n+    new PartialResult(new BoundedDouble(c, 1.0, c, c), true)\n+  }\n+\n+  override def isEmpty(): Boolean = count == 0L\n+\n+  override def take(num: Int): Array[R] = {\n+    val nonEmptyPartitions = this.partitions\n+      .map(_.asInstanceOf[KafkaRDDPartition])\n+      .filter(_.count > 0)\n+\n+    if (num < 1 || nonEmptyPartitions.size < 1) {\n+      return new Array[R](0)\n+    }\n+\n+    // Determine in advance how many messages need to be taken from each partition\n+    val parts = nonEmptyPartitions.foldLeft(Map[Int, Int]()) { (result, part) =>\n+      val remain = num - result.values.sum\n+      if (remain > 0) {\n+        val taken = Math.min(remain, part.count)\n+        result + (part.index -> taken.toInt)\n+      } else {\n+        result\n+      }\n+    }\n+\n+    val buf = new ArrayBuffer[R]\n+    val res = context.runJob(\n+      this,\n+      (tc: TaskContext, it: Iterator[R]) => it.take(parts(tc.partitionId)).toArray,\n+      parts.keys.toArray)\n+    res.foreach(buf ++= _)\n+    buf.toArray\n+  }\n+\n+  override def getPreferredLocations(thePart: Partition): Seq[String] = {\n+    val part = thePart.asInstanceOf[KafkaRDDPartition]\n+    // TODO is additional hostname resolution necessary here\n+    if (part.host != null) {\n+      Seq(part.host)\n+    } else {\n+      Seq()\n+    }\n+  }\n+",
    "line": 103
  }],
  "prId": 10953
}]