[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "This does not need to be on a new line.\n",
    "commit": "f93a07c9e6cd4b34a276e9c75aa601fdafa67ef7",
    "createdAt": "2014-08-27T02:07:49Z",
    "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.google.common.base.Throwables\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.streaming.flume.sink._\n+\n+private[flume] class FlumePollingRunnable(val parent: FlumePollingReceiver) extends Runnable with\n+Logging {\n+\n+  def run(): Unit = {\n+    while (!parent.isStopped()) {\n+      val connection = parent.getConnections.poll()\n+      val client = connection.client\n+      var batchReceived = false\n+      var seq: CharSequence = null\n+      try {\n+        getBatch(client) match {\n+          case Some(eventBatch) =>\n+            batchReceived = true\n+            seq = eventBatch.getSequenceNumber\n+            val events = toSparkFlumeEvents(eventBatch.getEvents)\n+            logDebug(\n+              \"Received batch of \" + events.size + \" events with sequence number: \" + seq)\n+            if (store(events)) {\n+              ack(client, seq)\n+            } else {\n+              sendNack(batchReceived, client, seq)\n+            }\n+          case None =>\n+        }\n+      } catch {\n+        case e: Exception =>\n+          Throwables.getRootCause(e) match {\n+            // If the cause was an InterruptedException, then check if the receiver is stopped -\n+            // if yes, just break out of the loop. Else send a Nack and log a warning.\n+            // In the unlikely case, the cause was not an Exception,\n+            // then just throw it out and exit.\n+            case interrupted: InterruptedException =>\n+              if (!parent.isStopped()) {\n+                logWarning(\"Interrupted while receiving data from Flume\", interrupted)\n+                sendNack(batchReceived, client, seq)\n+              }\n+            case exception: Exception =>\n+              logWarning(\"Error while receiving data from Flume\", exception)\n+              sendNack(batchReceived, client, seq)\n+          }\n+      } finally {\n+        parent.getConnections.add(connection)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Gets a batch of events from the specified client. This method does not handle any exceptions\n+   * which will be propogated to the caller.\n+   * @param client Client to get events from\n+   * @return [[Some]] which contains the event batch if Flume sent any events back, else [[None]]\n+   */\n+  private def getBatch(client: SparkFlumeProtocol.Callback): Option[EventBatch] = {\n+    val eventBatch = client.getEventBatch(parent.getMaxBatchSize)\n+    if (!SparkSinkUtils.isErrorBatch(eventBatch)) {\n+      // No error, proceed with processing data\n+      Some(eventBatch)\n+    } else {\n+      logWarning(\n+        \"Did not receive events from Flume agent due to error on the Flume \" +"
  }],
  "prId": 2065
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Name should be \"sendAck\" for consistency with sendNack.\n",
    "commit": "f93a07c9e6cd4b34a276e9c75aa601fdafa67ef7",
    "createdAt": "2014-08-27T02:10:51Z",
    "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.google.common.base.Throwables\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.streaming.flume.sink._\n+\n+private[flume] class FlumePollingRunnable(val parent: FlumePollingReceiver) extends Runnable with\n+Logging {\n+\n+  def run(): Unit = {\n+    while (!parent.isStopped()) {\n+      val connection = parent.getConnections.poll()\n+      val client = connection.client\n+      var batchReceived = false\n+      var seq: CharSequence = null\n+      try {\n+        getBatch(client) match {\n+          case Some(eventBatch) =>\n+            batchReceived = true\n+            seq = eventBatch.getSequenceNumber\n+            val events = toSparkFlumeEvents(eventBatch.getEvents)\n+            logDebug(\n+              \"Received batch of \" + events.size + \" events with sequence number: \" + seq)\n+            if (store(events)) {\n+              ack(client, seq)\n+            } else {\n+              sendNack(batchReceived, client, seq)\n+            }\n+          case None =>\n+        }\n+      } catch {\n+        case e: Exception =>\n+          Throwables.getRootCause(e) match {\n+            // If the cause was an InterruptedException, then check if the receiver is stopped -\n+            // if yes, just break out of the loop. Else send a Nack and log a warning.\n+            // In the unlikely case, the cause was not an Exception,\n+            // then just throw it out and exit.\n+            case interrupted: InterruptedException =>\n+              if (!parent.isStopped()) {\n+                logWarning(\"Interrupted while receiving data from Flume\", interrupted)\n+                sendNack(batchReceived, client, seq)\n+              }\n+            case exception: Exception =>\n+              logWarning(\"Error while receiving data from Flume\", exception)\n+              sendNack(batchReceived, client, seq)\n+          }\n+      } finally {\n+        parent.getConnections.add(connection)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Gets a batch of events from the specified client. This method does not handle any exceptions\n+   * which will be propogated to the caller.\n+   * @param client Client to get events from\n+   * @return [[Some]] which contains the event batch if Flume sent any events back, else [[None]]\n+   */\n+  private def getBatch(client: SparkFlumeProtocol.Callback): Option[EventBatch] = {\n+    val eventBatch = client.getEventBatch(parent.getMaxBatchSize)\n+    if (!SparkSinkUtils.isErrorBatch(eventBatch)) {\n+      // No error, proceed with processing data\n+      Some(eventBatch)\n+    } else {\n+      logWarning(\n+        \"Did not receive events from Flume agent due to error on the Flume \" +\n+          \"agent: \" + eventBatch.getErrorMsg)\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Store the events in the buffer to Spark. This method will not propogate any exceptions,\n+   * but will propogate any other errors.\n+   * @param buffer The buffer to store\n+   * @return true if the data was stored without any exception being thrown, else false\n+   */\n+  private def store(buffer: ArrayBuffer[SparkFlumeEvent]): Boolean = {\n+    try {\n+      parent.store(buffer)\n+      true\n+    } catch {\n+      case e: Exception =>\n+        logWarning(\"Error while attempting to store data received from Flume\", e)\n+        false\n+    }\n+  }\n+\n+  /**\n+   * Send an ack to the client for the sequence number. This method does not handle any exceptions\n+   * which will be propogated to the caller.\n+   * @param client client to send the ack to\n+   * @param seq sequence number of the batch to be ack-ed.\n+   * @return\n+   */\n+  private def ack(client: SparkFlumeProtocol.Callback, seq: CharSequence): Unit = {"
  }],
  "prId": 2065
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "The debug statement is wrong! Its not sending nack !!!!!\n",
    "commit": "f93a07c9e6cd4b34a276e9c75aa601fdafa67ef7",
    "createdAt": "2014-08-27T02:13:43Z",
    "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.google.common.base.Throwables\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.streaming.flume.sink._\n+\n+private[flume] class FlumePollingRunnable(val parent: FlumePollingReceiver) extends Runnable with\n+Logging {\n+\n+  def run(): Unit = {\n+    while (!parent.isStopped()) {\n+      val connection = parent.getConnections.poll()\n+      val client = connection.client\n+      var batchReceived = false\n+      var seq: CharSequence = null\n+      try {\n+        getBatch(client) match {\n+          case Some(eventBatch) =>\n+            batchReceived = true\n+            seq = eventBatch.getSequenceNumber\n+            val events = toSparkFlumeEvents(eventBatch.getEvents)\n+            logDebug(\n+              \"Received batch of \" + events.size + \" events with sequence number: \" + seq)\n+            if (store(events)) {\n+              ack(client, seq)\n+            } else {\n+              sendNack(batchReceived, client, seq)\n+            }\n+          case None =>\n+        }\n+      } catch {\n+        case e: Exception =>\n+          Throwables.getRootCause(e) match {\n+            // If the cause was an InterruptedException, then check if the receiver is stopped -\n+            // if yes, just break out of the loop. Else send a Nack and log a warning.\n+            // In the unlikely case, the cause was not an Exception,\n+            // then just throw it out and exit.\n+            case interrupted: InterruptedException =>\n+              if (!parent.isStopped()) {\n+                logWarning(\"Interrupted while receiving data from Flume\", interrupted)\n+                sendNack(batchReceived, client, seq)\n+              }\n+            case exception: Exception =>\n+              logWarning(\"Error while receiving data from Flume\", exception)\n+              sendNack(batchReceived, client, seq)\n+          }\n+      } finally {\n+        parent.getConnections.add(connection)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Gets a batch of events from the specified client. This method does not handle any exceptions\n+   * which will be propogated to the caller.\n+   * @param client Client to get events from\n+   * @return [[Some]] which contains the event batch if Flume sent any events back, else [[None]]\n+   */\n+  private def getBatch(client: SparkFlumeProtocol.Callback): Option[EventBatch] = {\n+    val eventBatch = client.getEventBatch(parent.getMaxBatchSize)\n+    if (!SparkSinkUtils.isErrorBatch(eventBatch)) {\n+      // No error, proceed with processing data\n+      Some(eventBatch)\n+    } else {\n+      logWarning(\n+        \"Did not receive events from Flume agent due to error on the Flume \" +\n+          \"agent: \" + eventBatch.getErrorMsg)\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Store the events in the buffer to Spark. This method will not propogate any exceptions,\n+   * but will propogate any other errors.\n+   * @param buffer The buffer to store\n+   * @return true if the data was stored without any exception being thrown, else false\n+   */\n+  private def store(buffer: ArrayBuffer[SparkFlumeEvent]): Boolean = {\n+    try {\n+      parent.store(buffer)\n+      true\n+    } catch {\n+      case e: Exception =>\n+        logWarning(\"Error while attempting to store data received from Flume\", e)\n+        false\n+    }\n+  }\n+\n+  /**\n+   * Send an ack to the client for the sequence number. This method does not handle any exceptions\n+   * which will be propogated to the caller.\n+   * @param client client to send the ack to\n+   * @param seq sequence number of the batch to be ack-ed.\n+   * @return\n+   */\n+  private def ack(client: SparkFlumeProtocol.Callback, seq: CharSequence): Unit = {\n+    logDebug(\"Sending nack for sequence number: \" + seq)"
  }],
  "prId": 2065
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Better to call the FlumePollingReceiver object as \"receiver\" . Parent is kind arbitrary (its not a tree or a DAG like data structure).\n",
    "commit": "f93a07c9e6cd4b34a276e9c75aa601fdafa67ef7",
    "createdAt": "2014-08-27T02:15:19Z",
    "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.google.common.base.Throwables\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.streaming.flume.sink._\n+\n+private[flume] class FlumePollingRunnable(val parent: FlumePollingReceiver) extends Runnable with"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "And this does not need to be `val` as it is not accessed outside this class.\n",
    "commit": "f93a07c9e6cd4b34a276e9c75aa601fdafa67ef7",
    "createdAt": "2014-08-27T02:16:10Z",
    "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.google.common.base.Throwables\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.streaming.flume.sink._\n+\n+private[flume] class FlumePollingRunnable(val parent: FlumePollingReceiver) extends Runnable with"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Does a name `FlumeBatchFetcher` sound more semantically meaningful to what this class does?\n",
    "commit": "f93a07c9e6cd4b34a276e9c75aa601fdafa67ef7",
    "createdAt": "2014-08-27T02:17:37Z",
    "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.google.common.base.Throwables\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.streaming.flume.sink._\n+\n+private[flume] class FlumePollingRunnable(val parent: FlumePollingReceiver) extends Runnable with"
  }],
  "prId": 2065
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Incorrect formatting.\n",
    "commit": "f93a07c9e6cd4b34a276e9c75aa601fdafa67ef7",
    "createdAt": "2014-08-27T02:15:30Z",
    "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.google.common.base.Throwables\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.streaming.flume.sink._\n+\n+private[flume] class FlumePollingRunnable(val parent: FlumePollingReceiver) extends Runnable with\n+Logging {"
  }],
  "prId": 2065
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "It maybe more consistent to move this logDebug into the getBatch function, just like the None case.\n",
    "commit": "f93a07c9e6cd4b34a276e9c75aa601fdafa67ef7",
    "createdAt": "2014-08-27T02:30:54Z",
    "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.google.common.base.Throwables\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.streaming.flume.sink._\n+\n+private[flume] class FlumePollingRunnable(val parent: FlumePollingReceiver) extends Runnable with\n+Logging {\n+\n+  def run(): Unit = {\n+    while (!parent.isStopped()) {\n+      val connection = parent.getConnections.poll()\n+      val client = connection.client\n+      var batchReceived = false\n+      var seq: CharSequence = null\n+      try {\n+        getBatch(client) match {\n+          case Some(eventBatch) =>\n+            batchReceived = true\n+            seq = eventBatch.getSequenceNumber\n+            val events = toSparkFlumeEvents(eventBatch.getEvents)\n+            logDebug("
  }],
  "prId": 2065
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "unnecessary empty lines.\n",
    "commit": "f93a07c9e6cd4b34a276e9c75aa601fdafa67ef7",
    "createdAt": "2014-08-27T03:02:01Z",
    "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.google.common.base.Throwables\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.streaming.flume.sink._\n+\n+private[flume] class FlumePollingRunnable(val parent: FlumePollingReceiver) extends Runnable with\n+Logging {\n+\n+  def run(): Unit = {\n+    while (!parent.isStopped()) {\n+      val connection = parent.getConnections.poll()\n+      val client = connection.client\n+      var batchReceived = false\n+      var seq: CharSequence = null\n+      try {\n+        getBatch(client) match {\n+          case Some(eventBatch) =>\n+            batchReceived = true\n+            seq = eventBatch.getSequenceNumber\n+            val events = toSparkFlumeEvents(eventBatch.getEvents)\n+            logDebug(\n+              \"Received batch of \" + events.size + \" events with sequence number: \" + seq)\n+            if (store(events)) {\n+              ack(client, seq)\n+            } else {\n+              sendNack(batchReceived, client, seq)\n+            }\n+          case None =>\n+        }\n+      } catch {\n+        case e: Exception =>\n+          Throwables.getRootCause(e) match {\n+            // If the cause was an InterruptedException, then check if the receiver is stopped -\n+            // if yes, just break out of the loop. Else send a Nack and log a warning.\n+            // In the unlikely case, the cause was not an Exception,\n+            // then just throw it out and exit.\n+            case interrupted: InterruptedException =>\n+              if (!parent.isStopped()) {\n+                logWarning(\"Interrupted while receiving data from Flume\", interrupted)\n+                sendNack(batchReceived, client, seq)\n+              }\n+            case exception: Exception =>\n+              logWarning(\"Error while receiving data from Flume\", exception)\n+              sendNack(batchReceived, client, seq)\n+          }\n+      } finally {\n+        parent.getConnections.add(connection)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Gets a batch of events from the specified client. This method does not handle any exceptions\n+   * which will be propogated to the caller.\n+   * @param client Client to get events from\n+   * @return [[Some]] which contains the event batch if Flume sent any events back, else [[None]]\n+   */\n+  private def getBatch(client: SparkFlumeProtocol.Callback): Option[EventBatch] = {\n+    val eventBatch = client.getEventBatch(parent.getMaxBatchSize)\n+    if (!SparkSinkUtils.isErrorBatch(eventBatch)) {\n+      // No error, proceed with processing data\n+      Some(eventBatch)\n+    } else {\n+      logWarning(\n+        \"Did not receive events from Flume agent due to error on the Flume \" +\n+          \"agent: \" + eventBatch.getErrorMsg)\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Store the events in the buffer to Spark. This method will not propogate any exceptions,\n+   * but will propogate any other errors.\n+   * @param buffer The buffer to store\n+   * @return true if the data was stored without any exception being thrown, else false\n+   */\n+  private def store(buffer: ArrayBuffer[SparkFlumeEvent]): Boolean = {\n+    try {\n+      parent.store(buffer)\n+      true\n+    } catch {\n+      case e: Exception =>\n+        logWarning(\"Error while attempting to store data received from Flume\", e)\n+        false\n+    }\n+  }\n+\n+  /**\n+   * Send an ack to the client for the sequence number. This method does not handle any exceptions\n+   * which will be propogated to the caller.\n+   * @param client client to send the ack to\n+   * @param seq sequence number of the batch to be ack-ed.\n+   * @return\n+   */\n+  private def ack(client: SparkFlumeProtocol.Callback, seq: CharSequence): Unit = {\n+    logDebug(\"Sending nack for sequence number: \" + seq)\n+    client.ack(seq)\n+    logDebug(\"Nack sent for sequence number: \" + seq)\n+  }\n+\n+  /**\n+   * This method sends a Nack if a batch was received to the client with with the given sequence\n+   * number. Any exceptions thrown by the RPC call is simply thrown out as is - no effort is made\n+   * to handle it.\n+   * @param batchReceived true if a batch was received. If this is false, no nack is sent\n+   * @param client The client to which the nack should be sent\n+   * @param seq The sequence number of the batch that is being nack-ed.\n+   */\n+  private def sendNack(batchReceived: Boolean, client: SparkFlumeProtocol.Callback,\n+    seq: CharSequence): Unit = {\n+    if (batchReceived) {\n+      // Let Flume know that the events need to be pushed back into the channel.\n+      logDebug(\"Sending nack for sequence number: \" + seq)\n+      client.nack(seq) // If the agent is down, even this could fail and throw\n+      logDebug(\"Nack sent for sequence number: \" + seq)\n+    }\n+  }\n+\n+\n+  /**\n+   * Utility method to convert [[SparkSinkEvent]]s to [[SparkFlumeEvent]]s\n+   * @param events - Events to convert to SparkFlumeEvents\n+   * @return - The SparkFlumeEvent generated from SparkSinkEvent\n+   */\n+  private def toSparkFlumeEvents(events: java.util.List[SparkSinkEvent]):\n+    ArrayBuffer[SparkFlumeEvent] = {\n+    // Convert each Flume event to a serializable SparkFlumeEvent\n+    val buffer = new ArrayBuffer[SparkFlumeEvent](events.size())\n+    var j = 0\n+    while (j < events.size()) {\n+      val event = events(j)\n+      val sparkFlumeEvent = new SparkFlumeEvent()\n+      sparkFlumeEvent.event.setBody(event.getBody)\n+      sparkFlumeEvent.event.setHeaders(event.getHeaders)\n+      buffer += sparkFlumeEvent\n+      j += 1\n+    }\n+    buffer\n+  }\n+"
  }],
  "prId": 2065
}]