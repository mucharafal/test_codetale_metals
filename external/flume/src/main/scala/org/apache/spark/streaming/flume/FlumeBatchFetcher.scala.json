[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Shouldnt this be inside the try as well?? Its a non-trivial code with potential chance for exception. The next line as well. What if `connection.client` is null?\n",
    "commit": "f93a07c9e6cd4b34a276e9c75aa601fdafa67ef7",
    "createdAt": "2014-08-27T03:55:56Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.google.common.base.Throwables\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.streaming.flume.sink._\n+\n+private[flume] class FlumeBatchFetcher(receiver: FlumePollingReceiver) extends Runnable with\n+  Logging {\n+\n+  def run(): Unit = {\n+    while (!receiver.isStopped()) {\n+      val connection = receiver.getConnections.poll()",
    "line": 40
  }, {
    "author": {
      "login": "harishreedharan"
    },
    "body": "If the client is null the receiver would never have started, since that is when the client is created. It would be null only if the `SpecificRequestor.getClient` threw, which means that the receiver would not start. poll is guaranteed to not throw a non-fatal exception: http://docs.oracle.com/javase/7/docs/api/java/util/concurrent/LinkedBlockingQueue.html#poll()\n",
    "commit": "f93a07c9e6cd4b34a276e9c75aa601fdafa67ef7",
    "createdAt": "2014-08-27T04:02:51Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.google.common.base.Throwables\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.streaming.flume.sink._\n+\n+private[flume] class FlumeBatchFetcher(receiver: FlumePollingReceiver) extends Runnable with\n+  Logging {\n+\n+  def run(): Unit = {\n+    while (!receiver.isStopped()) {\n+      val connection = receiver.getConnections.poll()",
    "line": 40
  }],
  "prId": 2065
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "propogated --> propagated\n",
    "commit": "f93a07c9e6cd4b34a276e9c75aa601fdafa67ef7",
    "createdAt": "2014-08-27T03:56:44Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.google.common.base.Throwables\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.streaming.flume.sink._\n+\n+private[flume] class FlumeBatchFetcher(receiver: FlumePollingReceiver) extends Runnable with\n+  Logging {\n+\n+  def run(): Unit = {\n+    while (!receiver.isStopped()) {\n+      val connection = receiver.getConnections.poll()\n+      val client = connection.client\n+      var batchReceived = false\n+      var seq: CharSequence = null\n+      try {\n+        getBatch(client) match {\n+          case Some(eventBatch) =>\n+            batchReceived = true\n+            seq = eventBatch.getSequenceNumber\n+            val events = toSparkFlumeEvents(eventBatch.getEvents)\n+            if (store(events)) {\n+              sendAck(client, seq)\n+            } else {\n+              sendNack(batchReceived, client, seq)\n+            }\n+          case None =>\n+        }\n+      } catch {\n+        case e: Exception =>\n+          Throwables.getRootCause(e) match {\n+            // If the cause was an InterruptedException, then check if the receiver is stopped -\n+            // if yes, just break out of the loop. Else send a Nack and log a warning.\n+            // In the unlikely case, the cause was not an Exception,\n+            // then just throw it out and exit.\n+            case interrupted: InterruptedException =>\n+              if (!receiver.isStopped()) {\n+                logWarning(\"Interrupted while receiving data from Flume\", interrupted)\n+                sendNack(batchReceived, client, seq)\n+              }\n+            case exception: Exception =>\n+              logWarning(\"Error while receiving data from Flume\", exception)\n+              sendNack(batchReceived, client, seq)\n+          }\n+      } finally {\n+        receiver.getConnections.add(connection)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Gets a batch of events from the specified client. This method does not handle any exceptions\n+   * which will be propogated to the caller.\n+   * @param client Client to get events from\n+   * @return [[Some]] which contains the event batch if Flume sent any events back, else [[None]]\n+   */\n+  private def getBatch(client: SparkFlumeProtocol.Callback): Option[EventBatch] = {\n+    val eventBatch = client.getEventBatch(receiver.getMaxBatchSize)\n+    if (!SparkSinkUtils.isErrorBatch(eventBatch)) {\n+      // No error, proceed with processing data\n+      logDebug(\"Received batch of \" + eventBatch.getEvents.size + \" events with sequence number: \"\n+        + eventBatch.getSequenceNumber)\n+      Some(eventBatch)\n+    } else {\n+      logWarning(\n+        \"Did not receive events from Flume agent due to error on the Flume \" +\n+          \"agent: \" + eventBatch.getErrorMsg)\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Store the events in the buffer to Spark. This method will not propogate any exceptions,\n+   * but will propogate any other errors.\n+   * @param buffer The buffer to store\n+   * @return true if the data was stored without any exception being thrown, else false\n+   */\n+  private def store(buffer: ArrayBuffer[SparkFlumeEvent]): Boolean = {\n+    try {\n+      receiver.store(buffer)\n+      true\n+    } catch {\n+      case e: Exception =>\n+        logWarning(\"Error while attempting to store data received from Flume\", e)\n+        false\n+    }\n+  }\n+\n+  /**\n+   * Send an ack to the client for the sequence number. This method does not handle any exceptions\n+   * which will be propogated to the caller."
  }],
  "prId": 2065
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Dude, this line still says \"nack\" :)\n",
    "commit": "f93a07c9e6cd4b34a276e9c75aa601fdafa67ef7",
    "createdAt": "2014-08-27T03:57:11Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.google.common.base.Throwables\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.streaming.flume.sink._\n+\n+private[flume] class FlumeBatchFetcher(receiver: FlumePollingReceiver) extends Runnable with\n+  Logging {\n+\n+  def run(): Unit = {\n+    while (!receiver.isStopped()) {\n+      val connection = receiver.getConnections.poll()\n+      val client = connection.client\n+      var batchReceived = false\n+      var seq: CharSequence = null\n+      try {\n+        getBatch(client) match {\n+          case Some(eventBatch) =>\n+            batchReceived = true\n+            seq = eventBatch.getSequenceNumber\n+            val events = toSparkFlumeEvents(eventBatch.getEvents)\n+            if (store(events)) {\n+              sendAck(client, seq)\n+            } else {\n+              sendNack(batchReceived, client, seq)\n+            }\n+          case None =>\n+        }\n+      } catch {\n+        case e: Exception =>\n+          Throwables.getRootCause(e) match {\n+            // If the cause was an InterruptedException, then check if the receiver is stopped -\n+            // if yes, just break out of the loop. Else send a Nack and log a warning.\n+            // In the unlikely case, the cause was not an Exception,\n+            // then just throw it out and exit.\n+            case interrupted: InterruptedException =>\n+              if (!receiver.isStopped()) {\n+                logWarning(\"Interrupted while receiving data from Flume\", interrupted)\n+                sendNack(batchReceived, client, seq)\n+              }\n+            case exception: Exception =>\n+              logWarning(\"Error while receiving data from Flume\", exception)\n+              sendNack(batchReceived, client, seq)\n+          }\n+      } finally {\n+        receiver.getConnections.add(connection)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Gets a batch of events from the specified client. This method does not handle any exceptions\n+   * which will be propogated to the caller.\n+   * @param client Client to get events from\n+   * @return [[Some]] which contains the event batch if Flume sent any events back, else [[None]]\n+   */\n+  private def getBatch(client: SparkFlumeProtocol.Callback): Option[EventBatch] = {\n+    val eventBatch = client.getEventBatch(receiver.getMaxBatchSize)\n+    if (!SparkSinkUtils.isErrorBatch(eventBatch)) {\n+      // No error, proceed with processing data\n+      logDebug(\"Received batch of \" + eventBatch.getEvents.size + \" events with sequence number: \"\n+        + eventBatch.getSequenceNumber)\n+      Some(eventBatch)\n+    } else {\n+      logWarning(\n+        \"Did not receive events from Flume agent due to error on the Flume \" +\n+          \"agent: \" + eventBatch.getErrorMsg)\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Store the events in the buffer to Spark. This method will not propogate any exceptions,\n+   * but will propogate any other errors.\n+   * @param buffer The buffer to store\n+   * @return true if the data was stored without any exception being thrown, else false\n+   */\n+  private def store(buffer: ArrayBuffer[SparkFlumeEvent]): Boolean = {\n+    try {\n+      receiver.store(buffer)\n+      true\n+    } catch {\n+      case e: Exception =>\n+        logWarning(\"Error while attempting to store data received from Flume\", e)\n+        false\n+    }\n+  }\n+\n+  /**\n+   * Send an ack to the client for the sequence number. This method does not handle any exceptions\n+   * which will be propogated to the caller.\n+   * @param client client to send the ack to\n+   * @param seq sequence number of the batch to be ack-ed.\n+   * @return\n+   */\n+  private def sendAck(client: SparkFlumeProtocol.Callback, seq: CharSequence): Unit = {\n+    logDebug(\"Sending Nack for sequence number: \" + seq)"
  }],
  "prId": 2065
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Double with in the docs\n",
    "commit": "f93a07c9e6cd4b34a276e9c75aa601fdafa67ef7",
    "createdAt": "2014-08-27T03:57:29Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.google.common.base.Throwables\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.streaming.flume.sink._\n+\n+private[flume] class FlumeBatchFetcher(receiver: FlumePollingReceiver) extends Runnable with\n+  Logging {\n+\n+  def run(): Unit = {\n+    while (!receiver.isStopped()) {\n+      val connection = receiver.getConnections.poll()\n+      val client = connection.client\n+      var batchReceived = false\n+      var seq: CharSequence = null\n+      try {\n+        getBatch(client) match {\n+          case Some(eventBatch) =>\n+            batchReceived = true\n+            seq = eventBatch.getSequenceNumber\n+            val events = toSparkFlumeEvents(eventBatch.getEvents)\n+            if (store(events)) {\n+              sendAck(client, seq)\n+            } else {\n+              sendNack(batchReceived, client, seq)\n+            }\n+          case None =>\n+        }\n+      } catch {\n+        case e: Exception =>\n+          Throwables.getRootCause(e) match {\n+            // If the cause was an InterruptedException, then check if the receiver is stopped -\n+            // if yes, just break out of the loop. Else send a Nack and log a warning.\n+            // In the unlikely case, the cause was not an Exception,\n+            // then just throw it out and exit.\n+            case interrupted: InterruptedException =>\n+              if (!receiver.isStopped()) {\n+                logWarning(\"Interrupted while receiving data from Flume\", interrupted)\n+                sendNack(batchReceived, client, seq)\n+              }\n+            case exception: Exception =>\n+              logWarning(\"Error while receiving data from Flume\", exception)\n+              sendNack(batchReceived, client, seq)\n+          }\n+      } finally {\n+        receiver.getConnections.add(connection)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Gets a batch of events from the specified client. This method does not handle any exceptions\n+   * which will be propogated to the caller.\n+   * @param client Client to get events from\n+   * @return [[Some]] which contains the event batch if Flume sent any events back, else [[None]]\n+   */\n+  private def getBatch(client: SparkFlumeProtocol.Callback): Option[EventBatch] = {\n+    val eventBatch = client.getEventBatch(receiver.getMaxBatchSize)\n+    if (!SparkSinkUtils.isErrorBatch(eventBatch)) {\n+      // No error, proceed with processing data\n+      logDebug(\"Received batch of \" + eventBatch.getEvents.size + \" events with sequence number: \"\n+        + eventBatch.getSequenceNumber)\n+      Some(eventBatch)\n+    } else {\n+      logWarning(\n+        \"Did not receive events from Flume agent due to error on the Flume \" +\n+          \"agent: \" + eventBatch.getErrorMsg)\n+      None\n+    }\n+  }\n+\n+  /**\n+   * Store the events in the buffer to Spark. This method will not propogate any exceptions,\n+   * but will propogate any other errors.\n+   * @param buffer The buffer to store\n+   * @return true if the data was stored without any exception being thrown, else false\n+   */\n+  private def store(buffer: ArrayBuffer[SparkFlumeEvent]): Boolean = {\n+    try {\n+      receiver.store(buffer)\n+      true\n+    } catch {\n+      case e: Exception =>\n+        logWarning(\"Error while attempting to store data received from Flume\", e)\n+        false\n+    }\n+  }\n+\n+  /**\n+   * Send an ack to the client for the sequence number. This method does not handle any exceptions\n+   * which will be propogated to the caller.\n+   * @param client client to send the ack to\n+   * @param seq sequence number of the batch to be ack-ed.\n+   * @return\n+   */\n+  private def sendAck(client: SparkFlumeProtocol.Callback, seq: CharSequence): Unit = {\n+    logDebug(\"Sending Nack for sequence number: \" + seq)\n+    client.ack(seq)\n+    logDebug(\"Ack sent for sequence number: \" + seq)\n+  }\n+\n+  /**\n+   * This method sends a Nack if a batch was received to the client with with the given sequence"
  }],
  "prId": 2065
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "I missed this earlier. Can you add docs on what this class does??\n",
    "commit": "f93a07c9e6cd4b34a276e9c75aa601fdafa67ef7",
    "createdAt": "2014-08-27T05:59:40Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.google.common.base.Throwables\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.streaming.flume.sink._\n+\n+private[flume] class FlumeBatchFetcher(receiver: FlumePollingReceiver) extends Runnable with"
  }],
  "prId": 2065
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: With so many clauses in the string, its better to use string interpolation.\n`logDebug(s\"Received batch of ${eventBatch.getEvents.size} events with sequence number: ${eventBatch.getSequenceNumber}\")`\n",
    "commit": "f93a07c9e6cd4b34a276e9c75aa601fdafa67ef7",
    "createdAt": "2014-08-27T06:01:15Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.google.common.base.Throwables\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.streaming.flume.sink._\n+\n+private[flume] class FlumeBatchFetcher(receiver: FlumePollingReceiver) extends Runnable with\n+  Logging {\n+\n+  def run(): Unit = {\n+    while (!receiver.isStopped()) {\n+      val connection = receiver.getConnections.poll()\n+      val client = connection.client\n+      var batchReceived = false\n+      var seq: CharSequence = null\n+      try {\n+        getBatch(client) match {\n+          case Some(eventBatch) =>\n+            batchReceived = true\n+            seq = eventBatch.getSequenceNumber\n+            val events = toSparkFlumeEvents(eventBatch.getEvents)\n+            if (store(events)) {\n+              sendAck(client, seq)\n+            } else {\n+              sendNack(batchReceived, client, seq)\n+            }\n+          case None =>\n+        }\n+      } catch {\n+        case e: Exception =>\n+          Throwables.getRootCause(e) match {\n+            // If the cause was an InterruptedException, then check if the receiver is stopped -\n+            // if yes, just break out of the loop. Else send a Nack and log a warning.\n+            // In the unlikely case, the cause was not an Exception,\n+            // then just throw it out and exit.\n+            case interrupted: InterruptedException =>\n+              if (!receiver.isStopped()) {\n+                logWarning(\"Interrupted while receiving data from Flume\", interrupted)\n+                sendNack(batchReceived, client, seq)\n+              }\n+            case exception: Exception =>\n+              logWarning(\"Error while receiving data from Flume\", exception)\n+              sendNack(batchReceived, client, seq)\n+          }\n+      } finally {\n+        receiver.getConnections.add(connection)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Gets a batch of events from the specified client. This method does not handle any exceptions\n+   * which will be propogated to the caller.\n+   * @param client Client to get events from\n+   * @return [[Some]] which contains the event batch if Flume sent any events back, else [[None]]\n+   */\n+  private def getBatch(client: SparkFlumeProtocol.Callback): Option[EventBatch] = {\n+    val eventBatch = client.getEventBatch(receiver.getMaxBatchSize)\n+    if (!SparkSinkUtils.isErrorBatch(eventBatch)) {\n+      // No error, proceed with processing data\n+      logDebug(\"Received batch of \" + eventBatch.getEvents.size + \" events with sequence number: \""
  }],
  "prId": 2065
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: Again, spilling into three lines is weird.\n\n```\nlogWarning(\"Did not receive events from Flume agent due to error on the Flume agent: \" + \n   eventBatch.getErrorMsg)\n```\n",
    "commit": "f93a07c9e6cd4b34a276e9c75aa601fdafa67ef7",
    "createdAt": "2014-08-27T06:02:26Z",
    "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.streaming.flume\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import com.google.common.base.Throwables\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.streaming.flume.sink._\n+\n+private[flume] class FlumeBatchFetcher(receiver: FlumePollingReceiver) extends Runnable with\n+  Logging {\n+\n+  def run(): Unit = {\n+    while (!receiver.isStopped()) {\n+      val connection = receiver.getConnections.poll()\n+      val client = connection.client\n+      var batchReceived = false\n+      var seq: CharSequence = null\n+      try {\n+        getBatch(client) match {\n+          case Some(eventBatch) =>\n+            batchReceived = true\n+            seq = eventBatch.getSequenceNumber\n+            val events = toSparkFlumeEvents(eventBatch.getEvents)\n+            if (store(events)) {\n+              sendAck(client, seq)\n+            } else {\n+              sendNack(batchReceived, client, seq)\n+            }\n+          case None =>\n+        }\n+      } catch {\n+        case e: Exception =>\n+          Throwables.getRootCause(e) match {\n+            // If the cause was an InterruptedException, then check if the receiver is stopped -\n+            // if yes, just break out of the loop. Else send a Nack and log a warning.\n+            // In the unlikely case, the cause was not an Exception,\n+            // then just throw it out and exit.\n+            case interrupted: InterruptedException =>\n+              if (!receiver.isStopped()) {\n+                logWarning(\"Interrupted while receiving data from Flume\", interrupted)\n+                sendNack(batchReceived, client, seq)\n+              }\n+            case exception: Exception =>\n+              logWarning(\"Error while receiving data from Flume\", exception)\n+              sendNack(batchReceived, client, seq)\n+          }\n+      } finally {\n+        receiver.getConnections.add(connection)\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Gets a batch of events from the specified client. This method does not handle any exceptions\n+   * which will be propogated to the caller.\n+   * @param client Client to get events from\n+   * @return [[Some]] which contains the event batch if Flume sent any events back, else [[None]]\n+   */\n+  private def getBatch(client: SparkFlumeProtocol.Callback): Option[EventBatch] = {\n+    val eventBatch = client.getEventBatch(receiver.getMaxBatchSize)\n+    if (!SparkSinkUtils.isErrorBatch(eventBatch)) {\n+      // No error, proceed with processing data\n+      logDebug(\"Received batch of \" + eventBatch.getEvents.size + \" events with sequence number: \"\n+        + eventBatch.getSequenceNumber)\n+      Some(eventBatch)\n+    } else {\n+      logWarning("
  }],
  "prId": 2065
}]