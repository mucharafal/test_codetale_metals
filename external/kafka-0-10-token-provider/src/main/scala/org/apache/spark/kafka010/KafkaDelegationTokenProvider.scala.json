[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "does not require",
    "commit": "e5bcc181d5b748490efdbf83625a47fb36892b82",
    "createdAt": "2019-04-26T20:12:19Z",
    "diffHunk": "@@ -39,24 +38,49 @@ private[spark] class KafkaDelegationTokenProvider\n       sparkConf: SparkConf,\n       creds: Credentials): Option[Long] = {\n     try {\n-      logDebug(\"Attempting to fetch Kafka security token.\")\n-      val (token, nextRenewalDate) = KafkaTokenUtil.obtainToken(sparkConf)\n-      creds.addToken(token.getService, token)\n-      return Some(nextRenewalDate)\n+      var lowestNextRenewalDate: Option[Long] = None\n+      KafkaTokenSparkConf.getAllClusterConfigs(sparkConf).foreach { clusterConf =>\n+        if (delegationTokensRequired(clusterConf)) {\n+          try {\n+            logDebug(\n+              s\"Attempting to fetch Kafka security token for cluster ${clusterConf.identifier}.\")\n+            val (token, nextRenewalDate) = KafkaTokenUtil.obtainToken(sparkConf, clusterConf)\n+            creds.addToken(token.getService, token)\n+            if (lowestNextRenewalDate.isEmpty || nextRenewalDate < lowestNextRenewalDate.get) {\n+              lowestNextRenewalDate = Some(nextRenewalDate)\n+            }\n+          } catch {\n+            case NonFatal(e) =>\n+              logWarning(s\"Failed to get token from service: $serviceName \" +\n+                s\"cluster: ${clusterConf.identifier}\", e)\n+          }\n+        } else {\n+          logDebug(\n+            s\"Cluster ${clusterConf.identifier} not requires delegation token, skipping.\")"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Fixed.",
    "commit": "e5bcc181d5b748490efdbf83625a47fb36892b82",
    "createdAt": "2019-04-29T10:41:07Z",
    "diffHunk": "@@ -39,24 +38,49 @@ private[spark] class KafkaDelegationTokenProvider\n       sparkConf: SparkConf,\n       creds: Credentials): Option[Long] = {\n     try {\n-      logDebug(\"Attempting to fetch Kafka security token.\")\n-      val (token, nextRenewalDate) = KafkaTokenUtil.obtainToken(sparkConf)\n-      creds.addToken(token.getService, token)\n-      return Some(nextRenewalDate)\n+      var lowestNextRenewalDate: Option[Long] = None\n+      KafkaTokenSparkConf.getAllClusterConfigs(sparkConf).foreach { clusterConf =>\n+        if (delegationTokensRequired(clusterConf)) {\n+          try {\n+            logDebug(\n+              s\"Attempting to fetch Kafka security token for cluster ${clusterConf.identifier}.\")\n+            val (token, nextRenewalDate) = KafkaTokenUtil.obtainToken(sparkConf, clusterConf)\n+            creds.addToken(token.getService, token)\n+            if (lowestNextRenewalDate.isEmpty || nextRenewalDate < lowestNextRenewalDate.get) {\n+              lowestNextRenewalDate = Some(nextRenewalDate)\n+            }\n+          } catch {\n+            case NonFatal(e) =>\n+              logWarning(s\"Failed to get token from service: $serviceName \" +\n+                s\"cluster: ${clusterConf.identifier}\", e)\n+          }\n+        } else {\n+          logDebug(\n+            s\"Cluster ${clusterConf.identifier} not requires delegation token, skipping.\")"
  }],
  "prId": 24305
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This isn't exactly correct, since you may get a few delegation tokens. I guess the question here is what should be the error behavior? Seems like the previous behavior is not just log the error and let the application fail later because of auth errors.\r\n\r\nSo the correct thing here seems to be to move the error handling above, where the code tries to get a token for a specific cluster, and keep going if that particular one fails.\r\n\r\nThat would also avoid the two levels of error catching you currently have.",
    "commit": "e5bcc181d5b748490efdbf83625a47fb36892b82",
    "createdAt": "2019-04-30T17:23:05Z",
    "diffHunk": "@@ -38,24 +37,49 @@ private[spark] class KafkaDelegationTokenProvider\n       sparkConf: SparkConf,\n       creds: Credentials): Option[Long] = {\n     try {\n-      logDebug(\"Attempting to fetch Kafka security token.\")\n-      val (token, nextRenewalDate) = KafkaTokenUtil.obtainToken(sparkConf)\n-      creds.addToken(token.getService, token)\n-      return Some(nextRenewalDate)\n+      var lowestNextRenewalDate: Option[Long] = None\n+      KafkaTokenSparkConf.getAllClusterConfigs(sparkConf).foreach { clusterConf =>\n+        if (delegationTokensRequired(clusterConf)) {\n+          try {\n+            logDebug(\n+              s\"Attempting to fetch Kafka security token for cluster ${clusterConf.identifier}.\")\n+            val (token, nextRenewalDate) = KafkaTokenUtil.obtainToken(sparkConf, clusterConf)\n+            creds.addToken(token.getService, token)\n+            if (lowestNextRenewalDate.isEmpty || nextRenewalDate < lowestNextRenewalDate.get) {\n+              lowestNextRenewalDate = Some(nextRenewalDate)\n+            }\n+          } catch {\n+            case NonFatal(e) =>\n+              logWarning(s\"Failed to get token from service: $serviceName \" +\n+                s\"cluster: ${clusterConf.identifier}\", e)\n+          }\n+        } else {\n+          logDebug(\n+            s\"Cluster ${clusterConf.identifier} does not require delegation token, skipping.\")\n+        }\n+      }\n+      lowestNextRenewalDate\n     } catch {\n       case NonFatal(e) =>\n-        logWarning(s\"Failed to get token from service $serviceName\", e)\n+        logWarning(s\"Failed to get token cluster configuration\", e)\n+        None"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Having 2 level of error catching is added for the following reasons:\r\n* Outer catch protects delegation token manager from exception coming from for example `getAllClusterConfigs`\r\n* Inner catch protects the inner loop and just skips a particular cluster\r\n\r\nWhat I think is nice catch that `delegationTokensRequired` is not covered by the inner block. As a result if the first token obtained properly but the second throws exception in `delegationTokensRequired` then the whole function will return `None`.\r\n\r\n> That would also avoid the two levels of error catching you currently have.\r\n\r\n Don't know how this could work with one level of catch block. Such case if first fails the rest won't be processed. I've modified the code and share your thoughts.\r\n",
    "commit": "e5bcc181d5b748490efdbf83625a47fb36892b82",
    "createdAt": "2019-05-02T17:49:26Z",
    "diffHunk": "@@ -38,24 +37,49 @@ private[spark] class KafkaDelegationTokenProvider\n       sparkConf: SparkConf,\n       creds: Credentials): Option[Long] = {\n     try {\n-      logDebug(\"Attempting to fetch Kafka security token.\")\n-      val (token, nextRenewalDate) = KafkaTokenUtil.obtainToken(sparkConf)\n-      creds.addToken(token.getService, token)\n-      return Some(nextRenewalDate)\n+      var lowestNextRenewalDate: Option[Long] = None\n+      KafkaTokenSparkConf.getAllClusterConfigs(sparkConf).foreach { clusterConf =>\n+        if (delegationTokensRequired(clusterConf)) {\n+          try {\n+            logDebug(\n+              s\"Attempting to fetch Kafka security token for cluster ${clusterConf.identifier}.\")\n+            val (token, nextRenewalDate) = KafkaTokenUtil.obtainToken(sparkConf, clusterConf)\n+            creds.addToken(token.getService, token)\n+            if (lowestNextRenewalDate.isEmpty || nextRenewalDate < lowestNextRenewalDate.get) {\n+              lowestNextRenewalDate = Some(nextRenewalDate)\n+            }\n+          } catch {\n+            case NonFatal(e) =>\n+              logWarning(s\"Failed to get token from service: $serviceName \" +\n+                s\"cluster: ${clusterConf.identifier}\", e)\n+          }\n+        } else {\n+          logDebug(\n+            s\"Cluster ${clusterConf.identifier} does not require delegation token, skipping.\")\n+        }\n+      }\n+      lowestNextRenewalDate\n     } catch {\n       case NonFatal(e) =>\n-        logWarning(s\"Failed to get token from service $serviceName\", e)\n+        logWarning(s\"Failed to get token cluster configuration\", e)\n+        None"
  }],
  "prId": 24305
}]