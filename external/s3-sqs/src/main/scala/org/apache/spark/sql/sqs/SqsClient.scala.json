[{
  "comments": [{
    "author": {
      "login": "steveloughran"
    },
    "body": "I'm going to point you at the S3A logic to interpret AWS exceptions and remap\r\nhttps://github.com/steveloughran/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AUtils.java#L179\r\n\r\nAnd then retry policy we've evolved over time\r\nhttps://github.com/steveloughran/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ARetryPolicy.java#L158\r\n\r\nEach AWS service has its own failure modes which you get to learn over time.",
    "commit": "1f4628fb6ee306450652171e9789f94c648f89d6",
    "createdAt": "2019-07-26T10:46:26Z",
    "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sqs\n+\n+import java.text.SimpleDateFormat\n+import java.util.TimeZone\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.JavaConverters._\n+\n+import com.amazonaws.{AmazonClientException, AmazonServiceException, ClientConfiguration}\n+import com.amazonaws.services.sqs.{AmazonSQS, AmazonSQSClientBuilder}\n+import com.amazonaws.services.sqs.model.{DeleteMessageBatchRequestEntry, Message, ReceiveMessageRequest}\n+import org.apache.hadoop.conf.Configuration\n+import org.json4s.{DefaultFormats, MappingException}\n+import org.json4s.JsonAST.JValue\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.ThreadUtils\n+\n+class SqsClient(sourceOptions: SqsSourceOptions,\n+                hadoopConf: Configuration) extends Logging {\n+\n+  private val sqsFetchIntervalSeconds = sourceOptions.fetchIntervalSeconds\n+  private val sqsLongPollWaitTimeSeconds = sourceOptions.longPollWaitTimeSeconds\n+  private val sqsMaxRetries = sourceOptions.maxRetries\n+  private val maxConnections = sourceOptions.maxConnections\n+  private val ignoreFileDeletion = sourceOptions.ignoreFileDeletion\n+  private val region = sourceOptions.region\n+  val sqsUrl = sourceOptions.sqsUrl\n+\n+  @volatile var exception: Option[Exception] = None\n+\n+  private val timestampFormat = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\") // ISO8601\n+  timestampFormat.setTimeZone(TimeZone.getTimeZone(\"UTC\"))\n+  private var retriesOnFailure = 0\n+  private val sqsClient = createSqsClient()\n+\n+  val sqsScheduler = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"sqs-scheduler\")\n+\n+  val sqsFileCache = new SqsFileCache(sourceOptions.maxFileAgeMs, sourceOptions.fileNameOnly)\n+\n+  val deleteMessageQueue = new java.util.concurrent.ConcurrentLinkedQueue[String]()\n+\n+  private val sqsFetchMessagesThread = new Runnable {\n+    override def run(): Unit = {\n+      try {\n+        // Fetching messages from Amazon SQS\n+        val newMessages = sqsFetchMessages()\n+\n+        // Filtering the new messages which are already not seen\n+        if (newMessages.nonEmpty) {\n+          newMessages.filter(message => sqsFileCache.isNewFile(message._1, message._2))\n+            .foreach(message =>\n+              sqsFileCache.add(message._1, MessageDescription(message._2, false, message._3)))\n+        }\n+      } catch {\n+        case e: Exception =>\n+          exception = Some(e)\n+      }\n+    }\n+  }\n+\n+  sqsScheduler.scheduleWithFixedDelay(\n+    sqsFetchMessagesThread,\n+    0,\n+    sqsFetchIntervalSeconds,\n+    TimeUnit.SECONDS)\n+\n+  private def sqsFetchMessages(): Seq[(String, Long, String)] = {\n+    val messageList = try {\n+      val receiveMessageRequest = new ReceiveMessageRequest()\n+        .withQueueUrl(sqsUrl)\n+        .withWaitTimeSeconds(sqsLongPollWaitTimeSeconds)\n+      val messages = sqsClient.receiveMessage(receiveMessageRequest).getMessages.asScala\n+      retriesOnFailure = 0\n+      logDebug(s\"successfully received ${messages.size} messages\")\n+      messages\n+    } catch {\n+      case ase: AmazonServiceException =>",
    "line": 97
  }, {
    "author": {
      "login": "abhishekd0907"
    },
    "body": "I was following aws docs for covering various exception handling cases, they may not be very exhaustive though. Thanks for pointing to this code, it looks like a more sophisticated way to write the exception handling code.",
    "commit": "1f4628fb6ee306450652171e9789f94c648f89d6",
    "createdAt": "2019-07-31T07:16:13Z",
    "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sqs\n+\n+import java.text.SimpleDateFormat\n+import java.util.TimeZone\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.JavaConverters._\n+\n+import com.amazonaws.{AmazonClientException, AmazonServiceException, ClientConfiguration}\n+import com.amazonaws.services.sqs.{AmazonSQS, AmazonSQSClientBuilder}\n+import com.amazonaws.services.sqs.model.{DeleteMessageBatchRequestEntry, Message, ReceiveMessageRequest}\n+import org.apache.hadoop.conf.Configuration\n+import org.json4s.{DefaultFormats, MappingException}\n+import org.json4s.JsonAST.JValue\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.ThreadUtils\n+\n+class SqsClient(sourceOptions: SqsSourceOptions,\n+                hadoopConf: Configuration) extends Logging {\n+\n+  private val sqsFetchIntervalSeconds = sourceOptions.fetchIntervalSeconds\n+  private val sqsLongPollWaitTimeSeconds = sourceOptions.longPollWaitTimeSeconds\n+  private val sqsMaxRetries = sourceOptions.maxRetries\n+  private val maxConnections = sourceOptions.maxConnections\n+  private val ignoreFileDeletion = sourceOptions.ignoreFileDeletion\n+  private val region = sourceOptions.region\n+  val sqsUrl = sourceOptions.sqsUrl\n+\n+  @volatile var exception: Option[Exception] = None\n+\n+  private val timestampFormat = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\") // ISO8601\n+  timestampFormat.setTimeZone(TimeZone.getTimeZone(\"UTC\"))\n+  private var retriesOnFailure = 0\n+  private val sqsClient = createSqsClient()\n+\n+  val sqsScheduler = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"sqs-scheduler\")\n+\n+  val sqsFileCache = new SqsFileCache(sourceOptions.maxFileAgeMs, sourceOptions.fileNameOnly)\n+\n+  val deleteMessageQueue = new java.util.concurrent.ConcurrentLinkedQueue[String]()\n+\n+  private val sqsFetchMessagesThread = new Runnable {\n+    override def run(): Unit = {\n+      try {\n+        // Fetching messages from Amazon SQS\n+        val newMessages = sqsFetchMessages()\n+\n+        // Filtering the new messages which are already not seen\n+        if (newMessages.nonEmpty) {\n+          newMessages.filter(message => sqsFileCache.isNewFile(message._1, message._2))\n+            .foreach(message =>\n+              sqsFileCache.add(message._1, MessageDescription(message._2, false, message._3)))\n+        }\n+      } catch {\n+        case e: Exception =>\n+          exception = Some(e)\n+      }\n+    }\n+  }\n+\n+  sqsScheduler.scheduleWithFixedDelay(\n+    sqsFetchMessagesThread,\n+    0,\n+    sqsFetchIntervalSeconds,\n+    TimeUnit.SECONDS)\n+\n+  private def sqsFetchMessages(): Seq[(String, Long, String)] = {\n+    val messageList = try {\n+      val receiveMessageRequest = new ReceiveMessageRequest()\n+        .withQueueUrl(sqsUrl)\n+        .withWaitTimeSeconds(sqsLongPollWaitTimeSeconds)\n+      val messages = sqsClient.receiveMessage(receiveMessageRequest).getMessages.asScala\n+      retriesOnFailure = 0\n+      logDebug(s\"successfully received ${messages.size} messages\")\n+      messages\n+    } catch {\n+      case ase: AmazonServiceException =>",
    "line": 97
  }],
  "prId": 24934
}, {
  "comments": [{
    "author": {
      "login": "steveloughran"
    },
    "body": "you should be using the fs.s3a values, given that that the S3n connector has been deleted from hadoop, though I'll add some more comments on that topic later",
    "commit": "1f4628fb6ee306450652171e9789f94c648f89d6",
    "createdAt": "2019-07-26T10:47:46Z",
    "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sqs\n+\n+import java.text.SimpleDateFormat\n+import java.util.TimeZone\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.JavaConverters._\n+\n+import com.amazonaws.{AmazonClientException, AmazonServiceException, ClientConfiguration}\n+import com.amazonaws.services.sqs.{AmazonSQS, AmazonSQSClientBuilder}\n+import com.amazonaws.services.sqs.model.{DeleteMessageBatchRequestEntry, Message, ReceiveMessageRequest}\n+import org.apache.hadoop.conf.Configuration\n+import org.json4s.{DefaultFormats, MappingException}\n+import org.json4s.JsonAST.JValue\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.ThreadUtils\n+\n+class SqsClient(sourceOptions: SqsSourceOptions,\n+                hadoopConf: Configuration) extends Logging {\n+\n+  private val sqsFetchIntervalSeconds = sourceOptions.fetchIntervalSeconds\n+  private val sqsLongPollWaitTimeSeconds = sourceOptions.longPollWaitTimeSeconds\n+  private val sqsMaxRetries = sourceOptions.maxRetries\n+  private val maxConnections = sourceOptions.maxConnections\n+  private val ignoreFileDeletion = sourceOptions.ignoreFileDeletion\n+  private val region = sourceOptions.region\n+  val sqsUrl = sourceOptions.sqsUrl\n+\n+  @volatile var exception: Option[Exception] = None\n+\n+  private val timestampFormat = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\") // ISO8601\n+  timestampFormat.setTimeZone(TimeZone.getTimeZone(\"UTC\"))\n+  private var retriesOnFailure = 0\n+  private val sqsClient = createSqsClient()\n+\n+  val sqsScheduler = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"sqs-scheduler\")\n+\n+  val sqsFileCache = new SqsFileCache(sourceOptions.maxFileAgeMs, sourceOptions.fileNameOnly)\n+\n+  val deleteMessageQueue = new java.util.concurrent.ConcurrentLinkedQueue[String]()\n+\n+  private val sqsFetchMessagesThread = new Runnable {\n+    override def run(): Unit = {\n+      try {\n+        // Fetching messages from Amazon SQS\n+        val newMessages = sqsFetchMessages()\n+\n+        // Filtering the new messages which are already not seen\n+        if (newMessages.nonEmpty) {\n+          newMessages.filter(message => sqsFileCache.isNewFile(message._1, message._2))\n+            .foreach(message =>\n+              sqsFileCache.add(message._1, MessageDescription(message._2, false, message._3)))\n+        }\n+      } catch {\n+        case e: Exception =>\n+          exception = Some(e)\n+      }\n+    }\n+  }\n+\n+  sqsScheduler.scheduleWithFixedDelay(\n+    sqsFetchMessagesThread,\n+    0,\n+    sqsFetchIntervalSeconds,\n+    TimeUnit.SECONDS)\n+\n+  private def sqsFetchMessages(): Seq[(String, Long, String)] = {\n+    val messageList = try {\n+      val receiveMessageRequest = new ReceiveMessageRequest()\n+        .withQueueUrl(sqsUrl)\n+        .withWaitTimeSeconds(sqsLongPollWaitTimeSeconds)\n+      val messages = sqsClient.receiveMessage(receiveMessageRequest).getMessages.asScala\n+      retriesOnFailure = 0\n+      logDebug(s\"successfully received ${messages.size} messages\")\n+      messages\n+    } catch {\n+      case ase: AmazonServiceException =>\n+        val message =\n+        \"\"\"\n+          |Caught an AmazonServiceException, which means your request made it to Amazon SQS,\n+          | rejected with an error response for some reason.\n+        \"\"\".stripMargin\n+        logWarning(message)\n+        logWarning(s\"Error Message: ${ase.getMessage}\")\n+        logWarning(s\"HTTP Status Code: ${ase.getStatusCode}, AWS Error Code: ${ase.getErrorCode}\")\n+        logWarning(s\"Error Type: ${ase.getErrorType}, Request ID: ${ase.getRequestId}\")\n+        evaluateRetries()\n+        List.empty\n+      case ace: AmazonClientException =>\n+        val message =\n+        \"\"\"\n+           |Caught an AmazonClientException, which means, the client encountered a serious\n+           | internal problem while trying to communicate with Amazon SQS, such as not\n+           |  being able to access the network.\n+        \"\"\".stripMargin\n+        logWarning(message)\n+        logWarning(s\"Error Message: ${ace.getMessage()}\")\n+        evaluateRetries()\n+        List.empty\n+      case e: Exception =>\n+        val message = \"Received unexpected error from SQS\"\n+        logWarning(message)\n+        logWarning(s\"Error Message: ${e.getMessage()}\")\n+        evaluateRetries()\n+        List.empty\n+    }\n+    if (messageList.nonEmpty) {\n+      parseSqsMessages(messageList)\n+    } else {\n+      Seq.empty\n+    }\n+  }\n+\n+  private def parseSqsMessages(messageList: Seq[Message]): Seq[(String, Long, String)] = {\n+    val errorMessages = scala.collection.mutable.ListBuffer[String]()\n+    val parsedMessages = messageList.foldLeft(Seq[(String, Long, String)]()) { (list, message) =>\n+      implicit val formats = DefaultFormats\n+      try {\n+        val messageReceiptHandle = message.getReceiptHandle\n+        val messageJson = parse(message.getBody).extract[JValue]\n+        val bucketName = (\n+          messageJson \\ \"Records\" \\ \"s3\" \\ \"bucket\" \\ \"name\").extract[Array[String]].head\n+        val eventName = (messageJson \\ \"Records\" \\ \"eventName\").extract[Array[String]].head\n+        if (eventName.contains(\"ObjectCreated\")) {\n+          val timestamp = (messageJson \\ \"Records\" \\ \"eventTime\").extract[Array[String]].head\n+          val timestampMills = convertTimestampToMills(timestamp)\n+          val path = \"s3://\" +\n+            bucketName + \"/\" +\n+            (messageJson \\ \"Records\" \\ \"s3\" \\ \"object\" \\ \"key\").extract[Array[String]].head\n+          logDebug(\"Successfully parsed sqs message\")\n+          list :+ ((path, timestampMills, messageReceiptHandle))\n+        } else {\n+          if (eventName.contains(\"ObjectRemoved\")) {\n+            if (!ignoreFileDeletion) {\n+              exception = Some(new SparkException(\"ObjectDelete message detected in SQS\"))\n+            } else {\n+              logInfo(\"Ignoring file deletion message since ignoreFileDeletion is true\")\n+            }\n+          } else {\n+            logWarning(\"Ignoring unexpected message detected in SQS\")\n+          }\n+          errorMessages.append(messageReceiptHandle)\n+          list\n+        }\n+      } catch {\n+        case me: MappingException =>\n+          errorMessages.append(message.getReceiptHandle)\n+          logWarning(s\"Error in parsing SQS message ${me.getMessage}\")\n+          list\n+        case e: Exception =>\n+          errorMessages.append(message.getReceiptHandle)\n+          logWarning(s\"Unexpected error while parsing SQS message ${e.getMessage}\")\n+          list\n+      }\n+    }\n+    if (errorMessages.nonEmpty) {\n+      addToDeleteMessageQueue(errorMessages.toList)\n+    }\n+    parsedMessages\n+  }\n+\n+  private def convertTimestampToMills(timestamp: String): Long = {\n+    val timeInMillis = timestampFormat.parse(timestamp).getTime()\n+    timeInMillis\n+  }\n+\n+  private def evaluateRetries(): Unit = {\n+    retriesOnFailure += 1\n+    if (retriesOnFailure >= sqsMaxRetries) {\n+      logError(\"Max retries reached\")\n+      exception = Some(new SparkException(\"Unable to receive Messages from SQS for \" +\n+        s\"${sqsMaxRetries} times Giving up. Check logs for details.\"))\n+    } else {\n+      logWarning(s\"Attempt ${retriesOnFailure}.\" +\n+        s\"Will reattempt after ${sqsFetchIntervalSeconds} seconds\")\n+    }\n+  }\n+\n+  private def createSqsClient(): AmazonSQS = {\n+    try {\n+      val isClusterOnEc2Role = hadoopConf.getBoolean(\n+        \"fs.s3.isClusterOnEc2Role\", false) || hadoopConf.getBoolean(\n+        \"fs.s3n.isClusterOnEc2Role\", false) || sourceOptions.useInstanceProfileCredentials\n+      if (!isClusterOnEc2Role) {\n+        val accessKey = hadoopConf.getTrimmed(\"fs.s3n.awsAccessKeyId\")",
    "line": 205
  }],
  "prId": 24934
}, {
  "comments": [{
    "author": {
      "login": "steveloughran"
    },
    "body": "See: https://github.com/steveloughran/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AUtils.java#L1206\r\n\r\nproxy support is essential; you can grab the keys from org.apache.hadoop.fs.s3a.Constants; things like timeout and buffer size a matter of preference",
    "commit": "1f4628fb6ee306450652171e9789f94c648f89d6",
    "createdAt": "2019-07-26T10:49:53Z",
    "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sqs\n+\n+import java.text.SimpleDateFormat\n+import java.util.TimeZone\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.JavaConverters._\n+\n+import com.amazonaws.{AmazonClientException, AmazonServiceException, ClientConfiguration}\n+import com.amazonaws.services.sqs.{AmazonSQS, AmazonSQSClientBuilder}\n+import com.amazonaws.services.sqs.model.{DeleteMessageBatchRequestEntry, Message, ReceiveMessageRequest}\n+import org.apache.hadoop.conf.Configuration\n+import org.json4s.{DefaultFormats, MappingException}\n+import org.json4s.JsonAST.JValue\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.ThreadUtils\n+\n+class SqsClient(sourceOptions: SqsSourceOptions,\n+                hadoopConf: Configuration) extends Logging {\n+\n+  private val sqsFetchIntervalSeconds = sourceOptions.fetchIntervalSeconds\n+  private val sqsLongPollWaitTimeSeconds = sourceOptions.longPollWaitTimeSeconds\n+  private val sqsMaxRetries = sourceOptions.maxRetries\n+  private val maxConnections = sourceOptions.maxConnections\n+  private val ignoreFileDeletion = sourceOptions.ignoreFileDeletion\n+  private val region = sourceOptions.region\n+  val sqsUrl = sourceOptions.sqsUrl\n+\n+  @volatile var exception: Option[Exception] = None\n+\n+  private val timestampFormat = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\") // ISO8601\n+  timestampFormat.setTimeZone(TimeZone.getTimeZone(\"UTC\"))\n+  private var retriesOnFailure = 0\n+  private val sqsClient = createSqsClient()\n+\n+  val sqsScheduler = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"sqs-scheduler\")\n+\n+  val sqsFileCache = new SqsFileCache(sourceOptions.maxFileAgeMs, sourceOptions.fileNameOnly)\n+\n+  val deleteMessageQueue = new java.util.concurrent.ConcurrentLinkedQueue[String]()\n+\n+  private val sqsFetchMessagesThread = new Runnable {\n+    override def run(): Unit = {\n+      try {\n+        // Fetching messages from Amazon SQS\n+        val newMessages = sqsFetchMessages()\n+\n+        // Filtering the new messages which are already not seen\n+        if (newMessages.nonEmpty) {\n+          newMessages.filter(message => sqsFileCache.isNewFile(message._1, message._2))\n+            .foreach(message =>\n+              sqsFileCache.add(message._1, MessageDescription(message._2, false, message._3)))\n+        }\n+      } catch {\n+        case e: Exception =>\n+          exception = Some(e)\n+      }\n+    }\n+  }\n+\n+  sqsScheduler.scheduleWithFixedDelay(\n+    sqsFetchMessagesThread,\n+    0,\n+    sqsFetchIntervalSeconds,\n+    TimeUnit.SECONDS)\n+\n+  private def sqsFetchMessages(): Seq[(String, Long, String)] = {\n+    val messageList = try {\n+      val receiveMessageRequest = new ReceiveMessageRequest()\n+        .withQueueUrl(sqsUrl)\n+        .withWaitTimeSeconds(sqsLongPollWaitTimeSeconds)\n+      val messages = sqsClient.receiveMessage(receiveMessageRequest).getMessages.asScala\n+      retriesOnFailure = 0\n+      logDebug(s\"successfully received ${messages.size} messages\")\n+      messages\n+    } catch {\n+      case ase: AmazonServiceException =>\n+        val message =\n+        \"\"\"\n+          |Caught an AmazonServiceException, which means your request made it to Amazon SQS,\n+          | rejected with an error response for some reason.\n+        \"\"\".stripMargin\n+        logWarning(message)\n+        logWarning(s\"Error Message: ${ase.getMessage}\")\n+        logWarning(s\"HTTP Status Code: ${ase.getStatusCode}, AWS Error Code: ${ase.getErrorCode}\")\n+        logWarning(s\"Error Type: ${ase.getErrorType}, Request ID: ${ase.getRequestId}\")\n+        evaluateRetries()\n+        List.empty\n+      case ace: AmazonClientException =>\n+        val message =\n+        \"\"\"\n+           |Caught an AmazonClientException, which means, the client encountered a serious\n+           | internal problem while trying to communicate with Amazon SQS, such as not\n+           |  being able to access the network.\n+        \"\"\".stripMargin\n+        logWarning(message)\n+        logWarning(s\"Error Message: ${ace.getMessage()}\")\n+        evaluateRetries()\n+        List.empty\n+      case e: Exception =>\n+        val message = \"Received unexpected error from SQS\"\n+        logWarning(message)\n+        logWarning(s\"Error Message: ${e.getMessage()}\")\n+        evaluateRetries()\n+        List.empty\n+    }\n+    if (messageList.nonEmpty) {\n+      parseSqsMessages(messageList)\n+    } else {\n+      Seq.empty\n+    }\n+  }\n+\n+  private def parseSqsMessages(messageList: Seq[Message]): Seq[(String, Long, String)] = {\n+    val errorMessages = scala.collection.mutable.ListBuffer[String]()\n+    val parsedMessages = messageList.foldLeft(Seq[(String, Long, String)]()) { (list, message) =>\n+      implicit val formats = DefaultFormats\n+      try {\n+        val messageReceiptHandle = message.getReceiptHandle\n+        val messageJson = parse(message.getBody).extract[JValue]\n+        val bucketName = (\n+          messageJson \\ \"Records\" \\ \"s3\" \\ \"bucket\" \\ \"name\").extract[Array[String]].head\n+        val eventName = (messageJson \\ \"Records\" \\ \"eventName\").extract[Array[String]].head\n+        if (eventName.contains(\"ObjectCreated\")) {\n+          val timestamp = (messageJson \\ \"Records\" \\ \"eventTime\").extract[Array[String]].head\n+          val timestampMills = convertTimestampToMills(timestamp)\n+          val path = \"s3://\" +\n+            bucketName + \"/\" +\n+            (messageJson \\ \"Records\" \\ \"s3\" \\ \"object\" \\ \"key\").extract[Array[String]].head\n+          logDebug(\"Successfully parsed sqs message\")\n+          list :+ ((path, timestampMills, messageReceiptHandle))\n+        } else {\n+          if (eventName.contains(\"ObjectRemoved\")) {\n+            if (!ignoreFileDeletion) {\n+              exception = Some(new SparkException(\"ObjectDelete message detected in SQS\"))\n+            } else {\n+              logInfo(\"Ignoring file deletion message since ignoreFileDeletion is true\")\n+            }\n+          } else {\n+            logWarning(\"Ignoring unexpected message detected in SQS\")\n+          }\n+          errorMessages.append(messageReceiptHandle)\n+          list\n+        }\n+      } catch {\n+        case me: MappingException =>\n+          errorMessages.append(message.getReceiptHandle)\n+          logWarning(s\"Error in parsing SQS message ${me.getMessage}\")\n+          list\n+        case e: Exception =>\n+          errorMessages.append(message.getReceiptHandle)\n+          logWarning(s\"Unexpected error while parsing SQS message ${e.getMessage}\")\n+          list\n+      }\n+    }\n+    if (errorMessages.nonEmpty) {\n+      addToDeleteMessageQueue(errorMessages.toList)\n+    }\n+    parsedMessages\n+  }\n+\n+  private def convertTimestampToMills(timestamp: String): Long = {\n+    val timeInMillis = timestampFormat.parse(timestamp).getTime()\n+    timeInMillis\n+  }\n+\n+  private def evaluateRetries(): Unit = {\n+    retriesOnFailure += 1\n+    if (retriesOnFailure >= sqsMaxRetries) {\n+      logError(\"Max retries reached\")\n+      exception = Some(new SparkException(\"Unable to receive Messages from SQS for \" +\n+        s\"${sqsMaxRetries} times Giving up. Check logs for details.\"))\n+    } else {\n+      logWarning(s\"Attempt ${retriesOnFailure}.\" +\n+        s\"Will reattempt after ${sqsFetchIntervalSeconds} seconds\")\n+    }\n+  }\n+\n+  private def createSqsClient(): AmazonSQS = {\n+    try {\n+      val isClusterOnEc2Role = hadoopConf.getBoolean(\n+        \"fs.s3.isClusterOnEc2Role\", false) || hadoopConf.getBoolean(\n+        \"fs.s3n.isClusterOnEc2Role\", false) || sourceOptions.useInstanceProfileCredentials\n+      if (!isClusterOnEc2Role) {\n+        val accessKey = hadoopConf.getTrimmed(\"fs.s3n.awsAccessKeyId\")\n+        val secretAccessKey = new String(hadoopConf.getPassword(\"fs.s3n.awsSecretAccessKey\")).trim\n+        logInfo(\"Using credentials from keys provided\")\n+        val basicAwsCredentialsProvider = new BasicAWSCredentialsProvider(\n+          accessKey, secretAccessKey)\n+        AmazonSQSClientBuilder\n+          .standard()\n+          .withClientConfiguration(new ClientConfiguration().withMaxConnections(maxConnections))\n+          .withCredentials(basicAwsCredentialsProvider)\n+          .withRegion(region)\n+          .build()\n+      } else {\n+        logInfo(\"Using the credentials attached to the instance\")\n+        val instanceProfileCredentialsProvider = new InstanceProfileCredentialsProviderWithRetries()\n+        AmazonSQSClientBuilder",
    "line": 219
  }, {
    "author": {
      "login": "abhishekd0907"
    },
    "body": "yeah thanks for pointing out, will pick this",
    "commit": "1f4628fb6ee306450652171e9789f94c648f89d6",
    "createdAt": "2019-07-31T07:12:44Z",
    "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sqs\n+\n+import java.text.SimpleDateFormat\n+import java.util.TimeZone\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.JavaConverters._\n+\n+import com.amazonaws.{AmazonClientException, AmazonServiceException, ClientConfiguration}\n+import com.amazonaws.services.sqs.{AmazonSQS, AmazonSQSClientBuilder}\n+import com.amazonaws.services.sqs.model.{DeleteMessageBatchRequestEntry, Message, ReceiveMessageRequest}\n+import org.apache.hadoop.conf.Configuration\n+import org.json4s.{DefaultFormats, MappingException}\n+import org.json4s.JsonAST.JValue\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.ThreadUtils\n+\n+class SqsClient(sourceOptions: SqsSourceOptions,\n+                hadoopConf: Configuration) extends Logging {\n+\n+  private val sqsFetchIntervalSeconds = sourceOptions.fetchIntervalSeconds\n+  private val sqsLongPollWaitTimeSeconds = sourceOptions.longPollWaitTimeSeconds\n+  private val sqsMaxRetries = sourceOptions.maxRetries\n+  private val maxConnections = sourceOptions.maxConnections\n+  private val ignoreFileDeletion = sourceOptions.ignoreFileDeletion\n+  private val region = sourceOptions.region\n+  val sqsUrl = sourceOptions.sqsUrl\n+\n+  @volatile var exception: Option[Exception] = None\n+\n+  private val timestampFormat = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\") // ISO8601\n+  timestampFormat.setTimeZone(TimeZone.getTimeZone(\"UTC\"))\n+  private var retriesOnFailure = 0\n+  private val sqsClient = createSqsClient()\n+\n+  val sqsScheduler = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"sqs-scheduler\")\n+\n+  val sqsFileCache = new SqsFileCache(sourceOptions.maxFileAgeMs, sourceOptions.fileNameOnly)\n+\n+  val deleteMessageQueue = new java.util.concurrent.ConcurrentLinkedQueue[String]()\n+\n+  private val sqsFetchMessagesThread = new Runnable {\n+    override def run(): Unit = {\n+      try {\n+        // Fetching messages from Amazon SQS\n+        val newMessages = sqsFetchMessages()\n+\n+        // Filtering the new messages which are already not seen\n+        if (newMessages.nonEmpty) {\n+          newMessages.filter(message => sqsFileCache.isNewFile(message._1, message._2))\n+            .foreach(message =>\n+              sqsFileCache.add(message._1, MessageDescription(message._2, false, message._3)))\n+        }\n+      } catch {\n+        case e: Exception =>\n+          exception = Some(e)\n+      }\n+    }\n+  }\n+\n+  sqsScheduler.scheduleWithFixedDelay(\n+    sqsFetchMessagesThread,\n+    0,\n+    sqsFetchIntervalSeconds,\n+    TimeUnit.SECONDS)\n+\n+  private def sqsFetchMessages(): Seq[(String, Long, String)] = {\n+    val messageList = try {\n+      val receiveMessageRequest = new ReceiveMessageRequest()\n+        .withQueueUrl(sqsUrl)\n+        .withWaitTimeSeconds(sqsLongPollWaitTimeSeconds)\n+      val messages = sqsClient.receiveMessage(receiveMessageRequest).getMessages.asScala\n+      retriesOnFailure = 0\n+      logDebug(s\"successfully received ${messages.size} messages\")\n+      messages\n+    } catch {\n+      case ase: AmazonServiceException =>\n+        val message =\n+        \"\"\"\n+          |Caught an AmazonServiceException, which means your request made it to Amazon SQS,\n+          | rejected with an error response for some reason.\n+        \"\"\".stripMargin\n+        logWarning(message)\n+        logWarning(s\"Error Message: ${ase.getMessage}\")\n+        logWarning(s\"HTTP Status Code: ${ase.getStatusCode}, AWS Error Code: ${ase.getErrorCode}\")\n+        logWarning(s\"Error Type: ${ase.getErrorType}, Request ID: ${ase.getRequestId}\")\n+        evaluateRetries()\n+        List.empty\n+      case ace: AmazonClientException =>\n+        val message =\n+        \"\"\"\n+           |Caught an AmazonClientException, which means, the client encountered a serious\n+           | internal problem while trying to communicate with Amazon SQS, such as not\n+           |  being able to access the network.\n+        \"\"\".stripMargin\n+        logWarning(message)\n+        logWarning(s\"Error Message: ${ace.getMessage()}\")\n+        evaluateRetries()\n+        List.empty\n+      case e: Exception =>\n+        val message = \"Received unexpected error from SQS\"\n+        logWarning(message)\n+        logWarning(s\"Error Message: ${e.getMessage()}\")\n+        evaluateRetries()\n+        List.empty\n+    }\n+    if (messageList.nonEmpty) {\n+      parseSqsMessages(messageList)\n+    } else {\n+      Seq.empty\n+    }\n+  }\n+\n+  private def parseSqsMessages(messageList: Seq[Message]): Seq[(String, Long, String)] = {\n+    val errorMessages = scala.collection.mutable.ListBuffer[String]()\n+    val parsedMessages = messageList.foldLeft(Seq[(String, Long, String)]()) { (list, message) =>\n+      implicit val formats = DefaultFormats\n+      try {\n+        val messageReceiptHandle = message.getReceiptHandle\n+        val messageJson = parse(message.getBody).extract[JValue]\n+        val bucketName = (\n+          messageJson \\ \"Records\" \\ \"s3\" \\ \"bucket\" \\ \"name\").extract[Array[String]].head\n+        val eventName = (messageJson \\ \"Records\" \\ \"eventName\").extract[Array[String]].head\n+        if (eventName.contains(\"ObjectCreated\")) {\n+          val timestamp = (messageJson \\ \"Records\" \\ \"eventTime\").extract[Array[String]].head\n+          val timestampMills = convertTimestampToMills(timestamp)\n+          val path = \"s3://\" +\n+            bucketName + \"/\" +\n+            (messageJson \\ \"Records\" \\ \"s3\" \\ \"object\" \\ \"key\").extract[Array[String]].head\n+          logDebug(\"Successfully parsed sqs message\")\n+          list :+ ((path, timestampMills, messageReceiptHandle))\n+        } else {\n+          if (eventName.contains(\"ObjectRemoved\")) {\n+            if (!ignoreFileDeletion) {\n+              exception = Some(new SparkException(\"ObjectDelete message detected in SQS\"))\n+            } else {\n+              logInfo(\"Ignoring file deletion message since ignoreFileDeletion is true\")\n+            }\n+          } else {\n+            logWarning(\"Ignoring unexpected message detected in SQS\")\n+          }\n+          errorMessages.append(messageReceiptHandle)\n+          list\n+        }\n+      } catch {\n+        case me: MappingException =>\n+          errorMessages.append(message.getReceiptHandle)\n+          logWarning(s\"Error in parsing SQS message ${me.getMessage}\")\n+          list\n+        case e: Exception =>\n+          errorMessages.append(message.getReceiptHandle)\n+          logWarning(s\"Unexpected error while parsing SQS message ${e.getMessage}\")\n+          list\n+      }\n+    }\n+    if (errorMessages.nonEmpty) {\n+      addToDeleteMessageQueue(errorMessages.toList)\n+    }\n+    parsedMessages\n+  }\n+\n+  private def convertTimestampToMills(timestamp: String): Long = {\n+    val timeInMillis = timestampFormat.parse(timestamp).getTime()\n+    timeInMillis\n+  }\n+\n+  private def evaluateRetries(): Unit = {\n+    retriesOnFailure += 1\n+    if (retriesOnFailure >= sqsMaxRetries) {\n+      logError(\"Max retries reached\")\n+      exception = Some(new SparkException(\"Unable to receive Messages from SQS for \" +\n+        s\"${sqsMaxRetries} times Giving up. Check logs for details.\"))\n+    } else {\n+      logWarning(s\"Attempt ${retriesOnFailure}.\" +\n+        s\"Will reattempt after ${sqsFetchIntervalSeconds} seconds\")\n+    }\n+  }\n+\n+  private def createSqsClient(): AmazonSQS = {\n+    try {\n+      val isClusterOnEc2Role = hadoopConf.getBoolean(\n+        \"fs.s3.isClusterOnEc2Role\", false) || hadoopConf.getBoolean(\n+        \"fs.s3n.isClusterOnEc2Role\", false) || sourceOptions.useInstanceProfileCredentials\n+      if (!isClusterOnEc2Role) {\n+        val accessKey = hadoopConf.getTrimmed(\"fs.s3n.awsAccessKeyId\")\n+        val secretAccessKey = new String(hadoopConf.getPassword(\"fs.s3n.awsSecretAccessKey\")).trim\n+        logInfo(\"Using credentials from keys provided\")\n+        val basicAwsCredentialsProvider = new BasicAWSCredentialsProvider(\n+          accessKey, secretAccessKey)\n+        AmazonSQSClientBuilder\n+          .standard()\n+          .withClientConfiguration(new ClientConfiguration().withMaxConnections(maxConnections))\n+          .withCredentials(basicAwsCredentialsProvider)\n+          .withRegion(region)\n+          .build()\n+      } else {\n+        logInfo(\"Using the credentials attached to the instance\")\n+        val instanceProfileCredentialsProvider = new InstanceProfileCredentialsProviderWithRetries()\n+        AmazonSQSClientBuilder",
    "line": 219
  }],
  "prId": 24934
}, {
  "comments": [{
    "author": {
      "login": "steveloughran"
    },
    "body": "needs to include the inner exception",
    "commit": "1f4628fb6ee306450652171e9789f94c648f89d6",
    "createdAt": "2019-07-26T10:50:12Z",
    "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sqs\n+\n+import java.text.SimpleDateFormat\n+import java.util.TimeZone\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.JavaConverters._\n+\n+import com.amazonaws.{AmazonClientException, AmazonServiceException, ClientConfiguration}\n+import com.amazonaws.services.sqs.{AmazonSQS, AmazonSQSClientBuilder}\n+import com.amazonaws.services.sqs.model.{DeleteMessageBatchRequestEntry, Message, ReceiveMessageRequest}\n+import org.apache.hadoop.conf.Configuration\n+import org.json4s.{DefaultFormats, MappingException}\n+import org.json4s.JsonAST.JValue\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.ThreadUtils\n+\n+class SqsClient(sourceOptions: SqsSourceOptions,\n+                hadoopConf: Configuration) extends Logging {\n+\n+  private val sqsFetchIntervalSeconds = sourceOptions.fetchIntervalSeconds\n+  private val sqsLongPollWaitTimeSeconds = sourceOptions.longPollWaitTimeSeconds\n+  private val sqsMaxRetries = sourceOptions.maxRetries\n+  private val maxConnections = sourceOptions.maxConnections\n+  private val ignoreFileDeletion = sourceOptions.ignoreFileDeletion\n+  private val region = sourceOptions.region\n+  val sqsUrl = sourceOptions.sqsUrl\n+\n+  @volatile var exception: Option[Exception] = None\n+\n+  private val timestampFormat = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\") // ISO8601\n+  timestampFormat.setTimeZone(TimeZone.getTimeZone(\"UTC\"))\n+  private var retriesOnFailure = 0\n+  private val sqsClient = createSqsClient()\n+\n+  val sqsScheduler = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"sqs-scheduler\")\n+\n+  val sqsFileCache = new SqsFileCache(sourceOptions.maxFileAgeMs, sourceOptions.fileNameOnly)\n+\n+  val deleteMessageQueue = new java.util.concurrent.ConcurrentLinkedQueue[String]()\n+\n+  private val sqsFetchMessagesThread = new Runnable {\n+    override def run(): Unit = {\n+      try {\n+        // Fetching messages from Amazon SQS\n+        val newMessages = sqsFetchMessages()\n+\n+        // Filtering the new messages which are already not seen\n+        if (newMessages.nonEmpty) {\n+          newMessages.filter(message => sqsFileCache.isNewFile(message._1, message._2))\n+            .foreach(message =>\n+              sqsFileCache.add(message._1, MessageDescription(message._2, false, message._3)))\n+        }\n+      } catch {\n+        case e: Exception =>\n+          exception = Some(e)\n+      }\n+    }\n+  }\n+\n+  sqsScheduler.scheduleWithFixedDelay(\n+    sqsFetchMessagesThread,\n+    0,\n+    sqsFetchIntervalSeconds,\n+    TimeUnit.SECONDS)\n+\n+  private def sqsFetchMessages(): Seq[(String, Long, String)] = {\n+    val messageList = try {\n+      val receiveMessageRequest = new ReceiveMessageRequest()\n+        .withQueueUrl(sqsUrl)\n+        .withWaitTimeSeconds(sqsLongPollWaitTimeSeconds)\n+      val messages = sqsClient.receiveMessage(receiveMessageRequest).getMessages.asScala\n+      retriesOnFailure = 0\n+      logDebug(s\"successfully received ${messages.size} messages\")\n+      messages\n+    } catch {\n+      case ase: AmazonServiceException =>\n+        val message =\n+        \"\"\"\n+          |Caught an AmazonServiceException, which means your request made it to Amazon SQS,\n+          | rejected with an error response for some reason.\n+        \"\"\".stripMargin\n+        logWarning(message)\n+        logWarning(s\"Error Message: ${ase.getMessage}\")\n+        logWarning(s\"HTTP Status Code: ${ase.getStatusCode}, AWS Error Code: ${ase.getErrorCode}\")\n+        logWarning(s\"Error Type: ${ase.getErrorType}, Request ID: ${ase.getRequestId}\")\n+        evaluateRetries()\n+        List.empty\n+      case ace: AmazonClientException =>\n+        val message =\n+        \"\"\"\n+           |Caught an AmazonClientException, which means, the client encountered a serious\n+           | internal problem while trying to communicate with Amazon SQS, such as not\n+           |  being able to access the network.\n+        \"\"\".stripMargin\n+        logWarning(message)\n+        logWarning(s\"Error Message: ${ace.getMessage()}\")\n+        evaluateRetries()\n+        List.empty\n+      case e: Exception =>\n+        val message = \"Received unexpected error from SQS\"\n+        logWarning(message)\n+        logWarning(s\"Error Message: ${e.getMessage()}\")\n+        evaluateRetries()\n+        List.empty\n+    }\n+    if (messageList.nonEmpty) {\n+      parseSqsMessages(messageList)\n+    } else {\n+      Seq.empty\n+    }\n+  }\n+\n+  private def parseSqsMessages(messageList: Seq[Message]): Seq[(String, Long, String)] = {\n+    val errorMessages = scala.collection.mutable.ListBuffer[String]()\n+    val parsedMessages = messageList.foldLeft(Seq[(String, Long, String)]()) { (list, message) =>\n+      implicit val formats = DefaultFormats\n+      try {\n+        val messageReceiptHandle = message.getReceiptHandle\n+        val messageJson = parse(message.getBody).extract[JValue]\n+        val bucketName = (\n+          messageJson \\ \"Records\" \\ \"s3\" \\ \"bucket\" \\ \"name\").extract[Array[String]].head\n+        val eventName = (messageJson \\ \"Records\" \\ \"eventName\").extract[Array[String]].head\n+        if (eventName.contains(\"ObjectCreated\")) {\n+          val timestamp = (messageJson \\ \"Records\" \\ \"eventTime\").extract[Array[String]].head\n+          val timestampMills = convertTimestampToMills(timestamp)\n+          val path = \"s3://\" +\n+            bucketName + \"/\" +\n+            (messageJson \\ \"Records\" \\ \"s3\" \\ \"object\" \\ \"key\").extract[Array[String]].head\n+          logDebug(\"Successfully parsed sqs message\")\n+          list :+ ((path, timestampMills, messageReceiptHandle))\n+        } else {\n+          if (eventName.contains(\"ObjectRemoved\")) {\n+            if (!ignoreFileDeletion) {\n+              exception = Some(new SparkException(\"ObjectDelete message detected in SQS\"))\n+            } else {\n+              logInfo(\"Ignoring file deletion message since ignoreFileDeletion is true\")\n+            }\n+          } else {\n+            logWarning(\"Ignoring unexpected message detected in SQS\")\n+          }\n+          errorMessages.append(messageReceiptHandle)\n+          list\n+        }\n+      } catch {\n+        case me: MappingException =>\n+          errorMessages.append(message.getReceiptHandle)\n+          logWarning(s\"Error in parsing SQS message ${me.getMessage}\")\n+          list\n+        case e: Exception =>\n+          errorMessages.append(message.getReceiptHandle)\n+          logWarning(s\"Unexpected error while parsing SQS message ${e.getMessage}\")\n+          list\n+      }\n+    }\n+    if (errorMessages.nonEmpty) {\n+      addToDeleteMessageQueue(errorMessages.toList)\n+    }\n+    parsedMessages\n+  }\n+\n+  private def convertTimestampToMills(timestamp: String): Long = {\n+    val timeInMillis = timestampFormat.parse(timestamp).getTime()\n+    timeInMillis\n+  }\n+\n+  private def evaluateRetries(): Unit = {\n+    retriesOnFailure += 1\n+    if (retriesOnFailure >= sqsMaxRetries) {\n+      logError(\"Max retries reached\")\n+      exception = Some(new SparkException(\"Unable to receive Messages from SQS for \" +\n+        s\"${sqsMaxRetries} times Giving up. Check logs for details.\"))\n+    } else {\n+      logWarning(s\"Attempt ${retriesOnFailure}.\" +\n+        s\"Will reattempt after ${sqsFetchIntervalSeconds} seconds\")\n+    }\n+  }\n+\n+  private def createSqsClient(): AmazonSQS = {\n+    try {\n+      val isClusterOnEc2Role = hadoopConf.getBoolean(\n+        \"fs.s3.isClusterOnEc2Role\", false) || hadoopConf.getBoolean(\n+        \"fs.s3n.isClusterOnEc2Role\", false) || sourceOptions.useInstanceProfileCredentials\n+      if (!isClusterOnEc2Role) {\n+        val accessKey = hadoopConf.getTrimmed(\"fs.s3n.awsAccessKeyId\")\n+        val secretAccessKey = new String(hadoopConf.getPassword(\"fs.s3n.awsSecretAccessKey\")).trim\n+        logInfo(\"Using credentials from keys provided\")\n+        val basicAwsCredentialsProvider = new BasicAWSCredentialsProvider(\n+          accessKey, secretAccessKey)\n+        AmazonSQSClientBuilder\n+          .standard()\n+          .withClientConfiguration(new ClientConfiguration().withMaxConnections(maxConnections))\n+          .withCredentials(basicAwsCredentialsProvider)\n+          .withRegion(region)\n+          .build()\n+      } else {\n+        logInfo(\"Using the credentials attached to the instance\")\n+        val instanceProfileCredentialsProvider = new InstanceProfileCredentialsProviderWithRetries()\n+        AmazonSQSClientBuilder\n+          .standard()\n+          .withClientConfiguration(new ClientConfiguration().withMaxConnections(maxConnections))\n+          .withCredentials(instanceProfileCredentialsProvider)\n+          .build()\n+      }\n+    } catch {\n+      case e: Exception =>\n+        throw new SparkException(s\"Error occured while creating Amazon SQS Client ${e.getMessage}\")",
    "line": 227
  }, {
    "author": {
      "login": "abhishekd0907"
    },
    "body": "will pick this change",
    "commit": "1f4628fb6ee306450652171e9789f94c648f89d6",
    "createdAt": "2019-07-31T07:08:00Z",
    "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sqs\n+\n+import java.text.SimpleDateFormat\n+import java.util.TimeZone\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.JavaConverters._\n+\n+import com.amazonaws.{AmazonClientException, AmazonServiceException, ClientConfiguration}\n+import com.amazonaws.services.sqs.{AmazonSQS, AmazonSQSClientBuilder}\n+import com.amazonaws.services.sqs.model.{DeleteMessageBatchRequestEntry, Message, ReceiveMessageRequest}\n+import org.apache.hadoop.conf.Configuration\n+import org.json4s.{DefaultFormats, MappingException}\n+import org.json4s.JsonAST.JValue\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.ThreadUtils\n+\n+class SqsClient(sourceOptions: SqsSourceOptions,\n+                hadoopConf: Configuration) extends Logging {\n+\n+  private val sqsFetchIntervalSeconds = sourceOptions.fetchIntervalSeconds\n+  private val sqsLongPollWaitTimeSeconds = sourceOptions.longPollWaitTimeSeconds\n+  private val sqsMaxRetries = sourceOptions.maxRetries\n+  private val maxConnections = sourceOptions.maxConnections\n+  private val ignoreFileDeletion = sourceOptions.ignoreFileDeletion\n+  private val region = sourceOptions.region\n+  val sqsUrl = sourceOptions.sqsUrl\n+\n+  @volatile var exception: Option[Exception] = None\n+\n+  private val timestampFormat = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\") // ISO8601\n+  timestampFormat.setTimeZone(TimeZone.getTimeZone(\"UTC\"))\n+  private var retriesOnFailure = 0\n+  private val sqsClient = createSqsClient()\n+\n+  val sqsScheduler = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"sqs-scheduler\")\n+\n+  val sqsFileCache = new SqsFileCache(sourceOptions.maxFileAgeMs, sourceOptions.fileNameOnly)\n+\n+  val deleteMessageQueue = new java.util.concurrent.ConcurrentLinkedQueue[String]()\n+\n+  private val sqsFetchMessagesThread = new Runnable {\n+    override def run(): Unit = {\n+      try {\n+        // Fetching messages from Amazon SQS\n+        val newMessages = sqsFetchMessages()\n+\n+        // Filtering the new messages which are already not seen\n+        if (newMessages.nonEmpty) {\n+          newMessages.filter(message => sqsFileCache.isNewFile(message._1, message._2))\n+            .foreach(message =>\n+              sqsFileCache.add(message._1, MessageDescription(message._2, false, message._3)))\n+        }\n+      } catch {\n+        case e: Exception =>\n+          exception = Some(e)\n+      }\n+    }\n+  }\n+\n+  sqsScheduler.scheduleWithFixedDelay(\n+    sqsFetchMessagesThread,\n+    0,\n+    sqsFetchIntervalSeconds,\n+    TimeUnit.SECONDS)\n+\n+  private def sqsFetchMessages(): Seq[(String, Long, String)] = {\n+    val messageList = try {\n+      val receiveMessageRequest = new ReceiveMessageRequest()\n+        .withQueueUrl(sqsUrl)\n+        .withWaitTimeSeconds(sqsLongPollWaitTimeSeconds)\n+      val messages = sqsClient.receiveMessage(receiveMessageRequest).getMessages.asScala\n+      retriesOnFailure = 0\n+      logDebug(s\"successfully received ${messages.size} messages\")\n+      messages\n+    } catch {\n+      case ase: AmazonServiceException =>\n+        val message =\n+        \"\"\"\n+          |Caught an AmazonServiceException, which means your request made it to Amazon SQS,\n+          | rejected with an error response for some reason.\n+        \"\"\".stripMargin\n+        logWarning(message)\n+        logWarning(s\"Error Message: ${ase.getMessage}\")\n+        logWarning(s\"HTTP Status Code: ${ase.getStatusCode}, AWS Error Code: ${ase.getErrorCode}\")\n+        logWarning(s\"Error Type: ${ase.getErrorType}, Request ID: ${ase.getRequestId}\")\n+        evaluateRetries()\n+        List.empty\n+      case ace: AmazonClientException =>\n+        val message =\n+        \"\"\"\n+           |Caught an AmazonClientException, which means, the client encountered a serious\n+           | internal problem while trying to communicate with Amazon SQS, such as not\n+           |  being able to access the network.\n+        \"\"\".stripMargin\n+        logWarning(message)\n+        logWarning(s\"Error Message: ${ace.getMessage()}\")\n+        evaluateRetries()\n+        List.empty\n+      case e: Exception =>\n+        val message = \"Received unexpected error from SQS\"\n+        logWarning(message)\n+        logWarning(s\"Error Message: ${e.getMessage()}\")\n+        evaluateRetries()\n+        List.empty\n+    }\n+    if (messageList.nonEmpty) {\n+      parseSqsMessages(messageList)\n+    } else {\n+      Seq.empty\n+    }\n+  }\n+\n+  private def parseSqsMessages(messageList: Seq[Message]): Seq[(String, Long, String)] = {\n+    val errorMessages = scala.collection.mutable.ListBuffer[String]()\n+    val parsedMessages = messageList.foldLeft(Seq[(String, Long, String)]()) { (list, message) =>\n+      implicit val formats = DefaultFormats\n+      try {\n+        val messageReceiptHandle = message.getReceiptHandle\n+        val messageJson = parse(message.getBody).extract[JValue]\n+        val bucketName = (\n+          messageJson \\ \"Records\" \\ \"s3\" \\ \"bucket\" \\ \"name\").extract[Array[String]].head\n+        val eventName = (messageJson \\ \"Records\" \\ \"eventName\").extract[Array[String]].head\n+        if (eventName.contains(\"ObjectCreated\")) {\n+          val timestamp = (messageJson \\ \"Records\" \\ \"eventTime\").extract[Array[String]].head\n+          val timestampMills = convertTimestampToMills(timestamp)\n+          val path = \"s3://\" +\n+            bucketName + \"/\" +\n+            (messageJson \\ \"Records\" \\ \"s3\" \\ \"object\" \\ \"key\").extract[Array[String]].head\n+          logDebug(\"Successfully parsed sqs message\")\n+          list :+ ((path, timestampMills, messageReceiptHandle))\n+        } else {\n+          if (eventName.contains(\"ObjectRemoved\")) {\n+            if (!ignoreFileDeletion) {\n+              exception = Some(new SparkException(\"ObjectDelete message detected in SQS\"))\n+            } else {\n+              logInfo(\"Ignoring file deletion message since ignoreFileDeletion is true\")\n+            }\n+          } else {\n+            logWarning(\"Ignoring unexpected message detected in SQS\")\n+          }\n+          errorMessages.append(messageReceiptHandle)\n+          list\n+        }\n+      } catch {\n+        case me: MappingException =>\n+          errorMessages.append(message.getReceiptHandle)\n+          logWarning(s\"Error in parsing SQS message ${me.getMessage}\")\n+          list\n+        case e: Exception =>\n+          errorMessages.append(message.getReceiptHandle)\n+          logWarning(s\"Unexpected error while parsing SQS message ${e.getMessage}\")\n+          list\n+      }\n+    }\n+    if (errorMessages.nonEmpty) {\n+      addToDeleteMessageQueue(errorMessages.toList)\n+    }\n+    parsedMessages\n+  }\n+\n+  private def convertTimestampToMills(timestamp: String): Long = {\n+    val timeInMillis = timestampFormat.parse(timestamp).getTime()\n+    timeInMillis\n+  }\n+\n+  private def evaluateRetries(): Unit = {\n+    retriesOnFailure += 1\n+    if (retriesOnFailure >= sqsMaxRetries) {\n+      logError(\"Max retries reached\")\n+      exception = Some(new SparkException(\"Unable to receive Messages from SQS for \" +\n+        s\"${sqsMaxRetries} times Giving up. Check logs for details.\"))\n+    } else {\n+      logWarning(s\"Attempt ${retriesOnFailure}.\" +\n+        s\"Will reattempt after ${sqsFetchIntervalSeconds} seconds\")\n+    }\n+  }\n+\n+  private def createSqsClient(): AmazonSQS = {\n+    try {\n+      val isClusterOnEc2Role = hadoopConf.getBoolean(\n+        \"fs.s3.isClusterOnEc2Role\", false) || hadoopConf.getBoolean(\n+        \"fs.s3n.isClusterOnEc2Role\", false) || sourceOptions.useInstanceProfileCredentials\n+      if (!isClusterOnEc2Role) {\n+        val accessKey = hadoopConf.getTrimmed(\"fs.s3n.awsAccessKeyId\")\n+        val secretAccessKey = new String(hadoopConf.getPassword(\"fs.s3n.awsSecretAccessKey\")).trim\n+        logInfo(\"Using credentials from keys provided\")\n+        val basicAwsCredentialsProvider = new BasicAWSCredentialsProvider(\n+          accessKey, secretAccessKey)\n+        AmazonSQSClientBuilder\n+          .standard()\n+          .withClientConfiguration(new ClientConfiguration().withMaxConnections(maxConnections))\n+          .withCredentials(basicAwsCredentialsProvider)\n+          .withRegion(region)\n+          .build()\n+      } else {\n+        logInfo(\"Using the credentials attached to the instance\")\n+        val instanceProfileCredentialsProvider = new InstanceProfileCredentialsProviderWithRetries()\n+        AmazonSQSClientBuilder\n+          .standard()\n+          .withClientConfiguration(new ClientConfiguration().withMaxConnections(maxConnections))\n+          .withCredentials(instanceProfileCredentialsProvider)\n+          .build()\n+      }\n+    } catch {\n+      case e: Exception =>\n+        throw new SparkException(s\"Error occured while creating Amazon SQS Client ${e.getMessage}\")",
    "line": 227
  }],
  "prId": 24934
}, {
  "comments": [{
    "author": {
      "login": "steveloughran"
    },
    "body": "should log the exception. Also, some java exceptions (including NullPointerException) actually have a null .getMessage; the toString value is better",
    "commit": "1f4628fb6ee306450652171e9789f94c648f89d6",
    "createdAt": "2019-07-26T10:50:59Z",
    "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sqs\n+\n+import java.text.SimpleDateFormat\n+import java.util.TimeZone\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.JavaConverters._\n+\n+import com.amazonaws.{AmazonClientException, AmazonServiceException, ClientConfiguration}\n+import com.amazonaws.services.sqs.{AmazonSQS, AmazonSQSClientBuilder}\n+import com.amazonaws.services.sqs.model.{DeleteMessageBatchRequestEntry, Message, ReceiveMessageRequest}\n+import org.apache.hadoop.conf.Configuration\n+import org.json4s.{DefaultFormats, MappingException}\n+import org.json4s.JsonAST.JValue\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.ThreadUtils\n+\n+class SqsClient(sourceOptions: SqsSourceOptions,\n+                hadoopConf: Configuration) extends Logging {\n+\n+  private val sqsFetchIntervalSeconds = sourceOptions.fetchIntervalSeconds\n+  private val sqsLongPollWaitTimeSeconds = sourceOptions.longPollWaitTimeSeconds\n+  private val sqsMaxRetries = sourceOptions.maxRetries\n+  private val maxConnections = sourceOptions.maxConnections\n+  private val ignoreFileDeletion = sourceOptions.ignoreFileDeletion\n+  private val region = sourceOptions.region\n+  val sqsUrl = sourceOptions.sqsUrl\n+\n+  @volatile var exception: Option[Exception] = None\n+\n+  private val timestampFormat = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\") // ISO8601\n+  timestampFormat.setTimeZone(TimeZone.getTimeZone(\"UTC\"))\n+  private var retriesOnFailure = 0\n+  private val sqsClient = createSqsClient()\n+\n+  val sqsScheduler = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"sqs-scheduler\")\n+\n+  val sqsFileCache = new SqsFileCache(sourceOptions.maxFileAgeMs, sourceOptions.fileNameOnly)\n+\n+  val deleteMessageQueue = new java.util.concurrent.ConcurrentLinkedQueue[String]()\n+\n+  private val sqsFetchMessagesThread = new Runnable {\n+    override def run(): Unit = {\n+      try {\n+        // Fetching messages from Amazon SQS\n+        val newMessages = sqsFetchMessages()\n+\n+        // Filtering the new messages which are already not seen\n+        if (newMessages.nonEmpty) {\n+          newMessages.filter(message => sqsFileCache.isNewFile(message._1, message._2))\n+            .foreach(message =>\n+              sqsFileCache.add(message._1, MessageDescription(message._2, false, message._3)))\n+        }\n+      } catch {\n+        case e: Exception =>\n+          exception = Some(e)\n+      }\n+    }\n+  }\n+\n+  sqsScheduler.scheduleWithFixedDelay(\n+    sqsFetchMessagesThread,\n+    0,\n+    sqsFetchIntervalSeconds,\n+    TimeUnit.SECONDS)\n+\n+  private def sqsFetchMessages(): Seq[(String, Long, String)] = {\n+    val messageList = try {\n+      val receiveMessageRequest = new ReceiveMessageRequest()\n+        .withQueueUrl(sqsUrl)\n+        .withWaitTimeSeconds(sqsLongPollWaitTimeSeconds)\n+      val messages = sqsClient.receiveMessage(receiveMessageRequest).getMessages.asScala\n+      retriesOnFailure = 0\n+      logDebug(s\"successfully received ${messages.size} messages\")\n+      messages\n+    } catch {\n+      case ase: AmazonServiceException =>\n+        val message =\n+        \"\"\"\n+          |Caught an AmazonServiceException, which means your request made it to Amazon SQS,\n+          | rejected with an error response for some reason.\n+        \"\"\".stripMargin\n+        logWarning(message)\n+        logWarning(s\"Error Message: ${ase.getMessage}\")\n+        logWarning(s\"HTTP Status Code: ${ase.getStatusCode}, AWS Error Code: ${ase.getErrorCode}\")\n+        logWarning(s\"Error Type: ${ase.getErrorType}, Request ID: ${ase.getRequestId}\")\n+        evaluateRetries()\n+        List.empty\n+      case ace: AmazonClientException =>\n+        val message =\n+        \"\"\"\n+           |Caught an AmazonClientException, which means, the client encountered a serious\n+           | internal problem while trying to communicate with Amazon SQS, such as not\n+           |  being able to access the network.\n+        \"\"\".stripMargin\n+        logWarning(message)\n+        logWarning(s\"Error Message: ${ace.getMessage()}\")\n+        evaluateRetries()\n+        List.empty\n+      case e: Exception =>\n+        val message = \"Received unexpected error from SQS\"\n+        logWarning(message)\n+        logWarning(s\"Error Message: ${e.getMessage()}\")\n+        evaluateRetries()\n+        List.empty\n+    }\n+    if (messageList.nonEmpty) {\n+      parseSqsMessages(messageList)\n+    } else {\n+      Seq.empty\n+    }\n+  }\n+\n+  private def parseSqsMessages(messageList: Seq[Message]): Seq[(String, Long, String)] = {\n+    val errorMessages = scala.collection.mutable.ListBuffer[String]()\n+    val parsedMessages = messageList.foldLeft(Seq[(String, Long, String)]()) { (list, message) =>\n+      implicit val formats = DefaultFormats\n+      try {\n+        val messageReceiptHandle = message.getReceiptHandle\n+        val messageJson = parse(message.getBody).extract[JValue]\n+        val bucketName = (\n+          messageJson \\ \"Records\" \\ \"s3\" \\ \"bucket\" \\ \"name\").extract[Array[String]].head\n+        val eventName = (messageJson \\ \"Records\" \\ \"eventName\").extract[Array[String]].head\n+        if (eventName.contains(\"ObjectCreated\")) {\n+          val timestamp = (messageJson \\ \"Records\" \\ \"eventTime\").extract[Array[String]].head\n+          val timestampMills = convertTimestampToMills(timestamp)\n+          val path = \"s3://\" +\n+            bucketName + \"/\" +\n+            (messageJson \\ \"Records\" \\ \"s3\" \\ \"object\" \\ \"key\").extract[Array[String]].head\n+          logDebug(\"Successfully parsed sqs message\")\n+          list :+ ((path, timestampMills, messageReceiptHandle))\n+        } else {\n+          if (eventName.contains(\"ObjectRemoved\")) {\n+            if (!ignoreFileDeletion) {\n+              exception = Some(new SparkException(\"ObjectDelete message detected in SQS\"))\n+            } else {\n+              logInfo(\"Ignoring file deletion message since ignoreFileDeletion is true\")\n+            }\n+          } else {\n+            logWarning(\"Ignoring unexpected message detected in SQS\")\n+          }\n+          errorMessages.append(messageReceiptHandle)\n+          list\n+        }\n+      } catch {\n+        case me: MappingException =>\n+          errorMessages.append(message.getReceiptHandle)\n+          logWarning(s\"Error in parsing SQS message ${me.getMessage}\")\n+          list\n+        case e: Exception =>\n+          errorMessages.append(message.getReceiptHandle)\n+          logWarning(s\"Unexpected error while parsing SQS message ${e.getMessage}\")\n+          list\n+      }\n+    }\n+    if (errorMessages.nonEmpty) {\n+      addToDeleteMessageQueue(errorMessages.toList)\n+    }\n+    parsedMessages\n+  }\n+\n+  private def convertTimestampToMills(timestamp: String): Long = {\n+    val timeInMillis = timestampFormat.parse(timestamp).getTime()\n+    timeInMillis\n+  }\n+\n+  private def evaluateRetries(): Unit = {\n+    retriesOnFailure += 1\n+    if (retriesOnFailure >= sqsMaxRetries) {\n+      logError(\"Max retries reached\")\n+      exception = Some(new SparkException(\"Unable to receive Messages from SQS for \" +\n+        s\"${sqsMaxRetries} times Giving up. Check logs for details.\"))\n+    } else {\n+      logWarning(s\"Attempt ${retriesOnFailure}.\" +\n+        s\"Will reattempt after ${sqsFetchIntervalSeconds} seconds\")\n+    }\n+  }\n+\n+  private def createSqsClient(): AmazonSQS = {\n+    try {\n+      val isClusterOnEc2Role = hadoopConf.getBoolean(\n+        \"fs.s3.isClusterOnEc2Role\", false) || hadoopConf.getBoolean(\n+        \"fs.s3n.isClusterOnEc2Role\", false) || sourceOptions.useInstanceProfileCredentials\n+      if (!isClusterOnEc2Role) {\n+        val accessKey = hadoopConf.getTrimmed(\"fs.s3n.awsAccessKeyId\")\n+        val secretAccessKey = new String(hadoopConf.getPassword(\"fs.s3n.awsSecretAccessKey\")).trim\n+        logInfo(\"Using credentials from keys provided\")\n+        val basicAwsCredentialsProvider = new BasicAWSCredentialsProvider(\n+          accessKey, secretAccessKey)\n+        AmazonSQSClientBuilder\n+          .standard()\n+          .withClientConfiguration(new ClientConfiguration().withMaxConnections(maxConnections))\n+          .withCredentials(basicAwsCredentialsProvider)\n+          .withRegion(region)\n+          .build()\n+      } else {\n+        logInfo(\"Using the credentials attached to the instance\")\n+        val instanceProfileCredentialsProvider = new InstanceProfileCredentialsProviderWithRetries()\n+        AmazonSQSClientBuilder\n+          .standard()\n+          .withClientConfiguration(new ClientConfiguration().withMaxConnections(maxConnections))\n+          .withCredentials(instanceProfileCredentialsProvider)\n+          .build()\n+      }\n+    } catch {\n+      case e: Exception =>\n+        throw new SparkException(s\"Error occured while creating Amazon SQS Client ${e.getMessage}\")\n+    }\n+  }\n+\n+  def addToDeleteMessageQueue(messageReceiptHandles: List[String]): Unit = {\n+    deleteMessageQueue.addAll(messageReceiptHandles.asJava)\n+  }\n+\n+  def deleteMessagesFromQueue(): Unit = {\n+    try {\n+      var count = -1\n+      val messageReceiptHandles = deleteMessageQueue.asScala.toList\n+      val messageGroups = messageReceiptHandles.sliding(10, 10).toList\n+      messageGroups.foreach { messageGroup =>\n+        val requestEntries = messageGroup.foldLeft(List[DeleteMessageBatchRequestEntry]()) {\n+          (list, messageReceiptHandle) =>\n+            count = count + 1\n+            list :+ new DeleteMessageBatchRequestEntry(count.toString, messageReceiptHandle)\n+        }.asJava\n+        val batchResult = sqsClient.deleteMessageBatch(sqsUrl, requestEntries)\n+        if (!batchResult.getFailed.isEmpty) {\n+          batchResult.getFailed.asScala.foreach { entry =>\n+            sqsClient.deleteMessage(\n+              sqsUrl, requestEntries.get(entry.getId.toInt).getReceiptHandle)\n+          }\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logWarning(s\"Unable to delete message from SQS ${e.getMessage}\")",
    "line": 256
  }, {
    "author": {
      "login": "abhishekd0907"
    },
    "body": "will pick this change",
    "commit": "1f4628fb6ee306450652171e9789f94c648f89d6",
    "createdAt": "2019-07-31T07:07:50Z",
    "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.sqs\n+\n+import java.text.SimpleDateFormat\n+import java.util.TimeZone\n+import java.util.concurrent.TimeUnit\n+\n+import scala.collection.JavaConverters._\n+\n+import com.amazonaws.{AmazonClientException, AmazonServiceException, ClientConfiguration}\n+import com.amazonaws.services.sqs.{AmazonSQS, AmazonSQSClientBuilder}\n+import com.amazonaws.services.sqs.model.{DeleteMessageBatchRequestEntry, Message, ReceiveMessageRequest}\n+import org.apache.hadoop.conf.Configuration\n+import org.json4s.{DefaultFormats, MappingException}\n+import org.json4s.JsonAST.JValue\n+import org.json4s.jackson.JsonMethods.parse\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.util.ThreadUtils\n+\n+class SqsClient(sourceOptions: SqsSourceOptions,\n+                hadoopConf: Configuration) extends Logging {\n+\n+  private val sqsFetchIntervalSeconds = sourceOptions.fetchIntervalSeconds\n+  private val sqsLongPollWaitTimeSeconds = sourceOptions.longPollWaitTimeSeconds\n+  private val sqsMaxRetries = sourceOptions.maxRetries\n+  private val maxConnections = sourceOptions.maxConnections\n+  private val ignoreFileDeletion = sourceOptions.ignoreFileDeletion\n+  private val region = sourceOptions.region\n+  val sqsUrl = sourceOptions.sqsUrl\n+\n+  @volatile var exception: Option[Exception] = None\n+\n+  private val timestampFormat = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\") // ISO8601\n+  timestampFormat.setTimeZone(TimeZone.getTimeZone(\"UTC\"))\n+  private var retriesOnFailure = 0\n+  private val sqsClient = createSqsClient()\n+\n+  val sqsScheduler = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"sqs-scheduler\")\n+\n+  val sqsFileCache = new SqsFileCache(sourceOptions.maxFileAgeMs, sourceOptions.fileNameOnly)\n+\n+  val deleteMessageQueue = new java.util.concurrent.ConcurrentLinkedQueue[String]()\n+\n+  private val sqsFetchMessagesThread = new Runnable {\n+    override def run(): Unit = {\n+      try {\n+        // Fetching messages from Amazon SQS\n+        val newMessages = sqsFetchMessages()\n+\n+        // Filtering the new messages which are already not seen\n+        if (newMessages.nonEmpty) {\n+          newMessages.filter(message => sqsFileCache.isNewFile(message._1, message._2))\n+            .foreach(message =>\n+              sqsFileCache.add(message._1, MessageDescription(message._2, false, message._3)))\n+        }\n+      } catch {\n+        case e: Exception =>\n+          exception = Some(e)\n+      }\n+    }\n+  }\n+\n+  sqsScheduler.scheduleWithFixedDelay(\n+    sqsFetchMessagesThread,\n+    0,\n+    sqsFetchIntervalSeconds,\n+    TimeUnit.SECONDS)\n+\n+  private def sqsFetchMessages(): Seq[(String, Long, String)] = {\n+    val messageList = try {\n+      val receiveMessageRequest = new ReceiveMessageRequest()\n+        .withQueueUrl(sqsUrl)\n+        .withWaitTimeSeconds(sqsLongPollWaitTimeSeconds)\n+      val messages = sqsClient.receiveMessage(receiveMessageRequest).getMessages.asScala\n+      retriesOnFailure = 0\n+      logDebug(s\"successfully received ${messages.size} messages\")\n+      messages\n+    } catch {\n+      case ase: AmazonServiceException =>\n+        val message =\n+        \"\"\"\n+          |Caught an AmazonServiceException, which means your request made it to Amazon SQS,\n+          | rejected with an error response for some reason.\n+        \"\"\".stripMargin\n+        logWarning(message)\n+        logWarning(s\"Error Message: ${ase.getMessage}\")\n+        logWarning(s\"HTTP Status Code: ${ase.getStatusCode}, AWS Error Code: ${ase.getErrorCode}\")\n+        logWarning(s\"Error Type: ${ase.getErrorType}, Request ID: ${ase.getRequestId}\")\n+        evaluateRetries()\n+        List.empty\n+      case ace: AmazonClientException =>\n+        val message =\n+        \"\"\"\n+           |Caught an AmazonClientException, which means, the client encountered a serious\n+           | internal problem while trying to communicate with Amazon SQS, such as not\n+           |  being able to access the network.\n+        \"\"\".stripMargin\n+        logWarning(message)\n+        logWarning(s\"Error Message: ${ace.getMessage()}\")\n+        evaluateRetries()\n+        List.empty\n+      case e: Exception =>\n+        val message = \"Received unexpected error from SQS\"\n+        logWarning(message)\n+        logWarning(s\"Error Message: ${e.getMessage()}\")\n+        evaluateRetries()\n+        List.empty\n+    }\n+    if (messageList.nonEmpty) {\n+      parseSqsMessages(messageList)\n+    } else {\n+      Seq.empty\n+    }\n+  }\n+\n+  private def parseSqsMessages(messageList: Seq[Message]): Seq[(String, Long, String)] = {\n+    val errorMessages = scala.collection.mutable.ListBuffer[String]()\n+    val parsedMessages = messageList.foldLeft(Seq[(String, Long, String)]()) { (list, message) =>\n+      implicit val formats = DefaultFormats\n+      try {\n+        val messageReceiptHandle = message.getReceiptHandle\n+        val messageJson = parse(message.getBody).extract[JValue]\n+        val bucketName = (\n+          messageJson \\ \"Records\" \\ \"s3\" \\ \"bucket\" \\ \"name\").extract[Array[String]].head\n+        val eventName = (messageJson \\ \"Records\" \\ \"eventName\").extract[Array[String]].head\n+        if (eventName.contains(\"ObjectCreated\")) {\n+          val timestamp = (messageJson \\ \"Records\" \\ \"eventTime\").extract[Array[String]].head\n+          val timestampMills = convertTimestampToMills(timestamp)\n+          val path = \"s3://\" +\n+            bucketName + \"/\" +\n+            (messageJson \\ \"Records\" \\ \"s3\" \\ \"object\" \\ \"key\").extract[Array[String]].head\n+          logDebug(\"Successfully parsed sqs message\")\n+          list :+ ((path, timestampMills, messageReceiptHandle))\n+        } else {\n+          if (eventName.contains(\"ObjectRemoved\")) {\n+            if (!ignoreFileDeletion) {\n+              exception = Some(new SparkException(\"ObjectDelete message detected in SQS\"))\n+            } else {\n+              logInfo(\"Ignoring file deletion message since ignoreFileDeletion is true\")\n+            }\n+          } else {\n+            logWarning(\"Ignoring unexpected message detected in SQS\")\n+          }\n+          errorMessages.append(messageReceiptHandle)\n+          list\n+        }\n+      } catch {\n+        case me: MappingException =>\n+          errorMessages.append(message.getReceiptHandle)\n+          logWarning(s\"Error in parsing SQS message ${me.getMessage}\")\n+          list\n+        case e: Exception =>\n+          errorMessages.append(message.getReceiptHandle)\n+          logWarning(s\"Unexpected error while parsing SQS message ${e.getMessage}\")\n+          list\n+      }\n+    }\n+    if (errorMessages.nonEmpty) {\n+      addToDeleteMessageQueue(errorMessages.toList)\n+    }\n+    parsedMessages\n+  }\n+\n+  private def convertTimestampToMills(timestamp: String): Long = {\n+    val timeInMillis = timestampFormat.parse(timestamp).getTime()\n+    timeInMillis\n+  }\n+\n+  private def evaluateRetries(): Unit = {\n+    retriesOnFailure += 1\n+    if (retriesOnFailure >= sqsMaxRetries) {\n+      logError(\"Max retries reached\")\n+      exception = Some(new SparkException(\"Unable to receive Messages from SQS for \" +\n+        s\"${sqsMaxRetries} times Giving up. Check logs for details.\"))\n+    } else {\n+      logWarning(s\"Attempt ${retriesOnFailure}.\" +\n+        s\"Will reattempt after ${sqsFetchIntervalSeconds} seconds\")\n+    }\n+  }\n+\n+  private def createSqsClient(): AmazonSQS = {\n+    try {\n+      val isClusterOnEc2Role = hadoopConf.getBoolean(\n+        \"fs.s3.isClusterOnEc2Role\", false) || hadoopConf.getBoolean(\n+        \"fs.s3n.isClusterOnEc2Role\", false) || sourceOptions.useInstanceProfileCredentials\n+      if (!isClusterOnEc2Role) {\n+        val accessKey = hadoopConf.getTrimmed(\"fs.s3n.awsAccessKeyId\")\n+        val secretAccessKey = new String(hadoopConf.getPassword(\"fs.s3n.awsSecretAccessKey\")).trim\n+        logInfo(\"Using credentials from keys provided\")\n+        val basicAwsCredentialsProvider = new BasicAWSCredentialsProvider(\n+          accessKey, secretAccessKey)\n+        AmazonSQSClientBuilder\n+          .standard()\n+          .withClientConfiguration(new ClientConfiguration().withMaxConnections(maxConnections))\n+          .withCredentials(basicAwsCredentialsProvider)\n+          .withRegion(region)\n+          .build()\n+      } else {\n+        logInfo(\"Using the credentials attached to the instance\")\n+        val instanceProfileCredentialsProvider = new InstanceProfileCredentialsProviderWithRetries()\n+        AmazonSQSClientBuilder\n+          .standard()\n+          .withClientConfiguration(new ClientConfiguration().withMaxConnections(maxConnections))\n+          .withCredentials(instanceProfileCredentialsProvider)\n+          .build()\n+      }\n+    } catch {\n+      case e: Exception =>\n+        throw new SparkException(s\"Error occured while creating Amazon SQS Client ${e.getMessage}\")\n+    }\n+  }\n+\n+  def addToDeleteMessageQueue(messageReceiptHandles: List[String]): Unit = {\n+    deleteMessageQueue.addAll(messageReceiptHandles.asJava)\n+  }\n+\n+  def deleteMessagesFromQueue(): Unit = {\n+    try {\n+      var count = -1\n+      val messageReceiptHandles = deleteMessageQueue.asScala.toList\n+      val messageGroups = messageReceiptHandles.sliding(10, 10).toList\n+      messageGroups.foreach { messageGroup =>\n+        val requestEntries = messageGroup.foldLeft(List[DeleteMessageBatchRequestEntry]()) {\n+          (list, messageReceiptHandle) =>\n+            count = count + 1\n+            list :+ new DeleteMessageBatchRequestEntry(count.toString, messageReceiptHandle)\n+        }.asJava\n+        val batchResult = sqsClient.deleteMessageBatch(sqsUrl, requestEntries)\n+        if (!batchResult.getFailed.isEmpty) {\n+          batchResult.getFailed.asScala.foreach { entry =>\n+            sqsClient.deleteMessage(\n+              sqsUrl, requestEntries.get(entry.getId.toInt).getReceiptHandle)\n+          }\n+        }\n+      }\n+    } catch {\n+      case e: Exception =>\n+        logWarning(s\"Unable to delete message from SQS ${e.getMessage}\")",
    "line": 256
  }],
  "prId": 24934
}]