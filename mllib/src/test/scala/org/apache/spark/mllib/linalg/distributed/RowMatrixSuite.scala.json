[{
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Nit: use `===` to trigger the scalatest macro that will print a better error if this check fails.",
    "commit": "4f61d86f67314e7270ec9fdb9ba3b11537057df0",
    "createdAt": "2019-03-07T16:28:32Z",
    "diffHunk": "@@ -101,6 +101,14 @@ class RowMatrixSuite extends SparkFunSuite with MLlibTestSparkContext {\n     }\n   }\n \n+  test(\"getTreeAggregateIdealDepth\") {\n+    val rowMat = new RowMatrix(sc.emptyRDD[Vector])\n+    val driverResultSizeInMb = 10000\n+    val nbPartitions = 10000\n+    assert(rowMat.getTreeAggregateIdealDepth(99, nbPartitions, driverResultSizeInMb) == 2)"
  }, {
    "author": {
      "login": "gagafunctor"
    },
    "body": "nice, another trick I'll apply in my code from now on, thanks.",
    "commit": "4f61d86f67314e7270ec9fdb9ba3b11537057df0",
    "createdAt": "2019-03-08T13:20:38Z",
    "diffHunk": "@@ -101,6 +101,14 @@ class RowMatrixSuite extends SparkFunSuite with MLlibTestSparkContext {\n     }\n   }\n \n+  test(\"getTreeAggregateIdealDepth\") {\n+    val rowMat = new RowMatrix(sc.emptyRDD[Vector])\n+    val driverResultSizeInMb = 10000\n+    val nbPartitions = 10000\n+    assert(rowMat.getTreeAggregateIdealDepth(99, nbPartitions, driverResultSizeInMb) == 2)"
  }],
  "prId": 23983
}, {
  "comments": [{
    "author": {
      "login": "gagafunctor"
    },
    "body": "So about this line: I figured out that changing this will affect the sparkConf for tests that will be executed after this one aswell (I tried to set maxResultSize to 10bytes here and it broke several later tests).\r\nDo you know what's the disired way to make this only change the sparkConf for the closure of this test ? ",
    "commit": "4f61d86f67314e7270ec9fdb9ba3b11537057df0",
    "createdAt": "2019-03-08T16:37:42Z",
    "diffHunk": "@@ -101,6 +101,17 @@ class RowMatrixSuite extends SparkFunSuite with MLlibTestSparkContext {\n     }\n   }\n \n+  test(\"getTreeAggregateIdealDepth\") {\n+    val nbPartitions = 10000\n+    val vectors = sc.emptyRDD[Vector]\n+      .repartition(nbPartitions)\n+    vectors.conf.set(\"spark.driver.maxResultSize\", \"10g\")"
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "Do you have to change this from 2g for it to select a depth of 3? does a little larger number of partitions help? I thought you might have to reduce it if anything. There are ways to isolate the conf change to just the test, but if it's avoidable that's better",
    "commit": "4f61d86f67314e7270ec9fdb9ba3b11537057df0",
    "createdAt": "2019-03-08T18:53:27Z",
    "diffHunk": "@@ -101,6 +101,17 @@ class RowMatrixSuite extends SparkFunSuite with MLlibTestSparkContext {\n     }\n   }\n \n+  test(\"getTreeAggregateIdealDepth\") {\n+    val nbPartitions = 10000\n+    val vectors = sc.emptyRDD[Vector]\n+      .repartition(nbPartitions)\n+    vectors.conf.set(\"spark.driver.maxResultSize\", \"10g\")"
  }, {
    "author": {
      "login": "gagafunctor"
    },
    "body": "I checked the maxResultSize parameter value in the test and it's the default one (1gig, e.g 1024 Mb) -> I'll change the numPartitions and object size to:\r\n- numPartitions -> 100\r\n- objectSize -> 100 to get a desired depth of 2 (because sqrt(100) * 100 <= 1024)\r\n- objectSize -> 110 to get a desired depth of 3 (because sqrt(100) * 110 > 1024 and math.pow(100, 0.33) * 110 < 1024)\r\n\r\nSo the (semantically) same test doesn't require a change in the sharedContext spark conf ",
    "commit": "4f61d86f67314e7270ec9fdb9ba3b11537057df0",
    "createdAt": "2019-03-11T10:25:42Z",
    "diffHunk": "@@ -101,6 +101,17 @@ class RowMatrixSuite extends SparkFunSuite with MLlibTestSparkContext {\n     }\n   }\n \n+  test(\"getTreeAggregateIdealDepth\") {\n+    val nbPartitions = 10000\n+    val vectors = sc.emptyRDD[Vector]\n+      .repartition(nbPartitions)\n+    vectors.conf.set(\"spark.driver.maxResultSize\", \"10g\")"
  }],
  "prId": 23983
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "I think the style checker wants this blank line",
    "commit": "4f61d86f67314e7270ec9fdb9ba3b11537057df0",
    "createdAt": "2019-03-18T11:20:42Z",
    "diffHunk": "@@ -20,10 +20,8 @@ package org.apache.spark.mllib.linalg.distributed\n import java.util.Arrays\n \n import scala.util.Random\n-\n-import breeze.linalg.{norm => brzNorm, svd => brzSvd, DenseMatrix => BDM, DenseVector => BDV}\n+import breeze.linalg.{DenseMatrix => BDM, DenseVector => BDV, norm => brzNorm, svd => brzSvd}\n import breeze.numerics.abs\n-"
  }, {
    "author": {
      "login": "gagafunctor"
    },
    "body": "fixed",
    "commit": "4f61d86f67314e7270ec9fdb9ba3b11537057df0",
    "createdAt": "2019-03-18T18:04:19Z",
    "diffHunk": "@@ -20,10 +20,8 @@ package org.apache.spark.mllib.linalg.distributed\n import java.util.Arrays\n \n import scala.util.Random\n-\n-import breeze.linalg.{norm => brzNorm, svd => brzSvd, DenseMatrix => BDM, DenseVector => BDV}\n+import breeze.linalg.{DenseMatrix => BDM, DenseVector => BDV, norm => brzNorm, svd => brzSvd}\n import breeze.numerics.abs\n-"
  }],
  "prId": 23983
}]