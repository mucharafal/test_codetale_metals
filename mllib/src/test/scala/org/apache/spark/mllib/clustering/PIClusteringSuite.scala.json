[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Remove this method. You can run single test with `sbt/sbt \"mllib/testOnly *.PICSuite\"`.\n",
    "commit": "45508507ddb0f097224176f6688ecf740100caa2",
    "createdAt": "2015-01-28T22:03:30Z",
    "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import org.apache.log4j.Logger\n+import org.apache.spark.graphx._\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.scalatest.FunSuite\n+\n+import scala.util.Random\n+\n+class PIClusteringSuite extends FunSuite with LocalSparkContext {\n+\n+  val logger = Logger.getLogger(getClass.getName)\n+\n+  import org.apache.spark.mllib.clustering.PIClusteringSuite._\n+\n+  val PIC = PIClustering\n+  val A = Array\n+\n+  test(\"concentricCirclesTest\") {\n+    concentricCirclesTest()\n+  }\n+\n+\n+  def concentricCirclesTest() = {\n+    val sigma = 1.0\n+    val nIterations = 10\n+\n+    val circleSpecs = Seq(\n+      // Best results for 30 points\n+      CircleSpec(Point(0.0, 0.0), 0.03, 0.1, 3),\n+      CircleSpec(Point(0.0, 0.0), 0.3, 0.03, 12),\n+      CircleSpec(Point(0.0, 0.0), 1.0, 0.01, 15)\n+      // Add following to get 100 points\n+      , CircleSpec(Point(0.0, 0.0), 1.5, 0.005, 30),\n+      CircleSpec(Point(0.0, 0.0), 2.0, 0.002, 40)\n+    )\n+\n+    val nClusters = circleSpecs.size\n+    val cdata = createConcentricCirclesData(circleSpecs)\n+    withSpark { sc =>\n+      val vertices = new Random().shuffle(cdata.map { p =>\n+        (p.label, new BDV(Array(p.x, p.y)))\n+      })\n+\n+      val nVertices = vertices.length\n+      val (ccenters, estCollected) = PIC.run(sc, vertices, nClusters, nIterations)\n+      logger.info(s\"Cluster centers: ${ccenters.mkString(\",\")} \" +\n+        s\"\\nEstimates: ${estCollected.mkString(\"[\", \",\", \"]\")}\")\n+      assert(ccenters.size == circleSpecs.length, \"Did not get correct number of centers\")\n+\n+    }\n+  }\n+\n+}\n+\n+object PIClusteringSuite {\n+  val logger = Logger.getLogger(getClass.getName)\n+  val A = Array\n+\n+  def pdoub(d: Double) = f\"$d%1.6f\"\n+\n+  case class Point(label: Long, x: Double, y: Double) {\n+    def this(x: Double, y: Double) = this(-1L, x, y)\n+\n+    override def toString() = s\"($label, (${pdoub(x)},${pdoub(y)}))\"\n+  }\n+\n+  object Point {\n+    def apply(x: Double, y: Double) = new Point(-1L, x, y)\n+  }\n+\n+  case class CircleSpec(center: Point, radius: Double, noiseToRadiusRatio: Double,\n+                        nPoints: Int, uniformDistOnCircle: Boolean = true)\n+\n+  def createConcentricCirclesData(circleSpecs: Seq[CircleSpec]) = {\n+    import org.apache.spark.mllib.random.StandardNormalGenerator\n+    val normalGen = new StandardNormalGenerator\n+    var idStart = 0\n+    val circles = for (csp <- circleSpecs) yield {\n+      idStart += 1000\n+      val circlePoints = for (thetax <- 0 until csp.nPoints) yield {\n+        val theta = thetax * 2 * Math.PI / csp.nPoints\n+        val (x, y) = (csp.radius * Math.cos(theta)\n+          * (1 + normalGen.nextValue * csp.noiseToRadiusRatio),\n+          csp.radius * Math.sin(theta) * (1 + normalGen.nextValue * csp.noiseToRadiusRatio))\n+        (Point(idStart + thetax, x, y))\n+      }\n+      circlePoints\n+    }\n+    val points = circles.flatten.sortBy(_.label)\n+    logger.info(printPoints(points))\n+    points\n+  }\n+\n+  def printPoints(points: Seq[Point]) = {\n+    points.mkString(\"[\", \" , \", \"]\")\n+  }\n+\n+  def main(args: Array[String]) {"
  }, {
    "author": {
      "login": "javadba"
    },
    "body": "OK\n",
    "commit": "45508507ddb0f097224176f6688ecf740100caa2",
    "createdAt": "2015-01-28T22:14:30Z",
    "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import org.apache.log4j.Logger\n+import org.apache.spark.graphx._\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.scalatest.FunSuite\n+\n+import scala.util.Random\n+\n+class PIClusteringSuite extends FunSuite with LocalSparkContext {\n+\n+  val logger = Logger.getLogger(getClass.getName)\n+\n+  import org.apache.spark.mllib.clustering.PIClusteringSuite._\n+\n+  val PIC = PIClustering\n+  val A = Array\n+\n+  test(\"concentricCirclesTest\") {\n+    concentricCirclesTest()\n+  }\n+\n+\n+  def concentricCirclesTest() = {\n+    val sigma = 1.0\n+    val nIterations = 10\n+\n+    val circleSpecs = Seq(\n+      // Best results for 30 points\n+      CircleSpec(Point(0.0, 0.0), 0.03, 0.1, 3),\n+      CircleSpec(Point(0.0, 0.0), 0.3, 0.03, 12),\n+      CircleSpec(Point(0.0, 0.0), 1.0, 0.01, 15)\n+      // Add following to get 100 points\n+      , CircleSpec(Point(0.0, 0.0), 1.5, 0.005, 30),\n+      CircleSpec(Point(0.0, 0.0), 2.0, 0.002, 40)\n+    )\n+\n+    val nClusters = circleSpecs.size\n+    val cdata = createConcentricCirclesData(circleSpecs)\n+    withSpark { sc =>\n+      val vertices = new Random().shuffle(cdata.map { p =>\n+        (p.label, new BDV(Array(p.x, p.y)))\n+      })\n+\n+      val nVertices = vertices.length\n+      val (ccenters, estCollected) = PIC.run(sc, vertices, nClusters, nIterations)\n+      logger.info(s\"Cluster centers: ${ccenters.mkString(\",\")} \" +\n+        s\"\\nEstimates: ${estCollected.mkString(\"[\", \",\", \"]\")}\")\n+      assert(ccenters.size == circleSpecs.length, \"Did not get correct number of centers\")\n+\n+    }\n+  }\n+\n+}\n+\n+object PIClusteringSuite {\n+  val logger = Logger.getLogger(getClass.getName)\n+  val A = Array\n+\n+  def pdoub(d: Double) = f\"$d%1.6f\"\n+\n+  case class Point(label: Long, x: Double, y: Double) {\n+    def this(x: Double, y: Double) = this(-1L, x, y)\n+\n+    override def toString() = s\"($label, (${pdoub(x)},${pdoub(y)}))\"\n+  }\n+\n+  object Point {\n+    def apply(x: Double, y: Double) = new Point(-1L, x, y)\n+  }\n+\n+  case class CircleSpec(center: Point, radius: Double, noiseToRadiusRatio: Double,\n+                        nPoints: Int, uniformDistOnCircle: Boolean = true)\n+\n+  def createConcentricCirclesData(circleSpecs: Seq[CircleSpec]) = {\n+    import org.apache.spark.mllib.random.StandardNormalGenerator\n+    val normalGen = new StandardNormalGenerator\n+    var idStart = 0\n+    val circles = for (csp <- circleSpecs) yield {\n+      idStart += 1000\n+      val circlePoints = for (thetax <- 0 until csp.nPoints) yield {\n+        val theta = thetax * 2 * Math.PI / csp.nPoints\n+        val (x, y) = (csp.radius * Math.cos(theta)\n+          * (1 + normalGen.nextValue * csp.noiseToRadiusRatio),\n+          csp.radius * Math.sin(theta) * (1 + normalGen.nextValue * csp.noiseToRadiusRatio))\n+        (Point(idStart + thetax, x, y))\n+      }\n+      circlePoints\n+    }\n+    val points = circles.flatten.sortBy(_.label)\n+    logger.info(printPoints(points))\n+    points\n+  }\n+\n+  def printPoints(points: Seq[Point]) = {\n+    points.mkString(\"[\", \" , \", \"]\")\n+  }\n+\n+  def main(args: Array[String]) {"
  }],
  "prId": 4254
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Is it really necessary? What is the error message if we use `MLlibTestSparkContext`?\n",
    "commit": "45508507ddb0f097224176f6688ecf740100caa2",
    "createdAt": "2015-01-28T22:04:44Z",
    "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import org.apache.log4j.Logger\n+import org.apache.spark.graphx._\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.scalatest.FunSuite\n+\n+import scala.util.Random\n+\n+class PIClusteringSuite extends FunSuite with LocalSparkContext {\n+\n+  val logger = Logger.getLogger(getClass.getName)\n+\n+  import org.apache.spark.mllib.clustering.PIClusteringSuite._\n+\n+  val PIC = PIClustering\n+  val A = Array\n+\n+  test(\"concentricCirclesTest\") {\n+    concentricCirclesTest()\n+  }\n+\n+\n+  def concentricCirclesTest() = {\n+    val sigma = 1.0\n+    val nIterations = 10\n+\n+    val circleSpecs = Seq(\n+      // Best results for 30 points\n+      CircleSpec(Point(0.0, 0.0), 0.03, 0.1, 3),\n+      CircleSpec(Point(0.0, 0.0), 0.3, 0.03, 12),\n+      CircleSpec(Point(0.0, 0.0), 1.0, 0.01, 15)\n+      // Add following to get 100 points\n+      , CircleSpec(Point(0.0, 0.0), 1.5, 0.005, 30),\n+      CircleSpec(Point(0.0, 0.0), 2.0, 0.002, 40)\n+    )\n+\n+    val nClusters = circleSpecs.size\n+    val cdata = createConcentricCirclesData(circleSpecs)\n+    withSpark { sc =>\n+      val vertices = new Random().shuffle(cdata.map { p =>\n+        (p.label, new BDV(Array(p.x, p.y)))\n+      })\n+\n+      val nVertices = vertices.length\n+      val (ccenters, estCollected) = PIC.run(sc, vertices, nClusters, nIterations)\n+      logger.info(s\"Cluster centers: ${ccenters.mkString(\",\")} \" +\n+        s\"\\nEstimates: ${estCollected.mkString(\"[\", \",\", \"]\")}\")\n+      assert(ccenters.size == circleSpecs.length, \"Did not get correct number of centers\")\n+\n+    }\n+  }\n+\n+}\n+\n+object PIClusteringSuite {\n+  val logger = Logger.getLogger(getClass.getName)\n+  val A = Array\n+\n+  def pdoub(d: Double) = f\"$d%1.6f\"\n+\n+  case class Point(label: Long, x: Double, y: Double) {\n+    def this(x: Double, y: Double) = this(-1L, x, y)\n+\n+    override def toString() = s\"($label, (${pdoub(x)},${pdoub(y)}))\"\n+  }\n+\n+  object Point {\n+    def apply(x: Double, y: Double) = new Point(-1L, x, y)\n+  }\n+\n+  case class CircleSpec(center: Point, radius: Double, noiseToRadiusRatio: Double,\n+                        nPoints: Int, uniformDistOnCircle: Boolean = true)\n+\n+  def createConcentricCirclesData(circleSpecs: Seq[CircleSpec]) = {\n+    import org.apache.spark.mllib.random.StandardNormalGenerator\n+    val normalGen = new StandardNormalGenerator\n+    var idStart = 0\n+    val circles = for (csp <- circleSpecs) yield {\n+      idStart += 1000\n+      val circlePoints = for (thetax <- 0 until csp.nPoints) yield {\n+        val theta = thetax * 2 * Math.PI / csp.nPoints\n+        val (x, y) = (csp.radius * Math.cos(theta)\n+          * (1 + normalGen.nextValue * csp.noiseToRadiusRatio),\n+          csp.radius * Math.sin(theta) * (1 + normalGen.nextValue * csp.noiseToRadiusRatio))\n+        (Point(idStart + thetax, x, y))\n+      }\n+      circlePoints\n+    }\n+    val points = circles.flatten.sortBy(_.label)\n+    logger.info(printPoints(points))\n+    points\n+  }\n+\n+  def printPoints(points: Seq[Point]) = {\n+    points.mkString(\"[\", \" , \", \"]\")\n+  }\n+\n+  def main(args: Array[String]) {\n+    val pictest = new PIClusteringSuite\n+    pictest.concentricCirclesTest()\n+  }\n+}\n+\n+/**\n+ * Provides a method to run tests against a {@link SparkContext} variable that is correctly stopped\n+ * after each test.\n+ * TODO: import this from the graphx test cases package i.e. may need update to pom.xml\n+ */\n+trait LocalSparkContext {\n+  /** Runs `f` on a new SparkContext and ensures that it is stopped afterwards. */\n+  def withSpark[T](f: SparkContext => T) = {\n+    val conf = new SparkConf()\n+    GraphXUtils.registerKryoClasses(conf)"
  }, {
    "author": {
      "login": "javadba"
    },
    "body": "OK changed to use MLLibTestSparkContext\n",
    "commit": "45508507ddb0f097224176f6688ecf740100caa2",
    "createdAt": "2015-01-28T22:16:01Z",
    "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import org.apache.log4j.Logger\n+import org.apache.spark.graphx._\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.scalatest.FunSuite\n+\n+import scala.util.Random\n+\n+class PIClusteringSuite extends FunSuite with LocalSparkContext {\n+\n+  val logger = Logger.getLogger(getClass.getName)\n+\n+  import org.apache.spark.mllib.clustering.PIClusteringSuite._\n+\n+  val PIC = PIClustering\n+  val A = Array\n+\n+  test(\"concentricCirclesTest\") {\n+    concentricCirclesTest()\n+  }\n+\n+\n+  def concentricCirclesTest() = {\n+    val sigma = 1.0\n+    val nIterations = 10\n+\n+    val circleSpecs = Seq(\n+      // Best results for 30 points\n+      CircleSpec(Point(0.0, 0.0), 0.03, 0.1, 3),\n+      CircleSpec(Point(0.0, 0.0), 0.3, 0.03, 12),\n+      CircleSpec(Point(0.0, 0.0), 1.0, 0.01, 15)\n+      // Add following to get 100 points\n+      , CircleSpec(Point(0.0, 0.0), 1.5, 0.005, 30),\n+      CircleSpec(Point(0.0, 0.0), 2.0, 0.002, 40)\n+    )\n+\n+    val nClusters = circleSpecs.size\n+    val cdata = createConcentricCirclesData(circleSpecs)\n+    withSpark { sc =>\n+      val vertices = new Random().shuffle(cdata.map { p =>\n+        (p.label, new BDV(Array(p.x, p.y)))\n+      })\n+\n+      val nVertices = vertices.length\n+      val (ccenters, estCollected) = PIC.run(sc, vertices, nClusters, nIterations)\n+      logger.info(s\"Cluster centers: ${ccenters.mkString(\",\")} \" +\n+        s\"\\nEstimates: ${estCollected.mkString(\"[\", \",\", \"]\")}\")\n+      assert(ccenters.size == circleSpecs.length, \"Did not get correct number of centers\")\n+\n+    }\n+  }\n+\n+}\n+\n+object PIClusteringSuite {\n+  val logger = Logger.getLogger(getClass.getName)\n+  val A = Array\n+\n+  def pdoub(d: Double) = f\"$d%1.6f\"\n+\n+  case class Point(label: Long, x: Double, y: Double) {\n+    def this(x: Double, y: Double) = this(-1L, x, y)\n+\n+    override def toString() = s\"($label, (${pdoub(x)},${pdoub(y)}))\"\n+  }\n+\n+  object Point {\n+    def apply(x: Double, y: Double) = new Point(-1L, x, y)\n+  }\n+\n+  case class CircleSpec(center: Point, radius: Double, noiseToRadiusRatio: Double,\n+                        nPoints: Int, uniformDistOnCircle: Boolean = true)\n+\n+  def createConcentricCirclesData(circleSpecs: Seq[CircleSpec]) = {\n+    import org.apache.spark.mllib.random.StandardNormalGenerator\n+    val normalGen = new StandardNormalGenerator\n+    var idStart = 0\n+    val circles = for (csp <- circleSpecs) yield {\n+      idStart += 1000\n+      val circlePoints = for (thetax <- 0 until csp.nPoints) yield {\n+        val theta = thetax * 2 * Math.PI / csp.nPoints\n+        val (x, y) = (csp.radius * Math.cos(theta)\n+          * (1 + normalGen.nextValue * csp.noiseToRadiusRatio),\n+          csp.radius * Math.sin(theta) * (1 + normalGen.nextValue * csp.noiseToRadiusRatio))\n+        (Point(idStart + thetax, x, y))\n+      }\n+      circlePoints\n+    }\n+    val points = circles.flatten.sortBy(_.label)\n+    logger.info(printPoints(points))\n+    points\n+  }\n+\n+  def printPoints(points: Seq[Point]) = {\n+    points.mkString(\"[\", \" , \", \"]\")\n+  }\n+\n+  def main(args: Array[String]) {\n+    val pictest = new PIClusteringSuite\n+    pictest.concentricCirclesTest()\n+  }\n+}\n+\n+/**\n+ * Provides a method to run tests against a {@link SparkContext} variable that is correctly stopped\n+ * after each test.\n+ * TODO: import this from the graphx test cases package i.e. may need update to pom.xml\n+ */\n+trait LocalSparkContext {\n+  /** Runs `f` on a new SparkContext and ensures that it is stopped afterwards. */\n+  def withSpark[T](f: SparkContext => T) = {\n+    val conf = new SparkConf()\n+    GraphXUtils.registerKryoClasses(conf)"
  }],
  "prId": 4254
}]