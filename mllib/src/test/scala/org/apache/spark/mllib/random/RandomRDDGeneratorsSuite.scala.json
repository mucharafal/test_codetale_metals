[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Those comments are not JavaDoc. Could you move it inside the class closure and change the first line from `/**` to `/*`?\n",
    "commit": "01121ac8af3d7f6cd44160644d387b3747b925f4",
    "createdAt": "2014-07-25T07:55:03Z",
    "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.random\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.scalatest.FunSuite\n+\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.mllib.rdd.{RandomRDDPartition, RandomRDD}\n+import org.apache.spark.mllib.util.LocalSparkContext\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.StatCounter\n+\n+/**\n+ * Note: avoid including APIs that do not set the seed for the RNG in unit tests\n+ * in order to guarantee deterministic behavior.\n+ *\n+ * TODO update tests to use TestingUtils for floating point comparison after PR 1367 is merged\n+ */",
    "line": 36
  }],
  "prId": 1520
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`( p` -> `(p`\n",
    "commit": "01121ac8af3d7f6cd44160644d387b3747b925f4",
    "createdAt": "2014-07-25T07:55:08Z",
    "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.random\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.scalatest.FunSuite\n+\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.mllib.rdd.{RandomRDDPartition, RandomRDD}\n+import org.apache.spark.mllib.util.LocalSparkContext\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.StatCounter\n+\n+/**\n+ * Note: avoid including APIs that do not set the seed for the RNG in unit tests\n+ * in order to guarantee deterministic behavior.\n+ *\n+ * TODO update tests to use TestingUtils for floating point comparison after PR 1367 is merged\n+ */\n+class RandomRDDGeneratorsSuite extends FunSuite with LocalSparkContext with Serializable {\n+\n+  def testGeneratedRDD(rdd: RDD[Double],\n+      expectedSize: Long,\n+      expectedNumPartitions: Int,\n+      expectedMean: Double,\n+      expectedStddev: Double,\n+      epsilon: Double = 0.01) {\n+    val stats = rdd.stats()\n+    assert(expectedSize === stats.count)\n+    assert(expectedNumPartitions === rdd.partitions.size)\n+    assert(math.abs(stats.mean - expectedMean) < epsilon)\n+    assert(math.abs(stats.stdev - expectedStddev) < epsilon)\n+  }\n+\n+  // assume test RDDs are small\n+  def testGeneratedVectorRDD(rdd: RDD[Vector],\n+      expectedRows: Long,\n+      expectedColumns: Int,\n+      expectedNumPartitions: Int,\n+      expectedMean: Double,\n+      expectedStddev: Double,\n+      epsilon: Double = 0.01) {\n+    assert(expectedNumPartitions === rdd.partitions.size)\n+    val values = new ArrayBuffer[Double]()\n+    rdd.collect.foreach { vector => {\n+      assert(vector.size === expectedColumns)\n+      values ++= vector.toArray\n+    }}\n+    assert(expectedRows === values.size / expectedColumns)\n+    val stats = new StatCounter(values)\n+    assert(math.abs(stats.mean - expectedMean) < epsilon)\n+    assert(math.abs(stats.stdev - expectedStddev) < epsilon)\n+  }\n+\n+  test(\"RandomRDD sizes\") {\n+\n+    // some cases where size % numParts != 0 to test getPartitions behaves correctly\n+    for ((size, numPartitions) <- List((10000, 6), (12345, 1), (1000, 101))) {\n+      val rdd = new RandomRDD(sc, size, numPartitions, new UniformGenerator, 0L)\n+      assert(rdd.count() === size)\n+      assert(rdd.partitions.size === numPartitions)\n+\n+      // check that partition sizes are balanced\n+      val partSizes = rdd.partitions.map( p => p.asInstanceOf[RandomRDDPartition].size.toDouble)"
  }],
  "prId": 1520
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Why checking the stdev of partition sizes? It should be `maxPartitionSize - minPartitionSize <= 1`.\n",
    "commit": "01121ac8af3d7f6cd44160644d387b3747b925f4",
    "createdAt": "2014-07-25T07:55:40Z",
    "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.random\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.scalatest.FunSuite\n+\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.mllib.rdd.{RandomRDDPartition, RandomRDD}\n+import org.apache.spark.mllib.util.LocalSparkContext\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.StatCounter\n+\n+/**\n+ * Note: avoid including APIs that do not set the seed for the RNG in unit tests\n+ * in order to guarantee deterministic behavior.\n+ *\n+ * TODO update tests to use TestingUtils for floating point comparison after PR 1367 is merged\n+ */\n+class RandomRDDGeneratorsSuite extends FunSuite with LocalSparkContext with Serializable {\n+\n+  def testGeneratedRDD(rdd: RDD[Double],\n+      expectedSize: Long,\n+      expectedNumPartitions: Int,\n+      expectedMean: Double,\n+      expectedStddev: Double,\n+      epsilon: Double = 0.01) {\n+    val stats = rdd.stats()\n+    assert(expectedSize === stats.count)\n+    assert(expectedNumPartitions === rdd.partitions.size)\n+    assert(math.abs(stats.mean - expectedMean) < epsilon)\n+    assert(math.abs(stats.stdev - expectedStddev) < epsilon)\n+  }\n+\n+  // assume test RDDs are small\n+  def testGeneratedVectorRDD(rdd: RDD[Vector],\n+      expectedRows: Long,\n+      expectedColumns: Int,\n+      expectedNumPartitions: Int,\n+      expectedMean: Double,\n+      expectedStddev: Double,\n+      epsilon: Double = 0.01) {\n+    assert(expectedNumPartitions === rdd.partitions.size)\n+    val values = new ArrayBuffer[Double]()\n+    rdd.collect.foreach { vector => {\n+      assert(vector.size === expectedColumns)\n+      values ++= vector.toArray\n+    }}\n+    assert(expectedRows === values.size / expectedColumns)\n+    val stats = new StatCounter(values)\n+    assert(math.abs(stats.mean - expectedMean) < epsilon)\n+    assert(math.abs(stats.stdev - expectedStddev) < epsilon)\n+  }\n+\n+  test(\"RandomRDD sizes\") {\n+\n+    // some cases where size % numParts != 0 to test getPartitions behaves correctly\n+    for ((size, numPartitions) <- List((10000, 6), (12345, 1), (1000, 101))) {\n+      val rdd = new RandomRDD(sc, size, numPartitions, new UniformGenerator, 0L)\n+      assert(rdd.count() === size)\n+      assert(rdd.partitions.size === numPartitions)\n+\n+      // check that partition sizes are balanced\n+      val partSizes = rdd.partitions.map( p => p.asInstanceOf[RandomRDDPartition].size.toDouble)\n+      val partStats = new StatCounter(partSizes)\n+      assert(partStats.stdev < 1.0)"
  }],
  "prId": 1520
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Move `(count, part) =>` to the line above and insert a space between `){`.\n",
    "commit": "01121ac8af3d7f6cd44160644d387b3747b925f4",
    "createdAt": "2014-07-25T07:57:46Z",
    "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.random\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.scalatest.FunSuite\n+\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.mllib.rdd.{RandomRDDPartition, RandomRDD}\n+import org.apache.spark.mllib.util.LocalSparkContext\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.StatCounter\n+\n+/**\n+ * Note: avoid including APIs that do not set the seed for the RNG in unit tests\n+ * in order to guarantee deterministic behavior.\n+ *\n+ * TODO update tests to use TestingUtils for floating point comparison after PR 1367 is merged\n+ */\n+class RandomRDDGeneratorsSuite extends FunSuite with LocalSparkContext with Serializable {\n+\n+  def testGeneratedRDD(rdd: RDD[Double],\n+      expectedSize: Long,\n+      expectedNumPartitions: Int,\n+      expectedMean: Double,\n+      expectedStddev: Double,\n+      epsilon: Double = 0.01) {\n+    val stats = rdd.stats()\n+    assert(expectedSize === stats.count)\n+    assert(expectedNumPartitions === rdd.partitions.size)\n+    assert(math.abs(stats.mean - expectedMean) < epsilon)\n+    assert(math.abs(stats.stdev - expectedStddev) < epsilon)\n+  }\n+\n+  // assume test RDDs are small\n+  def testGeneratedVectorRDD(rdd: RDD[Vector],\n+      expectedRows: Long,\n+      expectedColumns: Int,\n+      expectedNumPartitions: Int,\n+      expectedMean: Double,\n+      expectedStddev: Double,\n+      epsilon: Double = 0.01) {\n+    assert(expectedNumPartitions === rdd.partitions.size)\n+    val values = new ArrayBuffer[Double]()\n+    rdd.collect.foreach { vector => {\n+      assert(vector.size === expectedColumns)\n+      values ++= vector.toArray\n+    }}\n+    assert(expectedRows === values.size / expectedColumns)\n+    val stats = new StatCounter(values)\n+    assert(math.abs(stats.mean - expectedMean) < epsilon)\n+    assert(math.abs(stats.stdev - expectedStddev) < epsilon)\n+  }\n+\n+  test(\"RandomRDD sizes\") {\n+\n+    // some cases where size % numParts != 0 to test getPartitions behaves correctly\n+    for ((size, numPartitions) <- List((10000, 6), (12345, 1), (1000, 101))) {\n+      val rdd = new RandomRDD(sc, size, numPartitions, new UniformGenerator, 0L)\n+      assert(rdd.count() === size)\n+      assert(rdd.partitions.size === numPartitions)\n+\n+      // check that partition sizes are balanced\n+      val partSizes = rdd.partitions.map( p => p.asInstanceOf[RandomRDDPartition].size.toDouble)\n+      val partStats = new StatCounter(partSizes)\n+      assert(partStats.stdev < 1.0)\n+    }\n+\n+    // size > Int.MaxValue\n+    val size = Int.MaxValue.toLong * 100L\n+    val numPartitions = 101\n+    val rdd = new RandomRDD(sc, size, numPartitions, new UniformGenerator, 0L)\n+    assert(rdd.partitions.size === numPartitions)\n+    val count = rdd.partitions.foldLeft(0L){\n+      (count, part) => count + part.asInstanceOf[RandomRDDPartition].size"
  }],
  "prId": 1520
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "You can use `Intercept[IllegalArgumentException]{ ... }`.\n",
    "commit": "01121ac8af3d7f6cd44160644d387b3747b925f4",
    "createdAt": "2014-07-25T07:58:23Z",
    "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.random\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.scalatest.FunSuite\n+\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.mllib.rdd.{RandomRDDPartition, RandomRDD}\n+import org.apache.spark.mllib.util.LocalSparkContext\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.StatCounter\n+\n+/**\n+ * Note: avoid including APIs that do not set the seed for the RNG in unit tests\n+ * in order to guarantee deterministic behavior.\n+ *\n+ * TODO update tests to use TestingUtils for floating point comparison after PR 1367 is merged\n+ */\n+class RandomRDDGeneratorsSuite extends FunSuite with LocalSparkContext with Serializable {\n+\n+  def testGeneratedRDD(rdd: RDD[Double],\n+      expectedSize: Long,\n+      expectedNumPartitions: Int,\n+      expectedMean: Double,\n+      expectedStddev: Double,\n+      epsilon: Double = 0.01) {\n+    val stats = rdd.stats()\n+    assert(expectedSize === stats.count)\n+    assert(expectedNumPartitions === rdd.partitions.size)\n+    assert(math.abs(stats.mean - expectedMean) < epsilon)\n+    assert(math.abs(stats.stdev - expectedStddev) < epsilon)\n+  }\n+\n+  // assume test RDDs are small\n+  def testGeneratedVectorRDD(rdd: RDD[Vector],\n+      expectedRows: Long,\n+      expectedColumns: Int,\n+      expectedNumPartitions: Int,\n+      expectedMean: Double,\n+      expectedStddev: Double,\n+      epsilon: Double = 0.01) {\n+    assert(expectedNumPartitions === rdd.partitions.size)\n+    val values = new ArrayBuffer[Double]()\n+    rdd.collect.foreach { vector => {\n+      assert(vector.size === expectedColumns)\n+      values ++= vector.toArray\n+    }}\n+    assert(expectedRows === values.size / expectedColumns)\n+    val stats = new StatCounter(values)\n+    assert(math.abs(stats.mean - expectedMean) < epsilon)\n+    assert(math.abs(stats.stdev - expectedStddev) < epsilon)\n+  }\n+\n+  test(\"RandomRDD sizes\") {\n+\n+    // some cases where size % numParts != 0 to test getPartitions behaves correctly\n+    for ((size, numPartitions) <- List((10000, 6), (12345, 1), (1000, 101))) {\n+      val rdd = new RandomRDD(sc, size, numPartitions, new UniformGenerator, 0L)\n+      assert(rdd.count() === size)\n+      assert(rdd.partitions.size === numPartitions)\n+\n+      // check that partition sizes are balanced\n+      val partSizes = rdd.partitions.map( p => p.asInstanceOf[RandomRDDPartition].size.toDouble)\n+      val partStats = new StatCounter(partSizes)\n+      assert(partStats.stdev < 1.0)\n+    }\n+\n+    // size > Int.MaxValue\n+    val size = Int.MaxValue.toLong * 100L\n+    val numPartitions = 101\n+    val rdd = new RandomRDD(sc, size, numPartitions, new UniformGenerator, 0L)\n+    assert(rdd.partitions.size === numPartitions)\n+    val count = rdd.partitions.foldLeft(0L){\n+      (count, part) => count + part.asInstanceOf[RandomRDDPartition].size\n+    }\n+    assert(count === size)\n+\n+    // size needs to be positive\n+    try {\n+      new RandomRDD(sc, 0, 10, new UniformGenerator, 0L)"
  }],
  "prId": 1520
}]