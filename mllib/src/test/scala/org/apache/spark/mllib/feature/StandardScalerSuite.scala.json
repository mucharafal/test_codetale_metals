[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "ditto\n",
    "commit": "78c15d3eb9a6003180a08cb53688455ff82d4463",
    "createdAt": "2014-08-04T03:58:30Z",
    "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.feature\n+\n+import org.scalatest.FunSuite\n+\n+import org.apache.spark.mllib.linalg.{DenseVector, SparseVector, Vector, Vectors}\n+import org.apache.spark.mllib.util.LocalSparkContext\n+import org.apache.spark.mllib.util.TestingUtils._\n+import org.apache.spark.mllib.rdd.RDDFunctions._\n+import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, MultivariateOnlineSummarizer}\n+import org.apache.spark.rdd.RDD\n+\n+class StandardScalerSuite extends FunSuite with LocalSparkContext {\n+\n+  private def computeSummary(data: RDD[Vector]): MultivariateStatisticalSummary = {\n+    data.treeAggregate(new MultivariateOnlineSummarizer)(\n+      (aggregator, data) => aggregator.add(data),\n+      (aggregator1, aggregator2) => aggregator1.merge(aggregator2))\n+  }\n+\n+  test(\"Standardization with dense input\") {\n+    val data = Array(\n+      Vectors.dense(-2.0, 2.3, 0),\n+      Vectors.dense(0.0, -1.0, -3.0),\n+      Vectors.dense(0.0, -5.1, 0.0),\n+      Vectors.dense(3.8, 0.0, 1.9),\n+      Vectors.dense(1.7, -0.6, 0.0),\n+      Vectors.dense(0.0, 1.9, 0.0)\n+    )\n+\n+    val dataRDD = sc.parallelize(data, 3)\n+\n+    val standardizer1 = new StandardScaler(withMean = true, withStd = true)\n+    val standardizer2 = new StandardScaler()\n+    val standardizer3 = new StandardScaler(withMean = true, withStd = false)\n+\n+    withClue(\"Using a standardizer before fitting the model should throw exception.\") {\n+      intercept[IllegalStateException] {\n+        data.map(standardizer1.transform)\n+      }\n+    }\n+\n+    standardizer1.fit(dataRDD)\n+    standardizer2.fit(dataRDD)\n+    standardizer3.fit(dataRDD)\n+\n+    val data1 = data.map(standardizer1.transform)\n+    val data2 = data.map(standardizer2.transform)\n+    val data3 = data.map(standardizer3.transform)\n+\n+    val data1RDD = standardizer1.transform(dataRDD)\n+    val data2RDD = standardizer2.transform(dataRDD)\n+    val data3RDD = standardizer3.transform(dataRDD)\n+\n+    val summary = computeSummary(dataRDD)\n+    val summary1 = computeSummary(data1RDD)\n+    val summary2 = computeSummary(data2RDD)\n+    val summary3 = computeSummary(data3RDD)\n+\n+    assert((data, data1, data1RDD.collect()).zipped.forall(\n+        (v1, v2, v3) => (v1, v2, v3) match {"
  }],
  "prId": 1207
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "ditto\n",
    "commit": "78c15d3eb9a6003180a08cb53688455ff82d4463",
    "createdAt": "2014-08-04T03:58:35Z",
    "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.feature\n+\n+import org.scalatest.FunSuite\n+\n+import org.apache.spark.mllib.linalg.{DenseVector, SparseVector, Vector, Vectors}\n+import org.apache.spark.mllib.util.LocalSparkContext\n+import org.apache.spark.mllib.util.TestingUtils._\n+import org.apache.spark.mllib.rdd.RDDFunctions._\n+import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, MultivariateOnlineSummarizer}\n+import org.apache.spark.rdd.RDD\n+\n+class StandardScalerSuite extends FunSuite with LocalSparkContext {\n+\n+  private def computeSummary(data: RDD[Vector]): MultivariateStatisticalSummary = {\n+    data.treeAggregate(new MultivariateOnlineSummarizer)(\n+      (aggregator, data) => aggregator.add(data),\n+      (aggregator1, aggregator2) => aggregator1.merge(aggregator2))\n+  }\n+\n+  test(\"Standardization with dense input\") {\n+    val data = Array(\n+      Vectors.dense(-2.0, 2.3, 0),\n+      Vectors.dense(0.0, -1.0, -3.0),\n+      Vectors.dense(0.0, -5.1, 0.0),\n+      Vectors.dense(3.8, 0.0, 1.9),\n+      Vectors.dense(1.7, -0.6, 0.0),\n+      Vectors.dense(0.0, 1.9, 0.0)\n+    )\n+\n+    val dataRDD = sc.parallelize(data, 3)\n+\n+    val standardizer1 = new StandardScaler(withMean = true, withStd = true)\n+    val standardizer2 = new StandardScaler()\n+    val standardizer3 = new StandardScaler(withMean = true, withStd = false)\n+\n+    withClue(\"Using a standardizer before fitting the model should throw exception.\") {\n+      intercept[IllegalStateException] {\n+        data.map(standardizer1.transform)\n+      }\n+    }\n+\n+    standardizer1.fit(dataRDD)\n+    standardizer2.fit(dataRDD)\n+    standardizer3.fit(dataRDD)\n+\n+    val data1 = data.map(standardizer1.transform)\n+    val data2 = data.map(standardizer2.transform)\n+    val data3 = data.map(standardizer3.transform)\n+\n+    val data1RDD = standardizer1.transform(dataRDD)\n+    val data2RDD = standardizer2.transform(dataRDD)\n+    val data3RDD = standardizer3.transform(dataRDD)\n+\n+    val summary = computeSummary(dataRDD)\n+    val summary1 = computeSummary(data1RDD)\n+    val summary2 = computeSummary(data2RDD)\n+    val summary3 = computeSummary(data3RDD)\n+\n+    assert((data, data1, data1RDD.collect()).zipped.forall(\n+        (v1, v2, v3) => (v1, v2, v3) match {\n+          case (v1: DenseVector, v2: DenseVector, v3: DenseVector) => true\n+          case (v1: SparseVector, v2: SparseVector, v3: SparseVector) => true\n+          case _ => false\n+        }\n+      ), \"The vector type should be preserved after standardization.\")\n+\n+    assert((data, data2, data2RDD.collect()).zipped.forall(\n+        (v1, v2, v3) => (v1, v2, v3) match {"
  }],
  "prId": 1207
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "ditto\n",
    "commit": "78c15d3eb9a6003180a08cb53688455ff82d4463",
    "createdAt": "2014-08-04T03:58:40Z",
    "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.feature\n+\n+import org.scalatest.FunSuite\n+\n+import org.apache.spark.mllib.linalg.{DenseVector, SparseVector, Vector, Vectors}\n+import org.apache.spark.mllib.util.LocalSparkContext\n+import org.apache.spark.mllib.util.TestingUtils._\n+import org.apache.spark.mllib.rdd.RDDFunctions._\n+import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, MultivariateOnlineSummarizer}\n+import org.apache.spark.rdd.RDD\n+\n+class StandardScalerSuite extends FunSuite with LocalSparkContext {\n+\n+  private def computeSummary(data: RDD[Vector]): MultivariateStatisticalSummary = {\n+    data.treeAggregate(new MultivariateOnlineSummarizer)(\n+      (aggregator, data) => aggregator.add(data),\n+      (aggregator1, aggregator2) => aggregator1.merge(aggregator2))\n+  }\n+\n+  test(\"Standardization with dense input\") {\n+    val data = Array(\n+      Vectors.dense(-2.0, 2.3, 0),\n+      Vectors.dense(0.0, -1.0, -3.0),\n+      Vectors.dense(0.0, -5.1, 0.0),\n+      Vectors.dense(3.8, 0.0, 1.9),\n+      Vectors.dense(1.7, -0.6, 0.0),\n+      Vectors.dense(0.0, 1.9, 0.0)\n+    )\n+\n+    val dataRDD = sc.parallelize(data, 3)\n+\n+    val standardizer1 = new StandardScaler(withMean = true, withStd = true)\n+    val standardizer2 = new StandardScaler()\n+    val standardizer3 = new StandardScaler(withMean = true, withStd = false)\n+\n+    withClue(\"Using a standardizer before fitting the model should throw exception.\") {\n+      intercept[IllegalStateException] {\n+        data.map(standardizer1.transform)\n+      }\n+    }\n+\n+    standardizer1.fit(dataRDD)\n+    standardizer2.fit(dataRDD)\n+    standardizer3.fit(dataRDD)\n+\n+    val data1 = data.map(standardizer1.transform)\n+    val data2 = data.map(standardizer2.transform)\n+    val data3 = data.map(standardizer3.transform)\n+\n+    val data1RDD = standardizer1.transform(dataRDD)\n+    val data2RDD = standardizer2.transform(dataRDD)\n+    val data3RDD = standardizer3.transform(dataRDD)\n+\n+    val summary = computeSummary(dataRDD)\n+    val summary1 = computeSummary(data1RDD)\n+    val summary2 = computeSummary(data2RDD)\n+    val summary3 = computeSummary(data3RDD)\n+\n+    assert((data, data1, data1RDD.collect()).zipped.forall(\n+        (v1, v2, v3) => (v1, v2, v3) match {\n+          case (v1: DenseVector, v2: DenseVector, v3: DenseVector) => true\n+          case (v1: SparseVector, v2: SparseVector, v3: SparseVector) => true\n+          case _ => false\n+        }\n+      ), \"The vector type should be preserved after standardization.\")\n+\n+    assert((data, data2, data2RDD.collect()).zipped.forall(\n+        (v1, v2, v3) => (v1, v2, v3) match {\n+          case (v1: DenseVector, v2: DenseVector, v3: DenseVector) => true\n+          case (v1: SparseVector, v2: SparseVector, v3: SparseVector) => true\n+          case _ => false\n+        }\n+      ), \"The vector type should be preserved after standardization.\")\n+\n+    assert((data, data3, data3RDD.collect()).zipped.forall(\n+        (v1, v2, v3) => (v1, v2, v3) match {"
  }],
  "prId": 1207
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "minor: `collect()` has been called more than once. Shall we cache the result of `collect()`?\n",
    "commit": "78c15d3eb9a6003180a08cb53688455ff82d4463",
    "createdAt": "2014-08-04T03:59:07Z",
    "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.feature\n+\n+import org.scalatest.FunSuite\n+\n+import org.apache.spark.mllib.linalg.{DenseVector, SparseVector, Vector, Vectors}\n+import org.apache.spark.mllib.util.LocalSparkContext\n+import org.apache.spark.mllib.util.TestingUtils._\n+import org.apache.spark.mllib.rdd.RDDFunctions._\n+import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, MultivariateOnlineSummarizer}\n+import org.apache.spark.rdd.RDD\n+\n+class StandardScalerSuite extends FunSuite with LocalSparkContext {\n+\n+  private def computeSummary(data: RDD[Vector]): MultivariateStatisticalSummary = {\n+    data.treeAggregate(new MultivariateOnlineSummarizer)(\n+      (aggregator, data) => aggregator.add(data),\n+      (aggregator1, aggregator2) => aggregator1.merge(aggregator2))\n+  }\n+\n+  test(\"Standardization with dense input\") {\n+    val data = Array(\n+      Vectors.dense(-2.0, 2.3, 0),\n+      Vectors.dense(0.0, -1.0, -3.0),\n+      Vectors.dense(0.0, -5.1, 0.0),\n+      Vectors.dense(3.8, 0.0, 1.9),\n+      Vectors.dense(1.7, -0.6, 0.0),\n+      Vectors.dense(0.0, 1.9, 0.0)\n+    )\n+\n+    val dataRDD = sc.parallelize(data, 3)\n+\n+    val standardizer1 = new StandardScaler(withMean = true, withStd = true)\n+    val standardizer2 = new StandardScaler()\n+    val standardizer3 = new StandardScaler(withMean = true, withStd = false)\n+\n+    withClue(\"Using a standardizer before fitting the model should throw exception.\") {\n+      intercept[IllegalStateException] {\n+        data.map(standardizer1.transform)\n+      }\n+    }\n+\n+    standardizer1.fit(dataRDD)\n+    standardizer2.fit(dataRDD)\n+    standardizer3.fit(dataRDD)\n+\n+    val data1 = data.map(standardizer1.transform)\n+    val data2 = data.map(standardizer2.transform)\n+    val data3 = data.map(standardizer3.transform)\n+\n+    val data1RDD = standardizer1.transform(dataRDD)\n+    val data2RDD = standardizer2.transform(dataRDD)\n+    val data3RDD = standardizer3.transform(dataRDD)\n+\n+    val summary = computeSummary(dataRDD)\n+    val summary1 = computeSummary(data1RDD)\n+    val summary2 = computeSummary(data2RDD)\n+    val summary3 = computeSummary(data3RDD)\n+\n+    assert((data, data1, data1RDD.collect()).zipped.forall(\n+        (v1, v2, v3) => (v1, v2, v3) match {\n+          case (v1: DenseVector, v2: DenseVector, v3: DenseVector) => true\n+          case (v1: SparseVector, v2: SparseVector, v3: SparseVector) => true\n+          case _ => false\n+        }\n+      ), \"The vector type should be preserved after standardization.\")\n+\n+    assert((data, data2, data2RDD.collect()).zipped.forall(\n+        (v1, v2, v3) => (v1, v2, v3) match {\n+          case (v1: DenseVector, v2: DenseVector, v3: DenseVector) => true\n+          case (v1: SparseVector, v2: SparseVector, v3: SparseVector) => true\n+          case _ => false\n+        }\n+      ), \"The vector type should be preserved after standardization.\")\n+\n+    assert((data, data3, data3RDD.collect()).zipped.forall(\n+        (v1, v2, v3) => (v1, v2, v3) match {\n+          case (v1: DenseVector, v2: DenseVector, v3: DenseVector) => true\n+          case (v1: SparseVector, v2: SparseVector, v3: SparseVector) => true\n+          case _ => false\n+        }\n+      ), \"The vector type should be preserved after standardization.\")\n+\n+    assert((data1, data1RDD.collect()).zipped.forall((v1, v2) => v1 ~== v2 absTol 1E-5))"
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "For each RDD, I just call twice of collect(). I don't want to add another variable for this. (ps, RDD version is used for computing the summary stats, so we need both.)\n",
    "commit": "78c15d3eb9a6003180a08cb53688455ff82d4463",
    "createdAt": "2014-08-04T04:23:00Z",
    "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.feature\n+\n+import org.scalatest.FunSuite\n+\n+import org.apache.spark.mllib.linalg.{DenseVector, SparseVector, Vector, Vectors}\n+import org.apache.spark.mllib.util.LocalSparkContext\n+import org.apache.spark.mllib.util.TestingUtils._\n+import org.apache.spark.mllib.rdd.RDDFunctions._\n+import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, MultivariateOnlineSummarizer}\n+import org.apache.spark.rdd.RDD\n+\n+class StandardScalerSuite extends FunSuite with LocalSparkContext {\n+\n+  private def computeSummary(data: RDD[Vector]): MultivariateStatisticalSummary = {\n+    data.treeAggregate(new MultivariateOnlineSummarizer)(\n+      (aggregator, data) => aggregator.add(data),\n+      (aggregator1, aggregator2) => aggregator1.merge(aggregator2))\n+  }\n+\n+  test(\"Standardization with dense input\") {\n+    val data = Array(\n+      Vectors.dense(-2.0, 2.3, 0),\n+      Vectors.dense(0.0, -1.0, -3.0),\n+      Vectors.dense(0.0, -5.1, 0.0),\n+      Vectors.dense(3.8, 0.0, 1.9),\n+      Vectors.dense(1.7, -0.6, 0.0),\n+      Vectors.dense(0.0, 1.9, 0.0)\n+    )\n+\n+    val dataRDD = sc.parallelize(data, 3)\n+\n+    val standardizer1 = new StandardScaler(withMean = true, withStd = true)\n+    val standardizer2 = new StandardScaler()\n+    val standardizer3 = new StandardScaler(withMean = true, withStd = false)\n+\n+    withClue(\"Using a standardizer before fitting the model should throw exception.\") {\n+      intercept[IllegalStateException] {\n+        data.map(standardizer1.transform)\n+      }\n+    }\n+\n+    standardizer1.fit(dataRDD)\n+    standardizer2.fit(dataRDD)\n+    standardizer3.fit(dataRDD)\n+\n+    val data1 = data.map(standardizer1.transform)\n+    val data2 = data.map(standardizer2.transform)\n+    val data3 = data.map(standardizer3.transform)\n+\n+    val data1RDD = standardizer1.transform(dataRDD)\n+    val data2RDD = standardizer2.transform(dataRDD)\n+    val data3RDD = standardizer3.transform(dataRDD)\n+\n+    val summary = computeSummary(dataRDD)\n+    val summary1 = computeSummary(data1RDD)\n+    val summary2 = computeSummary(data2RDD)\n+    val summary3 = computeSummary(data3RDD)\n+\n+    assert((data, data1, data1RDD.collect()).zipped.forall(\n+        (v1, v2, v3) => (v1, v2, v3) match {\n+          case (v1: DenseVector, v2: DenseVector, v3: DenseVector) => true\n+          case (v1: SparseVector, v2: SparseVector, v3: SparseVector) => true\n+          case _ => false\n+        }\n+      ), \"The vector type should be preserved after standardization.\")\n+\n+    assert((data, data2, data2RDD.collect()).zipped.forall(\n+        (v1, v2, v3) => (v1, v2, v3) match {\n+          case (v1: DenseVector, v2: DenseVector, v3: DenseVector) => true\n+          case (v1: SparseVector, v2: SparseVector, v3: SparseVector) => true\n+          case _ => false\n+        }\n+      ), \"The vector type should be preserved after standardization.\")\n+\n+    assert((data, data3, data3RDD.collect()).zipped.forall(\n+        (v1, v2, v3) => (v1, v2, v3) match {\n+          case (v1: DenseVector, v2: DenseVector, v3: DenseVector) => true\n+          case (v1: SparseVector, v2: SparseVector, v3: SparseVector) => true\n+          case _ => false\n+        }\n+      ), \"The vector type should be preserved after standardization.\")\n+\n+    assert((data1, data1RDD.collect()).zipped.forall((v1, v2) => v1 ~== v2 absTol 1E-5))"
  }],
  "prId": 1207
}]