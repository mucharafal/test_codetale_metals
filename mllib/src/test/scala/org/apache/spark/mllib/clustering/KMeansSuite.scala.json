[{
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "while we're here, can we make `var model = ...` into `val model = ...` and delete L74 `val center = ...` which is not used.\n",
    "commit": "f870fe9aba24bb982380ede19153f79557e98d18",
    "createdAt": "2016-10-20T00:07:45Z",
    "diffHunk": "@@ -75,7 +75,7 @@ class KMeansSuite extends SparkFunSuite with MLlibTestSparkContext {\n \n     // Make sure code runs."
  }],
  "prId": 15450
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "these tests don't cover `initRandom`. We should add both initialization methods to be complete\n",
    "commit": "f870fe9aba24bb982380ede19153f79557e98d18",
    "createdAt": "2016-10-20T00:14:32Z",
    "diffHunk": "@@ -75,7 +75,7 @@ class KMeansSuite extends SparkFunSuite with MLlibTestSparkContext {\n \n     // Make sure code runs.\n     var model = KMeans.train(data, k = 2, maxIterations = 1)\n-    assert(model.clusterCenters.size === 2)\n+    assert(model.clusterCenters.size === 1)"
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "I will add more tests, yes.\n",
    "commit": "f870fe9aba24bb982380ede19153f79557e98d18",
    "createdAt": "2016-10-20T09:45:11Z",
    "diffHunk": "@@ -75,7 +75,7 @@ class KMeansSuite extends SparkFunSuite with MLlibTestSparkContext {\n \n     // Make sure code runs.\n     var model = KMeans.train(data, k = 2, maxIterations = 1)\n-    assert(model.clusterCenters.size === 2)\n+    assert(model.clusterCenters.size === 1)"
  }],
  "prId": 15450
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "make `seed` a val and use it instead of literal 42.\n",
    "commit": "f870fe9aba24bb982380ede19153f79557e98d18",
    "createdAt": "2016-10-24T15:59:02Z",
    "diffHunk": "@@ -85,9 +101,50 @@ class KMeansSuite extends SparkFunSuite with MLlibTestSparkContext {\n         Vectors.dense(1.0, 3.0, 4.0)),\n       2)\n \n-    // Make sure code runs.\n-    var model = KMeans.train(data, k = 3, maxIterations = 1)\n-    assert(model.clusterCenters.size === 3)\n+    var model = KMeans.train(data, k = 3, maxIterations = 1, initializationMode = \"random\")\n+    assert(model.clusterCenters.length === 2)\n+\n+    model = KMeans.train(data, k = 3, maxIterations = 1, initializationMode = \"k-means||\")\n+    assert(model.clusterCenters.length === 2)\n+  }\n+\n+  test(\"unique cluster centers\") {\n+    val rng = new scala.util.Random(42)"
  }],
  "prId": 15450
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "maybe use 5 or less for max iterations. \n",
    "commit": "f870fe9aba24bb982380ede19153f79557e98d18",
    "createdAt": "2016-10-24T16:01:52Z",
    "diffHunk": "@@ -85,9 +101,50 @@ class KMeansSuite extends SparkFunSuite with MLlibTestSparkContext {\n         Vectors.dense(1.0, 3.0, 4.0)),\n       2)\n \n-    // Make sure code runs.\n-    var model = KMeans.train(data, k = 3, maxIterations = 1)\n-    assert(model.clusterCenters.size === 3)\n+    var model = KMeans.train(data, k = 3, maxIterations = 1, initializationMode = \"random\")\n+    assert(model.clusterCenters.length === 2)\n+\n+    model = KMeans.train(data, k = 3, maxIterations = 1, initializationMode = \"k-means||\")\n+    assert(model.clusterCenters.length === 2)\n+  }\n+\n+  test(\"unique cluster centers\") {\n+    val rng = new scala.util.Random(42)\n+    val points = (0 until 10).map(i => Vectors.dense(Array.fill(3)(rng.nextDouble)))\n+    val data = sc.parallelize(\n+      points.flatMap { point =>\n+        Array.fill(rng.nextInt(4))(point)\n+      }, 2\n+    )\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    val zippedData = data.zip(norms).map { case (v, norm) =>\n+      new VectorWithNorm(v, norm)\n+    }\n+    // less centers than k\n+    val km = new KMeans().setK(50)\n+      .setMaxIterations(10)"
  }],
  "prId": 15450
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "Just make this `val normedData = data.map(new VectorWithNorm(_))`. The zipping was done for performance (not necessary here) and I just copied the code originally.\n",
    "commit": "f870fe9aba24bb982380ede19153f79557e98d18",
    "createdAt": "2016-10-24T19:45:16Z",
    "diffHunk": "@@ -85,9 +101,50 @@ class KMeansSuite extends SparkFunSuite with MLlibTestSparkContext {\n         Vectors.dense(1.0, 3.0, 4.0)),\n       2)\n \n-    // Make sure code runs.\n-    var model = KMeans.train(data, k = 3, maxIterations = 1)\n-    assert(model.clusterCenters.size === 3)\n+    var model = KMeans.train(data, k = 3, maxIterations = 1, initializationMode = \"random\")\n+    assert(model.clusterCenters.length === 2)\n+\n+    model = KMeans.train(data, k = 3, maxIterations = 1, initializationMode = \"k-means||\")\n+    assert(model.clusterCenters.length === 2)\n+  }\n+\n+  test(\"unique cluster centers\") {\n+    val rng = new scala.util.Random(42)\n+    val points = (0 until 10).map(i => Vectors.dense(Array.fill(3)(rng.nextDouble)))\n+    val data = sc.parallelize(\n+      points.flatMap { point =>\n+        Array.fill(rng.nextInt(4))(point)\n+      }, 2\n+    )\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    val zippedData = data.zip(norms).map { case (v, norm) =>"
  }],
  "prId": 15450
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "I'd prefer to remove these two tests since we've added a more thorough test below. We can check the \"random\" init method in that test as well, then we can eliminate these two.\n",
    "commit": "f870fe9aba24bb982380ede19153f79557e98d18",
    "createdAt": "2016-10-24T20:27:45Z",
    "diffHunk": "@@ -64,18 +64,34 @@ class KMeansSuite extends SparkFunSuite with MLlibTestSparkContext {\n     assert(model.clusterCenters.head ~== center absTol 1E-5)\n   }\n \n-  test(\"no distinct points\") {\n+  test(\"fewer distinct points than clusters\") {\n     val data = sc.parallelize(\n       Array(\n         Vectors.dense(1.0, 2.0, 3.0),\n         Vectors.dense(1.0, 2.0, 3.0),\n         Vectors.dense(1.0, 2.0, 3.0)),\n       2)\n-    val center = Vectors.dense(1.0, 2.0, 3.0)\n \n-    // Make sure code runs.\n-    var model = KMeans.train(data, k = 2, maxIterations = 1)\n-    assert(model.clusterCenters.size === 2)\n+    var model = KMeans.train(data, k = 2, maxIterations = 1, initializationMode = \"random\")\n+    assert(model.clusterCenters.length === 1)\n+\n+    model = KMeans.train(data, k = 2, maxIterations = 1, initializationMode = \"k-means||\")\n+    assert(model.clusterCenters.length === 1)\n+  }\n+\n+\n+  test(\"fewer clusters than points\") {"
  }],
  "prId": 15450
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "also check `initialCenters.length <= numDistinctPoints` where `numDistinctPoints` is 10 (defined above) at the moment. \n",
    "commit": "f870fe9aba24bb982380ede19153f79557e98d18",
    "createdAt": "2016-10-24T20:28:53Z",
    "diffHunk": "@@ -85,9 +101,50 @@ class KMeansSuite extends SparkFunSuite with MLlibTestSparkContext {\n         Vectors.dense(1.0, 3.0, 4.0)),\n       2)\n \n-    // Make sure code runs.\n-    var model = KMeans.train(data, k = 3, maxIterations = 1)\n-    assert(model.clusterCenters.size === 3)\n+    var model = KMeans.train(data, k = 3, maxIterations = 1, initializationMode = \"random\")\n+    assert(model.clusterCenters.length === 2)\n+\n+    model = KMeans.train(data, k = 3, maxIterations = 1, initializationMode = \"k-means||\")\n+    assert(model.clusterCenters.length === 2)\n+  }\n+\n+  test(\"unique cluster centers\") {\n+    val rng = new scala.util.Random(42)\n+    val points = (0 until 10).map(i => Vectors.dense(Array.fill(3)(rng.nextDouble)))\n+    val data = sc.parallelize(\n+      points.flatMap { point =>\n+        Array.fill(rng.nextInt(4))(point)\n+      }, 2\n+    )\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    val zippedData = data.zip(norms).map { case (v, norm) =>\n+      new VectorWithNorm(v, norm)\n+    }\n+    // less centers than k\n+    val km = new KMeans().setK(50)\n+      .setMaxIterations(10)\n+      .setInitializationMode(\"k-means||\")\n+      .setInitializationSteps(10)\n+      .setSeed(42)\n+    val initialCenters = km.initKMeansParallel(zippedData).map(_.vector)\n+    assert(initialCenters.length === initialCenters.distinct.length)"
  }],
  "prId": 15450
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "also check `initialCenters2.length === k`\n",
    "commit": "f870fe9aba24bb982380ede19153f79557e98d18",
    "createdAt": "2016-10-24T20:29:15Z",
    "diffHunk": "@@ -85,9 +101,50 @@ class KMeansSuite extends SparkFunSuite with MLlibTestSparkContext {\n         Vectors.dense(1.0, 3.0, 4.0)),\n       2)\n \n-    // Make sure code runs.\n-    var model = KMeans.train(data, k = 3, maxIterations = 1)\n-    assert(model.clusterCenters.size === 3)\n+    var model = KMeans.train(data, k = 3, maxIterations = 1, initializationMode = \"random\")\n+    assert(model.clusterCenters.length === 2)\n+\n+    model = KMeans.train(data, k = 3, maxIterations = 1, initializationMode = \"k-means||\")\n+    assert(model.clusterCenters.length === 2)\n+  }\n+\n+  test(\"unique cluster centers\") {\n+    val rng = new scala.util.Random(42)\n+    val points = (0 until 10).map(i => Vectors.dense(Array.fill(3)(rng.nextDouble)))\n+    val data = sc.parallelize(\n+      points.flatMap { point =>\n+        Array.fill(rng.nextInt(4))(point)\n+      }, 2\n+    )\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    val zippedData = data.zip(norms).map { case (v, norm) =>\n+      new VectorWithNorm(v, norm)\n+    }\n+    // less centers than k\n+    val km = new KMeans().setK(50)\n+      .setMaxIterations(10)\n+      .setInitializationMode(\"k-means||\")\n+      .setInitializationSteps(10)\n+      .setSeed(42)\n+    val initialCenters = km.initKMeansParallel(zippedData).map(_.vector)\n+    assert(initialCenters.length === initialCenters.distinct.length)\n+\n+    val model = km.run(data)\n+    val finalCenters = model.clusterCenters\n+    assert(finalCenters.length === finalCenters.distinct.length)\n+\n+    // run local k-means\n+    val km2 = new KMeans().setK(10)\n+      .setMaxIterations(10)\n+      .setInitializationMode(\"k-means||\")\n+      .setInitializationSteps(10)\n+      .setSeed(42)\n+    val initialCenters2 = km2.initKMeansParallel(zippedData).map(_.vector)\n+    assert(initialCenters2.length === initialCenters2.distinct.length)"
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "This condition failed, though it should be OK. The problem was that the data setup maps each of 10 distinct points to 0-3 copies, meaning that there may be less than 10 distinct points in the end. I just make that 1-3 copies and it works.\n\nFixed the doc problem too, thanks.\n",
    "commit": "f870fe9aba24bb982380ede19153f79557e98d18",
    "createdAt": "2016-10-25T13:21:54Z",
    "diffHunk": "@@ -85,9 +101,50 @@ class KMeansSuite extends SparkFunSuite with MLlibTestSparkContext {\n         Vectors.dense(1.0, 3.0, 4.0)),\n       2)\n \n-    // Make sure code runs.\n-    var model = KMeans.train(data, k = 3, maxIterations = 1)\n-    assert(model.clusterCenters.size === 3)\n+    var model = KMeans.train(data, k = 3, maxIterations = 1, initializationMode = \"random\")\n+    assert(model.clusterCenters.length === 2)\n+\n+    model = KMeans.train(data, k = 3, maxIterations = 1, initializationMode = \"k-means||\")\n+    assert(model.clusterCenters.length === 2)\n+  }\n+\n+  test(\"unique cluster centers\") {\n+    val rng = new scala.util.Random(42)\n+    val points = (0 until 10).map(i => Vectors.dense(Array.fill(3)(rng.nextDouble)))\n+    val data = sc.parallelize(\n+      points.flatMap { point =>\n+        Array.fill(rng.nextInt(4))(point)\n+      }, 2\n+    )\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    val zippedData = data.zip(norms).map { case (v, norm) =>\n+      new VectorWithNorm(v, norm)\n+    }\n+    // less centers than k\n+    val km = new KMeans().setK(50)\n+      .setMaxIterations(10)\n+      .setInitializationMode(\"k-means||\")\n+      .setInitializationSteps(10)\n+      .setSeed(42)\n+    val initialCenters = km.initKMeansParallel(zippedData).map(_.vector)\n+    assert(initialCenters.length === initialCenters.distinct.length)\n+\n+    val model = km.run(data)\n+    val finalCenters = model.clusterCenters\n+    assert(finalCenters.length === finalCenters.distinct.length)\n+\n+    // run local k-means\n+    val km2 = new KMeans().setK(10)\n+      .setMaxIterations(10)\n+      .setInitializationMode(\"k-means||\")\n+      .setInitializationSteps(10)\n+      .setSeed(42)\n+    val initialCenters2 = km2.initKMeansParallel(zippedData).map(_.vector)\n+    assert(initialCenters2.length === initialCenters2.distinct.length)"
  }],
  "prId": 15450
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "should be `val model2 = km2.run(data)`\n",
    "commit": "f870fe9aba24bb982380ede19153f79557e98d18",
    "createdAt": "2016-10-24T20:29:34Z",
    "diffHunk": "@@ -85,9 +101,50 @@ class KMeansSuite extends SparkFunSuite with MLlibTestSparkContext {\n         Vectors.dense(1.0, 3.0, 4.0)),\n       2)\n \n-    // Make sure code runs.\n-    var model = KMeans.train(data, k = 3, maxIterations = 1)\n-    assert(model.clusterCenters.size === 3)\n+    var model = KMeans.train(data, k = 3, maxIterations = 1, initializationMode = \"random\")\n+    assert(model.clusterCenters.length === 2)\n+\n+    model = KMeans.train(data, k = 3, maxIterations = 1, initializationMode = \"k-means||\")\n+    assert(model.clusterCenters.length === 2)\n+  }\n+\n+  test(\"unique cluster centers\") {\n+    val rng = new scala.util.Random(42)\n+    val points = (0 until 10).map(i => Vectors.dense(Array.fill(3)(rng.nextDouble)))\n+    val data = sc.parallelize(\n+      points.flatMap { point =>\n+        Array.fill(rng.nextInt(4))(point)\n+      }, 2\n+    )\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    val zippedData = data.zip(norms).map { case (v, norm) =>\n+      new VectorWithNorm(v, norm)\n+    }\n+    // less centers than k\n+    val km = new KMeans().setK(50)\n+      .setMaxIterations(10)\n+      .setInitializationMode(\"k-means||\")\n+      .setInitializationSteps(10)\n+      .setSeed(42)\n+    val initialCenters = km.initKMeansParallel(zippedData).map(_.vector)\n+    assert(initialCenters.length === initialCenters.distinct.length)\n+\n+    val model = km.run(data)\n+    val finalCenters = model.clusterCenters\n+    assert(finalCenters.length === finalCenters.distinct.length)\n+\n+    // run local k-means\n+    val km2 = new KMeans().setK(10)\n+      .setMaxIterations(10)\n+      .setInitializationMode(\"k-means||\")\n+      .setInitializationSteps(10)\n+      .setSeed(42)\n+    val initialCenters2 = km2.initKMeansParallel(zippedData).map(_.vector)\n+    assert(initialCenters2.length === initialCenters2.distinct.length)\n+\n+    val model2 = km.run(data)"
  }],
  "prId": 15450
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "minor/nit: maybe make `k` a val here and use that instead. Since we use 10 for something else above, this can be obfuscated in the future.\n",
    "commit": "f870fe9aba24bb982380ede19153f79557e98d18",
    "createdAt": "2016-10-25T15:00:05Z",
    "diffHunk": "@@ -64,30 +66,55 @@ class KMeansSuite extends SparkFunSuite with MLlibTestSparkContext {\n     assert(model.clusterCenters.head ~== center absTol 1E-5)\n   }\n \n-  test(\"no distinct points\") {\n+  test(\"fewer distinct points than clusters\") {\n     val data = sc.parallelize(\n       Array(\n         Vectors.dense(1.0, 2.0, 3.0),\n         Vectors.dense(1.0, 2.0, 3.0),\n         Vectors.dense(1.0, 2.0, 3.0)),\n       2)\n-    val center = Vectors.dense(1.0, 2.0, 3.0)\n \n-    // Make sure code runs.\n-    var model = KMeans.train(data, k = 2, maxIterations = 1)\n-    assert(model.clusterCenters.size === 2)\n-  }\n+    var model = KMeans.train(data, k = 2, maxIterations = 1, initializationMode = \"random\")\n+    assert(model.clusterCenters.length === 1)\n \n-  test(\"more clusters than points\") {\n-    val data = sc.parallelize(\n-      Array(\n-        Vectors.dense(1.0, 2.0, 3.0),\n-        Vectors.dense(1.0, 3.0, 4.0)),\n-      2)\n+    model = KMeans.train(data, k = 2, maxIterations = 1, initializationMode = \"k-means||\")\n+    assert(model.clusterCenters.length === 1)\n+  }\n \n-    // Make sure code runs.\n-    var model = KMeans.train(data, k = 3, maxIterations = 1)\n-    assert(model.clusterCenters.size === 3)\n+    test(\"unique cluster centers\") {\n+    val rng = new Random(seed)\n+    val numDistinctPoints = 10\n+    val points = (0 until numDistinctPoints).map(i => Vectors.dense(Array.fill(3)(rng.nextDouble)))\n+    val data = sc.parallelize(points.flatMap(Array.fill(1 + rng.nextInt(3))(_)), 2)\n+    val normedData = data.map(new VectorWithNorm(_))\n+\n+    // less centers than k\n+    val km = new KMeans().setK(50)\n+      .setMaxIterations(5)\n+      .setInitializationMode(\"k-means||\")\n+      .setInitializationSteps(10)\n+      .setSeed(seed)\n+    val initialCenters = km.initKMeansParallel(normedData).map(_.vector)\n+    assert(initialCenters.length === initialCenters.distinct.length)\n+    assert(initialCenters.length <= numDistinctPoints)\n+\n+    val model = km.run(data)\n+    val finalCenters = model.clusterCenters\n+    assert(finalCenters.length === finalCenters.distinct.length)\n+\n+    // run local k-means\n+    val km2 = new KMeans().setK(10)\n+      .setMaxIterations(5)\n+      .setInitializationMode(\"k-means||\")\n+      .setInitializationSteps(10)\n+      .setSeed(seed)\n+    val initialCenters2 = km2.initKMeansParallel(normedData).map(_.vector)\n+    assert(initialCenters2.length === initialCenters2.distinct.length)\n+    assert(initialCenters2.length === 10)"
  }],
  "prId": 15450
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "nit: indentation\n",
    "commit": "f870fe9aba24bb982380ede19153f79557e98d18",
    "createdAt": "2016-10-25T15:04:01Z",
    "diffHunk": "@@ -64,30 +66,55 @@ class KMeansSuite extends SparkFunSuite with MLlibTestSparkContext {\n     assert(model.clusterCenters.head ~== center absTol 1E-5)\n   }\n \n-  test(\"no distinct points\") {\n+  test(\"fewer distinct points than clusters\") {\n     val data = sc.parallelize(\n       Array(\n         Vectors.dense(1.0, 2.0, 3.0),\n         Vectors.dense(1.0, 2.0, 3.0),\n         Vectors.dense(1.0, 2.0, 3.0)),\n       2)\n-    val center = Vectors.dense(1.0, 2.0, 3.0)\n \n-    // Make sure code runs.\n-    var model = KMeans.train(data, k = 2, maxIterations = 1)\n-    assert(model.clusterCenters.size === 2)\n-  }\n+    var model = KMeans.train(data, k = 2, maxIterations = 1, initializationMode = \"random\")\n+    assert(model.clusterCenters.length === 1)\n \n-  test(\"more clusters than points\") {\n-    val data = sc.parallelize(\n-      Array(\n-        Vectors.dense(1.0, 2.0, 3.0),\n-        Vectors.dense(1.0, 3.0, 4.0)),\n-      2)\n+    model = KMeans.train(data, k = 2, maxIterations = 1, initializationMode = \"k-means||\")\n+    assert(model.clusterCenters.length === 1)\n+  }\n \n-    // Make sure code runs.\n-    var model = KMeans.train(data, k = 3, maxIterations = 1)\n-    assert(model.clusterCenters.size === 3)\n+    test(\"unique cluster centers\") {"
  }],
  "prId": 15450
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "Can we also test the \"random\" method here? I was going to suggest putting the test cases inside `Seq(\"k-means||\", \"random\").foreach { initMode =>`, but we run the specific parallel case. Maybe just manually add it?\n",
    "commit": "f870fe9aba24bb982380ede19153f79557e98d18",
    "createdAt": "2016-10-25T15:04:46Z",
    "diffHunk": "@@ -64,30 +66,55 @@ class KMeansSuite extends SparkFunSuite with MLlibTestSparkContext {\n     assert(model.clusterCenters.head ~== center absTol 1E-5)\n   }\n \n-  test(\"no distinct points\") {\n+  test(\"fewer distinct points than clusters\") {\n     val data = sc.parallelize(\n       Array(\n         Vectors.dense(1.0, 2.0, 3.0),\n         Vectors.dense(1.0, 2.0, 3.0),\n         Vectors.dense(1.0, 2.0, 3.0)),\n       2)\n-    val center = Vectors.dense(1.0, 2.0, 3.0)\n \n-    // Make sure code runs.\n-    var model = KMeans.train(data, k = 2, maxIterations = 1)\n-    assert(model.clusterCenters.size === 2)\n-  }\n+    var model = KMeans.train(data, k = 2, maxIterations = 1, initializationMode = \"random\")\n+    assert(model.clusterCenters.length === 1)\n \n-  test(\"more clusters than points\") {\n-    val data = sc.parallelize(\n-      Array(\n-        Vectors.dense(1.0, 2.0, 3.0),\n-        Vectors.dense(1.0, 3.0, 4.0)),\n-      2)\n+    model = KMeans.train(data, k = 2, maxIterations = 1, initializationMode = \"k-means||\")\n+    assert(model.clusterCenters.length === 1)\n+  }\n \n-    // Make sure code runs.\n-    var model = KMeans.train(data, k = 3, maxIterations = 1)\n-    assert(model.clusterCenters.size === 3)\n+    test(\"unique cluster centers\") {\n+    val rng = new Random(seed)\n+    val numDistinctPoints = 10\n+    val points = (0 until numDistinctPoints).map(i => Vectors.dense(Array.fill(3)(rng.nextDouble)))\n+    val data = sc.parallelize(points.flatMap(Array.fill(1 + rng.nextInt(3))(_)), 2)\n+    val normedData = data.map(new VectorWithNorm(_))\n+\n+    // less centers than k",
    "line": 75
  }],
  "prId": 15450
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "Let's fix the other comments that reference runs. e.g. \"No matter how many iterations or runs we use ....\"\n",
    "commit": "f870fe9aba24bb982380ede19153f79557e98d18",
    "createdAt": "2016-10-25T17:32:41Z",
    "diffHunk": "@@ -257,11 +246,6 @@ class KMeansSuite extends SparkFunSuite with MLlibTestSparkContext {\n     model = KMeans.train(rdd, k = 5, maxIterations = 10)\n     assert(model.clusterCenters.sortBy(VectorWithCompare(_))\n       .zip(points.sortBy(VectorWithCompare(_))).forall(x => x._1 ~== (x._2) absTol 1E-5))\n-\n-    // Neither should more runs",
    "line": 194
  }],
  "prId": 15450
}]