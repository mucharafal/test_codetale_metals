[{
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "A 50% tolerance is quite large, can the tests still regularly pass with a smaller tolerance?\n",
    "commit": "0072400825c0c4a871467a92799d12752fe96402",
    "createdAt": "2015-08-25T01:34:02Z",
    "diffHunk": "@@ -184,4 +184,72 @@ class StreamingLogisticRegressionSuite extends SparkFunSuite with TestSuiteBase\n     )\n     val output: Seq[Seq[(Double, Double)]] = runStreams(ssc, numBatches, numBatches)\n   }\n+\n+  test(\"parameter accuracy with full memory (decayFactor = 1)\") {\n+\n+    val nPoints = 100\n+\n+    // create model\n+    val model = new StreamingLogisticRegressionWithSGD()\n+      .setDecayFactor(1)\n+      .setInitialWeights(Vectors.dense(0.0))\n+      .setStepSize(0.5)\n+      .setNumIterations(50)\n+\n+    // generate sequence of simulated data\n+    val numBatches = 20\n+    // the first few RDD's are generated under the model A\n+    val inputA = (0 until (numBatches - 1)).map { i =>\n+      LogisticRegressionSuite.generateLogisticInput(0.0, 0.5, nPoints, 42 * (i + 1))\n+    }\n+    // the last RDD is generated under the model B\n+    val inputB =\n+      LogisticRegressionSuite.generateLogisticInput(0.0, 1.5, nPoints, 42 * (numBatches + 1))\n+    val input = inputA :+ inputB\n+\n+    // apply model training to input stream\n+    ssc = setupStreams(input, (inputDStream: DStream[LabeledPoint]) => {\n+      model.trainOn(inputDStream)\n+      inputDStream.count()\n+    })\n+    runStreams(ssc, numBatches, numBatches)\n+\n+    // with full memory, the final parameter estimates should be close to model A\n+    assert(model.latestModel().weights(0) ~== 0.5 relTol 0.5)"
  }, {
    "author": {
      "login": "rotationsymmetry"
    },
    "body": "In the test \"parameter accuracy with full memory (decayFactor = 1)\", all RDD effectively contribute equally to the final estimate. Since the last RDD is generated with a different model than the previous RDD, the final estimate has more variability than usual.\n\nIn the test \"parameter accuracy with no memory (decayFactor = 0)\", effectively all the previous RDD are ignored and the algorithm only has the samples from the last RDD to work with. Therefore the sample size provided to the algorithm is small and the estimate could fluctuate quite a bit. \n\nIn summary, for a smaller tolerance, we basically need to reduce the variability of the final estimate. I have increased the number of records in each RDD in the stream along with small tweak to the parameters. Now we can pass these tests consistently with 10% tolerance. Will include these update in my next push to the PR.\n",
    "commit": "0072400825c0c4a871467a92799d12752fe96402",
    "createdAt": "2015-08-26T16:51:37Z",
    "diffHunk": "@@ -184,4 +184,72 @@ class StreamingLogisticRegressionSuite extends SparkFunSuite with TestSuiteBase\n     )\n     val output: Seq[Seq[(Double, Double)]] = runStreams(ssc, numBatches, numBatches)\n   }\n+\n+  test(\"parameter accuracy with full memory (decayFactor = 1)\") {\n+\n+    val nPoints = 100\n+\n+    // create model\n+    val model = new StreamingLogisticRegressionWithSGD()\n+      .setDecayFactor(1)\n+      .setInitialWeights(Vectors.dense(0.0))\n+      .setStepSize(0.5)\n+      .setNumIterations(50)\n+\n+    // generate sequence of simulated data\n+    val numBatches = 20\n+    // the first few RDD's are generated under the model A\n+    val inputA = (0 until (numBatches - 1)).map { i =>\n+      LogisticRegressionSuite.generateLogisticInput(0.0, 0.5, nPoints, 42 * (i + 1))\n+    }\n+    // the last RDD is generated under the model B\n+    val inputB =\n+      LogisticRegressionSuite.generateLogisticInput(0.0, 1.5, nPoints, 42 * (numBatches + 1))\n+    val input = inputA :+ inputB\n+\n+    // apply model training to input stream\n+    ssc = setupStreams(input, (inputDStream: DStream[LabeledPoint]) => {\n+      model.trainOn(inputDStream)\n+      inputDStream.count()\n+    })\n+    runStreams(ssc, numBatches, numBatches)\n+\n+    // with full memory, the final parameter estimates should be close to model A\n+    assert(model.latestModel().weights(0) ~== 0.5 relTol 0.5)"
  }, {
    "author": {
      "login": "feynmanliang"
    },
    "body": "Awesome, thanks for investigating! I just saw decayFactor=1 and I assumed that the weights would be ~ (1/numBatches = 1 / 20 = 0.05) off from the true value\n",
    "commit": "0072400825c0c4a871467a92799d12752fe96402",
    "createdAt": "2015-08-27T02:34:33Z",
    "diffHunk": "@@ -184,4 +184,72 @@ class StreamingLogisticRegressionSuite extends SparkFunSuite with TestSuiteBase\n     )\n     val output: Seq[Seq[(Double, Double)]] = runStreams(ssc, numBatches, numBatches)\n   }\n+\n+  test(\"parameter accuracy with full memory (decayFactor = 1)\") {\n+\n+    val nPoints = 100\n+\n+    // create model\n+    val model = new StreamingLogisticRegressionWithSGD()\n+      .setDecayFactor(1)\n+      .setInitialWeights(Vectors.dense(0.0))\n+      .setStepSize(0.5)\n+      .setNumIterations(50)\n+\n+    // generate sequence of simulated data\n+    val numBatches = 20\n+    // the first few RDD's are generated under the model A\n+    val inputA = (0 until (numBatches - 1)).map { i =>\n+      LogisticRegressionSuite.generateLogisticInput(0.0, 0.5, nPoints, 42 * (i + 1))\n+    }\n+    // the last RDD is generated under the model B\n+    val inputB =\n+      LogisticRegressionSuite.generateLogisticInput(0.0, 1.5, nPoints, 42 * (numBatches + 1))\n+    val input = inputA :+ inputB\n+\n+    // apply model training to input stream\n+    ssc = setupStreams(input, (inputDStream: DStream[LabeledPoint]) => {\n+      model.trainOn(inputDStream)\n+      inputDStream.count()\n+    })\n+    runStreams(ssc, numBatches, numBatches)\n+\n+    // with full memory, the final parameter estimates should be close to model A\n+    assert(model.latestModel().weights(0) ~== 0.5 relTol 0.5)"
  }],
  "prId": 8022
}]