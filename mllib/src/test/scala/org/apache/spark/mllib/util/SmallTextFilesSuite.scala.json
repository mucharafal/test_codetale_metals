[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "It is strange to see this function in the test. Does MiniDFSCluster read Hadoop configurations like `dfs.umaskmode`?\n",
    "commit": "4ed60d16b5d7c760c06dd0d95ee558eee5cfd398",
    "createdAt": "2014-03-20T08:52:04Z",
    "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.util\n+\n+\n+import java.io.{InputStreamReader, BufferedReader, DataOutputStream, FileOutputStream}\n+import java.nio.file.Files\n+import java.nio.file.{Path => JPath}\n+import java.nio.file.{Paths => JPaths}\n+\n+import org.scalatest.BeforeAndAfterAll\n+import org.scalatest.FunSuite\n+\n+import org.apache.hadoop.hdfs.MiniDFSCluster\n+import org.apache.hadoop.fs.FileSystem\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.io.Text\n+\n+import org.apache.spark.mllib.util.MLUtils._\n+import org.apache.spark.SparkContext\n+\n+/**\n+ * Tests HDFS IO and local disk IO of [[smallTextFiles]] in MLutils. HDFS tests create a mock DFS in\n+ * memory, while local disk test create a temp directory. All these temporal storages are deleted\n+ * in the end.\n+ */\n+\n+class SmallTextFilesSuite extends FunSuite with BeforeAndAfterAll {\n+  private var sc: SparkContext = _\n+  private var dfs: MiniDFSCluster = _\n+\n+  override def beforeAll() {\n+    sc = new SparkContext(\"local\", \"test\")\n+    sc.hadoopConfiguration.set(\"dfs.datanode.data.dir.perm\", SmallTextFilesSuite.dirPermission())\n+    dfs = new MiniDFSCluster(sc.hadoopConfiguration, 4, true,\n+                             Array(\"/rack0\", \"/rack0\", \"/rack1\", \"/rack1\"),\n+                             Array(\"host0\", \"host1\", \"host2\", \"host3\"))\n+  }\n+\n+  override def afterAll() {\n+    if (dfs != null) dfs.shutdown()\n+    sc.stop()\n+    System.clearProperty(\"spark.driver.port\")\n+  }\n+\n+\n+  private def createHDFSFile(\n+      fs: FileSystem,\n+      inputDir: Path,\n+      fileName: String,\n+      contents: Array[Byte]) = {\n+    val out: DataOutputStream = fs.create(new Path(inputDir, fileName), true, 4096, 2, 512, null)\n+    out.write(contents, 0, contents.length)\n+    out.close()\n+    System.out.println(\"Wrote HDFS file\")\n+  }\n+\n+  /**\n+   * This code will test the behaviors on HDFS. There are three aspects to test:\n+   *    1) is all files are read.\n+   *    2) is the fileNames are read correctly.\n+   *    3) is the contents must be the same.\n+   */\n+  test(\"Small file input || HDFS IO\") {\n+    val fs: FileSystem = dfs.getFileSystem\n+    val dir = \"/foo/\"\n+    val inputDir: Path = new Path(dir)\n+\n+    SmallTextFilesSuite.fileNames.zip(SmallTextFilesSuite.filesContents).foreach {\n+      case (fname, contents) =>\n+        createHDFSFile(fs, inputDir, fname, contents)\n+    }\n+\n+    println(s\"name node is ${dfs.getNameNode.getNameNodeAddress.getHostName}\")\n+    println(s\"name node port is ${dfs.getNameNodePort}\")\n+\n+    val hdfsAddressDir =\n+      s\"hdfs://${dfs.getNameNode.getNameNodeAddress.getHostName}:${dfs.getNameNodePort}$dir\"\n+    println(s\"HDFS address dir is $hdfsAddressDir\")\n+\n+    val res = smallTextFiles(sc, hdfsAddressDir).collect()\n+\n+    assert(res.size === SmallTextFilesSuite.fileNames.size,\n+      \"Number of files read out do not fit with the actual value\")\n+\n+    for ((fname, contents) <- res) {\n+      assert(SmallTextFilesSuite.fileNames.contains(fname),\n+        s\"Missing file name $fname.\")\n+      assert(contents.hashCode === SmallTextFilesSuite.hashCodeOfContents(fname),\n+        s\"file $fname contents can not match\")\n+    }\n+  }\n+\n+  private def createNativeFile(inputDir: JPath, fileName: String, contents: Array[Byte]) = {\n+    val out = new DataOutputStream(new FileOutputStream(s\"${inputDir.toString}/$fileName\"))\n+    out.write(contents, 0, contents.length)\n+    out.close()\n+    println(\"Wrote native file\")\n+  }\n+\n+  /**\n+   * This code will test the behaviors on native file system. There are three aspects:\n+   *    1) is all files are read.\n+   *    2) is the fileNames are read correctly.\n+   *    3) is the contents must be the same.\n+   */\n+  test(\"Small file input || native disk IO\") {\n+\n+    sc.hadoopConfiguration.clear()\n+\n+    val dir = Files.createTempDirectory(\"smallfiles\")\n+    println(s\"native disk address is ${dir.toString}\")\n+\n+    SmallTextFilesSuite.fileNames.zip(SmallTextFilesSuite.filesContents).foreach {\n+      case (fname, contents) =>\n+        createNativeFile(dir, fname, contents)\n+    }\n+\n+    val res = smallTextFiles(sc, dir.toString).collect()\n+\n+    assert(res.size === SmallTextFilesSuite.fileNames.size,\n+      \"Number of files read out do not fit with the actual value\")\n+\n+    for ((fname, contents) <- res) {\n+      assert(SmallTextFilesSuite.fileNames.contains(fname),\n+        s\"Missing file name $fname.\")\n+      assert(contents.hashCode === SmallTextFilesSuite.hashCodeOfContents(fname),\n+        s\"file $fname contents can not match\")\n+    }\n+\n+    SmallTextFilesSuite.fileNames.foreach { fname =>\n+      Files.deleteIfExists(JPaths.get(s\"${dir.toString}/$fname\"))\n+    }\n+    Files.deleteIfExists(dir)\n+  }\n+}\n+\n+/**\n+ * Some final values are defined here. chineseWordsSpark is refer to the Chinese character version\n+ * of \"Spark\", we use UTF-8 to encode the bytes together, with a '\\n' in the end. fileNames and\n+ * fileContents represent the test data that will be used later. hashCodeOfContents is a Map of\n+ * fileName to the hashcode of contents, which is used for the comparison of contents, i.e. the\n+ * \"read in\" contents should be same with the \"read out\" ones.\n+ */\n+\n+object SmallTextFilesSuite {\n+  private val chineseWordsSpark = Array(\n+    0xe7.toByte, 0x81.toByte, 0xab.toByte,\n+    0xe8.toByte, 0x8a.toByte, 0xb1.toByte,\n+    '\\n'.toByte)\n+\n+  private val fileNames = Array(\"part-00000\", \"part-00001\", \"part-00002\")\n+\n+  private val filesContents = Array(7, 70, 700).map { upperBound =>\n+    Stream.continually(chineseWordsSpark.toList.toStream).flatten.take(upperBound).toArray\n+  }\n+\n+  private val hashCodeOfContents = fileNames.zip(filesContents).map { case (fname, contents) =>\n+    fname -> new Text(contents).toString.hashCode\n+  }.toMap\n+\n+  /**\n+   * Functions getUmask and getDirPermission are used for get the default directory permission on\n+   * current system, so as to give the MiniDFSCluster proper permissions. Or the test will be abort\n+   * due to the improper permission.\n+   */\n+  private def umask(): Int = {"
  }, {
    "author": {
      "login": "yinxusen"
    },
    "body": "It is a bug in `MiniDFSCluster`, reference [here](http://stackoverflow.com/questions/17625938/hbase-minidfscluster-java-fails-in-certain-environments). MiniDFSCluster creates fake HDFS in your local disk when doing test. But if it encounters the improper `umask` in Linux system, it would complain about NPE, which took me lots of time to find the reason and fixed it.\n",
    "commit": "4ed60d16b5d7c760c06dd0d95ee558eee5cfd398",
    "createdAt": "2014-03-20T09:06:07Z",
    "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.util\n+\n+\n+import java.io.{InputStreamReader, BufferedReader, DataOutputStream, FileOutputStream}\n+import java.nio.file.Files\n+import java.nio.file.{Path => JPath}\n+import java.nio.file.{Paths => JPaths}\n+\n+import org.scalatest.BeforeAndAfterAll\n+import org.scalatest.FunSuite\n+\n+import org.apache.hadoop.hdfs.MiniDFSCluster\n+import org.apache.hadoop.fs.FileSystem\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.io.Text\n+\n+import org.apache.spark.mllib.util.MLUtils._\n+import org.apache.spark.SparkContext\n+\n+/**\n+ * Tests HDFS IO and local disk IO of [[smallTextFiles]] in MLutils. HDFS tests create a mock DFS in\n+ * memory, while local disk test create a temp directory. All these temporal storages are deleted\n+ * in the end.\n+ */\n+\n+class SmallTextFilesSuite extends FunSuite with BeforeAndAfterAll {\n+  private var sc: SparkContext = _\n+  private var dfs: MiniDFSCluster = _\n+\n+  override def beforeAll() {\n+    sc = new SparkContext(\"local\", \"test\")\n+    sc.hadoopConfiguration.set(\"dfs.datanode.data.dir.perm\", SmallTextFilesSuite.dirPermission())\n+    dfs = new MiniDFSCluster(sc.hadoopConfiguration, 4, true,\n+                             Array(\"/rack0\", \"/rack0\", \"/rack1\", \"/rack1\"),\n+                             Array(\"host0\", \"host1\", \"host2\", \"host3\"))\n+  }\n+\n+  override def afterAll() {\n+    if (dfs != null) dfs.shutdown()\n+    sc.stop()\n+    System.clearProperty(\"spark.driver.port\")\n+  }\n+\n+\n+  private def createHDFSFile(\n+      fs: FileSystem,\n+      inputDir: Path,\n+      fileName: String,\n+      contents: Array[Byte]) = {\n+    val out: DataOutputStream = fs.create(new Path(inputDir, fileName), true, 4096, 2, 512, null)\n+    out.write(contents, 0, contents.length)\n+    out.close()\n+    System.out.println(\"Wrote HDFS file\")\n+  }\n+\n+  /**\n+   * This code will test the behaviors on HDFS. There are three aspects to test:\n+   *    1) is all files are read.\n+   *    2) is the fileNames are read correctly.\n+   *    3) is the contents must be the same.\n+   */\n+  test(\"Small file input || HDFS IO\") {\n+    val fs: FileSystem = dfs.getFileSystem\n+    val dir = \"/foo/\"\n+    val inputDir: Path = new Path(dir)\n+\n+    SmallTextFilesSuite.fileNames.zip(SmallTextFilesSuite.filesContents).foreach {\n+      case (fname, contents) =>\n+        createHDFSFile(fs, inputDir, fname, contents)\n+    }\n+\n+    println(s\"name node is ${dfs.getNameNode.getNameNodeAddress.getHostName}\")\n+    println(s\"name node port is ${dfs.getNameNodePort}\")\n+\n+    val hdfsAddressDir =\n+      s\"hdfs://${dfs.getNameNode.getNameNodeAddress.getHostName}:${dfs.getNameNodePort}$dir\"\n+    println(s\"HDFS address dir is $hdfsAddressDir\")\n+\n+    val res = smallTextFiles(sc, hdfsAddressDir).collect()\n+\n+    assert(res.size === SmallTextFilesSuite.fileNames.size,\n+      \"Number of files read out do not fit with the actual value\")\n+\n+    for ((fname, contents) <- res) {\n+      assert(SmallTextFilesSuite.fileNames.contains(fname),\n+        s\"Missing file name $fname.\")\n+      assert(contents.hashCode === SmallTextFilesSuite.hashCodeOfContents(fname),\n+        s\"file $fname contents can not match\")\n+    }\n+  }\n+\n+  private def createNativeFile(inputDir: JPath, fileName: String, contents: Array[Byte]) = {\n+    val out = new DataOutputStream(new FileOutputStream(s\"${inputDir.toString}/$fileName\"))\n+    out.write(contents, 0, contents.length)\n+    out.close()\n+    println(\"Wrote native file\")\n+  }\n+\n+  /**\n+   * This code will test the behaviors on native file system. There are three aspects:\n+   *    1) is all files are read.\n+   *    2) is the fileNames are read correctly.\n+   *    3) is the contents must be the same.\n+   */\n+  test(\"Small file input || native disk IO\") {\n+\n+    sc.hadoopConfiguration.clear()\n+\n+    val dir = Files.createTempDirectory(\"smallfiles\")\n+    println(s\"native disk address is ${dir.toString}\")\n+\n+    SmallTextFilesSuite.fileNames.zip(SmallTextFilesSuite.filesContents).foreach {\n+      case (fname, contents) =>\n+        createNativeFile(dir, fname, contents)\n+    }\n+\n+    val res = smallTextFiles(sc, dir.toString).collect()\n+\n+    assert(res.size === SmallTextFilesSuite.fileNames.size,\n+      \"Number of files read out do not fit with the actual value\")\n+\n+    for ((fname, contents) <- res) {\n+      assert(SmallTextFilesSuite.fileNames.contains(fname),\n+        s\"Missing file name $fname.\")\n+      assert(contents.hashCode === SmallTextFilesSuite.hashCodeOfContents(fname),\n+        s\"file $fname contents can not match\")\n+    }\n+\n+    SmallTextFilesSuite.fileNames.foreach { fname =>\n+      Files.deleteIfExists(JPaths.get(s\"${dir.toString}/$fname\"))\n+    }\n+    Files.deleteIfExists(dir)\n+  }\n+}\n+\n+/**\n+ * Some final values are defined here. chineseWordsSpark is refer to the Chinese character version\n+ * of \"Spark\", we use UTF-8 to encode the bytes together, with a '\\n' in the end. fileNames and\n+ * fileContents represent the test data that will be used later. hashCodeOfContents is a Map of\n+ * fileName to the hashcode of contents, which is used for the comparison of contents, i.e. the\n+ * \"read in\" contents should be same with the \"read out\" ones.\n+ */\n+\n+object SmallTextFilesSuite {\n+  private val chineseWordsSpark = Array(\n+    0xe7.toByte, 0x81.toByte, 0xab.toByte,\n+    0xe8.toByte, 0x8a.toByte, 0xb1.toByte,\n+    '\\n'.toByte)\n+\n+  private val fileNames = Array(\"part-00000\", \"part-00001\", \"part-00002\")\n+\n+  private val filesContents = Array(7, 70, 700).map { upperBound =>\n+    Stream.continually(chineseWordsSpark.toList.toStream).flatten.take(upperBound).toArray\n+  }\n+\n+  private val hashCodeOfContents = fileNames.zip(filesContents).map { case (fname, contents) =>\n+    fname -> new Text(contents).toString.hashCode\n+  }.toMap\n+\n+  /**\n+   * Functions getUmask and getDirPermission are used for get the default directory permission on\n+   * current system, so as to give the MiniDFSCluster proper permissions. Or the test will be abort\n+   * due to the improper permission.\n+   */\n+  private def umask(): Int = {"
  }],
  "prId": 164
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Is it necessary to use UTF-8 characters to create the test data?\n",
    "commit": "4ed60d16b5d7c760c06dd0d95ee558eee5cfd398",
    "createdAt": "2014-03-20T08:54:21Z",
    "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.util\n+\n+\n+import java.io.{InputStreamReader, BufferedReader, DataOutputStream, FileOutputStream}\n+import java.nio.file.Files\n+import java.nio.file.{Path => JPath}\n+import java.nio.file.{Paths => JPaths}\n+\n+import org.scalatest.BeforeAndAfterAll\n+import org.scalatest.FunSuite\n+\n+import org.apache.hadoop.hdfs.MiniDFSCluster\n+import org.apache.hadoop.fs.FileSystem\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.io.Text\n+\n+import org.apache.spark.mllib.util.MLUtils._\n+import org.apache.spark.SparkContext\n+\n+/**\n+ * Tests HDFS IO and local disk IO of [[smallTextFiles]] in MLutils. HDFS tests create a mock DFS in\n+ * memory, while local disk test create a temp directory. All these temporal storages are deleted\n+ * in the end.\n+ */\n+\n+class SmallTextFilesSuite extends FunSuite with BeforeAndAfterAll {\n+  private var sc: SparkContext = _\n+  private var dfs: MiniDFSCluster = _\n+\n+  override def beforeAll() {\n+    sc = new SparkContext(\"local\", \"test\")\n+    sc.hadoopConfiguration.set(\"dfs.datanode.data.dir.perm\", SmallTextFilesSuite.dirPermission())\n+    dfs = new MiniDFSCluster(sc.hadoopConfiguration, 4, true,\n+                             Array(\"/rack0\", \"/rack0\", \"/rack1\", \"/rack1\"),\n+                             Array(\"host0\", \"host1\", \"host2\", \"host3\"))\n+  }\n+\n+  override def afterAll() {\n+    if (dfs != null) dfs.shutdown()\n+    sc.stop()\n+    System.clearProperty(\"spark.driver.port\")\n+  }\n+\n+\n+  private def createHDFSFile(\n+      fs: FileSystem,\n+      inputDir: Path,\n+      fileName: String,\n+      contents: Array[Byte]) = {\n+    val out: DataOutputStream = fs.create(new Path(inputDir, fileName), true, 4096, 2, 512, null)\n+    out.write(contents, 0, contents.length)\n+    out.close()\n+    System.out.println(\"Wrote HDFS file\")\n+  }\n+\n+  /**\n+   * This code will test the behaviors on HDFS. There are three aspects to test:\n+   *    1) is all files are read.\n+   *    2) is the fileNames are read correctly.\n+   *    3) is the contents must be the same.\n+   */\n+  test(\"Small file input || HDFS IO\") {\n+    val fs: FileSystem = dfs.getFileSystem\n+    val dir = \"/foo/\"\n+    val inputDir: Path = new Path(dir)\n+\n+    SmallTextFilesSuite.fileNames.zip(SmallTextFilesSuite.filesContents).foreach {\n+      case (fname, contents) =>\n+        createHDFSFile(fs, inputDir, fname, contents)\n+    }\n+\n+    println(s\"name node is ${dfs.getNameNode.getNameNodeAddress.getHostName}\")\n+    println(s\"name node port is ${dfs.getNameNodePort}\")\n+\n+    val hdfsAddressDir =\n+      s\"hdfs://${dfs.getNameNode.getNameNodeAddress.getHostName}:${dfs.getNameNodePort}$dir\"\n+    println(s\"HDFS address dir is $hdfsAddressDir\")\n+\n+    val res = smallTextFiles(sc, hdfsAddressDir).collect()\n+\n+    assert(res.size === SmallTextFilesSuite.fileNames.size,\n+      \"Number of files read out do not fit with the actual value\")\n+\n+    for ((fname, contents) <- res) {\n+      assert(SmallTextFilesSuite.fileNames.contains(fname),\n+        s\"Missing file name $fname.\")\n+      assert(contents.hashCode === SmallTextFilesSuite.hashCodeOfContents(fname),\n+        s\"file $fname contents can not match\")\n+    }\n+  }\n+\n+  private def createNativeFile(inputDir: JPath, fileName: String, contents: Array[Byte]) = {\n+    val out = new DataOutputStream(new FileOutputStream(s\"${inputDir.toString}/$fileName\"))\n+    out.write(contents, 0, contents.length)\n+    out.close()\n+    println(\"Wrote native file\")\n+  }\n+\n+  /**\n+   * This code will test the behaviors on native file system. There are three aspects:\n+   *    1) is all files are read.\n+   *    2) is the fileNames are read correctly.\n+   *    3) is the contents must be the same.\n+   */\n+  test(\"Small file input || native disk IO\") {\n+\n+    sc.hadoopConfiguration.clear()\n+\n+    val dir = Files.createTempDirectory(\"smallfiles\")\n+    println(s\"native disk address is ${dir.toString}\")\n+\n+    SmallTextFilesSuite.fileNames.zip(SmallTextFilesSuite.filesContents).foreach {\n+      case (fname, contents) =>\n+        createNativeFile(dir, fname, contents)\n+    }\n+\n+    val res = smallTextFiles(sc, dir.toString).collect()\n+\n+    assert(res.size === SmallTextFilesSuite.fileNames.size,\n+      \"Number of files read out do not fit with the actual value\")\n+\n+    for ((fname, contents) <- res) {\n+      assert(SmallTextFilesSuite.fileNames.contains(fname),\n+        s\"Missing file name $fname.\")\n+      assert(contents.hashCode === SmallTextFilesSuite.hashCodeOfContents(fname),\n+        s\"file $fname contents can not match\")\n+    }\n+\n+    SmallTextFilesSuite.fileNames.foreach { fname =>\n+      Files.deleteIfExists(JPaths.get(s\"${dir.toString}/$fname\"))\n+    }\n+    Files.deleteIfExists(dir)\n+  }\n+}\n+\n+/**\n+ * Some final values are defined here. chineseWordsSpark is refer to the Chinese character version\n+ * of \"Spark\", we use UTF-8 to encode the bytes together, with a '\\n' in the end. fileNames and\n+ * fileContents represent the test data that will be used later. hashCodeOfContents is a Map of\n+ * fileName to the hashcode of contents, which is used for the comparison of contents, i.e. the\n+ * \"read in\" contents should be same with the \"read out\" ones.\n+ */\n+\n+object SmallTextFilesSuite {\n+  private val chineseWordsSpark = Array("
  }, {
    "author": {
      "login": "yinxusen"
    },
    "body": "Hmm.. sorry I forget to modify the comments in testsuite along with other code.\n\nIt is not necessary to do that, I'll fix it. \n",
    "commit": "4ed60d16b5d7c760c06dd0d95ee558eee5cfd398",
    "createdAt": "2014-03-20T09:07:46Z",
    "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.util\n+\n+\n+import java.io.{InputStreamReader, BufferedReader, DataOutputStream, FileOutputStream}\n+import java.nio.file.Files\n+import java.nio.file.{Path => JPath}\n+import java.nio.file.{Paths => JPaths}\n+\n+import org.scalatest.BeforeAndAfterAll\n+import org.scalatest.FunSuite\n+\n+import org.apache.hadoop.hdfs.MiniDFSCluster\n+import org.apache.hadoop.fs.FileSystem\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.io.Text\n+\n+import org.apache.spark.mllib.util.MLUtils._\n+import org.apache.spark.SparkContext\n+\n+/**\n+ * Tests HDFS IO and local disk IO of [[smallTextFiles]] in MLutils. HDFS tests create a mock DFS in\n+ * memory, while local disk test create a temp directory. All these temporal storages are deleted\n+ * in the end.\n+ */\n+\n+class SmallTextFilesSuite extends FunSuite with BeforeAndAfterAll {\n+  private var sc: SparkContext = _\n+  private var dfs: MiniDFSCluster = _\n+\n+  override def beforeAll() {\n+    sc = new SparkContext(\"local\", \"test\")\n+    sc.hadoopConfiguration.set(\"dfs.datanode.data.dir.perm\", SmallTextFilesSuite.dirPermission())\n+    dfs = new MiniDFSCluster(sc.hadoopConfiguration, 4, true,\n+                             Array(\"/rack0\", \"/rack0\", \"/rack1\", \"/rack1\"),\n+                             Array(\"host0\", \"host1\", \"host2\", \"host3\"))\n+  }\n+\n+  override def afterAll() {\n+    if (dfs != null) dfs.shutdown()\n+    sc.stop()\n+    System.clearProperty(\"spark.driver.port\")\n+  }\n+\n+\n+  private def createHDFSFile(\n+      fs: FileSystem,\n+      inputDir: Path,\n+      fileName: String,\n+      contents: Array[Byte]) = {\n+    val out: DataOutputStream = fs.create(new Path(inputDir, fileName), true, 4096, 2, 512, null)\n+    out.write(contents, 0, contents.length)\n+    out.close()\n+    System.out.println(\"Wrote HDFS file\")\n+  }\n+\n+  /**\n+   * This code will test the behaviors on HDFS. There are three aspects to test:\n+   *    1) is all files are read.\n+   *    2) is the fileNames are read correctly.\n+   *    3) is the contents must be the same.\n+   */\n+  test(\"Small file input || HDFS IO\") {\n+    val fs: FileSystem = dfs.getFileSystem\n+    val dir = \"/foo/\"\n+    val inputDir: Path = new Path(dir)\n+\n+    SmallTextFilesSuite.fileNames.zip(SmallTextFilesSuite.filesContents).foreach {\n+      case (fname, contents) =>\n+        createHDFSFile(fs, inputDir, fname, contents)\n+    }\n+\n+    println(s\"name node is ${dfs.getNameNode.getNameNodeAddress.getHostName}\")\n+    println(s\"name node port is ${dfs.getNameNodePort}\")\n+\n+    val hdfsAddressDir =\n+      s\"hdfs://${dfs.getNameNode.getNameNodeAddress.getHostName}:${dfs.getNameNodePort}$dir\"\n+    println(s\"HDFS address dir is $hdfsAddressDir\")\n+\n+    val res = smallTextFiles(sc, hdfsAddressDir).collect()\n+\n+    assert(res.size === SmallTextFilesSuite.fileNames.size,\n+      \"Number of files read out do not fit with the actual value\")\n+\n+    for ((fname, contents) <- res) {\n+      assert(SmallTextFilesSuite.fileNames.contains(fname),\n+        s\"Missing file name $fname.\")\n+      assert(contents.hashCode === SmallTextFilesSuite.hashCodeOfContents(fname),\n+        s\"file $fname contents can not match\")\n+    }\n+  }\n+\n+  private def createNativeFile(inputDir: JPath, fileName: String, contents: Array[Byte]) = {\n+    val out = new DataOutputStream(new FileOutputStream(s\"${inputDir.toString}/$fileName\"))\n+    out.write(contents, 0, contents.length)\n+    out.close()\n+    println(\"Wrote native file\")\n+  }\n+\n+  /**\n+   * This code will test the behaviors on native file system. There are three aspects:\n+   *    1) is all files are read.\n+   *    2) is the fileNames are read correctly.\n+   *    3) is the contents must be the same.\n+   */\n+  test(\"Small file input || native disk IO\") {\n+\n+    sc.hadoopConfiguration.clear()\n+\n+    val dir = Files.createTempDirectory(\"smallfiles\")\n+    println(s\"native disk address is ${dir.toString}\")\n+\n+    SmallTextFilesSuite.fileNames.zip(SmallTextFilesSuite.filesContents).foreach {\n+      case (fname, contents) =>\n+        createNativeFile(dir, fname, contents)\n+    }\n+\n+    val res = smallTextFiles(sc, dir.toString).collect()\n+\n+    assert(res.size === SmallTextFilesSuite.fileNames.size,\n+      \"Number of files read out do not fit with the actual value\")\n+\n+    for ((fname, contents) <- res) {\n+      assert(SmallTextFilesSuite.fileNames.contains(fname),\n+        s\"Missing file name $fname.\")\n+      assert(contents.hashCode === SmallTextFilesSuite.hashCodeOfContents(fname),\n+        s\"file $fname contents can not match\")\n+    }\n+\n+    SmallTextFilesSuite.fileNames.foreach { fname =>\n+      Files.deleteIfExists(JPaths.get(s\"${dir.toString}/$fname\"))\n+    }\n+    Files.deleteIfExists(dir)\n+  }\n+}\n+\n+/**\n+ * Some final values are defined here. chineseWordsSpark is refer to the Chinese character version\n+ * of \"Spark\", we use UTF-8 to encode the bytes together, with a '\\n' in the end. fileNames and\n+ * fileContents represent the test data that will be used later. hashCodeOfContents is a Map of\n+ * fileName to the hashcode of contents, which is used for the comparison of contents, i.e. the\n+ * \"read in\" contents should be same with the \"read out\" ones.\n+ */\n+\n+object SmallTextFilesSuite {\n+  private val chineseWordsSpark = Array("
  }],
  "prId": 164
}]