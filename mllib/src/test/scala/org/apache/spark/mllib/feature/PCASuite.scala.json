[{
  "comments": [{
    "author": {
      "login": "yanboliang"
    },
    "body": "Fix numerical accuracy issue.\n",
    "commit": "dbfcbff6945dfd3e02180d95c4bccf7f5500e199",
    "createdAt": "2016-07-14T15:07:17Z",
    "diffHunk": "@@ -42,7 +43,9 @@ class PCASuite extends SparkFunSuite with MLlibTestSparkContext {\n     val pca_transform = pca.transform(dataRDD).collect()\n     val mat_multiply = mat.multiply(pc).rows.collect()\n \n-    assert(pca_transform.toSet === mat_multiply.toSet)\n-    assert(pca.explainedVariance === explainedVariance)\n+    pca_transform.zip(mat_multiply).foreach { case (calculated: Vector, expected: Vector) =>\n+      assert(calculated ~== expected relTol 1e-8)\n+    }\n+    assert(pca.explainedVariance ~== explainedVariance relTol 1e-8)"
  }],
  "prId": 14150
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Not a big deal but do you really need the explicit types?\n",
    "commit": "dbfcbff6945dfd3e02180d95c4bccf7f5500e199",
    "createdAt": "2016-07-15T08:20:37Z",
    "diffHunk": "@@ -42,7 +43,9 @@ class PCASuite extends SparkFunSuite with MLlibTestSparkContext {\n     val pca_transform = pca.transform(dataRDD).collect()\n     val mat_multiply = mat.multiply(pc).rows.collect()\n \n-    assert(pca_transform.toSet === mat_multiply.toSet)\n-    assert(pca.explainedVariance === explainedVariance)\n+    pca_transform.zip(mat_multiply).foreach { case (calculated: Vector, expected: Vector) =>"
  }],
  "prId": 14150
}]