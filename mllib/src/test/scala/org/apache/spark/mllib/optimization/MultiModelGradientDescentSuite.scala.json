[{
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "What is this testing?  Is this supposed to throw & then catch an error?\n",
    "commit": "2ea711c96990c24ed29a4b9476a3809ef26403d6",
    "createdAt": "2014-09-19T22:41:27Z",
    "diffHunk": "@@ -0,0 +1,444 @@\n+package org.apache.spark.mllib.optimization\n+\n+import scala.collection.JavaConversions._\n+import scala.util.Random\n+\n+import org.scalatest.{FunSuite, Matchers}\n+\n+import org.apache.spark.mllib.linalg.{DenseMatrix, Matrices, Vectors}\n+import org.apache.spark.mllib.regression._\n+import org.apache.spark.mllib.util.{LinearDataGenerator, LocalClusterSparkContext, LocalSparkContext}\n+import org.apache.spark.mllib.util.TestingUtils._\n+\n+object MultiModelGradientDescentSuite {\n+\n+  def generateLogisticInputAsList(\n+                                   offset: Double,\n+                                   scale: Double,\n+                                   nPoints: Int,\n+                                   seed: Int): java.util.List[LabeledPoint] = {\n+    seqAsJavaList(generateGDInput(offset, scale, nPoints, seed))\n+  }\n+\n+  // Generate input of the form Y = logistic(offset + scale * X)\n+  def generateGDInput(\n+                       offset: Double,\n+                       scale: Double,\n+                       nPoints: Int,\n+                       seed: Int): Seq[LabeledPoint]  = {\n+    val rnd = new Random(seed)\n+    val x1 = Array.fill[Double](nPoints)(rnd.nextGaussian())\n+\n+    val unifRand = new Random(45)\n+    val rLogis = (0 until nPoints).map { i =>\n+      val u = unifRand.nextDouble()\n+      math.log(u) - math.log(1.0-u)\n+    }\n+\n+    val y: Seq[Int] = (0 until nPoints).map { i =>\n+      val yVal = offset + scale * x1(i) + rLogis(i)\n+      if (yVal > 0) 1 else 0\n+    }\n+\n+    (0 until nPoints).map(i => LabeledPoint(y(i), Vectors.dense(x1(i))))\n+  }\n+\n+  def generateSVMInputAsList(\n+                              intercept: Double,\n+                              weights: Array[Double],\n+                              nPoints: Int,\n+                              seed: Int): java.util.List[LabeledPoint] = {\n+    seqAsJavaList(generateSVMInput(intercept, weights, nPoints, seed))\n+  }\n+\n+  // Generate noisy input of the form Y = signum(x.dot(weights) + intercept + noise)\n+  def generateSVMInput(\n+                        intercept: Double,\n+                        weights: Array[Double],\n+                        nPoints: Int,\n+                        seed: Int): Seq[LabeledPoint] = {\n+    val rnd = new Random(seed)\n+    val weightsMat = new DenseMatrix(weights.length, 1, weights)\n+    val x = Array.fill[Array[Double]](nPoints)(\n+      Array.fill[Double](weights.length)(rnd.nextDouble() * 2.0 - 1.0))\n+    val y = x.map { xi =>\n+      val yD = (new DenseMatrix(1, xi.length, xi) multiply weightsMat) +\n+        intercept + 0.01 * rnd.nextGaussian()\n+      if (yD.toArray(0) < 0) 0.0 else 1.0\n+    }\n+    y.zip(x).map(p => LabeledPoint(p._1, Vectors.dense(p._2)))\n+  }\n+}\n+\n+class MultiModelGradientDescentSuite extends FunSuite with LocalSparkContext with Matchers {\n+  test(\"Assert the loss is decreasing.\") {\n+    val nPoints = 10000\n+    val A = 2.0\n+    val B = -1.5\n+\n+    val initialB = -1.0\n+    val initialWeights = Array(initialB)\n+\n+    val gradient = new MultiModelLogisticGradient()\n+    val updater: Array[MultiModelUpdater] = Array(new MultiModelSimpleUpdater())\n+    val stepSize = Array(1.0, 0.1)\n+    val numIterations = Array(10)\n+    val regParam = Array(0.0)\n+    val miniBatchFrac = 1.0\n+\n+    // Add a extra variable consisting of all 1.0's for the intercept.\n+    val testData = GradientDescentSuite.generateGDInput(A, B, nPoints, 42)\n+    val data = testData.map { case LabeledPoint(label, features) =>\n+      label -> Vectors.dense(1.0 +: features.toArray)\n+    }\n+\n+    val dataRDD = sc.parallelize(data, 2).cache()\n+    val initialWeightsWithIntercept = Vectors.dense(1.0 +: initialWeights.toArray)\n+\n+    val (_, loss) = MultiModelGradientDescent.runMiniBatchMMSGD(\n+      dataRDD,\n+      gradient,\n+      updater,\n+      stepSize,\n+      numIterations,\n+      regParam,\n+      miniBatchFrac,\n+      initialWeightsWithIntercept)\n+\n+    assert(loss.last(0) - loss.head(0) < 0, \"loss isn't decreasing.\")\n+\n+    val lossDiff = loss.init.zip(loss.tail).map { case (lhs, rhs) => lhs(0) - rhs(0) }\n+    assert(lossDiff.count(_ > 0).toDouble / lossDiff.size > 0.8)\n+  }\n+\n+  test(\"Test the loss and gradient of first iteration with regularization.\") {\n+\n+    val gradient = new MultiModelLogisticGradient()\n+    val updater: Array[MultiModelUpdater] = Array(new MultiModelSquaredL2Updater())\n+\n+    // Add a extra variable consisting of all 1.0's for the intercept.\n+    val testData = GradientDescentSuite.generateGDInput(2.0, -1.5, 10000, 42)\n+    val data = testData.map { case LabeledPoint(label, features) =>\n+      label -> Vectors.dense(1.0 +: features.toArray)\n+    }\n+\n+    val dataRDD = sc.parallelize(data, 2).cache()\n+\n+    // Prepare non-zero weights\n+    val initialWeightsWithIntercept = Vectors.dense(1.0, 0.5)\n+\n+    val regParam0 = Array(0.0)\n+    val (newWeights0, loss0) = MultiModelGradientDescent.runMiniBatchMMSGD(\n+      dataRDD, gradient, updater, Array(1.0), Array(1), regParam0, 1.0, initialWeightsWithIntercept)\n+\n+    val regParam1 = Array(1.0)\n+    val (newWeights1, loss1) = MultiModelGradientDescent.runMiniBatchMMSGD(\n+      dataRDD, gradient, updater, Array(1.0), Array(1), regParam1, 1.0, initialWeightsWithIntercept)\n+\n+    assert(\n+      loss1(0)(0) ~== (loss0(0)(0) + (math.pow(initialWeightsWithIntercept(0), 2) +\n+        math.pow(initialWeightsWithIntercept(1), 2)) / 2) absTol 1E-5,\n+      \"\"\"For non-zero weights, the regVal should be \\frac{1}{2}\\sum_i w_i^2.\"\"\")\n+\n+    assert(\n+      (newWeights1(0, 0) ~== (newWeights0(0, 0) - initialWeightsWithIntercept(0)) absTol 1E-5) &&\n+        (newWeights1(1, 0) ~== (newWeights0(1, 0) - initialWeightsWithIntercept(1)) absTol 1E-5),\n+      \"The different between newWeights with/without regularization \" +\n+        \"should be initialWeightsWithIntercept.\")\n+  }\n+\n+  test(\"Check for correctness: LogisticRegression-(SimpleUpdater+SquaredL2Updater)\") {\n+    val nPoints = 100\n+    val A = 2.0\n+    val B = -1.5\n+\n+    val initialB = -1.0\n+    val initialWeights = Array(initialB)\n+\n+    val gradient = new MultiModelLogisticGradient()\n+    val updater: Array[MultiModelUpdater] =\n+      Array(new MultiModelSimpleUpdater(), new MultiModelSquaredL2Updater())\n+    val stepSize = Array(1.0, 0.1)\n+    val numIterations = Array(10)\n+    val regParam = Array(0.0, 0.1, 1.0)\n+    val miniBatchFrac = 1.0\n+\n+    // Add a extra variable consisting of all 1.0's for the intercept.\n+    val testData = GradientDescentSuite.generateGDInput(A, B, nPoints, 42)\n+    val data = testData.map { case LabeledPoint(label, features) =>\n+      label -> Vectors.dense(1.0 +: features.toArray)\n+    }\n+    val numModels = stepSize.length * regParam.length\n+\n+    val dataRDD = sc.parallelize(data, 2).cache()\n+\n+    val forLoop = (0 until numModels).map { i =>\n+      val (weightsGD, loss) = GradientDescent.runMiniBatchSGD(\n+        dataRDD,\n+        new LogisticGradient(),\n+        new SimpleUpdater(),\n+        stepSize(math.round(i * 1.0 / numModels).toInt),\n+        numIterations(0),\n+        regParam(i % regParam.length),\n+        miniBatchFrac,\n+        Vectors.dense(1.0 +: initialWeights.toArray.clone()))\n+      (weightsGD, loss)\n+    }\n+    val forLoop2 = (0 until numModels).map { i =>\n+      val (weightsGD2, loss) = GradientDescent.runMiniBatchSGD(\n+        dataRDD,\n+        new LogisticGradient(),\n+        new SquaredL2Updater(),\n+        stepSize(math.round(i * 1.0 / numModels).toInt),\n+        numIterations(0),\n+        regParam(i % regParam.length),\n+        miniBatchFrac,\n+        Vectors.dense(1.0 +: initialWeights.toArray.clone()))\n+      (weightsGD2, loss)\n+    }\n+\n+    val res2 = Matrices.horzCat(forLoop.map(v => new DenseMatrix(v._1.size, 1, v._1.toArray)) ++\n+      forLoop2.map(v => new DenseMatrix(v._1.size, 1, v._1.toArray)))\n+\n+    val (weightsMMGD, mmLoss) = MultiModelGradientDescent.runMiniBatchMMSGD(\n+      dataRDD,\n+      gradient,\n+      updater,\n+      stepSize,\n+      numIterations,\n+      regParam,\n+      miniBatchFrac,\n+      Vectors.dense(1.0 +: initialWeights.toArray))\n+\n+    assert(res2 ~== weightsMMGD absTol 1e-10)\n+\n+    val gdLosses1 = forLoop.map(_._2.last)\n+    val gdLosses2 = forLoop2.map(_._2.last)\n+    val lastFromGD = Vectors.dense((gdLosses1 ++ gdLosses2).toArray[Double])\n+\n+    assert(lastFromGD ~== mmLoss.last absTol 1e-10)\n+  }\n+\n+  // Test if we can correctly learn Y = 10*X1 + 10*X10000\n+  test(\"use sparse matrices instead of dense\") {\n+    val nPoints = 100\n+\n+    val denseRDD = sc.parallelize(\n+      LinearDataGenerator.generateLinearInput(0.0, Array(10.0, 10.0), nPoints, 42), 2)\n+    val sparseRDD = denseRDD.map { case LabeledPoint(label, v) =>\n+      val sv = Vectors.sparse(10000, Seq((0, v(0)), (9999, v(1))))\n+      (label, sv)\n+    }.cache()\n+    val gradient = new MultiModelLeastSquaresGradient()\n+    val updater: Array[MultiModelUpdater] = Array(new MultiModelSquaredL2Updater())\n+    val stepSize = Array(1.0, 0.1)\n+    val numIterations = Array(10)\n+    val regParam = Array(0.0, 0.1, 1.0)\n+    val miniBatchFrac = 1.0\n+    val initialWeights = Array.fill(10000)(0.0)\n+    // Add a extra variable consisting of all 1.0's for the intercept.\n+\n+    val numModels = stepSize.length * regParam.length\n+\n+    val forLoop = (0 until numModels).map { i =>\n+      val (weightsGD, loss) = GradientDescent.runMiniBatchSGD(\n+        sparseRDD,\n+        new LeastSquaresGradient(),\n+        new SquaredL2Updater(),\n+        stepSize(math.round(i * 1.0 / numModels).toInt),\n+        numIterations(0),\n+        regParam(i % regParam.length),\n+        miniBatchFrac,\n+        Vectors.dense(initialWeights.clone()))\n+      (weightsGD, loss)\n+    }\n+\n+    val res = Matrices.horzCat(forLoop.map(v => new DenseMatrix(v._1.size, 1, v._1.toArray)))\n+\n+    val (weightsMMGD, mmLoss) = MultiModelGradientDescent.runMiniBatchMMSGD(\n+      sparseRDD,\n+      gradient,\n+      updater,\n+      stepSize,\n+      numIterations,\n+      regParam,\n+      miniBatchFrac,\n+      Vectors.dense(initialWeights))\n+\n+    assert(res ~== weightsMMGD absTol 1e-10)\n+\n+    val gdLosses1 = forLoop.map(_._2.last)\n+    val lastFromGD = Vectors.dense(gdLosses1.toArray[Double])\n+\n+    assert(lastFromGD ~== mmLoss.last absTol 1e-10)\n+  }\n+\n+  test(\"Check for correctness: LeastSquaresRegression-SquaredL2Updater & multiple numIterations\") {\n+    val nPoints = 100\n+    val numFeatures = 5\n+\n+    val initialWeights = Matrices.zeros(numFeatures, 1).toArray\n+\n+    // Pick weights as random values distributed uniformly in [-0.5, 0.5]\n+    val w = Matrices.rand(numFeatures, 1) -= 0.5\n+\n+    // Use half of data for training and other half for validation\n+    val data = LinearDataGenerator.generateLinearInput(0.0, w.toArray, nPoints, 42, 10.0)\n+\n+    val gradient = new MultiModelLeastSquaresGradient()\n+    val updater: Array[MultiModelUpdater] = Array(new MultiModelSquaredL2Updater())\n+    val stepSize = Array(1.0, 0.1)\n+    val numIterations = Array(10, 20)\n+    val regParam = Array(0.0, 0.1, 1.0)\n+    val miniBatchFrac = 1.0\n+\n+    val dataRDD = sc.parallelize(data, 2).map( p => (p.label, p.features)).cache()\n+    val numModels = stepSize.length * regParam.length\n+\n+    val forLoop = (0 until numModels).map { i =>\n+      val (weightsGD2, loss) = GradientDescent.runMiniBatchSGD(\n+        dataRDD,\n+        new LeastSquaresGradient(),\n+        new SquaredL2Updater(),\n+        stepSize(math.round(i * 1.0 / numModels).toInt),\n+        numIterations(0),\n+        regParam(i % regParam.length),\n+        miniBatchFrac,\n+        Vectors.dense(initialWeights.clone()))\n+      (weightsGD2, loss)\n+    }\n+    val forLoop2 = (0 until numModels).map { i =>\n+      val (weightsGD2, loss) = GradientDescent.runMiniBatchSGD(\n+        dataRDD,\n+        new LeastSquaresGradient(),\n+        new SquaredL2Updater(),\n+        stepSize(math.round(i * 1.0 / numModels).toInt),\n+        numIterations(1),\n+        regParam(i % regParam.length),\n+        miniBatchFrac,\n+        Vectors.dense(initialWeights.clone()))\n+      (weightsGD2, loss)\n+    }\n+    val res = Matrices.horzCat(forLoop.map( v => new DenseMatrix(v._1.size, 1, v._1.toArray)) ++\n+      forLoop2.map( v => new DenseMatrix(v._1.size, 1, v._1.toArray)))\n+\n+    val (weightsMMGD, mmLoss) = MultiModelGradientDescent.runMiniBatchMMSGD(\n+      dataRDD,\n+      gradient,\n+      updater,\n+      stepSize,\n+      numIterations,\n+      regParam,\n+      miniBatchFrac,\n+      Vectors.dense(initialWeights))\n+\n+    assert(res ~== weightsMMGD absTol 1e-10)\n+\n+    val gdLosses1 = forLoop.map(_._2.last)\n+    val gdLosses2 = forLoop2.map(_._2.last)\n+    val lastFromGD = Vectors.dense((gdLosses1 ++ gdLosses2).toArray)\n+\n+    val mmLossTogether = Vectors.dense(mmLoss(numIterations(0) - 1).toArray ++\n+      mmLoss(numIterations(1) - 1).toArray)\n+\n+    assert(lastFromGD ~== mmLossTogether absTol 1e-10)\n+  }\n+\n+  test(\"Check for correctness: SVM-(L1Updater+SquaredL2Updater)\") {\n+    val nPoints = 100\n+\n+    val initialWeights = Array(1.0, 0.0, 0.0)\n+\n+    val A = 0.01\n+    val B = -1.5\n+    val C = 1.0\n+\n+    val testData = MultiModelGradientDescentSuite.\n+      generateSVMInput(A, Array[Double](B, C), nPoints, 42)\n+\n+    val data = testData.map { case LabeledPoint(label, features) =>\n+      label -> Vectors.dense(1.0 +: features.toArray)\n+    }\n+\n+    val gradient = new MultiModelHingeGradient()\n+    val updater: Array[MultiModelUpdater] =\n+      Array(new MultiModelL1Updater, new MultiModelSquaredL2Updater)\n+    val stepSize = Array(1.0, 0.1)\n+    val numIterations = Array(10)\n+    val regParam = Array(0.0, 0.1)\n+    val miniBatchFrac = 1.0\n+\n+    val dataRDD = sc.parallelize(data, 2).cache()\n+    val numModels = stepSize.length * regParam.length\n+\n+    val forLoop1 = (0 until numModels).map { i =>\n+      val (weightsGD2, loss) = GradientDescent.runMiniBatchSGD(\n+        dataRDD,\n+        new HingeGradient(),\n+        new L1Updater(),\n+        stepSize(math.round(i * 1.0 / numModels).toInt),\n+        numIterations(0),\n+        regParam(i % regParam.length),\n+        miniBatchFrac,\n+        Vectors.dense(initialWeights.clone()))\n+      (weightsGD2, loss)\n+    }\n+    val forLoop2 = (0 until numModels).map { i =>\n+      val (weightsGD2, loss) = GradientDescent.runMiniBatchSGD(\n+        dataRDD,\n+        new HingeGradient(),\n+        new SquaredL2Updater(),\n+        stepSize(math.round(i * 1.0 / numModels).toInt),\n+        numIterations(0),\n+        regParam(i % regParam.length),\n+        miniBatchFrac,\n+        Vectors.dense(initialWeights.clone()))\n+      (weightsGD2, loss)\n+    }\n+\n+    val res = Matrices.horzCat(forLoop1.map( v => new DenseMatrix(v._1.size, 1, v._1.toArray)) ++\n+      forLoop2.map( v => new DenseMatrix(v._1.size, 1, v._1.toArray)))\n+\n+    val (weightsMMGD, mmLoss) = MultiModelGradientDescent.runMiniBatchMMSGD(\n+      dataRDD,\n+      gradient,\n+      updater,\n+      stepSize,\n+      numIterations,\n+      regParam,\n+      miniBatchFrac,\n+      Vectors.dense(initialWeights))\n+\n+    assert(res ~== weightsMMGD absTol 1e-10)\n+\n+    val gdLosses1 = forLoop1.map(_._2.last)\n+    val gdLosses2 = forLoop2.map(_._2.last)\n+    val lastFromGD = Vectors.dense((gdLosses1 ++ gdLosses2).toArray[Double])\n+\n+    assert(lastFromGD ~== mmLoss.last absTol 1e-10)\n+  }\n+}\n+\n+class MultiModelGradientDescentClusterSuite extends FunSuite with LocalClusterSparkContext {\n+\n+  test(\"task size should be small\") {",
    "line": 424
  }],
  "prId": 2451
}]