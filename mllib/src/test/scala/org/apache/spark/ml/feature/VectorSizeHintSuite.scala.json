[{
  "comments": [{
    "author": {
      "login": "WeichenXu123"
    },
    "body": "Use `intercept[SparkException] {...}` is better.",
    "commit": "9c3dcec30d152ab2c5c242db03ed466ae1d320ae",
    "createdAt": "2017-11-20T10:45:30Z",
    "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.attribute.AttributeGroup\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.util.DefaultReadWriteTest\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.StreamTest\n+\n+class VectorSizeHintSuite\n+  extends SparkFunSuite with MLlibTestSparkContext with DefaultReadWriteTest {\n+\n+  import testImplicits._\n+\n+  test(\"Test Param Validators\") {\n+    assertThrows[IllegalArgumentException] (new VectorSizeHint().setHandleInvalid(\"invalidValue\"))\n+    assertThrows[IllegalArgumentException] (new VectorSizeHint().setSize(-3))\n+  }\n+\n+  test(\"Adding size to column of vectors.\") {\n+\n+    val size = 3\n+    val denseVector = Vectors.dense(1, 2, 3)\n+    val sparseVector = Vectors.sparse(size, Array(), Array())\n+\n+    val data = Seq(denseVector, denseVector, sparseVector).map(Tuple1.apply)\n+    val dataFrame = data.toDF(\"vector\")\n+\n+    val transformer = new VectorSizeHint()\n+      .setInputCol(\"vector\")\n+      .setSize(3)\n+      .setHandleInvalid(\"error\")\n+    val withSize = transformer.transform(dataFrame)\n+    assert(\n+      AttributeGroup.fromStructField(withSize.schema(\"vector\")).size == size,\n+      \"Transformer did not add expected size data.\")\n+  }\n+\n+  test(\"Size hint preserves attributes.\") {\n+\n+    case class Foo(x: Double, y: Double, z: Double)\n+    val size = 3\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val boo = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(\"vector\")\n+    val dataFrameWithMeatadata = assembler.transform(boo)\n+    val group = AttributeGroup.fromStructField(dataFrameWithMeatadata.schema(\"vector\"))\n+\n+    val transformer = new VectorSizeHint()\n+      .setInputCol(\"vector\")\n+      .setSize(3)\n+      .setHandleInvalid(\"error\")\n+    val withSize = transformer.transform(dataFrameWithMeatadata)\n+\n+    val newGroup = AttributeGroup.fromStructField(withSize.schema(\"vector\"))\n+    assert(newGroup.size == size, \"Transformer did not add expected size data.\")\n+    assert(\n+      newGroup.attributes.get.deep === group.attributes.get.deep,\n+      \"SizeHintTransformer did not preserve attributes.\")\n+  }\n+\n+  test(\"Handle invalid does the right thing.\") {\n+\n+    val vector = Vectors.dense(1, 2, 3)\n+    val short = Vectors.dense(2)\n+    val dataWithNull = Seq(vector, null).map(Tuple1.apply).toDF(\"vector\")\n+    val dataWithShort = Seq(vector, short).map(Tuple1.apply).toDF(\"vector\")\n+\n+    val sizeHint = new VectorSizeHint()\n+      .setInputCol(\"vector\")\n+      .setHandleInvalid(\"error\")\n+      .setSize(3)\n+\n+    assertThrows[SparkException](sizeHint.transform(dataWithNull).collect)"
  }, {
    "author": {
      "login": "MrBago"
    },
    "body": "I've made the change. Just out of curiosity, why is `intercept` better than `assertThrows`?",
    "commit": "9c3dcec30d152ab2c5c242db03ed466ae1d320ae",
    "createdAt": "2017-11-20T21:08:22Z",
    "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.attribute.AttributeGroup\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.util.DefaultReadWriteTest\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.StreamTest\n+\n+class VectorSizeHintSuite\n+  extends SparkFunSuite with MLlibTestSparkContext with DefaultReadWriteTest {\n+\n+  import testImplicits._\n+\n+  test(\"Test Param Validators\") {\n+    assertThrows[IllegalArgumentException] (new VectorSizeHint().setHandleInvalid(\"invalidValue\"))\n+    assertThrows[IllegalArgumentException] (new VectorSizeHint().setSize(-3))\n+  }\n+\n+  test(\"Adding size to column of vectors.\") {\n+\n+    val size = 3\n+    val denseVector = Vectors.dense(1, 2, 3)\n+    val sparseVector = Vectors.sparse(size, Array(), Array())\n+\n+    val data = Seq(denseVector, denseVector, sparseVector).map(Tuple1.apply)\n+    val dataFrame = data.toDF(\"vector\")\n+\n+    val transformer = new VectorSizeHint()\n+      .setInputCol(\"vector\")\n+      .setSize(3)\n+      .setHandleInvalid(\"error\")\n+    val withSize = transformer.transform(dataFrame)\n+    assert(\n+      AttributeGroup.fromStructField(withSize.schema(\"vector\")).size == size,\n+      \"Transformer did not add expected size data.\")\n+  }\n+\n+  test(\"Size hint preserves attributes.\") {\n+\n+    case class Foo(x: Double, y: Double, z: Double)\n+    val size = 3\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val boo = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(\"vector\")\n+    val dataFrameWithMeatadata = assembler.transform(boo)\n+    val group = AttributeGroup.fromStructField(dataFrameWithMeatadata.schema(\"vector\"))\n+\n+    val transformer = new VectorSizeHint()\n+      .setInputCol(\"vector\")\n+      .setSize(3)\n+      .setHandleInvalid(\"error\")\n+    val withSize = transformer.transform(dataFrameWithMeatadata)\n+\n+    val newGroup = AttributeGroup.fromStructField(withSize.schema(\"vector\"))\n+    assert(newGroup.size == size, \"Transformer did not add expected size data.\")\n+    assert(\n+      newGroup.attributes.get.deep === group.attributes.get.deep,\n+      \"SizeHintTransformer did not preserve attributes.\")\n+  }\n+\n+  test(\"Handle invalid does the right thing.\") {\n+\n+    val vector = Vectors.dense(1, 2, 3)\n+    val short = Vectors.dense(2)\n+    val dataWithNull = Seq(vector, null).map(Tuple1.apply).toDF(\"vector\")\n+    val dataWithShort = Seq(vector, short).map(Tuple1.apply).toDF(\"vector\")\n+\n+    val sizeHint = new VectorSizeHint()\n+      .setInputCol(\"vector\")\n+      .setHandleInvalid(\"error\")\n+      .setSize(3)\n+\n+    assertThrows[SparkException](sizeHint.transform(dataWithNull).collect)"
  }],
  "prId": 19746
}, {
  "comments": [{
    "author": {
      "login": "WeichenXu123"
    },
    "body": "I don't find a test for `optimistic` option. We should test:\r\nIf input dataset vector column do not include metadata, the `VectorSizeHint` should add metadata with proper size, or input vector column include metadata with different `size`, the `VectorSizeHint` should replace it.",
    "commit": "9c3dcec30d152ab2c5c242db03ed466ae1d320ae",
    "createdAt": "2017-11-20T10:50:50Z",
    "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.attribute.AttributeGroup\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.util.DefaultReadWriteTest\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.StreamTest\n+\n+class VectorSizeHintSuite\n+  extends SparkFunSuite with MLlibTestSparkContext with DefaultReadWriteTest {\n+\n+  import testImplicits._\n+\n+  test(\"Test Param Validators\") {\n+    assertThrows[IllegalArgumentException] (new VectorSizeHint().setHandleInvalid(\"invalidValue\"))\n+    assertThrows[IllegalArgumentException] (new VectorSizeHint().setSize(-3))\n+  }\n+\n+  test(\"Adding size to column of vectors.\") {\n+\n+    val size = 3\n+    val denseVector = Vectors.dense(1, 2, 3)\n+    val sparseVector = Vectors.sparse(size, Array(), Array())\n+\n+    val data = Seq(denseVector, denseVector, sparseVector).map(Tuple1.apply)\n+    val dataFrame = data.toDF(\"vector\")\n+\n+    val transformer = new VectorSizeHint()\n+      .setInputCol(\"vector\")\n+      .setSize(3)\n+      .setHandleInvalid(\"error\")\n+    val withSize = transformer.transform(dataFrame)\n+    assert(\n+      AttributeGroup.fromStructField(withSize.schema(\"vector\")).size == size,\n+      \"Transformer did not add expected size data.\")\n+  }\n+\n+  test(\"Size hint preserves attributes.\") {\n+\n+    case class Foo(x: Double, y: Double, z: Double)\n+    val size = 3\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val boo = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(\"vector\")\n+    val dataFrameWithMeatadata = assembler.transform(boo)\n+    val group = AttributeGroup.fromStructField(dataFrameWithMeatadata.schema(\"vector\"))\n+\n+    val transformer = new VectorSizeHint()\n+      .setInputCol(\"vector\")\n+      .setSize(3)\n+      .setHandleInvalid(\"error\")\n+    val withSize = transformer.transform(dataFrameWithMeatadata)\n+\n+    val newGroup = AttributeGroup.fromStructField(withSize.schema(\"vector\"))\n+    assert(newGroup.size == size, \"Transformer did not add expected size data.\")\n+    assert(\n+      newGroup.attributes.get.deep === group.attributes.get.deep,\n+      \"SizeHintTransformer did not preserve attributes.\")\n+  }\n+\n+  test(\"Handle invalid does the right thing.\") {"
  }, {
    "author": {
      "login": "MrBago"
    },
    "body": "I talked offline to @jkbradley and I think it's better to throw an exception unless if the column includes metadata & the there is a mismatch between the new and original size.\r\n\r\nI've added a new test for this exception and made sure the other tests are run with all `handleInvalid` cases. Does it look ok now?",
    "commit": "9c3dcec30d152ab2c5c242db03ed466ae1d320ae",
    "createdAt": "2017-11-20T21:07:21Z",
    "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.attribute.AttributeGroup\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.util.DefaultReadWriteTest\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.StreamTest\n+\n+class VectorSizeHintSuite\n+  extends SparkFunSuite with MLlibTestSparkContext with DefaultReadWriteTest {\n+\n+  import testImplicits._\n+\n+  test(\"Test Param Validators\") {\n+    assertThrows[IllegalArgumentException] (new VectorSizeHint().setHandleInvalid(\"invalidValue\"))\n+    assertThrows[IllegalArgumentException] (new VectorSizeHint().setSize(-3))\n+  }\n+\n+  test(\"Adding size to column of vectors.\") {\n+\n+    val size = 3\n+    val denseVector = Vectors.dense(1, 2, 3)\n+    val sparseVector = Vectors.sparse(size, Array(), Array())\n+\n+    val data = Seq(denseVector, denseVector, sparseVector).map(Tuple1.apply)\n+    val dataFrame = data.toDF(\"vector\")\n+\n+    val transformer = new VectorSizeHint()\n+      .setInputCol(\"vector\")\n+      .setSize(3)\n+      .setHandleInvalid(\"error\")\n+    val withSize = transformer.transform(dataFrame)\n+    assert(\n+      AttributeGroup.fromStructField(withSize.schema(\"vector\")).size == size,\n+      \"Transformer did not add expected size data.\")\n+  }\n+\n+  test(\"Size hint preserves attributes.\") {\n+\n+    case class Foo(x: Double, y: Double, z: Double)\n+    val size = 3\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val boo = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(\"vector\")\n+    val dataFrameWithMeatadata = assembler.transform(boo)\n+    val group = AttributeGroup.fromStructField(dataFrameWithMeatadata.schema(\"vector\"))\n+\n+    val transformer = new VectorSizeHint()\n+      .setInputCol(\"vector\")\n+      .setSize(3)\n+      .setHandleInvalid(\"error\")\n+    val withSize = transformer.transform(dataFrameWithMeatadata)\n+\n+    val newGroup = AttributeGroup.fromStructField(withSize.schema(\"vector\"))\n+    assert(newGroup.size == size, \"Transformer did not add expected size data.\")\n+    assert(\n+      newGroup.attributes.get.deep === group.attributes.get.deep,\n+      \"SizeHintTransformer did not preserve attributes.\")\n+  }\n+\n+  test(\"Handle invalid does the right thing.\") {"
  }],
  "prId": 19746
}, {
  "comments": [{
    "author": {
      "login": "WeichenXu123"
    },
    "body": "Add checking for added metadata here ?\r\n\r\nAnd should test if metadata exists, but size do not match, exception will be thrown.",
    "commit": "9c3dcec30d152ab2c5c242db03ed466ae1d320ae",
    "createdAt": "2017-11-21T01:38:23Z",
    "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.attribute.AttributeGroup\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.util.DefaultReadWriteTest\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.StreamTest\n+\n+class VectorSizeHintSuite\n+  extends SparkFunSuite with MLlibTestSparkContext with DefaultReadWriteTest {\n+\n+  import testImplicits._\n+\n+  test(\"Test Param Validators\") {\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setHandleInvalid(\"invalidValue\"))\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setSize(-3))\n+  }\n+\n+  test(\"Adding size to column of vectors.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val denseVector = Vectors.dense(1, 2, 3)\n+    val sparseVector = Vectors.sparse(size, Array(), Array())\n+\n+    val data = Seq(denseVector, denseVector, sparseVector).map(Tuple1.apply)\n+    val dataFrame = data.toDF(vectorColName)\n+    assert(\n+      AttributeGroup.fromStructField(dataFrame.schema(vectorColName)).size == -1,\n+      \"Transformer did not add expected size data.\")\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrame)\n+      assert(\n+        AttributeGroup.fromStructField(withSize.schema(vectorColName)).size == size,\n+        \"Transformer did not add expected size data.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size hint preserves attributes.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+    val group = AttributeGroup.fromStructField(dataFrameWithMetadata.schema(vectorColName))\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrameWithMetadata)\n+\n+      val newGroup = AttributeGroup.fromStructField(withSize.schema(vectorColName))\n+      assert(newGroup.size === size, \"Transformer did not add expected size data.\")\n+      assert(\n+        newGroup.attributes.get.deep === group.attributes.get.deep,\n+        \"SizeHintTransformer did not preserve attributes.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size miss-match between current and target size raises an error.\") {\n+    val size = 4\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      intercept[SparkException](transformer.transform(dataFrameWithMetadata))\n+    }\n+  }\n+\n+  test(\"Handle invalid does the right thing.\") {\n+\n+    val vector = Vectors.dense(1, 2, 3)\n+    val short = Vectors.dense(2)\n+    val dataWithNull = Seq(vector, null).map(Tuple1.apply).toDF(\"vector\")\n+    val dataWithShort = Seq(vector, short).map(Tuple1.apply).toDF(\"vector\")\n+\n+    val sizeHint = new VectorSizeHint()\n+      .setInputCol(\"vector\")\n+      .setHandleInvalid(\"error\")\n+      .setSize(3)\n+\n+    intercept[SparkException](sizeHint.transform(dataWithNull).collect)\n+    intercept[SparkException](sizeHint.transform(dataWithShort).collect)\n+\n+    sizeHint.setHandleInvalid(\"skip\")\n+    assert(sizeHint.transform(dataWithNull).count() === 1)\n+    assert(sizeHint.transform(dataWithShort).count() === 1)\n+  }\n+\n+  test(\"Test correct behaviour for handleInvalid == optimistic\") {\n+    val vector = Vectors.dense(1, 2, 3)\n+    val data = Seq(vector, vector).map(Tuple1.apply).toDF(\"vector\")\n+\n+    val sizeHint = new VectorSizeHint()\n+      .setInputCol(\"vector\")\n+      .setHandleInvalid(\"optimistic\")\n+      .setSize(3)\n+\n+    val transformed = sizeHint.transform(data)"
  }, {
    "author": {
      "login": "MrBago"
    },
    "body": "Can I just remove this test? I feel like all of that is tested in the first 3 tests.",
    "commit": "9c3dcec30d152ab2c5c242db03ed466ae1d320ae",
    "createdAt": "2017-11-21T01:46:03Z",
    "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.attribute.AttributeGroup\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.util.DefaultReadWriteTest\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.StreamTest\n+\n+class VectorSizeHintSuite\n+  extends SparkFunSuite with MLlibTestSparkContext with DefaultReadWriteTest {\n+\n+  import testImplicits._\n+\n+  test(\"Test Param Validators\") {\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setHandleInvalid(\"invalidValue\"))\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setSize(-3))\n+  }\n+\n+  test(\"Adding size to column of vectors.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val denseVector = Vectors.dense(1, 2, 3)\n+    val sparseVector = Vectors.sparse(size, Array(), Array())\n+\n+    val data = Seq(denseVector, denseVector, sparseVector).map(Tuple1.apply)\n+    val dataFrame = data.toDF(vectorColName)\n+    assert(\n+      AttributeGroup.fromStructField(dataFrame.schema(vectorColName)).size == -1,\n+      \"Transformer did not add expected size data.\")\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrame)\n+      assert(\n+        AttributeGroup.fromStructField(withSize.schema(vectorColName)).size == size,\n+        \"Transformer did not add expected size data.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size hint preserves attributes.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+    val group = AttributeGroup.fromStructField(dataFrameWithMetadata.schema(vectorColName))\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrameWithMetadata)\n+\n+      val newGroup = AttributeGroup.fromStructField(withSize.schema(vectorColName))\n+      assert(newGroup.size === size, \"Transformer did not add expected size data.\")\n+      assert(\n+        newGroup.attributes.get.deep === group.attributes.get.deep,\n+        \"SizeHintTransformer did not preserve attributes.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size miss-match between current and target size raises an error.\") {\n+    val size = 4\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      intercept[SparkException](transformer.transform(dataFrameWithMetadata))\n+    }\n+  }\n+\n+  test(\"Handle invalid does the right thing.\") {\n+\n+    val vector = Vectors.dense(1, 2, 3)\n+    val short = Vectors.dense(2)\n+    val dataWithNull = Seq(vector, null).map(Tuple1.apply).toDF(\"vector\")\n+    val dataWithShort = Seq(vector, short).map(Tuple1.apply).toDF(\"vector\")\n+\n+    val sizeHint = new VectorSizeHint()\n+      .setInputCol(\"vector\")\n+      .setHandleInvalid(\"error\")\n+      .setSize(3)\n+\n+    intercept[SparkException](sizeHint.transform(dataWithNull).collect)\n+    intercept[SparkException](sizeHint.transform(dataWithShort).collect)\n+\n+    sizeHint.setHandleInvalid(\"skip\")\n+    assert(sizeHint.transform(dataWithNull).count() === 1)\n+    assert(sizeHint.transform(dataWithShort).count() === 1)\n+  }\n+\n+  test(\"Test correct behaviour for handleInvalid == optimistic\") {\n+    val vector = Vectors.dense(1, 2, 3)\n+    val data = Seq(vector, vector).map(Tuple1.apply).toDF(\"vector\")\n+\n+    val sizeHint = new VectorSizeHint()\n+      .setInputCol(\"vector\")\n+      .setHandleInvalid(\"optimistic\")\n+      .setSize(3)\n+\n+    val transformed = sizeHint.transform(data)"
  }, {
    "author": {
      "login": "WeichenXu123"
    },
    "body": "OK. I agree. Other testcases already cover them.",
    "commit": "9c3dcec30d152ab2c5c242db03ed466ae1d320ae",
    "createdAt": "2017-11-21T02:41:14Z",
    "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.attribute.AttributeGroup\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.util.DefaultReadWriteTest\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.StreamTest\n+\n+class VectorSizeHintSuite\n+  extends SparkFunSuite with MLlibTestSparkContext with DefaultReadWriteTest {\n+\n+  import testImplicits._\n+\n+  test(\"Test Param Validators\") {\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setHandleInvalid(\"invalidValue\"))\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setSize(-3))\n+  }\n+\n+  test(\"Adding size to column of vectors.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val denseVector = Vectors.dense(1, 2, 3)\n+    val sparseVector = Vectors.sparse(size, Array(), Array())\n+\n+    val data = Seq(denseVector, denseVector, sparseVector).map(Tuple1.apply)\n+    val dataFrame = data.toDF(vectorColName)\n+    assert(\n+      AttributeGroup.fromStructField(dataFrame.schema(vectorColName)).size == -1,\n+      \"Transformer did not add expected size data.\")\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrame)\n+      assert(\n+        AttributeGroup.fromStructField(withSize.schema(vectorColName)).size == size,\n+        \"Transformer did not add expected size data.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size hint preserves attributes.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+    val group = AttributeGroup.fromStructField(dataFrameWithMetadata.schema(vectorColName))\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrameWithMetadata)\n+\n+      val newGroup = AttributeGroup.fromStructField(withSize.schema(vectorColName))\n+      assert(newGroup.size === size, \"Transformer did not add expected size data.\")\n+      assert(\n+        newGroup.attributes.get.deep === group.attributes.get.deep,\n+        \"SizeHintTransformer did not preserve attributes.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size miss-match between current and target size raises an error.\") {\n+    val size = 4\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      intercept[SparkException](transformer.transform(dataFrameWithMetadata))\n+    }\n+  }\n+\n+  test(\"Handle invalid does the right thing.\") {\n+\n+    val vector = Vectors.dense(1, 2, 3)\n+    val short = Vectors.dense(2)\n+    val dataWithNull = Seq(vector, null).map(Tuple1.apply).toDF(\"vector\")\n+    val dataWithShort = Seq(vector, short).map(Tuple1.apply).toDF(\"vector\")\n+\n+    val sizeHint = new VectorSizeHint()\n+      .setInputCol(\"vector\")\n+      .setHandleInvalid(\"error\")\n+      .setSize(3)\n+\n+    intercept[SparkException](sizeHint.transform(dataWithNull).collect)\n+    intercept[SparkException](sizeHint.transform(dataWithShort).collect)\n+\n+    sizeHint.setHandleInvalid(\"skip\")\n+    assert(sizeHint.transform(dataWithNull).count() === 1)\n+    assert(sizeHint.transform(dataWithShort).count() === 1)\n+  }\n+\n+  test(\"Test correct behaviour for handleInvalid == optimistic\") {\n+    val vector = Vectors.dense(1, 2, 3)\n+    val data = Seq(vector, vector).map(Tuple1.apply).toDF(\"vector\")\n+\n+    val sizeHint = new VectorSizeHint()\n+      .setInputCol(\"vector\")\n+      .setHandleInvalid(\"optimistic\")\n+      .setSize(3)\n+\n+    val transformed = sizeHint.transform(data)"
  }],
  "prId": 19746
}, {
  "comments": [{
    "author": {
      "login": "WeichenXu123"
    },
    "body": "`case (data, transform)` ==> `case (data, transformer)`",
    "commit": "9c3dcec30d152ab2c5c242db03ed466ae1d320ae",
    "createdAt": "2017-11-28T08:22:43Z",
    "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.attribute.AttributeGroup\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.util.DefaultReadWriteTest\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.StreamTest\n+\n+class VectorSizeHintSuite\n+  extends SparkFunSuite with MLlibTestSparkContext with DefaultReadWriteTest {\n+\n+  import testImplicits._\n+\n+  test(\"Test Param Validators\") {\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setHandleInvalid(\"invalidValue\"))\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setSize(-3))\n+  }\n+\n+  test(\"Adding size to column of vectors.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val denseVector = Vectors.dense(1, 2, 3)\n+    val sparseVector = Vectors.sparse(size, Array(), Array())\n+\n+    val data = Seq(denseVector, denseVector, sparseVector).map(Tuple1.apply)\n+    val dataFrame = data.toDF(vectorColName)\n+    assert(\n+      AttributeGroup.fromStructField(dataFrame.schema(vectorColName)).size == -1,\n+      \"Transformer did not add expected size data.\")\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrame)\n+      assert(\n+        AttributeGroup.fromStructField(withSize.schema(vectorColName)).size == size,\n+        \"Transformer did not add expected size data.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size hint preserves attributes.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+    val group = AttributeGroup.fromStructField(dataFrameWithMetadata.schema(vectorColName))\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrameWithMetadata)\n+\n+      val newGroup = AttributeGroup.fromStructField(withSize.schema(vectorColName))\n+      assert(newGroup.size === size, \"Transformer did not add expected size data.\")\n+      assert(\n+        newGroup.attributes.get.deep === group.attributes.get.deep,\n+        \"SizeHintTransformer did not preserve attributes.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size miss-match between current and target size raises an error.\") {\n+    val size = 4\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      intercept[SparkException](transformer.transform(dataFrameWithMetadata))\n+    }\n+  }\n+\n+  test(\"Handle invalid does the right thing.\") {\n+\n+    val vector = Vectors.dense(1, 2, 3)\n+    val short = Vectors.dense(2)\n+    val dataWithNull = Seq(vector, null).map(Tuple1.apply).toDF(\"vector\")\n+    val dataWithShort = Seq(vector, short).map(Tuple1.apply).toDF(\"vector\")\n+\n+    val sizeHint = new VectorSizeHint()\n+      .setInputCol(\"vector\")\n+      .setHandleInvalid(\"error\")\n+      .setSize(3)\n+\n+    intercept[SparkException](sizeHint.transform(dataWithNull).collect)\n+    intercept[SparkException](sizeHint.transform(dataWithShort).collect)\n+\n+    sizeHint.setHandleInvalid(\"skip\")\n+    assert(sizeHint.transform(dataWithNull).count() === 1)\n+    assert(sizeHint.transform(dataWithShort).count() === 1)\n+  }\n+\n+  test(\"read/write\") {\n+    val sizeHint = new VectorSizeHint()\n+      .setInputCol(\"myInputCol\")\n+      .setSize(11)\n+      .setHandleInvalid(\"skip\")\n+    testDefaultReadWrite(sizeHint)\n+  }\n+}\n+\n+class VectorSizeHintStreamingSuite extends StreamTest {\n+\n+  import testImplicits._\n+\n+  test(\"Test assemble vectors with size hint in steaming.\") {\n+    val a = Vectors.dense(0, 1, 2)\n+    val b = Vectors.sparse(4, Array(0, 3), Array(3, 6))\n+\n+    val stream = MemoryStream[(Vector, Vector)]\n+    val streamingDF = stream.toDS.toDF(\"a\", \"b\")\n+    val sizeHintA = new VectorSizeHint()\n+      .setSize(3)\n+      .setInputCol(\"a\")\n+    val sizeHintB = new VectorSizeHint()\n+      .setSize(4)\n+      .setInputCol(\"b\")\n+    val vectorAssembler = new VectorAssembler()\n+      .setInputCols(Array(\"a\", \"b\"))\n+      .setOutputCol(\"assembled\")\n+    val output = Seq(sizeHintA, sizeHintB, vectorAssembler).foldLeft(streamingDF) {\n+      case (data, transform) => transform.transform(data)"
  }],
  "prId": 19746
}, {
  "comments": [{
    "author": {
      "login": "WeichenXu123"
    },
    "body": "Use `CheckAnswer(expected, expected)` will be simpler.",
    "commit": "9c3dcec30d152ab2c5c242db03ed466ae1d320ae",
    "createdAt": "2017-11-28T08:23:15Z",
    "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.attribute.AttributeGroup\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.util.DefaultReadWriteTest\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.StreamTest\n+\n+class VectorSizeHintSuite\n+  extends SparkFunSuite with MLlibTestSparkContext with DefaultReadWriteTest {\n+\n+  import testImplicits._\n+\n+  test(\"Test Param Validators\") {\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setHandleInvalid(\"invalidValue\"))\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setSize(-3))\n+  }\n+\n+  test(\"Adding size to column of vectors.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val denseVector = Vectors.dense(1, 2, 3)\n+    val sparseVector = Vectors.sparse(size, Array(), Array())\n+\n+    val data = Seq(denseVector, denseVector, sparseVector).map(Tuple1.apply)\n+    val dataFrame = data.toDF(vectorColName)\n+    assert(\n+      AttributeGroup.fromStructField(dataFrame.schema(vectorColName)).size == -1,\n+      \"Transformer did not add expected size data.\")\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrame)\n+      assert(\n+        AttributeGroup.fromStructField(withSize.schema(vectorColName)).size == size,\n+        \"Transformer did not add expected size data.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size hint preserves attributes.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+    val group = AttributeGroup.fromStructField(dataFrameWithMetadata.schema(vectorColName))\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrameWithMetadata)\n+\n+      val newGroup = AttributeGroup.fromStructField(withSize.schema(vectorColName))\n+      assert(newGroup.size === size, \"Transformer did not add expected size data.\")\n+      assert(\n+        newGroup.attributes.get.deep === group.attributes.get.deep,\n+        \"SizeHintTransformer did not preserve attributes.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size miss-match between current and target size raises an error.\") {\n+    val size = 4\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      intercept[SparkException](transformer.transform(dataFrameWithMetadata))\n+    }\n+  }\n+\n+  test(\"Handle invalid does the right thing.\") {\n+\n+    val vector = Vectors.dense(1, 2, 3)\n+    val short = Vectors.dense(2)\n+    val dataWithNull = Seq(vector, null).map(Tuple1.apply).toDF(\"vector\")\n+    val dataWithShort = Seq(vector, short).map(Tuple1.apply).toDF(\"vector\")\n+\n+    val sizeHint = new VectorSizeHint()\n+      .setInputCol(\"vector\")\n+      .setHandleInvalid(\"error\")\n+      .setSize(3)\n+\n+    intercept[SparkException](sizeHint.transform(dataWithNull).collect)\n+    intercept[SparkException](sizeHint.transform(dataWithShort).collect)\n+\n+    sizeHint.setHandleInvalid(\"skip\")\n+    assert(sizeHint.transform(dataWithNull).count() === 1)\n+    assert(sizeHint.transform(dataWithShort).count() === 1)\n+  }\n+\n+  test(\"read/write\") {\n+    val sizeHint = new VectorSizeHint()\n+      .setInputCol(\"myInputCol\")\n+      .setSize(11)\n+      .setHandleInvalid(\"skip\")\n+    testDefaultReadWrite(sizeHint)\n+  }\n+}\n+\n+class VectorSizeHintStreamingSuite extends StreamTest {\n+\n+  import testImplicits._\n+\n+  test(\"Test assemble vectors with size hint in steaming.\") {\n+    val a = Vectors.dense(0, 1, 2)\n+    val b = Vectors.sparse(4, Array(0, 3), Array(3, 6))\n+\n+    val stream = MemoryStream[(Vector, Vector)]\n+    val streamingDF = stream.toDS.toDF(\"a\", \"b\")\n+    val sizeHintA = new VectorSizeHint()\n+      .setSize(3)\n+      .setInputCol(\"a\")\n+    val sizeHintB = new VectorSizeHint()\n+      .setSize(4)\n+      .setInputCol(\"b\")\n+    val vectorAssembler = new VectorAssembler()\n+      .setInputCols(Array(\"a\", \"b\"))\n+      .setOutputCol(\"assembled\")\n+    val output = Seq(sizeHintA, sizeHintB, vectorAssembler).foldLeft(streamingDF) {\n+      case (data, transform) => transform.transform(data)\n+    }.select(\"assembled\")\n+\n+    val expected = Vectors.dense(0, 1, 2, 3, 0, 0, 6)\n+\n+    testStream (output) (\n+      AddData(stream, (a, b), (a, b)),\n+      CheckAnswerRows(Seq(Row(expected), Row(expected)), false, false)"
  }, {
    "author": {
      "login": "MrBago"
    },
    "body": "The reason I didn't use `CheckAnswer` is because there isn't an implicit encoder in `testImplicits` that handles `Vector`. I tried `CheckAnswer[Vector](expected, expected)` but that doesn't work either :(. Is there an encoder that works for Vectors?",
    "commit": "9c3dcec30d152ab2c5c242db03ed466ae1d320ae",
    "createdAt": "2017-12-05T00:23:09Z",
    "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.attribute.AttributeGroup\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.util.DefaultReadWriteTest\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.StreamTest\n+\n+class VectorSizeHintSuite\n+  extends SparkFunSuite with MLlibTestSparkContext with DefaultReadWriteTest {\n+\n+  import testImplicits._\n+\n+  test(\"Test Param Validators\") {\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setHandleInvalid(\"invalidValue\"))\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setSize(-3))\n+  }\n+\n+  test(\"Adding size to column of vectors.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val denseVector = Vectors.dense(1, 2, 3)\n+    val sparseVector = Vectors.sparse(size, Array(), Array())\n+\n+    val data = Seq(denseVector, denseVector, sparseVector).map(Tuple1.apply)\n+    val dataFrame = data.toDF(vectorColName)\n+    assert(\n+      AttributeGroup.fromStructField(dataFrame.schema(vectorColName)).size == -1,\n+      \"Transformer did not add expected size data.\")\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrame)\n+      assert(\n+        AttributeGroup.fromStructField(withSize.schema(vectorColName)).size == size,\n+        \"Transformer did not add expected size data.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size hint preserves attributes.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+    val group = AttributeGroup.fromStructField(dataFrameWithMetadata.schema(vectorColName))\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrameWithMetadata)\n+\n+      val newGroup = AttributeGroup.fromStructField(withSize.schema(vectorColName))\n+      assert(newGroup.size === size, \"Transformer did not add expected size data.\")\n+      assert(\n+        newGroup.attributes.get.deep === group.attributes.get.deep,\n+        \"SizeHintTransformer did not preserve attributes.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size miss-match between current and target size raises an error.\") {\n+    val size = 4\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      intercept[SparkException](transformer.transform(dataFrameWithMetadata))\n+    }\n+  }\n+\n+  test(\"Handle invalid does the right thing.\") {\n+\n+    val vector = Vectors.dense(1, 2, 3)\n+    val short = Vectors.dense(2)\n+    val dataWithNull = Seq(vector, null).map(Tuple1.apply).toDF(\"vector\")\n+    val dataWithShort = Seq(vector, short).map(Tuple1.apply).toDF(\"vector\")\n+\n+    val sizeHint = new VectorSizeHint()\n+      .setInputCol(\"vector\")\n+      .setHandleInvalid(\"error\")\n+      .setSize(3)\n+\n+    intercept[SparkException](sizeHint.transform(dataWithNull).collect)\n+    intercept[SparkException](sizeHint.transform(dataWithShort).collect)\n+\n+    sizeHint.setHandleInvalid(\"skip\")\n+    assert(sizeHint.transform(dataWithNull).count() === 1)\n+    assert(sizeHint.transform(dataWithShort).count() === 1)\n+  }\n+\n+  test(\"read/write\") {\n+    val sizeHint = new VectorSizeHint()\n+      .setInputCol(\"myInputCol\")\n+      .setSize(11)\n+      .setHandleInvalid(\"skip\")\n+    testDefaultReadWrite(sizeHint)\n+  }\n+}\n+\n+class VectorSizeHintStreamingSuite extends StreamTest {\n+\n+  import testImplicits._\n+\n+  test(\"Test assemble vectors with size hint in steaming.\") {\n+    val a = Vectors.dense(0, 1, 2)\n+    val b = Vectors.sparse(4, Array(0, 3), Array(3, 6))\n+\n+    val stream = MemoryStream[(Vector, Vector)]\n+    val streamingDF = stream.toDS.toDF(\"a\", \"b\")\n+    val sizeHintA = new VectorSizeHint()\n+      .setSize(3)\n+      .setInputCol(\"a\")\n+    val sizeHintB = new VectorSizeHint()\n+      .setSize(4)\n+      .setInputCol(\"b\")\n+    val vectorAssembler = new VectorAssembler()\n+      .setInputCols(Array(\"a\", \"b\"))\n+      .setOutputCol(\"assembled\")\n+    val output = Seq(sizeHintA, sizeHintB, vectorAssembler).foldLeft(streamingDF) {\n+      case (data, transform) => transform.transform(data)\n+    }.select(\"assembled\")\n+\n+    val expected = Vectors.dense(0, 1, 2, 3, 0, 0, 6)\n+\n+    testStream (output) (\n+      AddData(stream, (a, b), (a, b)),\n+      CheckAnswerRows(Seq(Row(expected), Row(expected)), false, false)"
  }, {
    "author": {
      "login": "WeichenXu123"
    },
    "body": "ah, sorry, it should be `CheckAnswer(Tuple1(expected), Tuple1(expected))`. It should work I think.",
    "commit": "9c3dcec30d152ab2c5c242db03ed466ae1d320ae",
    "createdAt": "2017-12-05T00:56:20Z",
    "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.attribute.AttributeGroup\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.util.DefaultReadWriteTest\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.StreamTest\n+\n+class VectorSizeHintSuite\n+  extends SparkFunSuite with MLlibTestSparkContext with DefaultReadWriteTest {\n+\n+  import testImplicits._\n+\n+  test(\"Test Param Validators\") {\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setHandleInvalid(\"invalidValue\"))\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setSize(-3))\n+  }\n+\n+  test(\"Adding size to column of vectors.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val denseVector = Vectors.dense(1, 2, 3)\n+    val sparseVector = Vectors.sparse(size, Array(), Array())\n+\n+    val data = Seq(denseVector, denseVector, sparseVector).map(Tuple1.apply)\n+    val dataFrame = data.toDF(vectorColName)\n+    assert(\n+      AttributeGroup.fromStructField(dataFrame.schema(vectorColName)).size == -1,\n+      \"Transformer did not add expected size data.\")\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrame)\n+      assert(\n+        AttributeGroup.fromStructField(withSize.schema(vectorColName)).size == size,\n+        \"Transformer did not add expected size data.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size hint preserves attributes.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+    val group = AttributeGroup.fromStructField(dataFrameWithMetadata.schema(vectorColName))\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrameWithMetadata)\n+\n+      val newGroup = AttributeGroup.fromStructField(withSize.schema(vectorColName))\n+      assert(newGroup.size === size, \"Transformer did not add expected size data.\")\n+      assert(\n+        newGroup.attributes.get.deep === group.attributes.get.deep,\n+        \"SizeHintTransformer did not preserve attributes.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size miss-match between current and target size raises an error.\") {\n+    val size = 4\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      intercept[SparkException](transformer.transform(dataFrameWithMetadata))\n+    }\n+  }\n+\n+  test(\"Handle invalid does the right thing.\") {\n+\n+    val vector = Vectors.dense(1, 2, 3)\n+    val short = Vectors.dense(2)\n+    val dataWithNull = Seq(vector, null).map(Tuple1.apply).toDF(\"vector\")\n+    val dataWithShort = Seq(vector, short).map(Tuple1.apply).toDF(\"vector\")\n+\n+    val sizeHint = new VectorSizeHint()\n+      .setInputCol(\"vector\")\n+      .setHandleInvalid(\"error\")\n+      .setSize(3)\n+\n+    intercept[SparkException](sizeHint.transform(dataWithNull).collect)\n+    intercept[SparkException](sizeHint.transform(dataWithShort).collect)\n+\n+    sizeHint.setHandleInvalid(\"skip\")\n+    assert(sizeHint.transform(dataWithNull).count() === 1)\n+    assert(sizeHint.transform(dataWithShort).count() === 1)\n+  }\n+\n+  test(\"read/write\") {\n+    val sizeHint = new VectorSizeHint()\n+      .setInputCol(\"myInputCol\")\n+      .setSize(11)\n+      .setHandleInvalid(\"skip\")\n+    testDefaultReadWrite(sizeHint)\n+  }\n+}\n+\n+class VectorSizeHintStreamingSuite extends StreamTest {\n+\n+  import testImplicits._\n+\n+  test(\"Test assemble vectors with size hint in steaming.\") {\n+    val a = Vectors.dense(0, 1, 2)\n+    val b = Vectors.sparse(4, Array(0, 3), Array(3, 6))\n+\n+    val stream = MemoryStream[(Vector, Vector)]\n+    val streamingDF = stream.toDS.toDF(\"a\", \"b\")\n+    val sizeHintA = new VectorSizeHint()\n+      .setSize(3)\n+      .setInputCol(\"a\")\n+    val sizeHintB = new VectorSizeHint()\n+      .setSize(4)\n+      .setInputCol(\"b\")\n+    val vectorAssembler = new VectorAssembler()\n+      .setInputCols(Array(\"a\", \"b\"))\n+      .setOutputCol(\"assembled\")\n+    val output = Seq(sizeHintA, sizeHintB, vectorAssembler).foldLeft(streamingDF) {\n+      case (data, transform) => transform.transform(data)\n+    }.select(\"assembled\")\n+\n+    val expected = Vectors.dense(0, 1, 2, 3, 0, 0, 6)\n+\n+    testStream (output) (\n+      AddData(stream, (a, b), (a, b)),\n+      CheckAnswerRows(Seq(Row(expected), Row(expected)), false, false)"
  }],
  "prId": 19746
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "incorrect error message",
    "commit": "9c3dcec30d152ab2c5c242db03ed466ae1d320ae",
    "createdAt": "2017-12-08T19:49:35Z",
    "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.attribute.AttributeGroup\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.util.DefaultReadWriteTest\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.StreamTest\n+\n+class VectorSizeHintSuite\n+  extends SparkFunSuite with MLlibTestSparkContext with DefaultReadWriteTest {\n+\n+  import testImplicits._\n+\n+  test(\"Test Param Validators\") {\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setHandleInvalid(\"invalidValue\"))\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setSize(-3))\n+  }\n+\n+  test(\"Adding size to column of vectors.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val denseVector = Vectors.dense(1, 2, 3)\n+    val sparseVector = Vectors.sparse(size, Array(), Array())\n+\n+    val data = Seq(denseVector, denseVector, sparseVector).map(Tuple1.apply)\n+    val dataFrame = data.toDF(vectorColName)\n+    assert(\n+      AttributeGroup.fromStructField(dataFrame.schema(vectorColName)).size == -1,\n+      \"Transformer did not add expected size data.\")"
  }],
  "prId": 19746
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Might as well assert ```withSize.collect().length === 3``` to make sure no Rows were incorrectly filtered",
    "commit": "9c3dcec30d152ab2c5c242db03ed466ae1d320ae",
    "createdAt": "2017-12-08T19:51:11Z",
    "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.attribute.AttributeGroup\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.util.DefaultReadWriteTest\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.StreamTest\n+\n+class VectorSizeHintSuite\n+  extends SparkFunSuite with MLlibTestSparkContext with DefaultReadWriteTest {\n+\n+  import testImplicits._\n+\n+  test(\"Test Param Validators\") {\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setHandleInvalid(\"invalidValue\"))\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setSize(-3))\n+  }\n+\n+  test(\"Adding size to column of vectors.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val denseVector = Vectors.dense(1, 2, 3)\n+    val sparseVector = Vectors.sparse(size, Array(), Array())\n+\n+    val data = Seq(denseVector, denseVector, sparseVector).map(Tuple1.apply)\n+    val dataFrame = data.toDF(vectorColName)\n+    assert(\n+      AttributeGroup.fromStructField(dataFrame.schema(vectorColName)).size == -1,\n+      \"Transformer did not add expected size data.\")\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrame)\n+      assert(\n+        AttributeGroup.fromStructField(withSize.schema(vectorColName)).size == size,\n+        \"Transformer did not add expected size data.\")\n+      withSize.collect"
  }],
  "prId": 19746
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "misleading error message",
    "commit": "9c3dcec30d152ab2c5c242db03ed466ae1d320ae",
    "createdAt": "2017-12-08T19:56:09Z",
    "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.attribute.AttributeGroup\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.util.DefaultReadWriteTest\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.StreamTest\n+\n+class VectorSizeHintSuite\n+  extends SparkFunSuite with MLlibTestSparkContext with DefaultReadWriteTest {\n+\n+  import testImplicits._\n+\n+  test(\"Test Param Validators\") {\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setHandleInvalid(\"invalidValue\"))\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setSize(-3))\n+  }\n+\n+  test(\"Adding size to column of vectors.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val denseVector = Vectors.dense(1, 2, 3)\n+    val sparseVector = Vectors.sparse(size, Array(), Array())\n+\n+    val data = Seq(denseVector, denseVector, sparseVector).map(Tuple1.apply)\n+    val dataFrame = data.toDF(vectorColName)\n+    assert(\n+      AttributeGroup.fromStructField(dataFrame.schema(vectorColName)).size == -1,\n+      \"Transformer did not add expected size data.\")\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrame)\n+      assert(\n+        AttributeGroup.fromStructField(withSize.schema(vectorColName)).size == size,\n+        \"Transformer did not add expected size data.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size hint preserves attributes.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+    val group = AttributeGroup.fromStructField(dataFrameWithMetadata.schema(vectorColName))\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrameWithMetadata)\n+\n+      val newGroup = AttributeGroup.fromStructField(withSize.schema(vectorColName))\n+      assert(newGroup.size === size, \"Transformer did not add expected size data.\")"
  }],
  "prId": 19746
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "nit: Triple-equals actually calls .deep under the hood",
    "commit": "9c3dcec30d152ab2c5c242db03ed466ae1d320ae",
    "createdAt": "2017-12-08T19:58:15Z",
    "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.attribute.AttributeGroup\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.util.DefaultReadWriteTest\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.StreamTest\n+\n+class VectorSizeHintSuite\n+  extends SparkFunSuite with MLlibTestSparkContext with DefaultReadWriteTest {\n+\n+  import testImplicits._\n+\n+  test(\"Test Param Validators\") {\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setHandleInvalid(\"invalidValue\"))\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setSize(-3))\n+  }\n+\n+  test(\"Adding size to column of vectors.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val denseVector = Vectors.dense(1, 2, 3)\n+    val sparseVector = Vectors.sparse(size, Array(), Array())\n+\n+    val data = Seq(denseVector, denseVector, sparseVector).map(Tuple1.apply)\n+    val dataFrame = data.toDF(vectorColName)\n+    assert(\n+      AttributeGroup.fromStructField(dataFrame.schema(vectorColName)).size == -1,\n+      \"Transformer did not add expected size data.\")\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrame)\n+      assert(\n+        AttributeGroup.fromStructField(withSize.schema(vectorColName)).size == size,\n+        \"Transformer did not add expected size data.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size hint preserves attributes.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+    val group = AttributeGroup.fromStructField(dataFrameWithMetadata.schema(vectorColName))\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrameWithMetadata)\n+\n+      val newGroup = AttributeGroup.fromStructField(withSize.schema(vectorColName))\n+      assert(newGroup.size === size, \"Transformer did not add expected size data.\")\n+      assert(\n+        newGroup.attributes.get.deep === group.attributes.get.deep,"
  }],
  "prId": 19746
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "SizeHintTransformer -> VectorSizeHint",
    "commit": "9c3dcec30d152ab2c5c242db03ed466ae1d320ae",
    "createdAt": "2017-12-08T19:59:51Z",
    "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.attribute.AttributeGroup\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.util.DefaultReadWriteTest\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.StreamTest\n+\n+class VectorSizeHintSuite\n+  extends SparkFunSuite with MLlibTestSparkContext with DefaultReadWriteTest {\n+\n+  import testImplicits._\n+\n+  test(\"Test Param Validators\") {\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setHandleInvalid(\"invalidValue\"))\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setSize(-3))\n+  }\n+\n+  test(\"Adding size to column of vectors.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val denseVector = Vectors.dense(1, 2, 3)\n+    val sparseVector = Vectors.sparse(size, Array(), Array())\n+\n+    val data = Seq(denseVector, denseVector, sparseVector).map(Tuple1.apply)\n+    val dataFrame = data.toDF(vectorColName)\n+    assert(\n+      AttributeGroup.fromStructField(dataFrame.schema(vectorColName)).size == -1,\n+      \"Transformer did not add expected size data.\")\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrame)\n+      assert(\n+        AttributeGroup.fromStructField(withSize.schema(vectorColName)).size == size,\n+        \"Transformer did not add expected size data.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size hint preserves attributes.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+    val group = AttributeGroup.fromStructField(dataFrameWithMetadata.schema(vectorColName))\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrameWithMetadata)\n+\n+      val newGroup = AttributeGroup.fromStructField(withSize.schema(vectorColName))\n+      assert(newGroup.size === size, \"Transformer did not add expected size data.\")\n+      assert(\n+        newGroup.attributes.get.deep === group.attributes.get.deep,\n+        \"SizeHintTransformer did not preserve attributes.\")"
  }],
  "prId": 19746
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "typo: mismatch",
    "commit": "9c3dcec30d152ab2c5c242db03ed466ae1d320ae",
    "createdAt": "2017-12-08T20:00:02Z",
    "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.attribute.AttributeGroup\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.util.DefaultReadWriteTest\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.StreamTest\n+\n+class VectorSizeHintSuite\n+  extends SparkFunSuite with MLlibTestSparkContext with DefaultReadWriteTest {\n+\n+  import testImplicits._\n+\n+  test(\"Test Param Validators\") {\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setHandleInvalid(\"invalidValue\"))\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setSize(-3))\n+  }\n+\n+  test(\"Adding size to column of vectors.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val denseVector = Vectors.dense(1, 2, 3)\n+    val sparseVector = Vectors.sparse(size, Array(), Array())\n+\n+    val data = Seq(denseVector, denseVector, sparseVector).map(Tuple1.apply)\n+    val dataFrame = data.toDF(vectorColName)\n+    assert(\n+      AttributeGroup.fromStructField(dataFrame.schema(vectorColName)).size == -1,\n+      \"Transformer did not add expected size data.\")\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrame)\n+      assert(\n+        AttributeGroup.fromStructField(withSize.schema(vectorColName)).size == size,\n+        \"Transformer did not add expected size data.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size hint preserves attributes.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+    val group = AttributeGroup.fromStructField(dataFrameWithMetadata.schema(vectorColName))\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrameWithMetadata)\n+\n+      val newGroup = AttributeGroup.fromStructField(withSize.schema(vectorColName))\n+      assert(newGroup.size === size, \"Transformer did not add expected size data.\")\n+      assert(\n+        newGroup.attributes.get.deep === group.attributes.get.deep,\n+        \"SizeHintTransformer did not preserve attributes.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size miss-match between current and target size raises an error.\") {"
  }],
  "prId": 19746
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "style nit: Call collect() with parentheses",
    "commit": "9c3dcec30d152ab2c5c242db03ed466ae1d320ae",
    "createdAt": "2017-12-08T20:02:32Z",
    "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.attribute.AttributeGroup\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.util.DefaultReadWriteTest\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.StreamTest\n+\n+class VectorSizeHintSuite\n+  extends SparkFunSuite with MLlibTestSparkContext with DefaultReadWriteTest {\n+\n+  import testImplicits._\n+\n+  test(\"Test Param Validators\") {\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setHandleInvalid(\"invalidValue\"))\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setSize(-3))\n+  }\n+\n+  test(\"Adding size to column of vectors.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val denseVector = Vectors.dense(1, 2, 3)\n+    val sparseVector = Vectors.sparse(size, Array(), Array())\n+\n+    val data = Seq(denseVector, denseVector, sparseVector).map(Tuple1.apply)\n+    val dataFrame = data.toDF(vectorColName)\n+    assert(\n+      AttributeGroup.fromStructField(dataFrame.schema(vectorColName)).size == -1,\n+      \"Transformer did not add expected size data.\")\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrame)\n+      assert(\n+        AttributeGroup.fromStructField(withSize.schema(vectorColName)).size == size,\n+        \"Transformer did not add expected size data.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size hint preserves attributes.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+    val group = AttributeGroup.fromStructField(dataFrameWithMetadata.schema(vectorColName))\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrameWithMetadata)\n+\n+      val newGroup = AttributeGroup.fromStructField(withSize.schema(vectorColName))\n+      assert(newGroup.size === size, \"Transformer did not add expected size data.\")\n+      assert(\n+        newGroup.attributes.get.deep === group.attributes.get.deep,\n+        \"SizeHintTransformer did not preserve attributes.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size miss-match between current and target size raises an error.\") {\n+    val size = 4\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      intercept[SparkException](transformer.transform(dataFrameWithMetadata))\n+    }\n+  }\n+\n+  test(\"Handle invalid does the right thing.\") {\n+\n+    val vector = Vectors.dense(1, 2, 3)\n+    val short = Vectors.dense(2)\n+    val dataWithNull = Seq(vector, null).map(Tuple1.apply).toDF(\"vector\")\n+    val dataWithShort = Seq(vector, short).map(Tuple1.apply).toDF(\"vector\")\n+\n+    val sizeHint = new VectorSizeHint()\n+      .setInputCol(\"vector\")\n+      .setHandleInvalid(\"error\")\n+      .setSize(3)\n+\n+    intercept[SparkException](sizeHint.transform(dataWithNull).collect)"
  }],
  "prId": 19746
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Test keep/optimistic too",
    "commit": "9c3dcec30d152ab2c5c242db03ed466ae1d320ae",
    "createdAt": "2017-12-08T20:03:19Z",
    "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.attribute.AttributeGroup\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.util.DefaultReadWriteTest\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.StreamTest\n+\n+class VectorSizeHintSuite\n+  extends SparkFunSuite with MLlibTestSparkContext with DefaultReadWriteTest {\n+\n+  import testImplicits._\n+\n+  test(\"Test Param Validators\") {\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setHandleInvalid(\"invalidValue\"))\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setSize(-3))\n+  }\n+\n+  test(\"Adding size to column of vectors.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val denseVector = Vectors.dense(1, 2, 3)\n+    val sparseVector = Vectors.sparse(size, Array(), Array())\n+\n+    val data = Seq(denseVector, denseVector, sparseVector).map(Tuple1.apply)\n+    val dataFrame = data.toDF(vectorColName)\n+    assert(\n+      AttributeGroup.fromStructField(dataFrame.schema(vectorColName)).size == -1,\n+      \"Transformer did not add expected size data.\")\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrame)\n+      assert(\n+        AttributeGroup.fromStructField(withSize.schema(vectorColName)).size == size,\n+        \"Transformer did not add expected size data.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size hint preserves attributes.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+    val group = AttributeGroup.fromStructField(dataFrameWithMetadata.schema(vectorColName))\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrameWithMetadata)\n+\n+      val newGroup = AttributeGroup.fromStructField(withSize.schema(vectorColName))\n+      assert(newGroup.size === size, \"Transformer did not add expected size data.\")\n+      assert(\n+        newGroup.attributes.get.deep === group.attributes.get.deep,\n+        \"SizeHintTransformer did not preserve attributes.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size miss-match between current and target size raises an error.\") {\n+    val size = 4\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      intercept[SparkException](transformer.transform(dataFrameWithMetadata))\n+    }\n+  }\n+\n+  test(\"Handle invalid does the right thing.\") {\n+\n+    val vector = Vectors.dense(1, 2, 3)\n+    val short = Vectors.dense(2)\n+    val dataWithNull = Seq(vector, null).map(Tuple1.apply).toDF(\"vector\")\n+    val dataWithShort = Seq(vector, short).map(Tuple1.apply).toDF(\"vector\")\n+\n+    val sizeHint = new VectorSizeHint()\n+      .setInputCol(\"vector\")\n+      .setHandleInvalid(\"error\")\n+      .setSize(3)\n+\n+    intercept[SparkException](sizeHint.transform(dataWithNull).collect)\n+    intercept[SparkException](sizeHint.transform(dataWithShort).collect)\n+\n+    sizeHint.setHandleInvalid(\"skip\")\n+    assert(sizeHint.transform(dataWithNull).count() === 1)\n+    assert(sizeHint.transform(dataWithShort).count() === 1)\n+  }"
  }, {
    "author": {
      "login": "MrBago"
    },
    "body": "Did you a thought on how to test `keep`/`optimistic`. I could verify that the invalid data is not removed but that's a little bit weird to test. It's ensuring that this option allows the column to get into a \"bad state\" where the metadata doesn't match the contents. Is that what you had in mind?",
    "commit": "9c3dcec30d152ab2c5c242db03ed466ae1d320ae",
    "createdAt": "2017-12-13T18:58:38Z",
    "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.attribute.AttributeGroup\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.util.DefaultReadWriteTest\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.StreamTest\n+\n+class VectorSizeHintSuite\n+  extends SparkFunSuite with MLlibTestSparkContext with DefaultReadWriteTest {\n+\n+  import testImplicits._\n+\n+  test(\"Test Param Validators\") {\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setHandleInvalid(\"invalidValue\"))\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setSize(-3))\n+  }\n+\n+  test(\"Adding size to column of vectors.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val denseVector = Vectors.dense(1, 2, 3)\n+    val sparseVector = Vectors.sparse(size, Array(), Array())\n+\n+    val data = Seq(denseVector, denseVector, sparseVector).map(Tuple1.apply)\n+    val dataFrame = data.toDF(vectorColName)\n+    assert(\n+      AttributeGroup.fromStructField(dataFrame.schema(vectorColName)).size == -1,\n+      \"Transformer did not add expected size data.\")\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrame)\n+      assert(\n+        AttributeGroup.fromStructField(withSize.schema(vectorColName)).size == size,\n+        \"Transformer did not add expected size data.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size hint preserves attributes.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+    val group = AttributeGroup.fromStructField(dataFrameWithMetadata.schema(vectorColName))\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrameWithMetadata)\n+\n+      val newGroup = AttributeGroup.fromStructField(withSize.schema(vectorColName))\n+      assert(newGroup.size === size, \"Transformer did not add expected size data.\")\n+      assert(\n+        newGroup.attributes.get.deep === group.attributes.get.deep,\n+        \"SizeHintTransformer did not preserve attributes.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size miss-match between current and target size raises an error.\") {\n+    val size = 4\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      intercept[SparkException](transformer.transform(dataFrameWithMetadata))\n+    }\n+  }\n+\n+  test(\"Handle invalid does the right thing.\") {\n+\n+    val vector = Vectors.dense(1, 2, 3)\n+    val short = Vectors.dense(2)\n+    val dataWithNull = Seq(vector, null).map(Tuple1.apply).toDF(\"vector\")\n+    val dataWithShort = Seq(vector, short).map(Tuple1.apply).toDF(\"vector\")\n+\n+    val sizeHint = new VectorSizeHint()\n+      .setInputCol(\"vector\")\n+      .setHandleInvalid(\"error\")\n+      .setSize(3)\n+\n+    intercept[SparkException](sizeHint.transform(dataWithNull).collect)\n+    intercept[SparkException](sizeHint.transform(dataWithShort).collect)\n+\n+    sizeHint.setHandleInvalid(\"skip\")\n+    assert(sizeHint.transform(dataWithNull).count() === 1)\n+    assert(sizeHint.transform(dataWithShort).count() === 1)\n+  }"
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "Yep, that's what I had in mind.  That is the expected behavior, so we can test that behavior...even if it's not what most use cases would need.",
    "commit": "9c3dcec30d152ab2c5c242db03ed466ae1d320ae",
    "createdAt": "2017-12-13T21:42:24Z",
    "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.attribute.AttributeGroup\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.util.DefaultReadWriteTest\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.StreamTest\n+\n+class VectorSizeHintSuite\n+  extends SparkFunSuite with MLlibTestSparkContext with DefaultReadWriteTest {\n+\n+  import testImplicits._\n+\n+  test(\"Test Param Validators\") {\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setHandleInvalid(\"invalidValue\"))\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setSize(-3))\n+  }\n+\n+  test(\"Adding size to column of vectors.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val denseVector = Vectors.dense(1, 2, 3)\n+    val sparseVector = Vectors.sparse(size, Array(), Array())\n+\n+    val data = Seq(denseVector, denseVector, sparseVector).map(Tuple1.apply)\n+    val dataFrame = data.toDF(vectorColName)\n+    assert(\n+      AttributeGroup.fromStructField(dataFrame.schema(vectorColName)).size == -1,\n+      \"Transformer did not add expected size data.\")\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrame)\n+      assert(\n+        AttributeGroup.fromStructField(withSize.schema(vectorColName)).size == size,\n+        \"Transformer did not add expected size data.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size hint preserves attributes.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+    val group = AttributeGroup.fromStructField(dataFrameWithMetadata.schema(vectorColName))\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrameWithMetadata)\n+\n+      val newGroup = AttributeGroup.fromStructField(withSize.schema(vectorColName))\n+      assert(newGroup.size === size, \"Transformer did not add expected size data.\")\n+      assert(\n+        newGroup.attributes.get.deep === group.attributes.get.deep,\n+        \"SizeHintTransformer did not preserve attributes.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size miss-match between current and target size raises an error.\") {\n+    val size = 4\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      intercept[SparkException](transformer.transform(dataFrameWithMetadata))\n+    }\n+  }\n+\n+  test(\"Handle invalid does the right thing.\") {\n+\n+    val vector = Vectors.dense(1, 2, 3)\n+    val short = Vectors.dense(2)\n+    val dataWithNull = Seq(vector, null).map(Tuple1.apply).toDF(\"vector\")\n+    val dataWithShort = Seq(vector, short).map(Tuple1.apply).toDF(\"vector\")\n+\n+    val sizeHint = new VectorSizeHint()\n+      .setInputCol(\"vector\")\n+      .setHandleInvalid(\"error\")\n+      .setSize(3)\n+\n+    intercept[SparkException](sizeHint.transform(dataWithNull).collect)\n+    intercept[SparkException](sizeHint.transform(dataWithShort).collect)\n+\n+    sizeHint.setHandleInvalid(\"skip\")\n+    assert(sizeHint.transform(dataWithNull).count() === 1)\n+    assert(sizeHint.transform(dataWithShort).count() === 1)\n+  }"
  }],
  "prId": 19746
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "steaming streaming",
    "commit": "9c3dcec30d152ab2c5c242db03ed466ae1d320ae",
    "createdAt": "2017-12-08T20:03:52Z",
    "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.attribute.AttributeGroup\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.util.DefaultReadWriteTest\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.StreamTest\n+\n+class VectorSizeHintSuite\n+  extends SparkFunSuite with MLlibTestSparkContext with DefaultReadWriteTest {\n+\n+  import testImplicits._\n+\n+  test(\"Test Param Validators\") {\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setHandleInvalid(\"invalidValue\"))\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setSize(-3))\n+  }\n+\n+  test(\"Adding size to column of vectors.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val denseVector = Vectors.dense(1, 2, 3)\n+    val sparseVector = Vectors.sparse(size, Array(), Array())\n+\n+    val data = Seq(denseVector, denseVector, sparseVector).map(Tuple1.apply)\n+    val dataFrame = data.toDF(vectorColName)\n+    assert(\n+      AttributeGroup.fromStructField(dataFrame.schema(vectorColName)).size == -1,\n+      \"Transformer did not add expected size data.\")\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrame)\n+      assert(\n+        AttributeGroup.fromStructField(withSize.schema(vectorColName)).size == size,\n+        \"Transformer did not add expected size data.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size hint preserves attributes.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+    val group = AttributeGroup.fromStructField(dataFrameWithMetadata.schema(vectorColName))\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrameWithMetadata)\n+\n+      val newGroup = AttributeGroup.fromStructField(withSize.schema(vectorColName))\n+      assert(newGroup.size === size, \"Transformer did not add expected size data.\")\n+      assert(\n+        newGroup.attributes.get.deep === group.attributes.get.deep,\n+        \"SizeHintTransformer did not preserve attributes.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size miss-match between current and target size raises an error.\") {\n+    val size = 4\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      intercept[SparkException](transformer.transform(dataFrameWithMetadata))\n+    }\n+  }\n+\n+  test(\"Handle invalid does the right thing.\") {\n+\n+    val vector = Vectors.dense(1, 2, 3)\n+    val short = Vectors.dense(2)\n+    val dataWithNull = Seq(vector, null).map(Tuple1.apply).toDF(\"vector\")\n+    val dataWithShort = Seq(vector, short).map(Tuple1.apply).toDF(\"vector\")\n+\n+    val sizeHint = new VectorSizeHint()\n+      .setInputCol(\"vector\")\n+      .setHandleInvalid(\"error\")\n+      .setSize(3)\n+\n+    intercept[SparkException](sizeHint.transform(dataWithNull).collect)\n+    intercept[SparkException](sizeHint.transform(dataWithShort).collect)\n+\n+    sizeHint.setHandleInvalid(\"skip\")\n+    assert(sizeHint.transform(dataWithNull).count() === 1)\n+    assert(sizeHint.transform(dataWithShort).count() === 1)\n+  }\n+\n+  test(\"read/write\") {\n+    val sizeHint = new VectorSizeHint()\n+      .setInputCol(\"myInputCol\")\n+      .setSize(11)\n+      .setHandleInvalid(\"skip\")\n+    testDefaultReadWrite(sizeHint)\n+  }\n+}\n+\n+class VectorSizeHintStreamingSuite extends StreamTest {\n+\n+  import testImplicits._\n+\n+  test(\"Test assemble vectors with size hint in steaming.\") {"
  }],
  "prId": 19746
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "You can just put these in a PipelineModel to avoid using foldLeft.",
    "commit": "9c3dcec30d152ab2c5c242db03ed466ae1d320ae",
    "createdAt": "2017-12-08T20:05:23Z",
    "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.attribute.AttributeGroup\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.util.DefaultReadWriteTest\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.StreamTest\n+\n+class VectorSizeHintSuite\n+  extends SparkFunSuite with MLlibTestSparkContext with DefaultReadWriteTest {\n+\n+  import testImplicits._\n+\n+  test(\"Test Param Validators\") {\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setHandleInvalid(\"invalidValue\"))\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setSize(-3))\n+  }\n+\n+  test(\"Adding size to column of vectors.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val denseVector = Vectors.dense(1, 2, 3)\n+    val sparseVector = Vectors.sparse(size, Array(), Array())\n+\n+    val data = Seq(denseVector, denseVector, sparseVector).map(Tuple1.apply)\n+    val dataFrame = data.toDF(vectorColName)\n+    assert(\n+      AttributeGroup.fromStructField(dataFrame.schema(vectorColName)).size == -1,\n+      \"Transformer did not add expected size data.\")\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrame)\n+      assert(\n+        AttributeGroup.fromStructField(withSize.schema(vectorColName)).size == size,\n+        \"Transformer did not add expected size data.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size hint preserves attributes.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+    val group = AttributeGroup.fromStructField(dataFrameWithMetadata.schema(vectorColName))\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrameWithMetadata)\n+\n+      val newGroup = AttributeGroup.fromStructField(withSize.schema(vectorColName))\n+      assert(newGroup.size === size, \"Transformer did not add expected size data.\")\n+      assert(\n+        newGroup.attributes.get.deep === group.attributes.get.deep,\n+        \"SizeHintTransformer did not preserve attributes.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size miss-match between current and target size raises an error.\") {\n+    val size = 4\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      intercept[SparkException](transformer.transform(dataFrameWithMetadata))\n+    }\n+  }\n+\n+  test(\"Handle invalid does the right thing.\") {\n+\n+    val vector = Vectors.dense(1, 2, 3)\n+    val short = Vectors.dense(2)\n+    val dataWithNull = Seq(vector, null).map(Tuple1.apply).toDF(\"vector\")\n+    val dataWithShort = Seq(vector, short).map(Tuple1.apply).toDF(\"vector\")\n+\n+    val sizeHint = new VectorSizeHint()\n+      .setInputCol(\"vector\")\n+      .setHandleInvalid(\"error\")\n+      .setSize(3)\n+\n+    intercept[SparkException](sizeHint.transform(dataWithNull).collect)\n+    intercept[SparkException](sizeHint.transform(dataWithShort).collect)\n+\n+    sizeHint.setHandleInvalid(\"skip\")\n+    assert(sizeHint.transform(dataWithNull).count() === 1)\n+    assert(sizeHint.transform(dataWithShort).count() === 1)\n+  }\n+\n+  test(\"read/write\") {\n+    val sizeHint = new VectorSizeHint()\n+      .setInputCol(\"myInputCol\")\n+      .setSize(11)\n+      .setHandleInvalid(\"skip\")\n+    testDefaultReadWrite(sizeHint)\n+  }\n+}\n+\n+class VectorSizeHintStreamingSuite extends StreamTest {\n+\n+  import testImplicits._\n+\n+  test(\"Test assemble vectors with size hint in steaming.\") {\n+    val a = Vectors.dense(0, 1, 2)\n+    val b = Vectors.sparse(4, Array(0, 3), Array(3, 6))\n+\n+    val stream = MemoryStream[(Vector, Vector)]\n+    val streamingDF = stream.toDS.toDF(\"a\", \"b\")\n+    val sizeHintA = new VectorSizeHint()\n+      .setSize(3)\n+      .setInputCol(\"a\")\n+    val sizeHintB = new VectorSizeHint()\n+      .setSize(4)\n+      .setInputCol(\"b\")\n+    val vectorAssembler = new VectorAssembler()\n+      .setInputCols(Array(\"a\", \"b\"))\n+      .setOutputCol(\"assembled\")\n+    val output = Seq(sizeHintA, sizeHintB, vectorAssembler).foldLeft(streamingDF) {"
  }],
  "prId": 19746
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "remove unused code?",
    "commit": "9c3dcec30d152ab2c5c242db03ed466ae1d320ae",
    "createdAt": "2017-12-13T21:55:42Z",
    "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.Pipeline\n+import org.apache.spark.ml.attribute.AttributeGroup\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.util.DefaultReadWriteTest\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.execution.streaming.MemoryStream\n+import org.apache.spark.sql.streaming.StreamTest\n+\n+class VectorSizeHintSuite\n+  extends SparkFunSuite with MLlibTestSparkContext with DefaultReadWriteTest {\n+\n+  import testImplicits._\n+\n+  test(\"Test Param Validators\") {\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setHandleInvalid(\"invalidValue\"))\n+    intercept[IllegalArgumentException] (new VectorSizeHint().setSize(-3))\n+  }\n+\n+  test(\"Required params must be set before transform.\") {\n+    val data = Seq((Vectors.dense(1, 2), 0)).toDF(\"vector\", \"intValue\")\n+\n+    val noSizeTransformer = new VectorSizeHint().setInputCol(\"vector\")\n+    intercept[NoSuchElementException] (noSizeTransformer.transform(data))\n+    intercept[NoSuchElementException] (noSizeTransformer.transformSchema(data.schema))\n+\n+    val noInputColTransformer = new VectorSizeHint().setSize(2)\n+    intercept[NoSuchElementException] (noInputColTransformer.transform(data))\n+    intercept[NoSuchElementException] (noInputColTransformer.transformSchema(data.schema))\n+  }\n+\n+  test(\"Adding size to column of vectors.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val denseVector = Vectors.dense(1, 2, 3)\n+    val sparseVector = Vectors.sparse(size, Array(), Array())\n+\n+    val data = Seq(denseVector, denseVector, sparseVector).map(Tuple1.apply)\n+    val dataFrame = data.toDF(vectorColName)\n+    assert(\n+      AttributeGroup.fromStructField(dataFrame.schema(vectorColName)).size == -1,\n+      s\"This test requires that column '$vectorColName' not have size metadata.\")\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrame)\n+      assert(\n+        AttributeGroup.fromStructField(withSize.schema(vectorColName)).size == size,\n+        \"Transformer did not add expected size data.\")\n+      val numRows = withSize.collect().length\n+      assert(numRows === data.length, s\"Expecting ${data.length} rows, got $numRows.\")\n+    }\n+  }\n+\n+  test(\"Size hint preserves attributes.\") {\n+\n+    val size = 3\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+    val group = AttributeGroup.fromStructField(dataFrameWithMetadata.schema(vectorColName))\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      val withSize = transformer.transform(dataFrameWithMetadata)\n+\n+      val newGroup = AttributeGroup.fromStructField(withSize.schema(vectorColName))\n+      assert(newGroup.size === size, \"Column has incorrect size metadata.\")\n+      assert(\n+        newGroup.attributes.get === group.attributes.get,\n+        \"VectorSizeHint did not preserve attributes.\")\n+      withSize.collect\n+    }\n+  }\n+\n+  test(\"Size mismatch between current and target size raises an error.\") {\n+    val size = 4\n+    val vectorColName = \"vector\"\n+    val data = Seq((1, 2, 3), (2, 3, 3))\n+    val dataFrame = data.toDF(\"x\", \"y\", \"z\")\n+\n+    val assembler = new VectorAssembler()\n+      .setInputCols(Array(\"x\", \"y\", \"z\"))\n+      .setOutputCol(vectorColName)\n+    val dataFrameWithMetadata = assembler.transform(dataFrame)\n+\n+    for (handleInvalid <- VectorSizeHint.supportedHandleInvalids) {\n+      val transformer = new VectorSizeHint()\n+        .setInputCol(vectorColName)\n+        .setSize(size)\n+        .setHandleInvalid(handleInvalid)\n+      intercept[IllegalArgumentException](transformer.transform(dataFrameWithMetadata))\n+    }\n+  }\n+\n+  test(\"Handle invalid does the right thing.\") {\n+\n+    val vector = Vectors.dense(1, 2, 3)\n+    val short = Vectors.dense(2)\n+    val dataWithNull = Seq(vector, null).map(Tuple1.apply).toDF(\"vector\")\n+    val dataWithShort = Seq(vector, short).map(Tuple1.apply).toDF(\"vector\")\n+\n+    val sizeHint = new VectorSizeHint()\n+      .setInputCol(\"vector\")\n+      .setHandleInvalid(\"error\")\n+      .setSize(3)\n+\n+    intercept[SparkException](sizeHint.transform(dataWithNull).collect())\n+    intercept[SparkException](sizeHint.transform(dataWithShort).collect())\n+\n+    sizeHint.setHandleInvalid(\"skip\")\n+    assert(sizeHint.transform(dataWithNull).count() === 1)\n+    assert(sizeHint.transform(dataWithShort).count() === 1)\n+  }\n+\n+  test(\"read/write\") {\n+    val sizeHint = new VectorSizeHint()\n+      .setInputCol(\"myInputCol\")\n+      .setSize(11)\n+      .setHandleInvalid(\"skip\")\n+    testDefaultReadWrite(sizeHint)\n+  }\n+}\n+\n+class VectorSizeHintStreamingSuite extends StreamTest {\n+\n+  import testImplicits._\n+\n+  test(\"Test assemble vectors with size hint in streaming.\") {\n+    val a = Vectors.dense(0, 1, 2)\n+    val b = Vectors.sparse(4, Array(0, 3), Array(3, 6))\n+\n+    val stream = MemoryStream[(Vector, Vector)]\n+    val streamingDF = stream.toDS.toDF(\"a\", \"b\")\n+    val sizeHintA = new VectorSizeHint()\n+      .setSize(3)\n+      .setInputCol(\"a\")\n+    val sizeHintB = new VectorSizeHint()\n+      .setSize(4)\n+      .setInputCol(\"b\")\n+    val vectorAssembler = new VectorAssembler()\n+      .setInputCols(Array(\"a\", \"b\"))\n+      .setOutputCol(\"assembled\")\n+    val pipeline = new Pipeline().setStages(Array(sizeHintA, sizeHintB, vectorAssembler))\n+    /**"
  }],
  "prId": 19746
}]