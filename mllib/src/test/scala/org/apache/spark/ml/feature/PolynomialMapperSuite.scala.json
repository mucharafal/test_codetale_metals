[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "remove `println`\n",
    "commit": "dc461a6c0253b584ac1e6b0f320e2198e1667751",
    "createdAt": "2015-04-22T06:11:47Z",
    "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.scalatest.FunSuite\n+\n+import org.apache.spark.mllib.linalg.{DenseVector, SparseVector, Vector, Vectors}\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.mllib.util.TestingUtils._\n+import org.apache.spark.sql.{DataFrame, Row, SQLContext}\n+\n+class PolynomialMapperSuite extends FunSuite with MLlibTestSparkContext {\n+\n+  def collectResult(result: DataFrame): Array[Vector] = {\n+    result.select(\"poly_features\").collect().map {\n+      case Row(features: Vector) => features\n+    }\n+  }\n+\n+  def assertTypeOfVector(lhs: Array[Vector], rhs: Array[Vector]): Unit = {\n+    assert((lhs, rhs).zipped.forall {\n+      case (v1: DenseVector, v2: DenseVector) => true\n+      case (v1: SparseVector, v2: SparseVector) => true\n+      case _ => false\n+    }, \"The vector type should be preserved after normalization.\")\n+  }\n+\n+  def assertValues(lhs: Array[Vector], rhs: Array[Vector]): Unit = {\n+    assert((lhs, rhs).zipped.forall { (vector1, vector2) =>\n+      vector1 ~== vector2 absTol 1E-1\n+    }, \"The vector value is not correct after normalization.\")\n+  }\n+\n+  test(\"Polynomial expansion with default parameter\") {\n+    val data = Array(\n+      Vectors.sparse(3, Seq((0, -2.0), (1, 2.3))),\n+      Vectors.dense(-2.0, 2.3),\n+      Vectors.dense(0.0, 0.0, 0.0),\n+      Vectors.dense(0.6, -1.1, -3.0),\n+      Vectors.sparse(3, Seq())\n+    )\n+\n+    val sqlContext = new SQLContext(sc)\n+    val dataFrame = sqlContext\n+      .createDataFrame(sc.parallelize(data, 2).map(Tuple1.apply)).toDF(\"features\")\n+\n+    val polynomialMapper = new PolynomialMapper()\n+      .setInputCol(\"features\")\n+      .setOutputCol(\"poly_features\")\n+\n+    val twoDegreeExpansion: Array[Vector] = Array(\n+      Vectors.sparse(9, Array(0, 1, 3, 4, 6), Array(-2.0, 2.3, 4.0, -4.6, 5.29)),\n+      Vectors.dense(-2.0, 2.3, 4.0, -4.6, 5.29),\n+      Vectors.dense(Array.fill[Double](9)(0.0)),\n+      Vectors.dense(0.6, -1.1, -3.0, 0.36, -0.66, -1.8, 1.21, 3.3, 9.0),\n+      Vectors.sparse(9, Array.empty[Int], Array.empty[Double]))\n+\n+    val result = collectResult(polynomialMapper.transform(dataFrame))\n+\n+    println(result.mkString(\"\\n\"))"
  }],
  "prId": 5245
}]