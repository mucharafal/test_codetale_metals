[{
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "fuzzy?",
    "commit": "a569dac8998d63adbc9adba1f2eb2f42967533e7",
    "createdAt": "2017-03-27T16:30:56Z",
    "diffHunk": "@@ -0,0 +1,338 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.stat\n+\n+import org.scalatest.exceptions.TestFailedException\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.stat.SummaryBuilderImpl.Buffer\n+import org.apache.spark.ml.util.TestingUtils._\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema\n+\n+class SummarizerSuite extends SparkFunSuite with MLlibTestSparkContext {\n+\n+  import testImplicits._\n+  import Summarizer._\n+\n+  private case class ExpectedMetrics(\n+      mean: Seq[Double],\n+      variance: Seq[Double],\n+      count: Long,\n+      numNonZeros: Seq[Long],\n+      max: Seq[Double],\n+      min: Seq[Double],\n+      normL2: Seq[Double],\n+      normL1: Seq[Double])\n+\n+  // The input is expected to be either a sparse vector, a dense vector or an array of doubles\n+  // (which will be converted to a dense vector)\n+  // The expected is the list of all the known metrics.\n+  //\n+  // The tests take an list of input vectors and a list of all the summary values that\n+  // are expected for this input. They currently test against some fixed subset of the\n+  // metrics, but should be made fuzzy in the future.",
    "line": 53
  }],
  "prId": 17419
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "rename s -> summarizer",
    "commit": "a569dac8998d63adbc9adba1f2eb2f42967533e7",
    "createdAt": "2017-03-27T16:40:43Z",
    "diffHunk": "@@ -0,0 +1,338 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.stat\n+\n+import org.scalatest.exceptions.TestFailedException\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.stat.SummaryBuilderImpl.Buffer\n+import org.apache.spark.ml.util.TestingUtils._\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema\n+\n+class SummarizerSuite extends SparkFunSuite with MLlibTestSparkContext {\n+\n+  import testImplicits._\n+  import Summarizer._\n+\n+  private case class ExpectedMetrics(\n+      mean: Seq[Double],\n+      variance: Seq[Double],\n+      count: Long,\n+      numNonZeros: Seq[Long],\n+      max: Seq[Double],\n+      min: Seq[Double],\n+      normL2: Seq[Double],\n+      normL1: Seq[Double])\n+\n+  // The input is expected to be either a sparse vector, a dense vector or an array of doubles\n+  // (which will be converted to a dense vector)\n+  // The expected is the list of all the known metrics.\n+  //\n+  // The tests take an list of input vectors and a list of all the summary values that\n+  // are expected for this input. They currently test against some fixed subset of the\n+  // metrics, but should be made fuzzy in the future.\n+\n+  private def testExample(name: String, input: Seq[Any], exp: ExpectedMetrics): Unit = {\n+    def inputVec: Seq[Vector] = input.map {\n+      case x: Array[Double @unchecked] => Vectors.dense(x)\n+      case x: Seq[Double @unchecked] => Vectors.dense(x.toArray)\n+      case x: Vector => x\n+      case x => throw new Exception(x.toString)\n+    }\n+\n+    val s = {",
    "line": 63
  }],
  "prId": 17419
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "rename wrapped -> wrappedInit",
    "commit": "a569dac8998d63adbc9adba1f2eb2f42967533e7",
    "createdAt": "2017-03-27T16:41:01Z",
    "diffHunk": "@@ -0,0 +1,338 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.stat\n+\n+import org.scalatest.exceptions.TestFailedException\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.stat.SummaryBuilderImpl.Buffer\n+import org.apache.spark.ml.util.TestingUtils._\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema\n+\n+class SummarizerSuite extends SparkFunSuite with MLlibTestSparkContext {\n+\n+  import testImplicits._\n+  import Summarizer._\n+\n+  private case class ExpectedMetrics(\n+      mean: Seq[Double],\n+      variance: Seq[Double],\n+      count: Long,\n+      numNonZeros: Seq[Long],\n+      max: Seq[Double],\n+      min: Seq[Double],\n+      normL2: Seq[Double],\n+      normL1: Seq[Double])\n+\n+  // The input is expected to be either a sparse vector, a dense vector or an array of doubles\n+  // (which will be converted to a dense vector)\n+  // The expected is the list of all the known metrics.\n+  //\n+  // The tests take an list of input vectors and a list of all the summary values that\n+  // are expected for this input. They currently test against some fixed subset of the\n+  // metrics, but should be made fuzzy in the future.\n+\n+  private def testExample(name: String, input: Seq[Any], exp: ExpectedMetrics): Unit = {\n+    def inputVec: Seq[Vector] = input.map {\n+      case x: Array[Double @unchecked] => Vectors.dense(x)\n+      case x: Seq[Double @unchecked] => Vectors.dense(x.toArray)\n+      case x: Vector => x\n+      case x => throw new Exception(x.toString)\n+    }\n+\n+    val s = {\n+      val s2 = new MultivariateOnlineSummarizer\n+      inputVec.foreach(v => s2.add(OldVectors.fromML(v)))\n+      s2\n+    }\n+\n+    // Because the Spark context is reset between tests, we cannot hold a reference onto it.\n+    def wrapped() = {",
    "line": 70
  }],
  "prId": 17419
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "How about ```val res = df.first().toSeq```?",
    "commit": "a569dac8998d63adbc9adba1f2eb2f42967533e7",
    "createdAt": "2017-03-27T16:43:49Z",
    "diffHunk": "@@ -0,0 +1,338 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.stat\n+\n+import org.scalatest.exceptions.TestFailedException\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.stat.SummaryBuilderImpl.Buffer\n+import org.apache.spark.ml.util.TestingUtils._\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema\n+\n+class SummarizerSuite extends SparkFunSuite with MLlibTestSparkContext {\n+\n+  import testImplicits._\n+  import Summarizer._\n+\n+  private case class ExpectedMetrics(\n+      mean: Seq[Double],\n+      variance: Seq[Double],\n+      count: Long,\n+      numNonZeros: Seq[Long],\n+      max: Seq[Double],\n+      min: Seq[Double],\n+      normL2: Seq[Double],\n+      normL1: Seq[Double])\n+\n+  // The input is expected to be either a sparse vector, a dense vector or an array of doubles\n+  // (which will be converted to a dense vector)\n+  // The expected is the list of all the known metrics.\n+  //\n+  // The tests take an list of input vectors and a list of all the summary values that\n+  // are expected for this input. They currently test against some fixed subset of the\n+  // metrics, but should be made fuzzy in the future.\n+\n+  private def testExample(name: String, input: Seq[Any], exp: ExpectedMetrics): Unit = {\n+    def inputVec: Seq[Vector] = input.map {\n+      case x: Array[Double @unchecked] => Vectors.dense(x)\n+      case x: Seq[Double @unchecked] => Vectors.dense(x.toArray)\n+      case x: Vector => x\n+      case x => throw new Exception(x.toString)\n+    }\n+\n+    val s = {\n+      val s2 = new MultivariateOnlineSummarizer\n+      inputVec.foreach(v => s2.add(OldVectors.fromML(v)))\n+      s2\n+    }\n+\n+    // Because the Spark context is reset between tests, we cannot hold a reference onto it.\n+    def wrapped() = {\n+      val df = sc.parallelize(inputVec).map(Tuple1.apply).toDF(\"features\")\n+      val c = df.col(\"features\")\n+      (df, c)\n+    }\n+\n+    registerTest(s\"$name - mean only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"mean\").summary(c), mean(c)), Seq(Row(exp.mean), s.mean))\n+    }\n+\n+    registerTest(s\"$name - mean only (direct)\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(mean(c)), Seq(exp.mean))\n+    }\n+\n+    registerTest(s\"$name - variance only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"variance\").summary(c), variance(c)),\n+        Seq(Row(exp.variance), s.variance))\n+    }\n+\n+    registerTest(s\"$name - variance only (direct)\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(variance(c)), Seq(s.variance))\n+    }\n+\n+    registerTest(s\"$name - count only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"count\").summary(c), count(c)),\n+        Seq(Row(exp.count), exp.count))\n+    }\n+\n+    registerTest(s\"$name - count only (direct)\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(count(c)),\n+        Seq(exp.count))\n+    }\n+\n+    registerTest(s\"$name - numNonZeros only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"numNonZeros\").summary(c), numNonZeros(c)),\n+        Seq(Row(exp.numNonZeros), exp.numNonZeros))\n+    }\n+\n+    registerTest(s\"$name - numNonZeros only (direct)\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(numNonZeros(c)),\n+        Seq(exp.numNonZeros))\n+    }\n+\n+    registerTest(s\"$name - min only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"min\").summary(c), min(c)),\n+        Seq(Row(exp.min), exp.min))\n+    }\n+\n+    registerTest(s\"$name - max only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"max\").summary(c), max(c)),\n+        Seq(Row(exp.max), exp.max))\n+    }\n+\n+    registerTest(s\"$name - normL1 only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"normL1\").summary(c), normL1(c)),\n+        Seq(Row(exp.normL1), exp.normL1))\n+    }\n+\n+    registerTest(s\"$name - normL2 only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"normL2\").summary(c), normL2(c)),\n+        Seq(Row(exp.normL2), exp.normL2))\n+    }\n+\n+    registerTest(s\"$name - all metrics at once\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(\n+        metrics(\"mean\", \"variance\", \"count\", \"numNonZeros\").summary(c),\n+        mean(c), variance(c), count(c), numNonZeros(c)),\n+        Seq(Row(exp.mean, exp.variance, exp.count, exp.numNonZeros),\n+          exp.mean, exp.variance, exp.count, exp.numNonZeros))\n+    }\n+  }\n+\n+  private def denseData(input: Seq[Seq[Double]]): DataFrame = {\n+    val data = input.map(_.toArray).map(Vectors.dense).map(Tuple1.apply)\n+    sc.parallelize(data).toDF(\"features\")\n+  }\n+\n+  private def compare(df: DataFrame, exp: Seq[Any]): Unit = {\n+    val coll = df.collect().toSeq\n+    val Seq(row) = coll",
    "line": 162
  }],
  "prId": 17419
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Are there bugs in direct comparison of Rows?  I'm wondering if we can avoid this here.  Or, does Spark SQL already have this implemented?",
    "commit": "a569dac8998d63adbc9adba1f2eb2f42967533e7",
    "createdAt": "2017-03-27T16:45:10Z",
    "diffHunk": "@@ -0,0 +1,338 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.stat\n+\n+import org.scalatest.exceptions.TestFailedException\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.stat.SummaryBuilderImpl.Buffer\n+import org.apache.spark.ml.util.TestingUtils._\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema\n+\n+class SummarizerSuite extends SparkFunSuite with MLlibTestSparkContext {\n+\n+  import testImplicits._\n+  import Summarizer._\n+\n+  private case class ExpectedMetrics(\n+      mean: Seq[Double],\n+      variance: Seq[Double],\n+      count: Long,\n+      numNonZeros: Seq[Long],\n+      max: Seq[Double],\n+      min: Seq[Double],\n+      normL2: Seq[Double],\n+      normL1: Seq[Double])\n+\n+  // The input is expected to be either a sparse vector, a dense vector or an array of doubles\n+  // (which will be converted to a dense vector)\n+  // The expected is the list of all the known metrics.\n+  //\n+  // The tests take an list of input vectors and a list of all the summary values that\n+  // are expected for this input. They currently test against some fixed subset of the\n+  // metrics, but should be made fuzzy in the future.\n+\n+  private def testExample(name: String, input: Seq[Any], exp: ExpectedMetrics): Unit = {\n+    def inputVec: Seq[Vector] = input.map {\n+      case x: Array[Double @unchecked] => Vectors.dense(x)\n+      case x: Seq[Double @unchecked] => Vectors.dense(x.toArray)\n+      case x: Vector => x\n+      case x => throw new Exception(x.toString)\n+    }\n+\n+    val s = {\n+      val s2 = new MultivariateOnlineSummarizer\n+      inputVec.foreach(v => s2.add(OldVectors.fromML(v)))\n+      s2\n+    }\n+\n+    // Because the Spark context is reset between tests, we cannot hold a reference onto it.\n+    def wrapped() = {\n+      val df = sc.parallelize(inputVec).map(Tuple1.apply).toDF(\"features\")\n+      val c = df.col(\"features\")\n+      (df, c)\n+    }\n+\n+    registerTest(s\"$name - mean only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"mean\").summary(c), mean(c)), Seq(Row(exp.mean), s.mean))\n+    }\n+\n+    registerTest(s\"$name - mean only (direct)\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(mean(c)), Seq(exp.mean))\n+    }\n+\n+    registerTest(s\"$name - variance only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"variance\").summary(c), variance(c)),\n+        Seq(Row(exp.variance), s.variance))\n+    }\n+\n+    registerTest(s\"$name - variance only (direct)\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(variance(c)), Seq(s.variance))\n+    }\n+\n+    registerTest(s\"$name - count only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"count\").summary(c), count(c)),\n+        Seq(Row(exp.count), exp.count))\n+    }\n+\n+    registerTest(s\"$name - count only (direct)\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(count(c)),\n+        Seq(exp.count))\n+    }\n+\n+    registerTest(s\"$name - numNonZeros only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"numNonZeros\").summary(c), numNonZeros(c)),\n+        Seq(Row(exp.numNonZeros), exp.numNonZeros))\n+    }\n+\n+    registerTest(s\"$name - numNonZeros only (direct)\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(numNonZeros(c)),\n+        Seq(exp.numNonZeros))\n+    }\n+\n+    registerTest(s\"$name - min only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"min\").summary(c), min(c)),\n+        Seq(Row(exp.min), exp.min))\n+    }\n+\n+    registerTest(s\"$name - max only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"max\").summary(c), max(c)),\n+        Seq(Row(exp.max), exp.max))\n+    }\n+\n+    registerTest(s\"$name - normL1 only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"normL1\").summary(c), normL1(c)),\n+        Seq(Row(exp.normL1), exp.normL1))\n+    }\n+\n+    registerTest(s\"$name - normL2 only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"normL2\").summary(c), normL2(c)),\n+        Seq(Row(exp.normL2), exp.normL2))\n+    }\n+\n+    registerTest(s\"$name - all metrics at once\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(\n+        metrics(\"mean\", \"variance\", \"count\", \"numNonZeros\").summary(c),\n+        mean(c), variance(c), count(c), numNonZeros(c)),\n+        Seq(Row(exp.mean, exp.variance, exp.count, exp.numNonZeros),\n+          exp.mean, exp.variance, exp.count, exp.numNonZeros))\n+    }\n+  }\n+\n+  private def denseData(input: Seq[Seq[Double]]): DataFrame = {\n+    val data = input.map(_.toArray).map(Vectors.dense).map(Tuple1.apply)\n+    sc.parallelize(data).toDF(\"features\")\n+  }\n+\n+  private def compare(df: DataFrame, exp: Seq[Any]): Unit = {\n+    val coll = df.collect().toSeq\n+    val Seq(row) = coll\n+    val res = row.toSeq\n+    val names = df.schema.fieldNames.zipWithIndex.map { case (n, idx) => s\"$n ($idx)\" }\n+    assert(res.size === exp.size, (res.size, exp.size))\n+    for (((x1, x2), name) <- res.zip(exp).zip(names)) {\n+      compareStructures(x1, x2, name)\n+    }\n+  }\n+\n+  // Compares structured content.\n+  private def compareStructures(x1: Any, x2: Any, name: String): Unit = (x1, x2) match {",
    "line": 172
  }],
  "prId": 17419
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Why do you need to catch and re-throw it as a TestFailedException?",
    "commit": "a569dac8998d63adbc9adba1f2eb2f42967533e7",
    "createdAt": "2017-03-27T16:47:09Z",
    "diffHunk": "@@ -0,0 +1,338 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.stat\n+\n+import org.scalatest.exceptions.TestFailedException\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.stat.SummaryBuilderImpl.Buffer\n+import org.apache.spark.ml.util.TestingUtils._\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema\n+\n+class SummarizerSuite extends SparkFunSuite with MLlibTestSparkContext {\n+\n+  import testImplicits._\n+  import Summarizer._\n+\n+  private case class ExpectedMetrics(\n+      mean: Seq[Double],\n+      variance: Seq[Double],\n+      count: Long,\n+      numNonZeros: Seq[Long],\n+      max: Seq[Double],\n+      min: Seq[Double],\n+      normL2: Seq[Double],\n+      normL1: Seq[Double])\n+\n+  // The input is expected to be either a sparse vector, a dense vector or an array of doubles\n+  // (which will be converted to a dense vector)\n+  // The expected is the list of all the known metrics.\n+  //\n+  // The tests take an list of input vectors and a list of all the summary values that\n+  // are expected for this input. They currently test against some fixed subset of the\n+  // metrics, but should be made fuzzy in the future.\n+\n+  private def testExample(name: String, input: Seq[Any], exp: ExpectedMetrics): Unit = {\n+    def inputVec: Seq[Vector] = input.map {\n+      case x: Array[Double @unchecked] => Vectors.dense(x)\n+      case x: Seq[Double @unchecked] => Vectors.dense(x.toArray)\n+      case x: Vector => x\n+      case x => throw new Exception(x.toString)\n+    }\n+\n+    val s = {\n+      val s2 = new MultivariateOnlineSummarizer\n+      inputVec.foreach(v => s2.add(OldVectors.fromML(v)))\n+      s2\n+    }\n+\n+    // Because the Spark context is reset between tests, we cannot hold a reference onto it.\n+    def wrapped() = {\n+      val df = sc.parallelize(inputVec).map(Tuple1.apply).toDF(\"features\")\n+      val c = df.col(\"features\")\n+      (df, c)\n+    }\n+\n+    registerTest(s\"$name - mean only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"mean\").summary(c), mean(c)), Seq(Row(exp.mean), s.mean))\n+    }\n+\n+    registerTest(s\"$name - mean only (direct)\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(mean(c)), Seq(exp.mean))\n+    }\n+\n+    registerTest(s\"$name - variance only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"variance\").summary(c), variance(c)),\n+        Seq(Row(exp.variance), s.variance))\n+    }\n+\n+    registerTest(s\"$name - variance only (direct)\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(variance(c)), Seq(s.variance))\n+    }\n+\n+    registerTest(s\"$name - count only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"count\").summary(c), count(c)),\n+        Seq(Row(exp.count), exp.count))\n+    }\n+\n+    registerTest(s\"$name - count only (direct)\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(count(c)),\n+        Seq(exp.count))\n+    }\n+\n+    registerTest(s\"$name - numNonZeros only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"numNonZeros\").summary(c), numNonZeros(c)),\n+        Seq(Row(exp.numNonZeros), exp.numNonZeros))\n+    }\n+\n+    registerTest(s\"$name - numNonZeros only (direct)\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(numNonZeros(c)),\n+        Seq(exp.numNonZeros))\n+    }\n+\n+    registerTest(s\"$name - min only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"min\").summary(c), min(c)),\n+        Seq(Row(exp.min), exp.min))\n+    }\n+\n+    registerTest(s\"$name - max only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"max\").summary(c), max(c)),\n+        Seq(Row(exp.max), exp.max))\n+    }\n+\n+    registerTest(s\"$name - normL1 only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"normL1\").summary(c), normL1(c)),\n+        Seq(Row(exp.normL1), exp.normL1))\n+    }\n+\n+    registerTest(s\"$name - normL2 only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"normL2\").summary(c), normL2(c)),\n+        Seq(Row(exp.normL2), exp.normL2))\n+    }\n+\n+    registerTest(s\"$name - all metrics at once\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(\n+        metrics(\"mean\", \"variance\", \"count\", \"numNonZeros\").summary(c),\n+        mean(c), variance(c), count(c), numNonZeros(c)),\n+        Seq(Row(exp.mean, exp.variance, exp.count, exp.numNonZeros),\n+          exp.mean, exp.variance, exp.count, exp.numNonZeros))\n+    }\n+  }\n+\n+  private def denseData(input: Seq[Seq[Double]]): DataFrame = {\n+    val data = input.map(_.toArray).map(Vectors.dense).map(Tuple1.apply)\n+    sc.parallelize(data).toDF(\"features\")\n+  }\n+\n+  private def compare(df: DataFrame, exp: Seq[Any]): Unit = {\n+    val coll = df.collect().toSeq\n+    val Seq(row) = coll\n+    val res = row.toSeq\n+    val names = df.schema.fieldNames.zipWithIndex.map { case (n, idx) => s\"$n ($idx)\" }\n+    assert(res.size === exp.size, (res.size, exp.size))\n+    for (((x1, x2), name) <- res.zip(exp).zip(names)) {\n+      compareStructures(x1, x2, name)\n+    }\n+  }\n+\n+  // Compares structured content.\n+  private def compareStructures(x1: Any, x2: Any, name: String): Unit = (x1, x2) match {\n+    case (y1: Seq[Double @unchecked], v1: OldVector) =>\n+      compareStructures(y1, v1.toArray.toSeq, name)\n+    case (d1: Double, d2: Double) =>\n+      assert2(Vectors.dense(d1) ~== Vectors.dense(d2) absTol 1e-4, name)\n+    case (r1: GenericRowWithSchema, r2: Row) =>\n+      assert(r1.size === r2.size, (r1, r2))\n+      for (((fname, x1), x2) <- r1.schema.fieldNames.zip(r1.toSeq).zip(r2.toSeq)) {\n+        compareStructures(x1, x2, s\"$name.$fname\")\n+      }\n+    case (r1: Row, r2: Row) =>\n+      assert(r1.size === r2.size, (r1, r2))\n+      for ((x1, x2) <- r1.toSeq.zip(r2.toSeq)) { compareStructures(x1, x2, name) }\n+    case (v1: Vector, v2: Vector) =>\n+      assert2(v1 ~== v2 absTol 1e-4, name)\n+    case (l1: Long, l2: Long) => assert(l1 === l2)\n+    case (s1: Seq[_], s2: Seq[_]) =>\n+      assert(s1.size === s2.size, s\"$name ${(s1, s2)}\")\n+      for (((x1, idx), x2) <- s1.zipWithIndex.zip(s2)) {\n+        compareStructures(x1, x2, s\"$name.$idx\")\n+      }\n+    case (arr1: Array[_], arr2: Array[_]) =>\n+      assert(arr1.toSeq === arr2.toSeq)\n+    case _ => throw new Exception(s\"$name: ${x1.getClass} ${x2.getClass} $x1 $x2\")\n+  }\n+\n+  private def assert2(x: => Boolean, hint: String): Unit = {\n+    try {\n+      assert(x, hint)\n+    } catch {\n+      case tfe: TestFailedException =>\n+        throw new TestFailedException(Some(s\"Failure with hint $hint\"), Some(tfe), 1)",
    "line": 203
  }],
  "prId": 17419
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Rename these to something clearer",
    "commit": "a569dac8998d63adbc9adba1f2eb2f42967533e7",
    "createdAt": "2017-03-27T16:47:53Z",
    "diffHunk": "@@ -0,0 +1,338 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.stat\n+\n+import org.scalatest.exceptions.TestFailedException\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.stat.SummaryBuilderImpl.Buffer\n+import org.apache.spark.ml.util.TestingUtils._\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema\n+\n+class SummarizerSuite extends SparkFunSuite with MLlibTestSparkContext {\n+\n+  import testImplicits._\n+  import Summarizer._\n+\n+  private case class ExpectedMetrics(\n+      mean: Seq[Double],\n+      variance: Seq[Double],\n+      count: Long,\n+      numNonZeros: Seq[Long],\n+      max: Seq[Double],\n+      min: Seq[Double],\n+      normL2: Seq[Double],\n+      normL1: Seq[Double])\n+\n+  // The input is expected to be either a sparse vector, a dense vector or an array of doubles\n+  // (which will be converted to a dense vector)\n+  // The expected is the list of all the known metrics.\n+  //\n+  // The tests take an list of input vectors and a list of all the summary values that\n+  // are expected for this input. They currently test against some fixed subset of the\n+  // metrics, but should be made fuzzy in the future.\n+\n+  private def testExample(name: String, input: Seq[Any], exp: ExpectedMetrics): Unit = {\n+    def inputVec: Seq[Vector] = input.map {\n+      case x: Array[Double @unchecked] => Vectors.dense(x)\n+      case x: Seq[Double @unchecked] => Vectors.dense(x.toArray)\n+      case x: Vector => x\n+      case x => throw new Exception(x.toString)\n+    }\n+\n+    val s = {\n+      val s2 = new MultivariateOnlineSummarizer\n+      inputVec.foreach(v => s2.add(OldVectors.fromML(v)))\n+      s2\n+    }\n+\n+    // Because the Spark context is reset between tests, we cannot hold a reference onto it.\n+    def wrapped() = {\n+      val df = sc.parallelize(inputVec).map(Tuple1.apply).toDF(\"features\")\n+      val c = df.col(\"features\")\n+      (df, c)\n+    }\n+\n+    registerTest(s\"$name - mean only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"mean\").summary(c), mean(c)), Seq(Row(exp.mean), s.mean))\n+    }\n+\n+    registerTest(s\"$name - mean only (direct)\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(mean(c)), Seq(exp.mean))\n+    }\n+\n+    registerTest(s\"$name - variance only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"variance\").summary(c), variance(c)),\n+        Seq(Row(exp.variance), s.variance))\n+    }\n+\n+    registerTest(s\"$name - variance only (direct)\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(variance(c)), Seq(s.variance))\n+    }\n+\n+    registerTest(s\"$name - count only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"count\").summary(c), count(c)),\n+        Seq(Row(exp.count), exp.count))\n+    }\n+\n+    registerTest(s\"$name - count only (direct)\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(count(c)),\n+        Seq(exp.count))\n+    }\n+\n+    registerTest(s\"$name - numNonZeros only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"numNonZeros\").summary(c), numNonZeros(c)),\n+        Seq(Row(exp.numNonZeros), exp.numNonZeros))\n+    }\n+\n+    registerTest(s\"$name - numNonZeros only (direct)\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(numNonZeros(c)),\n+        Seq(exp.numNonZeros))\n+    }\n+\n+    registerTest(s\"$name - min only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"min\").summary(c), min(c)),\n+        Seq(Row(exp.min), exp.min))\n+    }\n+\n+    registerTest(s\"$name - max only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"max\").summary(c), max(c)),\n+        Seq(Row(exp.max), exp.max))\n+    }\n+\n+    registerTest(s\"$name - normL1 only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"normL1\").summary(c), normL1(c)),\n+        Seq(Row(exp.normL1), exp.normL1))\n+    }\n+\n+    registerTest(s\"$name - normL2 only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"normL2\").summary(c), normL2(c)),\n+        Seq(Row(exp.normL2), exp.normL2))\n+    }\n+\n+    registerTest(s\"$name - all metrics at once\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(\n+        metrics(\"mean\", \"variance\", \"count\", \"numNonZeros\").summary(c),\n+        mean(c), variance(c), count(c), numNonZeros(c)),\n+        Seq(Row(exp.mean, exp.variance, exp.count, exp.numNonZeros),\n+          exp.mean, exp.variance, exp.count, exp.numNonZeros))\n+    }\n+  }\n+\n+  private def denseData(input: Seq[Seq[Double]]): DataFrame = {\n+    val data = input.map(_.toArray).map(Vectors.dense).map(Tuple1.apply)\n+    sc.parallelize(data).toDF(\"features\")\n+  }\n+\n+  private def compare(df: DataFrame, exp: Seq[Any]): Unit = {\n+    val coll = df.collect().toSeq\n+    val Seq(row) = coll\n+    val res = row.toSeq\n+    val names = df.schema.fieldNames.zipWithIndex.map { case (n, idx) => s\"$n ($idx)\" }\n+    assert(res.size === exp.size, (res.size, exp.size))\n+    for (((x1, x2), name) <- res.zip(exp).zip(names)) {\n+      compareStructures(x1, x2, name)\n+    }\n+  }\n+\n+  // Compares structured content.\n+  private def compareStructures(x1: Any, x2: Any, name: String): Unit = (x1, x2) match {\n+    case (y1: Seq[Double @unchecked], v1: OldVector) =>\n+      compareStructures(y1, v1.toArray.toSeq, name)\n+    case (d1: Double, d2: Double) =>\n+      assert2(Vectors.dense(d1) ~== Vectors.dense(d2) absTol 1e-4, name)\n+    case (r1: GenericRowWithSchema, r2: Row) =>\n+      assert(r1.size === r2.size, (r1, r2))\n+      for (((fname, x1), x2) <- r1.schema.fieldNames.zip(r1.toSeq).zip(r2.toSeq)) {\n+        compareStructures(x1, x2, s\"$name.$fname\")\n+      }\n+    case (r1: Row, r2: Row) =>\n+      assert(r1.size === r2.size, (r1, r2))\n+      for ((x1, x2) <- r1.toSeq.zip(r2.toSeq)) { compareStructures(x1, x2, name) }\n+    case (v1: Vector, v2: Vector) =>\n+      assert2(v1 ~== v2 absTol 1e-4, name)\n+    case (l1: Long, l2: Long) => assert(l1 === l2)\n+    case (s1: Seq[_], s2: Seq[_]) =>\n+      assert(s1.size === s2.size, s\"$name ${(s1, s2)}\")\n+      for (((x1, idx), x2) <- s1.zipWithIndex.zip(s2)) {\n+        compareStructures(x1, x2, s\"$name.$idx\")\n+      }\n+    case (arr1: Array[_], arr2: Array[_]) =>\n+      assert(arr1.toSeq === arr2.toSeq)\n+    case _ => throw new Exception(s\"$name: ${x1.getClass} ${x2.getClass} $x1 $x2\")\n+  }\n+\n+  private def assert2(x: => Boolean, hint: String): Unit = {\n+    try {\n+      assert(x, hint)\n+    } catch {\n+      case tfe: TestFailedException =>\n+        throw new TestFailedException(Some(s\"Failure with hint $hint\"), Some(tfe), 1)\n+    }\n+  }\n+\n+  private def makeBuffer(vecs: Seq[Vector]): Buffer = {\n+    val b = Buffer.allMetrics()\n+    for (v <- vecs) { Buffer.updateInPlace(b, v, 1.0) }\n+    b\n+  }\n+\n+  private def b(x: Array[Double]): Vector = Vectors.dense(x)",
    "line": 213
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "+1",
    "commit": "a569dac8998d63adbc9adba1f2eb2f42967533e7",
    "createdAt": "2017-03-30T04:58:09Z",
    "diffHunk": "@@ -0,0 +1,338 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.stat\n+\n+import org.scalatest.exceptions.TestFailedException\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.stat.SummaryBuilderImpl.Buffer\n+import org.apache.spark.ml.util.TestingUtils._\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema\n+\n+class SummarizerSuite extends SparkFunSuite with MLlibTestSparkContext {\n+\n+  import testImplicits._\n+  import Summarizer._\n+\n+  private case class ExpectedMetrics(\n+      mean: Seq[Double],\n+      variance: Seq[Double],\n+      count: Long,\n+      numNonZeros: Seq[Long],\n+      max: Seq[Double],\n+      min: Seq[Double],\n+      normL2: Seq[Double],\n+      normL1: Seq[Double])\n+\n+  // The input is expected to be either a sparse vector, a dense vector or an array of doubles\n+  // (which will be converted to a dense vector)\n+  // The expected is the list of all the known metrics.\n+  //\n+  // The tests take an list of input vectors and a list of all the summary values that\n+  // are expected for this input. They currently test against some fixed subset of the\n+  // metrics, but should be made fuzzy in the future.\n+\n+  private def testExample(name: String, input: Seq[Any], exp: ExpectedMetrics): Unit = {\n+    def inputVec: Seq[Vector] = input.map {\n+      case x: Array[Double @unchecked] => Vectors.dense(x)\n+      case x: Seq[Double @unchecked] => Vectors.dense(x.toArray)\n+      case x: Vector => x\n+      case x => throw new Exception(x.toString)\n+    }\n+\n+    val s = {\n+      val s2 = new MultivariateOnlineSummarizer\n+      inputVec.foreach(v => s2.add(OldVectors.fromML(v)))\n+      s2\n+    }\n+\n+    // Because the Spark context is reset between tests, we cannot hold a reference onto it.\n+    def wrapped() = {\n+      val df = sc.parallelize(inputVec).map(Tuple1.apply).toDF(\"features\")\n+      val c = df.col(\"features\")\n+      (df, c)\n+    }\n+\n+    registerTest(s\"$name - mean only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"mean\").summary(c), mean(c)), Seq(Row(exp.mean), s.mean))\n+    }\n+\n+    registerTest(s\"$name - mean only (direct)\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(mean(c)), Seq(exp.mean))\n+    }\n+\n+    registerTest(s\"$name - variance only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"variance\").summary(c), variance(c)),\n+        Seq(Row(exp.variance), s.variance))\n+    }\n+\n+    registerTest(s\"$name - variance only (direct)\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(variance(c)), Seq(s.variance))\n+    }\n+\n+    registerTest(s\"$name - count only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"count\").summary(c), count(c)),\n+        Seq(Row(exp.count), exp.count))\n+    }\n+\n+    registerTest(s\"$name - count only (direct)\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(count(c)),\n+        Seq(exp.count))\n+    }\n+\n+    registerTest(s\"$name - numNonZeros only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"numNonZeros\").summary(c), numNonZeros(c)),\n+        Seq(Row(exp.numNonZeros), exp.numNonZeros))\n+    }\n+\n+    registerTest(s\"$name - numNonZeros only (direct)\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(numNonZeros(c)),\n+        Seq(exp.numNonZeros))\n+    }\n+\n+    registerTest(s\"$name - min only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"min\").summary(c), min(c)),\n+        Seq(Row(exp.min), exp.min))\n+    }\n+\n+    registerTest(s\"$name - max only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"max\").summary(c), max(c)),\n+        Seq(Row(exp.max), exp.max))\n+    }\n+\n+    registerTest(s\"$name - normL1 only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"normL1\").summary(c), normL1(c)),\n+        Seq(Row(exp.normL1), exp.normL1))\n+    }\n+\n+    registerTest(s\"$name - normL2 only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"normL2\").summary(c), normL2(c)),\n+        Seq(Row(exp.normL2), exp.normL2))\n+    }\n+\n+    registerTest(s\"$name - all metrics at once\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(\n+        metrics(\"mean\", \"variance\", \"count\", \"numNonZeros\").summary(c),\n+        mean(c), variance(c), count(c), numNonZeros(c)),\n+        Seq(Row(exp.mean, exp.variance, exp.count, exp.numNonZeros),\n+          exp.mean, exp.variance, exp.count, exp.numNonZeros))\n+    }\n+  }\n+\n+  private def denseData(input: Seq[Seq[Double]]): DataFrame = {\n+    val data = input.map(_.toArray).map(Vectors.dense).map(Tuple1.apply)\n+    sc.parallelize(data).toDF(\"features\")\n+  }\n+\n+  private def compare(df: DataFrame, exp: Seq[Any]): Unit = {\n+    val coll = df.collect().toSeq\n+    val Seq(row) = coll\n+    val res = row.toSeq\n+    val names = df.schema.fieldNames.zipWithIndex.map { case (n, idx) => s\"$n ($idx)\" }\n+    assert(res.size === exp.size, (res.size, exp.size))\n+    for (((x1, x2), name) <- res.zip(exp).zip(names)) {\n+      compareStructures(x1, x2, name)\n+    }\n+  }\n+\n+  // Compares structured content.\n+  private def compareStructures(x1: Any, x2: Any, name: String): Unit = (x1, x2) match {\n+    case (y1: Seq[Double @unchecked], v1: OldVector) =>\n+      compareStructures(y1, v1.toArray.toSeq, name)\n+    case (d1: Double, d2: Double) =>\n+      assert2(Vectors.dense(d1) ~== Vectors.dense(d2) absTol 1e-4, name)\n+    case (r1: GenericRowWithSchema, r2: Row) =>\n+      assert(r1.size === r2.size, (r1, r2))\n+      for (((fname, x1), x2) <- r1.schema.fieldNames.zip(r1.toSeq).zip(r2.toSeq)) {\n+        compareStructures(x1, x2, s\"$name.$fname\")\n+      }\n+    case (r1: Row, r2: Row) =>\n+      assert(r1.size === r2.size, (r1, r2))\n+      for ((x1, x2) <- r1.toSeq.zip(r2.toSeq)) { compareStructures(x1, x2, name) }\n+    case (v1: Vector, v2: Vector) =>\n+      assert2(v1 ~== v2 absTol 1e-4, name)\n+    case (l1: Long, l2: Long) => assert(l1 === l2)\n+    case (s1: Seq[_], s2: Seq[_]) =>\n+      assert(s1.size === s2.size, s\"$name ${(s1, s2)}\")\n+      for (((x1, idx), x2) <- s1.zipWithIndex.zip(s2)) {\n+        compareStructures(x1, x2, s\"$name.$idx\")\n+      }\n+    case (arr1: Array[_], arr2: Array[_]) =>\n+      assert(arr1.toSeq === arr2.toSeq)\n+    case _ => throw new Exception(s\"$name: ${x1.getClass} ${x2.getClass} $x1 $x2\")\n+  }\n+\n+  private def assert2(x: => Boolean, hint: String): Unit = {\n+    try {\n+      assert(x, hint)\n+    } catch {\n+      case tfe: TestFailedException =>\n+        throw new TestFailedException(Some(s\"Failure with hint $hint\"), Some(tfe), 1)\n+    }\n+  }\n+\n+  private def makeBuffer(vecs: Seq[Vector]): Buffer = {\n+    val b = Buffer.allMetrics()\n+    for (v <- vecs) { Buffer.updateInPlace(b, v, 1.0) }\n+    b\n+  }\n+\n+  private def b(x: Array[Double]): Vector = Vectors.dense(x)",
    "line": 213
  }],
  "prId": 17419
}, {
  "comments": [{
    "author": {
      "login": "kiszk"
    },
    "body": "I think that 10 times without warmup is too small for performance measurement.\r\nCan we use `Benchmark` class or add warmup run as `Benchmark` class does?",
    "commit": "a569dac8998d63adbc9adba1f2eb2f42967533e7",
    "createdAt": "2017-03-29T17:48:22Z",
    "diffHunk": "@@ -335,4 +335,65 @@ class SummarizerSuite extends SparkFunSuite with MLlibTestSparkContext {\n     assert(Buffer.totalCount(summarizer) === 6)\n   }\n \n+  // TODO: this test should not be committed. It is here to isolate some performance hotspots.\n+  test(\"perf test\") {\n+    val n = 10000000\n+    val rdd1 = sc.parallelize(1 to n).map { idx =>\n+      OldVectors.dense(idx.toDouble)\n+    }\n+    val trieouts = 10"
  }, {
    "author": {
      "login": "thunterdb"
    },
    "body": "I did not try about that class, thanks. It should stabilize the results.",
    "commit": "a569dac8998d63adbc9adba1f2eb2f42967533e7",
    "createdAt": "2017-03-29T17:54:04Z",
    "diffHunk": "@@ -335,4 +335,65 @@ class SummarizerSuite extends SparkFunSuite with MLlibTestSparkContext {\n     assert(Buffer.totalCount(summarizer) === 6)\n   }\n \n+  // TODO: this test should not be committed. It is here to isolate some performance hotspots.\n+  test(\"perf test\") {\n+    val n = 10000000\n+    val rdd1 = sc.parallelize(1 to n).map { idx =>\n+      OldVectors.dense(idx.toDouble)\n+    }\n+    val trieouts = 10"
  }, {
    "author": {
      "login": "kiszk"
    },
    "body": "When results are stabilized, I think that it would be good to keep results with ``ignore(\"benchmark\")`` as [other benchmarks does](https://github.com/apache/spark/blob/master/sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/MiscBenchmark.scala).",
    "commit": "a569dac8998d63adbc9adba1f2eb2f42967533e7",
    "createdAt": "2017-03-29T18:06:00Z",
    "diffHunk": "@@ -335,4 +335,65 @@ class SummarizerSuite extends SparkFunSuite with MLlibTestSparkContext {\n     assert(Buffer.totalCount(summarizer) === 6)\n   }\n \n+  // TODO: this test should not be committed. It is here to isolate some performance hotspots.\n+  test(\"perf test\") {\n+    val n = 10000000\n+    val rdd1 = sc.parallelize(1 to n).map { idx =>\n+      OldVectors.dense(idx.toDouble)\n+    }\n+    val trieouts = 10"
  }],
  "prId": 17419
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "And this. It is better to rename it.",
    "commit": "a569dac8998d63adbc9adba1f2eb2f42967533e7",
    "createdAt": "2017-03-30T04:59:03Z",
    "diffHunk": "@@ -0,0 +1,399 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.stat\n+\n+import org.scalatest.exceptions.TestFailedException\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.stat.SummaryBuilderImpl.Buffer\n+import org.apache.spark.ml.util.TestingUtils._\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.stat.{MultivariateOnlineSummarizer, Statistics}\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema\n+\n+class SummarizerSuite extends SparkFunSuite with MLlibTestSparkContext {\n+\n+  import testImplicits._\n+  import Summarizer._\n+\n+  private case class ExpectedMetrics(\n+      mean: Seq[Double],\n+      variance: Seq[Double],\n+      count: Long,\n+      numNonZeros: Seq[Long],\n+      max: Seq[Double],\n+      min: Seq[Double],\n+      normL2: Seq[Double],\n+      normL1: Seq[Double])\n+\n+  // The input is expected to be either a sparse vector, a dense vector or an array of doubles\n+  // (which will be converted to a dense vector)\n+  // The expected is the list of all the known metrics.\n+  //\n+  // The tests take an list of input vectors and a list of all the summary values that\n+  // are expected for this input. They currently test against some fixed subset of the\n+  // metrics, but should be made fuzzy in the future.\n+\n+  private def testExample(name: String, input: Seq[Any], exp: ExpectedMetrics): Unit = {\n+    def inputVec: Seq[Vector] = input.map {\n+      case x: Array[Double @unchecked] => Vectors.dense(x)\n+      case x: Seq[Double @unchecked] => Vectors.dense(x.toArray)\n+      case x: Vector => x\n+      case x => throw new Exception(x.toString)\n+    }\n+\n+    val s = {\n+      val s2 = new MultivariateOnlineSummarizer\n+      inputVec.foreach(v => s2.add(OldVectors.fromML(v)))\n+      s2\n+    }\n+\n+    // Because the Spark context is reset between tests, we cannot hold a reference onto it.\n+    def wrapped() = {\n+      val df = sc.parallelize(inputVec).map(Tuple1.apply).toDF(\"features\")\n+      val c = df.col(\"features\")\n+      (df, c)\n+    }\n+\n+    registerTest(s\"$name - mean only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"mean\").summary(c), mean(c)), Seq(Row(exp.mean), s.mean))\n+    }\n+\n+    registerTest(s\"$name - mean only (direct)\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(mean(c)), Seq(exp.mean))\n+    }\n+\n+    registerTest(s\"$name - variance only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"variance\").summary(c), variance(c)),\n+        Seq(Row(exp.variance), s.variance))\n+    }\n+\n+    registerTest(s\"$name - variance only (direct)\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(variance(c)), Seq(s.variance))\n+    }\n+\n+    registerTest(s\"$name - count only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"count\").summary(c), count(c)),\n+        Seq(Row(exp.count), exp.count))\n+    }\n+\n+    registerTest(s\"$name - count only (direct)\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(count(c)),\n+        Seq(exp.count))\n+    }\n+\n+    registerTest(s\"$name - numNonZeros only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"numNonZeros\").summary(c), numNonZeros(c)),\n+        Seq(Row(exp.numNonZeros), exp.numNonZeros))\n+    }\n+\n+    registerTest(s\"$name - numNonZeros only (direct)\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(numNonZeros(c)),\n+        Seq(exp.numNonZeros))\n+    }\n+\n+    registerTest(s\"$name - min only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"min\").summary(c), min(c)),\n+        Seq(Row(exp.min), exp.min))\n+    }\n+\n+    registerTest(s\"$name - max only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"max\").summary(c), max(c)),\n+        Seq(Row(exp.max), exp.max))\n+    }\n+\n+    registerTest(s\"$name - normL1 only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"normL1\").summary(c), normL1(c)),\n+        Seq(Row(exp.normL1), exp.normL1))\n+    }\n+\n+    registerTest(s\"$name - normL2 only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"normL2\").summary(c), normL2(c)),\n+        Seq(Row(exp.normL2), exp.normL2))\n+    }\n+\n+    registerTest(s\"$name - all metrics at once\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(\n+        metrics(\"mean\", \"variance\", \"count\", \"numNonZeros\").summary(c),\n+        mean(c), variance(c), count(c), numNonZeros(c)),\n+        Seq(Row(exp.mean, exp.variance, exp.count, exp.numNonZeros),\n+          exp.mean, exp.variance, exp.count, exp.numNonZeros))\n+    }\n+  }\n+\n+  private def denseData(input: Seq[Seq[Double]]): DataFrame = {\n+    val data = input.map(_.toArray).map(Vectors.dense).map(Tuple1.apply)\n+    sc.parallelize(data).toDF(\"features\")\n+  }\n+\n+  private def compare(df: DataFrame, exp: Seq[Any]): Unit = {\n+    val coll = df.collect().toSeq\n+    val Seq(row) = coll\n+    val res = row.toSeq\n+    val names = df.schema.fieldNames.zipWithIndex.map { case (n, idx) => s\"$n ($idx)\" }\n+    assert(res.size === exp.size, (res.size, exp.size))\n+    for (((x1, x2), name) <- res.zip(exp).zip(names)) {\n+      compareStructures(x1, x2, name)\n+    }\n+  }\n+\n+  // Compares structured content.\n+  private def compareStructures(x1: Any, x2: Any, name: String): Unit = (x1, x2) match {\n+    case (y1: Seq[Double @unchecked], v1: OldVector) =>\n+      compareStructures(y1, v1.toArray.toSeq, name)\n+    case (d1: Double, d2: Double) =>\n+      assert2(Vectors.dense(d1) ~== Vectors.dense(d2) absTol 1e-4, name)\n+    case (r1: GenericRowWithSchema, r2: Row) =>\n+      assert(r1.size === r2.size, (r1, r2))\n+      for (((fname, x1), x2) <- r1.schema.fieldNames.zip(r1.toSeq).zip(r2.toSeq)) {\n+        compareStructures(x1, x2, s\"$name.$fname\")\n+      }\n+    case (r1: Row, r2: Row) =>\n+      assert(r1.size === r2.size, (r1, r2))\n+      for ((x1, x2) <- r1.toSeq.zip(r2.toSeq)) { compareStructures(x1, x2, name) }\n+    case (v1: Vector, v2: Vector) =>\n+      assert2(v1 ~== v2 absTol 1e-4, name)\n+    case (l1: Long, l2: Long) => assert(l1 === l2)\n+    case (s1: Seq[_], s2: Seq[_]) =>\n+      assert(s1.size === s2.size, s\"$name ${(s1, s2)}\")\n+      for (((x1, idx), x2) <- s1.zipWithIndex.zip(s2)) {\n+        compareStructures(x1, x2, s\"$name.$idx\")\n+      }\n+    case (arr1: Array[_], arr2: Array[_]) =>\n+      assert(arr1.toSeq === arr2.toSeq)\n+    case _ => throw new Exception(s\"$name: ${x1.getClass} ${x2.getClass} $x1 $x2\")\n+  }\n+\n+  private def assert2(x: => Boolean, hint: String): Unit = {\n+    try {\n+      assert(x, hint)\n+    } catch {\n+      case tfe: TestFailedException =>\n+        throw new TestFailedException(Some(s\"Failure with hint $hint\"), Some(tfe), 1)\n+    }\n+  }\n+\n+  private def makeBuffer(vecs: Seq[Vector]): Buffer = {\n+    val b = Buffer.allMetrics()\n+    for (v <- vecs) { Buffer.updateInPlace(b, v, 1.0) }\n+    b\n+  }\n+\n+  private def b(x: Array[Double]): Vector = Vectors.dense(x)\n+\n+  private def l(x: Array[Long]): Vector = b(x.map(_.toDouble))",
    "line": 215
  }],
  "prId": 17419
}, {
  "comments": [{
    "author": {
      "login": "WeichenXu123"
    },
    "body": "The print statement should move out from the loop.",
    "commit": "a569dac8998d63adbc9adba1f2eb2f42967533e7",
    "createdAt": "2017-07-20T05:46:31Z",
    "diffHunk": "@@ -0,0 +1,406 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.stat\n+\n+import org.scalatest.exceptions.TestFailedException\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.stat.SummaryBuilderImpl.Buffer\n+import org.apache.spark.ml.util.TestingUtils._\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.stat.{MultivariateOnlineSummarizer, Statistics}\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema\n+\n+class SummarizerSuite extends SparkFunSuite with MLlibTestSparkContext {\n+\n+  import testImplicits._\n+  import Summarizer._\n+\n+  private case class ExpectedMetrics(\n+      mean: Seq[Double],\n+      variance: Seq[Double],\n+      count: Long,\n+      numNonZeros: Seq[Long],\n+      max: Seq[Double],\n+      min: Seq[Double],\n+      normL2: Seq[Double],\n+      normL1: Seq[Double])\n+\n+  // The input is expected to be either a sparse vector, a dense vector or an array of doubles\n+  // (which will be converted to a dense vector)\n+  // The expected is the list of all the known metrics.\n+  //\n+  // The tests take an list of input vectors and a list of all the summary values that\n+  // are expected for this input. They currently test against some fixed subset of the\n+  // metrics, but should be made fuzzy in the future.\n+\n+  private def testExample(name: String, input: Seq[Any], exp: ExpectedMetrics): Unit = {\n+    def inputVec: Seq[Vector] = input.map {\n+      case x: Array[Double @unchecked] => Vectors.dense(x)\n+      case x: Seq[Double @unchecked] => Vectors.dense(x.toArray)\n+      case x: Vector => x\n+      case x => throw new Exception(x.toString)\n+    }\n+\n+    val s = {\n+      val s2 = new MultivariateOnlineSummarizer\n+      inputVec.foreach(v => s2.add(OldVectors.fromML(v)))\n+      s2\n+    }\n+\n+    // Because the Spark context is reset between tests, we cannot hold a reference onto it.\n+    def wrapped() = {\n+      val df = sc.parallelize(inputVec).map(Tuple1.apply).toDF(\"features\")\n+      val c = df.col(\"features\")\n+      (df, c)\n+    }\n+\n+    registerTest(s\"$name - mean only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"mean\").summary(c), mean(c)), Seq(Row(exp.mean), s.mean))\n+    }\n+\n+    registerTest(s\"$name - mean only (direct)\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(mean(c)), Seq(exp.mean))\n+    }\n+\n+    registerTest(s\"$name - variance only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"variance\").summary(c), variance(c)),\n+        Seq(Row(exp.variance), s.variance))\n+    }\n+\n+    registerTest(s\"$name - variance only (direct)\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(variance(c)), Seq(s.variance))\n+    }\n+\n+    registerTest(s\"$name - count only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"count\").summary(c), count(c)),\n+        Seq(Row(exp.count), exp.count))\n+    }\n+\n+    registerTest(s\"$name - count only (direct)\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(count(c)),\n+        Seq(exp.count))\n+    }\n+\n+    registerTest(s\"$name - numNonZeros only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"numNonZeros\").summary(c), numNonZeros(c)),\n+        Seq(Row(exp.numNonZeros), exp.numNonZeros))\n+    }\n+\n+    registerTest(s\"$name - numNonZeros only (direct)\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(numNonZeros(c)),\n+        Seq(exp.numNonZeros))\n+    }\n+\n+    registerTest(s\"$name - min only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"min\").summary(c), min(c)),\n+        Seq(Row(exp.min), exp.min))\n+    }\n+\n+    registerTest(s\"$name - max only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"max\").summary(c), max(c)),\n+        Seq(Row(exp.max), exp.max))\n+    }\n+\n+    registerTest(s\"$name - normL1 only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"normL1\").summary(c), normL1(c)),\n+        Seq(Row(exp.normL1), exp.normL1))\n+    }\n+\n+    registerTest(s\"$name - normL2 only\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(metrics(\"normL2\").summary(c), normL2(c)),\n+        Seq(Row(exp.normL2), exp.normL2))\n+    }\n+\n+    registerTest(s\"$name - all metrics at once\") {\n+      val (df, c) = wrapped()\n+      compare(df.select(\n+        metrics(\"mean\", \"variance\", \"count\", \"numNonZeros\").summary(c),\n+        mean(c), variance(c), count(c), numNonZeros(c)),\n+        Seq(Row(exp.mean, exp.variance, exp.count, exp.numNonZeros),\n+          exp.mean, exp.variance, exp.count, exp.numNonZeros))\n+    }\n+  }\n+\n+  private def denseData(input: Seq[Seq[Double]]): DataFrame = {\n+    val data = input.map(_.toArray).map(Vectors.dense).map(Tuple1.apply)\n+    sc.parallelize(data).toDF(\"features\")\n+  }\n+\n+  private def compare(df: DataFrame, exp: Seq[Any]): Unit = {\n+    val coll = df.collect().toSeq\n+    val Seq(row) = coll\n+    val res = row.toSeq\n+    val names = df.schema.fieldNames.zipWithIndex.map { case (n, idx) => s\"$n ($idx)\" }\n+    assert(res.size === exp.size, (res.size, exp.size))\n+    for (((x1, x2), name) <- res.zip(exp).zip(names)) {\n+      compareStructures(x1, x2, name)\n+    }\n+  }\n+\n+  // Compares structured content.\n+  private def compareStructures(x1: Any, x2: Any, name: String): Unit = (x1, x2) match {\n+    case (y1: Seq[Double @unchecked], v1: OldVector) =>\n+      compareStructures(y1, v1.toArray.toSeq, name)\n+    case (d1: Double, d2: Double) =>\n+      assert2(Vectors.dense(d1) ~== Vectors.dense(d2) absTol 1e-4, name)\n+    case (r1: GenericRowWithSchema, r2: Row) =>\n+      assert(r1.size === r2.size, (r1, r2))\n+      for (((fname, x1), x2) <- r1.schema.fieldNames.zip(r1.toSeq).zip(r2.toSeq)) {\n+        compareStructures(x1, x2, s\"$name.$fname\")\n+      }\n+    case (r1: Row, r2: Row) =>\n+      assert(r1.size === r2.size, (r1, r2))\n+      for ((x1, x2) <- r1.toSeq.zip(r2.toSeq)) { compareStructures(x1, x2, name) }\n+    case (v1: Vector, v2: Vector) =>\n+      assert2(v1 ~== v2 absTol 1e-4, name)\n+    case (l1: Long, l2: Long) => assert(l1 === l2)\n+    case (s1: Seq[_], s2: Seq[_]) =>\n+      assert(s1.size === s2.size, s\"$name ${(s1, s2)}\")\n+      for (((x1, idx), x2) <- s1.zipWithIndex.zip(s2)) {\n+        compareStructures(x1, x2, s\"$name.$idx\")\n+      }\n+    case (arr1: Array[_], arr2: Array[_]) =>\n+      assert(arr1.toSeq === arr2.toSeq)\n+    case _ => throw new Exception(s\"$name: ${x1.getClass} ${x2.getClass} $x1 $x2\")\n+  }\n+\n+  private def assert2(x: => Boolean, hint: String): Unit = {\n+    try {\n+      assert(x, hint)\n+    } catch {\n+      case tfe: TestFailedException =>\n+        throw new TestFailedException(Some(s\"Failure with hint $hint\"), Some(tfe), 1)\n+    }\n+  }\n+\n+  private def makeBuffer(vecs: Seq[Vector]): Buffer = {\n+    val b = Buffer.allMetrics()\n+    for (v <- vecs) { Buffer.updateInPlace(b, v, 1.0) }\n+    b\n+  }\n+\n+  private def b(x: Array[Double]): Vector = Vectors.dense(x)\n+\n+  private def l(x: Array[Long]): Vector = b(x.map(_.toDouble))\n+\n+  test(\"debugging test\") {\n+    val df = denseData(Nil)\n+    val c = df.col(\"features\")\n+    val c1 = metrics(\"mean\").summary(c)\n+    val res = df.select(c1)\n+    intercept[SparkException] {\n+      compare(res, Seq.empty)\n+    }\n+  }\n+\n+  test(\"basic error handling\") {\n+    val df = denseData(Nil)\n+    val c = df.col(\"features\")\n+    val res = df.select(metrics(\"mean\").summary(c), mean(c))\n+    intercept[SparkException] {\n+      compare(res, Seq.empty)\n+    }\n+  }\n+\n+  test(\"no element, working metrics\") {\n+    val df = denseData(Nil)\n+    val c = df.col(\"features\")\n+    val res = df.select(metrics(\"count\").summary(c), count(c))\n+    compare(res, Seq(Row(0L), 0L))\n+  }\n+\n+  {\n+    val x = Seq(0.0, 1.0, 2.0)\n+    testExample(\"single element\", Seq(x), ExpectedMetrics(\n+      mean = x,\n+      variance = Seq(0.0, 0.0, 0.0),\n+      count = 1,\n+      numNonZeros = Seq(0, 1, 1),\n+      max = x,\n+      min = x,\n+      normL1 = x,\n+      normL2 = x\n+    ))\n+  }\n+\n+  testExample(\"two elements\", Seq(Seq(0.0, 1.0, 2.0), Seq(0.0, -1.0, -2.0)), ExpectedMetrics(\n+    mean = Seq(0.0, 0.0, 0.0),\n+    // TODO: I have a doubt about these values, they are not normalized.\n+    variance = Seq(0.0, 2.0, 8.0),\n+    count = 2,\n+    numNonZeros = Seq(0, 2, 2),\n+    max = Seq(0.0, 1.0, 2.0),\n+    min = Seq(0.0, -1.0, -2.0),\n+    normL1 = Seq(0.0, 2.0, 4.0),\n+    normL2 = Seq(0.0, math.sqrt(2.0), math.sqrt(2.0) * 2.0)\n+  ))\n+\n+  testExample(\"dense vector input\",\n+    Seq(Seq(-1.0, 0.0, 6.0), Seq(3.0, -3.0, 0.0)),\n+    ExpectedMetrics(\n+      mean = Seq(1.0, -1.5, 3.0),\n+      variance = Seq(8.0, 4.5, 18.0),\n+      count = 2,\n+      numNonZeros = Seq(2, 1, 1),\n+      max = Seq(3.0, 0.0, 6.0),\n+      min = Seq(-1.0, -3, 0.0),\n+      normL1 = Seq(4.0, 3.0, 6.0),\n+      normL2 = Seq(math.sqrt(10), 3, 6.0)\n+  ))\n+\n+  test(\"mixing dense and sparse vector input\") {\n+    val summarizer = makeBuffer(Seq(\n+      Vectors.sparse(3, Seq((0, -2.0), (1, 2.3))),\n+      Vectors.dense(0.0, -1.0, -3.0),\n+      Vectors.sparse(3, Seq((1, -5.1))),\n+      Vectors.dense(3.8, 0.0, 1.9),\n+      Vectors.dense(1.7, -0.6, 0.0),\n+      Vectors.sparse(3, Seq((1, 1.9), (2, 0.0)))))\n+\n+    assert(b(Buffer.mean(summarizer)) ~==\n+      Vectors.dense(0.583333333333, -0.416666666666, -0.183333333333) absTol 1E-5, \"mean mismatch\")\n+\n+    assert(b(Buffer.min(summarizer)) ~== Vectors.dense(-2.0, -5.1, -3) absTol 1E-5, \"min \" +\n+      \"mismatch\")\n+\n+    assert(b(Buffer.max(summarizer)) ~== Vectors.dense(3.8, 2.3, 1.9) absTol 1E-5, \"max mismatch\")\n+\n+    assert(l(Buffer.nnz(summarizer)) ~== Vectors.dense(3, 5, 2) absTol 1E-5, \"numNonzeros mismatch\")\n+\n+    assert(b(Buffer.variance(summarizer)) ~==\n+      Vectors.dense(3.857666666666, 7.0456666666666, 2.48166666666666) absTol 1E-5,\n+      \"variance mismatch\")\n+\n+    assert(Buffer.totalCount(summarizer) === 6)\n+  }\n+\n+\n+  test(\"merging two summarizers\") {\n+    val summarizer1 = makeBuffer(Seq(\n+      Vectors.sparse(3, Seq((0, -2.0), (1, 2.3))),\n+      Vectors.dense(0.0, -1.0, -3.0)))\n+\n+    val summarizer2 = makeBuffer(Seq(\n+      Vectors.sparse(3, Seq((1, -5.1))),\n+      Vectors.dense(3.8, 0.0, 1.9),\n+      Vectors.dense(1.7, -0.6, 0.0),\n+      Vectors.sparse(3, Seq((1, 1.9), (2, 0.0)))))\n+\n+    val summarizer = Buffer.mergeBuffers(summarizer1, summarizer2)\n+\n+    assert(b(Buffer.mean(summarizer)) ~==\n+      Vectors.dense(0.583333333333, -0.416666666666, -0.183333333333) absTol 1E-5, \"mean mismatch\")\n+\n+    assert(b(Buffer.min(summarizer)) ~== Vectors.dense(-2.0, -5.1, -3) absTol 1E-5, \"min mismatch\")\n+\n+    assert(b(Buffer.max(summarizer)) ~== Vectors.dense(3.8, 2.3, 1.9) absTol 1E-5, \"max mismatch\")\n+\n+    assert(l(Buffer.nnz(summarizer)) ~== Vectors.dense(3, 5, 2) absTol 1E-5, \"numNonzeros mismatch\")\n+\n+    assert(b(Buffer.variance(summarizer)) ~==\n+      Vectors.dense(3.857666666666, 7.0456666666666, 2.48166666666666) absTol 1E-5,\n+      \"variance mismatch\")\n+\n+    assert(Buffer.totalCount(summarizer) === 6)\n+  }\n+\n+  // TODO: this test should not be committed. It is here to isolate some performance hotspots.\n+  test(\"perf test\") {\n+    val n = 10000000\n+    val rdd1 = sc.parallelize(1 to n).map { idx =>\n+      OldVectors.dense(idx.toDouble)\n+    }\n+    val trieouts = 100\n+    rdd1.cache()\n+    rdd1.count()\n+    val rdd2 = sc.parallelize(1 to n).map { idx =>\n+      Vectors.dense(idx.toDouble)\n+    }\n+    rdd2.cache()\n+    rdd2.count()\n+    val df = rdd2.map(Tuple1.apply).toDF(\"features\")\n+    df.cache()\n+    df.count()\n+    val x = df.select(\n+      metrics(\"mean\", \"variance\", \"count\", \"numNonZeros\", \"max\", \"min\", \"normL1\",\n+              \"normL2\").summary($\"features\"))\n+    val x1 = df.select(metrics(\"variance\").summary($\"features\"))\n+\n+    def print(name: String, l: List[Long]): Unit = {\n+      def f(z: Long) = (1e6 * n.toDouble) / z\n+      val min = f(l.max)\n+      val max = f(l.min)\n+      val med = f(l.sorted.drop(l.size / 2).head)\n+\n+      // scalastyle:off println\n+      println(s\"$name = [$min ~ $med ~ $max] records / milli\")\n+    }\n+\n+\n+    var times_df: List[Long] = Nil\n+    for (_ <- 1 to trieouts) {\n+      System.gc()\n+      val t21 = System.nanoTime()\n+      x.head()\n+      val t22 = System.nanoTime()\n+      val dt = t22 - t21\n+      times_df ::= dt\n+      // scalastyle:off\n+      print(\"Dataframes\", times_df)\n+      // scalastyle:on",
    "line": 381
  }],
  "prId": 17419
}]