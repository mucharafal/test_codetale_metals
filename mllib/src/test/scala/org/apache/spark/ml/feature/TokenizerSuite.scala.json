[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "remove extra empty line\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-03-18T14:07:51Z",
    "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.scalatest.FunSuite\n+\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.{DataFrame, Row, SQLContext}\n+\n+"
  }],
  "prId": 4504
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "space after `,` and put an empty line after.\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-03-18T14:07:52Z",
    "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.scalatest.FunSuite\n+\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.{DataFrame, Row, SQLContext}\n+\n+\n+case class TextData(rawText : String,wantedTokens: Seq[String])"
  }],
  "prId": 4504
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "initialize `dataset` here to avoid duplicate code. If they are not the same in tests, please remove dataset from TokenizerSuite and define local variables in each test.\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-03-18T14:07:54Z",
    "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.scalatest.FunSuite\n+\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.{DataFrame, Row, SQLContext}\n+\n+\n+case class TextData(rawText : String,wantedTokens: Seq[String])\n+class TokenizerSuite extends FunSuite with MLlibTestSparkContext {\n+  \n+  @transient var sqlContext: SQLContext = _\n+  @transient var dataset: DataFrame = _\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    sqlContext = new SQLContext(sc)"
  }],
  "prId": 4504
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "space before `{`\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-03-18T14:07:55Z",
    "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.scalatest.FunSuite\n+\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.{DataFrame, Row, SQLContext}\n+\n+\n+case class TextData(rawText : String,wantedTokens: Seq[String])\n+class TokenizerSuite extends FunSuite with MLlibTestSparkContext {\n+  \n+  @transient var sqlContext: SQLContext = _\n+  @transient var dataset: DataFrame = _\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    sqlContext = new SQLContext(sc)\n+  }\n+\n+  test(\"RegexTokenizer\"){\n+    var myRegExTokenizer = new RegexTokenizer()\n+      .setInputCol(\"rawText\")\n+      .setOutputCol(\"tokens\")\n+\n+    dataset = sqlContext.createDataFrame(\n+      sc.parallelize(List(\n+        TextData(\"Test for tokenization.\",List(\"Test\",\"for\",\"tokenization\",\".\")),\n+        TextData(\"Te,st. punct\",List(\"Te\",\",\",\"st\",\".\",\"punct\"))\n+    )))\n+    testTokenizer(myRegExTokenizer,dataset)\n+\n+    dataset = sqlContext.createDataFrame(\n+      sc.parallelize(List(\n+        TextData(\"Test for tokenization.\",List(\"Test\",\"for\",\"tokenization\")),\n+        TextData(\"Te,st. punct\",List(\"punct\"))\n+    )))\n+    myRegExTokenizer.asInstanceOf[RegexTokenizer]\n+      .setMinTokenLength(3)\n+    testTokenizer(myRegExTokenizer,dataset)\n+\n+    myRegExTokenizer.asInstanceOf[RegexTokenizer]\n+      .setPattern(\"\\\\s\")\n+      .setGaps(true)\n+      .setMinTokenLength(0)\n+    dataset = sqlContext.createDataFrame(\n+      sc.parallelize(List(\n+        TextData(\"Test for tokenization.\",List(\"Test\",\"for\",\"tokenization.\")),\n+        TextData(\"Te,st.  punct\",List(\"Te,st.\",\"\",\"punct\"))\n+    )))\n+    testTokenizer(myRegExTokenizer,dataset)\n+  }\n+\n+  test(\"Tokenizer\"){\n+    val oldTokenizer =  new Tokenizer()\n+      .setInputCol(\"rawText\")\n+      .setOutputCol(\"tokens\")\n+    dataset = sqlContext.createDataFrame(\n+      sc.parallelize(List(\n+        TextData(\"Test for tokenization.\",List(\"test\",\"for\",\"tokenization.\")),\n+        TextData(\"Te,st.  punct\",List(\"te,st.\",\"\",\"punct\"))\n+    )))\n+    testTokenizer(oldTokenizer,dataset)\n+  }\n+\n+  def testTokenizer(t: Tokenizer,dataset: DataFrame){"
  }],
  "prId": 4504
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Please configure your editor to not use tabs.\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-03-18T14:07:56Z",
    "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.scalatest.FunSuite\n+\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.{DataFrame, Row, SQLContext}\n+\n+\n+case class TextData(rawText : String,wantedTokens: Seq[String])\n+class TokenizerSuite extends FunSuite with MLlibTestSparkContext {\n+  \n+  @transient var sqlContext: SQLContext = _\n+  @transient var dataset: DataFrame = _\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    sqlContext = new SQLContext(sc)\n+  }\n+\n+  test(\"RegexTokenizer\"){\n+    var myRegExTokenizer = new RegexTokenizer()\n+      .setInputCol(\"rawText\")\n+      .setOutputCol(\"tokens\")\n+\n+    dataset = sqlContext.createDataFrame(\n+      sc.parallelize(List(\n+        TextData(\"Test for tokenization.\",List(\"Test\",\"for\",\"tokenization\",\".\")),\n+        TextData(\"Te,st. punct\",List(\"Te\",\",\",\"st\",\".\",\"punct\"))\n+    )))\n+    testTokenizer(myRegExTokenizer,dataset)\n+\n+    dataset = sqlContext.createDataFrame(\n+      sc.parallelize(List(\n+        TextData(\"Test for tokenization.\",List(\"Test\",\"for\",\"tokenization\")),\n+        TextData(\"Te,st. punct\",List(\"punct\"))\n+    )))\n+    myRegExTokenizer.asInstanceOf[RegexTokenizer]\n+      .setMinTokenLength(3)\n+    testTokenizer(myRegExTokenizer,dataset)\n+\n+    myRegExTokenizer.asInstanceOf[RegexTokenizer]\n+      .setPattern(\"\\\\s\")\n+      .setGaps(true)\n+      .setMinTokenLength(0)\n+    dataset = sqlContext.createDataFrame(\n+      sc.parallelize(List(\n+        TextData(\"Test for tokenization.\",List(\"Test\",\"for\",\"tokenization.\")),\n+        TextData(\"Te,st.  punct\",List(\"Te,st.\",\"\",\"punct\"))\n+    )))\n+    testTokenizer(myRegExTokenizer,dataset)\n+  }\n+\n+  test(\"Tokenizer\"){\n+    val oldTokenizer =  new Tokenizer()\n+      .setInputCol(\"rawText\")\n+      .setOutputCol(\"tokens\")\n+    dataset = sqlContext.createDataFrame(\n+      sc.parallelize(List(\n+        TextData(\"Test for tokenization.\",List(\"test\",\"for\",\"tokenization.\")),\n+        TextData(\"Te,st.  punct\",List(\"te,st.\",\"\",\"punct\"))\n+    )))\n+    testTokenizer(oldTokenizer,dataset)\n+  }\n+\n+  def testTokenizer(t: Tokenizer,dataset: DataFrame){\n+  \tt.transform(dataset)"
  }],
  "prId": 4504
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`assert(tokens === wantedTokens)` should work.\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-03-18T14:07:58Z",
    "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.scalatest.FunSuite\n+\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.{DataFrame, Row, SQLContext}\n+\n+\n+case class TextData(rawText : String,wantedTokens: Seq[String])\n+class TokenizerSuite extends FunSuite with MLlibTestSparkContext {\n+  \n+  @transient var sqlContext: SQLContext = _\n+  @transient var dataset: DataFrame = _\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    sqlContext = new SQLContext(sc)\n+  }\n+\n+  test(\"RegexTokenizer\"){\n+    var myRegExTokenizer = new RegexTokenizer()\n+      .setInputCol(\"rawText\")\n+      .setOutputCol(\"tokens\")\n+\n+    dataset = sqlContext.createDataFrame(\n+      sc.parallelize(List(\n+        TextData(\"Test for tokenization.\",List(\"Test\",\"for\",\"tokenization\",\".\")),\n+        TextData(\"Te,st. punct\",List(\"Te\",\",\",\"st\",\".\",\"punct\"))\n+    )))\n+    testTokenizer(myRegExTokenizer,dataset)\n+\n+    dataset = sqlContext.createDataFrame(\n+      sc.parallelize(List(\n+        TextData(\"Test for tokenization.\",List(\"Test\",\"for\",\"tokenization\")),\n+        TextData(\"Te,st. punct\",List(\"punct\"))\n+    )))\n+    myRegExTokenizer.asInstanceOf[RegexTokenizer]\n+      .setMinTokenLength(3)\n+    testTokenizer(myRegExTokenizer,dataset)\n+\n+    myRegExTokenizer.asInstanceOf[RegexTokenizer]\n+      .setPattern(\"\\\\s\")\n+      .setGaps(true)\n+      .setMinTokenLength(0)\n+    dataset = sqlContext.createDataFrame(\n+      sc.parallelize(List(\n+        TextData(\"Test for tokenization.\",List(\"Test\",\"for\",\"tokenization.\")),\n+        TextData(\"Te,st.  punct\",List(\"Te,st.\",\"\",\"punct\"))\n+    )))\n+    testTokenizer(myRegExTokenizer,dataset)\n+  }\n+\n+  test(\"Tokenizer\"){\n+    val oldTokenizer =  new Tokenizer()\n+      .setInputCol(\"rawText\")\n+      .setOutputCol(\"tokens\")\n+    dataset = sqlContext.createDataFrame(\n+      sc.parallelize(List(\n+        TextData(\"Test for tokenization.\",List(\"test\",\"for\",\"tokenization.\")),\n+        TextData(\"Te,st.  punct\",List(\"te,st.\",\"\",\"punct\"))\n+    )))\n+    testTokenizer(oldTokenizer,dataset)\n+  }\n+\n+  def testTokenizer(t: Tokenizer,dataset: DataFrame){\n+  \tt.transform(dataset)\n+      .select(\"tokens\",\"wantedTokens\")\n+      .collect().foreach{ \n+        case Row(tokens: Seq[String], wantedTokens: Seq[String]) =>\n+          assert(tokens.length == wantedTokens.length)"
  }],
  "prId": 4504
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "remove `println` and throw an exception with the content of the Row.\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-03-18T14:07:59Z",
    "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.scalatest.FunSuite\n+\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.{DataFrame, Row, SQLContext}\n+\n+\n+case class TextData(rawText : String,wantedTokens: Seq[String])\n+class TokenizerSuite extends FunSuite with MLlibTestSparkContext {\n+  \n+  @transient var sqlContext: SQLContext = _\n+  @transient var dataset: DataFrame = _\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    sqlContext = new SQLContext(sc)\n+  }\n+\n+  test(\"RegexTokenizer\"){\n+    var myRegExTokenizer = new RegexTokenizer()\n+      .setInputCol(\"rawText\")\n+      .setOutputCol(\"tokens\")\n+\n+    dataset = sqlContext.createDataFrame(\n+      sc.parallelize(List(\n+        TextData(\"Test for tokenization.\",List(\"Test\",\"for\",\"tokenization\",\".\")),\n+        TextData(\"Te,st. punct\",List(\"Te\",\",\",\"st\",\".\",\"punct\"))\n+    )))\n+    testTokenizer(myRegExTokenizer,dataset)\n+\n+    dataset = sqlContext.createDataFrame(\n+      sc.parallelize(List(\n+        TextData(\"Test for tokenization.\",List(\"Test\",\"for\",\"tokenization\")),\n+        TextData(\"Te,st. punct\",List(\"punct\"))\n+    )))\n+    myRegExTokenizer.asInstanceOf[RegexTokenizer]\n+      .setMinTokenLength(3)\n+    testTokenizer(myRegExTokenizer,dataset)\n+\n+    myRegExTokenizer.asInstanceOf[RegexTokenizer]\n+      .setPattern(\"\\\\s\")\n+      .setGaps(true)\n+      .setMinTokenLength(0)\n+    dataset = sqlContext.createDataFrame(\n+      sc.parallelize(List(\n+        TextData(\"Test for tokenization.\",List(\"Test\",\"for\",\"tokenization.\")),\n+        TextData(\"Te,st.  punct\",List(\"Te,st.\",\"\",\"punct\"))\n+    )))\n+    testTokenizer(myRegExTokenizer,dataset)\n+  }\n+\n+  test(\"Tokenizer\"){\n+    val oldTokenizer =  new Tokenizer()\n+      .setInputCol(\"rawText\")\n+      .setOutputCol(\"tokens\")\n+    dataset = sqlContext.createDataFrame(\n+      sc.parallelize(List(\n+        TextData(\"Test for tokenization.\",List(\"test\",\"for\",\"tokenization.\")),\n+        TextData(\"Te,st.  punct\",List(\"te,st.\",\"\",\"punct\"))\n+    )))\n+    testTokenizer(oldTokenizer,dataset)\n+  }\n+\n+  def testTokenizer(t: Tokenizer,dataset: DataFrame){\n+  \tt.transform(dataset)\n+      .select(\"tokens\",\"wantedTokens\")\n+      .collect().foreach{ \n+        case Row(tokens: Seq[String], wantedTokens: Seq[String]) =>\n+          assert(tokens.length == wantedTokens.length)\n+          tokens.zip(wantedTokens).foreach(x => assert(x._1 == x._2))\n+        case _ => \n+          println()"
  }],
  "prId": 4504
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "insert an empty line between class declarations.\nremove space before `:`.\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-03-20T19:19:45Z",
    "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.scalatest.FunSuite\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.{DataFrame, Row, SQLContext}\n+\n+case class TextData(rawText : String, wantedTokens: Seq[String])"
  }],
  "prId": 4504
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "minor: `Seq` is used more often than `List` in the MLlib codebase.\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-03-20T19:19:47Z",
    "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.scalatest.FunSuite\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.{DataFrame, Row, SQLContext}\n+\n+case class TextData(rawText : String, wantedTokens: Seq[String])\n+class TokenizerSuite extends FunSuite with MLlibTestSparkContext {\n+  \n+  @transient var sqlContext: SQLContext = _\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    sqlContext = new SQLContext(sc)\n+  }\n+\n+  test(\"RegexTokenizer\"){\n+    val myRegExTokenizer = new RegexTokenizer()\n+      .setInputCol(\"rawText\")\n+      .setOutputCol(\"tokens\")\n+\n+    var dataset = sqlContext.createDataFrame(\n+      sc.parallelize(List("
  }],
  "prId": 4504
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "space after `,` (please also check this in other places.)\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-03-20T19:19:48Z",
    "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.scalatest.FunSuite\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.{DataFrame, Row, SQLContext}\n+\n+case class TextData(rawText : String, wantedTokens: Seq[String])\n+class TokenizerSuite extends FunSuite with MLlibTestSparkContext {\n+  \n+  @transient var sqlContext: SQLContext = _\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    sqlContext = new SQLContext(sc)\n+  }\n+\n+  test(\"RegexTokenizer\"){\n+    val myRegExTokenizer = new RegexTokenizer()\n+      .setInputCol(\"rawText\")\n+      .setOutputCol(\"tokens\")\n+\n+    var dataset = sqlContext.createDataFrame(\n+      sc.parallelize(List(\n+        TextData(\"Test for tokenization.\",List(\"Test\",\"for\",\"tokenization\",\".\")),"
  }],
  "prId": 4504
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "space before `{` (please also check this in other places)\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-03-20T19:19:50Z",
    "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.scalatest.FunSuite\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.{DataFrame, Row, SQLContext}\n+\n+case class TextData(rawText : String, wantedTokens: Seq[String])\n+class TokenizerSuite extends FunSuite with MLlibTestSparkContext {\n+  \n+  @transient var sqlContext: SQLContext = _\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    sqlContext = new SQLContext(sc)\n+  }\n+\n+  test(\"RegexTokenizer\"){\n+    val myRegExTokenizer = new RegexTokenizer()\n+      .setInputCol(\"rawText\")\n+      .setOutputCol(\"tokens\")\n+\n+    var dataset = sqlContext.createDataFrame(\n+      sc.parallelize(List(\n+        TextData(\"Test for tokenization.\",List(\"Test\",\"for\",\"tokenization\",\".\")),\n+        TextData(\"Te,st. punct\",List(\"Te\",\",\",\"st\",\".\",\"punct\"))\n+    )))\n+    testRegexTokenizer(myRegExTokenizer,dataset)\n+\n+    dataset = sqlContext.createDataFrame(\n+      sc.parallelize(List(\n+        TextData(\"Test for tokenization.\",List(\"Test\",\"for\",\"tokenization\")),\n+        TextData(\"Te,st. punct\",List(\"punct\"))\n+    )))\n+    myRegExTokenizer.asInstanceOf[RegexTokenizer]\n+      .setMinTokenLength(3)\n+    testRegexTokenizer(myRegExTokenizer,dataset)\n+\n+    myRegExTokenizer.asInstanceOf[RegexTokenizer]\n+      .setPattern(\"\\\\s\")\n+      .setGaps(true)\n+      .setMinTokenLength(0)\n+    dataset = sqlContext.createDataFrame(\n+      sc.parallelize(List(\n+        TextData(\"Test for tokenization.\",List(\"Test\",\"for\",\"tokenization.\")),\n+        TextData(\"Te,st.  punct\",List(\"Te,st.\",\"\",\"punct\"))\n+    )))\n+    testRegexTokenizer(myRegExTokenizer,dataset)\n+  }\n+\n+  test(\"Tokenizer\") {\n+    val oldTokenizer =  new Tokenizer()\n+      .setInputCol(\"rawText\")\n+      .setOutputCol(\"tokens\")\n+    var dataset = sqlContext.createDataFrame(\n+      sc.parallelize(List(\n+        TextData(\"Test for tokenization.\",List(\"test\",\"for\",\"tokenization.\")),\n+        TextData(\"Te,st.  punct\",List(\"te,st.\",\"\",\"punct\"))\n+    )))\n+    testTokenizer(oldTokenizer,dataset)\n+  }\n+\n+  def testTokenizer(t: Tokenizer,dataset: DataFrame): Unit = {\n+    t.transform(dataset)\n+      .select(\"tokens\",\"wantedTokens\")\n+      .collect().foreach{ "
  }],
  "prId": 4504
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`SparkException` should happen on workers. Since data is already collected, we can use `fail(\"...\")`. For this test, maybe the following is sufficient:\n\n``` scala\n.collect()\n.foreach { case Row(actual, expected) =>\n  assert(actual === expected)\n}\n```\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-03-20T19:19:51Z",
    "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.scalatest.FunSuite\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.sql.{DataFrame, Row, SQLContext}\n+\n+case class TextData(rawText : String, wantedTokens: Seq[String])\n+class TokenizerSuite extends FunSuite with MLlibTestSparkContext {\n+  \n+  @transient var sqlContext: SQLContext = _\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    sqlContext = new SQLContext(sc)\n+  }\n+\n+  test(\"RegexTokenizer\"){\n+    val myRegExTokenizer = new RegexTokenizer()\n+      .setInputCol(\"rawText\")\n+      .setOutputCol(\"tokens\")\n+\n+    var dataset = sqlContext.createDataFrame(\n+      sc.parallelize(List(\n+        TextData(\"Test for tokenization.\",List(\"Test\",\"for\",\"tokenization\",\".\")),\n+        TextData(\"Te,st. punct\",List(\"Te\",\",\",\"st\",\".\",\"punct\"))\n+    )))\n+    testRegexTokenizer(myRegExTokenizer,dataset)\n+\n+    dataset = sqlContext.createDataFrame(\n+      sc.parallelize(List(\n+        TextData(\"Test for tokenization.\",List(\"Test\",\"for\",\"tokenization\")),\n+        TextData(\"Te,st. punct\",List(\"punct\"))\n+    )))\n+    myRegExTokenizer.asInstanceOf[RegexTokenizer]\n+      .setMinTokenLength(3)\n+    testRegexTokenizer(myRegExTokenizer,dataset)\n+\n+    myRegExTokenizer.asInstanceOf[RegexTokenizer]\n+      .setPattern(\"\\\\s\")\n+      .setGaps(true)\n+      .setMinTokenLength(0)\n+    dataset = sqlContext.createDataFrame(\n+      sc.parallelize(List(\n+        TextData(\"Test for tokenization.\",List(\"Test\",\"for\",\"tokenization.\")),\n+        TextData(\"Te,st.  punct\",List(\"Te,st.\",\"\",\"punct\"))\n+    )))\n+    testRegexTokenizer(myRegExTokenizer,dataset)\n+  }\n+\n+  test(\"Tokenizer\") {\n+    val oldTokenizer =  new Tokenizer()\n+      .setInputCol(\"rawText\")\n+      .setOutputCol(\"tokens\")\n+    var dataset = sqlContext.createDataFrame(\n+      sc.parallelize(List(\n+        TextData(\"Test for tokenization.\",List(\"test\",\"for\",\"tokenization.\")),\n+        TextData(\"Te,st.  punct\",List(\"te,st.\",\"\",\"punct\"))\n+    )))\n+    testTokenizer(oldTokenizer,dataset)\n+  }\n+\n+  def testTokenizer(t: Tokenizer,dataset: DataFrame): Unit = {\n+    t.transform(dataset)\n+      .select(\"tokens\",\"wantedTokens\")\n+      .collect().foreach{ \n+        case Row(tokens: Seq[Any], wantedTokens: Seq[Any]) =>\n+          assert(tokens === wantedTokens)\n+        case e =>\n+          throw new SparkException(s\"Row $e should contain only tokens and wantedTokens columns\")"
  }],
  "prId": 4504
}]