[{
  "comments": [{
    "author": {
      "login": "imatiach-msft"
    },
    "body": "can the special value that is above the max categorical limit of 10000 be refactored to a constant?",
    "commit": "19fa02ad6d8cd73553cc804828e659918c6fa872",
    "createdAt": "2017-03-01T23:08:56Z",
    "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.stat\n+\n+import java.util.Random\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.feature.LabeledPoint\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.util.DefaultReadWriteTest\n+import org.apache.spark.ml.util.TestingUtils._\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+\n+class ChiSquareSuite\n+  extends SparkFunSuite with MLlibTestSparkContext with DefaultReadWriteTest {\n+\n+  import testImplicits._\n+\n+  test(\"test DataFrame of labeled points\") {\n+    // labels: 1.0 (2 / 6), 0.0 (4 / 6)\n+    // feature1: 0.5 (1 / 6), 1.5 (2 / 6), 3.5 (3 / 6)\n+    // feature2: 10.0 (1 / 6), 20.0 (1 / 6), 30.0 (2 / 6), 40.0 (2 / 6)\n+    val data = Seq(\n+      LabeledPoint(0.0, Vectors.dense(0.5, 10.0)),\n+      LabeledPoint(0.0, Vectors.dense(1.5, 20.0)),\n+      LabeledPoint(1.0, Vectors.dense(1.5, 30.0)),\n+      LabeledPoint(0.0, Vectors.dense(3.5, 30.0)),\n+      LabeledPoint(0.0, Vectors.dense(3.5, 40.0)),\n+      LabeledPoint(1.0, Vectors.dense(3.5, 40.0)))\n+    for (numParts <- List(2, 4, 6, 8)) {\n+      val df = spark.createDataFrame(sc.parallelize(data, numParts))\n+      val chi = ChiSquare.test(df, \"features\", \"label\")\n+      val (pValues: Vector, degreesOfFreedom: Array[Int], statistics: Vector) =\n+        chi.select(\"pValues\", \"degreesOfFreedom\", \"statistics\")\n+          .as[(Vector, Array[Int], Vector)].head()\n+      assert(pValues ~== Vectors.dense(0.6873, 0.6823) relTol 1e-4)\n+      assert(degreesOfFreedom === Array(2, 3))\n+      assert(statistics ~== Vectors.dense(0.75, 1.5) relTol 1e-4)\n+    }\n+  }\n+\n+  test(\"large number of features (SPARK-3087)\") {\n+    // Test that the right number of results is returned\n+    val numCols = 1001\n+    val sparseData = Array(\n+      LabeledPoint(0.0, Vectors.sparse(numCols, Seq((100, 2.0)))),\n+      LabeledPoint(0.1, Vectors.sparse(numCols, Seq((200, 1.0)))))\n+    val df = spark.createDataFrame(sparseData)\n+    val chi = ChiSquare.test(df, \"features\", \"label\")\n+    val (pValues: Vector, degreesOfFreedom: Array[Int], statistics: Vector) =\n+      chi.select(\"pValues\", \"degreesOfFreedom\", \"statistics\")\n+        .as[(Vector, Array[Int], Vector)].head()\n+    assert(pValues.size === numCols)\n+    assert(degreesOfFreedom.length === numCols)\n+    assert(statistics.size === numCols)\n+    assert(pValues(1000) !== null)  // SPARK-3087\n+  }\n+\n+  test(\"fail on continuous features or labels\") {\n+    // Detect continuous features or labels\n+    val random = new Random(11L)\n+    val continuousLabel =\n+      Seq.fill(100000)(LabeledPoint(random.nextDouble(), Vectors.dense(random.nextInt(2))))"
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "Good idea, done now.",
    "commit": "19fa02ad6d8cd73553cc804828e659918c6fa872",
    "createdAt": "2017-03-03T19:01:16Z",
    "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.stat\n+\n+import java.util.Random\n+\n+import org.apache.spark.{SparkException, SparkFunSuite}\n+import org.apache.spark.ml.feature.LabeledPoint\n+import org.apache.spark.ml.linalg.{Vector, Vectors}\n+import org.apache.spark.ml.util.DefaultReadWriteTest\n+import org.apache.spark.ml.util.TestingUtils._\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+\n+class ChiSquareSuite\n+  extends SparkFunSuite with MLlibTestSparkContext with DefaultReadWriteTest {\n+\n+  import testImplicits._\n+\n+  test(\"test DataFrame of labeled points\") {\n+    // labels: 1.0 (2 / 6), 0.0 (4 / 6)\n+    // feature1: 0.5 (1 / 6), 1.5 (2 / 6), 3.5 (3 / 6)\n+    // feature2: 10.0 (1 / 6), 20.0 (1 / 6), 30.0 (2 / 6), 40.0 (2 / 6)\n+    val data = Seq(\n+      LabeledPoint(0.0, Vectors.dense(0.5, 10.0)),\n+      LabeledPoint(0.0, Vectors.dense(1.5, 20.0)),\n+      LabeledPoint(1.0, Vectors.dense(1.5, 30.0)),\n+      LabeledPoint(0.0, Vectors.dense(3.5, 30.0)),\n+      LabeledPoint(0.0, Vectors.dense(3.5, 40.0)),\n+      LabeledPoint(1.0, Vectors.dense(3.5, 40.0)))\n+    for (numParts <- List(2, 4, 6, 8)) {\n+      val df = spark.createDataFrame(sc.parallelize(data, numParts))\n+      val chi = ChiSquare.test(df, \"features\", \"label\")\n+      val (pValues: Vector, degreesOfFreedom: Array[Int], statistics: Vector) =\n+        chi.select(\"pValues\", \"degreesOfFreedom\", \"statistics\")\n+          .as[(Vector, Array[Int], Vector)].head()\n+      assert(pValues ~== Vectors.dense(0.6873, 0.6823) relTol 1e-4)\n+      assert(degreesOfFreedom === Array(2, 3))\n+      assert(statistics ~== Vectors.dense(0.75, 1.5) relTol 1e-4)\n+    }\n+  }\n+\n+  test(\"large number of features (SPARK-3087)\") {\n+    // Test that the right number of results is returned\n+    val numCols = 1001\n+    val sparseData = Array(\n+      LabeledPoint(0.0, Vectors.sparse(numCols, Seq((100, 2.0)))),\n+      LabeledPoint(0.1, Vectors.sparse(numCols, Seq((200, 1.0)))))\n+    val df = spark.createDataFrame(sparseData)\n+    val chi = ChiSquare.test(df, \"features\", \"label\")\n+    val (pValues: Vector, degreesOfFreedom: Array[Int], statistics: Vector) =\n+      chi.select(\"pValues\", \"degreesOfFreedom\", \"statistics\")\n+        .as[(Vector, Array[Int], Vector)].head()\n+    assert(pValues.size === numCols)\n+    assert(degreesOfFreedom.length === numCols)\n+    assert(statistics.size === numCols)\n+    assert(pValues(1000) !== null)  // SPARK-3087\n+  }\n+\n+  test(\"fail on continuous features or labels\") {\n+    // Detect continuous features or labels\n+    val random = new Random(11L)\n+    val continuousLabel =\n+      Seq.fill(100000)(LabeledPoint(random.nextDouble(), Vectors.dense(random.nextInt(2))))"
  }],
  "prId": 17110
}]