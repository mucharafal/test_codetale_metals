[{
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "nit: No need for the explicit type `DataFrame`.",
    "commit": "486b68d1de9e9dc480133d8680baebc98f6e572c",
    "createdAt": "2017-11-12T06:47:16Z",
    "diffHunk": "@@ -146,4 +146,172 @@ class QuantileDiscretizerSuite\n     val model = discretizer.fit(df)\n     assert(model.hasParent)\n   }\n+\n+  test(\"Multiple Columns: Test observed number of buckets and their sizes match expected values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val datasetSize = 100000\n+    val numBuckets = 5\n+    val data1 = Array.range(1, 100001, 1).map(_.toDouble)\n+    val data2 = Array.range(1, 200000, 2).map(_.toDouble)\n+    val data = (0 until 100000).map { idx =>\n+      (data1(idx), data2(idx))\n+    }\n+    val df: DataFrame = data.toSeq.toDF(\"input1\", \"input2\")"
  }, {
    "author": {
      "login": "huaxingao"
    },
    "body": "Will remove DataFrame",
    "commit": "486b68d1de9e9dc480133d8680baebc98f6e572c",
    "createdAt": "2017-11-13T05:23:33Z",
    "diffHunk": "@@ -146,4 +146,172 @@ class QuantileDiscretizerSuite\n     val model = discretizer.fit(df)\n     assert(model.hasParent)\n   }\n+\n+  test(\"Multiple Columns: Test observed number of buckets and their sizes match expected values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val datasetSize = 100000\n+    val numBuckets = 5\n+    val data1 = Array.range(1, 100001, 1).map(_.toDouble)\n+    val data2 = Array.range(1, 200000, 2).map(_.toDouble)\n+    val data = (0 until 100000).map { idx =>\n+      (data1(idx), data2(idx))\n+    }\n+    val df: DataFrame = data.toSeq.toDF(\"input1\", \"input2\")"
  }],
  "prId": 19715
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "`val data` seems just as `data1.zip(data2)`?",
    "commit": "486b68d1de9e9dc480133d8680baebc98f6e572c",
    "createdAt": "2017-11-12T06:49:26Z",
    "diffHunk": "@@ -146,4 +146,172 @@ class QuantileDiscretizerSuite\n     val model = discretizer.fit(df)\n     assert(model.hasParent)\n   }\n+\n+  test(\"Multiple Columns: Test observed number of buckets and their sizes match expected values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val datasetSize = 100000\n+    val numBuckets = 5\n+    val data1 = Array.range(1, 100001, 1).map(_.toDouble)\n+    val data2 = Array.range(1, 200000, 2).map(_.toDouble)\n+    val data = (0 until 100000).map { idx =>\n+      (data1(idx), data2(idx))\n+    }"
  }, {
    "author": {
      "login": "huaxingao"
    },
    "body": "Yes. Will change to data1.zip(data2)",
    "commit": "486b68d1de9e9dc480133d8680baebc98f6e572c",
    "createdAt": "2017-11-13T05:23:13Z",
    "diffHunk": "@@ -146,4 +146,172 @@ class QuantileDiscretizerSuite\n     val model = discretizer.fit(df)\n     assert(model.hasParent)\n   }\n+\n+  test(\"Multiple Columns: Test observed number of buckets and their sizes match expected values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val datasetSize = 100000\n+    val numBuckets = 5\n+    val data1 = Array.range(1, 100001, 1).map(_.toDouble)\n+    val data2 = Array.range(1, 200000, 2).map(_.toDouble)\n+    val data = (0 until 100000).map { idx =>\n+      (data1(idx), data2(idx))\n+    }"
  }],
  "prId": 19715
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "Use `data1.zip(data2)`?",
    "commit": "486b68d1de9e9dc480133d8680baebc98f6e572c",
    "createdAt": "2017-11-12T06:51:27Z",
    "diffHunk": "@@ -146,4 +146,172 @@ class QuantileDiscretizerSuite\n     val model = discretizer.fit(df)\n     assert(model.hasParent)\n   }\n+\n+  test(\"Multiple Columns: Test observed number of buckets and their sizes match expected values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val datasetSize = 100000\n+    val numBuckets = 5\n+    val data1 = Array.range(1, 100001, 1).map(_.toDouble)\n+    val data2 = Array.range(1, 200000, 2).map(_.toDouble)\n+    val data = (0 until 100000).map { idx =>\n+      (data1(idx), data2(idx))\n+    }\n+    val df: DataFrame = data.toSeq.toDF(\"input1\", \"input2\")\n+\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBuckets(numBuckets)\n+    assert(discretizer.isQuantileDiscretizeMultipleColumns())\n+    val result = discretizer.fit(df).transform(df)\n+\n+    val relativeError = discretizer.getRelativeError\n+    val isGoodBucket = udf {\n+      (size: Int) => math.abs( size - (datasetSize / numBuckets)) <= (relativeError * datasetSize)\n+    }\n+\n+    for (i <- 1 to 2) {\n+      val observedNumBuckets = result.select(\"result\" + i).distinct.count\n+      assert(observedNumBuckets === numBuckets,\n+        \"Observed number of buckets does not equal expected number of buckets.\")\n+\n+      val numGoodBuckets = result.groupBy(\"result\" + i).count.filter(isGoodBucket($\"count\")).count\n+      assert(numGoodBuckets === numBuckets,\n+        \"Bucket sizes are not within expected relative error tolerance.\")\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Test on data with high proportion of duplicated values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val numBuckets = 5\n+    val expectedNumBucket = 3\n+    val data1 = Array(1.0, 3.0, 2.0, 1.0, 1.0, 2.0, 3.0, 2.0, 2.0, 2.0, 1.0, 3.0)\n+    val data2 = Array(1.0, 2.0, 3.0, 1.0, 1.0, 1.0, 1.0, 3.0, 2.0, 3.0, 1.0, 2.0)\n+    val data = (0 until data1.length).map { idx =>\n+      (data1(idx), data2(idx))\n+    }"
  }, {
    "author": {
      "login": "huaxingao"
    },
    "body": "Will change to data1.zip(data2). ",
    "commit": "486b68d1de9e9dc480133d8680baebc98f6e572c",
    "createdAt": "2017-11-13T05:24:01Z",
    "diffHunk": "@@ -146,4 +146,172 @@ class QuantileDiscretizerSuite\n     val model = discretizer.fit(df)\n     assert(model.hasParent)\n   }\n+\n+  test(\"Multiple Columns: Test observed number of buckets and their sizes match expected values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val datasetSize = 100000\n+    val numBuckets = 5\n+    val data1 = Array.range(1, 100001, 1).map(_.toDouble)\n+    val data2 = Array.range(1, 200000, 2).map(_.toDouble)\n+    val data = (0 until 100000).map { idx =>\n+      (data1(idx), data2(idx))\n+    }\n+    val df: DataFrame = data.toSeq.toDF(\"input1\", \"input2\")\n+\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBuckets(numBuckets)\n+    assert(discretizer.isQuantileDiscretizeMultipleColumns())\n+    val result = discretizer.fit(df).transform(df)\n+\n+    val relativeError = discretizer.getRelativeError\n+    val isGoodBucket = udf {\n+      (size: Int) => math.abs( size - (datasetSize / numBuckets)) <= (relativeError * datasetSize)\n+    }\n+\n+    for (i <- 1 to 2) {\n+      val observedNumBuckets = result.select(\"result\" + i).distinct.count\n+      assert(observedNumBuckets === numBuckets,\n+        \"Observed number of buckets does not equal expected number of buckets.\")\n+\n+      val numGoodBuckets = result.groupBy(\"result\" + i).count.filter(isGoodBucket($\"count\")).count\n+      assert(numGoodBuckets === numBuckets,\n+        \"Bucket sizes are not within expected relative error tolerance.\")\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Test on data with high proportion of duplicated values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val numBuckets = 5\n+    val expectedNumBucket = 3\n+    val data1 = Array(1.0, 3.0, 2.0, 1.0, 1.0, 2.0, 3.0, 2.0, 2.0, 2.0, 1.0, 3.0)\n+    val data2 = Array(1.0, 2.0, 3.0, 1.0, 1.0, 1.0, 1.0, 3.0, 2.0, 3.0, 1.0, 2.0)\n+    val data = (0 until data1.length).map { idx =>\n+      (data1(idx), data2(idx))\n+    }"
  }],
  "prId": 19715
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "nit: Remove `DataFrame`.",
    "commit": "486b68d1de9e9dc480133d8680baebc98f6e572c",
    "createdAt": "2017-11-12T06:51:50Z",
    "diffHunk": "@@ -146,4 +146,172 @@ class QuantileDiscretizerSuite\n     val model = discretizer.fit(df)\n     assert(model.hasParent)\n   }\n+\n+  test(\"Multiple Columns: Test observed number of buckets and their sizes match expected values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val datasetSize = 100000\n+    val numBuckets = 5\n+    val data1 = Array.range(1, 100001, 1).map(_.toDouble)\n+    val data2 = Array.range(1, 200000, 2).map(_.toDouble)\n+    val data = (0 until 100000).map { idx =>\n+      (data1(idx), data2(idx))\n+    }\n+    val df: DataFrame = data.toSeq.toDF(\"input1\", \"input2\")\n+\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBuckets(numBuckets)\n+    assert(discretizer.isQuantileDiscretizeMultipleColumns())\n+    val result = discretizer.fit(df).transform(df)\n+\n+    val relativeError = discretizer.getRelativeError\n+    val isGoodBucket = udf {\n+      (size: Int) => math.abs( size - (datasetSize / numBuckets)) <= (relativeError * datasetSize)\n+    }\n+\n+    for (i <- 1 to 2) {\n+      val observedNumBuckets = result.select(\"result\" + i).distinct.count\n+      assert(observedNumBuckets === numBuckets,\n+        \"Observed number of buckets does not equal expected number of buckets.\")\n+\n+      val numGoodBuckets = result.groupBy(\"result\" + i).count.filter(isGoodBucket($\"count\")).count\n+      assert(numGoodBuckets === numBuckets,\n+        \"Bucket sizes are not within expected relative error tolerance.\")\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Test on data with high proportion of duplicated values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val numBuckets = 5\n+    val expectedNumBucket = 3\n+    val data1 = Array(1.0, 3.0, 2.0, 1.0, 1.0, 2.0, 3.0, 2.0, 2.0, 2.0, 1.0, 3.0)\n+    val data2 = Array(1.0, 2.0, 3.0, 1.0, 1.0, 1.0, 1.0, 3.0, 2.0, 3.0, 1.0, 2.0)\n+    val data = (0 until data1.length).map { idx =>\n+      (data1(idx), data2(idx))\n+    }\n+    val df: DataFrame = data.toSeq.toDF(\"input1\", \"input2\")"
  }, {
    "author": {
      "login": "huaxingao"
    },
    "body": "Will remove DataFrame.",
    "commit": "486b68d1de9e9dc480133d8680baebc98f6e572c",
    "createdAt": "2017-11-13T05:23:47Z",
    "diffHunk": "@@ -146,4 +146,172 @@ class QuantileDiscretizerSuite\n     val model = discretizer.fit(df)\n     assert(model.hasParent)\n   }\n+\n+  test(\"Multiple Columns: Test observed number of buckets and their sizes match expected values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val datasetSize = 100000\n+    val numBuckets = 5\n+    val data1 = Array.range(1, 100001, 1).map(_.toDouble)\n+    val data2 = Array.range(1, 200000, 2).map(_.toDouble)\n+    val data = (0 until 100000).map { idx =>\n+      (data1(idx), data2(idx))\n+    }\n+    val df: DataFrame = data.toSeq.toDF(\"input1\", \"input2\")\n+\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBuckets(numBuckets)\n+    assert(discretizer.isQuantileDiscretizeMultipleColumns())\n+    val result = discretizer.fit(df).transform(df)\n+\n+    val relativeError = discretizer.getRelativeError\n+    val isGoodBucket = udf {\n+      (size: Int) => math.abs( size - (datasetSize / numBuckets)) <= (relativeError * datasetSize)\n+    }\n+\n+    for (i <- 1 to 2) {\n+      val observedNumBuckets = result.select(\"result\" + i).distinct.count\n+      assert(observedNumBuckets === numBuckets,\n+        \"Observed number of buckets does not equal expected number of buckets.\")\n+\n+      val numGoodBuckets = result.groupBy(\"result\" + i).count.filter(isGoodBucket($\"count\")).count\n+      assert(numGoodBuckets === numBuckets,\n+        \"Bucket sizes are not within expected relative error tolerance.\")\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Test on data with high proportion of duplicated values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val numBuckets = 5\n+    val expectedNumBucket = 3\n+    val data1 = Array(1.0, 3.0, 2.0, 1.0, 1.0, 2.0, 3.0, 2.0, 2.0, 2.0, 1.0, 3.0)\n+    val data2 = Array(1.0, 2.0, 3.0, 1.0, 1.0, 1.0, 1.0, 3.0, 2.0, 3.0, 1.0, 2.0)\n+    val data = (0 until data1.length).map { idx =>\n+      (data1(idx), data2(idx))\n+    }\n+    val df: DataFrame = data.toSeq.toDF(\"input1\", \"input2\")"
  }],
  "prId": 19715
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "Is this correct? I tried to apply the same data on current `QuantileDiscretizer`:\r\n\r\n```scala\r\nval data1 = Array.range(1, 21, 1).map(_.toDouble)\r\nval df = data1.toSeq.toDF\r\nval discretizer = new QuantileDiscretizer().setInputCol(\"value\").setOutputCol(\"result\").setNumBuckets(2)\r\ndiscretizer.fit(df).transform(df).show\r\n```\r\n```\r\n+-----+------+\r\n|value|result|\r\n+-----+------+\r\n|  1.0|   0.0|\r\n|  2.0|   0.0|\r\n|  3.0|   0.0|\r\n|  4.0|   0.0|\r\n|  5.0|   0.0|\r\n|  6.0|   0.0|\r\n|  7.0|   0.0|\r\n|  8.0|   0.0|\r\n|  9.0|   0.0|\r\n| 10.0|   1.0|\r\n| 11.0|   1.0|\r\n| 12.0|   1.0|\r\n| 13.0|   1.0|\r\n| 14.0|   1.0|\r\n| 15.0|   1.0|\r\n| 16.0|   1.0|\r\n| 17.0|   1.0|\r\n| 18.0|   1.0|\r\n| 19.0|   1.0|\r\n| 20.0|   1.0|\r\n+-----+------+\r\n```",
    "commit": "486b68d1de9e9dc480133d8680baebc98f6e572c",
    "createdAt": "2017-11-12T09:16:16Z",
    "diffHunk": "@@ -146,4 +146,172 @@ class QuantileDiscretizerSuite\n     val model = discretizer.fit(df)\n     assert(model.hasParent)\n   }\n+\n+  test(\"Multiple Columns: Test observed number of buckets and their sizes match expected values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val datasetSize = 100000\n+    val numBuckets = 5\n+    val data1 = Array.range(1, 100001, 1).map(_.toDouble)\n+    val data2 = Array.range(1, 200000, 2).map(_.toDouble)\n+    val data = (0 until 100000).map { idx =>\n+      (data1(idx), data2(idx))\n+    }\n+    val df: DataFrame = data.toSeq.toDF(\"input1\", \"input2\")\n+\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBuckets(numBuckets)\n+    assert(discretizer.isQuantileDiscretizeMultipleColumns())\n+    val result = discretizer.fit(df).transform(df)\n+\n+    val relativeError = discretizer.getRelativeError\n+    val isGoodBucket = udf {\n+      (size: Int) => math.abs( size - (datasetSize / numBuckets)) <= (relativeError * datasetSize)\n+    }\n+\n+    for (i <- 1 to 2) {\n+      val observedNumBuckets = result.select(\"result\" + i).distinct.count\n+      assert(observedNumBuckets === numBuckets,\n+        \"Observed number of buckets does not equal expected number of buckets.\")\n+\n+      val numGoodBuckets = result.groupBy(\"result\" + i).count.filter(isGoodBucket($\"count\")).count\n+      assert(numGoodBuckets === numBuckets,\n+        \"Bucket sizes are not within expected relative error tolerance.\")\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Test on data with high proportion of duplicated values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val numBuckets = 5\n+    val expectedNumBucket = 3\n+    val data1 = Array(1.0, 3.0, 2.0, 1.0, 1.0, 2.0, 3.0, 2.0, 2.0, 2.0, 1.0, 3.0)\n+    val data2 = Array(1.0, 2.0, 3.0, 1.0, 1.0, 1.0, 1.0, 3.0, 2.0, 3.0, 1.0, 2.0)\n+    val data = (0 until data1.length).map { idx =>\n+      (data1(idx), data2(idx))\n+    }\n+    val df: DataFrame = data.toSeq.toDF(\"input1\", \"input2\")\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBuckets(numBuckets)\n+    assert(discretizer.isQuantileDiscretizeMultipleColumns())\n+    val result = discretizer.fit(df).transform(df)\n+    for (i <- 1 to 2) {\n+      val observedNumBuckets = result.select(\"result\" + i).distinct.count\n+      assert(observedNumBuckets == expectedNumBucket,\n+        s\"Observed number of buckets are not correct.\" +\n+          s\" Expected $expectedNumBucket but found ($observedNumBuckets\")\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Test transform on data with NaN value\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val numBuckets = 3\n+    val validData1 = Array(-0.9, -0.5, -0.3, 0.0, 0.2, 0.5, 0.9, Double.NaN, Double.NaN, Double.NaN)\n+    val expectedKeep1 = Array(0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0)\n+    val validData2 = Array(0.2, -0.1, 0.3, 0.0, 0.1, 0.3, 0.5, 0.8, Double.NaN, Double.NaN)\n+    val expectedKeep2 = Array(1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0)\n+\n+    val data = (0 until validData1.length).map { idx =>\n+      (validData1(idx), validData2(idx), expectedKeep1(idx), expectedKeep2(idx))\n+    }\n+    val dataFrame: DataFrame = data.toSeq.toDF(\"input1\", \"input2\", \"expected1\", \"expected2\")\n+\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBuckets(numBuckets)\n+    assert(discretizer.isQuantileDiscretizeMultipleColumns())\n+\n+    withClue(\"QuantileDiscretizer with handleInvalid=error should throw exception for NaN values\") {\n+      intercept[SparkException] {\n+        discretizer.fit(dataFrame).transform(dataFrame).collect()\n+      }\n+    }\n+\n+    discretizer.setHandleInvalid(\"keep\")\n+    discretizer.fit(dataFrame).transform(dataFrame).\n+      select(\"result1\", \"expected1\", \"result2\", \"expected2\")\n+      .collect().foreach {\n+      case Row(r1: Double, e1: Double, r2: Double, e2: Double) =>\n+        assert(r1 === e1,\n+          s\"The result value is not correct after bucketing. Expected $e1 but found $r1\")\n+        assert(r2 === e2,\n+          s\"The result value is not correct after bucketing. Expected $e2 but found $r2\")\n+    }\n+\n+    discretizer.setHandleInvalid(\"skip\")\n+    val result = discretizer.fit(dataFrame).transform(dataFrame)\n+    for (i <- 1 to 2) {\n+      val skipResults1: Array[Double] = result.select(\"result\" + i).as[Double].collect()\n+      assert(skipResults1.length === 7)\n+      assert(skipResults1.forall(_ !== 4.0))\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Test numBucketsArray\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val datasetSize = 20\n+    val numBucketsArray: Array[Int] = Array(2, 5, 10)\n+    val data1 = Array.range(1, 21, 1).map(_.toDouble)\n+    val expected1 = Array (0.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 4.0, 4.0, 5.0,\n+      5.0, 5.0, 6.0, 6.0, 7.0, 8.0, 8.0, 9.0, 9.0, 9.0)"
  }, {
    "author": {
      "login": "huaxingao"
    },
    "body": "I thought we are going to get all the probabilities derived from the numBucketsArray and use them for all the columns. In this case, all the probabilities for numBucketsArray (2,5,10) are (0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0). I am using these probabilities for all the input columns. In another word, I am using numsBuckets 10 for all the input columns. Is this right?",
    "commit": "486b68d1de9e9dc480133d8680baebc98f6e572c",
    "createdAt": "2017-11-13T05:33:15Z",
    "diffHunk": "@@ -146,4 +146,172 @@ class QuantileDiscretizerSuite\n     val model = discretizer.fit(df)\n     assert(model.hasParent)\n   }\n+\n+  test(\"Multiple Columns: Test observed number of buckets and their sizes match expected values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val datasetSize = 100000\n+    val numBuckets = 5\n+    val data1 = Array.range(1, 100001, 1).map(_.toDouble)\n+    val data2 = Array.range(1, 200000, 2).map(_.toDouble)\n+    val data = (0 until 100000).map { idx =>\n+      (data1(idx), data2(idx))\n+    }\n+    val df: DataFrame = data.toSeq.toDF(\"input1\", \"input2\")\n+\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBuckets(numBuckets)\n+    assert(discretizer.isQuantileDiscretizeMultipleColumns())\n+    val result = discretizer.fit(df).transform(df)\n+\n+    val relativeError = discretizer.getRelativeError\n+    val isGoodBucket = udf {\n+      (size: Int) => math.abs( size - (datasetSize / numBuckets)) <= (relativeError * datasetSize)\n+    }\n+\n+    for (i <- 1 to 2) {\n+      val observedNumBuckets = result.select(\"result\" + i).distinct.count\n+      assert(observedNumBuckets === numBuckets,\n+        \"Observed number of buckets does not equal expected number of buckets.\")\n+\n+      val numGoodBuckets = result.groupBy(\"result\" + i).count.filter(isGoodBucket($\"count\")).count\n+      assert(numGoodBuckets === numBuckets,\n+        \"Bucket sizes are not within expected relative error tolerance.\")\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Test on data with high proportion of duplicated values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val numBuckets = 5\n+    val expectedNumBucket = 3\n+    val data1 = Array(1.0, 3.0, 2.0, 1.0, 1.0, 2.0, 3.0, 2.0, 2.0, 2.0, 1.0, 3.0)\n+    val data2 = Array(1.0, 2.0, 3.0, 1.0, 1.0, 1.0, 1.0, 3.0, 2.0, 3.0, 1.0, 2.0)\n+    val data = (0 until data1.length).map { idx =>\n+      (data1(idx), data2(idx))\n+    }\n+    val df: DataFrame = data.toSeq.toDF(\"input1\", \"input2\")\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBuckets(numBuckets)\n+    assert(discretizer.isQuantileDiscretizeMultipleColumns())\n+    val result = discretizer.fit(df).transform(df)\n+    for (i <- 1 to 2) {\n+      val observedNumBuckets = result.select(\"result\" + i).distinct.count\n+      assert(observedNumBuckets == expectedNumBucket,\n+        s\"Observed number of buckets are not correct.\" +\n+          s\" Expected $expectedNumBucket but found ($observedNumBuckets\")\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Test transform on data with NaN value\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val numBuckets = 3\n+    val validData1 = Array(-0.9, -0.5, -0.3, 0.0, 0.2, 0.5, 0.9, Double.NaN, Double.NaN, Double.NaN)\n+    val expectedKeep1 = Array(0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0)\n+    val validData2 = Array(0.2, -0.1, 0.3, 0.0, 0.1, 0.3, 0.5, 0.8, Double.NaN, Double.NaN)\n+    val expectedKeep2 = Array(1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0)\n+\n+    val data = (0 until validData1.length).map { idx =>\n+      (validData1(idx), validData2(idx), expectedKeep1(idx), expectedKeep2(idx))\n+    }\n+    val dataFrame: DataFrame = data.toSeq.toDF(\"input1\", \"input2\", \"expected1\", \"expected2\")\n+\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBuckets(numBuckets)\n+    assert(discretizer.isQuantileDiscretizeMultipleColumns())\n+\n+    withClue(\"QuantileDiscretizer with handleInvalid=error should throw exception for NaN values\") {\n+      intercept[SparkException] {\n+        discretizer.fit(dataFrame).transform(dataFrame).collect()\n+      }\n+    }\n+\n+    discretizer.setHandleInvalid(\"keep\")\n+    discretizer.fit(dataFrame).transform(dataFrame).\n+      select(\"result1\", \"expected1\", \"result2\", \"expected2\")\n+      .collect().foreach {\n+      case Row(r1: Double, e1: Double, r2: Double, e2: Double) =>\n+        assert(r1 === e1,\n+          s\"The result value is not correct after bucketing. Expected $e1 but found $r1\")\n+        assert(r2 === e2,\n+          s\"The result value is not correct after bucketing. Expected $e2 but found $r2\")\n+    }\n+\n+    discretizer.setHandleInvalid(\"skip\")\n+    val result = discretizer.fit(dataFrame).transform(dataFrame)\n+    for (i <- 1 to 2) {\n+      val skipResults1: Array[Double] = result.select(\"result\" + i).as[Double].collect()\n+      assert(skipResults1.length === 7)\n+      assert(skipResults1.forall(_ !== 4.0))\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Test numBucketsArray\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val datasetSize = 20\n+    val numBucketsArray: Array[Int] = Array(2, 5, 10)\n+    val data1 = Array.range(1, 21, 1).map(_.toDouble)\n+    val expected1 = Array (0.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 4.0, 4.0, 5.0,\n+      5.0, 5.0, 6.0, 6.0, 7.0, 8.0, 8.0, 9.0, 9.0, 9.0)"
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "I think to set `numBucketsArray`, the first number of bucket is for first column, although we retrieve the approx-quantile for all probabilities at once.",
    "commit": "486b68d1de9e9dc480133d8680baebc98f6e572c",
    "createdAt": "2017-11-13T11:59:01Z",
    "diffHunk": "@@ -146,4 +146,172 @@ class QuantileDiscretizerSuite\n     val model = discretizer.fit(df)\n     assert(model.hasParent)\n   }\n+\n+  test(\"Multiple Columns: Test observed number of buckets and their sizes match expected values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val datasetSize = 100000\n+    val numBuckets = 5\n+    val data1 = Array.range(1, 100001, 1).map(_.toDouble)\n+    val data2 = Array.range(1, 200000, 2).map(_.toDouble)\n+    val data = (0 until 100000).map { idx =>\n+      (data1(idx), data2(idx))\n+    }\n+    val df: DataFrame = data.toSeq.toDF(\"input1\", \"input2\")\n+\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBuckets(numBuckets)\n+    assert(discretizer.isQuantileDiscretizeMultipleColumns())\n+    val result = discretizer.fit(df).transform(df)\n+\n+    val relativeError = discretizer.getRelativeError\n+    val isGoodBucket = udf {\n+      (size: Int) => math.abs( size - (datasetSize / numBuckets)) <= (relativeError * datasetSize)\n+    }\n+\n+    for (i <- 1 to 2) {\n+      val observedNumBuckets = result.select(\"result\" + i).distinct.count\n+      assert(observedNumBuckets === numBuckets,\n+        \"Observed number of buckets does not equal expected number of buckets.\")\n+\n+      val numGoodBuckets = result.groupBy(\"result\" + i).count.filter(isGoodBucket($\"count\")).count\n+      assert(numGoodBuckets === numBuckets,\n+        \"Bucket sizes are not within expected relative error tolerance.\")\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Test on data with high proportion of duplicated values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val numBuckets = 5\n+    val expectedNumBucket = 3\n+    val data1 = Array(1.0, 3.0, 2.0, 1.0, 1.0, 2.0, 3.0, 2.0, 2.0, 2.0, 1.0, 3.0)\n+    val data2 = Array(1.0, 2.0, 3.0, 1.0, 1.0, 1.0, 1.0, 3.0, 2.0, 3.0, 1.0, 2.0)\n+    val data = (0 until data1.length).map { idx =>\n+      (data1(idx), data2(idx))\n+    }\n+    val df: DataFrame = data.toSeq.toDF(\"input1\", \"input2\")\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBuckets(numBuckets)\n+    assert(discretizer.isQuantileDiscretizeMultipleColumns())\n+    val result = discretizer.fit(df).transform(df)\n+    for (i <- 1 to 2) {\n+      val observedNumBuckets = result.select(\"result\" + i).distinct.count\n+      assert(observedNumBuckets == expectedNumBucket,\n+        s\"Observed number of buckets are not correct.\" +\n+          s\" Expected $expectedNumBucket but found ($observedNumBuckets\")\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Test transform on data with NaN value\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val numBuckets = 3\n+    val validData1 = Array(-0.9, -0.5, -0.3, 0.0, 0.2, 0.5, 0.9, Double.NaN, Double.NaN, Double.NaN)\n+    val expectedKeep1 = Array(0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0)\n+    val validData2 = Array(0.2, -0.1, 0.3, 0.0, 0.1, 0.3, 0.5, 0.8, Double.NaN, Double.NaN)\n+    val expectedKeep2 = Array(1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0)\n+\n+    val data = (0 until validData1.length).map { idx =>\n+      (validData1(idx), validData2(idx), expectedKeep1(idx), expectedKeep2(idx))\n+    }\n+    val dataFrame: DataFrame = data.toSeq.toDF(\"input1\", \"input2\", \"expected1\", \"expected2\")\n+\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBuckets(numBuckets)\n+    assert(discretizer.isQuantileDiscretizeMultipleColumns())\n+\n+    withClue(\"QuantileDiscretizer with handleInvalid=error should throw exception for NaN values\") {\n+      intercept[SparkException] {\n+        discretizer.fit(dataFrame).transform(dataFrame).collect()\n+      }\n+    }\n+\n+    discretizer.setHandleInvalid(\"keep\")\n+    discretizer.fit(dataFrame).transform(dataFrame).\n+      select(\"result1\", \"expected1\", \"result2\", \"expected2\")\n+      .collect().foreach {\n+      case Row(r1: Double, e1: Double, r2: Double, e2: Double) =>\n+        assert(r1 === e1,\n+          s\"The result value is not correct after bucketing. Expected $e1 but found $r1\")\n+        assert(r2 === e2,\n+          s\"The result value is not correct after bucketing. Expected $e2 but found $r2\")\n+    }\n+\n+    discretizer.setHandleInvalid(\"skip\")\n+    val result = discretizer.fit(dataFrame).transform(dataFrame)\n+    for (i <- 1 to 2) {\n+      val skipResults1: Array[Double] = result.select(\"result\" + i).as[Double].collect()\n+      assert(skipResults1.length === 7)\n+      assert(skipResults1.forall(_ !== 4.0))\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Test numBucketsArray\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val datasetSize = 20\n+    val numBucketsArray: Array[Int] = Array(2, 5, 10)\n+    val data1 = Array.range(1, 21, 1).map(_.toDouble)\n+    val expected1 = Array (0.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 4.0, 4.0, 5.0,\n+      5.0, 5.0, 6.0, 6.0, 7.0, 8.0, 8.0, 9.0, 9.0, 9.0)"
  }],
  "prId": 19715
}, {
  "comments": [{
    "author": {
      "login": "MLnick"
    },
    "body": "We should add 2 tests:\r\n\r\n1. test setting `numBuckets` is the same as setting `numBucketsArray` explicitly with identical values\r\n2. test that QD over multiple columns produces the same results as 2x QDs over the same columns (as we did for Bucketizer)",
    "commit": "486b68d1de9e9dc480133d8680baebc98f6e572c",
    "createdAt": "2017-11-29T13:30:25Z",
    "diffHunk": "@@ -146,4 +146,166 @@ class QuantileDiscretizerSuite\n     val model = discretizer.fit(df)\n     assert(model.hasParent)\n   }\n+",
    "line": 12
  }],
  "prId": 19715
}, {
  "comments": [{
    "author": {
      "login": "MLnick"
    },
    "body": "remove the `show` call here",
    "commit": "486b68d1de9e9dc480133d8680baebc98f6e572c",
    "createdAt": "2017-12-15T09:32:47Z",
    "diffHunk": "@@ -146,4 +147,258 @@ class QuantileDiscretizerSuite\n     val model = discretizer.fit(df)\n     assert(model.hasParent)\n   }\n+\n+  test(\"Multiple Columns: Test observed number of buckets and their sizes match expected values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val datasetSize = 100000\n+    val numBuckets = 5\n+    val data1 = Array.range(1, 100001, 1).map(_.toDouble)\n+    val data2 = Array.range(1, 200000, 2).map(_.toDouble)\n+    val df = data1.zip(data2).toSeq.toDF(\"input1\", \"input2\")\n+\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBuckets(numBuckets)\n+    val result = discretizer.fit(df).transform(df)\n+\n+    val relativeError = discretizer.getRelativeError\n+    val isGoodBucket = udf {\n+      (size: Int) => math.abs( size - (datasetSize / numBuckets)) <= (relativeError * datasetSize)\n+    }\n+\n+    for (i <- 1 to 2) {\n+      val observedNumBuckets = result.select(\"result\" + i).distinct.count\n+      assert(observedNumBuckets === numBuckets,\n+        \"Observed number of buckets does not equal expected number of buckets.\")\n+\n+      val numGoodBuckets = result.groupBy(\"result\" + i).count.filter(isGoodBucket($\"count\")).count\n+      assert(numGoodBuckets === numBuckets,\n+        \"Bucket sizes are not within expected relative error tolerance.\")\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Test on data with high proportion of duplicated values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val numBuckets = 5\n+    val expectedNumBucket = 3\n+    val data1 = Array(1.0, 3.0, 2.0, 1.0, 1.0, 2.0, 3.0, 2.0, 2.0, 2.0, 1.0, 3.0)\n+    val data2 = Array(1.0, 2.0, 3.0, 1.0, 1.0, 1.0, 1.0, 3.0, 2.0, 3.0, 1.0, 2.0)\n+    val df = data1.zip(data2).toSeq.toDF(\"input1\", \"input2\")\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBuckets(numBuckets)\n+    val result = discretizer.fit(df).transform(df)\n+    for (i <- 1 to 2) {\n+      val observedNumBuckets = result.select(\"result\" + i).distinct.count\n+      assert(observedNumBuckets == expectedNumBucket,\n+        s\"Observed number of buckets are not correct.\" +\n+          s\" Expected $expectedNumBucket but found ($observedNumBuckets\")\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Test transform on data with NaN value\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val numBuckets = 3\n+    val validData1 = Array(-0.9, -0.5, -0.3, 0.0, 0.2, 0.5, 0.9, Double.NaN, Double.NaN, Double.NaN)\n+    val expectedKeep1 = Array(0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0)\n+    val expectedSkip1 = Array(0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 2.0)\n+    val validData2 = Array(0.2, -0.1, 0.3, 0.0, 0.1, 0.3, 0.5, Double.NaN, Double.NaN, Double.NaN)\n+    val expectedKeep2 = Array(1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 2.0, 3.0, 3.0, 3.0)\n+    val expectedSkip2 = Array(1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 2.0)\n+\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBuckets(numBuckets)\n+\n+    withClue(\"QuantileDiscretizer with handleInvalid=error should throw exception for NaN values\") {\n+      val dataFrame: DataFrame = validData1.zip(validData2).toSeq.toDF(\"input1\", \"input2\")\n+      intercept[SparkException] {\n+        discretizer.fit(dataFrame).transform(dataFrame).collect()\n+      }\n+    }\n+\n+    List((\"keep\", expectedKeep1, expectedKeep2), (\"skip\", expectedSkip1, expectedSkip2)).foreach {\n+      case (u, v, w) =>\n+        discretizer.setHandleInvalid(u)\n+        val dataFrame: DataFrame = validData1.zip(validData2).zip(v).zip(w).map {\n+          case (((a, b), c), d) => (a, b, c, d)\n+        }.toSeq.toDF(\"input1\", \"input2\", \"expected1\", \"expected2\")\n+        dataFrame.show"
  }],
  "prId": 19715
}, {
  "comments": [{
    "author": {
      "login": "MLnick"
    },
    "body": "here too",
    "commit": "486b68d1de9e9dc480133d8680baebc98f6e572c",
    "createdAt": "2017-12-15T09:33:02Z",
    "diffHunk": "@@ -146,4 +147,258 @@ class QuantileDiscretizerSuite\n     val model = discretizer.fit(df)\n     assert(model.hasParent)\n   }\n+\n+  test(\"Multiple Columns: Test observed number of buckets and their sizes match expected values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val datasetSize = 100000\n+    val numBuckets = 5\n+    val data1 = Array.range(1, 100001, 1).map(_.toDouble)\n+    val data2 = Array.range(1, 200000, 2).map(_.toDouble)\n+    val df = data1.zip(data2).toSeq.toDF(\"input1\", \"input2\")\n+\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBuckets(numBuckets)\n+    val result = discretizer.fit(df).transform(df)\n+\n+    val relativeError = discretizer.getRelativeError\n+    val isGoodBucket = udf {\n+      (size: Int) => math.abs( size - (datasetSize / numBuckets)) <= (relativeError * datasetSize)\n+    }\n+\n+    for (i <- 1 to 2) {\n+      val observedNumBuckets = result.select(\"result\" + i).distinct.count\n+      assert(observedNumBuckets === numBuckets,\n+        \"Observed number of buckets does not equal expected number of buckets.\")\n+\n+      val numGoodBuckets = result.groupBy(\"result\" + i).count.filter(isGoodBucket($\"count\")).count\n+      assert(numGoodBuckets === numBuckets,\n+        \"Bucket sizes are not within expected relative error tolerance.\")\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Test on data with high proportion of duplicated values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val numBuckets = 5\n+    val expectedNumBucket = 3\n+    val data1 = Array(1.0, 3.0, 2.0, 1.0, 1.0, 2.0, 3.0, 2.0, 2.0, 2.0, 1.0, 3.0)\n+    val data2 = Array(1.0, 2.0, 3.0, 1.0, 1.0, 1.0, 1.0, 3.0, 2.0, 3.0, 1.0, 2.0)\n+    val df = data1.zip(data2).toSeq.toDF(\"input1\", \"input2\")\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBuckets(numBuckets)\n+    val result = discretizer.fit(df).transform(df)\n+    for (i <- 1 to 2) {\n+      val observedNumBuckets = result.select(\"result\" + i).distinct.count\n+      assert(observedNumBuckets == expectedNumBucket,\n+        s\"Observed number of buckets are not correct.\" +\n+          s\" Expected $expectedNumBucket but found ($observedNumBuckets\")\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Test transform on data with NaN value\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val numBuckets = 3\n+    val validData1 = Array(-0.9, -0.5, -0.3, 0.0, 0.2, 0.5, 0.9, Double.NaN, Double.NaN, Double.NaN)\n+    val expectedKeep1 = Array(0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0)\n+    val expectedSkip1 = Array(0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 2.0)\n+    val validData2 = Array(0.2, -0.1, 0.3, 0.0, 0.1, 0.3, 0.5, Double.NaN, Double.NaN, Double.NaN)\n+    val expectedKeep2 = Array(1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 2.0, 3.0, 3.0, 3.0)\n+    val expectedSkip2 = Array(1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 2.0)\n+\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBuckets(numBuckets)\n+\n+    withClue(\"QuantileDiscretizer with handleInvalid=error should throw exception for NaN values\") {\n+      val dataFrame: DataFrame = validData1.zip(validData2).toSeq.toDF(\"input1\", \"input2\")\n+      intercept[SparkException] {\n+        discretizer.fit(dataFrame).transform(dataFrame).collect()\n+      }\n+    }\n+\n+    List((\"keep\", expectedKeep1, expectedKeep2), (\"skip\", expectedSkip1, expectedSkip2)).foreach {\n+      case (u, v, w) =>\n+        discretizer.setHandleInvalid(u)\n+        val dataFrame: DataFrame = validData1.zip(validData2).zip(v).zip(w).map {\n+          case (((a, b), c), d) => (a, b, c, d)\n+        }.toSeq.toDF(\"input1\", \"input2\", \"expected1\", \"expected2\")\n+        dataFrame.show\n+        val result = discretizer.fit(dataFrame).transform(dataFrame)\n+        result.show"
  }],
  "prId": 19715
}, {
  "comments": [{
    "author": {
      "login": "MLnick"
    },
    "body": "This is unused?",
    "commit": "486b68d1de9e9dc480133d8680baebc98f6e572c",
    "createdAt": "2017-12-15T09:36:39Z",
    "diffHunk": "@@ -146,4 +147,258 @@ class QuantileDiscretizerSuite\n     val model = discretizer.fit(df)\n     assert(model.hasParent)\n   }\n+\n+  test(\"Multiple Columns: Test observed number of buckets and their sizes match expected values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val datasetSize = 100000\n+    val numBuckets = 5\n+    val data1 = Array.range(1, 100001, 1).map(_.toDouble)\n+    val data2 = Array.range(1, 200000, 2).map(_.toDouble)\n+    val df = data1.zip(data2).toSeq.toDF(\"input1\", \"input2\")\n+\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBuckets(numBuckets)\n+    val result = discretizer.fit(df).transform(df)\n+\n+    val relativeError = discretizer.getRelativeError\n+    val isGoodBucket = udf {\n+      (size: Int) => math.abs( size - (datasetSize / numBuckets)) <= (relativeError * datasetSize)\n+    }\n+\n+    for (i <- 1 to 2) {\n+      val observedNumBuckets = result.select(\"result\" + i).distinct.count\n+      assert(observedNumBuckets === numBuckets,\n+        \"Observed number of buckets does not equal expected number of buckets.\")\n+\n+      val numGoodBuckets = result.groupBy(\"result\" + i).count.filter(isGoodBucket($\"count\")).count\n+      assert(numGoodBuckets === numBuckets,\n+        \"Bucket sizes are not within expected relative error tolerance.\")\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Test on data with high proportion of duplicated values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val numBuckets = 5\n+    val expectedNumBucket = 3\n+    val data1 = Array(1.0, 3.0, 2.0, 1.0, 1.0, 2.0, 3.0, 2.0, 2.0, 2.0, 1.0, 3.0)\n+    val data2 = Array(1.0, 2.0, 3.0, 1.0, 1.0, 1.0, 1.0, 3.0, 2.0, 3.0, 1.0, 2.0)\n+    val df = data1.zip(data2).toSeq.toDF(\"input1\", \"input2\")\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBuckets(numBuckets)\n+    val result = discretizer.fit(df).transform(df)\n+    for (i <- 1 to 2) {\n+      val observedNumBuckets = result.select(\"result\" + i).distinct.count\n+      assert(observedNumBuckets == expectedNumBucket,\n+        s\"Observed number of buckets are not correct.\" +\n+          s\" Expected $expectedNumBucket but found ($observedNumBuckets\")\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Test transform on data with NaN value\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val numBuckets = 3\n+    val validData1 = Array(-0.9, -0.5, -0.3, 0.0, 0.2, 0.5, 0.9, Double.NaN, Double.NaN, Double.NaN)\n+    val expectedKeep1 = Array(0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0)\n+    val expectedSkip1 = Array(0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 2.0)\n+    val validData2 = Array(0.2, -0.1, 0.3, 0.0, 0.1, 0.3, 0.5, Double.NaN, Double.NaN, Double.NaN)\n+    val expectedKeep2 = Array(1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 2.0, 3.0, 3.0, 3.0)\n+    val expectedSkip2 = Array(1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 2.0)\n+\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBuckets(numBuckets)\n+\n+    withClue(\"QuantileDiscretizer with handleInvalid=error should throw exception for NaN values\") {\n+      val dataFrame: DataFrame = validData1.zip(validData2).toSeq.toDF(\"input1\", \"input2\")\n+      intercept[SparkException] {\n+        discretizer.fit(dataFrame).transform(dataFrame).collect()\n+      }\n+    }\n+\n+    List((\"keep\", expectedKeep1, expectedKeep2), (\"skip\", expectedSkip1, expectedSkip2)).foreach {\n+      case (u, v, w) =>\n+        discretizer.setHandleInvalid(u)\n+        val dataFrame: DataFrame = validData1.zip(validData2).zip(v).zip(w).map {\n+          case (((a, b), c), d) => (a, b, c, d)\n+        }.toSeq.toDF(\"input1\", \"input2\", \"expected1\", \"expected2\")\n+        dataFrame.show\n+        val result = discretizer.fit(dataFrame).transform(dataFrame)\n+        result.show\n+        result.select(\"result1\", \"expected1\", \"result2\", \"expected2\").collect().foreach {\n+          case Row(x: Double, y: Double, z: Double, w: Double) =>\n+            assert(x === y && w === z)\n+        }\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Test numBucketsArray\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val numBucketsArray: Array[Int] = Array(2, 5, 10)\n+    val data1 = Array.range(1, 21, 1).map(_.toDouble)\n+    val expected1 = Array (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,\n+      1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0)\n+    val data2 = Array.range(1, 40, 2).map(_.toDouble)\n+    val expected2 = Array (0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0,\n+      2.0, 2.0, 3.0, 3.0, 3.0, 4.0, 4.0, 4.0, 4.0, 4.0)\n+    val data3 = Array.range(1, 60, 3).map(_.toDouble)\n+    val expected3 = Array (0.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 4.0, 4.0, 5.0,\n+      5.0, 5.0, 6.0, 6.0, 7.0, 8.0, 8.0, 9.0, 9.0, 9.0)\n+    val data = (0 until 20).map { idx =>\n+      (data1(idx), data2(idx), data3(idx), expected1(idx), expected2(idx), expected3(idx))\n+    }\n+    val df =\n+      data.toDF(\"input1\", \"input2\", \"input3\", \"expected1\", \"expected2\", \"expected3\")\n+\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\", \"input3\"))\n+      .setOutputCols(Array(\"result1\", \"result2\", \"result3\"))\n+      .setNumBucketsArray(numBucketsArray)\n+\n+    discretizer.fit(df).transform(df).\n+      select(\"result1\", \"expected1\", \"result2\", \"expected2\", \"result3\", \"expected3\")\n+      .collect().foreach {\n+      case Row(r1: Double, e1: Double, r2: Double, e2: Double, r3: Double, e3: Double) =>\n+        assert(r1 === e1,\n+          s\"The result value is not correct after bucketing. Expected $e1 but found $r1\")\n+        assert(r2 === e2,\n+          s\"The result value is not correct after bucketing. Expected $e2 but found $r2\")\n+        assert(r3 === e3,\n+          s\"The result value is not correct after bucketing. Expected $e3 but found $r3\")\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Compare single/multiple column(s) QuantileDiscretizer in pipeline\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val numBucketsArray: Array[Int] = Array(2, 5, 10)\n+    val data1 = Array.range(1, 21, 1).map(_.toDouble)\n+    val data2 = Array.range(1, 40, 2).map(_.toDouble)\n+    val data3 = Array.range(1, 60, 3).map(_.toDouble)\n+    val data = (0 until 20).map { idx =>\n+      (data1(idx), data2(idx), data3(idx))\n+    }\n+    val df =\n+      data.toDF(\"input1\", \"input2\", \"input3\")\n+\n+    val multiColsDiscretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\", \"input3\"))\n+      .setOutputCols(Array(\"result1\", \"result2\", \"result3\"))\n+      .setNumBucketsArray(numBucketsArray)\n+    val plForMultiCols = new Pipeline()\n+      .setStages(Array(multiColsDiscretizer))\n+      .fit(df)\n+\n+    val discretizerForCol1 = new QuantileDiscretizer()\n+      .setInputCol(\"input1\")\n+      .setOutputCol(\"result1\")\n+      .setNumBuckets(numBucketsArray(0))\n+\n+    val discretizerForCol2 = new QuantileDiscretizer()\n+      .setInputCol(\"input2\")\n+      .setOutputCol(\"result2\")\n+      .setNumBuckets(numBucketsArray(1))\n+\n+    val discretizerForCol3 = new QuantileDiscretizer()\n+      .setInputCol(\"input3\")\n+      .setOutputCol(\"result3\")\n+      .setNumBuckets(numBucketsArray(2))\n+\n+    val plForSingleCol = new Pipeline()\n+      .setStages(Array(discretizerForCol1, discretizerForCol2, discretizerForCol3))\n+      .fit(df)\n+\n+    val resultForMultiCols = plForMultiCols.transform(df)\n+      .select(\"result1\", \"result2\", \"result3\")\n+      .collect()\n+\n+    val resultForSingleCol = plForSingleCol.transform(df)\n+      .select(\"result1\", \"result2\", \"result3\")\n+      .collect()\n+\n+    resultForSingleCol.zip(resultForMultiCols).foreach {\n+      case (rowForSingle, rowForMultiCols) =>\n+        assert(rowForSingle.getDouble(0) == rowForMultiCols.getDouble(0) &&\n+          rowForSingle.getDouble(1) == rowForMultiCols.getDouble(1) &&\n+          rowForSingle.getDouble(2) == rowForMultiCols.getDouble(2))\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Comparing setting numBuckets with setting numBucketsArray \" +\n+    \"explicitly with identical values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val datasetSize = 20\n+    val numBucketsArray: Array[Int] = Array(2, 5, 10)"
  }],
  "prId": 19715
}, {
  "comments": [{
    "author": {
      "login": "MLnick"
    },
    "body": "can use `datasetSize` here? Or remove `datasetSize` as it's unused.",
    "commit": "486b68d1de9e9dc480133d8680baebc98f6e572c",
    "createdAt": "2017-12-15T09:36:56Z",
    "diffHunk": "@@ -146,4 +147,258 @@ class QuantileDiscretizerSuite\n     val model = discretizer.fit(df)\n     assert(model.hasParent)\n   }\n+\n+  test(\"Multiple Columns: Test observed number of buckets and their sizes match expected values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val datasetSize = 100000\n+    val numBuckets = 5\n+    val data1 = Array.range(1, 100001, 1).map(_.toDouble)\n+    val data2 = Array.range(1, 200000, 2).map(_.toDouble)\n+    val df = data1.zip(data2).toSeq.toDF(\"input1\", \"input2\")\n+\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBuckets(numBuckets)\n+    val result = discretizer.fit(df).transform(df)\n+\n+    val relativeError = discretizer.getRelativeError\n+    val isGoodBucket = udf {\n+      (size: Int) => math.abs( size - (datasetSize / numBuckets)) <= (relativeError * datasetSize)\n+    }\n+\n+    for (i <- 1 to 2) {\n+      val observedNumBuckets = result.select(\"result\" + i).distinct.count\n+      assert(observedNumBuckets === numBuckets,\n+        \"Observed number of buckets does not equal expected number of buckets.\")\n+\n+      val numGoodBuckets = result.groupBy(\"result\" + i).count.filter(isGoodBucket($\"count\")).count\n+      assert(numGoodBuckets === numBuckets,\n+        \"Bucket sizes are not within expected relative error tolerance.\")\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Test on data with high proportion of duplicated values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val numBuckets = 5\n+    val expectedNumBucket = 3\n+    val data1 = Array(1.0, 3.0, 2.0, 1.0, 1.0, 2.0, 3.0, 2.0, 2.0, 2.0, 1.0, 3.0)\n+    val data2 = Array(1.0, 2.0, 3.0, 1.0, 1.0, 1.0, 1.0, 3.0, 2.0, 3.0, 1.0, 2.0)\n+    val df = data1.zip(data2).toSeq.toDF(\"input1\", \"input2\")\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBuckets(numBuckets)\n+    val result = discretizer.fit(df).transform(df)\n+    for (i <- 1 to 2) {\n+      val observedNumBuckets = result.select(\"result\" + i).distinct.count\n+      assert(observedNumBuckets == expectedNumBucket,\n+        s\"Observed number of buckets are not correct.\" +\n+          s\" Expected $expectedNumBucket but found ($observedNumBuckets\")\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Test transform on data with NaN value\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val numBuckets = 3\n+    val validData1 = Array(-0.9, -0.5, -0.3, 0.0, 0.2, 0.5, 0.9, Double.NaN, Double.NaN, Double.NaN)\n+    val expectedKeep1 = Array(0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0)\n+    val expectedSkip1 = Array(0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 2.0)\n+    val validData2 = Array(0.2, -0.1, 0.3, 0.0, 0.1, 0.3, 0.5, Double.NaN, Double.NaN, Double.NaN)\n+    val expectedKeep2 = Array(1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 2.0, 3.0, 3.0, 3.0)\n+    val expectedSkip2 = Array(1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 2.0)\n+\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBuckets(numBuckets)\n+\n+    withClue(\"QuantileDiscretizer with handleInvalid=error should throw exception for NaN values\") {\n+      val dataFrame: DataFrame = validData1.zip(validData2).toSeq.toDF(\"input1\", \"input2\")\n+      intercept[SparkException] {\n+        discretizer.fit(dataFrame).transform(dataFrame).collect()\n+      }\n+    }\n+\n+    List((\"keep\", expectedKeep1, expectedKeep2), (\"skip\", expectedSkip1, expectedSkip2)).foreach {\n+      case (u, v, w) =>\n+        discretizer.setHandleInvalid(u)\n+        val dataFrame: DataFrame = validData1.zip(validData2).zip(v).zip(w).map {\n+          case (((a, b), c), d) => (a, b, c, d)\n+        }.toSeq.toDF(\"input1\", \"input2\", \"expected1\", \"expected2\")\n+        dataFrame.show\n+        val result = discretizer.fit(dataFrame).transform(dataFrame)\n+        result.show\n+        result.select(\"result1\", \"expected1\", \"result2\", \"expected2\").collect().foreach {\n+          case Row(x: Double, y: Double, z: Double, w: Double) =>\n+            assert(x === y && w === z)\n+        }\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Test numBucketsArray\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val numBucketsArray: Array[Int] = Array(2, 5, 10)\n+    val data1 = Array.range(1, 21, 1).map(_.toDouble)\n+    val expected1 = Array (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,\n+      1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0)\n+    val data2 = Array.range(1, 40, 2).map(_.toDouble)\n+    val expected2 = Array (0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0,\n+      2.0, 2.0, 3.0, 3.0, 3.0, 4.0, 4.0, 4.0, 4.0, 4.0)\n+    val data3 = Array.range(1, 60, 3).map(_.toDouble)\n+    val expected3 = Array (0.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 4.0, 4.0, 5.0,\n+      5.0, 5.0, 6.0, 6.0, 7.0, 8.0, 8.0, 9.0, 9.0, 9.0)\n+    val data = (0 until 20).map { idx =>\n+      (data1(idx), data2(idx), data3(idx), expected1(idx), expected2(idx), expected3(idx))\n+    }\n+    val df =\n+      data.toDF(\"input1\", \"input2\", \"input3\", \"expected1\", \"expected2\", \"expected3\")\n+\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\", \"input3\"))\n+      .setOutputCols(Array(\"result1\", \"result2\", \"result3\"))\n+      .setNumBucketsArray(numBucketsArray)\n+\n+    discretizer.fit(df).transform(df).\n+      select(\"result1\", \"expected1\", \"result2\", \"expected2\", \"result3\", \"expected3\")\n+      .collect().foreach {\n+      case Row(r1: Double, e1: Double, r2: Double, e2: Double, r3: Double, e3: Double) =>\n+        assert(r1 === e1,\n+          s\"The result value is not correct after bucketing. Expected $e1 but found $r1\")\n+        assert(r2 === e2,\n+          s\"The result value is not correct after bucketing. Expected $e2 but found $r2\")\n+        assert(r3 === e3,\n+          s\"The result value is not correct after bucketing. Expected $e3 but found $r3\")\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Compare single/multiple column(s) QuantileDiscretizer in pipeline\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val numBucketsArray: Array[Int] = Array(2, 5, 10)\n+    val data1 = Array.range(1, 21, 1).map(_.toDouble)\n+    val data2 = Array.range(1, 40, 2).map(_.toDouble)\n+    val data3 = Array.range(1, 60, 3).map(_.toDouble)\n+    val data = (0 until 20).map { idx =>\n+      (data1(idx), data2(idx), data3(idx))\n+    }\n+    val df =\n+      data.toDF(\"input1\", \"input2\", \"input3\")\n+\n+    val multiColsDiscretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\", \"input3\"))\n+      .setOutputCols(Array(\"result1\", \"result2\", \"result3\"))\n+      .setNumBucketsArray(numBucketsArray)\n+    val plForMultiCols = new Pipeline()\n+      .setStages(Array(multiColsDiscretizer))\n+      .fit(df)\n+\n+    val discretizerForCol1 = new QuantileDiscretizer()\n+      .setInputCol(\"input1\")\n+      .setOutputCol(\"result1\")\n+      .setNumBuckets(numBucketsArray(0))\n+\n+    val discretizerForCol2 = new QuantileDiscretizer()\n+      .setInputCol(\"input2\")\n+      .setOutputCol(\"result2\")\n+      .setNumBuckets(numBucketsArray(1))\n+\n+    val discretizerForCol3 = new QuantileDiscretizer()\n+      .setInputCol(\"input3\")\n+      .setOutputCol(\"result3\")\n+      .setNumBuckets(numBucketsArray(2))\n+\n+    val plForSingleCol = new Pipeline()\n+      .setStages(Array(discretizerForCol1, discretizerForCol2, discretizerForCol3))\n+      .fit(df)\n+\n+    val resultForMultiCols = plForMultiCols.transform(df)\n+      .select(\"result1\", \"result2\", \"result3\")\n+      .collect()\n+\n+    val resultForSingleCol = plForSingleCol.transform(df)\n+      .select(\"result1\", \"result2\", \"result3\")\n+      .collect()\n+\n+    resultForSingleCol.zip(resultForMultiCols).foreach {\n+      case (rowForSingle, rowForMultiCols) =>\n+        assert(rowForSingle.getDouble(0) == rowForMultiCols.getDouble(0) &&\n+          rowForSingle.getDouble(1) == rowForMultiCols.getDouble(1) &&\n+          rowForSingle.getDouble(2) == rowForMultiCols.getDouble(2))\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Comparing setting numBuckets with setting numBucketsArray \" +\n+    \"explicitly with identical values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val datasetSize = 20\n+    val numBucketsArray: Array[Int] = Array(2, 5, 10)\n+    val data1 = Array.range(1, 21, 1).map(_.toDouble)\n+    val data2 = Array.range(1, 40, 2).map(_.toDouble)\n+    val data3 = Array.range(1, 60, 3).map(_.toDouble)\n+    val data = (0 until 20).map { idx =>",
    "line": 151
  }],
  "prId": 19715
}, {
  "comments": [{
    "author": {
      "login": "MLnick"
    },
    "body": "Maybe intercept `IllegalArgumentException` to be more specific.",
    "commit": "486b68d1de9e9dc480133d8680baebc98f6e572c",
    "createdAt": "2017-12-15T09:43:08Z",
    "diffHunk": "@@ -146,4 +147,258 @@ class QuantileDiscretizerSuite\n     val model = discretizer.fit(df)\n     assert(model.hasParent)\n   }\n+\n+  test(\"Multiple Columns: Test observed number of buckets and their sizes match expected values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val datasetSize = 100000\n+    val numBuckets = 5\n+    val data1 = Array.range(1, 100001, 1).map(_.toDouble)\n+    val data2 = Array.range(1, 200000, 2).map(_.toDouble)\n+    val df = data1.zip(data2).toSeq.toDF(\"input1\", \"input2\")\n+\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBuckets(numBuckets)\n+    val result = discretizer.fit(df).transform(df)\n+\n+    val relativeError = discretizer.getRelativeError\n+    val isGoodBucket = udf {\n+      (size: Int) => math.abs( size - (datasetSize / numBuckets)) <= (relativeError * datasetSize)\n+    }\n+\n+    for (i <- 1 to 2) {\n+      val observedNumBuckets = result.select(\"result\" + i).distinct.count\n+      assert(observedNumBuckets === numBuckets,\n+        \"Observed number of buckets does not equal expected number of buckets.\")\n+\n+      val numGoodBuckets = result.groupBy(\"result\" + i).count.filter(isGoodBucket($\"count\")).count\n+      assert(numGoodBuckets === numBuckets,\n+        \"Bucket sizes are not within expected relative error tolerance.\")\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Test on data with high proportion of duplicated values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val numBuckets = 5\n+    val expectedNumBucket = 3\n+    val data1 = Array(1.0, 3.0, 2.0, 1.0, 1.0, 2.0, 3.0, 2.0, 2.0, 2.0, 1.0, 3.0)\n+    val data2 = Array(1.0, 2.0, 3.0, 1.0, 1.0, 1.0, 1.0, 3.0, 2.0, 3.0, 1.0, 2.0)\n+    val df = data1.zip(data2).toSeq.toDF(\"input1\", \"input2\")\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBuckets(numBuckets)\n+    val result = discretizer.fit(df).transform(df)\n+    for (i <- 1 to 2) {\n+      val observedNumBuckets = result.select(\"result\" + i).distinct.count\n+      assert(observedNumBuckets == expectedNumBucket,\n+        s\"Observed number of buckets are not correct.\" +\n+          s\" Expected $expectedNumBucket but found ($observedNumBuckets\")\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Test transform on data with NaN value\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val numBuckets = 3\n+    val validData1 = Array(-0.9, -0.5, -0.3, 0.0, 0.2, 0.5, 0.9, Double.NaN, Double.NaN, Double.NaN)\n+    val expectedKeep1 = Array(0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0)\n+    val expectedSkip1 = Array(0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 2.0)\n+    val validData2 = Array(0.2, -0.1, 0.3, 0.0, 0.1, 0.3, 0.5, Double.NaN, Double.NaN, Double.NaN)\n+    val expectedKeep2 = Array(1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 2.0, 3.0, 3.0, 3.0)\n+    val expectedSkip2 = Array(1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 2.0)\n+\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBuckets(numBuckets)\n+\n+    withClue(\"QuantileDiscretizer with handleInvalid=error should throw exception for NaN values\") {\n+      val dataFrame: DataFrame = validData1.zip(validData2).toSeq.toDF(\"input1\", \"input2\")\n+      intercept[SparkException] {\n+        discretizer.fit(dataFrame).transform(dataFrame).collect()\n+      }\n+    }\n+\n+    List((\"keep\", expectedKeep1, expectedKeep2), (\"skip\", expectedSkip1, expectedSkip2)).foreach {\n+      case (u, v, w) =>\n+        discretizer.setHandleInvalid(u)\n+        val dataFrame: DataFrame = validData1.zip(validData2).zip(v).zip(w).map {\n+          case (((a, b), c), d) => (a, b, c, d)\n+        }.toSeq.toDF(\"input1\", \"input2\", \"expected1\", \"expected2\")\n+        dataFrame.show\n+        val result = discretizer.fit(dataFrame).transform(dataFrame)\n+        result.show\n+        result.select(\"result1\", \"expected1\", \"result2\", \"expected2\").collect().foreach {\n+          case Row(x: Double, y: Double, z: Double, w: Double) =>\n+            assert(x === y && w === z)\n+        }\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Test numBucketsArray\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val numBucketsArray: Array[Int] = Array(2, 5, 10)\n+    val data1 = Array.range(1, 21, 1).map(_.toDouble)\n+    val expected1 = Array (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,\n+      1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0)\n+    val data2 = Array.range(1, 40, 2).map(_.toDouble)\n+    val expected2 = Array (0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0,\n+      2.0, 2.0, 3.0, 3.0, 3.0, 4.0, 4.0, 4.0, 4.0, 4.0)\n+    val data3 = Array.range(1, 60, 3).map(_.toDouble)\n+    val expected3 = Array (0.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 4.0, 4.0, 5.0,\n+      5.0, 5.0, 6.0, 6.0, 7.0, 8.0, 8.0, 9.0, 9.0, 9.0)\n+    val data = (0 until 20).map { idx =>\n+      (data1(idx), data2(idx), data3(idx), expected1(idx), expected2(idx), expected3(idx))\n+    }\n+    val df =\n+      data.toDF(\"input1\", \"input2\", \"input3\", \"expected1\", \"expected2\", \"expected3\")\n+\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\", \"input3\"))\n+      .setOutputCols(Array(\"result1\", \"result2\", \"result3\"))\n+      .setNumBucketsArray(numBucketsArray)\n+\n+    discretizer.fit(df).transform(df).\n+      select(\"result1\", \"expected1\", \"result2\", \"expected2\", \"result3\", \"expected3\")\n+      .collect().foreach {\n+      case Row(r1: Double, e1: Double, r2: Double, e2: Double, r3: Double, e3: Double) =>\n+        assert(r1 === e1,\n+          s\"The result value is not correct after bucketing. Expected $e1 but found $r1\")\n+        assert(r2 === e2,\n+          s\"The result value is not correct after bucketing. Expected $e2 but found $r2\")\n+        assert(r3 === e3,\n+          s\"The result value is not correct after bucketing. Expected $e3 but found $r3\")\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Compare single/multiple column(s) QuantileDiscretizer in pipeline\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val numBucketsArray: Array[Int] = Array(2, 5, 10)\n+    val data1 = Array.range(1, 21, 1).map(_.toDouble)\n+    val data2 = Array.range(1, 40, 2).map(_.toDouble)\n+    val data3 = Array.range(1, 60, 3).map(_.toDouble)\n+    val data = (0 until 20).map { idx =>\n+      (data1(idx), data2(idx), data3(idx))\n+    }\n+    val df =\n+      data.toDF(\"input1\", \"input2\", \"input3\")\n+\n+    val multiColsDiscretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\", \"input3\"))\n+      .setOutputCols(Array(\"result1\", \"result2\", \"result3\"))\n+      .setNumBucketsArray(numBucketsArray)\n+    val plForMultiCols = new Pipeline()\n+      .setStages(Array(multiColsDiscretizer))\n+      .fit(df)\n+\n+    val discretizerForCol1 = new QuantileDiscretizer()\n+      .setInputCol(\"input1\")\n+      .setOutputCol(\"result1\")\n+      .setNumBuckets(numBucketsArray(0))\n+\n+    val discretizerForCol2 = new QuantileDiscretizer()\n+      .setInputCol(\"input2\")\n+      .setOutputCol(\"result2\")\n+      .setNumBuckets(numBucketsArray(1))\n+\n+    val discretizerForCol3 = new QuantileDiscretizer()\n+      .setInputCol(\"input3\")\n+      .setOutputCol(\"result3\")\n+      .setNumBuckets(numBucketsArray(2))\n+\n+    val plForSingleCol = new Pipeline()\n+      .setStages(Array(discretizerForCol1, discretizerForCol2, discretizerForCol3))\n+      .fit(df)\n+\n+    val resultForMultiCols = plForMultiCols.transform(df)\n+      .select(\"result1\", \"result2\", \"result3\")\n+      .collect()\n+\n+    val resultForSingleCol = plForSingleCol.transform(df)\n+      .select(\"result1\", \"result2\", \"result3\")\n+      .collect()\n+\n+    resultForSingleCol.zip(resultForMultiCols).foreach {\n+      case (rowForSingle, rowForMultiCols) =>\n+        assert(rowForSingle.getDouble(0) == rowForMultiCols.getDouble(0) &&\n+          rowForSingle.getDouble(1) == rowForMultiCols.getDouble(1) &&\n+          rowForSingle.getDouble(2) == rowForMultiCols.getDouble(2))\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Comparing setting numBuckets with setting numBucketsArray \" +\n+    \"explicitly with identical values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val datasetSize = 20\n+    val numBucketsArray: Array[Int] = Array(2, 5, 10)\n+    val data1 = Array.range(1, 21, 1).map(_.toDouble)\n+    val data2 = Array.range(1, 40, 2).map(_.toDouble)\n+    val data3 = Array.range(1, 60, 3).map(_.toDouble)\n+    val data = (0 until 20).map { idx =>\n+      (data1(idx), data2(idx), data3(idx))\n+    }\n+    val df =\n+      data.toDF(\"input1\", \"input2\", \"input3\")\n+\n+    val discretizerSingleNumBuckets = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\", \"input3\"))\n+      .setOutputCols(Array(\"result1\", \"result2\", \"result3\"))\n+      .setNumBuckets(10)\n+\n+    val discretizerNumBucketsArray = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\", \"input3\"))\n+      .setOutputCols(Array(\"result1\", \"result2\", \"result3\"))\n+      .setNumBucketsArray(Array(10, 10, 10))\n+\n+    val result1 = discretizerSingleNumBuckets.fit(df).transform(df)\n+      .select(\"result1\", \"result2\", \"result3\")\n+      .collect()\n+    val result2 = discretizerNumBucketsArray.fit(df).transform(df)\n+      .select(\"result1\", \"result2\", \"result3\")\n+      .collect()\n+\n+    result1.zip(result2).foreach {\n+      case (row1, row2) =>\n+        assert(row1.getDouble(0) == row2.getDouble(0) &&\n+          row1.getDouble(1) == row2.getDouble(1) &&\n+          row1.getDouble(2) == row2.getDouble(2))\n+    }\n+  }\n+\n+  test(\"multiple columns: read/write\") {\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBucketsArray(Array(5, 10))\n+    testDefaultReadWrite(discretizer)\n+  }\n+\n+  test(\"Both inputCol and inputCols are set\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCol(\"input\")\n+      .setOutputCol(\"result\")\n+      .setNumBuckets(3)\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+    val df = sc.parallelize(Array(1.0, 2.0, 3.0, 4.0, 5.0, 6.0))\n+      .map(Tuple1.apply).toDF(\"input\")\n+    // When both inputCol and inputCols are set, we throw Exception.\n+    intercept[Exception] {"
  }, {
    "author": {
      "login": "huaxingao"
    },
    "body": "@MLnick Thanks a lot for your comments. I will change these. \r\n",
    "commit": "486b68d1de9e9dc480133d8680baebc98f6e572c",
    "createdAt": "2017-12-15T19:32:59Z",
    "diffHunk": "@@ -146,4 +147,258 @@ class QuantileDiscretizerSuite\n     val model = discretizer.fit(df)\n     assert(model.hasParent)\n   }\n+\n+  test(\"Multiple Columns: Test observed number of buckets and their sizes match expected values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val datasetSize = 100000\n+    val numBuckets = 5\n+    val data1 = Array.range(1, 100001, 1).map(_.toDouble)\n+    val data2 = Array.range(1, 200000, 2).map(_.toDouble)\n+    val df = data1.zip(data2).toSeq.toDF(\"input1\", \"input2\")\n+\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBuckets(numBuckets)\n+    val result = discretizer.fit(df).transform(df)\n+\n+    val relativeError = discretizer.getRelativeError\n+    val isGoodBucket = udf {\n+      (size: Int) => math.abs( size - (datasetSize / numBuckets)) <= (relativeError * datasetSize)\n+    }\n+\n+    for (i <- 1 to 2) {\n+      val observedNumBuckets = result.select(\"result\" + i).distinct.count\n+      assert(observedNumBuckets === numBuckets,\n+        \"Observed number of buckets does not equal expected number of buckets.\")\n+\n+      val numGoodBuckets = result.groupBy(\"result\" + i).count.filter(isGoodBucket($\"count\")).count\n+      assert(numGoodBuckets === numBuckets,\n+        \"Bucket sizes are not within expected relative error tolerance.\")\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Test on data with high proportion of duplicated values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val numBuckets = 5\n+    val expectedNumBucket = 3\n+    val data1 = Array(1.0, 3.0, 2.0, 1.0, 1.0, 2.0, 3.0, 2.0, 2.0, 2.0, 1.0, 3.0)\n+    val data2 = Array(1.0, 2.0, 3.0, 1.0, 1.0, 1.0, 1.0, 3.0, 2.0, 3.0, 1.0, 2.0)\n+    val df = data1.zip(data2).toSeq.toDF(\"input1\", \"input2\")\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBuckets(numBuckets)\n+    val result = discretizer.fit(df).transform(df)\n+    for (i <- 1 to 2) {\n+      val observedNumBuckets = result.select(\"result\" + i).distinct.count\n+      assert(observedNumBuckets == expectedNumBucket,\n+        s\"Observed number of buckets are not correct.\" +\n+          s\" Expected $expectedNumBucket but found ($observedNumBuckets\")\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Test transform on data with NaN value\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val numBuckets = 3\n+    val validData1 = Array(-0.9, -0.5, -0.3, 0.0, 0.2, 0.5, 0.9, Double.NaN, Double.NaN, Double.NaN)\n+    val expectedKeep1 = Array(0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0)\n+    val expectedSkip1 = Array(0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 2.0)\n+    val validData2 = Array(0.2, -0.1, 0.3, 0.0, 0.1, 0.3, 0.5, Double.NaN, Double.NaN, Double.NaN)\n+    val expectedKeep2 = Array(1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 2.0, 3.0, 3.0, 3.0)\n+    val expectedSkip2 = Array(1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 2.0)\n+\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBuckets(numBuckets)\n+\n+    withClue(\"QuantileDiscretizer with handleInvalid=error should throw exception for NaN values\") {\n+      val dataFrame: DataFrame = validData1.zip(validData2).toSeq.toDF(\"input1\", \"input2\")\n+      intercept[SparkException] {\n+        discretizer.fit(dataFrame).transform(dataFrame).collect()\n+      }\n+    }\n+\n+    List((\"keep\", expectedKeep1, expectedKeep2), (\"skip\", expectedSkip1, expectedSkip2)).foreach {\n+      case (u, v, w) =>\n+        discretizer.setHandleInvalid(u)\n+        val dataFrame: DataFrame = validData1.zip(validData2).zip(v).zip(w).map {\n+          case (((a, b), c), d) => (a, b, c, d)\n+        }.toSeq.toDF(\"input1\", \"input2\", \"expected1\", \"expected2\")\n+        dataFrame.show\n+        val result = discretizer.fit(dataFrame).transform(dataFrame)\n+        result.show\n+        result.select(\"result1\", \"expected1\", \"result2\", \"expected2\").collect().foreach {\n+          case Row(x: Double, y: Double, z: Double, w: Double) =>\n+            assert(x === y && w === z)\n+        }\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Test numBucketsArray\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val numBucketsArray: Array[Int] = Array(2, 5, 10)\n+    val data1 = Array.range(1, 21, 1).map(_.toDouble)\n+    val expected1 = Array (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,\n+      1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0)\n+    val data2 = Array.range(1, 40, 2).map(_.toDouble)\n+    val expected2 = Array (0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0,\n+      2.0, 2.0, 3.0, 3.0, 3.0, 4.0, 4.0, 4.0, 4.0, 4.0)\n+    val data3 = Array.range(1, 60, 3).map(_.toDouble)\n+    val expected3 = Array (0.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 4.0, 4.0, 5.0,\n+      5.0, 5.0, 6.0, 6.0, 7.0, 8.0, 8.0, 9.0, 9.0, 9.0)\n+    val data = (0 until 20).map { idx =>\n+      (data1(idx), data2(idx), data3(idx), expected1(idx), expected2(idx), expected3(idx))\n+    }\n+    val df =\n+      data.toDF(\"input1\", \"input2\", \"input3\", \"expected1\", \"expected2\", \"expected3\")\n+\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\", \"input3\"))\n+      .setOutputCols(Array(\"result1\", \"result2\", \"result3\"))\n+      .setNumBucketsArray(numBucketsArray)\n+\n+    discretizer.fit(df).transform(df).\n+      select(\"result1\", \"expected1\", \"result2\", \"expected2\", \"result3\", \"expected3\")\n+      .collect().foreach {\n+      case Row(r1: Double, e1: Double, r2: Double, e2: Double, r3: Double, e3: Double) =>\n+        assert(r1 === e1,\n+          s\"The result value is not correct after bucketing. Expected $e1 but found $r1\")\n+        assert(r2 === e2,\n+          s\"The result value is not correct after bucketing. Expected $e2 but found $r2\")\n+        assert(r3 === e3,\n+          s\"The result value is not correct after bucketing. Expected $e3 but found $r3\")\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Compare single/multiple column(s) QuantileDiscretizer in pipeline\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val numBucketsArray: Array[Int] = Array(2, 5, 10)\n+    val data1 = Array.range(1, 21, 1).map(_.toDouble)\n+    val data2 = Array.range(1, 40, 2).map(_.toDouble)\n+    val data3 = Array.range(1, 60, 3).map(_.toDouble)\n+    val data = (0 until 20).map { idx =>\n+      (data1(idx), data2(idx), data3(idx))\n+    }\n+    val df =\n+      data.toDF(\"input1\", \"input2\", \"input3\")\n+\n+    val multiColsDiscretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\", \"input3\"))\n+      .setOutputCols(Array(\"result1\", \"result2\", \"result3\"))\n+      .setNumBucketsArray(numBucketsArray)\n+    val plForMultiCols = new Pipeline()\n+      .setStages(Array(multiColsDiscretizer))\n+      .fit(df)\n+\n+    val discretizerForCol1 = new QuantileDiscretizer()\n+      .setInputCol(\"input1\")\n+      .setOutputCol(\"result1\")\n+      .setNumBuckets(numBucketsArray(0))\n+\n+    val discretizerForCol2 = new QuantileDiscretizer()\n+      .setInputCol(\"input2\")\n+      .setOutputCol(\"result2\")\n+      .setNumBuckets(numBucketsArray(1))\n+\n+    val discretizerForCol3 = new QuantileDiscretizer()\n+      .setInputCol(\"input3\")\n+      .setOutputCol(\"result3\")\n+      .setNumBuckets(numBucketsArray(2))\n+\n+    val plForSingleCol = new Pipeline()\n+      .setStages(Array(discretizerForCol1, discretizerForCol2, discretizerForCol3))\n+      .fit(df)\n+\n+    val resultForMultiCols = plForMultiCols.transform(df)\n+      .select(\"result1\", \"result2\", \"result3\")\n+      .collect()\n+\n+    val resultForSingleCol = plForSingleCol.transform(df)\n+      .select(\"result1\", \"result2\", \"result3\")\n+      .collect()\n+\n+    resultForSingleCol.zip(resultForMultiCols).foreach {\n+      case (rowForSingle, rowForMultiCols) =>\n+        assert(rowForSingle.getDouble(0) == rowForMultiCols.getDouble(0) &&\n+          rowForSingle.getDouble(1) == rowForMultiCols.getDouble(1) &&\n+          rowForSingle.getDouble(2) == rowForMultiCols.getDouble(2))\n+    }\n+  }\n+\n+  test(\"Multiple Columns: Comparing setting numBuckets with setting numBucketsArray \" +\n+    \"explicitly with identical values\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+\n+    val datasetSize = 20\n+    val numBucketsArray: Array[Int] = Array(2, 5, 10)\n+    val data1 = Array.range(1, 21, 1).map(_.toDouble)\n+    val data2 = Array.range(1, 40, 2).map(_.toDouble)\n+    val data3 = Array.range(1, 60, 3).map(_.toDouble)\n+    val data = (0 until 20).map { idx =>\n+      (data1(idx), data2(idx), data3(idx))\n+    }\n+    val df =\n+      data.toDF(\"input1\", \"input2\", \"input3\")\n+\n+    val discretizerSingleNumBuckets = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\", \"input3\"))\n+      .setOutputCols(Array(\"result1\", \"result2\", \"result3\"))\n+      .setNumBuckets(10)\n+\n+    val discretizerNumBucketsArray = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\", \"input3\"))\n+      .setOutputCols(Array(\"result1\", \"result2\", \"result3\"))\n+      .setNumBucketsArray(Array(10, 10, 10))\n+\n+    val result1 = discretizerSingleNumBuckets.fit(df).transform(df)\n+      .select(\"result1\", \"result2\", \"result3\")\n+      .collect()\n+    val result2 = discretizerNumBucketsArray.fit(df).transform(df)\n+      .select(\"result1\", \"result2\", \"result3\")\n+      .collect()\n+\n+    result1.zip(result2).foreach {\n+      case (row1, row2) =>\n+        assert(row1.getDouble(0) == row2.getDouble(0) &&\n+          row1.getDouble(1) == row2.getDouble(1) &&\n+          row1.getDouble(2) == row2.getDouble(2))\n+    }\n+  }\n+\n+  test(\"multiple columns: read/write\") {\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+      .setOutputCols(Array(\"result1\", \"result2\"))\n+      .setNumBucketsArray(Array(5, 10))\n+    testDefaultReadWrite(discretizer)\n+  }\n+\n+  test(\"Both inputCol and inputCols are set\") {\n+    val spark = this.spark\n+    import spark.implicits._\n+    val discretizer = new QuantileDiscretizer()\n+      .setInputCol(\"input\")\n+      .setOutputCol(\"result\")\n+      .setNumBuckets(3)\n+      .setInputCols(Array(\"input1\", \"input2\"))\n+    val df = sc.parallelize(Array(1.0, 2.0, 3.0, 4.0, 5.0, 6.0))\n+      .map(Tuple1.apply).toDF(\"input\")\n+    // When both inputCol and inputCols are set, we throw Exception.\n+    intercept[Exception] {"
  }],
  "prId": 19715
}, {
  "comments": [{
    "author": {
      "login": "MLnick"
    },
    "body": "I think I slightly prefer to actually test that the error is thrown during `transform`",
    "commit": "486b68d1de9e9dc480133d8680baebc98f6e572c",
    "createdAt": "2017-12-21T10:11:39Z",
    "diffHunk": "@@ -386,19 +382,16 @@ class QuantileDiscretizerSuite\n     testDefaultReadWrite(discretizer)\n   }\n \n-  test(\"Both inputCol and inputCols are set\") {\n-    val spark = this.spark\n-    import spark.implicits._\n-    val discretizer = new QuantileDiscretizer()\n-      .setInputCol(\"input\")\n-      .setOutputCol(\"result\")\n-      .setNumBuckets(3)\n-      .setInputCols(Array(\"input1\", \"input2\"))\n-    val df = sc.parallelize(Array(1.0, 2.0, 3.0, 4.0, 5.0, 6.0))\n-      .map(Tuple1.apply).toDF(\"input\")\n-    // When both inputCol and inputCols are set, we throw Exception.\n-    intercept[Exception] {\n-      discretizer.fit(df)\n+  test(\"multiple columns: Both inputCol and inputCols are set\") {\n+    intercept[IllegalArgumentException] {\n+      new QuantileDiscretizer().setInputCol(\"in\").setInputCols(Array(\"in1\", \"in2\")).getInOutCols"
  }, {
    "author": {
      "login": "huaxingao"
    },
    "body": "Thanks for spending so much time to review this PR. I will change this. ",
    "commit": "486b68d1de9e9dc480133d8680baebc98f6e572c",
    "createdAt": "2017-12-21T18:33:52Z",
    "diffHunk": "@@ -386,19 +382,16 @@ class QuantileDiscretizerSuite\n     testDefaultReadWrite(discretizer)\n   }\n \n-  test(\"Both inputCol and inputCols are set\") {\n-    val spark = this.spark\n-    import spark.implicits._\n-    val discretizer = new QuantileDiscretizer()\n-      .setInputCol(\"input\")\n-      .setOutputCol(\"result\")\n-      .setNumBuckets(3)\n-      .setInputCols(Array(\"input1\", \"input2\"))\n-    val df = sc.parallelize(Array(1.0, 2.0, 3.0, 4.0, 5.0, 6.0))\n-      .map(Tuple1.apply).toDF(\"input\")\n-    // When both inputCol and inputCols are set, we throw Exception.\n-    intercept[Exception] {\n-      discretizer.fit(df)\n+  test(\"multiple columns: Both inputCol and inputCols are set\") {\n+    intercept[IllegalArgumentException] {\n+      new QuantileDiscretizer().setInputCol(\"in\").setInputCols(Array(\"in1\", \"in2\")).getInOutCols"
  }],
  "prId": 19715
}]