[{
  "comments": [{
    "author": {
      "login": "jaceklaskowski"
    },
    "body": "Do you need `sc.parallelize`?\n",
    "commit": "b60c95279c809c59cd08ed1e1b239b3688f50b12",
    "createdAt": "2016-07-03T12:42:42Z",
    "diffHunk": "@@ -102,7 +103,7 @@ class VectorIndexerSuite extends SparkFunSuite with MLlibTestSparkContext\n   }\n \n   test(\"Cannot fit an empty DataFrame\") {\n-    val rdd = spark.createDataFrame(sc.parallelize(Array.empty[Vector], 2).map(FeatureData))\n+    val rdd = sc.parallelize(Array.empty[Vector], 2).map(FeatureData).toDF()"
  }],
  "prId": 14035
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "BTW, It seems a test is failed when I change this to `densePoints2Seq.map(FeatureData).toDF()` for an unknown reason.\n",
    "commit": "b60c95279c809c59cd08ed1e1b239b3688f50b12",
    "createdAt": "2016-09-25T12:04:24Z",
    "diffHunk": "@@ -85,11 +87,13 @@ class VectorIndexerSuite extends SparkFunSuite with MLlibTestSparkContext\n     checkPair(densePoints1Seq, sparsePoints1Seq)\n     checkPair(densePoints2Seq, sparsePoints2Seq)\n \n-    densePoints1 = spark.createDataFrame(sc.parallelize(densePoints1Seq, 2).map(FeatureData))\n-    sparsePoints1 = spark.createDataFrame(sc.parallelize(sparsePoints1Seq, 2).map(FeatureData))\n-    densePoints2 = spark.createDataFrame(sc.parallelize(densePoints2Seq, 2).map(FeatureData))\n-    sparsePoints2 = spark.createDataFrame(sc.parallelize(sparsePoints2Seq, 2).map(FeatureData))\n-    badPoints = spark.createDataFrame(sc.parallelize(badPointsSeq, 2).map(FeatureData))\n+    densePoints1 = densePoints1Seq.map(FeatureData).toDF()\n+    sparsePoints1 = sparsePoints1Seq.map(FeatureData).toDF()\n+    // TODO: If we directly use `toDF` without parallelize, the test in\n+    // \"Throws error when given RDDs with different size vectors\" is failed for an unknown reason.\n+    densePoints2 = sc.parallelize(densePoints2Seq, 2).map(FeatureData).toDF()",
    "line": 21
  }],
  "prId": 14035
}]