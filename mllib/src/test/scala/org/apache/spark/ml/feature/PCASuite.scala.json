[{
  "comments": [{
    "author": {
      "login": "yanboliang"
    },
    "body": "The same as [StandardScalerModel](https://github.com/apache/spark/pull/9839/files#diff-7ae15fe7b1bc41e6a876c2424380d764R132), we can not construct PCAModel directly by specifying the variable `k`, so test estimator and model in one case with `testEstimatorAndModelReadWrite`.\n",
    "commit": "0cb01a34909c32cc5db3073dac2785c9cba8991e",
    "createdAt": "2015-11-19T15:04:07Z",
    "diffHunk": "@@ -65,4 +65,24 @@ class PCASuite extends SparkFunSuite with MLlibTestSparkContext {\n         assert(x ~== y absTol 1e-5, \"Transformed vector is different with expected vector.\")\n     }\n   }\n+\n+  test(\"read/write\") {\n+\n+    def checkModelData(model1: PCAModel, model2: PCAModel): Unit = {\n+      assert(model1.pc === model2.pc)\n+    }\n+    val allParams: Map[String, Any] = Map(\n+      \"k\" -> 3,\n+      \"inputCol\" -> \"features\",\n+      \"outputCol\" -> \"pca_features\"\n+    )\n+    val data = Seq(\n+      (0.0, Vectors.sparse(5, Seq((1, 1.0), (3, 7.0)))),\n+      (1.0, Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0)),\n+      (2.0, Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0))\n+    )\n+    val df = sqlContext.createDataFrame(data).toDF(\"id\", \"features\")\n+    val pca = new PCA().setK(3)\n+    testEstimatorAndModelReadWrite(pca, df, allParams, checkModelData)\n+  }\n }",
    "line": 43
  }],
  "prId": 9838
}]