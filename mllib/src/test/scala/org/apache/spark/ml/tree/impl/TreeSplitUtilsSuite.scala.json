[{
  "comments": [{
    "author": {
      "login": "WeichenXu123"
    },
    "body": "In order to make the test more strict, can you increase the `featureArity`, `numExamples` and `numClasses` ? e.g., featureArity = 6 and numExamples = 10 and numClasses = 5",
    "commit": "5bcccda6a599f60d2d084ecf871649675a792d5d",
    "createdAt": "2017-11-23T09:15:06Z",
    "diffHunk": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.tree.impl\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.ml.tree.{CategoricalSplit, ContinuousSplit, Split}\n+import org.apache.spark.ml.util.DefaultReadWriteTest\n+import org.apache.spark.mllib.tree.impurity.{Entropy, Impurity}\n+import org.apache.spark.mllib.tree.model.ImpurityStats\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+\n+/** Suite exercising helper methods for making split decisions during decision tree training. */\n+class TreeSplitUtilsSuite\n+  extends SparkFunSuite with MLlibTestSparkContext with DefaultReadWriteTest {\n+\n+  /**\n+   * Get a DTStatsAggregator for sufficient stat collection/impurity calculation populated\n+   * with the data from the specified training points. Assumes a feature index of 0 and that\n+   * all training points have the same weights (1.0).\n+   */\n+  private def getAggregator(\n+      metadata: DecisionTreeMetadata,\n+      values: Array[Int],\n+      labels: Array[Double],\n+      featureSplits: Array[Split]): DTStatsAggregator = {\n+    // Create stats aggregator\n+    val statsAggregator = new DTStatsAggregator(metadata, featureSubset = None)\n+    // Update parent impurity stats\n+    val featureIndex = 0\n+    val instanceWeights = Array.fill[Double](values.length)(1.0)\n+    AggUpdateUtils.updateParentImpurity(statsAggregator, indices = values.indices.toArray,\n+      from = 0, to = values.length, instanceWeights, labels)\n+    // Update current aggregator's impurity stats\n+    values.zip(labels).foreach { case (value: Int, label: Double) =>\n+      if (metadata.isUnordered(featureIndex)) {\n+        AggUpdateUtils.updateUnorderedFeature(statsAggregator, value, label,\n+          featureIndex = featureIndex, featureIndexIdx = 0, featureSplits, instanceWeight = 1.0)\n+      } else {\n+        AggUpdateUtils.updateOrderedFeature(statsAggregator, value, label, featureIndexIdx = 0,\n+          instanceWeight = 1.0)\n+      }\n+    }\n+    statsAggregator\n+  }\n+\n+  /**\n+   * Check that left/right impurities match what we'd expect for a split.\n+   * @param labels Labels whose impurity information should be reflected in stats\n+   * @param stats ImpurityStats object containing impurity info for the left/right sides of a split\n+   */\n+  private def validateImpurityStats(\n+      impurity: Impurity,\n+      labels: Array[Double],\n+      stats: ImpurityStats,\n+      expectedLeftStats: Array[Double],\n+      expectedRightStats: Array[Double]): Unit = {\n+    // Compute impurity for our data points manually\n+    val numClasses = (labels.max + 1).toInt\n+    val fullImpurityStatsArray\n+      = Array.tabulate[Double](numClasses)((label: Int) => labels.count(_ == label).toDouble)\n+    val fullImpurity = Entropy.calculate(fullImpurityStatsArray, labels.length)\n+    // Verify that impurity stats were computed correctly for split\n+    assert(stats.impurityCalculator.stats === fullImpurityStatsArray)\n+    assert(stats.impurity === fullImpurity)\n+    assert(stats.leftImpurityCalculator.stats === expectedLeftStats)\n+    assert(stats.rightImpurityCalculator.stats === expectedRightStats)\n+    assert(stats.valid)\n+  }\n+\n+  /* * * * * * * * * * * Choosing Splits  * * * * * * * * * * */\n+\n+  test(\"chooseSplit: choose correct type of split (continuous split)\") {\n+    // Construct (binned) continuous data\n+    val labels = Array(0.0, 0.0, 1.0)\n+    val values = Array(1, 2, 3)\n+    val featureIndex = 0\n+    // Get an array of continuous splits corresponding to values in our binned data\n+    val splits = TreeTests.getContinuousSplits(thresholds = values.distinct.sorted,\n+      featureIndex = 0)\n+    // Construct DTStatsAggregator, compute sufficient stats\n+    val metadata = TreeTests.getMetadata(numExamples = values.length, numFeatures = 1,\n+      numClasses = 2, Map.empty)\n+    val statsAggregator = getAggregator(metadata, values, labels, splits)\n+    // Choose split, check that it's a valid ContinuousSplit\n+    val (split, stats) = SplitUtils.chooseSplit(statsAggregator, featureIndex, featureIndex,\n+      splits)\n+    assert(stats.valid && split.isInstanceOf[ContinuousSplit])\n+  }\n+\n+  test(\"chooseSplit: choose correct type of split (categorical split)\") {\n+    // Construct categorical data\n+    val labels = Array(0.0, 0.0, 1.0, 1.0, 1.0)\n+    val featureArity = 3\n+    val values = Array(0, 0, 1, 2, 2)\n+    val featureIndex = 0\n+    // Construct DTStatsAggregator, compute sufficient stats\n+    val metadata = TreeTests.getMetadata(numExamples = values.length, numFeatures = 1,\n+      numClasses = 2, Map(featureIndex -> featureArity))\n+    val splits = RandomForest.findUnorderedSplits(metadata, featureIndex)\n+    val statsAggregator = getAggregator(metadata, values, labels, splits)\n+    // Choose split, check that it's a valid categorical split\n+    val (split, stats) = SplitUtils.chooseSplit(statsAggregator = statsAggregator,\n+      featureIndex = featureIndex, featureIndexIdx = featureIndex, featureSplits = splits)\n+    assert(stats.valid && split.isInstanceOf[CategoricalSplit])\n+  }\n+\n+  test(\"chooseOrderedCategoricalSplit: basic case\") {\n+    // Helper method for testing ordered categorical split\n+    def testHelper(\n+        values: Array[Int],\n+        labels: Array[Double],\n+        expectedLeftCategories: Array[Double],\n+        expectedLeftStats: Array[Double],\n+        expectedRightStats: Array[Double]): Unit = {\n+      // Set up metadata for ordered categorical feature\n+      val featureIndex = 0\n+      val featureArity = values.max + 1\n+      val arityMap = Map[Int, Int](featureIndex -> featureArity)\n+      val metadata = TreeTests.getMetadata(numExamples = values.length, numFeatures = 1,\n+        numClasses = 2, arityMap, unorderedFeatures = Some(Set.empty))\n+      // Construct DTStatsAggregator, compute sufficient stats\n+      val statsAggregator = getAggregator(metadata, values, labels, featureSplits = Array.empty)\n+      // Choose split\n+      val (split, stats) =\n+        SplitUtils.chooseOrderedCategoricalSplit(statsAggregator, featureIndex, featureIndex)\n+      // Verify that split has the expected left-side/right-side categories\n+      val expectedRightCategories = Range(0, featureArity)\n+        .filter(c => !expectedLeftCategories.contains(c)).map(_.toDouble).toArray\n+      split match {\n+        case s: CategoricalSplit =>\n+          assert(s.featureIndex === featureIndex)\n+          assert(s.leftCategories === expectedLeftCategories)\n+          assert(s.rightCategories === expectedRightCategories)\n+        case _ =>\n+          throw new AssertionError(\n+            s\"Expected CategoricalSplit but got ${split.getClass.getSimpleName}\")\n+      }\n+      validateImpurityStats(Entropy, labels, stats, expectedLeftStats, expectedRightStats)\n+    }\n+\n+    // Test a single split: The left side of our split should contain the two points with label 0,\n+    // the left side of our split should contain the five points with label 1\n+    val values = Array(0, 0, 1, 2, 2, 2, 2)\n+    val labels1 = Array(0, 0, 1, 1, 1, 1, 1).map(_.toDouble)\n+    testHelper(values, labels1, expectedLeftCategories = Array(0.0),\n+      expectedLeftStats = Array(2.0, 0.0), expectedRightStats = Array(0.0, 5.0))\n+\n+    // Test a single split: The left side of our split should contain the three points with label 0,\n+    // the left side of our split should contain the four points with label 1\n+    val labels2 = Array(0, 0, 0, 1, 1, 1, 1).map(_.toDouble)\n+    testHelper(values, labels2, expectedLeftCategories = Array(0.0, 1.0),\n+      expectedLeftStats = Array(3.0, 0.0), expectedRightStats = Array(0.0, 4.0))\n+  }\n+\n+  test(\"chooseOrderedCategoricalSplit: return bad stats if we should not split\") {\n+    // Construct categorical data\n+    val featureIndex = 0\n+    val values = Array(0, 0, 1, 2, 2, 2, 2)\n+    val featureArity = values.max + 1\n+    val labels = Array(1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0)\n+    // Construct DTStatsAggregator, compute sufficient stats\n+    val metadata = TreeTests.getMetadata(numExamples = values.length, numFeatures = 1,\n+      numClasses = 2, Map(featureIndex -> featureArity), unorderedFeatures = Some(Set.empty))\n+    val statsAggregator = getAggregator(metadata, values, labels, featureSplits = Array.empty)\n+    // Choose split, verify that it's invalid\n+    val (_, stats) = SplitUtils.chooseOrderedCategoricalSplit(statsAggregator, featureIndex,\n+        featureIndex)\n+    assert(!stats.valid)\n+  }\n+\n+  test(\"chooseUnorderedCategoricalSplit: basic case\") {\n+    val featureIndex = 0\n+    // Construct data for unordered categorical feature\n+    // label: 0 --> values: 1\n+    // label: 1 --> values: 0, 2\n+    // label: 2 --> values: 2\n+    // Expected split: feature value 1 on the left, values (0, 2) on the right\n+    val values = Array(1, 1, 0, 2, 2)\n+    val featureArity = values.max + 1"
  }, {
    "author": {
      "login": "smurching"
    },
    "body": "@WeichenXu123 thanks for the feedback! Definitely agree that the test is a little weak right now.\r\n\r\nIMO it's mainly weak due to the low feature arity (there only three possible splits, so the right one could be picked by chance). I think increasing the number of classes/examples substantially might make the test harder to reason about, but not opposed to that either - let me know what you think.\r\n\r\nWhat about something like:\r\n\r\n```\r\n    val values = Array(0, 1, 2, 3, 2, 2, 4)\r\n    val labels = Array(0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 2.0)\r\n    // label: 0 --> values: 0, 1\r\n    // label: 1 --> values: 2, 3\r\n    // label: 2 --> values: 2, 2, 4\r\n    // Expected split: feature values (0, 1) on the left, values (2, 3, 4) on the right\r\n```\r\n\r\nThis way we still test multiclass classification & test the split-selection logic more rigorously.\r\n\r\n",
    "commit": "5bcccda6a599f60d2d084ecf871649675a792d5d",
    "createdAt": "2017-12-01T07:56:51Z",
    "diffHunk": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.tree.impl\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.ml.tree.{CategoricalSplit, ContinuousSplit, Split}\n+import org.apache.spark.ml.util.DefaultReadWriteTest\n+import org.apache.spark.mllib.tree.impurity.{Entropy, Impurity}\n+import org.apache.spark.mllib.tree.model.ImpurityStats\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+\n+/** Suite exercising helper methods for making split decisions during decision tree training. */\n+class TreeSplitUtilsSuite\n+  extends SparkFunSuite with MLlibTestSparkContext with DefaultReadWriteTest {\n+\n+  /**\n+   * Get a DTStatsAggregator for sufficient stat collection/impurity calculation populated\n+   * with the data from the specified training points. Assumes a feature index of 0 and that\n+   * all training points have the same weights (1.0).\n+   */\n+  private def getAggregator(\n+      metadata: DecisionTreeMetadata,\n+      values: Array[Int],\n+      labels: Array[Double],\n+      featureSplits: Array[Split]): DTStatsAggregator = {\n+    // Create stats aggregator\n+    val statsAggregator = new DTStatsAggregator(metadata, featureSubset = None)\n+    // Update parent impurity stats\n+    val featureIndex = 0\n+    val instanceWeights = Array.fill[Double](values.length)(1.0)\n+    AggUpdateUtils.updateParentImpurity(statsAggregator, indices = values.indices.toArray,\n+      from = 0, to = values.length, instanceWeights, labels)\n+    // Update current aggregator's impurity stats\n+    values.zip(labels).foreach { case (value: Int, label: Double) =>\n+      if (metadata.isUnordered(featureIndex)) {\n+        AggUpdateUtils.updateUnorderedFeature(statsAggregator, value, label,\n+          featureIndex = featureIndex, featureIndexIdx = 0, featureSplits, instanceWeight = 1.0)\n+      } else {\n+        AggUpdateUtils.updateOrderedFeature(statsAggregator, value, label, featureIndexIdx = 0,\n+          instanceWeight = 1.0)\n+      }\n+    }\n+    statsAggregator\n+  }\n+\n+  /**\n+   * Check that left/right impurities match what we'd expect for a split.\n+   * @param labels Labels whose impurity information should be reflected in stats\n+   * @param stats ImpurityStats object containing impurity info for the left/right sides of a split\n+   */\n+  private def validateImpurityStats(\n+      impurity: Impurity,\n+      labels: Array[Double],\n+      stats: ImpurityStats,\n+      expectedLeftStats: Array[Double],\n+      expectedRightStats: Array[Double]): Unit = {\n+    // Compute impurity for our data points manually\n+    val numClasses = (labels.max + 1).toInt\n+    val fullImpurityStatsArray\n+      = Array.tabulate[Double](numClasses)((label: Int) => labels.count(_ == label).toDouble)\n+    val fullImpurity = Entropy.calculate(fullImpurityStatsArray, labels.length)\n+    // Verify that impurity stats were computed correctly for split\n+    assert(stats.impurityCalculator.stats === fullImpurityStatsArray)\n+    assert(stats.impurity === fullImpurity)\n+    assert(stats.leftImpurityCalculator.stats === expectedLeftStats)\n+    assert(stats.rightImpurityCalculator.stats === expectedRightStats)\n+    assert(stats.valid)\n+  }\n+\n+  /* * * * * * * * * * * Choosing Splits  * * * * * * * * * * */\n+\n+  test(\"chooseSplit: choose correct type of split (continuous split)\") {\n+    // Construct (binned) continuous data\n+    val labels = Array(0.0, 0.0, 1.0)\n+    val values = Array(1, 2, 3)\n+    val featureIndex = 0\n+    // Get an array of continuous splits corresponding to values in our binned data\n+    val splits = TreeTests.getContinuousSplits(thresholds = values.distinct.sorted,\n+      featureIndex = 0)\n+    // Construct DTStatsAggregator, compute sufficient stats\n+    val metadata = TreeTests.getMetadata(numExamples = values.length, numFeatures = 1,\n+      numClasses = 2, Map.empty)\n+    val statsAggregator = getAggregator(metadata, values, labels, splits)\n+    // Choose split, check that it's a valid ContinuousSplit\n+    val (split, stats) = SplitUtils.chooseSplit(statsAggregator, featureIndex, featureIndex,\n+      splits)\n+    assert(stats.valid && split.isInstanceOf[ContinuousSplit])\n+  }\n+\n+  test(\"chooseSplit: choose correct type of split (categorical split)\") {\n+    // Construct categorical data\n+    val labels = Array(0.0, 0.0, 1.0, 1.0, 1.0)\n+    val featureArity = 3\n+    val values = Array(0, 0, 1, 2, 2)\n+    val featureIndex = 0\n+    // Construct DTStatsAggregator, compute sufficient stats\n+    val metadata = TreeTests.getMetadata(numExamples = values.length, numFeatures = 1,\n+      numClasses = 2, Map(featureIndex -> featureArity))\n+    val splits = RandomForest.findUnorderedSplits(metadata, featureIndex)\n+    val statsAggregator = getAggregator(metadata, values, labels, splits)\n+    // Choose split, check that it's a valid categorical split\n+    val (split, stats) = SplitUtils.chooseSplit(statsAggregator = statsAggregator,\n+      featureIndex = featureIndex, featureIndexIdx = featureIndex, featureSplits = splits)\n+    assert(stats.valid && split.isInstanceOf[CategoricalSplit])\n+  }\n+\n+  test(\"chooseOrderedCategoricalSplit: basic case\") {\n+    // Helper method for testing ordered categorical split\n+    def testHelper(\n+        values: Array[Int],\n+        labels: Array[Double],\n+        expectedLeftCategories: Array[Double],\n+        expectedLeftStats: Array[Double],\n+        expectedRightStats: Array[Double]): Unit = {\n+      // Set up metadata for ordered categorical feature\n+      val featureIndex = 0\n+      val featureArity = values.max + 1\n+      val arityMap = Map[Int, Int](featureIndex -> featureArity)\n+      val metadata = TreeTests.getMetadata(numExamples = values.length, numFeatures = 1,\n+        numClasses = 2, arityMap, unorderedFeatures = Some(Set.empty))\n+      // Construct DTStatsAggregator, compute sufficient stats\n+      val statsAggregator = getAggregator(metadata, values, labels, featureSplits = Array.empty)\n+      // Choose split\n+      val (split, stats) =\n+        SplitUtils.chooseOrderedCategoricalSplit(statsAggregator, featureIndex, featureIndex)\n+      // Verify that split has the expected left-side/right-side categories\n+      val expectedRightCategories = Range(0, featureArity)\n+        .filter(c => !expectedLeftCategories.contains(c)).map(_.toDouble).toArray\n+      split match {\n+        case s: CategoricalSplit =>\n+          assert(s.featureIndex === featureIndex)\n+          assert(s.leftCategories === expectedLeftCategories)\n+          assert(s.rightCategories === expectedRightCategories)\n+        case _ =>\n+          throw new AssertionError(\n+            s\"Expected CategoricalSplit but got ${split.getClass.getSimpleName}\")\n+      }\n+      validateImpurityStats(Entropy, labels, stats, expectedLeftStats, expectedRightStats)\n+    }\n+\n+    // Test a single split: The left side of our split should contain the two points with label 0,\n+    // the left side of our split should contain the five points with label 1\n+    val values = Array(0, 0, 1, 2, 2, 2, 2)\n+    val labels1 = Array(0, 0, 1, 1, 1, 1, 1).map(_.toDouble)\n+    testHelper(values, labels1, expectedLeftCategories = Array(0.0),\n+      expectedLeftStats = Array(2.0, 0.0), expectedRightStats = Array(0.0, 5.0))\n+\n+    // Test a single split: The left side of our split should contain the three points with label 0,\n+    // the left side of our split should contain the four points with label 1\n+    val labels2 = Array(0, 0, 0, 1, 1, 1, 1).map(_.toDouble)\n+    testHelper(values, labels2, expectedLeftCategories = Array(0.0, 1.0),\n+      expectedLeftStats = Array(3.0, 0.0), expectedRightStats = Array(0.0, 4.0))\n+  }\n+\n+  test(\"chooseOrderedCategoricalSplit: return bad stats if we should not split\") {\n+    // Construct categorical data\n+    val featureIndex = 0\n+    val values = Array(0, 0, 1, 2, 2, 2, 2)\n+    val featureArity = values.max + 1\n+    val labels = Array(1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0)\n+    // Construct DTStatsAggregator, compute sufficient stats\n+    val metadata = TreeTests.getMetadata(numExamples = values.length, numFeatures = 1,\n+      numClasses = 2, Map(featureIndex -> featureArity), unorderedFeatures = Some(Set.empty))\n+    val statsAggregator = getAggregator(metadata, values, labels, featureSplits = Array.empty)\n+    // Choose split, verify that it's invalid\n+    val (_, stats) = SplitUtils.chooseOrderedCategoricalSplit(statsAggregator, featureIndex,\n+        featureIndex)\n+    assert(!stats.valid)\n+  }\n+\n+  test(\"chooseUnorderedCategoricalSplit: basic case\") {\n+    val featureIndex = 0\n+    // Construct data for unordered categorical feature\n+    // label: 0 --> values: 1\n+    // label: 1 --> values: 0, 2\n+    // label: 2 --> values: 2\n+    // Expected split: feature value 1 on the left, values (0, 2) on the right\n+    val values = Array(1, 1, 0, 2, 2)\n+    val featureArity = values.max + 1"
  }, {
    "author": {
      "login": "WeichenXu123"
    },
    "body": "I think it is OK. thanks!",
    "commit": "5bcccda6a599f60d2d084ecf871649675a792d5d",
    "createdAt": "2017-12-01T09:36:52Z",
    "diffHunk": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.tree.impl\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.ml.tree.{CategoricalSplit, ContinuousSplit, Split}\n+import org.apache.spark.ml.util.DefaultReadWriteTest\n+import org.apache.spark.mllib.tree.impurity.{Entropy, Impurity}\n+import org.apache.spark.mllib.tree.model.ImpurityStats\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+\n+/** Suite exercising helper methods for making split decisions during decision tree training. */\n+class TreeSplitUtilsSuite\n+  extends SparkFunSuite with MLlibTestSparkContext with DefaultReadWriteTest {\n+\n+  /**\n+   * Get a DTStatsAggregator for sufficient stat collection/impurity calculation populated\n+   * with the data from the specified training points. Assumes a feature index of 0 and that\n+   * all training points have the same weights (1.0).\n+   */\n+  private def getAggregator(\n+      metadata: DecisionTreeMetadata,\n+      values: Array[Int],\n+      labels: Array[Double],\n+      featureSplits: Array[Split]): DTStatsAggregator = {\n+    // Create stats aggregator\n+    val statsAggregator = new DTStatsAggregator(metadata, featureSubset = None)\n+    // Update parent impurity stats\n+    val featureIndex = 0\n+    val instanceWeights = Array.fill[Double](values.length)(1.0)\n+    AggUpdateUtils.updateParentImpurity(statsAggregator, indices = values.indices.toArray,\n+      from = 0, to = values.length, instanceWeights, labels)\n+    // Update current aggregator's impurity stats\n+    values.zip(labels).foreach { case (value: Int, label: Double) =>\n+      if (metadata.isUnordered(featureIndex)) {\n+        AggUpdateUtils.updateUnorderedFeature(statsAggregator, value, label,\n+          featureIndex = featureIndex, featureIndexIdx = 0, featureSplits, instanceWeight = 1.0)\n+      } else {\n+        AggUpdateUtils.updateOrderedFeature(statsAggregator, value, label, featureIndexIdx = 0,\n+          instanceWeight = 1.0)\n+      }\n+    }\n+    statsAggregator\n+  }\n+\n+  /**\n+   * Check that left/right impurities match what we'd expect for a split.\n+   * @param labels Labels whose impurity information should be reflected in stats\n+   * @param stats ImpurityStats object containing impurity info for the left/right sides of a split\n+   */\n+  private def validateImpurityStats(\n+      impurity: Impurity,\n+      labels: Array[Double],\n+      stats: ImpurityStats,\n+      expectedLeftStats: Array[Double],\n+      expectedRightStats: Array[Double]): Unit = {\n+    // Compute impurity for our data points manually\n+    val numClasses = (labels.max + 1).toInt\n+    val fullImpurityStatsArray\n+      = Array.tabulate[Double](numClasses)((label: Int) => labels.count(_ == label).toDouble)\n+    val fullImpurity = Entropy.calculate(fullImpurityStatsArray, labels.length)\n+    // Verify that impurity stats were computed correctly for split\n+    assert(stats.impurityCalculator.stats === fullImpurityStatsArray)\n+    assert(stats.impurity === fullImpurity)\n+    assert(stats.leftImpurityCalculator.stats === expectedLeftStats)\n+    assert(stats.rightImpurityCalculator.stats === expectedRightStats)\n+    assert(stats.valid)\n+  }\n+\n+  /* * * * * * * * * * * Choosing Splits  * * * * * * * * * * */\n+\n+  test(\"chooseSplit: choose correct type of split (continuous split)\") {\n+    // Construct (binned) continuous data\n+    val labels = Array(0.0, 0.0, 1.0)\n+    val values = Array(1, 2, 3)\n+    val featureIndex = 0\n+    // Get an array of continuous splits corresponding to values in our binned data\n+    val splits = TreeTests.getContinuousSplits(thresholds = values.distinct.sorted,\n+      featureIndex = 0)\n+    // Construct DTStatsAggregator, compute sufficient stats\n+    val metadata = TreeTests.getMetadata(numExamples = values.length, numFeatures = 1,\n+      numClasses = 2, Map.empty)\n+    val statsAggregator = getAggregator(metadata, values, labels, splits)\n+    // Choose split, check that it's a valid ContinuousSplit\n+    val (split, stats) = SplitUtils.chooseSplit(statsAggregator, featureIndex, featureIndex,\n+      splits)\n+    assert(stats.valid && split.isInstanceOf[ContinuousSplit])\n+  }\n+\n+  test(\"chooseSplit: choose correct type of split (categorical split)\") {\n+    // Construct categorical data\n+    val labels = Array(0.0, 0.0, 1.0, 1.0, 1.0)\n+    val featureArity = 3\n+    val values = Array(0, 0, 1, 2, 2)\n+    val featureIndex = 0\n+    // Construct DTStatsAggregator, compute sufficient stats\n+    val metadata = TreeTests.getMetadata(numExamples = values.length, numFeatures = 1,\n+      numClasses = 2, Map(featureIndex -> featureArity))\n+    val splits = RandomForest.findUnorderedSplits(metadata, featureIndex)\n+    val statsAggregator = getAggregator(metadata, values, labels, splits)\n+    // Choose split, check that it's a valid categorical split\n+    val (split, stats) = SplitUtils.chooseSplit(statsAggregator = statsAggregator,\n+      featureIndex = featureIndex, featureIndexIdx = featureIndex, featureSplits = splits)\n+    assert(stats.valid && split.isInstanceOf[CategoricalSplit])\n+  }\n+\n+  test(\"chooseOrderedCategoricalSplit: basic case\") {\n+    // Helper method for testing ordered categorical split\n+    def testHelper(\n+        values: Array[Int],\n+        labels: Array[Double],\n+        expectedLeftCategories: Array[Double],\n+        expectedLeftStats: Array[Double],\n+        expectedRightStats: Array[Double]): Unit = {\n+      // Set up metadata for ordered categorical feature\n+      val featureIndex = 0\n+      val featureArity = values.max + 1\n+      val arityMap = Map[Int, Int](featureIndex -> featureArity)\n+      val metadata = TreeTests.getMetadata(numExamples = values.length, numFeatures = 1,\n+        numClasses = 2, arityMap, unorderedFeatures = Some(Set.empty))\n+      // Construct DTStatsAggregator, compute sufficient stats\n+      val statsAggregator = getAggregator(metadata, values, labels, featureSplits = Array.empty)\n+      // Choose split\n+      val (split, stats) =\n+        SplitUtils.chooseOrderedCategoricalSplit(statsAggregator, featureIndex, featureIndex)\n+      // Verify that split has the expected left-side/right-side categories\n+      val expectedRightCategories = Range(0, featureArity)\n+        .filter(c => !expectedLeftCategories.contains(c)).map(_.toDouble).toArray\n+      split match {\n+        case s: CategoricalSplit =>\n+          assert(s.featureIndex === featureIndex)\n+          assert(s.leftCategories === expectedLeftCategories)\n+          assert(s.rightCategories === expectedRightCategories)\n+        case _ =>\n+          throw new AssertionError(\n+            s\"Expected CategoricalSplit but got ${split.getClass.getSimpleName}\")\n+      }\n+      validateImpurityStats(Entropy, labels, stats, expectedLeftStats, expectedRightStats)\n+    }\n+\n+    // Test a single split: The left side of our split should contain the two points with label 0,\n+    // the left side of our split should contain the five points with label 1\n+    val values = Array(0, 0, 1, 2, 2, 2, 2)\n+    val labels1 = Array(0, 0, 1, 1, 1, 1, 1).map(_.toDouble)\n+    testHelper(values, labels1, expectedLeftCategories = Array(0.0),\n+      expectedLeftStats = Array(2.0, 0.0), expectedRightStats = Array(0.0, 5.0))\n+\n+    // Test a single split: The left side of our split should contain the three points with label 0,\n+    // the left side of our split should contain the four points with label 1\n+    val labels2 = Array(0, 0, 0, 1, 1, 1, 1).map(_.toDouble)\n+    testHelper(values, labels2, expectedLeftCategories = Array(0.0, 1.0),\n+      expectedLeftStats = Array(3.0, 0.0), expectedRightStats = Array(0.0, 4.0))\n+  }\n+\n+  test(\"chooseOrderedCategoricalSplit: return bad stats if we should not split\") {\n+    // Construct categorical data\n+    val featureIndex = 0\n+    val values = Array(0, 0, 1, 2, 2, 2, 2)\n+    val featureArity = values.max + 1\n+    val labels = Array(1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0)\n+    // Construct DTStatsAggregator, compute sufficient stats\n+    val metadata = TreeTests.getMetadata(numExamples = values.length, numFeatures = 1,\n+      numClasses = 2, Map(featureIndex -> featureArity), unorderedFeatures = Some(Set.empty))\n+    val statsAggregator = getAggregator(metadata, values, labels, featureSplits = Array.empty)\n+    // Choose split, verify that it's invalid\n+    val (_, stats) = SplitUtils.chooseOrderedCategoricalSplit(statsAggregator, featureIndex,\n+        featureIndex)\n+    assert(!stats.valid)\n+  }\n+\n+  test(\"chooseUnorderedCategoricalSplit: basic case\") {\n+    val featureIndex = 0\n+    // Construct data for unordered categorical feature\n+    // label: 0 --> values: 1\n+    // label: 1 --> values: 0, 2\n+    // label: 2 --> values: 2\n+    // Expected split: feature value 1 on the left, values (0, 2) on the right\n+    val values = Array(1, 1, 0, 2, 2)\n+    val featureArity = values.max + 1"
  }],
  "prId": 19758
}]