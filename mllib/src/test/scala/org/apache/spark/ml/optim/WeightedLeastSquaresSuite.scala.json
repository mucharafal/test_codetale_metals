[{
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "IMO, comments referencing things like \"above\" should be avoided since the code can be rearranged. This comment would be confusing if the above data is declared in a different place or removed altogether. Maybe just give the entire R code instead?\n",
    "commit": "d591989f7383b713110750f80b2720bcf24814b5",
    "createdAt": "2015-12-14T17:48:20Z",
    "diffHunk": "@@ -43,6 +44,18 @@ class WeightedLeastSquaresSuite extends SparkFunSuite with MLlibTestSparkContext\n       Instance(23.0, 3.0, Vectors.dense(2.0, 11.0)),\n       Instance(29.0, 4.0, Vectors.dense(3.0, 13.0))\n     ), 2)\n+\n+    /*\n+       R code:\n+       same as above except make the label constant"
  }],
  "prId": 10274
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "Added `standardizeLabel = false` with non-zero `regParam` with analytic normal equation solution. \n\nAdded failure test of `standardizeLabel = true`, `regParam != 0` and `yStd == 0.0`.\n",
    "commit": "d591989f7383b713110750f80b2720bcf24814b5",
    "createdAt": "2016-01-12T00:21:22Z",
    "diffHunk": "@@ -74,6 +89,35 @@ class WeightedLeastSquaresSuite extends SparkFunSuite with MLlibTestSparkContext\n     }\n   }\n \n+  test(\"WLS against lm when label is constant\") {\n+    /*\n+       R code:\n+       # here b is constant\n+       df <- as.data.frame(cbind(A, b))\n+       for (formula in c(b ~ . -1, b ~ .)) {\n+         model <- lm(formula, data=df, weights=w)\n+         print(as.vector(coef(model)))\n+       }\n+\n+      [1] -9.221298  3.394343\n+      [1] 17  0  0\n+    */\n+\n+    val expected = Seq(\n+      Vectors.dense(0.0, -9.221298, 3.394343),\n+      Vectors.dense(17.0, 0.0, 0.0))\n+\n+    var idx = 0\n+    for (fitIntercept <- Seq(false, true)) {\n+      val wls = new WeightedLeastSquares(\n+        fitIntercept, regParam = 0.0, standardizeFeatures = false, standardizeLabel = true)\n+        .fit(instancesConstLabel)"
  }, {
    "author": {
      "login": "iyounus"
    },
    "body": "I've added exception and the test for `standardizeLabel = true`, `regParam != 0` and `yStd == 0.0`. The only thing now left is to add test for the case `standardizeLabel = false`, `regParam != 0` and `yStd == 0.0`. As I mentioned before, in this case I cannot compare with `glmnet`. So, I'll try to implement normal equation in python by myself and compare with that. The good thing is that, for this  particular case, both `normal` and `l-bfgs` give identical results!\n",
    "commit": "d591989f7383b713110750f80b2720bcf24814b5",
    "createdAt": "2016-01-15T02:39:51Z",
    "diffHunk": "@@ -74,6 +89,35 @@ class WeightedLeastSquaresSuite extends SparkFunSuite with MLlibTestSparkContext\n     }\n   }\n \n+  test(\"WLS against lm when label is constant\") {\n+    /*\n+       R code:\n+       # here b is constant\n+       df <- as.data.frame(cbind(A, b))\n+       for (formula in c(b ~ . -1, b ~ .)) {\n+         model <- lm(formula, data=df, weights=w)\n+         print(as.vector(coef(model)))\n+       }\n+\n+      [1] -9.221298  3.394343\n+      [1] 17  0  0\n+    */\n+\n+    val expected = Seq(\n+      Vectors.dense(0.0, -9.221298, 3.394343),\n+      Vectors.dense(17.0, 0.0, 0.0))\n+\n+    var idx = 0\n+    for (fitIntercept <- Seq(false, true)) {\n+      val wls = new WeightedLeastSquares(\n+        fitIntercept, regParam = 0.0, standardizeFeatures = false, standardizeLabel = true)\n+        .fit(instancesConstLabel)"
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "Awesome! Great that you see the same result! For normal equation, will be nice to in R so we can have it in comment consistently. I did implement it once when I implemented the LBFGS version, let me try to find it.\n",
    "commit": "d591989f7383b713110750f80b2720bcf24814b5",
    "createdAt": "2016-01-15T05:39:04Z",
    "diffHunk": "@@ -74,6 +89,35 @@ class WeightedLeastSquaresSuite extends SparkFunSuite with MLlibTestSparkContext\n     }\n   }\n \n+  test(\"WLS against lm when label is constant\") {\n+    /*\n+       R code:\n+       # here b is constant\n+       df <- as.data.frame(cbind(A, b))\n+       for (formula in c(b ~ . -1, b ~ .)) {\n+         model <- lm(formula, data=df, weights=w)\n+         print(as.vector(coef(model)))\n+       }\n+\n+      [1] -9.221298  3.394343\n+      [1] 17  0  0\n+    */\n+\n+    val expected = Seq(\n+      Vectors.dense(0.0, -9.221298, 3.394343),\n+      Vectors.dense(17.0, 0.0, 0.0))\n+\n+    var idx = 0\n+    for (fitIntercept <- Seq(false, true)) {\n+      val wls = new WeightedLeastSquares(\n+        fitIntercept, regParam = 0.0, standardizeFeatures = false, standardizeLabel = true)\n+        .fit(instancesConstLabel)"
  }, {
    "author": {
      "login": "iyounus"
    },
    "body": "@dbtsai I've implemented normal equation with regularization in R. Here is my code:\n\n```\nridge_regression <- function(A, b, lambda, intercept=TRUE){\n    if (intercept) {\n        A = cbind(rep(1.0, length(b)), A)\n        I = diag(ncol(A))\n        I[1,1] = 0.0\n    } else {\n        I = diag(ncol(A))\n    }\n    R = chol( t(A) %*% A + lambda*I )\n    z = solve(t(R), t(A) %*% b)\n    w = solve(R, z)\n    return(w)\n}\n```\n\nAnd here are the results I get using this function.\n\n```\nA <- matrix(c(0, 1, 2, 3, 5, 7, 11, 13), 4, 2)\nb <- c(17, 19, 23, 29)\nridge_regression(A, b, 0.1)\n           [,1]\n[1,] 12.9048179\n[2,]  2.1151586\n[3,]  0.6580494\n```\n\nThe problem is that these don't quite match with `glmnet`. The difference can be at few percent level:\n\n```\n> model <- glmnet(A, b, intercept=TRUE, lambda=0.1, standardize=FALSE,\n+ alpha=0, thresh=1E-20)\n> print(as.vector(coef(model)))\n[1] 13.1018870  2.2362361  0.6159732\n```\n\nBut, my results match exactly with what I get from ridge regression in sklearn:\n\n```\nfrom sklearn.linear_model import Ridge\nimport numpy as np\n\nA = np.array([[0, 1, 2, 3],[5, 7, 11, 13]]).T\nb = np.array([17.0, 19.0, 23.0, 29.0])\nmodel = Ridge(alpha=0.1, solver='cholesky', fit_intercept=True)\nmodel.fit(A, b)\nprint model.intercept_\nprint model.coef_\n\n12.9048178613\n[ 2.11515864  0.65804935]\n```\n\nEven if I use other solvers (`svd`, `lsqr`, `sparse_cg`) in `sklearn.linear_model.Ridge`, I get exactly the same results.\n\nIf I don't use regularization by setting `lambda=0`, then the results from `glmnet` are identical to what I get from normal equation and `sklearn.linear_model.Ridge`.\n\nHave you observed such differences before? Is `glmnet` making some other corrections or its just numerical precision issue?  I can't seem to reproduce `glmnet` results.\n",
    "commit": "d591989f7383b713110750f80b2720bcf24814b5",
    "createdAt": "2016-01-19T00:32:18Z",
    "diffHunk": "@@ -74,6 +89,35 @@ class WeightedLeastSquaresSuite extends SparkFunSuite with MLlibTestSparkContext\n     }\n   }\n \n+  test(\"WLS against lm when label is constant\") {\n+    /*\n+       R code:\n+       # here b is constant\n+       df <- as.data.frame(cbind(A, b))\n+       for (formula in c(b ~ . -1, b ~ .)) {\n+         model <- lm(formula, data=df, weights=w)\n+         print(as.vector(coef(model)))\n+       }\n+\n+      [1] -9.221298  3.394343\n+      [1] 17  0  0\n+    */\n+\n+    val expected = Seq(\n+      Vectors.dense(0.0, -9.221298, 3.394343),\n+      Vectors.dense(17.0, 0.0, 0.0))\n+\n+    var idx = 0\n+    for (fitIntercept <- Seq(false, true)) {\n+      val wls = new WeightedLeastSquares(\n+        fitIntercept, regParam = 0.0, standardizeFeatures = false, standardizeLabel = true)\n+        .fit(instancesConstLabel)"
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "Sorry for getting you back so late. The difference is due to that `glmnet` always standardizes labels even `standardization == false`. `standardization == false` is turning off the standardization on features. As a result, at least in `glmnet`, when `ystd == 0.0`, the training is not valid. \n",
    "commit": "d591989f7383b713110750f80b2720bcf24814b5",
    "createdAt": "2016-01-27T22:18:43Z",
    "diffHunk": "@@ -74,6 +89,35 @@ class WeightedLeastSquaresSuite extends SparkFunSuite with MLlibTestSparkContext\n     }\n   }\n \n+  test(\"WLS against lm when label is constant\") {\n+    /*\n+       R code:\n+       # here b is constant\n+       df <- as.data.frame(cbind(A, b))\n+       for (formula in c(b ~ . -1, b ~ .)) {\n+         model <- lm(formula, data=df, weights=w)\n+         print(as.vector(coef(model)))\n+       }\n+\n+      [1] -9.221298  3.394343\n+      [1] 17  0  0\n+    */\n+\n+    val expected = Seq(\n+      Vectors.dense(0.0, -9.221298, 3.394343),\n+      Vectors.dense(17.0, 0.0, 0.0))\n+\n+    var idx = 0\n+    for (fitIntercept <- Seq(false, true)) {\n+      val wls = new WeightedLeastSquares(\n+        fitIntercept, regParam = 0.0, standardizeFeatures = false, standardizeLabel = true)\n+        .fit(instancesConstLabel)"
  }],
  "prId": 10274
}]