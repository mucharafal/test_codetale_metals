[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "move `assert` into `foreach` to get more information.\n",
    "commit": "ee2b37ac50a816f7fa1c779a97cb2db881f0cc10",
    "createdAt": "2015-04-22T06:31:57Z",
    "diffHunk": "@@ -0,0 +1,87 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.scalatest.FunSuite\n+\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.mllib.util.TestingUtils._\n+import org.apache.spark.sql.{Row, SQLContext}\n+\n+class Word2VecSuite extends FunSuite with MLlibTestSparkContext {\n+\n+  test(\"Word2Vec\") {\n+    val sqlContext = new SQLContext(sc)\n+    import sqlContext.implicits._\n+\n+    val sentence = \"a b \" * 100 + \"a c \" * 10\n+    val localDoc = Seq(sentence, sentence)\n+    val doc = sc.parallelize(localDoc)\n+      .map(line => line.split(\" \"))\n+    val docDF = doc.map(text => Tuple1(text)).toDF(\"text\")\n+\n+    val model = new Word2Vec()\n+      .setVectorSize(3)\n+      .setSeed(42L)\n+      .setInputCol(\"text\")\n+      .setMaxIter(1)\n+      .fit(docDF)\n+\n+    val words = sc.parallelize(Seq(\"a\", \"b\", \"c\"))\n+    val codes = Map(\n+      \"a\" -> Vectors.dense(-0.2811822295188904,-0.6356269121170044,-0.3020961284637451),\n+      \"b\" -> Vectors.dense(1.0309048891067505,-1.29472815990448,0.22276712954044342),\n+      \"c\" -> Vectors.dense(-0.08456747233867645,0.5137411952018738,0.11731560528278351)\n+    )\n+\n+    val synonyms = Map(\n+      \"a\" -> Map(\"b\" -> 0.3680490553379059),\n+      \"b\" -> Map(\"a\" -> 0.3680490553379059),\n+      \"c\" -> Map(\"b\" -> -0.8148014545440674)\n+    )\n+    val wordsDF = words.map(word => Tuple3(word, codes(word), synonyms(word)))\n+      .toDF(\"word\", \"realCode\", \"realSynonyms\")\n+\n+    val res = model\n+      .setInputCol(\"word\")\n+      .setCodeCol(\"code\")\n+      .setSynonymsCol(\"syn\")\n+      .setNumSynonyms(1)\n+      .transform(wordsDF)\n+\n+    assert("
  }],
  "prId": 5596
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Please run this test multiple times and see whether this is deterministic.\n",
    "commit": "ee2b37ac50a816f7fa1c779a97cb2db881f0cc10",
    "createdAt": "2015-04-24T15:38:58Z",
    "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.scalatest.FunSuite\n+\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+import org.apache.spark.mllib.util.TestingUtils._\n+import org.apache.spark.sql.{Row, SQLContext}\n+\n+class Word2VecSuite extends FunSuite with MLlibTestSparkContext {\n+\n+  test(\"Word2Vec\") {\n+    val sqlContext = new SQLContext(sc)\n+    import sqlContext.implicits._\n+\n+    val sentence = \"a b \" * 100 + \"a c \" * 10\n+    val numOfWords = sentence.split(\" \").size\n+    val doc = sc.parallelize(Seq(sentence, sentence)).map(line => line.split(\" \"))\n+\n+    val codes = Map(\n+      \"a\" -> Array(-0.2811822295188904,-0.6356269121170044,-0.3020961284637451),\n+      \"b\" -> Array(1.0309048891067505,-1.29472815990448,0.22276712954044342),\n+      \"c\" -> Array(-0.08456747233867645,0.5137411952018738,0.11731560528278351)\n+    )\n+\n+    val expected = doc.map { sentence =>\n+      Vectors.dense(sentence.map(codes.apply).reduce((word1, word2) =>\n+        word1.zip(word2).map { case (v1, v2) => v1 + v2 }\n+      ).map(_ / numOfWords))\n+    }\n+\n+    val docDF = doc.zip(expected).toDF(\"text\", \"expected\")\n+\n+    val model = new Word2Vec()\n+      .setVectorSize(3)\n+      .setInputCol(\"text\")\n+      .setOutputCol(\"result\")\n+      .fit(docDF)\n+\n+    model.transform(docDF).select(\"result\", \"expected\").collect().foreach {\n+      case Row(vector1: Vector, vector2: Vector) =>\n+        assert(vector1 ~== vector2 absTol 1E-5, \"Transformed vector is different with expected.\")",
    "line": 59
  }],
  "prId": 5596
}]