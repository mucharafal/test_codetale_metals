[{
  "comments": [{
    "author": {
      "login": "WeichenXu123"
    },
    "body": "Use `~== relTol` instead of `===`",
    "commit": "d4369ffea2235ca74715b734da23d7c48e53140a",
    "createdAt": "2017-08-30T13:45:13Z",
    "diffHunk": "@@ -0,0 +1,170 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.ml.optim.aggregator\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg.{BLAS, Vector, Vectors}\n+import org.apache.spark.ml.util.TestingUtils._\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+\n+class HuberAggregatorSuite extends SparkFunSuite with MLlibTestSparkContext {\n+\n+  import DifferentiableLossAggregatorSuite.getRegressionSummarizers\n+\n+  @transient var instances: Array[Instance] = _\n+  @transient var instancesConstantFeature: Array[Instance] = _\n+  @transient var instancesConstantFeatureFiltered: Array[Instance] = _\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    instances = Array(\n+      Instance(0.0, 0.1, Vectors.dense(1.0, 2.0)),\n+      Instance(1.0, 0.5, Vectors.dense(1.5, 1.0)),\n+      Instance(2.0, 0.3, Vectors.dense(4.0, 0.5))\n+    )\n+    instancesConstantFeature = Array(\n+      Instance(0.0, 0.1, Vectors.dense(1.0, 2.0)),\n+      Instance(1.0, 0.5, Vectors.dense(1.0, 1.0)),\n+      Instance(2.0, 0.3, Vectors.dense(1.0, 0.5))\n+    )\n+    instancesConstantFeatureFiltered = Array(\n+      Instance(0.0, 0.1, Vectors.dense(2.0)),\n+      Instance(1.0, 0.5, Vectors.dense(1.0)),\n+      Instance(2.0, 0.3, Vectors.dense(0.5))\n+    )\n+  }\n+\n+  /** Get summary statistics for some data and create a new HuberAggregator. */\n+  private def getNewAggregator(\n+      instances: Array[Instance],\n+      parameters: Vector,\n+      fitIntercept: Boolean,\n+      m: Double): HuberAggregator = {\n+    val (featuresSummarizer, _) = getRegressionSummarizers(instances)\n+    val featuresStd = featuresSummarizer.variance.toArray.map(math.sqrt)\n+    val bcFeaturesStd = spark.sparkContext.broadcast(featuresStd)\n+    val bcParameters = spark.sparkContext.broadcast(parameters)\n+    new HuberAggregator(fitIntercept, m, bcFeaturesStd)(bcParameters)\n+  }\n+\n+  test(\"aggregator add method should check input size\") {\n+    val parameters = Vectors.dense(1.0, 2.0, 3.0, 4.0)\n+    val agg = getNewAggregator(instances, parameters, fitIntercept = true, m = 1.35)\n+    withClue(\"HuberAggregator features dimension must match parameters dimension\") {\n+      intercept[IllegalArgumentException] {\n+        agg.add(Instance(1.0, 1.0, Vectors.dense(2.0)))\n+      }\n+    }\n+  }\n+\n+  test(\"negative weight\") {\n+    val parameters = Vectors.dense(1.0, 2.0, 3.0, 4.0)\n+    val agg = getNewAggregator(instances, parameters, fitIntercept = true, m = 1.35)\n+    withClue(\"HuberAggregator does not support negative instance weights.\") {\n+      intercept[IllegalArgumentException] {\n+        agg.add(Instance(1.0, -1.0, Vectors.dense(2.0, 1.0)))\n+      }\n+    }\n+  }\n+\n+  test(\"check sizes\") {\n+    val paramWithIntercept = Vectors.dense(1.0, 2.0, 3.0, 4.0)\n+    val paramWithoutIntercept = Vectors.dense(1.0, 2.0, 4.0)\n+    val aggIntercept = getNewAggregator(instances, paramWithIntercept,\n+      fitIntercept = true, m = 1.35)\n+    val aggNoIntercept = getNewAggregator(instances, paramWithoutIntercept,\n+      fitIntercept = false, m = 1.35)\n+    instances.foreach(aggIntercept.add)\n+    instances.foreach(aggNoIntercept.add)\n+\n+    assert(aggIntercept.gradient.size === 4)\n+    assert(aggNoIntercept.gradient.size === 3)\n+  }\n+\n+  test(\"check correctness\") {\n+    val parameters = Vectors.dense(1.0, 2.0, 3.0, 4.0)\n+    val numFeatures = 2\n+    val (featuresSummarizer, _) = getRegressionSummarizers(instances)\n+    val featuresStd = featuresSummarizer.variance.toArray.map(math.sqrt)\n+    val m = 1.35\n+    val weightSum = instances.map(_.weight).sum\n+\n+    val agg = getNewAggregator(instances, parameters, fitIntercept = true, m)\n+    instances.foreach(agg.add)\n+\n+    // compute expected loss sum\n+    val coefficients = parameters.toArray.slice(0, 2)\n+    val intercept = parameters(2)\n+    val sigma = parameters(3)\n+    val stdCoef = coefficients.indices.map(i => coefficients(i) / featuresStd(i)).toArray\n+    val lossSum = instances.map { case Instance(label, weight, features) =>\n+      val margin = BLAS.dot(Vectors.dense(stdCoef), features) + intercept\n+      val linearLoss = label - margin\n+      if (math.abs(linearLoss) <= sigma * m) {\n+        0.5 * weight * (sigma +  math.pow(linearLoss, 2.0) / sigma)\n+      } else {\n+        0.5 * weight * (sigma + 2.0 * m * math.abs(linearLoss) - sigma * m * m)\n+      }\n+    }.sum\n+    val loss = lossSum / weightSum\n+\n+    // compute expected gradients\n+    val gradientCoef = new Array[Double](numFeatures + 2)\n+    instances.foreach { case Instance(label, weight, features) =>\n+      val margin = BLAS.dot(Vectors.dense(stdCoef), features) + intercept\n+      val linearLoss = label - margin\n+      if (math.abs(linearLoss) <= sigma * m) {\n+        features.toArray.indices.foreach { i =>\n+          gradientCoef(i) +=\n+            -1.0 * weight * linearLoss / sigma * (features(i) / featuresStd(i))\n+        }\n+        gradientCoef(2) += -1.0 * weight * linearLoss / sigma\n+        gradientCoef(3) += 0.5 * weight * (1.0 - math.pow(linearLoss / sigma, 2.0))\n+      } else {\n+        val sign = if (linearLoss >= 0) -1.0 else 1.0\n+        features.toArray.indices.foreach { i =>\n+          gradientCoef(i) += weight * sign * m * (features(i) / featuresStd(i))\n+        }\n+        gradientCoef(2) += weight * sign * m\n+        gradientCoef(3) += 0.5 * weight * (1.0 - m * m)\n+      }\n+    }\n+    val gradient = Vectors.dense(gradientCoef.map(_ / weightSum))\n+\n+    assert(loss ~== agg.loss relTol 0.01)\n+    assert(gradient ~== agg.gradient relTol 0.01)\n+  }\n+\n+  test(\"check with zero standard deviation\") {\n+    val parameters = Vectors.dense(1.0, 2.0, 3.0, 4.0)\n+    val parametersFiltered = Vectors.dense(2.0, 3.0, 4.0)\n+    val aggConstantFeature = getNewAggregator(instancesConstantFeature, parameters,\n+      fitIntercept = true, m = 1.35)\n+    val aggConstantFeatureFiltered = getNewAggregator(instancesConstantFeatureFiltered,\n+      parametersFiltered, fitIntercept = true, m = 1.35)\n+    instances.foreach(aggConstantFeature.add)\n+    instancesConstantFeatureFiltered.foreach(aggConstantFeatureFiltered.add)\n+    // constant features should not affect gradient\n+    def validateGradient(grad: Vector, gradFiltered: Vector): Unit = {\n+      assert(grad(0) === 0.0)\n+      assert(grad(1) === gradFiltered(0))"
  }],
  "prId": 19020
}]