[{
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "I think the precision and recall values should be swapped. `correctCount / expected.count()` should be recall. `correctCount / actual.count()` should be precision.\n",
    "commit": "35708458a0ee156c097ca604efeafaa37d3c8a6d",
    "createdAt": "2016-09-19T08:26:55Z",
    "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.lsh\n+\n+import org.apache.spark.ml.linalg.Vector\n+import org.apache.spark.sql.Dataset\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.DataTypes\n+\n+private[ml] object LSHTest {\n+  /**\n+   * For any locality sensitive function h in a metric space, we meed to verify whether\n+   * the following property is satisfied.\n+   *\n+   * There exist d1, d2, p1, p2, so that for any two elements e1 and e2,\n+   * If dist(e1, e2) >= dist1, then Pr{h(x) == h(y)} >= p1\n+   * If dist(e1, e2) <= dist2, then Pr{h(x) != h(y)} <= p2\n+   *\n+   * This is called locality sensitive property. This method checks the property on an\n+   * existing dataset and calculate the probabilities.\n+   * (https://en.wikipedia.org/wiki/Locality-sensitive_hashing#Definition)\n+   *\n+   * @param dataset The dataset to verify the locality sensitive hashing property.\n+   * @param lsh The lsh instance to perform the hashing\n+   * @param dist1 Distance threshold for false positive\n+   * @param dist2 Distance threshold for false negative\n+   * @tparam KeyType The input key type of LSH\n+   * @tparam T The class type of lsh\n+   * @return A tuple of two doubles, representing the false positive and false negative rate\n+   */\n+  def checkLSHProperty[KeyType, T <: LSHModel[KeyType, T]]\n+  (dataset: Dataset[_], lsh: LSH[KeyType, T], dist1: Double, dist2: Double): (Double, Double) = {\n+    val model = lsh.fit(dataset)\n+    val inputCol = model.getInputCol\n+    val outputCol = model.getOutputCol\n+    val transformedData = model.transform(dataset)\n+\n+    // Perform a cross join and label each pair of same_bucket and distance\n+    val pairs = transformedData.as(\"a\").crossJoin(transformedData.as(\"b\"))\n+    val distUDF = udf((x: KeyType, y: KeyType) => model.keyDistance(x, y), DataTypes.DoubleType)\n+    val sameBucket = udf((x: Vector, y: Vector) => model.hashDistance(x, y) == 0.0,\n+      DataTypes.BooleanType)\n+    val result = pairs\n+      .withColumn(\"same_bucket\", sameBucket(col(s\"a.$outputCol\"), col(s\"b.$outputCol\")))\n+      .withColumn(\"distance\", distUDF(col(s\"a.$inputCol\"), col(s\"b.$inputCol\")))\n+\n+    // Compute the probabilities based on the join result\n+    val positive = result.filter(col(\"same_bucket\"))\n+    val negative = result.filter(!col(\"same_bucket\"))\n+    val falsePositiveCount = positive.filter(col(\"distance\") > dist1).count().toDouble\n+    val falseNegativeCount = negative.filter(col(\"distance\") < dist2).count().toDouble\n+    (falsePositiveCount / positive.count(), falseNegativeCount / negative.count())\n+  }\n+\n+  /**\n+   * Check and compute the precision and recall of approximate nearest neighbors\n+   * @param lsh The lsh instance\n+   * @param dataset the dataset to look for the key\n+   * @param key The key to hash for the item\n+   * @param k The maximum number of items closest to the key\n+   * @tparam KeyType The input key type of LSH\n+   * @tparam T The class type of lsh\n+   * @return A tuple of two doubles, representing precision and recall rate\n+   */\n+  def checkApproxNearestNeighbors[KeyType, T <: LSHModel[KeyType, T]]\n+  (lsh: LSH[KeyType, T], dataset: Dataset[_], key: KeyType, k: Int): (Double, Double) = {\n+    val model = lsh.fit(dataset)\n+\n+    // Compute expected\n+    val distUDF = udf((x: KeyType) => model.keyDistance(x, key), DataTypes.DoubleType)\n+    val expected = dataset.sort(distUDF(col(model.getInputCol))).limit(k)\n+\n+    // Compute actual\n+    val actual = model.approxNearestNeighbors(dataset, key, k)\n+\n+    // Compute precision and recall\n+    val correctCount = expected.join(actual, model.getInputCol).count().toDouble\n+    (correctCount / expected.count(), correctCount / actual.count())"
  }, {
    "author": {
      "login": "Yunni"
    },
    "body": "Done.\n",
    "commit": "35708458a0ee156c097ca604efeafaa37d3c8a6d",
    "createdAt": "2016-09-19T22:43:59Z",
    "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.lsh\n+\n+import org.apache.spark.ml.linalg.Vector\n+import org.apache.spark.sql.Dataset\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.DataTypes\n+\n+private[ml] object LSHTest {\n+  /**\n+   * For any locality sensitive function h in a metric space, we meed to verify whether\n+   * the following property is satisfied.\n+   *\n+   * There exist d1, d2, p1, p2, so that for any two elements e1 and e2,\n+   * If dist(e1, e2) >= dist1, then Pr{h(x) == h(y)} >= p1\n+   * If dist(e1, e2) <= dist2, then Pr{h(x) != h(y)} <= p2\n+   *\n+   * This is called locality sensitive property. This method checks the property on an\n+   * existing dataset and calculate the probabilities.\n+   * (https://en.wikipedia.org/wiki/Locality-sensitive_hashing#Definition)\n+   *\n+   * @param dataset The dataset to verify the locality sensitive hashing property.\n+   * @param lsh The lsh instance to perform the hashing\n+   * @param dist1 Distance threshold for false positive\n+   * @param dist2 Distance threshold for false negative\n+   * @tparam KeyType The input key type of LSH\n+   * @tparam T The class type of lsh\n+   * @return A tuple of two doubles, representing the false positive and false negative rate\n+   */\n+  def checkLSHProperty[KeyType, T <: LSHModel[KeyType, T]]\n+  (dataset: Dataset[_], lsh: LSH[KeyType, T], dist1: Double, dist2: Double): (Double, Double) = {\n+    val model = lsh.fit(dataset)\n+    val inputCol = model.getInputCol\n+    val outputCol = model.getOutputCol\n+    val transformedData = model.transform(dataset)\n+\n+    // Perform a cross join and label each pair of same_bucket and distance\n+    val pairs = transformedData.as(\"a\").crossJoin(transformedData.as(\"b\"))\n+    val distUDF = udf((x: KeyType, y: KeyType) => model.keyDistance(x, y), DataTypes.DoubleType)\n+    val sameBucket = udf((x: Vector, y: Vector) => model.hashDistance(x, y) == 0.0,\n+      DataTypes.BooleanType)\n+    val result = pairs\n+      .withColumn(\"same_bucket\", sameBucket(col(s\"a.$outputCol\"), col(s\"b.$outputCol\")))\n+      .withColumn(\"distance\", distUDF(col(s\"a.$inputCol\"), col(s\"b.$inputCol\")))\n+\n+    // Compute the probabilities based on the join result\n+    val positive = result.filter(col(\"same_bucket\"))\n+    val negative = result.filter(!col(\"same_bucket\"))\n+    val falsePositiveCount = positive.filter(col(\"distance\") > dist1).count().toDouble\n+    val falseNegativeCount = negative.filter(col(\"distance\") < dist2).count().toDouble\n+    (falsePositiveCount / positive.count(), falseNegativeCount / negative.count())\n+  }\n+\n+  /**\n+   * Check and compute the precision and recall of approximate nearest neighbors\n+   * @param lsh The lsh instance\n+   * @param dataset the dataset to look for the key\n+   * @param key The key to hash for the item\n+   * @param k The maximum number of items closest to the key\n+   * @tparam KeyType The input key type of LSH\n+   * @tparam T The class type of lsh\n+   * @return A tuple of two doubles, representing precision and recall rate\n+   */\n+  def checkApproxNearestNeighbors[KeyType, T <: LSHModel[KeyType, T]]\n+  (lsh: LSH[KeyType, T], dataset: Dataset[_], key: KeyType, k: Int): (Double, Double) = {\n+    val model = lsh.fit(dataset)\n+\n+    // Compute expected\n+    val distUDF = udf((x: KeyType) => model.keyDistance(x, key), DataTypes.DoubleType)\n+    val expected = dataset.sort(distUDF(col(model.getInputCol))).limit(k)\n+\n+    // Compute actual\n+    val actual = model.approxNearestNeighbors(dataset, key, k)\n+\n+    // Compute precision and recall\n+    val correctCount = expected.join(actual, model.getInputCol).count().toDouble\n+    (correctCount / expected.count(), correctCount / actual.count())"
  }],
  "prId": 15148
}]