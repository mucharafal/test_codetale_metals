[{
  "comments": [{
    "author": {
      "login": "yanboliang"
    },
    "body": "We only need to check `mean` and `std` which are parts of the model, `withStd` and `withStd` are params. \n",
    "commit": "c6b6d7e65d96b681ecaa6ac004f8637f72bc0802",
    "createdAt": "2015-11-19T14:36:18Z",
    "diffHunk": "@@ -116,23 +116,19 @@ class StandardScalerSuite extends SparkFunSuite with MLlibTestSparkContext\n     assertResult(standardScaler3.transform(df3))\n   }\n \n-  test(\"StandardScaler read/write\") {\n-    val t = new StandardScaler()\n-      .setInputCol(\"myInputCol\")\n-      .setOutputCol(\"myOutputCol\")\n-      .setWithStd(false)\n-      .setWithMean(true)\n-    testDefaultReadWrite(t)\n-  }\n-\n-  test(\"StandardScalerModel read/write\") {\n-    val oldModel = new feature.StandardScalerModel(\n-      Vectors.dense(1.0, 2.0), Vectors.dense(3.0, 4.0), false, true)\n-    val instance = new StandardScalerModel(\"myStandardScalerModel\", oldModel)\n-    val newInstance = testDefaultReadWrite(instance)\n-    assert(newInstance.std === instance.std)\n-    assert(newInstance.mean === instance.mean)\n-    assert(newInstance.getWithStd === instance.getWithStd)\n-    assert(newInstance.getWithMean === instance.getWithMean)\n+  test(\"read/write\") {\n+    def checkModelData(model1: StandardScalerModel, model2: StandardScalerModel): Unit = {\n+      assert(model1.mean === model2.mean)\n+      assert(model1.std === model2.std)\n+    }"
  }],
  "prId": 9839
}, {
  "comments": [{
    "author": {
      "login": "yanboliang"
    },
    "body": "`withStd` and `withStd` of `StandardScalerModel` must be inherited from `StandardScaler`, so we can not construct  `StandardScalerModel` directly by specifying the two variables. Here we combine the original test cases into one with `testEstimatorAndModelReadWrite` which both test the estimator and model.\n",
    "commit": "c6b6d7e65d96b681ecaa6ac004f8637f72bc0802",
    "createdAt": "2015-11-19T14:49:23Z",
    "diffHunk": "@@ -116,23 +116,19 @@ class StandardScalerSuite extends SparkFunSuite with MLlibTestSparkContext\n     assertResult(standardScaler3.transform(df3))\n   }\n \n-  test(\"StandardScaler read/write\") {\n-    val t = new StandardScaler()\n-      .setInputCol(\"myInputCol\")\n-      .setOutputCol(\"myOutputCol\")\n-      .setWithStd(false)\n-      .setWithMean(true)\n-    testDefaultReadWrite(t)\n-  }\n-\n-  test(\"StandardScalerModel read/write\") {\n-    val oldModel = new feature.StandardScalerModel(\n-      Vectors.dense(1.0, 2.0), Vectors.dense(3.0, 4.0), false, true)\n-    val instance = new StandardScalerModel(\"myStandardScalerModel\", oldModel)\n-    val newInstance = testDefaultReadWrite(instance)\n-    assert(newInstance.std === instance.std)\n-    assert(newInstance.mean === instance.mean)\n-    assert(newInstance.getWithStd === instance.getWithStd)\n-    assert(newInstance.getWithMean === instance.getWithMean)\n+  test(\"read/write\") {\n+    def checkModelData(model1: StandardScalerModel, model2: StandardScalerModel): Unit = {\n+      assert(model1.mean === model2.mean)\n+      assert(model1.std === model2.std)\n+    }\n+    val allParams: Map[String, Any] = Map(\n+      \"inputCol\" -> \"features\",\n+      \"outputCol\" -> \"standardized_features\",\n+      \"withMean\" -> true,\n+      \"withStd\" -> true\n+    )\n+    val df = sqlContext.createDataFrame(data.zip(resWithBoth)).toDF(\"features\", \"expected\")\n+    val standardScaler = new StandardScaler()\n+    testEstimatorAndModelReadWrite(standardScaler, df, allParams, checkModelData)\n   }"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "I think this is not an ideal unit test for read/write because the model fitting part shouldn't be part of it, which is already covered by other tests. Constructing estimator and model directly can save some test time.\n",
    "commit": "c6b6d7e65d96b681ecaa6ac004f8637f72bc0802",
    "createdAt": "2015-11-20T06:21:41Z",
    "diffHunk": "@@ -116,23 +116,19 @@ class StandardScalerSuite extends SparkFunSuite with MLlibTestSparkContext\n     assertResult(standardScaler3.transform(df3))\n   }\n \n-  test(\"StandardScaler read/write\") {\n-    val t = new StandardScaler()\n-      .setInputCol(\"myInputCol\")\n-      .setOutputCol(\"myOutputCol\")\n-      .setWithStd(false)\n-      .setWithMean(true)\n-    testDefaultReadWrite(t)\n-  }\n-\n-  test(\"StandardScalerModel read/write\") {\n-    val oldModel = new feature.StandardScalerModel(\n-      Vectors.dense(1.0, 2.0), Vectors.dense(3.0, 4.0), false, true)\n-    val instance = new StandardScalerModel(\"myStandardScalerModel\", oldModel)\n-    val newInstance = testDefaultReadWrite(instance)\n-    assert(newInstance.std === instance.std)\n-    assert(newInstance.mean === instance.mean)\n-    assert(newInstance.getWithStd === instance.getWithStd)\n-    assert(newInstance.getWithMean === instance.getWithMean)\n+  test(\"read/write\") {\n+    def checkModelData(model1: StandardScalerModel, model2: StandardScalerModel): Unit = {\n+      assert(model1.mean === model2.mean)\n+      assert(model1.std === model2.std)\n+    }\n+    val allParams: Map[String, Any] = Map(\n+      \"inputCol\" -> \"features\",\n+      \"outputCol\" -> \"standardized_features\",\n+      \"withMean\" -> true,\n+      \"withStd\" -> true\n+    )\n+    val df = sqlContext.createDataFrame(data.zip(resWithBoth)).toDF(\"features\", \"expected\")\n+    val standardScaler = new StandardScaler()\n+    testEstimatorAndModelReadWrite(standardScaler, df, allParams, checkModelData)\n   }"
  }],
  "prId": 9839
}]