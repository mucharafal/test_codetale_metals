[{
  "comments": [{
    "author": {
      "login": "MLnick"
    },
    "body": "nit: `LogisticAggregator` -> `HingeAggregator`",
    "commit": "417308409eed00495f0516021549957b01af7ca6",
    "createdAt": "2017-07-27T11:23:21Z",
    "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.ml.optim.aggregator\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg.{BLAS, Vector, Vectors}\n+import org.apache.spark.ml.util.TestingUtils._\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+\n+class HingeAggregatorSuite extends SparkFunSuite with MLlibTestSparkContext {\n+\n+  import DifferentiableLossAggregatorSuite.getClassificationSummarizers\n+\n+  @transient var instances: Array[Instance] = _\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    instances = Array(\n+      Instance(0.0, 0.1, Vectors.dense(1.0, 2.0)),\n+      Instance(1.0, 0.5, Vectors.dense(1.5, 1.0)),\n+      Instance(0.0, 0.3, Vectors.dense(4.0, 0.5))\n+    )\n+  }\n+\n+   /** Get summary statistics for some data and create a new HingeAggregator. */\n+  private def getNewAggregator(\n+      instances: Array[Instance],\n+      coefficients: Vector,\n+      fitIntercept: Boolean): HingeAggregator = {\n+    val (featuresSummarizer, ySummarizer) =\n+      DifferentiableLossAggregatorSuite.getClassificationSummarizers(instances)\n+    val featuresStd = featuresSummarizer.variance.toArray.map(math.sqrt)\n+    val bcFeaturesStd = spark.sparkContext.broadcast(featuresStd)\n+    val bcCoefficients = spark.sparkContext.broadcast(coefficients)\n+    new HingeAggregator(bcFeaturesStd, fitIntercept)(bcCoefficients)\n+  }\n+\n+  test(\"aggregator add method input size\") {\n+    val coefArray = Array(1.0, 2.0)\n+    val interceptArray = Array(2.0)\n+    val agg = getNewAggregator(instances, Vectors.dense(coefArray ++ interceptArray),\n+      fitIntercept = true)\n+    withClue(\"HingeAggregator features dimension must match coefficients dimension\") {\n+      intercept[IllegalArgumentException] {\n+        agg.add(Instance(1.0, 1.0, Vectors.dense(2.0)))\n+      }\n+    }\n+  }\n+\n+  test(\"negative weight\") {\n+    val coefArray = Array(1.0, 2.0)\n+    val interceptArray = Array(2.0)\n+    val agg = getNewAggregator(instances, Vectors.dense(coefArray ++ interceptArray),\n+      fitIntercept = true)\n+    withClue(\"LogisticAggregator does not support negative instance weights\") {"
  }],
  "prId": 18315
}, {
  "comments": [{
    "author": {
      "login": "MLnick"
    },
    "body": "The other aggregator tests have one for \"zero standard deviation\". We should add one here too.",
    "commit": "417308409eed00495f0516021549957b01af7ca6",
    "createdAt": "2017-07-27T11:33:45Z",
    "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.ml.optim.aggregator\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg.{BLAS, Vector, Vectors}\n+import org.apache.spark.ml.util.TestingUtils._\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+\n+class HingeAggregatorSuite extends SparkFunSuite with MLlibTestSparkContext {\n+\n+  import DifferentiableLossAggregatorSuite.getClassificationSummarizers\n+\n+  @transient var instances: Array[Instance] = _\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    instances = Array(\n+      Instance(0.0, 0.1, Vectors.dense(1.0, 2.0)),\n+      Instance(1.0, 0.5, Vectors.dense(1.5, 1.0)),\n+      Instance(0.0, 0.3, Vectors.dense(4.0, 0.5))\n+    )\n+  }\n+\n+   /** Get summary statistics for some data and create a new HingeAggregator. */\n+  private def getNewAggregator(\n+      instances: Array[Instance],\n+      coefficients: Vector,\n+      fitIntercept: Boolean): HingeAggregator = {\n+    val (featuresSummarizer, ySummarizer) =\n+      DifferentiableLossAggregatorSuite.getClassificationSummarizers(instances)\n+    val featuresStd = featuresSummarizer.variance.toArray.map(math.sqrt)\n+    val bcFeaturesStd = spark.sparkContext.broadcast(featuresStd)\n+    val bcCoefficients = spark.sparkContext.broadcast(coefficients)\n+    new HingeAggregator(bcFeaturesStd, fitIntercept)(bcCoefficients)\n+  }\n+\n+  test(\"aggregator add method input size\") {\n+    val coefArray = Array(1.0, 2.0)\n+    val interceptArray = Array(2.0)\n+    val agg = getNewAggregator(instances, Vectors.dense(coefArray ++ interceptArray),\n+      fitIntercept = true)\n+    withClue(\"HingeAggregator features dimension must match coefficients dimension\") {\n+      intercept[IllegalArgumentException] {\n+        agg.add(Instance(1.0, 1.0, Vectors.dense(2.0)))\n+      }\n+    }\n+  }\n+\n+  test(\"negative weight\") {\n+    val coefArray = Array(1.0, 2.0)\n+    val interceptArray = Array(2.0)\n+    val agg = getNewAggregator(instances, Vectors.dense(coefArray ++ interceptArray),\n+      fitIntercept = true)\n+    withClue(\"LogisticAggregator does not support negative instance weights\") {\n+      intercept[IllegalArgumentException] {\n+        agg.add(Instance(1.0, -1.0, Vectors.dense(2.0, 1.0)))\n+      }\n+    }\n+  }\n+\n+  test(\"check sizes binomial\") {\n+    val rng = new scala.util.Random\n+    val numFeatures = instances.head.features.size\n+    val coefWithIntercept = Vectors.dense(Array.fill(numFeatures + 1)(rng.nextDouble))\n+    val coefWithoutIntercept = Vectors.dense(Array.fill(numFeatures)(rng.nextDouble))\n+    val aggIntercept = getNewAggregator(instances, coefWithIntercept, fitIntercept = true)\n+    val aggNoIntercept = getNewAggregator(instances, coefWithoutIntercept,\n+      fitIntercept = false)\n+    instances.foreach(aggIntercept.add)\n+    instances.foreach(aggNoIntercept.add)\n+\n+    assert(aggIntercept.gradient.size === numFeatures + 1)\n+    assert(aggNoIntercept.gradient.size === numFeatures)\n+  }\n+\n+\n+  test(\"check correctness binomial\") {\n+    val coefArray = Array(1.0, 2.0)\n+    val intercept = 1.0\n+    val numFeatures = instances.head.features.size\n+    val (featuresSummarizer, _) = getClassificationSummarizers(instances)\n+    val featuresStd = featuresSummarizer.variance.toArray.map(math.sqrt)\n+    val weightSum = instances.map(_.weight).sum\n+\n+    val agg = getNewAggregator(instances, Vectors.dense(coefArray ++ Array(intercept)),\n+      fitIntercept = true)\n+    instances.foreach(agg.add)\n+\n+    // compute the loss\n+    val stdCoef = coefArray.indices.map(i => coefArray(i) / featuresStd(i)).toArray\n+    val lossSum = instances.map { case Instance(l, w, f) =>\n+      val margin = BLAS.dot(Vectors.dense(stdCoef), f) + intercept\n+      val labelScaled = 2 * l - 1.0\n+      if (1.0 > labelScaled * margin) {\n+        (1.0 - labelScaled * margin) * w\n+      } else {\n+        0.0\n+      }\n+    }.sum\n+    val loss = lossSum / weightSum\n+\n+    // compute the gradients\n+    val gradientCoef = new Array[Double](numFeatures)\n+    var gradientIntercept = 0.0\n+    instances.foreach { case Instance(l, w, f) =>\n+      val margin = BLAS.dot(f, Vectors.dense(coefArray)) + intercept\n+      if (1.0 > (2 * l - 1.0) * margin) {\n+        gradientCoef.indices.foreach { i =>\n+          gradientCoef(i) += f(i) * -(2 * l - 1.0) * w / featuresStd(i)\n+        }\n+        gradientIntercept += -(2 * l - 1.0) * w\n+      }\n+    }\n+    val gradient = Vectors.dense((gradientCoef ++ Array(gradientIntercept)).map(_ / weightSum))\n+\n+    assert(loss ~== agg.loss relTol 0.01)\n+    assert(gradient ~== agg.gradient relTol 0.01)\n+  }\n+",
    "line": 146
  }, {
    "author": {
      "login": "hhbyyh"
    },
    "body": "Sure. Added.",
    "commit": "417308409eed00495f0516021549957b01af7ca6",
    "createdAt": "2017-08-20T02:43:20Z",
    "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.ml.optim.aggregator\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg.{BLAS, Vector, Vectors}\n+import org.apache.spark.ml.util.TestingUtils._\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+\n+class HingeAggregatorSuite extends SparkFunSuite with MLlibTestSparkContext {\n+\n+  import DifferentiableLossAggregatorSuite.getClassificationSummarizers\n+\n+  @transient var instances: Array[Instance] = _\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    instances = Array(\n+      Instance(0.0, 0.1, Vectors.dense(1.0, 2.0)),\n+      Instance(1.0, 0.5, Vectors.dense(1.5, 1.0)),\n+      Instance(0.0, 0.3, Vectors.dense(4.0, 0.5))\n+    )\n+  }\n+\n+   /** Get summary statistics for some data and create a new HingeAggregator. */\n+  private def getNewAggregator(\n+      instances: Array[Instance],\n+      coefficients: Vector,\n+      fitIntercept: Boolean): HingeAggregator = {\n+    val (featuresSummarizer, ySummarizer) =\n+      DifferentiableLossAggregatorSuite.getClassificationSummarizers(instances)\n+    val featuresStd = featuresSummarizer.variance.toArray.map(math.sqrt)\n+    val bcFeaturesStd = spark.sparkContext.broadcast(featuresStd)\n+    val bcCoefficients = spark.sparkContext.broadcast(coefficients)\n+    new HingeAggregator(bcFeaturesStd, fitIntercept)(bcCoefficients)\n+  }\n+\n+  test(\"aggregator add method input size\") {\n+    val coefArray = Array(1.0, 2.0)\n+    val interceptArray = Array(2.0)\n+    val agg = getNewAggregator(instances, Vectors.dense(coefArray ++ interceptArray),\n+      fitIntercept = true)\n+    withClue(\"HingeAggregator features dimension must match coefficients dimension\") {\n+      intercept[IllegalArgumentException] {\n+        agg.add(Instance(1.0, 1.0, Vectors.dense(2.0)))\n+      }\n+    }\n+  }\n+\n+  test(\"negative weight\") {\n+    val coefArray = Array(1.0, 2.0)\n+    val interceptArray = Array(2.0)\n+    val agg = getNewAggregator(instances, Vectors.dense(coefArray ++ interceptArray),\n+      fitIntercept = true)\n+    withClue(\"LogisticAggregator does not support negative instance weights\") {\n+      intercept[IllegalArgumentException] {\n+        agg.add(Instance(1.0, -1.0, Vectors.dense(2.0, 1.0)))\n+      }\n+    }\n+  }\n+\n+  test(\"check sizes binomial\") {\n+    val rng = new scala.util.Random\n+    val numFeatures = instances.head.features.size\n+    val coefWithIntercept = Vectors.dense(Array.fill(numFeatures + 1)(rng.nextDouble))\n+    val coefWithoutIntercept = Vectors.dense(Array.fill(numFeatures)(rng.nextDouble))\n+    val aggIntercept = getNewAggregator(instances, coefWithIntercept, fitIntercept = true)\n+    val aggNoIntercept = getNewAggregator(instances, coefWithoutIntercept,\n+      fitIntercept = false)\n+    instances.foreach(aggIntercept.add)\n+    instances.foreach(aggNoIntercept.add)\n+\n+    assert(aggIntercept.gradient.size === numFeatures + 1)\n+    assert(aggNoIntercept.gradient.size === numFeatures)\n+  }\n+\n+\n+  test(\"check correctness binomial\") {\n+    val coefArray = Array(1.0, 2.0)\n+    val intercept = 1.0\n+    val numFeatures = instances.head.features.size\n+    val (featuresSummarizer, _) = getClassificationSummarizers(instances)\n+    val featuresStd = featuresSummarizer.variance.toArray.map(math.sqrt)\n+    val weightSum = instances.map(_.weight).sum\n+\n+    val agg = getNewAggregator(instances, Vectors.dense(coefArray ++ Array(intercept)),\n+      fitIntercept = true)\n+    instances.foreach(agg.add)\n+\n+    // compute the loss\n+    val stdCoef = coefArray.indices.map(i => coefArray(i) / featuresStd(i)).toArray\n+    val lossSum = instances.map { case Instance(l, w, f) =>\n+      val margin = BLAS.dot(Vectors.dense(stdCoef), f) + intercept\n+      val labelScaled = 2 * l - 1.0\n+      if (1.0 > labelScaled * margin) {\n+        (1.0 - labelScaled * margin) * w\n+      } else {\n+        0.0\n+      }\n+    }.sum\n+    val loss = lossSum / weightSum\n+\n+    // compute the gradients\n+    val gradientCoef = new Array[Double](numFeatures)\n+    var gradientIntercept = 0.0\n+    instances.foreach { case Instance(l, w, f) =>\n+      val margin = BLAS.dot(f, Vectors.dense(coefArray)) + intercept\n+      if (1.0 > (2 * l - 1.0) * margin) {\n+        gradientCoef.indices.foreach { i =>\n+          gradientCoef(i) += f(i) * -(2 * l - 1.0) * w / featuresStd(i)\n+        }\n+        gradientIntercept += -(2 * l - 1.0) * w\n+      }\n+    }\n+    val gradient = Vectors.dense((gradientCoef ++ Array(gradientIntercept)).map(_ / weightSum))\n+\n+    assert(loss ~== agg.loss relTol 0.01)\n+    assert(gradient ~== agg.gradient relTol 0.01)\n+  }\n+",
    "line": 146
  }],
  "prId": 18315
}, {
  "comments": [{
    "author": {
      "login": "yanboliang"
    },
    "body": "Remove ```binomial```?",
    "commit": "417308409eed00495f0516021549957b01af7ca6",
    "createdAt": "2017-08-17T09:59:31Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.ml.optim.aggregator\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg.{BLAS, Vector, Vectors}\n+import org.apache.spark.ml.util.TestingUtils._\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+\n+class HingeAggregatorSuite extends SparkFunSuite with MLlibTestSparkContext {\n+\n+  import DifferentiableLossAggregatorSuite.getClassificationSummarizers\n+\n+  @transient var instances: Array[Instance] = _\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    instances = Array(\n+      Instance(0.0, 0.1, Vectors.dense(1.0, 2.0)),\n+      Instance(1.0, 0.5, Vectors.dense(1.5, 1.0)),\n+      Instance(0.0, 0.3, Vectors.dense(4.0, 0.5))\n+    )\n+  }\n+\n+   /** Get summary statistics for some data and create a new HingeAggregator. */\n+  private def getNewAggregator(\n+      instances: Array[Instance],\n+      coefficients: Vector,\n+      fitIntercept: Boolean): HingeAggregator = {\n+    val (featuresSummarizer, ySummarizer) =\n+      DifferentiableLossAggregatorSuite.getClassificationSummarizers(instances)\n+    val featuresStd = featuresSummarizer.variance.toArray.map(math.sqrt)\n+    val bcFeaturesStd = spark.sparkContext.broadcast(featuresStd)\n+    val bcCoefficients = spark.sparkContext.broadcast(coefficients)\n+    new HingeAggregator(bcFeaturesStd, fitIntercept)(bcCoefficients)\n+  }\n+\n+  test(\"aggregator add method input size\") {\n+    val coefArray = Array(1.0, 2.0)\n+    val interceptArray = Array(2.0)\n+    val agg = getNewAggregator(instances, Vectors.dense(coefArray ++ interceptArray),\n+      fitIntercept = true)\n+    withClue(\"HingeAggregator features dimension must match coefficients dimension\") {\n+      intercept[IllegalArgumentException] {\n+        agg.add(Instance(1.0, 1.0, Vectors.dense(2.0)))\n+      }\n+    }\n+  }\n+\n+  test(\"negative weight\") {\n+    val coefArray = Array(1.0, 2.0)\n+    val interceptArray = Array(2.0)\n+    val agg = getNewAggregator(instances, Vectors.dense(coefArray ++ interceptArray),\n+      fitIntercept = true)\n+    withClue(\"HingeAggregator does not support negative instance weights\") {\n+      intercept[IllegalArgumentException] {\n+        agg.add(Instance(1.0, -1.0, Vectors.dense(2.0, 1.0)))\n+      }\n+    }\n+  }\n+\n+  test(\"check sizes binomial\") {"
  }],
  "prId": 18315
}, {
  "comments": [{
    "author": {
      "login": "yanboliang"
    },
    "body": "Ditto.",
    "commit": "417308409eed00495f0516021549957b01af7ca6",
    "createdAt": "2017-08-17T10:00:20Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.ml.optim.aggregator\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg.{BLAS, Vector, Vectors}\n+import org.apache.spark.ml.util.TestingUtils._\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+\n+class HingeAggregatorSuite extends SparkFunSuite with MLlibTestSparkContext {\n+\n+  import DifferentiableLossAggregatorSuite.getClassificationSummarizers\n+\n+  @transient var instances: Array[Instance] = _\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    instances = Array(\n+      Instance(0.0, 0.1, Vectors.dense(1.0, 2.0)),\n+      Instance(1.0, 0.5, Vectors.dense(1.5, 1.0)),\n+      Instance(0.0, 0.3, Vectors.dense(4.0, 0.5))\n+    )\n+  }\n+\n+   /** Get summary statistics for some data and create a new HingeAggregator. */\n+  private def getNewAggregator(\n+      instances: Array[Instance],\n+      coefficients: Vector,\n+      fitIntercept: Boolean): HingeAggregator = {\n+    val (featuresSummarizer, ySummarizer) =\n+      DifferentiableLossAggregatorSuite.getClassificationSummarizers(instances)\n+    val featuresStd = featuresSummarizer.variance.toArray.map(math.sqrt)\n+    val bcFeaturesStd = spark.sparkContext.broadcast(featuresStd)\n+    val bcCoefficients = spark.sparkContext.broadcast(coefficients)\n+    new HingeAggregator(bcFeaturesStd, fitIntercept)(bcCoefficients)\n+  }\n+\n+  test(\"aggregator add method input size\") {\n+    val coefArray = Array(1.0, 2.0)\n+    val interceptArray = Array(2.0)\n+    val agg = getNewAggregator(instances, Vectors.dense(coefArray ++ interceptArray),\n+      fitIntercept = true)\n+    withClue(\"HingeAggregator features dimension must match coefficients dimension\") {\n+      intercept[IllegalArgumentException] {\n+        agg.add(Instance(1.0, 1.0, Vectors.dense(2.0)))\n+      }\n+    }\n+  }\n+\n+  test(\"negative weight\") {\n+    val coefArray = Array(1.0, 2.0)\n+    val interceptArray = Array(2.0)\n+    val agg = getNewAggregator(instances, Vectors.dense(coefArray ++ interceptArray),\n+      fitIntercept = true)\n+    withClue(\"HingeAggregator does not support negative instance weights\") {\n+      intercept[IllegalArgumentException] {\n+        agg.add(Instance(1.0, -1.0, Vectors.dense(2.0, 1.0)))\n+      }\n+    }\n+  }\n+\n+  test(\"check sizes binomial\") {\n+    val rng = new scala.util.Random\n+    val numFeatures = instances.head.features.size\n+    val coefWithIntercept = Vectors.dense(Array.fill(numFeatures + 1)(rng.nextDouble))\n+    val coefWithoutIntercept = Vectors.dense(Array.fill(numFeatures)(rng.nextDouble))\n+    val aggIntercept = getNewAggregator(instances, coefWithIntercept, fitIntercept = true)\n+    val aggNoIntercept = getNewAggregator(instances, coefWithoutIntercept,\n+      fitIntercept = false)\n+    instances.foreach(aggIntercept.add)\n+    instances.foreach(aggNoIntercept.add)\n+\n+    assert(aggIntercept.gradient.size === numFeatures + 1)\n+    assert(aggNoIntercept.gradient.size === numFeatures)\n+  }\n+\n+\n+  test(\"check correctness binomial\") {"
  }],
  "prId": 18315
}, {
  "comments": [{
    "author": {
      "login": "yanboliang"
    },
    "body": "What about move the dataset to the beginning of this class?",
    "commit": "417308409eed00495f0516021549957b01af7ca6",
    "createdAt": "2017-08-17T10:02:08Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.ml.optim.aggregator\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg.{BLAS, Vector, Vectors}\n+import org.apache.spark.ml.util.TestingUtils._\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+\n+class HingeAggregatorSuite extends SparkFunSuite with MLlibTestSparkContext {\n+\n+  import DifferentiableLossAggregatorSuite.getClassificationSummarizers\n+\n+  @transient var instances: Array[Instance] = _\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    instances = Array(\n+      Instance(0.0, 0.1, Vectors.dense(1.0, 2.0)),\n+      Instance(1.0, 0.5, Vectors.dense(1.5, 1.0)),\n+      Instance(0.0, 0.3, Vectors.dense(4.0, 0.5))\n+    )\n+  }\n+\n+   /** Get summary statistics for some data and create a new HingeAggregator. */\n+  private def getNewAggregator(\n+      instances: Array[Instance],\n+      coefficients: Vector,\n+      fitIntercept: Boolean): HingeAggregator = {\n+    val (featuresSummarizer, ySummarizer) =\n+      DifferentiableLossAggregatorSuite.getClassificationSummarizers(instances)\n+    val featuresStd = featuresSummarizer.variance.toArray.map(math.sqrt)\n+    val bcFeaturesStd = spark.sparkContext.broadcast(featuresStd)\n+    val bcCoefficients = spark.sparkContext.broadcast(coefficients)\n+    new HingeAggregator(bcFeaturesStd, fitIntercept)(bcCoefficients)\n+  }\n+\n+  test(\"aggregator add method input size\") {\n+    val coefArray = Array(1.0, 2.0)\n+    val interceptArray = Array(2.0)\n+    val agg = getNewAggregator(instances, Vectors.dense(coefArray ++ interceptArray),\n+      fitIntercept = true)\n+    withClue(\"HingeAggregator features dimension must match coefficients dimension\") {\n+      intercept[IllegalArgumentException] {\n+        agg.add(Instance(1.0, 1.0, Vectors.dense(2.0)))\n+      }\n+    }\n+  }\n+\n+  test(\"negative weight\") {\n+    val coefArray = Array(1.0, 2.0)\n+    val interceptArray = Array(2.0)\n+    val agg = getNewAggregator(instances, Vectors.dense(coefArray ++ interceptArray),\n+      fitIntercept = true)\n+    withClue(\"HingeAggregator does not support negative instance weights\") {\n+      intercept[IllegalArgumentException] {\n+        agg.add(Instance(1.0, -1.0, Vectors.dense(2.0, 1.0)))\n+      }\n+    }\n+  }\n+\n+  test(\"check sizes binomial\") {\n+    val rng = new scala.util.Random\n+    val numFeatures = instances.head.features.size\n+    val coefWithIntercept = Vectors.dense(Array.fill(numFeatures + 1)(rng.nextDouble))\n+    val coefWithoutIntercept = Vectors.dense(Array.fill(numFeatures)(rng.nextDouble))\n+    val aggIntercept = getNewAggregator(instances, coefWithIntercept, fitIntercept = true)\n+    val aggNoIntercept = getNewAggregator(instances, coefWithoutIntercept,\n+      fitIntercept = false)\n+    instances.foreach(aggIntercept.add)\n+    instances.foreach(aggNoIntercept.add)\n+\n+    assert(aggIntercept.gradient.size === numFeatures + 1)\n+    assert(aggNoIntercept.gradient.size === numFeatures)\n+  }\n+\n+\n+  test(\"check correctness binomial\") {\n+    val coefArray = Array(1.0, 2.0)\n+    val intercept = 1.0\n+    val numFeatures = instances.head.features.size\n+    val (featuresSummarizer, _) = getClassificationSummarizers(instances)\n+    val featuresStd = featuresSummarizer.variance.toArray.map(math.sqrt)\n+    val weightSum = instances.map(_.weight).sum\n+\n+    val agg = getNewAggregator(instances, Vectors.dense(coefArray ++ Array(intercept)),\n+      fitIntercept = true)\n+    instances.foreach(agg.add)\n+\n+    // compute the loss\n+    val stdCoef = coefArray.indices.map(i => coefArray(i) / featuresStd(i)).toArray\n+    val lossSum = instances.map { case Instance(l, w, f) =>\n+      val margin = BLAS.dot(Vectors.dense(stdCoef), f) + intercept\n+      val labelScaled = 2 * l - 1.0\n+      if (1.0 > labelScaled * margin) {\n+        (1.0 - labelScaled * margin) * w\n+      } else {\n+        0.0\n+      }\n+    }.sum\n+    val loss = lossSum / weightSum\n+\n+    // compute the gradients\n+    val gradientCoef = new Array[Double](numFeatures)\n+    var gradientIntercept = 0.0\n+    instances.foreach { case Instance(l, w, f) =>\n+      val margin = BLAS.dot(f, Vectors.dense(coefArray)) + intercept\n+      if (1.0 > (2 * l - 1.0) * margin) {\n+        gradientCoef.indices.foreach { i =>\n+          gradientCoef(i) += f(i) * -(2 * l - 1.0) * w / featuresStd(i)\n+        }\n+        gradientIntercept += -(2 * l - 1.0) * w\n+      }\n+    }\n+    val gradient = Vectors.dense((gradientCoef ++ Array(gradientIntercept)).map(_ / weightSum))\n+\n+    assert(loss ~== agg.loss relTol 0.01)\n+    assert(gradient ~== agg.gradient relTol 0.01)\n+  }\n+\n+  test(\"check with zero standard deviation\") {\n+    val instancesConstantFeature = Array("
  }, {
    "author": {
      "login": "hhbyyh"
    },
    "body": "OK",
    "commit": "417308409eed00495f0516021549957b01af7ca6",
    "createdAt": "2017-08-20T02:43:32Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.ml.optim.aggregator\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg.{BLAS, Vector, Vectors}\n+import org.apache.spark.ml.util.TestingUtils._\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+\n+class HingeAggregatorSuite extends SparkFunSuite with MLlibTestSparkContext {\n+\n+  import DifferentiableLossAggregatorSuite.getClassificationSummarizers\n+\n+  @transient var instances: Array[Instance] = _\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    instances = Array(\n+      Instance(0.0, 0.1, Vectors.dense(1.0, 2.0)),\n+      Instance(1.0, 0.5, Vectors.dense(1.5, 1.0)),\n+      Instance(0.0, 0.3, Vectors.dense(4.0, 0.5))\n+    )\n+  }\n+\n+   /** Get summary statistics for some data and create a new HingeAggregator. */\n+  private def getNewAggregator(\n+      instances: Array[Instance],\n+      coefficients: Vector,\n+      fitIntercept: Boolean): HingeAggregator = {\n+    val (featuresSummarizer, ySummarizer) =\n+      DifferentiableLossAggregatorSuite.getClassificationSummarizers(instances)\n+    val featuresStd = featuresSummarizer.variance.toArray.map(math.sqrt)\n+    val bcFeaturesStd = spark.sparkContext.broadcast(featuresStd)\n+    val bcCoefficients = spark.sparkContext.broadcast(coefficients)\n+    new HingeAggregator(bcFeaturesStd, fitIntercept)(bcCoefficients)\n+  }\n+\n+  test(\"aggregator add method input size\") {\n+    val coefArray = Array(1.0, 2.0)\n+    val interceptArray = Array(2.0)\n+    val agg = getNewAggregator(instances, Vectors.dense(coefArray ++ interceptArray),\n+      fitIntercept = true)\n+    withClue(\"HingeAggregator features dimension must match coefficients dimension\") {\n+      intercept[IllegalArgumentException] {\n+        agg.add(Instance(1.0, 1.0, Vectors.dense(2.0)))\n+      }\n+    }\n+  }\n+\n+  test(\"negative weight\") {\n+    val coefArray = Array(1.0, 2.0)\n+    val interceptArray = Array(2.0)\n+    val agg = getNewAggregator(instances, Vectors.dense(coefArray ++ interceptArray),\n+      fitIntercept = true)\n+    withClue(\"HingeAggregator does not support negative instance weights\") {\n+      intercept[IllegalArgumentException] {\n+        agg.add(Instance(1.0, -1.0, Vectors.dense(2.0, 1.0)))\n+      }\n+    }\n+  }\n+\n+  test(\"check sizes binomial\") {\n+    val rng = new scala.util.Random\n+    val numFeatures = instances.head.features.size\n+    val coefWithIntercept = Vectors.dense(Array.fill(numFeatures + 1)(rng.nextDouble))\n+    val coefWithoutIntercept = Vectors.dense(Array.fill(numFeatures)(rng.nextDouble))\n+    val aggIntercept = getNewAggregator(instances, coefWithIntercept, fitIntercept = true)\n+    val aggNoIntercept = getNewAggregator(instances, coefWithoutIntercept,\n+      fitIntercept = false)\n+    instances.foreach(aggIntercept.add)\n+    instances.foreach(aggNoIntercept.add)\n+\n+    assert(aggIntercept.gradient.size === numFeatures + 1)\n+    assert(aggNoIntercept.gradient.size === numFeatures)\n+  }\n+\n+\n+  test(\"check correctness binomial\") {\n+    val coefArray = Array(1.0, 2.0)\n+    val intercept = 1.0\n+    val numFeatures = instances.head.features.size\n+    val (featuresSummarizer, _) = getClassificationSummarizers(instances)\n+    val featuresStd = featuresSummarizer.variance.toArray.map(math.sqrt)\n+    val weightSum = instances.map(_.weight).sum\n+\n+    val agg = getNewAggregator(instances, Vectors.dense(coefArray ++ Array(intercept)),\n+      fitIntercept = true)\n+    instances.foreach(agg.add)\n+\n+    // compute the loss\n+    val stdCoef = coefArray.indices.map(i => coefArray(i) / featuresStd(i)).toArray\n+    val lossSum = instances.map { case Instance(l, w, f) =>\n+      val margin = BLAS.dot(Vectors.dense(stdCoef), f) + intercept\n+      val labelScaled = 2 * l - 1.0\n+      if (1.0 > labelScaled * margin) {\n+        (1.0 - labelScaled * margin) * w\n+      } else {\n+        0.0\n+      }\n+    }.sum\n+    val loss = lossSum / weightSum\n+\n+    // compute the gradients\n+    val gradientCoef = new Array[Double](numFeatures)\n+    var gradientIntercept = 0.0\n+    instances.foreach { case Instance(l, w, f) =>\n+      val margin = BLAS.dot(f, Vectors.dense(coefArray)) + intercept\n+      if (1.0 > (2 * l - 1.0) * margin) {\n+        gradientCoef.indices.foreach { i =>\n+          gradientCoef(i) += f(i) * -(2 * l - 1.0) * w / featuresStd(i)\n+        }\n+        gradientIntercept += -(2 * l - 1.0) * w\n+      }\n+    }\n+    val gradient = Vectors.dense((gradientCoef ++ Array(gradientIntercept)).map(_ / weightSum))\n+\n+    assert(loss ~== agg.loss relTol 0.01)\n+    assert(gradient ~== agg.gradient relTol 0.01)\n+  }\n+\n+  test(\"check with zero standard deviation\") {\n+    val instancesConstantFeature = Array("
  }],
  "prId": 18315
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "As we found out in https://github.com/apache/spark/pull/18896, this test is not thorough enough. We should check all elements of the gradient for correctness.",
    "commit": "417308409eed00495f0516021549957b01af7ca6",
    "createdAt": "2017-08-21T16:11:40Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.ml.optim.aggregator\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg.{BLAS, Vector, Vectors}\n+import org.apache.spark.ml.util.TestingUtils._\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+\n+class HingeAggregatorSuite extends SparkFunSuite with MLlibTestSparkContext {\n+\n+  import DifferentiableLossAggregatorSuite.getClassificationSummarizers\n+\n+  @transient var instances: Array[Instance] = _\n+  @transient var instancesConstantFeature: Array[Instance] = _\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    instances = Array(\n+      Instance(0.0, 0.1, Vectors.dense(1.0, 2.0)),\n+      Instance(1.0, 0.5, Vectors.dense(1.5, 1.0)),\n+      Instance(0.0, 0.3, Vectors.dense(4.0, 0.5))\n+    )\n+    instancesConstantFeature = Array(\n+      Instance(0.0, 0.1, Vectors.dense(1.0, 2.0)),\n+      Instance(1.0, 0.5, Vectors.dense(1.0, 1.0)),\n+      Instance(1.0, 0.3, Vectors.dense(1.0, 0.5)))\n+  }\n+\n+   /** Get summary statistics for some data and create a new HingeAggregator. */\n+  private def getNewAggregator(\n+      instances: Array[Instance],\n+      coefficients: Vector,\n+      fitIntercept: Boolean): HingeAggregator = {\n+    val (featuresSummarizer, ySummarizer) =\n+      DifferentiableLossAggregatorSuite.getClassificationSummarizers(instances)\n+    val featuresStd = featuresSummarizer.variance.toArray.map(math.sqrt)\n+    val bcFeaturesStd = spark.sparkContext.broadcast(featuresStd)\n+    val bcCoefficients = spark.sparkContext.broadcast(coefficients)\n+    new HingeAggregator(bcFeaturesStd, fitIntercept)(bcCoefficients)\n+  }\n+\n+  test(\"aggregator add method input size\") {\n+    val coefArray = Array(1.0, 2.0)\n+    val interceptArray = Array(2.0)\n+    val agg = getNewAggregator(instances, Vectors.dense(coefArray ++ interceptArray),\n+      fitIntercept = true)\n+    withClue(\"HingeAggregator features dimension must match coefficients dimension\") {\n+      intercept[IllegalArgumentException] {\n+        agg.add(Instance(1.0, 1.0, Vectors.dense(2.0)))\n+      }\n+    }\n+  }\n+\n+  test(\"negative weight\") {\n+    val coefArray = Array(1.0, 2.0)\n+    val interceptArray = Array(2.0)\n+    val agg = getNewAggregator(instances, Vectors.dense(coefArray ++ interceptArray),\n+      fitIntercept = true)\n+    withClue(\"HingeAggregator does not support negative instance weights\") {\n+      intercept[IllegalArgumentException] {\n+        agg.add(Instance(1.0, -1.0, Vectors.dense(2.0, 1.0)))\n+      }\n+    }\n+  }\n+\n+  test(\"check sizes\") {\n+    val rng = new scala.util.Random\n+    val numFeatures = instances.head.features.size\n+    val coefWithIntercept = Vectors.dense(Array.fill(numFeatures + 1)(rng.nextDouble))\n+    val coefWithoutIntercept = Vectors.dense(Array.fill(numFeatures)(rng.nextDouble))\n+    val aggIntercept = getNewAggregator(instances, coefWithIntercept, fitIntercept = true)\n+    val aggNoIntercept = getNewAggregator(instances, coefWithoutIntercept,\n+      fitIntercept = false)\n+    instances.foreach(aggIntercept.add)\n+    instances.foreach(aggNoIntercept.add)\n+\n+    assert(aggIntercept.gradient.size === numFeatures + 1)\n+    assert(aggNoIntercept.gradient.size === numFeatures)\n+  }\n+\n+  test(\"check correctness\") {\n+    val coefArray = Array(1.0, 2.0)\n+    val intercept = 1.0\n+    val numFeatures = instances.head.features.size\n+    val (featuresSummarizer, _) = getClassificationSummarizers(instances)\n+    val featuresStd = featuresSummarizer.variance.toArray.map(math.sqrt)\n+    val weightSum = instances.map(_.weight).sum\n+\n+    val agg = getNewAggregator(instances, Vectors.dense(coefArray ++ Array(intercept)),\n+      fitIntercept = true)\n+    instances.foreach(agg.add)\n+\n+    // compute the loss\n+    val stdCoef = coefArray.indices.map(i => coefArray(i) / featuresStd(i)).toArray\n+    val lossSum = instances.map { case Instance(l, w, f) =>\n+      val margin = BLAS.dot(Vectors.dense(stdCoef), f) + intercept\n+      val labelScaled = 2 * l - 1.0\n+      if (1.0 > labelScaled * margin) {\n+        (1.0 - labelScaled * margin) * w\n+      } else {\n+        0.0\n+      }\n+    }.sum\n+    val loss = lossSum / weightSum\n+\n+    // compute the gradients\n+    val gradientCoef = new Array[Double](numFeatures)\n+    var gradientIntercept = 0.0\n+    instances.foreach { case Instance(l, w, f) =>\n+      val margin = BLAS.dot(f, Vectors.dense(coefArray)) + intercept\n+      if (1.0 > (2 * l - 1.0) * margin) {\n+        gradientCoef.indices.foreach { i =>\n+          gradientCoef(i) += f(i) * -(2 * l - 1.0) * w / featuresStd(i)\n+        }\n+        gradientIntercept += -(2 * l - 1.0) * w\n+      }\n+    }\n+    val gradient = Vectors.dense((gradientCoef ++ Array(gradientIntercept)).map(_ / weightSum))\n+\n+    assert(loss ~== agg.loss relTol 0.01)\n+    assert(gradient ~== agg.gradient relTol 0.01)\n+  }\n+\n+  test(\"check with zero standard deviation\") {",
    "line": 147
  }],
  "prId": 18315
}, {
  "comments": [{
    "author": {
      "login": "yanboliang"
    },
    "body": "This is different from ```LogisticRegression``` which supports multi-classification, so ```interceptArray``` should be renamed to ```intercept```?",
    "commit": "417308409eed00495f0516021549957b01af7ca6",
    "createdAt": "2017-08-23T10:51:02Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.ml.optim.aggregator\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg.{BLAS, Vector, Vectors}\n+import org.apache.spark.ml.util.TestingUtils._\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+\n+class HingeAggregatorSuite extends SparkFunSuite with MLlibTestSparkContext {\n+\n+  import DifferentiableLossAggregatorSuite.getClassificationSummarizers\n+\n+  @transient var instances: Array[Instance] = _\n+  @transient var instancesConstantFeature: Array[Instance] = _\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    instances = Array(\n+      Instance(0.0, 0.1, Vectors.dense(1.0, 2.0)),\n+      Instance(1.0, 0.5, Vectors.dense(1.5, 1.0)),\n+      Instance(0.0, 0.3, Vectors.dense(4.0, 0.5))\n+    )\n+    instancesConstantFeature = Array(\n+      Instance(0.0, 0.1, Vectors.dense(1.0, 2.0)),\n+      Instance(1.0, 0.5, Vectors.dense(1.0, 1.0)),\n+      Instance(1.0, 0.3, Vectors.dense(1.0, 0.5)))\n+  }\n+\n+   /** Get summary statistics for some data and create a new HingeAggregator. */\n+  private def getNewAggregator(\n+      instances: Array[Instance],\n+      coefficients: Vector,\n+      fitIntercept: Boolean): HingeAggregator = {\n+    val (featuresSummarizer, ySummarizer) =\n+      DifferentiableLossAggregatorSuite.getClassificationSummarizers(instances)\n+    val featuresStd = featuresSummarizer.variance.toArray.map(math.sqrt)\n+    val bcFeaturesStd = spark.sparkContext.broadcast(featuresStd)\n+    val bcCoefficients = spark.sparkContext.broadcast(coefficients)\n+    new HingeAggregator(bcFeaturesStd, fitIntercept)(bcCoefficients)\n+  }\n+\n+  test(\"aggregator add method input size\") {\n+    val coefArray = Array(1.0, 2.0)\n+    val interceptArray = Array(2.0)",
    "line": 67
  }, {
    "author": {
      "login": "hhbyyh"
    },
    "body": "Changing it to intercept will cause conflict with `intercept[IllegalArgumentException] ` in the same test. Do you mind if we keep using interceptArray ?",
    "commit": "417308409eed00495f0516021549957b01af7ca6",
    "createdAt": "2017-08-23T20:14:40Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.ml.optim.aggregator\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg.{BLAS, Vector, Vectors}\n+import org.apache.spark.ml.util.TestingUtils._\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+\n+class HingeAggregatorSuite extends SparkFunSuite with MLlibTestSparkContext {\n+\n+  import DifferentiableLossAggregatorSuite.getClassificationSummarizers\n+\n+  @transient var instances: Array[Instance] = _\n+  @transient var instancesConstantFeature: Array[Instance] = _\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    instances = Array(\n+      Instance(0.0, 0.1, Vectors.dense(1.0, 2.0)),\n+      Instance(1.0, 0.5, Vectors.dense(1.5, 1.0)),\n+      Instance(0.0, 0.3, Vectors.dense(4.0, 0.5))\n+    )\n+    instancesConstantFeature = Array(\n+      Instance(0.0, 0.1, Vectors.dense(1.0, 2.0)),\n+      Instance(1.0, 0.5, Vectors.dense(1.0, 1.0)),\n+      Instance(1.0, 0.3, Vectors.dense(1.0, 0.5)))\n+  }\n+\n+   /** Get summary statistics for some data and create a new HingeAggregator. */\n+  private def getNewAggregator(\n+      instances: Array[Instance],\n+      coefficients: Vector,\n+      fitIntercept: Boolean): HingeAggregator = {\n+    val (featuresSummarizer, ySummarizer) =\n+      DifferentiableLossAggregatorSuite.getClassificationSummarizers(instances)\n+    val featuresStd = featuresSummarizer.variance.toArray.map(math.sqrt)\n+    val bcFeaturesStd = spark.sparkContext.broadcast(featuresStd)\n+    val bcCoefficients = spark.sparkContext.broadcast(coefficients)\n+    new HingeAggregator(bcFeaturesStd, fitIntercept)(bcCoefficients)\n+  }\n+\n+  test(\"aggregator add method input size\") {\n+    val coefArray = Array(1.0, 2.0)\n+    val interceptArray = Array(2.0)",
    "line": 67
  }, {
    "author": {
      "login": "yanboliang"
    },
    "body": "Okay.",
    "commit": "417308409eed00495f0516021549957b01af7ca6",
    "createdAt": "2017-08-25T02:20:50Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.ml.optim.aggregator\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg.{BLAS, Vector, Vectors}\n+import org.apache.spark.ml.util.TestingUtils._\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+\n+class HingeAggregatorSuite extends SparkFunSuite with MLlibTestSparkContext {\n+\n+  import DifferentiableLossAggregatorSuite.getClassificationSummarizers\n+\n+  @transient var instances: Array[Instance] = _\n+  @transient var instancesConstantFeature: Array[Instance] = _\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    instances = Array(\n+      Instance(0.0, 0.1, Vectors.dense(1.0, 2.0)),\n+      Instance(1.0, 0.5, Vectors.dense(1.5, 1.0)),\n+      Instance(0.0, 0.3, Vectors.dense(4.0, 0.5))\n+    )\n+    instancesConstantFeature = Array(\n+      Instance(0.0, 0.1, Vectors.dense(1.0, 2.0)),\n+      Instance(1.0, 0.5, Vectors.dense(1.0, 1.0)),\n+      Instance(1.0, 0.3, Vectors.dense(1.0, 0.5)))\n+  }\n+\n+   /** Get summary statistics for some data and create a new HingeAggregator. */\n+  private def getNewAggregator(\n+      instances: Array[Instance],\n+      coefficients: Vector,\n+      fitIntercept: Boolean): HingeAggregator = {\n+    val (featuresSummarizer, ySummarizer) =\n+      DifferentiableLossAggregatorSuite.getClassificationSummarizers(instances)\n+    val featuresStd = featuresSummarizer.variance.toArray.map(math.sqrt)\n+    val bcFeaturesStd = spark.sparkContext.broadcast(featuresStd)\n+    val bcCoefficients = spark.sparkContext.broadcast(coefficients)\n+    new HingeAggregator(bcFeaturesStd, fitIntercept)(bcCoefficients)\n+  }\n+\n+  test(\"aggregator add method input size\") {\n+    val coefArray = Array(1.0, 2.0)\n+    val interceptArray = Array(2.0)",
    "line": 67
  }],
  "prId": 18315
}, {
  "comments": [{
    "author": {
      "login": "yanboliang"
    },
    "body": "ditto",
    "commit": "417308409eed00495f0516021549957b01af7ca6",
    "createdAt": "2017-08-23T10:51:25Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.ml.optim.aggregator\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg.{BLAS, Vector, Vectors}\n+import org.apache.spark.ml.util.TestingUtils._\n+import org.apache.spark.mllib.util.MLlibTestSparkContext\n+\n+class HingeAggregatorSuite extends SparkFunSuite with MLlibTestSparkContext {\n+\n+  import DifferentiableLossAggregatorSuite.getClassificationSummarizers\n+\n+  @transient var instances: Array[Instance] = _\n+  @transient var instancesConstantFeature: Array[Instance] = _\n+\n+  override def beforeAll(): Unit = {\n+    super.beforeAll()\n+    instances = Array(\n+      Instance(0.0, 0.1, Vectors.dense(1.0, 2.0)),\n+      Instance(1.0, 0.5, Vectors.dense(1.5, 1.0)),\n+      Instance(0.0, 0.3, Vectors.dense(4.0, 0.5))\n+    )\n+    instancesConstantFeature = Array(\n+      Instance(0.0, 0.1, Vectors.dense(1.0, 2.0)),\n+      Instance(1.0, 0.5, Vectors.dense(1.0, 1.0)),\n+      Instance(1.0, 0.3, Vectors.dense(1.0, 0.5)))\n+  }\n+\n+   /** Get summary statistics for some data and create a new HingeAggregator. */\n+  private def getNewAggregator(\n+      instances: Array[Instance],\n+      coefficients: Vector,\n+      fitIntercept: Boolean): HingeAggregator = {\n+    val (featuresSummarizer, ySummarizer) =\n+      DifferentiableLossAggregatorSuite.getClassificationSummarizers(instances)\n+    val featuresStd = featuresSummarizer.variance.toArray.map(math.sqrt)\n+    val bcFeaturesStd = spark.sparkContext.broadcast(featuresStd)\n+    val bcCoefficients = spark.sparkContext.broadcast(coefficients)\n+    new HingeAggregator(bcFeaturesStd, fitIntercept)(bcCoefficients)\n+  }\n+\n+  test(\"aggregator add method input size\") {\n+    val coefArray = Array(1.0, 2.0)\n+    val interceptArray = Array(2.0)\n+    val agg = getNewAggregator(instances, Vectors.dense(coefArray ++ interceptArray),\n+      fitIntercept = true)\n+    withClue(\"HingeAggregator features dimension must match coefficients dimension\") {\n+      intercept[IllegalArgumentException] {\n+        agg.add(Instance(1.0, 1.0, Vectors.dense(2.0)))\n+      }\n+    }\n+  }\n+\n+  test(\"negative weight\") {\n+    val coefArray = Array(1.0, 2.0)\n+    val interceptArray = Array(2.0)",
    "line": 79
  }],
  "prId": 18315
}]