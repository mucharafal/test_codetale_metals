[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Unit tests should be minimal. Pulling in JSON RDD is not necessary. Please check how `LabeledDocument` is defined and used in Java:\n\nhttps://github.com/apache/spark/blob/4a17eedb16343413e5b6f8bb58c6da8952ee7ab6/examples/src/main/scala/org/apache/spark/examples/ml/SimpleTextClassificationPipeline.scala#L29\n\nhttps://github.com/apache/spark/blob/4a17eedb16343413e5b6f8bb58c6da8952ee7ab6/examples/src/main/java/org/apache/spark/examples/ml/JavaCrossValidatorExample.java#L74\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-03-20T19:19:43Z",
    "diffHunk": "@@ -0,0 +1,73 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.DataFrame;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+\n+public class JavaTokenizerSuite {\n+  private transient JavaSparkContext jsc;\n+  private transient SQLContext jsql;\n+\n+  @Before\n+  public void setUp() {\n+    jsc = new JavaSparkContext(\"local\", \"JavaTokenizerSuite\");\n+    jsql = new SQLContext(jsc);\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    jsc.stop();\n+    jsc = null;\n+  }\n+\n+  @Test\n+  public void RegexTokenizer() {\n+    RegexTokenizer myRegExTokenizer = new RegexTokenizer()\n+      .setInputCol(\"rawText\")\n+      .setOutputCol(\"tokens\")\n+      .setPattern(\"\\\\s\")\n+      .setGaps(true)\n+      .setMinTokenLength(0);\n+\n+    List<String> t = Arrays.asList("
  }, {
    "body": "That was actually what I tried to do in the first place.\nAfter solving the problem of passing a java.util.List to a Scala object, the RDD created with the code you showed me results in a empty dataframe being created and the test fails as the column with the name demanded by the tokenizer is not found. After spending far too much time on this I changed to using the json to RDD.\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-03-23T10:15:57Z",
    "diffHunk": "@@ -0,0 +1,73 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.DataFrame;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+\n+public class JavaTokenizerSuite {\n+  private transient JavaSparkContext jsc;\n+  private transient SQLContext jsql;\n+\n+  @Before\n+  public void setUp() {\n+    jsc = new JavaSparkContext(\"local\", \"JavaTokenizerSuite\");\n+    jsql = new SQLContext(jsc);\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    jsc.stop();\n+    jsc = null;\n+  }\n+\n+  @Test\n+  public void RegexTokenizer() {\n+    RegexTokenizer myRegExTokenizer = new RegexTokenizer()\n+      .setInputCol(\"rawText\")\n+      .setOutputCol(\"tokens\")\n+      .setPattern(\"\\\\s\")\n+      .setGaps(true)\n+      .setMinTokenLength(0);\n+\n+    List<String> t = Arrays.asList("
  }],
  "prId": 4504
}]