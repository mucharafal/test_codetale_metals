[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "2-space indentation\n",
    "commit": "729167ab86e406a452509cb4d081efe378d8cacd",
    "createdAt": "2015-04-24T05:37:00Z",
    "diffHunk": "@@ -0,0 +1,100 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification;\n+\n+import java.io.Serializable;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.ml.impl.TreeTests;\n+import org.apache.spark.mllib.classification.LogisticRegressionSuite;\n+import org.apache.spark.mllib.regression.LabeledPoint;\n+import org.apache.spark.sql.DataFrame;\n+\n+\n+public class JavaGBTClassifierSuite implements Serializable {\n+\n+  private transient JavaSparkContext sc;\n+\n+  @Before\n+  public void setUp() {\n+    sc = new JavaSparkContext(\"local\", \"JavaGBTClassifierSuite\");\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    sc.stop();\n+    sc = null;\n+  }\n+\n+  @Test\n+  public void runDT() {\n+    int nPoints = 20;\n+    double A = 2.0;\n+    double B = -1.5;\n+\n+    JavaRDD<LabeledPoint> data = sc.parallelize(\n+        LogisticRegressionSuite.generateLogisticInputAsList(A, B, nPoints, 42), 2).cache();"
  }],
  "prId": 5626
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "``` java\nfor (String lossType: GBTClassifier.supportedLossTypes()) {\n ...\n}\n```\n",
    "commit": "729167ab86e406a452509cb4d081efe378d8cacd",
    "createdAt": "2015-04-24T05:37:04Z",
    "diffHunk": "@@ -0,0 +1,100 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification;\n+\n+import java.io.Serializable;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.ml.impl.TreeTests;\n+import org.apache.spark.mllib.classification.LogisticRegressionSuite;\n+import org.apache.spark.mllib.regression.LabeledPoint;\n+import org.apache.spark.sql.DataFrame;\n+\n+\n+public class JavaGBTClassifierSuite implements Serializable {\n+\n+  private transient JavaSparkContext sc;\n+\n+  @Before\n+  public void setUp() {\n+    sc = new JavaSparkContext(\"local\", \"JavaGBTClassifierSuite\");\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    sc.stop();\n+    sc = null;\n+  }\n+\n+  @Test\n+  public void runDT() {\n+    int nPoints = 20;\n+    double A = 2.0;\n+    double B = -1.5;\n+\n+    JavaRDD<LabeledPoint> data = sc.parallelize(\n+        LogisticRegressionSuite.generateLogisticInputAsList(A, B, nPoints, 42), 2).cache();\n+    Map<Integer, Integer> categoricalFeatures = new HashMap<Integer, Integer>();\n+    DataFrame dataFrame = TreeTests.setMetadata(data, categoricalFeatures, 2);\n+\n+    // This tests setters. Training with various options is tested in Scala.\n+    GBTClassifier rf = new GBTClassifier()\n+        .setMaxDepth(2)\n+        .setMaxBins(10)\n+        .setMinInstancesPerNode(5)\n+        .setMinInfoGain(0.0)\n+        .setMaxMemoryInMB(256)\n+        .setCacheNodeIds(false)\n+        .setCheckpointInterval(10)\n+        .setSubsamplingRate(1.0)\n+        .setSeed(1234)\n+        .setMaxIter(3)\n+        .setStepSize(0.1)\n+        .setMaxDepth(2); // duplicate setMaxDepth to check builder pattern\n+    for (int i = 0; i < GBTClassifier.supportedLossTypes().length; ++i) {"
  }],
  "prId": 5626
}]